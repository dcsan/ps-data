{
  "episodeId": "A9eo02FP9k8",
  "channelSlug": "@eightythousandhours",
  "title": "The bewildering frontier of consciousness in insects, AI, and more | 17 experts weigh in",
  "publishedAt": "2025-05-23T14:33:13.000Z",
  "rawLines": [
    {
      "lang": "en",
      "text": "Meghan Barrett: The reason I am not \ngiving you a sentience score is the  ",
      "offset": 0,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "error bars are so large right now that it’s \nalmost a meaningless number. Because I’m waiting  ",
      "offset": 4.16,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "on so much evidence. So much evidence. So I think \nthat’s really an essential feature of it for me.",
      "offset": 9.6,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "The second thing is that I worry especially \nas an expert that that number would be  ",
      "offset": 15.04,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "overemphasised, that somebody would inevitably \nput into a spreadsheet, and they would use that  ",
      "offset": 18.8,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "spreadsheet to make all kinds of decisions. And \nthat number does not reflect the complexity that  ",
      "offset": 22.72,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "you and I have now spent three hours discussing, \nand barely scratched the surface of. I want to  ",
      "offset": 26.72,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "talk about the complexity and the nuance, \nand a number does not demonstrate that.s",
      "offset": 31.36,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "I think it’s important also that we understand \nthat if you have updated at all towards insects  ",
      "offset": 34.88,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "plausibly being sentient, scale takes the \nrest of the issue for you to a serious place.  ",
      "offset": 40.4,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "There are so, so, so many of them \nthat if you take it seriously at all,  ",
      "offset": 46.56,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "then you need to be thinking \nthat this is an issue to work on.",
      "offset": 51.28,
      "duration": 6.383
    },
    {
      "lang": "en",
      "text": "Luisa Rodriguez: Hey listeners, Luisa here. \nYou were just listening to Meghan Barrett,  ",
      "offset": 57.663,
      "duration": 3.537
    },
    {
      "lang": "en",
      "text": "an insect neurobiologist whose mind-blowing \nfacts about insects really opened my  ",
      "offset": 61.2,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "eyes about the likelihood of invertebrate \nsentience when I interviewed her last year.",
      "offset": 65.44,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "We’re back today with another compilation \nof our favourite bits from past shows — this  ",
      "offset": 69.6,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "time on a topic that listeners might \nknow I’m absolutely captivated by:  ",
      "offset": 74.08,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "consciousness and sentience in \nnonhuman minds, including digital ones.",
      "offset": 78.64,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "Over the last few years, this area of study has \nreally taken off, and many of the guests you’ll  ",
      "offset": 83.44,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "hear in this episode are pioneers in their fields. \nWhether they’re entomologists, philosophers,  ",
      "offset": 88.48,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "machine learning researchers, or neuroscientists, \nthey’re all studying angles on slippery questions  ",
      "offset": 94.08,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "like what consciousness is (even in humans!), \nhow we’d recognise it in minds unlike our own,  ",
      "offset": 100,
      "duration": 7.12
    },
    {
      "lang": "en",
      "text": "and how much we should worry about other \nbeings’ ability to experience pleasure and pain.",
      "offset": 107.12,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "You’ll hear from:",
      "offset": 112.72,
      "duration": 0.88
    },
    {
      "lang": "en",
      "text": "David Chalmers on why artificial \nconsciousness is possible ",
      "offset": 113.6,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "Jeff Sebo on what the threshold is for \nAI systems meriting moral consideration ",
      "offset": 117.52,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "Jonathan Birch on how we very recently \nthought newborns couldn’t feel pain,  ",
      "offset": 122.96,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "and how we’re likely making similar mistakes today\nRobert Long on how we might stumble into causing  ",
      "offset": 128.16,
      "duration": 6.64
    },
    {
      "lang": "en",
      "text": "enormous suffering for digital minds\nMeghan Barrett on the evolutionary  ",
      "offset": 134.8,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "origins of consciousness and sentience, and \nwhether brain size and sentience are related ",
      "offset": 139.12,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "Plus many more!\nYou also may have noticed that this  ",
      "offset": 144.08,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "is a pretty long episode. We decided not to cut \nit down more, given that this issue is important,  ",
      "offset": 147.04,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "and long episodes on this topic haven’t deterred \nlisteners in the past… on the contrary, our 4 hour  ",
      "offset": 153.2,
      "duration": 6.8
    },
    {
      "lang": "en",
      "text": "and 40 minute marathon session with David \nChalmers was one of our most popular episodes!",
      "offset": 160,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "All right, I hope you enjoy!",
      "offset": 165.36,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "Robert Long on what we should picture \nwhen we think about artificial sentience",
      "offset": 169.92,
      "duration": 6.383
    },
    {
      "lang": "en",
      "text": "Luisa Rodriguez: I basically don’t feel like I \nhave a great sense of what artificial sentience  ",
      "offset": 176.303,
      "duration": 4.337
    },
    {
      "lang": "en",
      "text": "would even look like. Can you help me get \na picture of what we’re talking about here?",
      "offset": 180.64,
      "duration": 4.467
    },
    {
      "lang": "en",
      "text": "Robert Long: Yeah. I mean, I think \nit’s absolutely fine and correct to  ",
      "offset": 185.107,
      "duration": 3.213
    },
    {
      "lang": "en",
      "text": "not know what it would look like. In \nterms of what we’re talking about,  ",
      "offset": 188.32,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "I think the short answer, or a short \nhook into it, is just to think about  ",
      "offset": 191.68,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "the problem of animal sentience. I \nthink that’s structurally very similar.",
      "offset": 195.36,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "So, we share the world with a lot of nonhuman \nanimals, and they look a lot different than we do,  ",
      "offset": 199.92,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "they act a lot differently than we do. They’re \nsomewhat similar to us. We’re made of the same  ",
      "offset": 206.08,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "stuff, they have brains. But we often face this \nquestion of, as we’re looking at a bee going  ",
      "offset": 211.2,
      "duration": 6.72
    },
    {
      "lang": "en",
      "text": "through the field, like we can tell that it’s \ndoing intelligent behaviour, but we also wonder,  ",
      "offset": 217.92,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "is there something it’s like to be that bee? And \nif so, what are its experiences like? And what  ",
      "offset": 221.92,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "would that entail for how we should treat \nbees, or try to share the world with bees?",
      "offset": 227.12,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "I think the general problem of AI sentience is \nthat question, and also harder. So I’m thinking  ",
      "offset": 231.76,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "of it in terms of this kind of new class of \nintelligent or intelligent-seeming complex  ",
      "offset": 238,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "systems. And in addition to wondering what they’re \nable to do and how they do it, we can also wonder  ",
      "offset": 244.16,
      "duration": 7.28
    },
    {
      "lang": "en",
      "text": "if there is, or will ever be, something that it’s \nlike to be them, and if they’ll have experiences,  ",
      "offset": 251.44,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "if they’ll have something like pain or pleasure. \nIt’s a natural question to occur to people,  ",
      "offset": 256.16,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "and it’s occurred to me, and I’ve been trying \nto work on it in the past couple of years.",
      "offset": 260.24,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "Luisa Rodriguez: I guess I have an almost \neven more basic question, which is like,  ",
      "offset": 265.28,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "when we talk about AI sentience — both in \nthe short term and in the long term — are  ",
      "offset": 270.64,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "we talking about like a thing that looks \nlike my laptop, that has like a code on it,  ",
      "offset": 275.92,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "that has been coded to have some \nkind of feelings or experience?",
      "offset": 281.2,
      "duration": 6.627
    },
    {
      "lang": "en",
      "text": "Robert Long: Yeah, sure. I use the term \n“artificial sentience.” Very generally,  ",
      "offset": 287.827,
      "duration": 4.573
    },
    {
      "lang": "en",
      "text": "it’s just like things that are made out of \ndifferent stuff than us — in particular,  ",
      "offset": 292.4,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "silicon and the computational hardware \nthat we run these things on — could  ",
      "offset": 297.28,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "things built out of that and running \ncomputations on that have experiences?",
      "offset": 302.8,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "So the most straightforward case to imagine would \nprobably be a robot — because there, you can kind  ",
      "offset": 307.6,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "of clearly think about what the physical system \nis that you’re trying to ask if it’s sentient.",
      "offset": 312.72,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "Things are more complicated with the \nmore disembodied AI systems of today,  ",
      "offset": 318.4,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "like ChatGPT — because there, it’s like a \nvirtual agent in a certain sense. And brain  ",
      "offset": 323.44,
      "duration": 6.72
    },
    {
      "lang": "en",
      "text": "emulations would also be like virtual agents. \nBut I think for all of those, you can ask,  ",
      "offset": 330.16,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "at some level of description or some way \nof carving up the system, “Is there any  ",
      "offset": 335.6,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "kind of subjective experience here? Is there \nconsciousness here? Is there sentience here?”",
      "offset": 339.68,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "Luisa Rodriguez: Yeah, cool. I guess the reason \nI’m asking is because for a long time I’ve had  ",
      "offset": 343.76,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "this sense that when people use the term “digital \nminds” or “artificial sentience,” I have like some  ",
      "offset": 349.92,
      "duration": 7.52
    },
    {
      "lang": "en",
      "text": "vague images that kind of come from sci-fi, but \nI mostly feel like I don’t even know what we’re  ",
      "offset": 357.44,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "talking about. But it sounds like it could \njust look like a bunch of different things,  ",
      "offset": 363.36,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "and the core of it is something that \nis sentient — in maybe a way similar,  ",
      "offset": 367.84,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "maybe a way that’s pretty different to humans \n— but that exists not in biological form,  ",
      "offset": 373.6,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "but in some grouping that’s made up \nof silicon. Is that basically right?",
      "offset": 379.76,
      "duration": 5.581
    },
    {
      "lang": "en",
      "text": "Robert Long: Yeah. And I should say, I guess \nsilicon is not that deep here. But yeah,  ",
      "offset": 385.341,
      "duration": 6.179
    },
    {
      "lang": "en",
      "text": "something having to do with running on \ncomputers, running on GPUs. I’m sure  ",
      "offset": 391.52,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "I could slice and dice it, and you could \nget into all sorts of philosophical-like  ",
      "offset": 395.76,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "classification terms for things. But yeah, \nthat’s the general thing I’m pointing at.",
      "offset": 400.24,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "And I in particular have been working on the \nquestion of AI systems. The questions about  ",
      "offset": 404.72,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "whole brain emulations I think would be different, \nbecause we would have something that at some level  ",
      "offset": 410.32,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "of description is extremely similar to the human \nbrain by definition. And then you could wonder  ",
      "offset": 415.44,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "about whether it matters that it’s an emulated \nbrain, and people have wondered about that.",
      "offset": 420,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "In the case of AIs, it’s even \nharder — because not only are  ",
      "offset": 425.52,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "they made on different stuff and maybe \nsomewhat virtual, they also are kind of  ",
      "offset": 429.04,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "strange and not necessarily working along \nthe same principles as the human brain.",
      "offset": 435.12,
      "duration": 7.68
    },
    {
      "lang": "en",
      "text": "Jeff Sebo on what the threshold is for \nAI systems meriting moral consideration",
      "offset": 442.8,
      "duration": 7.04
    },
    {
      "lang": "en",
      "text": "Jeff Sebo: The general case for extending \nmoral consideration to AI systems is that  ",
      "offset": 449.84,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "they might be conscious or sentient or \nagential or otherwise significant. And  ",
      "offset": 453.84,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "if they might have those features, \nthen we should extend them at least  ",
      "offset": 458.8,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "some moral consideration in the \nspirit of caution and humility.",
      "offset": 462.56,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "So the standard should not be, “Do they \ndefinitely matter?” and it should also  ",
      "offset": 466.48,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "not be, “Do they probably matter?” \nIt should be, “Is there a reasonable,  ",
      "offset": 470.48,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "non-negligible chance that they matter, \ngiven the information available?” And  ",
      "offset": 475.12,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "once we clarify that that is the bar \nfor moral inclusion, then it becomes  ",
      "offset": 478.72,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "much less obvious that AI systems will \nnot be passing that bar anytime soon.",
      "offset": 483.28,
      "duration": 5.743
    },
    {
      "lang": "en",
      "text": "Luisa Rodriguez: Yeah, I feel kind of \nconfused about how to think about that bar,  ",
      "offset": 489.023,
      "duration": 4.817
    },
    {
      "lang": "en",
      "text": "where I think you’re using the term \n“non-negligible chance.” I’m curious:  ",
      "offset": 493.84,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "What is a negligible chance? Where is the line? \nAt what point is something non-negligible?",
      "offset": 499.52,
      "duration": 5.349
    },
    {
      "lang": "en",
      "text": "Jeff Sebo: Yeah, this is a perfectly reasonable \nquestion. This is somewhat of a term of art in  ",
      "offset": 504.869,
      "duration": 5.131
    },
    {
      "lang": "en",
      "text": "philosophy and decision theory. And we \nmight not be able to very precisely or  ",
      "offset": 510,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "reliably say exactly where the threshold is \nbetween non-negligible risks and negligible  ",
      "offset": 514.24,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "risks — but what we can say, as a starting \npoint, is that a risk can be quite low;  ",
      "offset": 520.16,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "the probability of harm can be quite low, and \nit can still be worthy of some consideration.",
      "offset": 525.6,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "For example: why is driving drunk wrong? Not \nbecause it will definitely kill someone. Not  ",
      "offset": 531.6,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "even because it will probably kill someone. \nIt might have only a 1-in-100 or 1-in-1,000  ",
      "offset": 536.4,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "chance of killing someone. But if driving drunk \nhas a 1-in-100 or 1-in-1,000 chance of killing  ",
      "offset": 541.44,
      "duration": 6.88
    },
    {
      "lang": "en",
      "text": "someone against their will unnecessarily, that \ncan be reason enough to get an Uber or a Lyft,  ",
      "offset": 548.32,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "or stay where I am and sober up. It at least \nmerits consideration, and it can even in some  ",
      "offset": 553.76,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "situations be decisive. So as a starting point, \nwe can simply acknowledge that in some cases  ",
      "offset": 559.36,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "a risk can be as low as one in 100 or one in \n1,000, and it can still merit consideration.",
      "offset": 565.2,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "Luisa Rodriguez: Right. It does seem totally \nclear and good that regularly in our daily lives  ",
      "offset": 570.72,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "we consider small risks of big things that might \nbe either very good or very bad. And we think  ",
      "offset": 577.68,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "that’s just clearly worth doing and sensible. \nSometimes probably, in personal experience,  ",
      "offset": 583.04,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "I may not do it as much as I should — but \non reflection, I certainly endorse it.",
      "offset": 589.2,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "So I guess the thinking here is that, \ngiven that there’s the potential for many,  ",
      "offset": 594.24,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "many, many beings with a potential for \nsentience, albeit some small likelihood,  ",
      "offset": 599.2,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "it’s kind of at that point that we \nmight start wanting to give them moral  ",
      "offset": 604.88,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "consideration. Do you want to say exactly what \nmoral consideration is warranted at that point?",
      "offset": 608.24,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "Jeff Sebo: This is a really good question, and \nit actually breaks down into multiple questions.",
      "offset": 614.32,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "One is a question about moral weight. We \nalready have a sense that we should give  ",
      "offset": 617.84,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "different moral weights to beings \nwith different welfare capacities:  ",
      "offset": 623.2,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "If an elephant can suffer much more than an \nant, then the elephant should get priority  ",
      "offset": 626.96,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "over the ant to that degree. Should we also \ngive more moral weight to beings who are more  ",
      "offset": 631.92,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "likely to matter in the first place? If \nan elephant is 90% likely to matter and  ",
      "offset": 636.72,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "an ant is 10% likely to matter, should I also \ngive the elephant more weight for that reason?",
      "offset": 640.96,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "And then another question is what these \nbeings might even want and need in the  ",
      "offset": 646.48,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "first place. What would it actually mean \nto treat an AI system well if they were  ",
      "offset": 650.8,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "sentient or otherwise morally significant? That \nquestion is going to be very difficult to answer.",
      "offset": 656.48,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "So there are no immediate implications to the idea \nthat we should give some moral consideration to AI  ",
      "offset": 661.76,
      "duration": 6.72
    },
    {
      "lang": "en",
      "text": "systems if they have a non-negligible chance \nof being sentient. All that it means is that  ",
      "offset": 668.48,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "we should give them at least some weight \nwhen making decisions that affect them,  ",
      "offset": 674.08,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "and then we might disagree about how \nmuch weight and what follows from that.",
      "offset": 678.56,
      "duration": 6.32
    },
    {
      "lang": "en",
      "text": "Meghan Barrett on the evolutionary \nargument for insect sentience",
      "offset": 684.88,
      "duration": 5.824
    },
    {
      "lang": "en",
      "text": "Meghan Barrett: Importantly, I think another \nfact that a lot of people don’t realise from this  ",
      "offset": 690.704,
      "duration": 3.856
    },
    {
      "lang": "en",
      "text": "evolutionary perspective is that insects are thus \nactually most closely related to crustaceans. They  ",
      "offset": 694.56,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "form this group together called the pancrustacea, \nand that group shares a common ancestor.",
      "offset": 700.4,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "So I think that’s something else that people \nshould consider. If you’re somebody who takes  ",
      "offset": 705.36,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "crustacean sentience seriously, and sentience \nis a trait that has evolved — it didn’t just  ",
      "offset": 708.48,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "appear in organisms as they are today, which I’m \nsure will be something we repeatedly touch on  ",
      "offset": 713.68,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "throughout this episode — if you’re somebody who \nbelieves that crustaceans are given the benefit  ",
      "offset": 718.24,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "of the doubt for you, for sentience, then you \nmight also think that insects are worth giving  ",
      "offset": 723.84,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "the benefit of the doubt for some reasons \nas well, from an evolutionary perspective.",
      "offset": 727.36,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "And there are two possible hypotheses you \ncould entertain here. One is the idea that  ",
      "offset": 730.96,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "sentience evolved exactly one time — and so \neverybody descended from that common ancestor,  ",
      "offset": 736.4,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "unless maybe they lost it for some reason, \nhas that characteristic. So if you accept  ",
      "offset": 740.96,
      "duration": 6.8
    },
    {
      "lang": "en",
      "text": "both vertebrates and any of the inverts — so a \ncephalopod or a decapod — if you’re convinced on  ",
      "offset": 747.76,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "a single invertebrate and you also are convinced \non a single origin point for this, you have  ",
      "offset": 754,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "a problem, right? Because the closest common \nancestor to all of those folks is very far back.",
      "offset": 759.52,
      "duration": 8.16
    },
    {
      "lang": "en",
      "text": "And so you’re going to have all your insects, \nall your nematodes, all your decapods,  ",
      "offset": 767.68,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "all your annelids (which are another kind of \nworm) included, if you believe that there’s  ",
      "offset": 773.36,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "one common ancestor and no loss events. \nNow, maybe you think there’s loss events,  ",
      "offset": 778.24,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "but now you’re talking about multiple loss \nevents, because there’s so many invertebrates.  ",
      "offset": 781.92,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "So you’re going to have to justify each of \nthose losses, which you could potentially do.",
      "offset": 786.56,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "Another possible hypothesis is \nthat sentience evolved multiple  ",
      "offset": 791.2,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "times independently in different groups. I would \nprobably say this is more plausible in my view,  ",
      "offset": 794.32,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "in part because we see multiple emergence of \nthings all the time in evolutionary history.",
      "offset": 798.64,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "Vision is a great example: we know that \neyes might have evolved as many as 40 times  ",
      "offset": 803.6,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "during animal evolutionary history. And then, \nwhen we think about the development of eyes,  ",
      "offset": 808,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "it’s crucial to consider how they all \ngenerate the same basic function of being  ",
      "offset": 812.24,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "able to see something, even though they \nmay vary in a lot of ways structurally.",
      "offset": 815.6,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "For instance, we saw the multiple emergence of \nwhat we call these crystalline lenses in the eyes  ",
      "offset": 820.56,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "of animals: some were made from co-opting calcite, \nothers were made by co-opting heat shock proteins,  ",
      "offset": 824.4,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "still others were made by co-opting other \nnovel proteins. All of them make these  ",
      "offset": 828.72,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "crystalline lenses, right? Or you could \nconsider that independent but convergent  ",
      "offset": 832.32,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "evolution of similar structures in vertebrate \neyes and spider eyes — and that can result in,  ",
      "offset": 836.56,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "again, the same basic capacity to see something.",
      "offset": 840.08,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "Of course, then we can talk about \nhow the exact functions of seeing  ",
      "offset": 843.36,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "using these different structures or \nsimilar structures can vary. You know,  ",
      "offset": 846.16,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "acuity can vary, or wavelengths that the \nanimal can sense can vary. But still,  ",
      "offset": 849.84,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "we think that the same basic capacity of some \nkind of sight is there for all of these animals.",
      "offset": 853.44,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "This is all to say that it makes it more \ncomplicated if we’re looking for something like  ",
      "offset": 859.2,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "consciousness. So the function we’re interested \nin is consciousness instead of vision. Now we’re  ",
      "offset": 862.8,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "saying that if there’s more than one origin \npoint, we need to be looking for potentially  ",
      "offset": 867.52,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "divergent structures capable of producing \nthat common basic function. And of course,  ",
      "offset": 870.96,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "that common basic function can have lots of \nvariance and gradation. And we don’t even have  ",
      "offset": 875.04,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "a good grasp yet on human consciousness, so \nyou can see how acknowledging the possibility  ",
      "offset": 879.2,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "of multiple independent origins would then \nmake this all very challenging to figure out.",
      "offset": 883.36,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "But in any case, I think when you look \nat this from an evolutionary perspective,  ",
      "offset": 888.24,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "it’s important to consider who’s a close relative? \nWho are your common ancestors for that group? When  ",
      "offset": 891.44,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "you think these characteristics evolved, why \ndo you think they evolved there? And then,  ",
      "offset": 896.16,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "if you’re somebody who takes crustaceans \nseriously, given that their close relation  ",
      "offset": 899.76,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "is the insects, you’re going to need \nto seriously consider the hexapods too.",
      "offset": 903.04,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "Andrés Jiménez Zorrilla on whether \nthere’s something it’s like to be a shrimp",
      "offset": 909.6,
      "duration": 6.228
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: How do shrimp farmers feel \nabout their shrimp? Do they naturally care  ",
      "offset": 915.828,
      "duration": 6.252
    },
    {
      "lang": "en",
      "text": "about their wellbeing, or see them \nas moral patients that can suffer?",
      "offset": 922.08,
      "duration": 4.463
    },
    {
      "lang": "en",
      "text": "Andrés Jiménez Zorrilla: That’s something that \nsurprised us. When we did a survey in India,  ",
      "offset": 926.543,
      "duration": 5.537
    },
    {
      "lang": "en",
      "text": "it’s a small sample, but we asked whether \nthey felt that their shrimps could feel  ",
      "offset": 933.2,
      "duration": 6.8
    },
    {
      "lang": "en",
      "text": "pain and stress, and 95% of them said yes. \nOne of them actually had a very endearing  ",
      "offset": 940,
      "duration": 7.52
    },
    {
      "lang": "en",
      "text": "answer saying that he spent more time with his \nanimals than he did with his family and that,  ",
      "offset": 947.52,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "if his friends suffered, he said, “I also suffer.”",
      "offset": 953.76,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "It’s very interesting. On the other hand, I think \nit’s unsurprising, because these people spend a  ",
      "offset": 957.2,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "significant amount of time seeing the behaviour \nof the animals. They’re much less skewed than  ",
      "offset": 963.36,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "consumers, who never see them alive. I’m almost \nbetting that you or your audience have very rarely  ",
      "offset": 970.08,
      "duration": 7.92
    },
    {
      "lang": "en",
      "text": "ever seen an image of a shrimp that’s alive and \nswimming. Most of them will have just seen them  ",
      "offset": 978,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "in cocktails. And these farmers just see them \nall the time. They see them when they’re sick.  ",
      "offset": 983.92,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "They see them when they’re feeding, when they’re \nswimming about — so they really care about them.",
      "offset": 988.64,
      "duration": 4.788
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: I see. So they’re exposed and \nseeing shrimp all the time. It sounds like  ",
      "offset": 993.428,
      "duration": 4.252
    },
    {
      "lang": "en",
      "text": "their behaviour is moderately complicated, that \nthey’re doing interesting things that make them  ",
      "offset": 997.68,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "seem smart enough and reactive and responsive \nenough to circumstances that it’s very natural  ",
      "offset": 1002.56,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "to feel that they can suffer or experience \npleasure the same way that dogs or pigs do.",
      "offset": 1007.44,
      "duration": 5.813
    },
    {
      "lang": "en",
      "text": "Andrés Jiménez Zorrilla: Exactly. I couldn’t have \nput it better. It’s very difficult to see them in  ",
      "offset": 1013.253,
      "duration": 3.307
    },
    {
      "lang": "en",
      "text": "farms, because the water in which they’re \nraised typically has high turbidity — it’s  ",
      "offset": 1016.56,
      "duration": 6.72
    },
    {
      "lang": "en",
      "text": "very murky. But once you actually see them — \nsometimes in tanks and in trade conferences  ",
      "offset": 1023.28,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "and things like that — when they’re fed, they \nswim, they catch their feed, they take it to  ",
      "offset": 1028.4,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "a little corner where each of them can eat it \nin peace. It’s very rare to see them behaving.  ",
      "offset": 1032.72,
      "duration": 7.36
    },
    {
      "lang": "en",
      "text": "There’s good research ongoing to understand \nbehavioural issues of shrimps. It’s underway.",
      "offset": 1040.08,
      "duration": 6.548
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: I see. So they’re perhaps \nmore like crabs or lobsters or octopus  ",
      "offset": 1046.628,
      "duration": 4.012
    },
    {
      "lang": "en",
      "text": "even than one might imagine, in terms \nof just how their behaviour looks.",
      "offset": 1050.64,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "Andrés Jiménez Zorrilla: Exactly. That was \nthe argument that the scientists at the  ",
      "offset": 1054.16,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "London School of Economics made when they wrote \nthis paper for the UK sentience bill recently,  ",
      "offset": 1057.44,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "in November last year. They did a full \nreview of the evidence of sentience of  ",
      "offset": 1062.56,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "cephalopod mollusks and decapods, which \nare exactly the ones that you mentioned.",
      "offset": 1068.24,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "What they found out is that, for those \nspecies that have been extensively researched,  ",
      "offset": 1074.08,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "there is very good evidence that they’re \nsentient. What they say is the evidence in  ",
      "offset": 1079.6,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "some other species is not as strong, but it’s only \nbecause they haven’t been researched for sentience  ",
      "offset": 1083.68,
      "duration": 7.04
    },
    {
      "lang": "en",
      "text": "purposes as much as other species — like, for \nexample, crabs and octopuses, as you said.",
      "offset": 1090.72,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: What sort of criteria are they using? \nWhen they study a species to try to figure out  ",
      "offset": 1096.32,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "whether it’s sentient, what sort of things are \nthey looking at to try to reach an evaluation?",
      "offset": 1101.68,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "Andrés Jiménez Zorrilla: That’s a good point,  ",
      "offset": 1105.44,
      "duration": 1.12
    },
    {
      "lang": "en",
      "text": "because it’s very difficult to have a smoking \ngun that tells you that an animal is sentient.",
      "offset": 1106.56,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "So in this specific case, what they did is \nthey looked at eight different indicators  ",
      "offset": 1112.16,
      "duration": 6.96
    },
    {
      "lang": "en",
      "text": "of sentience. Those included whether they \nhad nociceptors, so the right body parts  ",
      "offset": 1119.12,
      "duration": 6.8
    },
    {
      "lang": "en",
      "text": "to detect noxious stimuli; whether they had \nprotective behaviours, adaptive behaviours,  ",
      "offset": 1125.92,
      "duration": 6.72
    },
    {
      "lang": "en",
      "text": "if anaesthesia was applied to certain body \nparts — whether their reaction changed,  ",
      "offset": 1132.64,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "which would indicate that it’s \nnot a reflex. Things like that.",
      "offset": 1137.76,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "Then they ranked whether the evidence \nwas very high, high, to moderately low,  ",
      "offset": 1141.12,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "et cetera, for each individual species. Then \nthey came up with an overall assessment that  ",
      "offset": 1146.4,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "all cephalopods and decapods should \nbe protected by UK animal welfare law,  ",
      "offset": 1151.76,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "and eventually they did. This was a report \nthat was commissioned by the UK government  ",
      "offset": 1157.92,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "to the London School of Economics. \nIt was very independent research.",
      "offset": 1162.64,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: OK, so shrimp respond to injuries. \nThey probably learn from negative experiences that  ",
      "offset": 1167.52,
      "duration": 7.12
    },
    {
      "lang": "en",
      "text": "they have. Did they respond to anaesthetic? I know \nthat’s one of the tests that people sometimes use.",
      "offset": 1174.64,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "Andrés Jiménez Zorrilla: There’s a paper that \nshows the responses that different decapods  ",
      "offset": 1179.04,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "have to anaesthetics, as you said. With shrimps \nin particular, what they do is they pinch one  ",
      "offset": 1185.28,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "of the antennas. They see how they behave: they \nflick their tails, they jump out of the water,  ",
      "offset": 1190.96,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "et cetera. And then in the second stage, they \napply anaesthetics and repeat the experiment,  ",
      "offset": 1196.72,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "and the behaviour changes significantly. \nThe time that they rub their little antenna  ",
      "offset": 1201.44,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "is much, much lower. They probably swim \nnormally, quicker, and things like that.",
      "offset": 1207.04,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: I see. They are tending to injuries, \nand they tend to them less when they’re given  ",
      "offset": 1213.04,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "anaesthetic. As an aside, it’s remarkable to me \nthat anaesthetics that we’ve presumably developed  ",
      "offset": 1218.24,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "for humans also work on shrimp. They’re so \nfar away in the phylogenetic tree of life,  ",
      "offset": 1223.2,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "and yet so much of the basic machinery \nof feeling seems to be similar enough.",
      "offset": 1228.32,
      "duration": 5.733
    },
    {
      "lang": "en",
      "text": "Andrés Jiménez Zorrilla: That’s true. One of \nthe arguments that some people made used to  ",
      "offset": 1234.053,
      "duration": 4.427
    },
    {
      "lang": "en",
      "text": "be that opioids don’t necessarily work the \nsame in some of the animals. As you’ve said,  ",
      "offset": 1238.48,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "not necessarily all of the anaesthetics need \nto work the same. But researchers have found  ",
      "offset": 1244.08,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "anaesthetics that do apply and do have an \neffect on animals, and it changes behaviour.",
      "offset": 1248.72,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: An audience \nmember was curious to know:  ",
      "offset": 1255.2,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "do you feel viscerally motivated by \nthe prospect of shrimp suffering,  ",
      "offset": 1258.24,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "or is your interest and motivation \nsomewhat more on the intellectual side?",
      "offset": 1262.4,
      "duration": 5.093
    },
    {
      "lang": "en",
      "text": "Andrés Jiménez Zorrilla: So until very recently,  ",
      "offset": 1267.493,
      "duration": 1.627
    },
    {
      "lang": "en",
      "text": "my response would’ve been completely on the \nintellectual side. It was not until I visited  ",
      "offset": 1269.12,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "farms. I think most of us during our lifetimes \nwill never visit a shrimp farm, or most people  ",
      "offset": 1275.2,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "in the world will not visit a shrimp farm or \nsee a shrimp being taken out of the water.",
      "offset": 1280.8,
      "duration": 6.36
    },
    {
      "lang": "en",
      "text": "But when my cofounder, our programme director \nin India, and myself went to see what is called  ",
      "offset": 1287.16,
      "duration": 7.8
    },
    {
      "lang": "en",
      "text": "“harvesting” — which is the moment in which \nthe animals are scooped out of the water  ",
      "offset": 1294.96,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "and eventually put in crates and things like \nthat — that process made me also viscerally  ",
      "offset": 1299.28,
      "duration": 6.64
    },
    {
      "lang": "en",
      "text": "care about this issue. But it definitely \ncame through the more intellectual part.",
      "offset": 1305.92,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "Jonathan Birch on the \ncautionary tale of newborn pain",
      "offset": 1314,
      "duration": 5.583
    },
    {
      "lang": "en",
      "text": "Luisa Rodriguez: Let’s turn to another \nedge case of sentience in humans:  ",
      "offset": 1319.583,
      "duration": 4.097
    },
    {
      "lang": "en",
      "text": "foetuses. You start this section \nof the book with what you call “the  ",
      "offset": 1324.24,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "cautionary tale of newborn pain.” \nCan you talk me through that case?",
      "offset": 1328.48,
      "duration": 5.504
    },
    {
      "lang": "en",
      "text": "Jonathan Birch: It’s another case \nI found unbelievable: in the 1980s,  ",
      "offset": 1333.984,
      "duration": 5.856
    },
    {
      "lang": "en",
      "text": "it was still apparently common to perform \nsurgery on newborn babies without anaesthetic  ",
      "offset": 1339.84,
      "duration": 6.8
    },
    {
      "lang": "en",
      "text": "on both sides of the Atlantic. This led \nto appalling cases, and to public outcry,  ",
      "offset": 1346.64,
      "duration": 9.36
    },
    {
      "lang": "en",
      "text": "and to campaigns to change clinical practice. \nThere was a public campaign led by someone  ",
      "offset": 1356,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "called Jill Lawson, whose baby son had \nbeen operated on in this way and had died.",
      "offset": 1360.56,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "And at the same time, evidence was being \ngathered to bear on the questions by some  ",
      "offset": 1366.56,
      "duration": 6.8
    },
    {
      "lang": "en",
      "text": "pretty courageous scientists, I would say. They \ngot very heavily attacked for doing this work,  ",
      "offset": 1373.36,
      "duration": 6.56
    },
    {
      "lang": "en",
      "text": "but they knew evidence was needed to change \nclinical practice. And they showed that,  ",
      "offset": 1379.92,
      "duration": 6.32
    },
    {
      "lang": "en",
      "text": "if this protocol is done, there were \nmassive stress responses in the baby,  ",
      "offset": 1387.44,
      "duration": 8.4
    },
    {
      "lang": "en",
      "text": "massive stress responses that reduce the \nchances of survival and lead to long-term  ",
      "offset": 1395.84,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "developmental damage. So as soon as they \nlooked for evidence, the evidence showed  ",
      "offset": 1401.2,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "that this practice was completely indefensible \nand then the clinical practice was changed.",
      "offset": 1406.96,
      "duration": 7.76
    },
    {
      "lang": "en",
      "text": "So, in a way, people don’t need convincing \nanymore that we should take newborn human  ",
      "offset": 1414.72,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "babies seriously as sentience candidates. \nBut the tale is a useful cautionary tale,  ",
      "offset": 1419.6,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "because it shows you how deep that overconfidence \ncan run and how problematic it can be. It just  ",
      "offset": 1425.28,
      "duration": 6.96
    },
    {
      "lang": "en",
      "text": "underlines this point that overconfidence \nabout sentience is everywhere and is dangerous.",
      "offset": 1432.24,
      "duration": 5.503
    },
    {
      "lang": "en",
      "text": "Luisa Rodriguez: Yeah, it really does. I’m \nsure that had I lived in a different time,  ",
      "offset": 1437.743,
      "duration": 7.457
    },
    {
      "lang": "en",
      "text": "I’d at least have been much more susceptible \nto this particular mistake. But from where  ",
      "offset": 1445.2,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "I’m standing now, it’s impossible for me to \nimagine thinking that newborns don’t feel pain,  ",
      "offset": 1450.16,
      "duration": 8.16
    },
    {
      "lang": "en",
      "text": "and therefore you can do massively invasive \nsurgery on them without anaesthetic.",
      "offset": 1458.32,
      "duration": 5.104
    },
    {
      "lang": "en",
      "text": "Jonathan Birch: It’s a hard one to believe, \nisn’t it? Of course, the consideration was  ",
      "offset": 1463.424,
      "duration": 4.496
    },
    {
      "lang": "en",
      "text": "sometimes made that anaesthesia has risks — \nand of course it does, but operating without  ",
      "offset": 1467.92,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "anaesthesia also has risks. So there was real \nnaivete about how the surgeons here were thinking  ",
      "offset": 1473.84,
      "duration": 6.88
    },
    {
      "lang": "en",
      "text": "about risk. And it’s what philosophers of science \nsometimes called the “epistemology of ignorance”:  ",
      "offset": 1480.72,
      "duration": 7.28
    },
    {
      "lang": "en",
      "text": "they were worried about the risks of anaesthesia, \nwhich is their job to worry about that,  ",
      "offset": 1488,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "so they just neglected the risks on the other \nside. That’s the truly unbelievable thing.",
      "offset": 1494.24,
      "duration": 6.303
    },
    {
      "lang": "en",
      "text": "Luisa Rodriguez: So the clinical updates that \nhave happened since: my sense is that now it  ",
      "offset": 1500.543,
      "duration": 6.657
    },
    {
      "lang": "en",
      "text": "is standard procedure to give newborns \nanaesthetic during surgeries, and that  ",
      "offset": 1507.2,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "the benefits outweigh the risks. You argue that \nthat wasn’t inevitable. What’s the case for that?",
      "offset": 1511.68,
      "duration": 8
    },
    {
      "lang": "en",
      "text": "Jonathan Birch: I think the public outcry also \nmattered. Clinical norms are very hard to shift.  ",
      "offset": 1519.68,
      "duration": 6.64
    },
    {
      "lang": "en",
      "text": "If it was really just these two people, Anand \nand Hickey, against the medical establishment,  ",
      "offset": 1526.32,
      "duration": 7.36
    },
    {
      "lang": "en",
      "text": "that’s not really how change happens. You know, \nwe talk about theories of change sometimes — and  ",
      "offset": 1533.68,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "“just get the evidence and take it to the \nestablishment” is not a good theory of change.  ",
      "offset": 1539.76,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "I think in this case, the fact that there was at \nthe same time a powerful public campaign going on  ",
      "offset": 1544.64,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "based on these horrible stories, that is why \nclinical practice got changed very quickly.",
      "offset": 1550.48,
      "duration": 6.863
    },
    {
      "lang": "en",
      "text": "Luisa Rodriguez: So there’s a lesson there. \nI mean, there’s really nothing to say,  ",
      "offset": 1557.343,
      "duration": 4.417
    },
    {
      "lang": "en",
      "text": "but that that’s horrific, and important to know \nit happened, because we may be doing it again.",
      "offset": 1561.76,
      "duration": 11.2
    },
    {
      "lang": "en",
      "text": "David Chalmers on why artificial \nconsciousness is possible",
      "offset": 1572.96,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "David Chalmers: Some people think that no \nsilicon-based computational system could  ",
      "offset": 1578.56,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "be conscious because biology is required. \nI’m inclined to reject views like that,  ",
      "offset": 1582.32,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "and there’s nothing special \nabout the biology here.",
      "offset": 1587.92,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "One way to think about that is to \nthink about cases of gradual uploading:  ",
      "offset": 1590.72,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "you replace your neurons one at a time by \nsilicon chips that play the same role. I  ",
      "offset": 1594,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "think cases like this make it particularly \nhard to say that, if you say that the system  ",
      "offset": 1599.04,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "at the other end is not conscious, then \nyou have to say that consciousness either  ",
      "offset": 1602.88,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "gradually fades out or during this process or \nit suddenly disappears during this process.",
      "offset": 1607.2,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "I think it’s at least difficult to maintain either \nof those lines. You could take the line that maybe  ",
      "offset": 1612.96,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "silicon will never even be able to simulate \nbiological neurons very well, even in terms of  ",
      "offset": 1617.68,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "its effects. Maybe there’s some special dynamic \nproperties that biology has that silicon could  ",
      "offset": 1621.84,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "never have. I think that would be very surprising, \nbecause it looks like all the laws of physics we  ",
      "offset": 1626.88,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "know about right now are computational. Roger \nPenrose has entertained the idea that’s false.",
      "offset": 1630.64,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "But if we assume that physics is computational, \nthat one can in principle simulate the action  ",
      "offset": 1634.96,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "of a physical system, then one ought to at least \nbe able to create one of these gradual uploading  ",
      "offset": 1639.28,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "processes and then someone who denies that the \nsystem on the other end could be conscious is  ",
      "offset": 1644.08,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "going to have to say either it fades out in a \nreally weird way during this process. You go  ",
      "offset": 1648.08,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "through half consciousness, quarter consciousness, \nwhile your behaviour stays the same,  ",
      "offset": 1652.88,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "or that it suddenly disappears at some point. \nYou replace the magic neuron and it disappears.",
      "offset": 1657.44,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "Those are arguments I gave years \nago now for why I think a silicon  ",
      "offset": 1662.48,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "duplicated device can be conscious in principle.",
      "offset": 1666.8,
      "duration": 2.305
    },
    {
      "lang": "en",
      "text": "Arden Koehler: I think I see why the \nsudden disappearance of consciousness  ",
      "offset": 1669.105,
      "duration": 4.495
    },
    {
      "lang": "en",
      "text": "in that scenario seems implausible. \nIt’s like, “Well, what’s so special  ",
      "offset": 1673.6,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "about that magic neuron?” But \nI don’t immediately see why the  ",
      "offset": 1677.2,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "gradual fadeout of consciousness isn’t a \nreasonable possibility to entertain there?",
      "offset": 1681.04,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "David Chalmers: How are you thinking \nthe gradual fadeout would go? First,  ",
      "offset": 1686.72,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "we’d lose visual consciousness, then \nwe’d lose auditory consciousness? Or…?",
      "offset": 1691.12,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "Arden Koehler: I don’t know exactly \nhow it would go, but if we assume  ",
      "offset": 1694.64,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "that consciousness can come in degrees, \nthen why can’t it disappear in degrees?",
      "offset": 1699.2,
      "duration": 6.144
    },
    {
      "lang": "en",
      "text": "David Chalmers: Yeah, I guess I’m \nthinking that I just put some crude  ",
      "offset": 1705.344,
      "duration": 3.056
    },
    {
      "lang": "en",
      "text": "measure on a state of consciousness. Like \nthe number of bits involved in your state  ",
      "offset": 1708.4,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "of consciousness. One way of imagining it \nfading is somehow lowering in intensity and  ",
      "offset": 1712.64,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "then suddenly the intensity goes \nto zero and it all disappears.",
      "offset": 1716.72,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "That to me sounds like a version of sudden \ndisappearance because the bits which still  ",
      "offset": 1719.76,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "go from being a million bits to zero bits all at \nonce. Strange in the way that sudden disappearance  ",
      "offset": 1723.52,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "is strange. Maybe looking more continuous then \nsomehow the number of bits in your consciousness  ",
      "offset": 1728.96,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "has to gradually decrease. You go from a \nmillion bits to 100,000 to 10,000 to whatever.",
      "offset": 1733.84,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "And how would this work? Maybe my \nvisual field would gradually lose  ",
      "offset": 1740.32,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "distinctions, will gradually \nbecome more coarse-grained,  ",
      "offset": 1744.24,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "maybe bits of it would disappear. Maybe one \nmodality would go and then another modality.",
      "offset": 1747.04,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "But anyway, you’re going to have these weird \nintermediate states where you say the system is  ",
      "offset": 1752.24,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "conscious, is saying it is fully conscious of all \nthese things because its behaviour is the same,  ",
      "offset": 1756.64,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "“I am visually conscious in full detail, \nI’m auditorily conscious.” In fact,  ",
      "offset": 1761.28,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "their consciousness state is going \nto be a very, very pale reflection  ",
      "offset": 1765.92,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "of the conscious state they’re talking about \nwith very few, say bits, of consciousness.",
      "offset": 1769.84,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "That situation is the one that strikes me \nas especially strange. A conscious being  ",
      "offset": 1775.68,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "that appears to be fully rational and believes \nit’s in this fully conscious state, but in fact,  ",
      "offset": 1780.88,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "it’s in a very, very limited conscious \nstate. If you’re an illusionist,  ",
      "offset": 1785.84,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "you might think this kind of \nthing happens to you all the time.",
      "offset": 1789.52,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: I think this is totally wrong. \nIt seems like you could have a view where  ",
      "offset": 1792.8,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "there’s information processing going on in the \ntransmission between the neurons and that’s what’s  ",
      "offset": 1797.12,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "generating the behaviour. But then there’s \nsome other secret sauce that’s happening in  ",
      "offset": 1800.8,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "the brain that we don’t understand and that we \nwould not then replicate on the silicon chips.",
      "offset": 1804.48,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "As you go replacing each neuron and each \nsynapse with the machine version of it,  ",
      "offset": 1808.4,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "the information processing continues as before \nand the behaviour remains the same. But you’ve  ",
      "offset": 1813.04,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "lost the part that was generating the \nconsciousness; you haven’t engineered  ",
      "offset": 1817.76,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "that into the computer components, and so \njust gradually the consciousness disappears.",
      "offset": 1820.72,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "David Chalmers: I can imagine \nthis is at least conceivable,  ",
      "offset": 1826.48,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "but then what are you going to \nsay about the intermediate cases?  ",
      "offset": 1830.56,
      "duration": 2.56
    },
    {
      "lang": "en",
      "text": "There will have to be cases where the being \nis conscious and just massively wrong about  ",
      "offset": 1834.56,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "its own consciousness. It says you’re not \nhaving experiences of red and green and  ",
      "offset": 1839.44,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "orange right now. The fact is that it’s having \na uniform gray visual field or thing like that.",
      "offset": 1845.6,
      "duration": 4.545
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: It seems possible, right?",
      "offset": 1850.145,
      "duration": 0.72
    },
    {
      "lang": "en",
      "text": "Arden Koehler: I guess I also don’t find \nit as implausible as you seem to, Dave,  ",
      "offset": 1850.865,
      "duration": 3.935
    },
    {
      "lang": "en",
      "text": "that we could be wrong about our \nconscious experience or how much  ",
      "offset": 1854.8,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "conscious experience we’re having \nin this gradual uploading example.",
      "offset": 1859.36,
      "duration": 4.624
    },
    {
      "lang": "en",
      "text": "David Chalmers: That’s fair, and there certainly \nare many cases where people are very wrong about  ",
      "offset": 1863.984,
      "duration": 4.576
    },
    {
      "lang": "en",
      "text": "their own conscious experiences. Certainly, \nthere are all kinds of pathologies where  ",
      "offset": 1868.56,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "there’s blindness denial — where people \nsay they’re having all kinds of visual  ",
      "offset": 1873.2,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "experiences when it appears that they’re blind \nand they’re not having them. Maybe it could be  ",
      "offset": 1877.6,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "like this. This is strange because functionally \nthe system doesn’t seem to have any pathologies.",
      "offset": 1882.8,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "Anyway, I do allow that this \nis conceivable and I certainly  ",
      "offset": 1887.92,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "can’t prove that it couldn’t happen. The \nmore open you are to beings being very,  ",
      "offset": 1891.76,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "very wrong about their consciousness, \nmaybe you’ll be more open to this case.",
      "offset": 1895.44,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "Here’s one thing I’ll say, at the very least: if \nthis actually happens, and we go through it and  ",
      "offset": 1899.28,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "our behavior is the same throughout, then we have \nbeings whose heads are first a quarter silicon,  ",
      "offset": 1903.36,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "then a half silicon as well. They say, \n“Everything is fine, everything is fine,  ",
      "offset": 1909.28,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "my conscious experience is exactly the \nway it was.” They’re telling us this,  ",
      "offset": 1913.68,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "they’re talking to us. They update every week with \na bit more silicon and we keep talking to them.",
      "offset": 1917.28,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "We are going to be very nearly completely \nconvinced that they are conscious throughout.  ",
      "offset": 1923.76,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "It’s going to become impossible to deny \nit. So at least as a matter of sociology,  ",
      "offset": 1929.6,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "I think this view is likely to \nbecome the obvious-seeming view.",
      "offset": 1933.52,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "Holden Karnofsky on how we’ll \nsee digital people as… people",
      "offset": 1938.56,
      "duration": 5.748
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: Let’s talk about these digital \npeople. What do you mean by digital people?",
      "offset": 1944.308,
      "duration": 4.172
    },
    {
      "lang": "en",
      "text": "Holden Karnofsky: So the basic idea of a digital \nperson is like a digital simulation of a person.  ",
      "offset": 1948.48,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "It’s really like if you just take one \nof these video games, like The Sims,  ",
      "offset": 1953.84,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "or… I use the example of a football game because \nI was able to get these different pictures of this  ",
      "offset": 1957.92,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "football player, Jerry Rice, because every year \nthey put out a new Madden video game. So, Jerry  ",
      "offset": 1963.84,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "Rice looks a little more realistic every year. \nYou have these video game simulations of people,  ",
      "offset": 1969.2,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "and if you just imagine it getting more and more \nrealistic until you have a perfect simulation.",
      "offset": 1974.48,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "So imagine a video game that has a character \ncalled Holden, and just does everything exactly  ",
      "offset": 1979.52,
      "duration": 7.52
    },
    {
      "lang": "en",
      "text": "how Holden would in response to whatever \nhappens. That’s it. That’s what a digital  ",
      "offset": 1987.04,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "person is. So it’s a fairly simple idea. In \nsome ways it’s a very far-out extrapolation  ",
      "offset": 1991.2,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "of stuff we’re already doing, which is \nwe’re already simulating these characters.",
      "offset": 1997.12,
      "duration": 4.628
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: I guess that’d be one way to look \nat it. The way I’ve usually heard it discussed  ",
      "offset": 2001.748,
      "duration": 4.252
    },
    {
      "lang": "en",
      "text": "or introduced is the idea that, well, we have \nthese brains and they’re doing calculations,  ",
      "offset": 2006,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "and couldn’t we eventually figure out \nhow to basically do all of the same  ",
      "offset": 2009.52,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "calculations that the brain is doing in \na simulation of the brain moving around?",
      "offset": 2012.32,
      "duration": 3.742
    },
    {
      "lang": "en",
      "text": "Holden Karnofsky: Yeah, exactly. You would have \na simulated brain in a simulated environment.  ",
      "offset": 2016.062,
      "duration": 3.458
    },
    {
      "lang": "en",
      "text": "Absolutely, that’s another way to think of it.",
      "offset": 2021.36,
      "duration": 1.84
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: This is a fairly out-there \ntechnology, the idea that we would be able  ",
      "offset": 2023.2,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "to reproduce a full human being, or at least \nthe most important parts of a human being,  ",
      "offset": 2027.68,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "running on a server. Why think \nthat’s likely to be possible?",
      "offset": 2032.08,
      "duration": 4.382
    },
    {
      "lang": "en",
      "text": "Holden Karnofsky: I mean, I think it’s \nsimilar to what I said before. We have  ",
      "offset": 2036.462,
      "duration": 2.818
    },
    {
      "lang": "en",
      "text": "this existence proof. We have these brains. \nThere’s lots of them, and all we’re trying  ",
      "offset": 2039.28,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "to do is build a computer program that can \nprocess information just how a brain would.",
      "offset": 2044.32,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "A really expensive and dumb way of doing it would \nbe to just simulate the brain in all its detail,  ",
      "offset": 2052,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "just simulate everything that’s going on in the \nbrain. But there may just be smarter and easier  ",
      "offset": 2057.28,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "ways to do it, where you capture the level of \nabstraction that matters. So maybe it doesn’t  ",
      "offset": 2061.92,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "matter what every single molecule in the brain \nis doing. Maybe a lot of that stuff is random,  ",
      "offset": 2065.84,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "and what really is going on that’s interesting, \nor important, or doing computational work in the  ",
      "offset": 2070.8,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "brain is maybe the neurons firing and some \nother stuff, and you could simulate that.",
      "offset": 2076.16,
      "duration": 7.04
    },
    {
      "lang": "en",
      "text": "But basically, there’s this process going on. It’s \ngoing on in a pretty small physical space. We have  ",
      "offset": 2083.2,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "tonnes of examples of it. We can literally study \nanimal brains. We do. I mean, neuroscientists  ",
      "offset": 2088.64,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "just pick them apart and study them and try \nto see what’s going on inside them. And so,  ",
      "offset": 2094.48,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "I’m not saying we’re close to being able to do \nthis, but when I try to think about why would  ",
      "offset": 2098.48,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "it be impossible, why would it be impossible to \nbuild an artefact, to build a digital artefact or  ",
      "offset": 2102.88,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "a computer that’s processing information just how \na brain would, and I guess I just come up empty.",
      "offset": 2107.28,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "But I can’t prove that this is possible. But \nyeah, the basic argument is just, it’s here,  ",
      "offset": 2112.64,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "it’s all around us. Why wouldn’t we be \nable to simulate it at some point in time?",
      "offset": 2118,
      "duration": 5.508
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: Are you envisaging these digital \npeople as being conscious like you and me,  ",
      "offset": 2123.508,
      "duration": 4.332
    },
    {
      "lang": "en",
      "text": "or is it more like an automaton situation?",
      "offset": 2127.84,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "Holden Karnofsky: One of the things that’s \ncome up is when I describe this idea of a  ",
      "offset": 2130.72,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "world of digital people, a lot of people have the \nintuition that even if digital people were able  ",
      "offset": 2135.52,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "to act just like real people, they wouldn’t count \nmorally the same way. They wouldn’t have feelings.  ",
      "offset": 2140.24,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "They wouldn’t have experiences. They wouldn’t \nbe conscious. We shouldn’t care about them.",
      "offset": 2145.84,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "And that’s an intuition that I disagree with. \nIt’s not a huge focus of the series, but I  ",
      "offset": 2149.2,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "do write about it. Basically if you dig all the \nway into philosophy of mind and think about what  ",
      "offset": 2152.88,
      "duration": 8.56
    },
    {
      "lang": "en",
      "text": "consciousness is, this is something we’re all very \nconfused about. No one has the answer to that. But  ",
      "offset": 2161.44,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "I think in general, there isn’t a great reason to \nthink that whatever consciousness is, it crucially  ",
      "offset": 2165.68,
      "duration": 6.32
    },
    {
      "lang": "en",
      "text": "relies on being made out of neurons instead \nof being made out of microchips or whatever.",
      "offset": 2172,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "And one way of thinking about this is, I think \nI’m conscious. Why do I think that? Is the fact  ",
      "offset": 2176.24,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "that I think I’m conscious, is that connected to \nthe actual truth of me being conscious? Because  ",
      "offset": 2182,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "the thing that makes me think I’m conscious has \nnothing to do with whether my brain is made out of  ",
      "offset": 2186.96,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "neurons. If you made a digital copy of me and you \nsaid, “Hey, Holden, are you conscious?” That thing  ",
      "offset": 2190.56,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "would say, “Yes, of course I am,” for the same \nexact reason I’m doing it: it would be processing  ",
      "offset": 2194.8,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "all the same information, it’d be considering \nall the same evidence, and it would say yes.",
      "offset": 2199.12,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "There’s this intuition that whatever \nconsciousness is, if we believe it’s  ",
      "offset": 2203.28,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "what’s causing us to think we’re conscious, \nthen it seems like it’s something about the  ",
      "offset": 2207.52,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "software our brain is running, or the algorithm \nit’s doing, or the information it’s processing.  ",
      "offset": 2212.72,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "It’s not something about the material the brain \nis made of. Because if you change that material,  ",
      "offset": 2216.72,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "you wouldn’t get different answers. \nYou wouldn’t get different beliefs.",
      "offset": 2220.96,
      "duration": 2.4
    },
    {
      "lang": "en",
      "text": "That’s the intuition. I’m not going to go into \nit a tonne more than that. There’s a thought  ",
      "offset": 2223.36,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "experiment that’s interesting that I got from \nDavid Chalmers, where you imagine that if you took  ",
      "offset": 2227.76,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "your brain and you just replaced one neuron with a \ndigital signal transmitter that just fired in all  ",
      "offset": 2233.68,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "the same exact ways, you wouldn’t notice anything \nchanging. You couldn’t notice anything changing,  ",
      "offset": 2239.12,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "because your brain would be doing all the \nsame things, and you’d be reaching all the  ",
      "offset": 2243.44,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "same conclusions. You’d be having all the same \nthoughts. Now, if you replaced another one,  ",
      "offset": 2246.16,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "you wouldn’t notice anything, and if you \nreplaced them all, you wouldn’t notice anything.",
      "offset": 2249.6,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "Anyway, so I think there’s some arguments out \nthere. But I think it is the better bet that  ",
      "offset": 2253.68,
      "duration": 7.2
    },
    {
      "lang": "en",
      "text": "if we had digital people that were acting \njust like us, and the digital brains were  ",
      "offset": 2260.88,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "doing the same thing as our brains, that \nwe should care about them. And we should  ",
      "offset": 2266.24,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "think of them as people. And we probably \nwould. Even if they weren’t conscious —",
      "offset": 2269.36,
      "duration": 5.4
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: Because they would act like it.",
      "offset": 2274.76,
      "duration": 0.12
    },
    {
      "lang": "en",
      "text": "Holden Karnofsky: Yeah, well, \nwe’d be friends with them.",
      "offset": 2274.88,
      "duration": 2.497
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: They would complain the same way.",
      "offset": 2277.377,
      "duration": 0.045
    },
    {
      "lang": "en",
      "text": "Holden Karnofsky: We’d talk to them and we would \nrelate to them. There are people I’ve never met,  ",
      "offset": 2277.422,
      "duration": 4.898
    },
    {
      "lang": "en",
      "text": "and they would just be like any \nother people I’ve never met,  ",
      "offset": 2282.32,
      "duration": 2.16
    },
    {
      "lang": "en",
      "text": "but I could have video calls with them \nand phone calls with them. And so,  ",
      "offset": 2284.48,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "we probably will and should care about what \nhappens to them. And even if we don’t, it only  ",
      "offset": 2288.08,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "changes some of the conclusions. But I basically \nthink that digital people would be people too.",
      "offset": 2292.64,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: I mean, the argument that jumps \nto mind for me is if you’re saying, “Well,  ",
      "offset": 2296.88,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "to be people, to be conscious, to have \nvalue, it has to be run on meat. It has  ",
      "offset": 2301.6,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "to be run on these cells with these electrical \ncharges going back and forth.” It’d be like,  ",
      "offset": 2304.72,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "“Did evolution just happen to stumble \non the one material that could do this?  ",
      "offset": 2308,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "Evolution presumably didn’t choose to use this \nparticular design because you’d be conscious.  ",
      "offset": 2312.96,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "So why would there be this coincidence that \nwe have the one material out of all of the  ",
      "offset": 2317.36,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "different materials that can do these \ncalculations that produces moral value?”",
      "offset": 2321.76,
      "duration": 3.662
    },
    {
      "lang": "en",
      "text": "Holden Karnofsky: That’s an interesting way \nof thinking of it. If I were to play devil’s  ",
      "offset": 2325.422,
      "duration": 2.738
    },
    {
      "lang": "en",
      "text": "advocate, I would be like, “Well, maybe every \nmaterial has its own kind of consciousness,  ",
      "offset": 2328.16,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "and we only care about the kind that’s \nlike our kind,” or something. But then  ",
      "offset": 2331.84,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "that would be an interesting question \nwhy we only care about our kind.",
      "offset": 2334.72,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "Jeff Sebo on grappling with our biases and \nignorance when thinking about sentience",
      "offset": 2339.92,
      "duration": 7.103
    },
    {
      "lang": "en",
      "text": "Luisa Rodriguez: Let’s actually \ntalk about another paper of yours,  ",
      "offset": 2347.023,
      "duration": 2.577
    },
    {
      "lang": "en",
      "text": "“The rebugnant conclusion.” I \nlove that title, by the way.",
      "offset": 2349.6,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "Jeff Sebo: Thank you!",
      "offset": 2352.96,
      "duration": 0.463
    },
    {
      "lang": "en",
      "text": "Luisa Rodriguez: In the paper you basically \nask: Suppose that we determine that large  ",
      "offset": 2353.423,
      "duration": 4.817
    },
    {
      "lang": "en",
      "text": "animals like humans have more welfare \non average, but that small animals like  ",
      "offset": 2358.24,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "insects have more welfare in total. \nWhat follows for ethics and politics?",
      "offset": 2363.04,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "And the paper focuses on small animals like \nnematodes, but the same question is relevant to AI  ",
      "offset": 2367.44,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "systems that might end up being super numerous — \nperhaps because they’re used all over the economy  ",
      "offset": 2372.16,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "— but that might also have some non-negligible \nchance of experiencing pain and pleasure.",
      "offset": 2376.48,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "So let’s start with the case that \nyou actually focus on in the paper,  ",
      "offset": 2382.16,
      "duration": 2.32
    },
    {
      "lang": "en",
      "text": "which is small animals. How \nshould we think about this case?",
      "offset": 2384.48,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "Jeff Sebo: I think we can start really at \nthe end of the last exchange about ways  ",
      "offset": 2387.76,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "of striking a balance if we worry about the \nharms of false positives and false negatives.  ",
      "offset": 2393.44,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "One thing that you can note is that, even \nif I include insects and AI systems and  ",
      "offset": 2398.64,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "other types of beings in my moral circle, \neven if I give them moral consideration,  ",
      "offset": 2404.8,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "I might still be able to prioritise \nbeings like me for different reasons.",
      "offset": 2409.28,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "One of them is that I might have a higher \ncapacity for welfare than an insect or an  ",
      "offset": 2414.8,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "AI system. I have a more complex brain \nand a longer lifespan than an insect,  ",
      "offset": 2419.92,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "so I can experience more happiness and suffering \nat any given time, as well as over time.",
      "offset": 2424.48,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "And humans in general, I might think, are more \nlikely to be sentient and morally significant —  ",
      "offset": 2429.52,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "given the evidence available to me — than insects, \nAI systems, other beings like that. So I might  ",
      "offset": 2435.44,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "think to myself that if a house is burning down \nand I can save either a human or an ant, but not  ",
      "offset": 2440.96,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "both, then I can justifiably save the human — both \nbecause the human is more likely to matter, and  ",
      "offset": 2447.44,
      "duration": 7.12
    },
    {
      "lang": "en",
      "text": "because the human is likely to matter more. And \nthose are perfectly valid ways of breaking a tie.",
      "offset": 2454.56,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "That might give us some peace of mind \nwhen we countenance the possibility of  ",
      "offset": 2460,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "including these very different, very numerous \nbeings in the moral circle. But then you have  ",
      "offset": 2465.36,
      "duration": 6.64
    },
    {
      "lang": "en",
      "text": "to consider how large these populations \nactually are — and this is where we get  ",
      "offset": 2472,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "to the problem that this paper addresses, \nwhich is a problem in population ethics.",
      "offset": 2478.08,
      "duration": 7.503
    },
    {
      "lang": "en",
      "text": "Luisa Rodriguez: Right. And population ethics is  ",
      "offset": 2485.583,
      "duration": 2.577
    },
    {
      "lang": "en",
      "text": "the philosophical study of the ethical \nproblems that come up when our actions  ",
      "offset": 2488.16,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "affect how many people are born in \nthe future, and who exactly is born.",
      "offset": 2492.08,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "But yeah, my understanding is that we don’t \nactually know how many of these small animals  ",
      "offset": 2495.92,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "there are — ants and nematodes and maybe even \nmicrobes — but that it’s at least plausible  ",
      "offset": 2499.76,
      "duration": 6.64
    },
    {
      "lang": "en",
      "text": "that there are so many of them that even if they \nhave very less significant kinds of suffering and  ",
      "offset": 2506.4,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "pleasure relative to humans, and even if we only \nput some small chance on them even having those  ",
      "offset": 2512.56,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "at all, their interests still just swamp humans’. \nAnd this argument just does sound plausible to me,  ",
      "offset": 2518.4,
      "duration": 7.92
    },
    {
      "lang": "en",
      "text": "and it also fills me with dread and \nfear. What is your experience of it?",
      "offset": 2526.32,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "Jeff Sebo: Well, I certainly have the same \nexperience as I think most humans do. And  ",
      "offset": 2531.6,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "the reason I gave that paper the title \n“The rebugnant conclusion” is that this is  ",
      "offset": 2536.24,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "based on a famous book by the philosopher \nDerek Parfit called Reasons and Persons,  ",
      "offset": 2541.28,
      "duration": 6.56
    },
    {
      "lang": "en",
      "text": "part four of which addresses population \nethics. In that part of that book,  ",
      "offset": 2547.84,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "Derek Parfit discusses what he calls the \nrepugnant conclusion. I can say briefly  ",
      "offset": 2554,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "what that is and why that has, for the past \nseveral decades, filled many people with dread.",
      "offset": 2559.84,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "So the repugnant conclusion results from the \nfollowing observations. If you could bring  ",
      "offset": 2564.96,
      "duration": 7.6
    },
    {
      "lang": "en",
      "text": "about one future where the world contained \n100 people and everyone experienced 100,000  ",
      "offset": 2572.56,
      "duration": 6.8
    },
    {
      "lang": "en",
      "text": "units of happiness, or you could bring about \nanother world with twice the number of people,  ",
      "offset": 2579.36,
      "duration": 6.4
    },
    {
      "lang": "en",
      "text": "200 people, but everyone experiences one fewer \nunit of happiness — 99,999 — which world is  ",
      "offset": 2585.76,
      "duration": 9.36
    },
    {
      "lang": "en",
      "text": "better? Well, many of us have the intuition that \nthe second world is better; I should bring about  ",
      "offset": 2595.12,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "that second population. Everybody might \nexperience one unit of happiness less per person,  ",
      "offset": 2600.4,
      "duration": 7.2
    },
    {
      "lang": "en",
      "text": "but since there are twice as many people, there \nis nearly twice as much happiness overall, and  ",
      "offset": 2607.6,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "everyone is still really happy. And so, all things \nconsidered, I should bring about that population.",
      "offset": 2612,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "But then you can imagine another \npopulation, once again twice as  ",
      "offset": 2617.92,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "big, and once again with a bit less happiness \nper person. Then another one twice as big,  ",
      "offset": 2621.76,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "a bit less happiness per person. And so on and \nso on and so on — until you reach a point where  ",
      "offset": 2626.32,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "you are imagining a world or a solar system or a \ngalaxy that contains a vast number of individuals,  ",
      "offset": 2631.92,
      "duration": 9.28
    },
    {
      "lang": "en",
      "text": "each of whom has a life only barely worth living \nat all. And your reasoning would commit you to the  ",
      "offset": 2641.2,
      "duration": 6.72
    },
    {
      "lang": "en",
      "text": "idea that that is the best possible world, the \none that you should most want to bring about.",
      "offset": 2647.92,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "Parfit thought the idea that we would favour \nthat world with a much larger population,  ",
      "offset": 2652.48,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "where everyone has a life \nbarely worth living at all,  ",
      "offset": 2657.76,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "over a world with a still significant population, \nwhere everyone has lives very much worth living,  ",
      "offset": 2660.96,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "he found that repugnant. And he spent much \nof the rest of his career trying and failing  ",
      "offset": 2667.2,
      "duration": 6.8
    },
    {
      "lang": "en",
      "text": "to find a better way to see the value of \npopulations that could avoid that result.",
      "offset": 2674,
      "duration": 5.023
    },
    {
      "lang": "en",
      "text": "Luisa Rodriguez: I guess in the case of \ninsects, there’s also this weird thing  ",
      "offset": 2679.023,
      "duration": 4.657
    },
    {
      "lang": "en",
      "text": "where, unlike humans eating potatoes and not \nparticularly enjoying their monotonous lives,  ",
      "offset": 2683.68,
      "duration": 8
    },
    {
      "lang": "en",
      "text": "we might think that being a spider \nand making a web sounds pretty boring,  ",
      "offset": 2691.68,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "but we actually just really do not know. \nIn many ways, they’re so different from us  ",
      "offset": 2696.4,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "that we should have much lower probability that \nthey’re not enjoying or enjoying that than we do  ",
      "offset": 2702.08,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "of humans in this repugnant conclusion \nscenario. How do you factor that in?",
      "offset": 2707.76,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "Jeff Sebo: I do share the intuition that a very \nlarge insect population is not better off in the  ",
      "offset": 2713.68,
      "duration": 8.32
    },
    {
      "lang": "en",
      "text": "aggregate than a much smaller human population or \nelephant population. But for some of the reasons  ",
      "offset": 2722,
      "duration": 8.4
    },
    {
      "lang": "en",
      "text": "that you just mentioned and other reasons, I \nam a little bit sceptical of that intuition.",
      "offset": 2730.4,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "We have a lot of bias here and we also have \na lot of ignorance here. We have speciesism;  ",
      "offset": 2736.24,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "we naturally prefer beings and relate \nto beings when they look like us — when  ",
      "offset": 2742,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "they have large eyes and large heads, \nand furry skin instead of scaly skin,  ",
      "offset": 2747.52,
      "duration": 7.28
    },
    {
      "lang": "en",
      "text": "and four limbs instead of six or eight \nlimbs, and are roughly the same size as  ",
      "offset": 2754.8,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "us instead of much smaller, and reproduce by \nhaving one or two or three or four children  ",
      "offset": 2760.32,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "instead of thousands or more. So already \nwe have a lot of bias in those ways.",
      "offset": 2765.84,
      "duration": 6.4
    },
    {
      "lang": "en",
      "text": "We also have scope insensitivity — we tend \nnot to be sensitive to the difference that  ",
      "offset": 2772.24,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "very large numbers can make — and we have a lot \nof self-interest. We recognise that if we were to  ",
      "offset": 2777.2,
      "duration": 7.36
    },
    {
      "lang": "en",
      "text": "accept the moral significance of small animals \nlike insects, and if we were to accept that  ",
      "offset": 2784.56,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "larger populations can be better off than smaller \npopulations overall, then we might face a future  ",
      "offset": 2791.04,
      "duration": 7.52
    },
    {
      "lang": "en",
      "text": "where these nonhuman populations carry a lot of \nweight, and we carry less weight in comparison.",
      "offset": 2798.56,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "And I think some of us find that idea \nso unthinkable that we search for ways  ",
      "offset": 2804.16,
      "duration": 7.12
    },
    {
      "lang": "en",
      "text": "to avoid thinking it, and we search \nfor theoretical frameworks that would  ",
      "offset": 2811.28,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "not have that implication. And it \nmight be that we should take those  ",
      "offset": 2815.92,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "theoretical frameworks seriously and \nconsider avoiding that implication,  ",
      "offset": 2820.56,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "but I least want to be sceptical of a kind \nof knee-jerk impulse in that direction.",
      "offset": 2824,
      "duration": 6.383
    },
    {
      "lang": "en",
      "text": "Luisa Rodriguez: Yeah, I am finding that \nvery persuasive. Even as you’re saying it,  ",
      "offset": 2830.383,
      "duration": 5.777
    },
    {
      "lang": "en",
      "text": "I’m trying to think my way out of describing what \nI’m experiencing as just a bunch of biases — and  ",
      "offset": 2836.16,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "that in itself is the biases in action. \nIt’s me being like, no, I really, really,  ",
      "offset": 2842.16,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "really want to confirm that people like me, and \nme, get to have… I don’t know. It’s not that we  ",
      "offset": 2847.28,
      "duration": 8.8
    },
    {
      "lang": "en",
      "text": "don’t have priority — we obviously have some \nreason to consider ourselves a priority — but  ",
      "offset": 2856.08,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "I want it to be like, end of discussion. I want \ndecisive reasons to give us the top spot. And  ",
      "offset": 2862.08,
      "duration": 6.96
    },
    {
      "lang": "en",
      "text": "that instinct is so strong that that in itself is \nmaking me a bit queasy about my own motivations.",
      "offset": 2869.04,
      "duration": 7.989
    },
    {
      "lang": "en",
      "text": "Jeff Sebo: Yeah, I agree with all of that. \nI do think that we have some reason to  ",
      "offset": 2877.029,
      "duration": 3.851
    },
    {
      "lang": "en",
      "text": "prioritise ourselves, and that includes our \nwelfare capacities and our knowledge about  ",
      "offset": 2880.88,
      "duration": 6.88
    },
    {
      "lang": "en",
      "text": "ourselves. It also includes more relational \nand pragmatic considerations. So we will,  ",
      "offset": 2887.76,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "at least in the near term, I \nthink have a fairly decisive  ",
      "offset": 2893.52,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "reason to prioritise ourselves \nto some extent in some contexts.",
      "offset": 2897.2,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "But yeah, I agree. I think that there is not \na knock-down decisive reason why humanity  ",
      "offset": 2901.2,
      "duration": 7.52
    },
    {
      "lang": "en",
      "text": "should always necessarily take priority \nover all other nonhuman populations — and  ",
      "offset": 2908.72,
      "duration": 6.64
    },
    {
      "lang": "en",
      "text": "that includes very large populations \nof very small nonhumans, like insects,  ",
      "offset": 2915.36,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "or very small populations of very large \nnonhumans. We could imagine some kind of  ",
      "offset": 2920.32,
      "duration": 7.36
    },
    {
      "lang": "en",
      "text": "super being that has a much more complex \nbrain and much longer lifespan than us. So  ",
      "offset": 2927.68,
      "duration": 6.88
    },
    {
      "lang": "en",
      "text": "we could find our moral significance and moral \npriority being questioned from both directions.",
      "offset": 2934.56,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "And I think that it will be important to ask \nthese questions with a lot of thought and care  ",
      "offset": 2940.16,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "and to take our time in asking them. But I do \nstart from the place of finding it implausible  ",
      "offset": 2945.6,
      "duration": 8.4
    },
    {
      "lang": "en",
      "text": "that it would miraculously be the case that this \nkind of population happens to be the best one:  ",
      "offset": 2954,
      "duration": 7.28
    },
    {
      "lang": "en",
      "text": "that a moderately large population of moderately \nlarge beings like humans happens to be the magic  ",
      "offset": 2961.28,
      "duration": 7.36
    },
    {
      "lang": "en",
      "text": "recipe, and we matter more than all populations in \neither direction. That strikes me as implausible.",
      "offset": 2968.64,
      "duration": 9.12
    },
    {
      "lang": "en",
      "text": "Bob Fischer on how to think about \nthe moral weight of a chicken",
      "offset": 2977.76,
      "duration": 5.663
    },
    {
      "lang": "en",
      "text": "Luisa Rodriguez: Just to make sure I understand, \nthe thing is saying that the capacity of welfare  ",
      "offset": 2983.423,
      "duration": 7.057
    },
    {
      "lang": "en",
      "text": "or suffering of a chicken in a given instant \nis about a third of the capacity for the  ",
      "offset": 2990.48,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "kind of pain and pleasure a human could \nexperience in a given instant. Is that it?",
      "offset": 2996.96,
      "duration": 5.347
    },
    {
      "lang": "en",
      "text": "Bob Fischer: That’s the way to think about it. \nAnd that might sound very counterintuitive,  ",
      "offset": 3002.307,
      "duration": 6.413
    },
    {
      "lang": "en",
      "text": "and I understand that. I think \nthere are a couple of things we  ",
      "offset": 3008.72,
      "duration": 2.48
    },
    {
      "lang": "en",
      "text": "can say to help get us in the right frame \nof mind for thinking about these results.",
      "offset": 3011.2,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "One is to think about it first like a \nbiologist. If you think that humans’ pain  ",
      "offset": 3016.88,
      "duration": 9.04
    },
    {
      "lang": "en",
      "text": "is orders of magnitude worse than the pain of \na chicken, you’ve got to point to some feature  ",
      "offset": 3025.92,
      "duration": 7.6
    },
    {
      "lang": "en",
      "text": "of human brains that’s going to explain why that \nwould be the case. And I think for a lot of folks,  ",
      "offset": 3033.52,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "they have a kind of simple picture — where \nthey say more neurons equals more compute  ",
      "offset": 3039.04,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "equals orders of magnitude difference \nin performance, or something like that.",
      "offset": 3044.56,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "And biologists are not going to think \nthat way. They’re going to say, look,  ",
      "offset": 3049.84,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "neurons produce certain functions, and the number \nof neurons isn’t necessarily that important to  ",
      "offset": 3055.2,
      "duration": 6.64
    },
    {
      "lang": "en",
      "text": "the function: you might achieve the exact same \nfunction using many more or many fewer neurons.  ",
      "offset": 3061.84,
      "duration": 7.52
    },
    {
      "lang": "en",
      "text": "So that’s just not the really interesting, \nrelevant thing. So that’s the first step:  ",
      "offset": 3069.36,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "just to try to think more like a biologist \nwho’s focused on functional capacities.",
      "offset": 3074.08,
      "duration": 7.6
    },
    {
      "lang": "en",
      "text": "The second thing to say is just that you’ve got \nto remember what hedonism says. What’s going on  ",
      "offset": 3082.4,
      "duration": 7.28
    },
    {
      "lang": "en",
      "text": "here is we’re assuming that welfare \nis about just this one narrow thing:  ",
      "offset": 3089.68,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "the intensities of pleasures and pains. You \nmight not think that’s true; you might think  ",
      "offset": 3095.92,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "welfare is about whether I know important facts \nabout the world or whatever else, right? But  ",
      "offset": 3102,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "that’s not what I’m assessing; I’m just looking \nat this question of how intense is the pain.",
      "offset": 3107.44,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "And you might also point out, quite \nrightly, “But look, my cognitive life  ",
      "offset": 3112.88,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "is richer. I have a more diverse range \nof negatively valenced states.” And I’m  ",
      "offset": 3117.52,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "going to say that I don’t care about \nthe range; I care about the intensity,  ",
      "offset": 3123.6,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "right? That’s what hedonism says: that \nwhat matters is how intense the pains are.",
      "offset": 3127.04,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "So yeah, “I’m very disappointed because…” — \nchoose unhappy event of your preference — “…my  ",
      "offset": 3131.52,
      "duration": 7.36
    },
    {
      "lang": "en",
      "text": "favourite team lost,” whatever the case may \nbe. And from the perspective of hedonism,  ",
      "offset": 3138.88,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "what matters about that is just how sad did \nit make me? Not the content of the experience,  ",
      "offset": 3144.32,
      "duration": 7.28
    },
    {
      "lang": "en",
      "text": "but just the amount of negatively \nvalenced state that I’m experiencing,  ",
      "offset": 3151.6,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "or rather the intensity of the negatively \nvalenced state that I’m experiencing. So  ",
      "offset": 3156.96,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "I think people often implicitly confuse variety \nin the range of valenced states with intensity.",
      "offset": 3160.72,
      "duration": 8.703
    },
    {
      "lang": "en",
      "text": "Luisa Rodriguez: Yeah, that’s definitely something \nI do. For sure there is a part of me that thinks  ",
      "offset": 3169.423,
      "duration": 7.057
    },
    {
      "lang": "en",
      "text": "that the thing that matters a lot here is that \nI can fall in love in a particularly meaningful  ",
      "offset": 3176.48,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "and big way; I can have friendships lasting 50 \nyears that involve really deep and meaningful  ",
      "offset": 3182.24,
      "duration": 10.08
    },
    {
      "lang": "en",
      "text": "conversations. And that even if a chicken has \nmeaningful relationships with other chickens,  ",
      "offset": 3192.32,
      "duration": 7.6
    },
    {
      "lang": "en",
      "text": "they’re not as complex and varied as the \nrelationships I have with people in my life.",
      "offset": 3199.92,
      "duration": 6.72
    },
    {
      "lang": "en",
      "text": "On the other hand, a big part of me puts a bunch \nof weight, when I really think about it, on just  ",
      "offset": 3206.64,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "like, no, what matters is the intensity. If a \nchicken feels more sad about her wing being broken  ",
      "offset": 3213.12,
      "duration": 9.52
    },
    {
      "lang": "en",
      "text": "than I feel about losing a friend, then so be it. \nWe should make sure that their wings aren’t broken  ",
      "offset": 3222.64,
      "duration": 8.32
    },
    {
      "lang": "en",
      "text": "before we should make sure that whatever threat \nthat could mean I lose my friend [is prevented].",
      "offset": 3230.96,
      "duration": 6.56
    },
    {
      "lang": "en",
      "text": "And I guess lots of listeners will have their \nown kind of internal turmoil about this,  ",
      "offset": 3237.52,
      "duration": 7.6
    },
    {
      "lang": "en",
      "text": "about what welfare even is. But for now, I guess \nif we’re just taking this assumption, which is  ",
      "offset": 3245.12,
      "duration": 6.8
    },
    {
      "lang": "en",
      "text": "that what matters is the intensity, your finding \nis that something like averting the suffering  ",
      "offset": 3251.92,
      "duration": 9.2
    },
    {
      "lang": "en",
      "text": "of three chickens for an hour is similarly \nimportant to averting the suffering of one  ",
      "offset": 3261.12,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "person for an hour. And that feels uncomfortable \nto me. Can you talk me through that discomfort?",
      "offset": 3266.96,
      "duration": 7.107
    },
    {
      "lang": "en",
      "text": "Bob Fischer: Sure. So the first thing to say \nis: you’re not alone. I don’t feel totally  ",
      "offset": 3274.067,
      "duration": 4.493
    },
    {
      "lang": "en",
      "text": "comfortable either. And we have to ask ourselves \nwhat our most serious moral commitments are when  ",
      "offset": 3278.56,
      "duration": 9.2
    },
    {
      "lang": "en",
      "text": "we’re approaching this question. So you’re \nnot going to avoid really uncomfortable,  ",
      "offset": 3287.76,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "challenging questions when we try to think \nabout moral weights — just not going to go away.",
      "offset": 3292.08,
      "duration": 6.88
    },
    {
      "lang": "en",
      "text": "But here are a few things to say. One is: \nis there any number that you wouldn’t be  ",
      "offset": 3298.96,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "uncomfortable with? Because notice that if you’re \ncommitted to this idea of doing conversions,  ",
      "offset": 3303.76,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "eventually it’s going to just work out \nthat you’ve got to say there is some  ",
      "offset": 3309.36,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "number of hours of chicken suffering that \nis more important than helping a human.",
      "offset": 3313.28,
      "duration": 6.4
    },
    {
      "lang": "en",
      "text": "And I think actually for a lot \nof people, they don’t really  ",
      "offset": 3319.68,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "think that there is any conversion at \nall, right? If I had said it was 300,  ",
      "offset": 3323.68,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "would you really have felt that much better? \nYou might have felt a little bit better;  ",
      "offset": 3328.64,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "I’m not saying you wouldn’t felt it at all. \nSure, it’s a difference, but you might still say,  ",
      "offset": 3332.8,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "when you really think about it: “Three hundred \nhours? Would I put somebody through that for…  ",
      "offset": 3338.16,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "chickens?” And then you might just have the same \nlevel of discomfort, or something close to it.",
      "offset": 3343.28,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "So I think to some degree we have to remember \nthat the tradeoffs that we’re talking about come  ",
      "offset": 3348.8,
      "duration": 7.28
    },
    {
      "lang": "en",
      "text": "from background theoretical commitments that \nhave nothing to do with our specific welfare  ",
      "offset": 3356.08,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "range estimates: it comes from the fact \nthat we’re trying to do the most good. We  ",
      "offset": 3360.48,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "think that means making comparisons across \nspecies, and we’re committed to this kind  ",
      "offset": 3364.8,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "of maximising ethic that says, yeah, there is \nsome tradeoff rate, and you’ve got to find it.",
      "offset": 3368.56,
      "duration": 7.52
    },
    {
      "lang": "en",
      "text": "So that’s the first thing to say \nabout the discomfort. Before I  ",
      "offset": 3376.08,
      "duration": 2.56
    },
    {
      "lang": "en",
      "text": "say anything else, what do you think about that?",
      "offset": 3378.64,
      "duration": 3.423
    },
    {
      "lang": "en",
      "text": "Luisa Rodriguez: Yes, some of that definitely \nworked for me. I think the thing that lands  ",
      "offset": 3382.063,
      "duration": 2.897
    },
    {
      "lang": "en",
      "text": "most is if I think about chickens on a \nrailroad track, and there’s a trolley coming,  ",
      "offset": 3384.96,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "and there’s a human on the other side, it is \npretty impossible for me to imagine getting  ",
      "offset": 3388.96,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "to the point where I’m ever super comfortable \nbeing like, “I’m going to let it hit the human,  ",
      "offset": 3394.24,
      "duration": 6.64
    },
    {
      "lang": "en",
      "text": "who I could have conversations with, who has a \nfamily I might know, who I could give a hug to,  ",
      "offset": 3400.88,
      "duration": 9.04
    },
    {
      "lang": "en",
      "text": "and who has a job…” These are all the things \nthat kind of run through my head as I’m  ",
      "offset": 3409.92,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "deciding whether to pull a lever to decide who \ngets hit by this trolley. And so, fair enough  ",
      "offset": 3415.84,
      "duration": 7.04
    },
    {
      "lang": "en",
      "text": "that that is something I have to grapple with, \nregardless of exactly what these numbers are.",
      "offset": 3422.88,
      "duration": 6.467
    },
    {
      "lang": "en",
      "text": "Bob Fischer: And just to tag on to that, \nthink about it when you put someone you  ",
      "offset": 3429.347,
      "duration": 4.973
    },
    {
      "lang": "en",
      "text": "really care about on the track. So I think \nabout this with my children, and say, look,  ",
      "offset": 3434.32,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "it might well be the case that there’s almost no \nnumber of other humans I would choose to spare,  ",
      "offset": 3440.24,
      "duration": 9.52
    },
    {
      "lang": "en",
      "text": "given the choice between killing my own children \nand them. But that’s not because I think they  ",
      "offset": 3449.76,
      "duration": 6.88
    },
    {
      "lang": "en",
      "text": "actually matter less in some objective sense. \nLike when I’m trying to do the impartial good,  ",
      "offset": 3456.64,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "I would never say, “Oh yes, my \nchildren are utility monsters:  ",
      "offset": 3462.4,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "they have infinite worth and everybody \nelse has just some tiny portion of that.”",
      "offset": 3466,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "So when we recognise that our moral judgements \nare so detached from our judgements of value,  ",
      "offset": 3472.48,
      "duration": 9.2
    },
    {
      "lang": "en",
      "text": "that also can help us think about why these \nwelfare ranges might not be quite so crazy.",
      "offset": 3481.68,
      "duration": 7.103
    },
    {
      "lang": "en",
      "text": "Luisa Rodriguez: Yeah. Was there anything \nelse that helps you with the discomfort?",
      "offset": 3488.783,
      "duration": 3.537
    },
    {
      "lang": "en",
      "text": "Bob Fischer: I think the thing that helps me \nto some degree is to say, look, we’re doing  ",
      "offset": 3492.32,
      "duration": 7.84
    },
    {
      "lang": "en",
      "text": "our best here under moral uncertainty. I think \nyou should update in the direction of animals  ",
      "offset": 3500.16,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "based on this kind of work if you’ve never \ntaken animals particularly seriously before.",
      "offset": 3506.08,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "But ethics is hard. There are lots of big \nquestions to ask. I don’t know if hedonism  ",
      "offset": 3512.32,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "is true. I mean, there are good arguments \nfor it; there are good arguments for all  ",
      "offset": 3517.12,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "the assumptions that go into the project. \nBut yeah, I’m uncertain at every step,  ",
      "offset": 3520.48,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "and some kind of higher-level caution \nabout the entire venture is appropriate.",
      "offset": 3524.88,
      "duration": 8.72
    },
    {
      "lang": "en",
      "text": "And if you look at the way people \nactually allocate their dollars,  ",
      "offset": 3533.6,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "they often do spread their bets in precisely \nthis way. Even if they’re really in on animals,  ",
      "offset": 3537.6,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "they’re still giving some money to AMF. \nAnd that makes sense, because we want to  ",
      "offset": 3543.12,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "make sure that we end up doing some good in \nthe world, and that’s a way of doing that.",
      "offset": 3547.52,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "Luisa Rodriguez: I guess I’m curious \nif there’s anything you learned,  ",
      "offset": 3552.08,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "like a narrative or story that you have \nthat makes this feel more plausible to  ",
      "offset": 3556.32,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "you? Anything particular about chickens or just \nabout philosophy? You’ve already said some things,  ",
      "offset": 3562,
      "duration": 8.16
    },
    {
      "lang": "en",
      "text": "but what story do you have in your head that \nmakes you feel comfortable being like, “Yes,  ",
      "offset": 3570.16,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "I actually want to use these moral weights \nwhen deciding how to allocate resources”?",
      "offset": 3575.44,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "Bob Fischer: There are two things that I want to \nsay about that. One is I really worry about my  ",
      "offset": 3580.24,
      "duration": 7.28
    },
    {
      "lang": "en",
      "text": "own deep biases, and part of the reason that I’m \nwilling to be part of the EA project is because I  ",
      "offset": 3587.52,
      "duration": 8.24
    },
    {
      "lang": "en",
      "text": "think that, at its best, it’s an attempt to say, \n“Yeah, my gut’s wrong. I shouldn’t trust it. I  ",
      "offset": 3595.76,
      "duration": 8.32
    },
    {
      "lang": "en",
      "text": "should take the math more seriously. I should \ntry to put numbers on things and calculate.  ",
      "offset": 3604.08,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "And when I’m uncomfortable with the results, I’m \ntypically the problem, and not the process that I  ",
      "offset": 3609.04,
      "duration": 7.12
    },
    {
      "lang": "en",
      "text": "used.” So that’s one thing. It’s a check \non my own tendency to discount animals,  ",
      "offset": 3616.16,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "even as someone who spends most of their life \nworking on animals. So I think that’s one piece.",
      "offset": 3622.16,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "The other thing is just to spend time thinking \nabout the kinds of things animals can do and  ",
      "offset": 3628.16,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "what their lives are like. Just how hard \na chicken will work to get to a nest box  ",
      "offset": 3634.4,
      "duration": 8.64
    },
    {
      "lang": "en",
      "text": "before she lays an egg, the amount of labour \nshe’s willing to go through to do that,  ",
      "offset": 3643.04,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "to think about how important that is to her. \nAnd to realise that we can quantify that,  ",
      "offset": 3647.2,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "and see how much they care, or to see that \nthey get stressed out when fellow chickens  ",
      "offset": 3651.28,
      "duration": 9.28
    },
    {
      "lang": "en",
      "text": "are threatened and that they seem to \nhave some sympathy for conspecifics.",
      "offset": 3660.56,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "Those kinds of things make me say there is \nsomething in there that is recognisable to  ",
      "offset": 3666.64,
      "duration": 7.28
    },
    {
      "lang": "en",
      "text": "me as another individual, with desires and \npreferences and a vantage point on the world,  ",
      "offset": 3673.92,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "who wants things to go a certain way and \nis frustrated and upset when they don’t.  ",
      "offset": 3678.88,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "And recognising the individuality, the perspective \nof nonhuman animals, for me, really challenges my  ",
      "offset": 3684.72,
      "duration": 10
    },
    {
      "lang": "en",
      "text": "tendency to not take them as seriously as \nI think I ought to, all things considered.",
      "offset": 3694.72,
      "duration": 7.6
    },
    {
      "lang": "en",
      "text": "Cameron Meyer Shorb on the range \nof suffering in wild animals",
      "offset": 3702.32,
      "duration": 6.063
    },
    {
      "lang": "en",
      "text": "Luisa Rodriguez: Fundamentally, why do you \nworry about suffering among wild animals?  ",
      "offset": 3708.383,
      "duration": 5.137
    },
    {
      "lang": "en",
      "text": "What kinds of things make you think that \nthey might be suffering a lot in particular?",
      "offset": 3714.56,
      "duration": 5.826
    },
    {
      "lang": "en",
      "text": "Cameron Meyer Shorb: I’m wildly uncertain \nabout what the nature of wild animals’ lives  ",
      "offset": 3720.386,
      "duration": 4.414
    },
    {
      "lang": "en",
      "text": "are like. But I got into this field because I \nchanged my mind about the possibilities here.  ",
      "offset": 3724.8,
      "duration": 7.76
    },
    {
      "lang": "en",
      "text": "I used to just assume that animals living in \nthe wild were perfectly in balance, and living  ",
      "offset": 3732.56,
      "duration": 6.64
    },
    {
      "lang": "en",
      "text": "totally fulfilled lives, and weren’t bothered by \nany of the stresses of modernity like I am — and  ",
      "offset": 3739.2,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "that the best thing humans could possibly \ndo for them is just leave them alone there.",
      "offset": 3744.72,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "But the more I learned about it and thought about \nit, the more I realised that there’s actually lots  ",
      "offset": 3749.2,
      "duration": 6.64
    },
    {
      "lang": "en",
      "text": "of reasons to think that wild animals might not \nbe living great lives, at least many of them.  ",
      "offset": 3755.84,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "For example, they often have to struggle to \nget enough food. They often need to struggle  ",
      "offset": 3762.08,
      "duration": 7.6
    },
    {
      "lang": "en",
      "text": "to protect themselves from extreme weather. \nThere are some kinds of things where they have  ",
      "offset": 3769.68,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "no protection at all: if a flood or a wildfire \ncomes, that’s just the end of it for them. There’s  ",
      "offset": 3774.88,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "also all sorts of diseases or parasites. They \nhave no healthcare. They also have no state to  ",
      "offset": 3780.96,
      "duration": 8.08
    },
    {
      "lang": "en",
      "text": "protect them from violence, either from other \nspecies or even members of their own species.",
      "offset": 3789.04,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "So the kinds of conditions we’re talking about \nhere, when humans live in those conditions,  ",
      "offset": 3793.84,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "we would call that poverty. And we wouldn’t \ntolerate that. We would say that those are  ",
      "offset": 3798.32,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "problems we need to solve; those are people who \ndeserve better lives. And if we have medicine,  ",
      "offset": 3803.92,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "we should help give them access to \nmedicine. If there’s ways to give  ",
      "offset": 3809.44,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "them more stable access to food, that’s \nsomething that would improve their lives.",
      "offset": 3812.56,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "That’s the kind of approach that I now think we \nshould consider taking when we think about wild  ",
      "offset": 3817.12,
      "duration": 6.96
    },
    {
      "lang": "en",
      "text": "animals: taking seriously the idea that they might \nbe struggling even in their natural habitats,  ",
      "offset": 3824.08,
      "duration": 6.88
    },
    {
      "lang": "en",
      "text": "and they might suffer even from naturally \noccurring harms. And we should try to figure  ",
      "offset": 3830.96,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "out to what extent that’s true, and if \nit’s possible to do anything about that.",
      "offset": 3836.08,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "Luisa Rodriguez: Yeah, I want to come back to some \nof those, because I feel like even though I’m a  ",
      "offset": 3840,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "little bit familiar with this problem, I still \nhave a super limited imagination for the kinds of  ",
      "offset": 3844.64,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "things that wild animals — and obviously there’s \nsuch a wide range across different species — might  ",
      "offset": 3850.8,
      "duration": 7.68
    },
    {
      "lang": "en",
      "text": "be going through, aside from the really \nobvious ones, like being eaten or something.",
      "offset": 3858.48,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "So putting a pin in that, what is the scale of  ",
      "offset": 3863.92,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "the problem? How many individuals \nalive right now are wild animals?",
      "offset": 3867.36,
      "duration": 7.579
    },
    {
      "lang": "en",
      "text": "Cameron Meyer Shorb: The scale \nis huge. It’s bigger than I can  ",
      "offset": 3874.939,
      "duration": 2.661
    },
    {
      "lang": "en",
      "text": "count. I’ve been humbled by learning about this,  ",
      "offset": 3877.6,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "but I do think the scale is such an \nimportant part of understanding the problem.",
      "offset": 3881.44,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "Just in the broadest possible strokes, \nbased on the rough numbers we know,  ",
      "offset": 3885.44,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "it looks like something like 99% of all sentient \nminds alive on Earth today are wild animals. So  ",
      "offset": 3890.24,
      "duration": 10.8
    },
    {
      "lang": "en",
      "text": "if you are a human or a farmed animal, that \nis an incredibly rare exception to the rule,  ",
      "offset": 3901.04,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "which is: things that can feel live in the wild. \nYou’re more of a rounding error than anything.",
      "offset": 3907.28,
      "duration": 7.12
    },
    {
      "lang": "en",
      "text": "Which is not to say that human and farmed \nanimal experiences aren’t important;  ",
      "offset": 3914.4,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "it’s just to say there is a lot more going \non. And a truly impartial view of ethics  ",
      "offset": 3918.4,
      "duration": 7.28
    },
    {
      "lang": "en",
      "text": "would have us believe that ethics is mostly \nabout wild animals… and also there are these  ",
      "offset": 3925.68,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "interesting subfields that are related \nto some primates and farmed animals.",
      "offset": 3930.56,
      "duration": 4.703
    },
    {
      "lang": "en",
      "text": "Luisa Rodriguez: OK, I’m interested in \nbreaking down the numbers a bit more.  ",
      "offset": 3935.263,
      "duration": 4.177
    },
    {
      "lang": "en",
      "text": "I feel like it seems at least kind of important \nto know, are we mostly thinking about zebras,  ",
      "offset": 3939.44,
      "duration": 7.28
    },
    {
      "lang": "en",
      "text": "or are we mostly thinking about fish, \nants…? What to think about feels like  ",
      "offset": 3946.72,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "it could have helpful effects in helping \nme figure out, what are we talking about?",
      "offset": 3952.24,
      "duration": 6.72
    },
    {
      "lang": "en",
      "text": "Cameron Meyer Shorb: That’s a great question \nto ask, because I think that the images that  ",
      "offset": 3958.96,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "normally come to mind are not the most \nrepresentative of wild animals. Most  ",
      "offset": 3964.08,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "minds are wild, weird, and wet. They’re \njust not humans or human-like things.",
      "offset": 3969.68,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "To try to get a sense of scale, I’ll suggest \na visualisation. Let’s imagine for the sake  ",
      "offset": 3976.24,
      "duration": 8.08
    },
    {
      "lang": "en",
      "text": "of this exercise that we’re going to put a dot \ndown of equal size for any individual that’s  ",
      "offset": 3984.32,
      "duration": 7.36
    },
    {
      "lang": "en",
      "text": "alive. So one dot for a human, one dot for a \nsquirrel — and you can debate later how you  ",
      "offset": 3991.68,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "want to make tradeoffs across species — but just \nfor starters, to get a sense of the raw numbers.",
      "offset": 3997.76,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "Now, let’s make these dots small enough so that \nwe can fit all 8.2 billion humans onto the face  ",
      "offset": 4003.92,
      "duration": 6.4
    },
    {
      "lang": "en",
      "text": "of a quarter or a euro — so something \na little smaller than one square inch.",
      "offset": 4010.32,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "If we’re keeping that scale, then \nthe 88 billion wild mammals would  ",
      "offset": 4015.52,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "take up an area about the size of \na credit card or a post-it note.",
      "offset": 4021.52,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "And then when we move on to birds — and I should \nsay these estimates are all very rough, and the  ",
      "offset": 4026.16,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "bigger the populations, the wider the error bars \nare — but for birds, let’s say there are about  ",
      "offset": 4032.32,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "200 billion living in the wild. That would be \nsomething about the size of a standard envelope.",
      "offset": 4037.28,
      "duration": 7.04
    },
    {
      "lang": "en",
      "text": "And then for reptiles and amphibians, each of \nthose numbers somewhere around one trillion  ",
      "offset": 4044.32,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "individuals, two trillion put together. So a \ntrillion would be a standard sheet of paper.  ",
      "offset": 4050.48,
      "duration": 8.88
    },
    {
      "lang": "en",
      "text": "I think this is a good place to pause and \njust think about how far we’ve come: from a  ",
      "offset": 4059.36,
      "duration": 6.32
    },
    {
      "lang": "en",
      "text": "quarter to an envelope, which is way bigger than \na quarter, to a couple sheets of paper — compared  ",
      "offset": 4065.68,
      "duration": 6.88
    },
    {
      "lang": "en",
      "text": "to a quarter that contains all of human experience \nand 8.2 billion lives. There’s that, times many,  ",
      "offset": 4072.56,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "many more, if you’re trying to encompass humans \nand mammals and birds and reptiles and amphibians.",
      "offset": 4078.48,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "Then the numbers get even more mind-boggling \nwhen we move on to fish. There’s something  ",
      "offset": 4084.96,
      "duration": 6.96
    },
    {
      "lang": "en",
      "text": "like 10 trillion fish in the world. So \n10 trillion fish would be something like  ",
      "offset": 4091.92,
      "duration": 7.84
    },
    {
      "lang": "en",
      "text": "the size of a medium-sized desk — the Linnmon \nfrom Ikea, if you will — or a large bath mat,  ",
      "offset": 4099.76,
      "duration": 10.08
    },
    {
      "lang": "en",
      "text": "or a couple of pillowcases maybe. That’s \nwhat the whole fish population would  ",
      "offset": 4109.84,
      "duration": 6.72
    },
    {
      "lang": "en",
      "text": "look like, relative to the human \npopulation fitting on a quarter.",
      "offset": 4116.56,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "And the numbers get really… I don’t \nknow what “to boggle” means literally,  ",
      "offset": 4120.72,
      "duration": 6.64
    },
    {
      "lang": "en",
      "text": "but I think it is something like what \nis happening to my mind. I think it  ",
      "offset": 4127.36,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "is mind-boggling to try to imagine the \nnumber of plausibly sentient invertebrates.",
      "offset": 4130.48,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "The only at all close-to-useful number I found \nhere is an estimate of the number of terrestrial  ",
      "offset": 4136.88,
      "duration": 8.16
    },
    {
      "lang": "en",
      "text": "arthropods — that would be animals with hard \nexoskeletons, like insects and arachnids and  ",
      "offset": 4145.04,
      "duration": 7.84
    },
    {
      "lang": "en",
      "text": "crustaceans. So for those that live on land, \nestimates are that their population is somewhere  ",
      "offset": 4152.88,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "around 100,000 trillion. If 8.2 billion humans \nfit on the face of a quarter, 100,000 trillion  ",
      "offset": 4159.04,
      "duration": 9.6
    },
    {
      "lang": "en",
      "text": "would need to be something the size of a city \nblock or a FIFA regulation-size soccer field.",
      "offset": 4168.64,
      "duration": 6.8
    },
    {
      "lang": "en",
      "text": "Luisa Rodriguez: That’s insane!",
      "offset": 4175.44,
      "duration": 1.019
    },
    {
      "lang": "en",
      "text": "Cameron Meyer Shorb: Imagine standing at any \npoint in a soccer field and looking at a quarter  ",
      "offset": 4176.459,
      "duration": 5.541
    },
    {
      "lang": "en",
      "text": "and then looking around at the rest of the field. \nIt really changes your perspective on what life  ",
      "offset": 4182,
      "duration": 7.76
    },
    {
      "lang": "en",
      "text": "on Earth is like, who’s really living here. \nAnd it’s hard to know whether many arthropods  ",
      "offset": 4189.76,
      "duration": 8.48
    },
    {
      "lang": "en",
      "text": "are sentient. I think there’s decent questions \nand considerations on either side. But one of  ",
      "offset": 4198.24,
      "duration": 6.32
    },
    {
      "lang": "en",
      "text": "the things that I think is important to consider \nis the expected value. So even if there’s just a  ",
      "offset": 4204.56,
      "duration": 6.64
    },
    {
      "lang": "en",
      "text": "10% chance that they are, 10% of a soccer \nfield is still way bigger than a quarter.",
      "offset": 4211.2,
      "duration": 6.703
    },
    {
      "lang": "en",
      "text": "Luisa Rodriguez: Yeah. This makes me really happy \nthat some of our most recent episodes… One is with  ",
      "offset": 4217.903,
      "duration": 5.617
    },
    {
      "lang": "en",
      "text": "Meghan Barrett, one of my favourite episodes of \nall time, on invertebrate sentience. And really,  ",
      "offset": 4223.52,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "it took me well above 10% probability or \ncredence that invertebrates are sentient.  ",
      "offset": 4229.68,
      "duration": 6.8
    },
    {
      "lang": "en",
      "text": "Another one is on fish with Sébastien Moro, just \ninfinite numbers of bewildering facts about fish.",
      "offset": 4237.12,
      "duration": 9.76
    },
    {
      "lang": "en",
      "text": "For me, invertebrates and fish both make \nup tremendous numbers of individuals,  ",
      "offset": 4247.84,
      "duration": 7.52
    },
    {
      "lang": "en",
      "text": "as you’ve just said, and just are clearly at least \nvery plausible sentience candidates. For the case  ",
      "offset": 4255.36,
      "duration": 8.16
    },
    {
      "lang": "en",
      "text": "of fish, it seems hard to even debate for me. So \ngiven that these are the animals making up most  ",
      "offset": 4263.52,
      "duration": 7.04
    },
    {
      "lang": "en",
      "text": "of the wild nature we’re talking about, for anyone \nwho’s interested, I can recommend those episodes.",
      "offset": 4270.56,
      "duration": 7.92
    },
    {
      "lang": "en",
      "text": "Sébastien Moro on whether \nfish are conscious or sentient",
      "offset": 4278.48,
      "duration": 6.063
    },
    {
      "lang": "en",
      "text": "Luisa Rodriguez: This is fascinating and \nmind-blowing, purely from the perspective  ",
      "offset": 4284.543,
      "duration": 3.297
    },
    {
      "lang": "en",
      "text": "of “fish are incredibly cool.” But do these \nfeel like they tell you anything about whether  ",
      "offset": 4287.84,
      "duration": 8.32
    },
    {
      "lang": "en",
      "text": "fish are experiencing these things in \nsome conscious way or affective way?",
      "offset": 4296.16,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "Sébastien Moro: As I said earlier, we \nhave to split consciousness and sentience,  ",
      "offset": 4301.36,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "which are not the same thing. It’s very hard \nactually talking about real consciousness,  ",
      "offset": 4306.48,
      "duration": 7.52
    },
    {
      "lang": "en",
      "text": "like high-order consciousness, like humans: \nwe don’t know how to assess it correctly,  ",
      "offset": 4314,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "even in humans. We don’t \nhave proper assessing tools.",
      "offset": 4318.4,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "So today we’re trying to build new ways \nto assess consciousness and sentience  ",
      "offset": 4324.56,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "and split them properly. I understood \nthat you’ve interviewed Jonathan Birch  ",
      "offset": 4330.64,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "about that. He’s a pioneer in it. \nHe’s a very important person on it.",
      "offset": 4336.08,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "There is a very good book, especially \non fish, a study which is named,  ",
      "offset": 4341.12,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "“What is it like to be a bass? Red herrings, fish \npain, and the study of animal sentience.” It’s a  ",
      "offset": 4347.04,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "publication from 2022. It’s really interesting \nbecause it’s coming back on all of this, and the  ",
      "offset": 4353.2,
      "duration": 6.32
    },
    {
      "lang": "en",
      "text": "famous study that was talking about modification \nof the mirror test we were talking about. Many of  ",
      "offset": 4359.52,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "the studies that were done that we’ve talked \nabout, they aren’t made to assess sentience.",
      "offset": 4365.28,
      "duration": 7.503
    },
    {
      "lang": "en",
      "text": "Luisa Rodriguez: From your perspective, \nyou know so much of what there is to know  ",
      "offset": 4372.783,
      "duration": 4.657
    },
    {
      "lang": "en",
      "text": "about fish that we have studied so far, so \nyou’ve got all of this wealth of knowledge:  ",
      "offset": 4377.44,
      "duration": 7.84
    },
    {
      "lang": "en",
      "text": "what is it that feels most compelling to you,  ",
      "offset": 4385.92,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "that makes you feel like you’ve got really high \nconfidence that fish are experiencing things?",
      "offset": 4388.96,
      "duration": 7.52
    },
    {
      "lang": "en",
      "text": "Sébastien Moro: Most of the papers \nwe have are going in this way,  ",
      "offset": 4396.48,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "and very few — very, very few \n— are going the other way.",
      "offset": 4401.52,
      "duration": 6.8
    },
    {
      "lang": "en",
      "text": "So what makes me so certain? I’m really talking \nabout my personal opinion here. I tend to think  ",
      "offset": 4408.32,
      "duration": 10.64
    },
    {
      "lang": "en",
      "text": "that emotions… What are emotions? What \nare emotions used for? They are putting  ",
      "offset": 4418.96,
      "duration": 7.28
    },
    {
      "lang": "en",
      "text": "a gloss on what is around us: “This stuff \nhas a positive gloss; I need more of this.  ",
      "offset": 4426.24,
      "duration": 6.32
    },
    {
      "lang": "en",
      "text": "This one has a negative gloss.” And we know \nthat emotions are very, very closely linked  ",
      "offset": 4432.56,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "to learning. So emotions are something that \nattract or repel. And it seems pretty obvious  ",
      "offset": 4438.24,
      "duration": 10.08
    },
    {
      "lang": "en",
      "text": "that it must have appeared very, very early \nin evolution, because this is how we work.",
      "offset": 4448.32,
      "duration": 7.36
    },
    {
      "lang": "en",
      "text": "This is maybe one of the biggest differences with \nalgorithms. Algorithms are following closed loops.  ",
      "offset": 4455.68,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "And this comes up when animals are more driven \nby emotions, by value of things, which is made  ",
      "offset": 4461.28,
      "duration": 10.16
    },
    {
      "lang": "en",
      "text": "by a kind of limbic system that says, “This \nis good, this is bad. You want more of this,  ",
      "offset": 4471.44,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "you want less of that.” And I don’t understand \nwhy other animals couldn’t have had that.",
      "offset": 4476.72,
      "duration": 6.32
    },
    {
      "lang": "en",
      "text": "And another thing is, I’m reading a lot about \nbees, so I know very well the corpus of knowledge  ",
      "offset": 4483.04,
      "duration": 8.32
    },
    {
      "lang": "en",
      "text": "on bees at the moment. And we’re starting to have \nthe same results in bees. So I’m not 100% certain  ",
      "offset": 4491.36,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "now that bees could be sentient, but the biggest \nleader of bees research today, Lars Chittka,  ",
      "offset": 4497.6,
      "duration": 8.64
    },
    {
      "lang": "en",
      "text": "has said on Twitter that bees are sentient for \nhim. And the results we have are going in this  ",
      "offset": 4506.24,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "way. So they have a one-million-neuron brain, and \nthe brains of fish are much, much bigger. And when  ",
      "offset": 4512.32,
      "duration": 7.04
    },
    {
      "lang": "en",
      "text": "we split from insects, brains were not existing \neither. So it’s just a convergent evolution.",
      "offset": 4519.36,
      "duration": 6.4
    },
    {
      "lang": "en",
      "text": "Fish have complicated lives; they have social \nlives, very social lives. I already introduced  ",
      "offset": 4527.2,
      "duration": 7.04
    },
    {
      "lang": "en",
      "text": "this with cleaner wrasses: their lives are very \ncomplicated. They have challenges that they  ",
      "offset": 4534.24,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "have to overcome that are as complex as what we \nfind in mammals and birds, maybe more sometimes.",
      "offset": 4539.84,
      "duration": 8.4
    },
    {
      "lang": "en",
      "text": "So them having no sentience, when we \nrecognise sentience in birds and mammals?  ",
      "offset": 4548.24,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "It’s either you refuse it for everyone, or \nyou accept it for everyone at the moment.  ",
      "offset": 4554.32,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "Not for everyone, because animals with \na brain or central nervous system,  ",
      "offset": 4560.8,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "today we think at least there \nshould be a kind of global network;  ",
      "offset": 4564.96,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "everything should be put in common to make a \nunified vision of you and this kind of thing.",
      "offset": 4568.8,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "But consciousness probably has many \ndegrees; sentience has pretty much  ",
      "offset": 4574.16,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "many degrees — but not degrees on a \nladder, degrees more on a circle. But  ",
      "offset": 4579.52,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "that would sound just weird actually, \nthat they wouldn’t be sentient.",
      "offset": 4587.28,
      "duration": 5.183
    },
    {
      "lang": "en",
      "text": "Luisa Rodriguez: Yeah, it would \njust be really surprising to you.",
      "offset": 4592.463,
      "duration": 5.057
    },
    {
      "lang": "en",
      "text": "David Chalmers on when to start \nworrying about artificial consciousness",
      "offset": 4597.52,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "Arden Koehler: So you said elsewhere that if \nmore fully autonomous artificial intelligence  ",
      "offset": 4602.56,
      "duration": 7.12
    },
    {
      "lang": "en",
      "text": "comes around, then we might have to start worrying \nabout it being conscious, and therefore presumably  ",
      "offset": 4609.68,
      "duration": 6.4
    },
    {
      "lang": "en",
      "text": "worthy of moral concern. But you don’t think \nwe have to worry about it too much before then.",
      "offset": 4616.08,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "I’m just wondering if you can say a bit \nabout why, and whether you think it’s  ",
      "offset": 4621.04,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "possible that programs or computers could \nbecome gradually more and more conscious,  ",
      "offset": 4625.12,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "and whether that process might start \nbefore they are fully autonomous?",
      "offset": 4629.6,
      "duration": 3.984
    },
    {
      "lang": "en",
      "text": "David Chalmers: Yeah, that’s an \ninteresting point. I guess one  ",
      "offset": 4633.584,
      "duration": 2.736
    },
    {
      "lang": "en",
      "text": "would expect to get to conscious AI well before we \nget human-level artificial general intelligence,  ",
      "offset": 4636.32,
      "duration": 7.12
    },
    {
      "lang": "en",
      "text": "simply because we’ve got a pretty good reason \nto believe there are many conscious creatures  ",
      "offset": 4643.44,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "whose degree of intelligence falls well short \nof human-level artificial general intelligence.",
      "offset": 4647.36,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "So if fish are conscious, for example, you \nmight think if an AI gets to sophistication and  ",
      "offset": 4652.32,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "information processing and whatever the relevant \nfactors are to the degree present in fish,  ",
      "offset": 4657.84,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "then that should be enough. And it does open up \nthe question as to whether any existing AI systems  ",
      "offset": 4662.64,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "may actually be conscious. I think the consensus \nview is that they’re not. But the more liberal you  ",
      "offset": 4668.16,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "are about descriptions of consciousness, the more \nwe should take seriously the chance that they are.",
      "offset": 4674.32,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "There is this website out there called \nPeople for the Ethical Treatment of  ",
      "offset": 4679.68,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "Reinforcement Learners that I quite \nlike. The idea is that every time you  ",
      "offset": 4682.88,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "give a reinforcement learning network its \nreward signal, then it may be experiencing  ",
      "offset": 4686.64,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "pleasure or correspondingly suffering, \ndepending on the valence of the signal.",
      "offset": 4692.16,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "As someone who’s committed to \ntaking panpsychism seriously,  ",
      "offset": 4697.28,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "I think I should at least take that possibility \nseriously. I don’t know where our current deep  ",
      "offset": 4700.4,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "learning networks fall on the scale of \norganic intelligence. Maybe they’re at  ",
      "offset": 4706.16,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "least as sophisticated as worms — like C. \nelegans, with 300 neurons. I take seriously  ",
      "offset": 4710.4,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "the possibility that those are conscious. So I \nguess I do take seriously the possibility that  ",
      "offset": 4716,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "AI consciousness could come along well before \nhuman-level AGI, and that it may exist already.",
      "offset": 4720.64,
      "duration": 5.56
    },
    {
      "lang": "en",
      "text": "Though then the question is, I suppose,  ",
      "offset": 4726.2,
      "duration": 2.28
    },
    {
      "lang": "en",
      "text": "how sophisticated the state of consciousness \nis. If it’s about as sophisticated as, say,  ",
      "offset": 4728.48,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "the consciousness of a worm, I think most of \nus are inclined to think that brings along,  ",
      "offset": 4733.36,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "say, some moral status with it, but it \ndoesn’t give it enormous weight in the  ",
      "offset": 4737.6,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "scheme of conscious creatures compared to the \nweight we give humans and mammals and so on.",
      "offset": 4741.92,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "So I guess then the question would \nbe whether current AIs get a truly  ",
      "offset": 4747.2,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "sophisticated moral status, but I \nguess I should be open to them at  ",
      "offset": 4751.68,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "least getting some relatively small moral \nstatus of the kind that, say, worms have.",
      "offset": 4754.64,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: Maybe this is getting outside your \narea of expertise, but with current ML systems,  ",
      "offset": 4759.76,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "how would we have any sense of whether the \naffective states are positive or negative? It  ",
      "offset": 4764.64,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "seems like once you have a reinforcement learner, \nI guess on average, does it get zero reinforcement  ",
      "offset": 4769.76,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "because it just has an equal balance of positive \nand negative reinforcements? And is there some  ",
      "offset": 4774.96,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "way that you could just scale all of them up \nto be more or less positive? Or does that even  ",
      "offset": 4778.48,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "mean anything? Like you just increase all the \nnumbers by 100. How would that help? It raises  ",
      "offset": 4782.24,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "this issue of the arbitrariness of the zero point \non this kind of scale of goodness of the states.",
      "offset": 4788.24,
      "duration": 5.424
    },
    {
      "lang": "en",
      "text": "David Chalmers: Yeah. This is getting to \nissues about value and morality that do  ",
      "offset": 4793.664,
      "duration": 4.656
    },
    {
      "lang": "en",
      "text": "go beyond my expertise to some extent. \nWe’ve got absolutely no way right now  ",
      "offset": 4798.32,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "to tell exactly what reinforcement learning \nsystems might be experiencing, if anything.",
      "offset": 4802.88,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "But if you were inclined to think they’re \nexperiencing something and that they’re  ",
      "offset": 4807.2,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "experiencing something with valence, I \nsuppose then they’d be having a mix of  ",
      "offset": 4810.16,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "positively valenced reinforcement and negatively \nvalenced reinforcement — therefore, a mix of  ",
      "offset": 4814.32,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "very simple precursor of, say, pleasure and of \nsuffering, proto-pleasure and proto-suffering.",
      "offset": 4820.4,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "Then a lot’s going to depend on \nyour ethical theory. If you’re  ",
      "offset": 4826.48,
      "duration": 2.08
    },
    {
      "lang": "en",
      "text": "feeling pleasure half the time \nbut suffering half the time,  ",
      "offset": 4828.56,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "is that net good? Is that net bad? I don’t \nknow. If you ask me, I think that’s net bad,  ",
      "offset": 4831.52,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "because all that suffering tends to outweigh the \npleasure, but maybe there’s weights on the scale.",
      "offset": 4835.44,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "At this point though, I should say \nthat it’s by no means obvious to me  ",
      "offset": 4841.04,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "that pleasure and suffering, that is, \nthat valenced states of consciousness,  ",
      "offset": 4843.84,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "are the ones that are relevant to moral status. \nI know people quite often have this issue. I’m  ",
      "offset": 4847.68,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "inclined to think that consciousness may ground \nmoral status in some cases quite independently of  ",
      "offset": 4852.56,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "its valence. Even beings with unvalenced states \nof consciousness could still have moral status.",
      "offset": 4857.52,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "Robert Long on how we might stumble into \ncausing AI systems enormous suffering",
      "offset": 4865.28,
      "duration": 7.107
    },
    {
      "lang": "en",
      "text": "Robert Long: So you can imagine that a robot has \nbeen created by a company or by some researchers.  ",
      "offset": 4872.387,
      "duration": 5.373
    },
    {
      "lang": "en",
      "text": "And as it happens, it registers damage to its body \nand processes it in the way that, as it turns out,  ",
      "offset": 4877.76,
      "duration": 7.2
    },
    {
      "lang": "en",
      "text": "is relevant to having an experience of unpleasant \npain. And maybe we don’t realise that, because we  ",
      "offset": 4884.96,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "don’t have good theories of what’s going on \nin the robot or what it takes to feel pain.",
      "offset": 4890.16,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "In that case, you can imagine that thing having \na bad time because we don’t realise it. You could  ",
      "offset": 4894.08,
      "duration": 7.36
    },
    {
      "lang": "en",
      "text": "also imagine this thing being rolled out and \nnow we’re economically dependent on systems  ",
      "offset": 4901.44,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "like this. And now we have an incentive not to \ncare and not to think too hard about whether  ",
      "offset": 4907.2,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "it might be having a bad time. So I mean, \nthat seems like something that could happen.",
      "offset": 4912.96,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "Luisa Rodriguez: Yeah, and that could \nhappen because there’s some reason why  ",
      "offset": 4917.52,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "it’s helpful to have the robot recognise that \nit’s sustained damage. It can be like, “Help,  ",
      "offset": 4921.92,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "I’m broken. I need someone to fix my part.” So \nthat’s something that you can imagine might get  ",
      "offset": 4927.6,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "programmed in. And then, it is just kind of \nwild to me that we don’t understand what the  ",
      "offset": 4932.8,
      "duration": 6.4
    },
    {
      "lang": "en",
      "text": "robot might be experiencing well enough to \nknow that that thing is pain. But in theory,  ",
      "offset": 4939.2,
      "duration": 6.32
    },
    {
      "lang": "en",
      "text": "that’s possible, just that \nit is that black-boxy to us.",
      "offset": 4945.52,
      "duration": 4.227
    },
    {
      "lang": "en",
      "text": "Robert Long: Yeah. It might be a little \nbit less likely with a robot. But now you  ",
      "offset": 4949.747,
      "duration": 4.013
    },
    {
      "lang": "en",
      "text": "can imagine more abstract or alien ways of \nfeeling bad. So I focus on pain because it’s  ",
      "offset": 4953.76,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "a very straightforward way of feeling bad. A \ndisembodied system like GPT obviously can’t  ",
      "offset": 4960,
      "duration": 7.2
    },
    {
      "lang": "en",
      "text": "feel ankle pain. Or almost certainly. That’d \nbe really weird. It doesn’t have an ankle.  ",
      "offset": 4967.2,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "Why would it have computations that \nrepresent its ankle is feeling bad?  ",
      "offset": 4972.24,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "But you can imagine maybe some strange form of \nvalenced experience that develops inside some  ",
      "offset": 4977.12,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "system like this that registers some kind of \ndispleasure or pleasure, something like that.",
      "offset": 4982.64,
      "duration": 4.383
    },
    {
      "lang": "en",
      "text": "Luisa Rodriguez: Right, right. Something like,  ",
      "offset": 4987.023,
      "duration": 2.257
    },
    {
      "lang": "en",
      "text": "you guessed the wrong set of words to come \nnext and that was bad. And the user isn’t  ",
      "offset": 4989.28,
      "duration": 7.2
    },
    {
      "lang": "en",
      "text": "happy with the string of words you came up \nwith. And then that feels something like pain.",
      "offset": 4996.48,
      "duration": 4.227
    },
    {
      "lang": "en",
      "text": "Robert Long: Exactly. And I will note \nthat I don’t think that getting negative  ",
      "offset": 5000.707,
      "duration": 5.133
    },
    {
      "lang": "en",
      "text": "feedback is going to be enough for that \nbad feeling, fortunately. But maybe some  ",
      "offset": 5005.84,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "combination of that and some way it’s ended up \nrepresenting it inside itself ends up like that.",
      "offset": 5010.96,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "And then we have something where it’s hard for \nus to map its internals to what we care about.  ",
      "offset": 5017.04,
      "duration": 8.64
    },
    {
      "lang": "en",
      "text": "We maybe have various incentives not to look \ntoo hard at that question. We have incentives  ",
      "offset": 5025.68,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "not to let it speak freely about if it thinks \nit’s conscious, because that would be a big  ",
      "offset": 5031.84,
      "duration": 6.64
    },
    {
      "lang": "en",
      "text": "headache. And because we’re also worried \nabout systems lying about being conscious  ",
      "offset": 5038.48,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "and giving misleading statements about whether \nthey’re conscious — which they definitely do.",
      "offset": 5043.12,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "So we’ve built this new kind of alien mind. \nWe don’t really have a good theory of pain,  ",
      "offset": 5048.08,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "even for ourselves. We don’t \nhave a good theory of what’s  ",
      "offset": 5053.2,
      "duration": 2.4
    },
    {
      "lang": "en",
      "text": "going on inside it. So that’s like a \nstumbling-into-this sort of scenario.",
      "offset": 5055.6,
      "duration": 5.663
    },
    {
      "lang": "en",
      "text": "Luisa Rodriguez: Yeah. But it sounds like we \ncould make things for economic reasons, like  ",
      "offset": 5061.263,
      "duration": 6.657
    },
    {
      "lang": "en",
      "text": "robots or chatbots, and we don’t realise those \nthings are suffering. And then we mass produce  ",
      "offset": 5067.92,
      "duration": 6.72
    },
    {
      "lang": "en",
      "text": "them because they’re valuable. … And those things \nare suffering and we didn’t know it and they’re  ",
      "offset": 5074.64,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "all over. And we don’t really want to change \nanything about those systems because we use them.",
      "offset": 5079.68,
      "duration": 5.827
    },
    {
      "lang": "en",
      "text": "Robert Long: Yeah. I mean, for \njust another dark scenario,  ",
      "offset": 5085.507,
      "duration": 2.813
    },
    {
      "lang": "en",
      "text": "you can imagine a system where we get pigs to be \nfarmed much more efficiently. And we’re just like,  ",
      "offset": 5088.32,
      "duration": 6.4
    },
    {
      "lang": "en",
      "text": "“Well, this has made meat cheaper. \nLet’s not think too much about that.”",
      "offset": 5094.72,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "I guess one thing I should note is I’ve been \nfocusing on this case where we’ve hit on it  ",
      "offset": 5100.16,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "accidentally. There are a lot of people who are \ninterested in building artificial consciousness.",
      "offset": 5105.12,
      "duration": 6.303
    },
    {
      "lang": "en",
      "text": "Luisa Rodriguez: On purpose, yeah.",
      "offset": 5111.423,
      "duration": 0.644
    },
    {
      "lang": "en",
      "text": "Robert Long: And understandably so. You \nknow, just from a purely intellectual or  ",
      "offset": 5112.067,
      "duration": 4.093
    },
    {
      "lang": "en",
      "text": "philosophical standpoint, it’s a fascinating \nproject and it can help us understand the  ",
      "offset": 5116.16,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "nature of consciousness. So for a very \nlong time, probably about as old as AI,  ",
      "offset": 5120.48,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "people were like, “Wow, I wonder if \nwe could make this thing conscious?”",
      "offset": 5125.76,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "There was a recent New York Times article about \nroboticists who want to build more self-awareness  ",
      "offset": 5129.6,
      "duration": 6.96
    },
    {
      "lang": "en",
      "text": "into robots, both for the intrinsic scientific \ninterest and also because it might make for better  ",
      "offset": 5136.56,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "robots. And some of them think, “Oh, well, \nwe’re not actually that close to doing that.  ",
      "offset": 5141.44,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "Maybe it’s too soon to worry about it.” Another \nperson quoted in that article is like, “Yeah,  ",
      "offset": 5146.48,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "it’s something to worry about, but we’ll deal with \nit.” And I am quoted in that piece as just kind  ",
      "offset": 5152.64,
      "duration": 7.04
    },
    {
      "lang": "en",
      "text": "of being like, “Ahhh, be careful, you know. Slow \ndown. We’re not really ready to deal with this.”",
      "offset": 5159.68,
      "duration": 7.413
    },
    {
      "lang": "en",
      "text": "Luisa Rodriguez: Not ready to “deal with that.”",
      "offset": 5167.093,
      "duration": 2.427
    },
    {
      "lang": "en",
      "text": "Robert Long: Yeah, exactly.",
      "offset": 5169.52,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "Jonathan Birch on how we might \naccidentally create artificial sentience",
      "offset": 5174.24,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "Luisa Rodriguez:  ",
      "offset": 5178.8,
      "duration": 1.76
    },
    {
      "lang": "en",
      "text": "You did raise a few interesting points about AI \nsentience I wanted to ask more about. One is that  ",
      "offset": 5180.56,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "you emphasise that AI sentience could arise in a \nnumber of ways. I think I intuitively imagine it  ",
      "offset": 5186.4,
      "duration": 7.12
    },
    {
      "lang": "en",
      "text": "arising either intentionally or unintentionally as \na result of work on LLMs. But one of these other  ",
      "offset": 5193.52,
      "duration": 6.8
    },
    {
      "lang": "en",
      "text": "ways is whole brain emulation. And one case \nI hadn’t heard that much about is OpenWorm.  ",
      "offset": 5200.32,
      "duration": 7.2
    },
    {
      "lang": "en",
      "text": "Can you talk a bit about the goals of \nOpenWorm and how that project has gone?",
      "offset": 5207.52,
      "duration": 3.96
    },
    {
      "lang": "en",
      "text": "Jonathan Birch: This was a project that caught \nmy eye around 2014, I think, because the goal  ",
      "offset": 5211.48,
      "duration": 7.56
    },
    {
      "lang": "en",
      "text": "was to emulate the entire nervous system of \nthe nematode worm C. elegans in software. And  ",
      "offset": 5219.04,
      "duration": 23.36
    },
    {
      "lang": "en",
      "text": "they had some striking initial results, where \nthey put their emulation in charge of a robot,  ",
      "offset": 5242.4,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "and the robot did some kind of worm-like \nthings in terms of navigating the environment,  ",
      "offset": 5248.48,
      "duration": 6.96
    },
    {
      "lang": "en",
      "text": "turning round when it hit an obstacle, that kind \nof thing. And it generated some initial hype.",
      "offset": 5255.44,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "Luisa Rodriguez: It feels naive now, but it was \neye-opening to me when you pointed out that we  ",
      "offset": 5261.6,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "actually just wouldn’t need whole brain emulation \nin humans or of human brains to start thinking  ",
      "offset": 5267.76,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "about the risks from AI sentience. We just need \nto go from OpenWorm to OpenZebrafish or OpenMouse,  ",
      "offset": 5273.52,
      "duration": 7.12
    },
    {
      "lang": "en",
      "text": "or maybe even OpenDrosophila — which sounds like \nnot an insane step from just where we are now.  ",
      "offset": 5280.64,
      "duration": 6.32
    },
    {
      "lang": "en",
      "text": "How likely is it, do you think, that researchers \nwould try to create something like OpenMouse?",
      "offset": 5286.96,
      "duration": 5.104
    },
    {
      "lang": "en",
      "text": "Jonathan Birch: Oh, it’s very likely. If \nthey knew how, of course they would. I think  ",
      "offset": 5292.064,
      "duration": 3.776
    },
    {
      "lang": "en",
      "text": "one of the main themes of that part of the book \nis that once we see the decoupling of sentience  ",
      "offset": 5296.88,
      "duration": 6.56
    },
    {
      "lang": "en",
      "text": "and intelligence — which is very important, to \nthink of these as distinct ideas — we realise  ",
      "offset": 5303.44,
      "duration": 6.56
    },
    {
      "lang": "en",
      "text": "that artificial sentience might not be the \nsort of thing that goes along with the most  ",
      "offset": 5310,
      "duration": 7.04
    },
    {
      "lang": "en",
      "text": "intelligent systems. It might actually be more \nlikely to be created by attempts to emulate  ",
      "offset": 5317.04,
      "duration": 7.28
    },
    {
      "lang": "en",
      "text": "the brain of an insect, for example — where \nthe intelligence would not be outperforming  ",
      "offset": 5324.32,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "ChatGPT on any benchmarks, but perhaps more of \nthe relevant brain dynamics might be recreated.",
      "offset": 5330.08,
      "duration": 7.2
    },
    {
      "lang": "en",
      "text": "Luisa Rodriguez: Yeah, it was a jarring \nobservation for me. I think part of it  ",
      "offset": 5337.28,
      "duration": 8.48
    },
    {
      "lang": "en",
      "text": "is that it hadn’t occurred to me that people \nwould be as motivated as they are to create  ",
      "offset": 5345.76,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "something like OpenMouse. Can you say more \nabout what the motivation is? Does it have  ",
      "offset": 5350.72,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "scientific value beyond being cool? Or is the \nfact that it’s just a cool thing to do enough?",
      "offset": 5356.88,
      "duration": 7.024
    },
    {
      "lang": "en",
      "text": "Jonathan Birch: I think it would have \nan immense scientific value. It would  ",
      "offset": 5363.904,
      "duration": 2.736
    },
    {
      "lang": "en",
      "text": "appear to be a long way in the future \nstill, as things stand. But of course,  ",
      "offset": 5366.64,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "we’re talking here about understanding \nthe brain. I think when you emulate the  ",
      "offset": 5371.04,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "functions of the C. elegans nervous system, \nyou can really say you understand what is  ",
      "offset": 5376.72,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "going on — and that just isn’t true for human \nbrains, currently. We have no idea. At quite a  ",
      "offset": 5382.32,
      "duration": 7.28
    },
    {
      "lang": "en",
      "text": "fundamental level, our understanding of \nC. elegans is in some ways far better.",
      "offset": 5389.6,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "And it’s another step again, if you \ndon’t just understand how lesioning  ",
      "offset": 5395.6,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "bits of the nervous system affects function,  ",
      "offset": 5400,
      "duration": 2.4
    },
    {
      "lang": "en",
      "text": "but you can also recreate the whole system in \ncomputer software, would be a tremendous step.",
      "offset": 5402.4,
      "duration": 7.12
    },
    {
      "lang": "en",
      "text": "And it holds the promise over the long \nterm of giving us a way to replace animal  ",
      "offset": 5409.52,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "research. Because once you’ve got \na functioning emulation of a brain,  ",
      "offset": 5413.84,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "you can step past that very crude method of \njust taking the living brain and injuring it,  ",
      "offset": 5419.28,
      "duration": 6.72
    },
    {
      "lang": "en",
      "text": "which is what a lot of research involves, or \nmodifying it through genome editing. You can  ",
      "offset": 5426,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "instead go straight to the system itself \nand do incredibly precise manipulations.",
      "offset": 5432.24,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "So I feel like, if anything, it hasn’t been \nhyped enough. I want more of this kind of thing,  ",
      "offset": 5438.48,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "to be honest, than has been the case so far.",
      "offset": 5444.72,
      "duration": 3.583
    },
    {
      "lang": "en",
      "text": "Luisa Rodriguez: Intuitively, it seems plausible \n— and maybe even likely — that if you were able  ",
      "offset": 5448.303,
      "duration": 6.017
    },
    {
      "lang": "en",
      "text": "to emulate a mind that we thought was \nsentient, that the emulation would also  ",
      "offset": 5454.32,
      "duration": 6.88
    },
    {
      "lang": "en",
      "text": "be sentient. But is there a reason to think \nthose come apart? Maybe we just don’t know.",
      "offset": 5461.2,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "Jonathan Birch: It’s another space where you get \nreasonable disagreement, because I think we have  ",
      "offset": 5465.6,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "to take seriously the view that philosophers call \n“computational functionalism”: a view on which,  ",
      "offset": 5470.96,
      "duration": 7.44
    },
    {
      "lang": "en",
      "text": "if you recreate the computations, you also \nrecreate the subjective experience. And that leads  ",
      "offset": 5478.4,
      "duration": 7.28
    },
    {
      "lang": "en",
      "text": "to further questions about at what grain does one \nhave to recreate the computations? Is it enough to  ",
      "offset": 5485.68,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "recreate the general type of computation? \nOr does every algorithm at every level,  ",
      "offset": 5491.68,
      "duration": 6.4
    },
    {
      "lang": "en",
      "text": "including the within-neuron level, have to be \nrecreated? And there too, there’s disagreement.",
      "offset": 5498.08,
      "duration": 6.4
    },
    {
      "lang": "en",
      "text": "I think we have to take seriously the possibility \nthat recreating the general types of computations  ",
      "offset": 5504.48,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "might be enough. I call this view “large-scale \ncomputational functionalism”: that it might be a  ",
      "offset": 5509.68,
      "duration": 6.88
    },
    {
      "lang": "en",
      "text": "matter of simply creating a global workspace \nor something like that, even if the details  ",
      "offset": 5516.56,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "are quite different from how the global \nworkspace is implemented in the human brain.",
      "offset": 5521.68,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "And if we take that view seriously, as we should,  ",
      "offset": 5526.4,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "it really does suggest a kind of parity. \nI wouldn’t want to overstate it, because  ",
      "offset": 5530.08,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "I’d say that the probability of sentience \nis higher in the biological system than  ",
      "offset": 5535.76,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "in its software emulation. But still, that \nsoftware emulation is potentially a candidate.",
      "offset": 5541.44,
      "duration": 12.8
    },
    {
      "lang": "en",
      "text": "Anil Seth on which parts of the \nbrain are required for consciousness",
      "offset": 5554.24,
      "duration": 6.383
    },
    {
      "lang": "en",
      "text": "Luisa Rodriguez: I’m really, really curious \nto get your thoughts on animals. I think  ",
      "offset": 5560.623,
      "duration": 4.177
    },
    {
      "lang": "en",
      "text": "maybe I would like to start with \ndifferent neuroscientific theories  ",
      "offset": 5564.8,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "of consciousness. Which parts of the brain \nare sufficient and required for consciousness  ",
      "offset": 5569.68,
      "duration": 7.12
    },
    {
      "lang": "en",
      "text": "feels like it might be a really key question for \nthinking about which nonhuman animals we should  ",
      "offset": 5576.8,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "expect to have conscious experiences — because \nsome nonhuman animals, like insects, only have  ",
      "offset": 5581.6,
      "duration": 6.96
    },
    {
      "lang": "en",
      "text": "things that look much more like the very old parts \nof the human brain, the parts that are deeper in.",
      "offset": 5588.56,
      "duration": 7.2
    },
    {
      "lang": "en",
      "text": "Do you have a view on which theories seem \nmost plausible to you? Are the really old  ",
      "offset": 5595.76,
      "duration": 9.44
    },
    {
      "lang": "en",
      "text": "parts of the brain, the subcortical \nparts, sufficient for consciousness?",
      "offset": 5605.2,
      "duration": 4.949
    },
    {
      "lang": "en",
      "text": "Anil Seth: To be honest, I don’t know. But I \nthink to help orient in this really critical  ",
      "offset": 5610.149,
      "duration": 5.931
    },
    {
      "lang": "en",
      "text": "discussion — critical because, of course, it \nhas massive implications for animal welfare  ",
      "offset": 5616.08,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "and how we organise society and so on — it’s \nworth taking a quick step back again and just  ",
      "offset": 5619.68,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "comparing the problem of animal consciousness \nwith the one of AI consciousness. Because in  ",
      "offset": 5625.52,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "both cases there’s uncertainty, but they \nare very different kinds of uncertainty.",
      "offset": 5631.36,
      "duration": 5.743
    },
    {
      "lang": "en",
      "text": "Luisa Rodriguez: Almost opposite ones.",
      "offset": 5637.103,
      "duration": 1.457
    },
    {
      "lang": "en",
      "text": "Anil Seth: Almost exactly opposite. \nIn AI, we have this uncertainty of,  ",
      "offset": 5638.56,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "does the stuff matter? AI is fundamentally \nmade out of something different. Animals  ",
      "offset": 5641.84,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "are fundamentally the same \nbecause we are also animals.",
      "offset": 5647.2,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "And then there’s the things that \nare different. Animals generally  ",
      "offset": 5651.36,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "do not speak to us, and often fail when \nmeasured against our highly questionable  ",
      "offset": 5657.04,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "standards of human intelligence. \nWhereas AI systems now speak to us,  ",
      "offset": 5662.96,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "and measured against these highly questionable \ncriteria, are doing increasingly well.",
      "offset": 5667.92,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "So I think we have to understand how our \npsychological biases are playing into this.  ",
      "offset": 5673.68,
      "duration": 6.8
    },
    {
      "lang": "en",
      "text": "It could well be that AI is more similar to us \nin ways that do not matter for consciousness,  ",
      "offset": 5680.48,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "and less similar in ways that do \n— and nonhuman animals the other  ",
      "offset": 5685.44,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "way around. We’ve got a horrible track record of \nwithholding conscious status and therefore moral  ",
      "offset": 5689.68,
      "duration": 6.4
    },
    {
      "lang": "en",
      "text": "considerability from other animals, but even \nfrom other humans. For some groups of humans,  ",
      "offset": 5696.08,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "we just do this. We’ve historically done \nthis all the time and are still doing it now.",
      "offset": 5701.84,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "There’s this principle in ethics called the \nprecautionary principle: that when we’re  ",
      "offset": 5707.36,
      "duration": 6.4
    },
    {
      "lang": "en",
      "text": "uncertain, we should basically err on the side of \ncaution, given the consequences. I think this is  ",
      "offset": 5713.76,
      "duration": 6.88
    },
    {
      "lang": "en",
      "text": "really worth bearing in mind for nonhuman animals. \nYou could apply the same to AI and say, well, we  ",
      "offset": 5720.64,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "should just apply that: since there’s uncertainty, \nwe should just assume AI is conscious. I think no:  ",
      "offset": 5725.04,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "I think the effect of bias is so strong, and we \ncan’t care for everything as if it’s conscious,  ",
      "offset": 5729.52,
      "duration": 7.92
    },
    {
      "lang": "en",
      "text": "because we just only have a certain \namount of care to go around.",
      "offset": 5737.44,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "But when it comes to nonhuman animals, they \nhave the brain regions, the brain processes,  ",
      "offset": 5742.48,
      "duration": 8
    },
    {
      "lang": "en",
      "text": "that seem highly analogous to the ones in human \nand mammalian brains for emotional experiences,  ",
      "offset": 5750.48,
      "duration": 8.32
    },
    {
      "lang": "en",
      "text": "pain, suffering, pleasure and so on, that I think  ",
      "offset": 5758.8,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "it pays to extend the precautionary \nprinciple more in that direction.",
      "offset": 5761.84,
      "duration": 6.64
    },
    {
      "lang": "en",
      "text": "Figuring out exactly which animals are \nconscious, of course, we don’t know.  ",
      "offset": 5768.48,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "But there are things that I think are relatively \nclear. If we just take mammals very broadly,  ",
      "offset": 5774.56,
      "duration": 8.24
    },
    {
      "lang": "en",
      "text": "from a mouse to a chimpanzee to a human, \nwe find very similar brain structures  ",
      "offset": 5782.8,
      "duration": 8.64
    },
    {
      "lang": "en",
      "text": "and similar sorts of behaviours and \nthings like that. So it seems very,  ",
      "offset": 5791.44,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "very unlikely that there are some mammals that \nlack consciousness. I think mammals are conscious.",
      "offset": 5796.64,
      "duration": 7.2
    },
    {
      "lang": "en",
      "text": "But even then, we’ve had to get rid of some of the \nthings that historically you might have thought  ",
      "offset": 5803.84,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "of as essential for consciousness, like higher \norder reasoning and language. I mean, Descartes  ",
      "offset": 5809.92,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "was infamous — but at the time, it was probably a \nvery sensible move because of the pressure he was  ",
      "offset": 5815.92,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "under from the religious authorities — he was \nvery clear that only humans had consciousness,  ",
      "offset": 5821.44,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "or the kind of consciousness that mattered, \nand that was because we had these rational  ",
      "offset": 5827.6,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "minds. So he associated consciousness \nwith these higher rational functions.",
      "offset": 5832,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "Now people generally don’t do that. So \nmammals are within the magic circle. What  ",
      "offset": 5837.6,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "else? Then it becomes really hard, because \nwe have to just walk this line: we have to  ",
      "offset": 5843.44,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "recognise we’re using humans — and then, by \nextension, mammals — as a kind of benchmark.",
      "offset": 5848,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "But you know, there might well be other ways \nof being conscious. What about the octopus,  ",
      "offset": 5853.52,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "as Peter Godfrey-Smith has written \nbeautifully about? And what about a  ",
      "offset": 5858.24,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "bumblebee? What about bacteria? It’s \nalmost impossible to say. It seems  ",
      "offset": 5861.2,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "intuitive to me that some degree of neural \ncomplexity is important, but I recognise  ",
      "offset": 5867.12,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "I don’t want to fall into the trap of using \nsomething like intelligence as a benchmark.",
      "offset": 5871.6,
      "duration": 5.583
    },
    {
      "lang": "en",
      "text": "Luisa Rodriguez: Yeah. I mean, that’s \njust basically what I find both maddening  ",
      "offset": 5877.183,
      "duration": 4.817
    },
    {
      "lang": "en",
      "text": "and fascinating about this question of nonhuman \nanimals. It feels like there’s this very  ",
      "offset": 5882,
      "duration": 8.32
    },
    {
      "lang": "en",
      "text": "frustrating slippery slope thing, where I don’t \nwant to be overly biased toward humans, or towards  ",
      "offset": 5890.32,
      "duration": 6.8
    },
    {
      "lang": "en",
      "text": "structures that kind of “create consciousness,” \nwhatever that means, in the way that ours does.",
      "offset": 5897.12,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "And it does seem like there might be multiple \nways to do it. And over time, I’ve become much,  ",
      "offset": 5902.16,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "much more sympathetic to the idea that \nnot just birds, and not just cephalopods,  ",
      "offset": 5906.88,
      "duration": 6.64
    },
    {
      "lang": "en",
      "text": "but insects have some kinds of experiences. And I \njust find it really confusing about where and how  ",
      "offset": 5913.52,
      "duration": 10.64
    },
    {
      "lang": "en",
      "text": "and whether it makes sense to draw a line, or \nmaybe that’s just philosophically nonsensical.",
      "offset": 5924.16,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "So I’m really drawn to this question, which is \nwhy I opened with it, of: Can neuroscience point  ",
      "offset": 5928.88,
      "duration": 6.96
    },
    {
      "lang": "en",
      "text": "to functions or places or parts of the brain that \nseem related enough to consciousness, that if we  ",
      "offset": 5935.84,
      "duration": 10.48
    },
    {
      "lang": "en",
      "text": "see analogous things in bees, we should update a \nlot on that? But I also have the impression that  ",
      "offset": 5946.32,
      "duration": 11.84
    },
    {
      "lang": "en",
      "text": "there’s still so much debate about subcortical and \ncortical theories, and which is more plausible,  ",
      "offset": 5958.16,
      "duration": 6.56
    },
    {
      "lang": "en",
      "text": "that maybe we’re just not there, and that’s \nnot possible now, and might not be for a while.",
      "offset": 5964.72,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "Anil Seth: Jonathan Birch, \nwho’s a philosopher at UCL,  ",
      "offset": 5969.6,
      "duration": 2.48
    },
    {
      "lang": "en",
      "text": "has this beautiful new book called The \nEdge of Sentience, which I think is all  ",
      "offset": 5972.08,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "about this. He’s trying to figure out how \nfar we can generalise and on what basis.",
      "offset": 5975.12,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "I think the issue is that it seems very sensible \nthat consciousness is multiply realisable to an  ",
      "offset": 5980.08,
      "duration": 9.84
    },
    {
      "lang": "en",
      "text": "extent: that different kinds of brains could \ngenerate different kinds of experience,  ",
      "offset": 5989.92,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "but it’d still be experience. But to know when \nthat’s the case, we have to understand the sort  ",
      "offset": 5995.44,
      "duration": 8.64
    },
    {
      "lang": "en",
      "text": "of basis of consciousness in a way that goes \nbeyond, “It requires this or that region.”",
      "offset": 6004.08,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "We need to know what is it that these \nbrain areas are doing or being that  ",
      "offset": 6008.88,
      "duration": 7.68
    },
    {
      "lang": "en",
      "text": "makes them important for consciousness in \na way that we could say, well, we obviously  ",
      "offset": 6016.56,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "don’t see a frontal cortex in a honeybee, \nbecause they don’t have that kind of brain,  ",
      "offset": 6020.32,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "but they’re doing something, or their brains are \nmade of the stuff and organised in the right way,  ",
      "offset": 6024.32,
      "duration": 6.8
    },
    {
      "lang": "en",
      "text": "that we can have some credence that \nthat’s enough for consciousness.",
      "offset": 6031.12,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "And we don’t really have that yet. I \nmean, the theories of consciousness  ",
      "offset": 6035.28,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "that exist are varied. Some of them are \npretty explicitly theories about human  ",
      "offset": 6038.24,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "consciousness, and they’re harder \nto extrapolate to nonhuman animals:  ",
      "offset": 6043.44,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "like what would be a global workspace in \na fruit fly? You could make some guesses,  ",
      "offset": 6049.36,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "but the theory as it is is more assuming there’s \na kind of cortical architecture like a human.",
      "offset": 6055.6,
      "duration": 7.04
    },
    {
      "lang": "en",
      "text": "And other theories, like integrated \ninformation theory, are much clearer:  ",
      "offset": 6062.64,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "wherever there is some nonzero integrated \ninformation maxima of X, there’s consciousness.  ",
      "offset": 6066.8,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "But it’s just impossible to actually measure \nthat in practice. So very, very hard.",
      "offset": 6072.16,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "But the path, I think, is clear that the \nbetter that we can understand consciousness,  ",
      "offset": 6076.96,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "where we are sure that it exists, the \nsurer our footing will be elsewhere  ",
      "offset": 6082.64,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "where we’re less confident, because \nwe will be able to generalise better.",
      "offset": 6088,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "So where are your areas of \nuncertainty? I’m always interested.",
      "offset": 6091.52,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "Luisa Rodriguez: I feel just through getting \nto learn about these topics for this show,  ",
      "offset": 6094.32,
      "duration": 11.68
    },
    {
      "lang": "en",
      "text": "I constantly get told these \namazing facts about fish and bees,  ",
      "offset": 6106,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "and the kinds of learning they can do, and \nthe kinds of motivational tradeoffs they make,  ",
      "offset": 6112,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "and the fact that they do nociception, \nand that nociception gets integrated  ",
      "offset": 6116.88,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "into other parts of their behaviour. And \nthat all feels really compelling to me.",
      "offset": 6122.56,
      "duration": 6.4
    },
    {
      "lang": "en",
      "text": "Then I talk to someone who’s like, “Yeah, \nbut a lot of that could just be happening  ",
      "offset": 6128.96,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "unconsciously.” And at what point it’s more \nplausible that they’re little robots doing  ",
      "offset": 6133.84,
      "duration": 8.48
    },
    {
      "lang": "en",
      "text": "things unconsciously, and then at what point \nit becomes more plausible that a little robot  ",
      "offset": 6142.32,
      "duration": 6.96
    },
    {
      "lang": "en",
      "text": "doing that is just a less likely story than \nit’s got the lights switched on in some way,  ",
      "offset": 6149.28,
      "duration": 7.12
    },
    {
      "lang": "en",
      "text": "and it’s making tradeoffs because the world is \ncomplicated and it’s evolved to have more complex  ",
      "offset": 6156.4,
      "duration": 8.24
    },
    {
      "lang": "en",
      "text": "systems going on so that it can survive. \nI just find that really confusing.",
      "offset": 6164.64,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "Anil Seth: Yeah. I think me too. But actually \nthere’s a point you make which I didn’t make,  ",
      "offset": 6170.88,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "so I’m grateful for you bringing that up, \nwhich is the functional point of view too. So  ",
      "offset": 6176,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "instead of just asking which brain regions or \ninteractions between brain regions do we see,  ",
      "offset": 6180,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "we can ask from the point of \nview of function and evolution,  ",
      "offset": 6185.04,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "which is usually the best way to make sense of \nthings comparatively between animals and biology.",
      "offset": 6189.04,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "So what is the function of consciousness? \nAnd if we can understand more about that,  ",
      "offset": 6193.92,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "then we can have another criterion for \nsaying, which other animals do we see  ",
      "offset": 6198.8,
      "duration": 6.64
    },
    {
      "lang": "en",
      "text": "facing and addressing those same kinds \nof functions? And of course there may be  ",
      "offset": 6205.44,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "other functions. We have to be sensitive to that \ntoo. But at least it’s another productive line.",
      "offset": 6210.72,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "And in humans and mammals, there’s no single \nanswer. But it seems as though consciousness  ",
      "offset": 6216.8,
      "duration": 6.32
    },
    {
      "lang": "en",
      "text": "gets into the picture when we need to bring \ntogether a lot of different kinds of information  ",
      "offset": 6223.12,
      "duration": 7.36
    },
    {
      "lang": "en",
      "text": "signals from the environment in a way that \nis very much centred on the possibilities  ",
      "offset": 6230.48,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "for flexible action — all kind of calibrated \nin the service of homeostasis and survival.",
      "offset": 6236.96,
      "duration": 7.76
    },
    {
      "lang": "en",
      "text": "So automatic actions, reflexive actions, don’t \nseem to involve consciousness. Flexibility,  ",
      "offset": 6244.72,
      "duration": 6.72
    },
    {
      "lang": "en",
      "text": "informational richness, and sort of \ngoal-directedness, those seem to be  ",
      "offset": 6252.08,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "functional clues. So to the extent we see \nanimals implementing those similar functions,  ",
      "offset": 6257.36,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "I think that’s quite a good reason for \nattributing consciousness. But it’s not 100%.",
      "offset": 6262.4,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "Luisa Rodriguez: Yeah. That’s basically \nwhere I am. And it means that I now feel  ",
      "offset": 6268.48,
      "duration": 6.56
    },
    {
      "lang": "en",
      "text": "a lot of feelings about the bees in my \ngarden. And mostly I feel really grateful  ",
      "offset": 6275.04,
      "duration": 6.96
    },
    {
      "lang": "en",
      "text": "to have learned about these topics, \nbut I also feel really overwhelmed.",
      "offset": 6282,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "Peter Godfrey-Smith on uploads of ourselves",
      "offset": 6288,
      "duration": 5.263
    },
    {
      "lang": "en",
      "text": "Luisa Rodriguez: Why are you doubtful \nthat we could create uploads of ourselves?",
      "offset": 6293.263,
      "duration": 4.497
    },
    {
      "lang": "en",
      "text": "Peter Godfrey-Smith: My view on that \ncomes from the general position that  ",
      "offset": 6297.76,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "I’m developing — slowly, cautiously — \non the biology of conscious experience,  ",
      "offset": 6303.2,
      "duration": 8.24
    },
    {
      "lang": "en",
      "text": "or the biology of felt experience. \nThis was developed in a bit more  ",
      "offset": 6311.44,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "detail in the second book, but it has a \ndiscussion in Living on Earth as well.",
      "offset": 6316.48,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "The view that I think is most justified — \nthe view I would at least put money on — is  ",
      "offset": 6322.32,
      "duration": 7.92
    },
    {
      "lang": "en",
      "text": "a view in which some of what it takes to be a \nsystem with felt experience involves relatively  ",
      "offset": 6330.24,
      "duration": 10.32
    },
    {
      "lang": "en",
      "text": "schematic functional properties: the way that \na system is organised in relation to sensing  ",
      "offset": 6340.56,
      "duration": 8.4
    },
    {
      "lang": "en",
      "text": "and action and memory and the internal \nprocessing and so on. And some of those,  ",
      "offset": 6348.96,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "what are often referred to as “functional \nproperties,” could exist in a variety of  ",
      "offset": 6354.8,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "different physical realisations, in different \nhardwares or different physical bases.",
      "offset": 6360.16,
      "duration": 7.68
    },
    {
      "lang": "en",
      "text": "But I don’t think that’s the whole story: \nI think nervous systems are special.  ",
      "offset": 6367.84,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "I think that the way that nervous systems \nwork, the way that our brains work… There are  ",
      "offset": 6371.68,
      "duration": 7.36
    },
    {
      "lang": "en",
      "text": "two kinds of properties that nervous systems \nhave. There’s a collection of point-to-point  ",
      "offset": 6379.04,
      "duration": 7.28
    },
    {
      "lang": "en",
      "text": "network interactions — where this cell makes that \ncell fire, and prevents that cell from firing,  ",
      "offset": 6386.32,
      "duration": 6.32
    },
    {
      "lang": "en",
      "text": "the spiking of neurons, and the great \npoint-to-point massive network interactions.",
      "offset": 6392.64,
      "duration": 7.36
    },
    {
      "lang": "en",
      "text": "And there’s also other stuff, which for years was \nsomewhat neglected I think in these discussions,  ",
      "offset": 6400,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "but which I think is probably very important. \nThere are more diffuse, large-scale dynamic  ",
      "offset": 6405.12,
      "duration": 6.56
    },
    {
      "lang": "en",
      "text": "properties that exist within nervous systems: \noscillatory patterns of different speeds,  ",
      "offset": 6411.68,
      "duration": 7.92
    },
    {
      "lang": "en",
      "text": "subtle forms of synchronisation that span the \nwhole or much of the brain. And these are the  ",
      "offset": 6419.6,
      "duration": 9.36
    },
    {
      "lang": "en",
      "text": "sorts of things picked up in an EEG machine, \nthat kind of large-scale electrical interaction.",
      "offset": 6428.96,
      "duration": 6.64
    },
    {
      "lang": "en",
      "text": "And I didn’t come up with this myself. There’s \na tradition. Francis Crick thought this,  ",
      "offset": 6435.6,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "neuroscientists like Wolf Singer, a number of \nother people have argued that this side of the  ",
      "offset": 6442.96,
      "duration": 6.8
    },
    {
      "lang": "en",
      "text": "brain is important to the biology of conscious \nexperience, along with the sort of networky,  ",
      "offset": 6449.76,
      "duration": 8.64
    },
    {
      "lang": "en",
      "text": "more computer computational side of the brain: \nthat both sets of properties of nervous systems  ",
      "offset": 6458.4,
      "duration": 7.36
    },
    {
      "lang": "en",
      "text": "are important. And in particular, the \nunity of experience — the way in which  ",
      "offset": 6465.76,
      "duration": 8.56
    },
    {
      "lang": "en",
      "text": "brain activity generates a unified point \nof view on the world — has a dependence  ",
      "offset": 6474.32,
      "duration": 8.96
    },
    {
      "lang": "en",
      "text": "upon the oscillatory and other large-scale \ndynamic patterns that you get in brains.",
      "offset": 6483.28,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "Now, if you look at computer systems, you \ncan program a computer to have a moderately  ",
      "offset": 6488.88,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "decent facsimile of the network properties in \na brain. But the large-scale dynamic patterns,  ",
      "offset": 6494.32,
      "duration": 9.6
    },
    {
      "lang": "en",
      "text": "the oscillatory patterns that span the whole, \nthey’re a totally different matter. I mean,  ",
      "offset": 6503.92,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "you could write a program that includes a kind \nof rough simulation, where you’d know what  ",
      "offset": 6508.72,
      "duration": 8.4
    },
    {
      "lang": "en",
      "text": "was happening if the physical system in fact had \nlarge scale dynamic patterns of the relevant kind,  ",
      "offset": 6517.12,
      "duration": 8.16
    },
    {
      "lang": "en",
      "text": "but that’s different from having in a \nphysical system those activities actually  ",
      "offset": 6525.28,
      "duration": 6.32
    },
    {
      "lang": "en",
      "text": "going on — present physically, rather than \njust being represented in a computer program.",
      "offset": 6531.6,
      "duration": 6.96
    },
    {
      "lang": "en",
      "text": "I do think there’s a real difference between \nthose generally, and especially in the case  ",
      "offset": 6538.56,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "of these brain oscillations and the like. \nYou would have to build a computer where the  ",
      "offset": 6543.52,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "hardware had a brain-like pattern of activities \nand tendencies. People might one day do that,  ",
      "offset": 6549.6,
      "duration": 11.12
    },
    {
      "lang": "en",
      "text": "but it’s not part of what people \nnormally discuss in debates about  ",
      "offset": 6560.72,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "artificial consciousness, uploading \nourselves to the cloud and so on.",
      "offset": 6566,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "People assume that you could take a computer, \nlike the ones you and I are using now,  ",
      "offset": 6569.92,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "with the same kind of hardware, and if you just \ngot the program right — if it was a big powerful  ",
      "offset": 6574.32,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "one and you programmed it just right — it could \nrun through not just a representation of what a  ",
      "offset": 6579.52,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "brain does, but a kind of realisation and another \ninstance or another form of that brain activity.",
      "offset": 6585.52,
      "duration": 7.84
    },
    {
      "lang": "en",
      "text": "Now, because I think the biology of consciousness \nis just not like that — I think that the second  ",
      "offset": 6593.36,
      "duration": 6.64
    },
    {
      "lang": "en",
      "text": "set of features of brains really matter — \nI think that it will be much harder than  ",
      "offset": 6600,
      "duration": 6.32
    },
    {
      "lang": "en",
      "text": "people normally suppose to build any kind \nof artificial sentient system that has no  ",
      "offset": 6606.32,
      "duration": 6.88
    },
    {
      "lang": "en",
      "text": "living parts. It’s not that I think there’s \na kind of absolute barrier from the materials  ",
      "offset": 6613.2,
      "duration": 7.2
    },
    {
      "lang": "en",
      "text": "themselves — I don’t know if there is — but \nI certainly think it would have to be much,  ",
      "offset": 6620.4,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "much more brain-like. The computer hardware would \nhave to be a lot more brain-like than it is now.",
      "offset": 6625.52,
      "duration": 7.28
    },
    {
      "lang": "en",
      "text": "I mean, who knows if we could build large \nnumbers of these, powered with a big solar  ",
      "offset": 6634.72,
      "duration": 7.2
    },
    {
      "lang": "en",
      "text": "array, and replicate our minds in them? I \nthink it’s very unlikely, I must say. Now,  ",
      "offset": 6641.92,
      "duration": 7.36
    },
    {
      "lang": "en",
      "text": "whether that’s unlikely or not, I don’t think \nI should be confident about. The thing I am  ",
      "offset": 6649.28,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "a bit confident about, or fairly confident \nabout, is the idea that there’s lots of what  ",
      "offset": 6654.16,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "happens in brains that’s probably important \nto conscious experience, which is just being  ",
      "offset": 6660.32,
      "duration": 6.4
    },
    {
      "lang": "en",
      "text": "ignored in discussions of uploading our \nminds to the cloud and things like that.",
      "offset": 6666.72,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "Luisa Rodriguez: Yeah, that’s really \nhelpful. I don’t know very much about  ",
      "offset": 6672.48,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "these large-scale dynamics. Are \nthey a result of neuronal firings,  ",
      "offset": 6677.12,
      "duration": 6.88
    },
    {
      "lang": "en",
      "text": "or are they a result of other \nthings going on in the brain?",
      "offset": 6684,
      "duration": 4.139
    },
    {
      "lang": "en",
      "text": "Peter Godfrey-Smith: This is quite a controversial \npoint, actually. It’s a good question.  ",
      "offset": 6688.139,
      "duration": 4.901
    },
    {
      "lang": "en",
      "text": "The sense I have — and I’m continually trying \nto learn the latest on this — is that most of  ",
      "offset": 6693.04,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "the activity in those large-scale dynamic \npatterns is not just a summing together of  ",
      "offset": 6699.52,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "lots of firing of neurons. Because if it \nwas, then you might say that the network,  ",
      "offset": 6704.88,
      "duration": 6.66
    },
    {
      "lang": "en",
      "text": "point-to-point, cell-to-cell things really are all \nthat matters once you’ve really captured those,  ",
      "offset": 6711.54,
      "duration": 6.3
    },
    {
      "lang": "en",
      "text": "and the other stuff is just a consequence or \na kind of zoomed-out manifestation of that.",
      "offset": 6717.84,
      "duration": 6.72
    },
    {
      "lang": "en",
      "text": "And that does not seem to be the case. It seems \nrather that within the cells that make up our  ",
      "offset": 6724.56,
      "duration": 9.36
    },
    {
      "lang": "en",
      "text": "brain, there’s a kind of to and fro of ions \nacross membranes that is below the threshold  ",
      "offset": 6733.92,
      "duration": 10.32
    },
    {
      "lang": "en",
      "text": "that is required to actually make the neuron \nfire, that dramatic spark-like firing. It’s  ",
      "offset": 6744.24,
      "duration": 7.2
    },
    {
      "lang": "en",
      "text": "more of a kind of rhythmic, lower-level, \nsub-threshold electrical oscillation.",
      "offset": 6751.44,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "And those oscillations affect the firing of \nneurons, and the firing of neurons affect the  ",
      "offset": 6757.76,
      "duration": 6.8
    },
    {
      "lang": "en",
      "text": "oscillations. But there is a kind of duality \nof processes there, and it’s not just that  ",
      "offset": 6764.56,
      "duration": 7.44
    },
    {
      "lang": "en",
      "text": "if you knew which cells were firing when, and you \nignored everything else, that you could recapture,  ",
      "offset": 6772,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "in a sense, the large-scale dynamic \npatterns. There’s more than that.",
      "offset": 6778.16,
      "duration": 3.983
    },
    {
      "lang": "en",
      "text": "Luisa Rodriguez: Right. So if you take this \nkind of, for me, what has been a very intuitive,  ",
      "offset": 6782.143,
      "duration": 6.577
    },
    {
      "lang": "en",
      "text": "but I guess would feel like an inadequate \nthought experiment of replacing each neuron  ",
      "offset": 6788.72,
      "duration": 7.68
    },
    {
      "lang": "en",
      "text": "in the human brain, one by one, with an artificial \nsilicon-based one, it sounds like you would guess  ",
      "offset": 6796.4,
      "duration": 8.64
    },
    {
      "lang": "en",
      "text": "that doing that, even if you got that to \nwork, it wouldn’t make the entire process  ",
      "offset": 6805.04,
      "duration": 8.96
    },
    {
      "lang": "en",
      "text": "and physical system of the brain be entirely \nartificial? There would be other things that  ",
      "offset": 6814,
      "duration": 6.96
    },
    {
      "lang": "en",
      "text": "remained biological that were playing a crucial \nrole that you hadn’t replaced and replicated yet?",
      "offset": 6820.96,
      "duration": 4.859
    },
    {
      "lang": "en",
      "text": "Peter Godfrey-Smith: Well, the way you described \nit, you made it sound like at the end of this  ",
      "offset": 6825.819,
      "duration": 2.901
    },
    {
      "lang": "en",
      "text": "process, the biological parts were gone. \nSo is it that we really replace everything,  ",
      "offset": 6828.72,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "or is it just we replace some stuff and leave \nsome of the biological material in place?",
      "offset": 6834.48,
      "duration": 6.8
    },
    {
      "lang": "en",
      "text": "Luisa Rodriguez: I think I’m curious about \nif you just replaced neurons with synthetic  ",
      "offset": 6841.28,
      "duration": 8.88
    },
    {
      "lang": "en",
      "text": "neurons, do you think there would be \na working brain at the end? And if so,  ",
      "offset": 6850.16,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "would that be because the other \nbiological stuff was still there?",
      "offset": 6855.6,
      "duration": 3.499
    },
    {
      "lang": "en",
      "text": "Peter Godfrey-Smith: OK, right. I see. It’s \na commitment of my view that if you tried to  ",
      "offset": 6859.099,
      "duration": 5.141
    },
    {
      "lang": "en",
      "text": "do that, you would change all sorts of stuff: you \ncan’t really preserve the things that the brain is  ",
      "offset": 6864.24,
      "duration": 7.84
    },
    {
      "lang": "en",
      "text": "doing at the start and have them continue once the \nneurons have been replaced by artificial devices.",
      "offset": 6872.08,
      "duration": 7.92
    },
    {
      "lang": "en",
      "text": "I mean, what are the “artificial devices”? When \npeople talk about thought experiments of this  ",
      "offset": 6881.36,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "kind — which they’ve done, I think, in quite \ninteresting ways for about 40 years now — what  ",
      "offset": 6887.28,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "they usually imagine is you’re replacing each \nneuron with a kind of relay object that sums up  ",
      "offset": 6892.16,
      "duration": 7.6
    },
    {
      "lang": "en",
      "text": "the inputs coming in and either fires or doesn’t \nfire, and its firing then contributes to the  ",
      "offset": 6899.76,
      "duration": 7.28
    },
    {
      "lang": "en",
      "text": "inputs to various other downstream cells. And it’s \njust doing that. It’s not doing anything else.",
      "offset": 6907.04,
      "duration": 6.8
    },
    {
      "lang": "en",
      "text": "Now, if the things you put into the brain were \njust doing that, then essentially you would  ",
      "offset": 6913.84,
      "duration": 7.6
    },
    {
      "lang": "en",
      "text": "drop all those large-scale dynamic patterns. \nThey just wouldn’t exist anymore. They would  ",
      "offset": 6921.44,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "be gone. So it would be a different thing \nphysically. It would do different things,  ",
      "offset": 6925.04,
      "duration": 8.48
    },
    {
      "lang": "en",
      "text": "because the relationship between the firing of \nneurons and those slower oscillations does make  ",
      "offset": 6933.52,
      "duration": 7.12
    },
    {
      "lang": "en",
      "text": "a difference to how the brain works, so you would \nchange some stuff that would have consequences.",
      "offset": 6940.64,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "Now, something that people haven’t talked about \nso much is whether you could put in an artificial  ",
      "offset": 6946.72,
      "duration": 7.36
    },
    {
      "lang": "en",
      "text": "object that did really everything that the neuron \nis doing, where there’s this sort of seepage of  ",
      "offset": 6954.08,
      "duration": 8.64
    },
    {
      "lang": "en",
      "text": "ions across the borders, and it has subtle \neffects on the electrical properties of  ",
      "offset": 6962.72,
      "duration": 7.52
    },
    {
      "lang": "en",
      "text": "neighbouring cells and so on. A replacement \nthat captured a lot more of what a neuron did.",
      "offset": 6970.24,
      "duration": 7.92
    },
    {
      "lang": "en",
      "text": "And then we reached the point where we \ngot to a few minutes ago, where I said  ",
      "offset": 6978.16,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "who knows what artificial hardwares might be \npossible? I don’t think I know nearly enough  ",
      "offset": 6983.04,
      "duration": 8.16
    },
    {
      "lang": "en",
      "text": "about these matters to be confident that you \ncould never build something that did that.",
      "offset": 6991.2,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "Luisa Rodriguez: Just to make sure \nI understand your position on this:  ",
      "offset": 6996,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "it’s not that you’re confident that it’s \nimpossible to create digital minds in general,  ",
      "offset": 7000.8,
      "duration": 7.44
    },
    {
      "lang": "en",
      "text": "or that it’s impossible to eventually come up \nwith some hardware system that does replicate  ",
      "offset": 7008.24,
      "duration": 9.92
    },
    {
      "lang": "en",
      "text": "everything relevant to consciousness in \na human — but that the current thought  ",
      "offset": 7018.16,
      "duration": 6.96
    },
    {
      "lang": "en",
      "text": "experiments where we only replicate some of \nthe basic functions of a neuron are not enough.",
      "offset": 7025.12,
      "duration": 7.44
    },
    {
      "lang": "en",
      "text": "Peter Godfrey-Smith: There was a phrase you used \n— “digital minds” — and I guess I don’t think  ",
      "offset": 7035.52,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "there will ever be digital minds. I think that’s \nimagining a system which lacks too much of what  ",
      "offset": 7040.88,
      "duration": 9.44
    },
    {
      "lang": "en",
      "text": "brains and nervous systems are like in order for \nit to have a mind. I don’t know if there might  ",
      "offset": 7050.32,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "be wholly artificial minds in the future that are \nmade of different stuff than brains are made of,  ",
      "offset": 7056.48,
      "duration": 7.68
    },
    {
      "lang": "en",
      "text": "but that have the important duality of \nproperties that you see in nervous systems,  ",
      "offset": 7064.16,
      "duration": 6.4
    },
    {
      "lang": "en",
      "text": "and thereby achieve a kind of sentience. I think \nit’s further away than people often suppose,  ",
      "offset": 7070.56,
      "duration": 7.84
    },
    {
      "lang": "en",
      "text": "but I would not want to say it could never happen.",
      "offset": 7078.4,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "The thing I want to press on in a kind of critical \nway is the habits that people have of thinking…  ",
      "offset": 7082.16,
      "duration": 8.64
    },
    {
      "lang": "en",
      "text": "People think, “I know what neurons and nervous \nsystems do. It’s like a telephone exchange:  ",
      "offset": 7090.8,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "you’ve got this thing that makes that \nthing go, and it’s like a big network,  ",
      "offset": 7095.92,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "and that’s all there is to it.” They \nthink that, and then they think, “Well,  ",
      "offset": 7099.44,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "a computer is a place where I can have that kind \nof thing going on, replicated to a high degree of  ",
      "offset": 7103.44,
      "duration": 6.96
    },
    {
      "lang": "en",
      "text": "fidelity. So I know that an ordinary computer \nhas what it takes as hardware to be sentient,  ",
      "offset": 7110.4,
      "duration": 9.2
    },
    {
      "lang": "en",
      "text": "basically, to be conscious if I put the right \nprogram in there — because the program would  ",
      "offset": 7119.6,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "just have to create a version of those \nnetworked, point-to-point interactions.”",
      "offset": 7124.08,
      "duration": 6.32
    },
    {
      "lang": "en",
      "text": "That claim I do want to push back against. I \ndon’t think we have any reason to believe that.  ",
      "offset": 7130.4,
      "duration": 7.04
    },
    {
      "lang": "en",
      "text": "And I think that artificial sentience is just \ngoing to be a harder and further-in-the-future  ",
      "offset": 7137.44,
      "duration": 6.4
    },
    {
      "lang": "en",
      "text": "thing than we have supposed. And as I say \nat one point in this book, Living on Earth,  ",
      "offset": 7143.84,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "I’m not sure whether that’s a bad thing. \nIf there turns out to be a kind of barrier  ",
      "offset": 7150.32,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "between the I side of AI — the intelligent side \nof AI — and the kind of conscious, sentient,  ",
      "offset": 7155.2,
      "duration": 8.96
    },
    {
      "lang": "en",
      "text": "feeling side of artificial minds, if there’s a \nbarrier there, that might not be a bad thing.",
      "offset": 7164.16,
      "duration": 6.8
    },
    {
      "lang": "en",
      "text": "One reason for that is if we start building lots \nof sentient systems that have this different kind  ",
      "offset": 7170.96,
      "duration": 6.72
    },
    {
      "lang": "en",
      "text": "of hardware, and they’re under our control, we \nmade them — I don’t know how likely it is that  ",
      "offset": 7177.68,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "their experiences are going to be positive, at \nleast in the early stages. It’s going to be a  ",
      "offset": 7184.16,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "sort of a klutzy, messed-up version of artificial \nsentience that we’re working with. And if I’m a  ",
      "offset": 7189.76,
      "duration": 8.08
    },
    {
      "lang": "en",
      "text": "disembodied spirit of the kind that you were \ntalking about earlier, I don’t want to come  ",
      "offset": 7197.84,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "down to Earth and live as an early effort in the \nhuman attempt to make artificial sentient systems.",
      "offset": 7202.64,
      "duration": 10.56
    },
    {
      "lang": "en",
      "text": "Jonathan Birch on treading lightly \naround the “edge cases” of sentience",
      "offset": 7213.2,
      "duration": 6.543
    },
    {
      "lang": "en",
      "text": "Luisa Rodriguez: So a big part of the book \nthen explores these candidates for sentience:  ",
      "offset": 7219.743,
      "duration": 4.977
    },
    {
      "lang": "en",
      "text": "beings that we think could plausibly be sentient, \nbut because we just know so little about exactly  ",
      "offset": 7225.44,
      "duration": 8.32
    },
    {
      "lang": "en",
      "text": "what sentience even is, and how different \nbeings feel it and in what proportions,  ",
      "offset": 7233.76,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "there’s not enough clear understanding or \nevidence to be sure what it is like to be them.",
      "offset": 7239.28,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "You look at some familiar candidates, \nincluding animals of different classes,  ",
      "offset": 7245.2,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "as well as AI. And then you also look at \nsome cases that I was really unfamiliar with,  ",
      "offset": 7250.48,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "including people with disorders of \nconsciousness — so people in comas,  ",
      "offset": 7256.32,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "for example — and then also “neural \norganoids,” which I’d never heard of.",
      "offset": 7260.72,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "To start us off, what \nexactly is a neural organoid?",
      "offset": 7266.4,
      "duration": 4.384
    },
    {
      "lang": "en",
      "text": "Jonathan Birch: This is another very \nfast-moving area of emerging technology.  ",
      "offset": 7270.784,
      "duration": 4.656
    },
    {
      "lang": "en",
      "text": "Basically, it uses human stem cells \nthat are induced to form neural tissue.  ",
      "offset": 7275.44,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "The aim is to produce a 3D model of some brain \nregion, or in some cases a whole developing brain.",
      "offset": 7282.24,
      "duration": 8.703
    },
    {
      "lang": "en",
      "text": "Luisa Rodriguez: And what’s \nthe case for creating them?",
      "offset": 7290.943,
      "duration": 2.561
    },
    {
      "lang": "en",
      "text": "Jonathan Birch: I think it’s a very exciting area \nof research. You can make organoids for any organ,  ",
      "offset": 7293.504,
      "duration": 5.696
    },
    {
      "lang": "en",
      "text": "really. In a way, it’s a potential replacement \nfor animal research. If you ask what we do now,  ",
      "offset": 7299.2,
      "duration": 7.12
    },
    {
      "lang": "en",
      "text": "usually people do research on whole animals, \nwhich are undeniably sentient. And here we  ",
      "offset": 7306.32,
      "duration": 8.16
    },
    {
      "lang": "en",
      "text": "have a potential way to gain insight into \nthe human version of the organ. It could be  ",
      "offset": 7314.48,
      "duration": 6.32
    },
    {
      "lang": "en",
      "text": "a better model, and it’s much less likely \nto be sentient if it’s something like a  ",
      "offset": 7320.8,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "kidney organoid or a stomach organoid. It’s \nreally only when we’re looking at the case  ",
      "offset": 7325.92,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "of the brain and neural organoids that the \npossibility of sentience starts to reemerge.",
      "offset": 7330.96,
      "duration": 6.863
    },
    {
      "lang": "en",
      "text": "Luisa Rodriguez: Yeah. And intuitively, \nthe case for sentience does feel like it  ",
      "offset": 7337.823,
      "duration": 3.937
    },
    {
      "lang": "en",
      "text": "immediately lands for me. If you are trying to \nmake an organoid that is enough like a brain that  ",
      "offset": 7341.76,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "we can learn about brains, it doesn’t seem totally \noutrageous that it would be a sentience candidate.  ",
      "offset": 7348.24,
      "duration": 8.72
    },
    {
      "lang": "en",
      "text": "What is the evidence that we have so far?",
      "offset": 7356.96,
      "duration": 3.984
    },
    {
      "lang": "en",
      "text": "Jonathan Birch: It’s a complicated \npicture. I think there are reasons  ",
      "offset": 7360.944,
      "duration": 3.856
    },
    {
      "lang": "en",
      "text": "to be quite sceptical about organoids as they \nare now, but the technology is moving so fast,  ",
      "offset": 7364.8,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "there’s always a risk of being ambushed \nby some new development. At present,  ",
      "offset": 7370.4,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "it really doesn’t seem like \nthere’s clear sleep/wake cycles;  ",
      "offset": 7375.76,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "it doesn’t seem like those brainstem structures \nor midbrain structures that regulate sleep/wake  ",
      "offset": 7379.84,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "cycles and that are so important on \nthe Merker/Panksepp view are in place.",
      "offset": 7385.84,
      "duration": 6.8
    },
    {
      "lang": "en",
      "text": "But there are reasons to be worried. For me,  ",
      "offset": 7392.64,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "the main reason to be worried was a study from \n2019 that allowed organoids to grow for about  ",
      "offset": 7397.28,
      "duration": 7.36
    },
    {
      "lang": "en",
      "text": "a year, I think, and compared them to the brains \nof preterm infants using EEG. So they used EEG  ",
      "offset": 7405.28,
      "duration": 13.12
    },
    {
      "lang": "en",
      "text": "data from the preterm infants to train a model, \nand then they used that model to try and guess  ",
      "offset": 7418.4,
      "duration": 8.56
    },
    {
      "lang": "en",
      "text": "the age of the organoid from its EEG data, \nand the model performed better than chance.",
      "offset": 7426.96,
      "duration": 5.903
    },
    {
      "lang": "en",
      "text": "Luisa Rodriguez: Wow.",
      "offset": 7432.863,
      "duration": 0.801
    },
    {
      "lang": "en",
      "text": "Jonathan Birch: So it’s hard to interpret \nthis kind of study, because some people,  ",
      "offset": 7433.664,
      "duration": 5.856
    },
    {
      "lang": "en",
      "text": "I suppose, read it superficially as saying these \norganoids are like the brains of preterm infants.  ",
      "offset": 7439.52,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "And that’s an exaggeration, because they’re very, \nvery different and much, much smaller. But still,  ",
      "offset": 7444.8,
      "duration": 9.12
    },
    {
      "lang": "en",
      "text": "there’s enough resemblance in the EEG to allow \nestimates of the age that are better than chance.",
      "offset": 7453.92,
      "duration": 8.383
    },
    {
      "lang": "en",
      "text": "Luisa Rodriguez: It’s definitely \nsomething. I find it unsettling, for sure.",
      "offset": 7462.303,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "Jonathan Birch: It is, yeah. I think a \nlot of people had that reaction as well,  ",
      "offset": 7466.544,
      "duration": 3.616
    },
    {
      "lang": "en",
      "text": "and I think that’s why we’re now seeing quite a \nlively debate in bioethics about how to regulate  ",
      "offset": 7470.16,
      "duration": 7.12
    },
    {
      "lang": "en",
      "text": "this emerging area of research. It’s currently \npretty unregulated, and it raises this worrying  ",
      "offset": 7477.28,
      "duration": 6.8
    },
    {
      "lang": "en",
      "text": "prospect of scientists taking things too far — \nwhere they will often say, “These systems are only  ",
      "offset": 7484.08,
      "duration": 6.4
    },
    {
      "lang": "en",
      "text": "a million neurons; we want to go up to 10 million, \nbut that’s so tiny compared to a human brain.”",
      "offset": 7490.48,
      "duration": 7.36
    },
    {
      "lang": "en",
      "text": "And it is tiny compared to a human brain. But \nif you compare it to the number of neurons in a  ",
      "offset": 7497.84,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "bee brain, for example, that’s about a million. So \nthese near-future organoids will be about the size  ",
      "offset": 7502.32,
      "duration": 6.8
    },
    {
      "lang": "en",
      "text": "of 10 bee brains in terms of neuron counts. \nAnd I think bees are sentience candidates,  ",
      "offset": 7509.12,
      "duration": 8.16
    },
    {
      "lang": "en",
      "text": "so naturally I take this risk quite seriously, \nand I think it would be wrong to dismiss it.",
      "offset": 7517.28,
      "duration": 6.8
    },
    {
      "lang": "en",
      "text": "Meghan Barrett on whether brain \nsize and sentience are related",
      "offset": 7525.6,
      "duration": 5.663
    },
    {
      "lang": "en",
      "text": "Luisa Rodriguez: I guess the thing I do \ncare about in the context of this sentience  ",
      "offset": 7531.263,
      "duration": 3.937
    },
    {
      "lang": "en",
      "text": "discussion is brain size. It feels like brain \nsize is at least more likely to be tracking  ",
      "offset": 7535.2,
      "duration": 6.96
    },
    {
      "lang": "en",
      "text": "the thing that I care about. And I guess I \ncare about whether these very large insects  ",
      "offset": 7542.16,
      "duration": 7.44
    },
    {
      "lang": "en",
      "text": "also have very large brains, or if the beetle is \nmostly like other guts, as opposed to brain guts.",
      "offset": 7549.6,
      "duration": 7.744
    },
    {
      "lang": "en",
      "text": "Meghan Barrett: This is a phenomenal question. \nI appreciate you asking it very much, and it’s  ",
      "offset": 7557.344,
      "duration": 3.696
    },
    {
      "lang": "en",
      "text": "very interesting to me. So I have a couple of \npoints to make on the brain size piece of this.  ",
      "offset": 7561.04,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "The first thing I’ll say is just that we should \nask ourselves if more actually always means better  ",
      "offset": 7567.04,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "or more sophisticated. That’s an assumption \nto challenge about our thinking about brains.",
      "offset": 7571.92,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "For example, elephants have more neurons and \nmore brain mass than you and I do, and yet I  ",
      "offset": 7577.04,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "don’t consider them more likely to be sentient \nthan me. Same thing with blue whales. And that’s  ",
      "offset": 7581.44,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "because we know a lot of those additional \nneurons are because they’re just bigger,  ",
      "offset": 7585.6,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "and so they have more mass to control, right? \nThey have more touchpoints they need to be  ",
      "offset": 7588.72,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "able to integrate and things like that. So \nmore doesn’t always mean more sophisticated;  ",
      "offset": 7592.96,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "it can often just mean more of the same repeat \nunit that performs the same kind of function.",
      "offset": 7598.16,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "And that explains, very plausibly I think, this \nphenomenon that we see, where very small brains  ",
      "offset": 7603.76,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "are capable of producing really complex abilities. \nInsects are capable of things like numerical  ",
      "offset": 7609.52,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "cognition, social learning, facial recognition, \ncognitive bias, and much more. And so because of  ",
      "offset": 7615.04,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "this behavioural data and this complexity that we \ninitially thought was just going to be something  ",
      "offset": 7619.92,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "you find in vertebrates, and now we’re seeing \nit in invertebrates with pretty small brains,  ",
      "offset": 7623.76,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "like honeybees, this suggests that maybe \nthose bigger brains aren’t actually better  ",
      "offset": 7628.4,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "or more sophisticated — they just have \nmore redundancies or repeating modules.",
      "offset": 7633.52,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "You could think of this as changing resolution \nor complexity of a capacity, without changing  ",
      "offset": 7637.52,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "the existence of that capacity itself. So \nimagine an image that has more pixels to it:  ",
      "offset": 7642,
      "duration": 6.4
    },
    {
      "lang": "en",
      "text": "it doesn’t change the fact that there’s still a \npicture; it just changes the resolution of that  ",
      "offset": 7648.4,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "picture. So that’s part one to this: that \nfirst we should just challenge that basic  ",
      "offset": 7652.72,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "assumption that bigger brains are necessarily \nbetter at producing a capacity at all.",
      "offset": 7656.24,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "The next thing we need to think about is whether \nor not they are actually that small, which is  ",
      "offset": 7660.88,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "what you were asking initially. So here again, \nI want to give some examples from vertebrates  ",
      "offset": 7664.8,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "and invertebrates to challenge our intuition that \nvertebrates are always bigger than invertebrates.",
      "offset": 7670,
      "duration": 7.44
    },
    {
      "lang": "en",
      "text": "Let’s first consider just the \nmammals, because I think we all  ",
      "offset": 7677.44,
      "duration": 2.4
    },
    {
      "lang": "en",
      "text": "feel really comfortable talking about mammal \nsentience. So the smallest mammal brain that  ",
      "offset": 7679.84,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "we have studied so far weighs in at 64.4 \nmilligrammes. That’s the Etruscan shrew.",
      "offset": 7685.28,
      "duration": 4.863
    },
    {
      "lang": "en",
      "text": "Luisa Rodriguez: That is tiny.",
      "offset": 7690.143,
      "duration": 1.057
    },
    {
      "lang": "en",
      "text": "Meghan Barrett: Shrews can get very small. \nVery, very small. And the body mass of that  ",
      "offset": 7691.2,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "shrew is about two grams or so, give or take. \nSo we get some pretty small mammals out there.",
      "offset": 7694.96,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "When we look at the largest insect brain \nwe’ve studied to date, it’s a solitary wasp,  ",
      "offset": 7700.64,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "and it weighs in at 11.7 milligrammes. That’s \nan insect with a body mass of about 0.5 grams,  ",
      "offset": 7704.56,
      "duration": 6.8
    },
    {
      "lang": "en",
      "text": "so a quarter the size of the shrew. That \nmakes that shrew’s brain just about six  ",
      "offset": 7711.36,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "times larger than the insect brain. \nThat’s a smaller difference than we  ",
      "offset": 7716.64,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "see between humans and whales. So already, \neven if we’re just considering the mammals,  ",
      "offset": 7720.72,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "we see relatively comparable \nbrain masses [with the wasp].",
      "offset": 7725.44,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "Now, that’s just one level of looking at \nthe brain. I want to also extend beyond  ",
      "offset": 7730.88,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "just mammals now and consider other vertebrates \nyou might take seriously — like your lizards,  ",
      "offset": 7734.96,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "say. So our smallest lizard brain that we’ve \nstudied is the Algerian sand gecko. The brain  ",
      "offset": 7739.84,
      "duration": 8.08
    },
    {
      "lang": "en",
      "text": "itself weighs about 10.8 to 11.8 milligrammes. \nSo that is smaller than the wasp at 11.7.",
      "offset": 7747.92,
      "duration": 7.343
    },
    {
      "lang": "en",
      "text": "Luisa Rodriguez: Yeah. Wow.",
      "offset": 7755.263,
      "duration": 0.641
    },
    {
      "lang": "en",
      "text": "Meghan Barrett: So already we’re seeing \nvertebrates close or comparable to the  ",
      "offset": 7755.904,
      "duration": 4.496
    },
    {
      "lang": "en",
      "text": "invertebrates, depending on whether you’re talking \nabout mammals or lizards. So we know we’re already  ",
      "offset": 7760.4,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "in the same ballpark on mass. Now, you might \nnot think that mass is the most relevant metric,  ",
      "offset": 7765.28,
      "duration": 6.56
    },
    {
      "lang": "en",
      "text": "though, for considering the brain. That’s \nbecause we might think that actually things  ",
      "offset": 7771.84,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "like neuron numbers, like the computational \nunits of the brain, are more important.",
      "offset": 7776.32,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "One note I want to make on this is that I’m \ngoing to talk about whole brain numbers today,  ",
      "offset": 7781.36,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "because that’s the only data that we have. But \nI again want to go back to our earlier point  ",
      "offset": 7786,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "that not all neurons do all things: neurons \nare specialised for particular functions;  ",
      "offset": 7789.68,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "they’re organised into \nfunctionally discrete regions.",
      "offset": 7795.04,
      "duration": 2.64
    },
    {
      "lang": "en",
      "text": "For example, wasps have many more optic lobe \nneurons than ants do. Optic lobe neurons are  ",
      "offset": 7797.68,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "the ones that bring in visual information from \nthe periphery. This is probably because wasps  ",
      "offset": 7803.28,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "fly and have to bring in a lot of visual \ninformation really fast. I don’t think that  ",
      "offset": 7807.36,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "all those additional optic lobe neurons are all \nthat sentience-relevant. So if I counted those,  ",
      "offset": 7811.92,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "I would think the wasps were more likely to \nbe sentient [than ants], when really they  ",
      "offset": 7816.64,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "just can process and input more \nvisual information more quickly.",
      "offset": 7819.36,
      "duration": 3.583
    },
    {
      "lang": "en",
      "text": "Luisa Rodriguez: Yes, that makes sense to me.",
      "offset": 7822.943,
      "duration": 1.297
    },
    {
      "lang": "en",
      "text": "Meghan Barrett: So that’s one thing to keep in \nmind: that we don’t have good data on what are  ",
      "offset": 7824.24,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "the sentience-relevant regions of the brain, and \nhow many cells are in those regions. So instead  ",
      "offset": 7828.16,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "we’re just going to talk about total neuron \nnumbers, because that’s the data we’ve got so far.",
      "offset": 7832.16,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "So with that caveat, let’s think about neuron \nnumbers. Let’s go back again to mammals,  ",
      "offset": 7835.6,
      "duration": 6.56
    },
    {
      "lang": "en",
      "text": "because we like starting with mammals, right? \nSo, as far as I know, the smallest number of  ",
      "offset": 7842.16,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "neurons we’ve found in any of the mammals so far \nis in the naked mole-rats, and that’s about 26.88  ",
      "offset": 7847.92,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "million neurons. The smallest studied vertebrate \nbrain is, again, that Algerian sand gecko I  ",
      "offset": 7854.08,
      "duration": 6.8
    },
    {
      "lang": "en",
      "text": "mentioned before. It has about 1.8 million \nneurons. And it turns out that that wasp  ",
      "offset": 7860.88,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "brain I mentioned to you before also contains \n1.8 million neurons. So our gecko and our wasp  ",
      "offset": 7865.04,
      "duration": 6.32
    },
    {
      "lang": "en",
      "text": "are very comparable. They’re both about 15 \ntimes smaller than that of the naked mole-rat.",
      "offset": 7871.36,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "Something interesting to consider here is that \nif we just look across the mammals, that naked  ",
      "offset": 7877.12,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "mole-rat has only one one-thousandth the number \nof neurons of an elephant. So again, there’s a  ",
      "offset": 7881.36,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "lot more difference within the mammals than we’re \nseeing between our smallest mammals and even just  ",
      "offset": 7886.56,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "the invertebrates that we’ve studied so far from \nan insect perspective. So I think, again, we might  ",
      "offset": 7892.4,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "consider just that there’s a lot of reasons to \nbe sceptical that insects have an unusually small  ",
      "offset": 7898.88,
      "duration": 7.04
    },
    {
      "lang": "en",
      "text": "number of neurons compared even to the mammals, \nmuch less the vertebrate lizards and snakes.",
      "offset": 7905.92,
      "duration": 6.943
    },
    {
      "lang": "en",
      "text": "Luisa Rodriguez: Yeah, interesting. I think \nI learned about 1,000 new facts there.",
      "offset": 7912.863,
      "duration": 9.297
    },
    {
      "lang": "en",
      "text": "Lewis Bollard on how animal advocacy has \nchanged in response to sentience studies",
      "offset": 7922.16,
      "duration": 7.103
    },
    {
      "lang": "en",
      "text": "Luisa Rodriguez: I was wondering if you’d be \nup for sharing some reflections from your time  ",
      "offset": 7929.263,
      "duration": 2.897
    },
    {
      "lang": "en",
      "text": "as programme officer at Open Phil. I think \nyou’ve now been doing this since 2015-ish,  ",
      "offset": 7932.16,
      "duration": 6.72
    },
    {
      "lang": "en",
      "text": "and you were also in the farmed animal \nwelfare space even before that. Is there  ",
      "offset": 7938.88,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "something that you’ve changed your views \non since you started doing this work?",
      "offset": 7943.12,
      "duration": 5.585
    },
    {
      "lang": "en",
      "text": "Lewis Bollard: I’ve become a lot more worried \nabout invertebrates. When I started this work,  ",
      "offset": 7948.705,
      "duration": 4.255
    },
    {
      "lang": "en",
      "text": "I — like many of us — just ignored them. \nI think I sort of quietly assumed that  ",
      "offset": 7952.96,
      "duration": 6.88
    },
    {
      "lang": "en",
      "text": "they weren’t sentient. And then I remember, \nsomeone asked me back in 2016, and I said,  ",
      "offset": 7959.84,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "maybe there’s like 10% chance that \nthey were sentient — and that was  ",
      "offset": 7964.96,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "enough to make me worry a bit, \nbut it really wasn’t that high.",
      "offset": 7967.76,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "Since then, thanks in large part to the work \nthat Rethink Priorities did with their Moral  ",
      "offset": 7971.92,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "Weight Sequence, I have really come to see a \nvery high probability that invertebrates are  ",
      "offset": 7976.96,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "sentient in some meaningful sense, \nand that their welfare matters.",
      "offset": 7982.88,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "Luisa Rodriguez: Yeah, that makes sense. We \nactually just interviewed Bob Fischer about  ",
      "offset": 7987.2,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "the Moral Weight Project, so listeners \nmight have heard that. I’m curious if  ",
      "offset": 7991.36,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "you remember any of the particular facts \nor research that felt compelling to you,  ",
      "offset": 7997.36,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "and convinced you that invertebrates are \nmore likely to be sentient than you thought?",
      "offset": 8003.84,
      "duration": 4.225
    },
    {
      "lang": "en",
      "text": "Lewis Bollard: I think it was more of \nthe absence of contrary facts. I mean,  ",
      "offset": 8008.065,
      "duration": 3.375
    },
    {
      "lang": "en",
      "text": "I had just assumed that because society \nacts as if insects aren’t sentient and  ",
      "offset": 8011.44,
      "duration": 6.8
    },
    {
      "lang": "en",
      "text": "shrimp aren’t sentient, that there \nmust be good evidence for that.  ",
      "offset": 8018.24,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "And I was really surprised when they started \nlooking into this and there just wasn’t.",
      "offset": 8021.28,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "On the flip side, there was \nevidence to worry. And for me,  ",
      "offset": 8025.92,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "the most compelling one is actually just \nthe evolutionary reason, which is that it  ",
      "offset": 8029.44,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "just does seem like an animal who has the \ncapacity to move and the capacity to learn,  ",
      "offset": 8033.76,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "there are reasons, unfortunately, for it \nto have the capacity to feel pain, too.",
      "offset": 8038.8,
      "duration": 3.423
    },
    {
      "lang": "en",
      "text": "Luisa Rodriguez: Yes, I feel the same \nway. Speaking of the Moral Weight Project,  ",
      "offset": 8042.223,
      "duration": 3.537
    },
    {
      "lang": "en",
      "text": "back in 2017, Rob Wiblin asked you if we \nhad any kind of quantitative measure that  ",
      "offset": 8045.76,
      "duration": 7.52
    },
    {
      "lang": "en",
      "text": "you can use to compare animal suffering to human \nsuffering. And I think you said something like,  ",
      "offset": 8053.28,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "“That’d be great, but no — our ability \nto understand the relative experiences  ",
      "offset": 8057.52,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "of different species is still really limited.”",
      "offset": 8061.44,
      "duration": 2.48
    },
    {
      "lang": "en",
      "text": "We’ve both alluded to this work a few times \nalready, but just to give a bit more context:  ",
      "offset": 8064.48,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "Bob Fischer and his colleagues looked at \na bunch of different physiological and  ",
      "offset": 8069.12,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "behavioural and cognitive traits \nin different animals, and then,  ",
      "offset": 8073.12,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "based on how many traits a given animal had, \nthey gave a rough estimate of how the capacity  ",
      "offset": 8076.56,
      "duration": 6.96
    },
    {
      "lang": "en",
      "text": "for pain and pleasure of a chicken or cow \nor fruit fly compares to that of a human.",
      "offset": 8083.52,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "And I found the results surprising. In \ngeneral, they were very animal friendly.  ",
      "offset": 8088.96,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "For example, they concluded that their best \nguess was that a chicken has something like  ",
      "offset": 8096,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "a third of the capacity for pain and pleasure \nas a human — which can imply some things that  ",
      "offset": 8101.2,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "feel very strange about the kinds of tradeoffs \nyou might make, for example, if you were doing a  ",
      "offset": 8107.36,
      "duration": 6.56
    },
    {
      "lang": "en",
      "text": "trolley problem with chickens and humans. But I’m \ncurious what your reactions were to their results?",
      "offset": 8113.92,
      "duration": 6.8
    },
    {
      "lang": "en",
      "text": "Lewis Bollard: Yeah, I found them \nreally interesting. And I agree:  ",
      "offset": 8120.72,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "for most people, they’re very counterintuitive.",
      "offset": 8123.68,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "Two things I’d say. One is to understand they’re \njust looking at that capacity for suffering,  ",
      "offset": 8127.44,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "and so there might be other reasons why you \nchoose to prefer humans. I mean, for one thing,  ",
      "offset": 8132.8,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "we have much longer lifespans, so I’d save \na human over a chicken because they have  ",
      "offset": 8136.8,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "many more years to live. But also, you might \nthink that they have more other more meaningful  ",
      "offset": 8140.24,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "things — that there are social networks who are \ngoing to be sad about losing them, and so on.",
      "offset": 8144.8,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "The second thing is, I would encourage people to \nreally approach this with a fresh mind and ask,  ",
      "offset": 8148.72,
      "duration": 7.12
    },
    {
      "lang": "en",
      "text": "why do I find this so counterintuitive? I \nthink we have such an ingrained hierarchy  ",
      "offset": 8156.48,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "in our minds of animals — where, of course, \nhumans are at the top, and every other animal  ",
      "offset": 8162,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "is below us — and we start out from that \nplace and then we sort of update from there.",
      "offset": 8166.32,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "And if you tried instead to start from more of a \nblank slate, where you just look at the different  ",
      "offset": 8172.24,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "capacities of these animals, and you don’t assume \nanything, then I think you end up more likely at  ",
      "offset": 8177.44,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "these more equal numbers. Or if you don’t, I think \nit’s because you make some unusual philosophical  ",
      "offset": 8182.88,
      "duration": 7.36
    },
    {
      "lang": "en",
      "text": "turns. And then I would just ask, are you happy \nwith where those philosophical turns take you?",
      "offset": 8190.24,
      "duration": 7.103
    },
    {
      "lang": "en",
      "text": "Luisa Rodriguez: Yeah. “Do you endorse having \nthe arbitrary view that only one species matters,  ",
      "offset": 8197.342,
      "duration": 6.177
    },
    {
      "lang": "en",
      "text": "or only things that are kind of like \nyou matter?” Those seem unpalatable.",
      "offset": 8203.52,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "I guess when I think about it, if I’m like, what \nis making me have this gut reaction that’s like,  ",
      "offset": 8208.72,
      "duration": 6.88
    },
    {
      "lang": "en",
      "text": "“No, surely not; surely there are bigger \ndifferences between these species”? It’s  ",
      "offset": 8215.6,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "not like I have evidence. It’s not like \nI’m like, “Once I saw a dog kicked,  ",
      "offset": 8221.68,
      "duration": 7.36
    },
    {
      "lang": "en",
      "text": "and it didn’t seem upset,” or it’s certainly \nnot like I know anything about the science  ",
      "offset": 8229.04,
      "duration": 6.319
    },
    {
      "lang": "en",
      "text": "of pain and how it presents or doesn’t \npresent in different animals’ brains  ",
      "offset": 8235.359,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "or something. It’s nothing. It feels very \nsociological — like you said, this hierarchy.",
      "offset": 8241.6,
      "duration": 7.601
    },
    {
      "lang": "en",
      "text": "And if I really try to think, what evidence \ndo I have, without kind of looking into it,  ",
      "offset": 8249.2,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "it’s really just like, “Well, I’m a being in \nthe world. They’re also beings in the world.  ",
      "offset": 8254.24,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "And maybe we should just actually think \nthat we’re all really similar, because  ",
      "offset": 8260,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "the world is hard and scary, and we have to have \nmechanisms that keep us alive and reproducing.”  ",
      "offset": 8264.96,
      "duration": 7.92
    },
    {
      "lang": "en",
      "text": "So if you start from there, then we’re \nactually on a really similar point.",
      "offset": 8272.88,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "And I found that a really helpful \nway to pump my intuitions about  ",
      "offset": 8275.92,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "where actually should we be starting? Is it with \nthese huge gaps between humans and insects, or  ",
      "offset": 8280.8,
      "duration": 9.2
    },
    {
      "lang": "en",
      "text": "is that just completely out of nowhere?",
      "offset": 8290,
      "duration": 2.545
    },
    {
      "lang": "en",
      "text": "Lewis Bollard: Yeah. One other \nthought I’d have on that is that I  ",
      "offset": 8292.545,
      "duration": 2.495
    },
    {
      "lang": "en",
      "text": "think it can be helpful thinking of the most \ncharismatic animal of a species or class.",
      "offset": 8295.04,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "So rather than thinking, what’s the moral \nweight of a chicken? — which just seems,  ",
      "offset": 8300.56,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "I think intuitively for many people, \nnot worth much — think of what’s the  ",
      "offset": 8304.56,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "moral weight of a bald eagle, and take out the \npreservation value or something. But just say,  ",
      "offset": 8307.68,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "for one thing, there aren’t that many of them. So \nyou don’t have this initial intuition of, oh god,  ",
      "offset": 8314.24,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "if I give them a lot of moral weight, \nthey’re going to trump everything else;  ",
      "offset": 8318.399,
      "duration": 2.561
    },
    {
      "lang": "en",
      "text": "they’re going to swamp everything. And \nyou also probably have a pretty positive  ",
      "offset": 8320.96,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "impression of them: their complexity, \ntheir grandeur, and everything.",
      "offset": 8324.88,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "Or even within insects, I would say rather \nthan a fruit fly, think about a bumblebee. Now,  ",
      "offset": 8329.52,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "it’s possible that a bumblebee has more \nsophisticated capacity, so I’m not saying  ",
      "offset": 8334.16,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "to skew it for that reason, but I think if \nyou think about the more charismatic animals,  ",
      "offset": 8338,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "there can at least be an intuition pump to \nthink, “Is what I’m doing here just choosing  ",
      "offset": 8342.96,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "animals that I don’t like, and thinking \nthey can’t possibly be worth very much?”",
      "offset": 8348.16,
      "duration": 4.063
    },
    {
      "lang": "en",
      "text": "Luisa Rodriguez: “Nah. They’re ugly,  ",
      "offset": 8352.223,
      "duration": 0.657
    },
    {
      "lang": "en",
      "text": "they give me the creeps. Yeah, they probably \ndon’t feel anything.” Those things are related.",
      "offset": 8352.88,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "Lewis Bollard: Right.",
      "offset": 8358.479,
      "duration": 1.081
    },
    {
      "lang": "en",
      "text": "Luisa Rodriguez: The other one that Meghan Barrett \ngave me was, if I think about the biggest insect  ",
      "offset": 8359.56,
      "duration": 7.959
    },
    {
      "lang": "en",
      "text": "I can think of — and she actually told me about \nsome insects of certain sizes I just didn’t even  ",
      "offset": 8367.52,
      "duration": 5.521
    },
    {
      "lang": "en",
      "text": "know existed; they’re just way bigger than I \nrealised — if a beetle is the size of a mouse,  ",
      "offset": 8373.04,
      "duration": 6.399
    },
    {
      "lang": "en",
      "text": "all of a sudden my brain’s like, “Oh, \nthat could be as smart as a mouse then.”",
      "offset": 8379.439,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "So yeah, there’s just clearly some size bias thing \n— which maybe there is something going on there,  ",
      "offset": 8384,
      "duration": 8
    },
    {
      "lang": "en",
      "text": "something about neurons and neurons being \nmore plentiful could have something to do  ",
      "offset": 8392,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "with capacity for experience — but it isn’t \nthe end-all and be-all. So the fact that we  ",
      "offset": 8396.96,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "have these really intense intuitions about \nsize seems like we should be suspicious.",
      "offset": 8402.16,
      "duration": 6.065
    },
    {
      "lang": "en",
      "text": "Lewis Bollard: Yeah, that’s a good way to put \nit. I mean, you can think of a lobster. My sense  ",
      "offset": 8408.225,
      "duration": 3.615
    },
    {
      "lang": "en",
      "text": "is that humans intuitively care more about \na lobster than an insect. My understanding  ",
      "offset": 8411.84,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "is their brains are relatively similar in terms \nof neuron count, in terms of a lot of features,  ",
      "offset": 8416.16,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "and that really is just a size difference. \nSimilarly, elephants are very smart animals,  ",
      "offset": 8420.24,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "but they’re not that much smarter than other \nmammals. But I think we really have that  ",
      "offset": 8425.12,
      "duration": 5.279
    },
    {
      "lang": "en",
      "text": "sense that, wow, they deserve protection. So I \nthink that’s right. The size bias is very real.",
      "offset": 8430.399,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "Luisa Rodriguez: Yeah. If I remember correctly,  ",
      "offset": 8436.479,
      "duration": 2.08
    },
    {
      "lang": "en",
      "text": "ants have more neurons than crabs. And \nthat’s another one where I’m like, yeah,  ",
      "offset": 8438.56,
      "duration": 6.4
    },
    {
      "lang": "en",
      "text": "crabs I can get on board with. I can \nget on board with caring about those.",
      "offset": 8444.96,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "OK, so yeah, there’s this work. It’s got some \npretty counterintuitive results. Has it changed  ",
      "offset": 8450.16,
      "duration": 6.159
    },
    {
      "lang": "en",
      "text": "the way you think about prioritising \nbetween different interventions,  ",
      "offset": 8456.319,
      "duration": 2.801
    },
    {
      "lang": "en",
      "text": "besides maybe putting more \nweight on invertebrates?",
      "offset": 8459.12,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "Lewis Bollard: Yeah, it’s definitely led \nus to put more weight on invertebrates.  ",
      "offset": 8462.64,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "I think at the same time, it’s always \na tradeoff between the importance of a  ",
      "offset": 8466.64,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "species and the tractability of \nwork on that — and I think that  ",
      "offset": 8470.88,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "we’ve had a greater track record of \ntractable work on chickens and fish.",
      "offset": 8475.6,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "I think there’s also a thing of how far \ndoes the Overton window go? How far can  ",
      "offset": 8480.16,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "you get people to understand things? And I \nthink there’s a risk that if our movement  ",
      "offset": 8484.8,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "just became an insect welfare movement, \nthat, for a lot of people, would be a  ",
      "offset": 8488.96,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "reductio ad absurdum. That would be, “Well, if \ninsects count too, then none of this matters.”",
      "offset": 8493.28,
      "duration": 6.319
    },
    {
      "lang": "en",
      "text": "I think, in a way, that it does make more sense \nfor our movement to bring people along with us  ",
      "offset": 8500.399,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "and to focus on species… Not to just wait where \npeople are currently — you definitely want to  ",
      "offset": 8506.56,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "lead people — but I think to lead people \nmore slowly, and also to work on a variety  ",
      "offset": 8511.12,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "of issues and species. So you have a \ngreater array of shots at progress.",
      "offset": 8516.8,
      "duration": 5.32
    },
    {
      "lang": "en",
      "text": "Luisa Rodriguez: Right. Yeah. It is just the \ncase that even if we have more evidence than we  ",
      "offset": 8522.12,
      "duration": 4.36
    },
    {
      "lang": "en",
      "text": "thought that insects feel pain, it seems like \nwe’re still really far away from knowing with  ",
      "offset": 8526.479,
      "duration": 6.641
    },
    {
      "lang": "en",
      "text": "any confidence about what any of these animals \nare feeling really concretely and confidently.  ",
      "offset": 8533.12,
      "duration": 6.239
    },
    {
      "lang": "en",
      "text": "So diversifying seems at least a plausible \napproach to dealing with that uncertainty.",
      "offset": 8540.08,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "Bob Fischer on using proxies \nto determine sentience",
      "offset": 8547.6,
      "duration": 5.504
    },
    {
      "lang": "en",
      "text": "Luisa Rodriguez: A colleague of yours has written \nthis report on why neuron counts aren’t actually  ",
      "offset": 8553.103,
      "duration": 3.456
    },
    {
      "lang": "en",
      "text": "a good proxy for what we care about here. Can \nyou give a quick summary of why they think that?",
      "offset": 8556.56,
      "duration": 6.947
    },
    {
      "lang": "en",
      "text": "Bob Fischer: Sure. There are two things to \nsay. One is that it isn’t totally crazy to  ",
      "offset": 8563.507,
      "duration": 4.333
    },
    {
      "lang": "en",
      "text": "use neuron counts. One way of seeing why you \nmight think it’s not totally crazy is to think  ",
      "offset": 8567.84,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "about the kinds of proxies that economists \nhave used when trying to estimate human  ",
      "offset": 8573.68,
      "duration": 5.521
    },
    {
      "lang": "en",
      "text": "welfare. Economists have for a long time \nused income as a proxy for human welfare.",
      "offset": 8579.2,
      "duration": 7.84
    },
    {
      "lang": "en",
      "text": "You might say that we know that there \nare all these ways in which that fails  ",
      "offset": 8587.04,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "as a proxy — and the right response \nfrom the economist is something like,  ",
      "offset": 8591.12,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "“Do you have anything better? \nWhere there’s actually data,  ",
      "offset": 8597.04,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "and where we can answer at least some of these \nhigh-level questions that we care about? Or at  ",
      "offset": 8599.92,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "least make progress on the high-level questions \nthat we care about relative to baseline?”",
      "offset": 8606.08,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "And I think that way of thinking about \nwhat neuron-count-based proxies are  ",
      "offset": 8610.64,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "is the charitable interpretation. It’s \njust like income in welfare economics:  ",
      "offset": 8615.68,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "imperfect, but maybe the best we \ncan do in certain circumstances.",
      "offset": 8622.319,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "That being said, the main problem is that there \nare lots of factors that really affect neuron  ",
      "offset": 8626.24,
      "duration": 6.72
    },
    {
      "lang": "en",
      "text": "count as a proxy that make it problematic. \nOne is that neuron counts alone are really  ",
      "offset": 8632.96,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "sensitive to body size, so that’s going \nto be a confounding factor. It seems like,  ",
      "offset": 8638.88,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "insofar as it tracks much of anything, it might \nbe tracking something like intelligence — and  ",
      "offset": 8644.72,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "it’s not totally obvious why intelligence is \nmorally important. At least in the human case,  ",
      "offset": 8650,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "we often think that it’s not important, \nand in fact, it’s a really pernicious thing  ",
      "offset": 8654.64,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "to make intelligence the metric \nby which we assess moral value.",
      "offset": 8659.12,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "And then, even if you think that neuron counts \nare proxies of some quality for something else,  ",
      "offset": 8663.04,
      "duration": 7.359
    },
    {
      "lang": "en",
      "text": "like the intensity of pain states or \nsomething — it’s not clear that that’s true,  ",
      "offset": 8670.399,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "but even if that were true — you’d still \nhave to ask, can we do any better? And  ",
      "offset": 8674.08,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "it’s not obvious that we can’t do better. Not \nobvious that we can, but we should at least try.",
      "offset": 8678.64,
      "duration": 5.183
    },
    {
      "lang": "en",
      "text": "Luisa Rodriguez: Yes. Makes sense. Are there \nany helpful thought experiments there? It  ",
      "offset": 8683.823,
      "duration": 5.057
    },
    {
      "lang": "en",
      "text": "doesn’t seem at all insane to me — though \nmaybe you wouldn’t expect it to happen on  ",
      "offset": 8688.88,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "its own through evolution — that there would be \na being who has many fewer neurons than I do,  ",
      "offset": 8694.8,
      "duration": 8.56
    },
    {
      "lang": "en",
      "text": "but that those neurons are primarily directed \nat going from extreme pain to extreme something  ",
      "offset": 8703.359,
      "duration": 8.4
    },
    {
      "lang": "en",
      "text": "like euphoria. It doesn’t seem like there’s \na good reason that’s not possible, and that  ",
      "offset": 8711.76,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "that extreme pain could just be much more than \nthe total amount of pain I could possibly feel.",
      "offset": 8716.88,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "Even though the types of pain might \nbe different for me — because I’ve  ",
      "offset": 8722.08,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "got different kinds of capacities for \nsadness and shame and embarrassment,  ",
      "offset": 8725.6,
      "duration": 7.681
    },
    {
      "lang": "en",
      "text": "like a wider variety of types of pain — it \nstill seems at least theoretically possible  ",
      "offset": 8733.28,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "that you could house a bunch of pain in a small \nbrain. And that feels like good reason to me to  ",
      "offset": 8738.88,
      "duration": 7.2
    },
    {
      "lang": "en",
      "text": "basically do what you’ve done, which is \nlook for better ways than neurons alone.",
      "offset": 8747.92,
      "duration": 4.547
    },
    {
      "lang": "en",
      "text": "Bob Fischer: Sure. And some evolutionary \nbiologists have basically said things along  ",
      "offset": 8752.466,
      "duration": 3.614
    },
    {
      "lang": "en",
      "text": "these lines. Richard Dawkins actually has this \nline at some point, where he says that maybe  ",
      "offset": 8756.08,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "simpler organisms actually need stronger pain \nsignals because they don’t learn as much as  ",
      "offset": 8762.24,
      "duration": 6.239
    },
    {
      "lang": "en",
      "text": "we do and they don’t remember all these facts, \nso they need big alarm bells to keep them away  ",
      "offset": 8768.479,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "from fitness-reducing threats. So it’s always \npossible that you have a complete inversion  ",
      "offset": 8773.84,
      "duration": 7.36
    },
    {
      "lang": "en",
      "text": "of the relationship that people imagine, and you \nwant to make sure that your model captures that.",
      "offset": 8781.2,
      "duration": 8
    },
    {
      "lang": "en",
      "text": "Cameron Meyer Shorb on how we can practically \nstudy wild animals’ subjective experiences",
      "offset": 8789.2,
      "duration": 7.503
    },
    {
      "lang": "en",
      "text": "Luisa Rodriguez: What do we do \nwith all of that uncertainty?  ",
      "offset": 8796.703,
      "duration": 2.337
    },
    {
      "lang": "en",
      "text": "Do we have any way of knowing whether \na rabbit’s experience of grazing in  ",
      "offset": 8799.04,
      "duration": 5.279
    },
    {
      "lang": "en",
      "text": "a field is more like terror or more \nlike leisure? It just seems so hard…",
      "offset": 8804.319,
      "duration": 5.979
    },
    {
      "lang": "en",
      "text": "Cameron Meyer Shorb: Right. So I \nthink those are answerable questions,  ",
      "offset": 8810.298,
      "duration": 3.141
    },
    {
      "lang": "en",
      "text": "is the bottom line. Sentience is a very hard \nthing to really concretely understand. You  ",
      "offset": 8813.439,
      "duration": 9.36
    },
    {
      "lang": "en",
      "text": "can’t really measure the thing itself, \nand there’s all these uncertainties. But  ",
      "offset": 8822.8,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "at the end of the day, we do have babies, we \ndo have dogs: they don’t know how to talk,  ",
      "offset": 8827.28,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "but we do a pretty good job \nkeeping them happy, right? So  ",
      "offset": 8831.6,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "I think that’s proof of concept that you can use \nindicators of welfare to make some good decisions,  ",
      "offset": 8837.28,
      "duration": 7.199
    },
    {
      "lang": "en",
      "text": "at least about the biggest things, the biggest \nsources of happiness or suffering or whatever.",
      "offset": 8844.479,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "With respect to questions like, Are \nanimals happy when they’re grazing?  ",
      "offset": 8849.92,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "Are animals afraid of predators? I do think \nthat these are questions that are going to  ",
      "offset": 8853.84,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "have to be answered for each species \nor each group of animals in its own  ",
      "offset": 8858.08,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "context. There just is a lot of diversity \nof life on Earth, and this is part of the  ",
      "offset": 8862.96,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "project of wild animal welfare science \nis learning to listen to everyone else.",
      "offset": 8867.12,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "But the tools we have to ask those kinds \nof questions are somewhat generalisable,  ",
      "offset": 8871.52,
      "duration": 5.601
    },
    {
      "lang": "en",
      "text": "and I divide those into the physical indicators of \nwelfare and the behavioural indicators of welfare.",
      "offset": 8877.12,
      "duration": 6.399
    },
    {
      "lang": "en",
      "text": "Physical indicators would be things like \nlooking at the condition of the body. You  ",
      "offset": 8884.16,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "can also look at their external appearance: \ndo they seem to have injuries or disease?",
      "offset": 8888.72,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "You can look at neurotransmitters \nor glucocorticoids (commonly called  ",
      "offset": 8893.52,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "stress hormones) in the blood, \nother hormones, body temperature.  ",
      "offset": 8898.16,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "So the effects of our mental states are \nmanifested in many ways in our bodies,  ",
      "offset": 8903.04,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "and by measuring those, we can get some useful \ninformation on animals’ interior states.",
      "offset": 8908.88,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "And then the final set of physical indicators \nis genetic indicators: looking at things like  ",
      "offset": 8914.8,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "biomarkers of ageing can be used as proxies for \nthe cumulative physiological stress that an animal  ",
      "offset": 8920.88,
      "duration": 8
    },
    {
      "lang": "en",
      "text": "has undergone over their lifetime. We’ve done some \nwork to validate this, and there’s more work that  ",
      "offset": 8928.88,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "needs to be done. But this is the kind of thing \nthat might be broadly useful across many kinds  ",
      "offset": 8933.439,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "of animals, right? Because that basic structure \nof DNA is preserved across all of animal life.",
      "offset": 8938.88,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "And then of course, there are \nthe behavioural indicators,  ",
      "offset": 8944.88,
      "duration": 2.64
    },
    {
      "lang": "en",
      "text": "which are the ones we use for our children \nand pets and others — looking at things like  ",
      "offset": 8947.52,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "vocalisations, activity, are they inactive, \nwhat sort of posture are they holding,  ",
      "offset": 8952.399,
      "duration": 5.681
    },
    {
      "lang": "en",
      "text": "are they engaging in activities like \nplay or showing fearful behaviour?",
      "offset": 8958.08,
      "duration": 5.359
    },
    {
      "lang": "en",
      "text": "Then there are these cognition \nor decision-relevant behavioural  ",
      "offset": 8963.439,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "indicators. You can actually set \nup experimental tests to see which  ",
      "offset": 8968.08,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "things animals prefer. This has been done for \nchickens to develop some of the information  ",
      "offset": 8972.96,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "we use to inform what conditions they \nprefer in factory farming contexts.",
      "offset": 8978.24,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "You can also do the same with \nwild animals. As you can imagine,  ",
      "offset": 8982.479,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "setting up these choice experiments in the \nwild is tricky, but not always impossible,  ",
      "offset": 8986.64,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "and you can set up these forced-choice \nexperiments to see which things animals  ",
      "offset": 8991.84,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "prefer that can sort of inform which of \nthese things is better for the animal.",
      "offset": 8997.12,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "You can also use them to assess what mental \nstate is the animal in right now. For example,  ",
      "offset": 9000.96,
      "duration": 7.359
    },
    {
      "lang": "en",
      "text": "you can measure whether they seem to be \nshowing a certain amount of cognitive bias,  ",
      "offset": 9008.319,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "like pessimism. The hypothesis is that if \nthe animal makes fewer efforts to look for  ",
      "offset": 9013.52,
      "duration": 7.12
    },
    {
      "lang": "en",
      "text": "food in areas where it had previously \nbeen trained there might be food,  ",
      "offset": 9020.64,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "that might demonstrate pessimism, and that \nmight indicate a just overall negative affect.",
      "offset": 9024.24,
      "duration": 6.399
    },
    {
      "lang": "en",
      "text": "As you can see, there are a lot of assumptions \nthat underlie a lot of these things,  ",
      "offset": 9030.64,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "and no single method is perfect. Our \ngeneral advice to researchers is: one,  ",
      "offset": 9035.439,
      "duration": 7.2
    },
    {
      "lang": "en",
      "text": "there’s no silver bullet; you want \nto use multiple different methods,  ",
      "offset": 9042.64,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "ideally of different types — so \nsome physical, some behavioural.",
      "offset": 9047.12,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "And then there are also, among these methods, \nsome that we have more confidence in than others.  ",
      "offset": 9051.439,
      "duration": 5.521
    },
    {
      "lang": "en",
      "text": "For that reason, it’s helpful to design \nexperiments in a way where you have at  ",
      "offset": 9058.56,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "least one metric that has more evidence, that we \nhave higher confidence in, and at least one metric  ",
      "offset": 9062.479,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "that we have less confidence in — and we can then \nuse those results to start building confidence  ",
      "offset": 9068.72,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "in those other metrics. So trying to see to what \nextent do these things correlate with each other.",
      "offset": 9073.84,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "And it’s all fuzzy; it’s kind of cloudy \nand never totally certain. But through  ",
      "offset": 9079.28,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "this process of iterating and using \nthese measurements in different species,  ",
      "offset": 9084.479,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "in different contexts, and seeing the \nrelationship between the measurements,  ",
      "offset": 9087.92,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "we’re slowly getting a better and better \nidea of how to interpret these things.",
      "offset": 9091.359,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "Luisa Rodriguez: That’s super cool. I think \npart of me has this worry that there are so  ",
      "offset": 9096.8,
      "duration": 10.08
    },
    {
      "lang": "en",
      "text": "many assumptions. And in some species, I can \nreally get behind that cortisol really does  ",
      "offset": 9106.88,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "probably mean the same thing, or a very similar \nthing, in chimps as it does in humans. Then the  ",
      "offset": 9111.76,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "farther you get on the evolutionary tree of life \nfrom humans, I feel more worried and uncertain.",
      "offset": 9117.52,
      "duration": 6.641
    },
    {
      "lang": "en",
      "text": "But I do just feel pretty good about a process \nthat uses tonnes of different indicators. I’m  ",
      "offset": 9124.16,
      "duration": 6.159
    },
    {
      "lang": "en",
      "text": "sure that takes into account what we know \nabout different species. For insects,  ",
      "offset": 9130.319,
      "duration": 5.521
    },
    {
      "lang": "en",
      "text": "there will probably have to be a different set \nof benchmarks, but we’ll be taking those into  ",
      "offset": 9135.84,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "account. And by thinking carefully and having \na very diverse set of indicators, I can just  ",
      "offset": 9139.76,
      "duration": 8.4
    },
    {
      "lang": "en",
      "text": "imagine being like, they’re in a situation \nthat seems like it might be harmful to them:  ",
      "offset": 9148.16,
      "duration": 6.88
    },
    {
      "lang": "en",
      "text": "they have physiological markers of distress, \nthey’ve got cortisol, they’ve got behaviours  ",
      "offset": 9155.04,
      "duration": 6.8
    },
    {
      "lang": "en",
      "text": "that seem like distress. And just like the \nwhole picture is one where it gets really hard  ",
      "offset": 9161.84,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "to imagine that all of those things are true \nand that that animal is not feeling stress.",
      "offset": 9167.84,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "Am I feeling too optimistic about this, or do \nyou feel like that kind of optimism is justified?",
      "offset": 9173.28,
      "duration": 7.899
    },
    {
      "lang": "en",
      "text": "Cameron Meyer Shorb: I think \nthat optimism is justified,  ",
      "offset": 9181.179,
      "duration": 2.26
    },
    {
      "lang": "en",
      "text": "because what you were describing here, \nyou’re optimistic about our ability to  ",
      "offset": 9183.439,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "at least eventually understand questions \nabout welfare by using metrics like these.",
      "offset": 9188.16,
      "duration": 9.04
    },
    {
      "lang": "en",
      "text": "Jeff Sebo on the problem of false \npositives in assessing artificial sentience",
      "offset": 9197.2,
      "duration": 7.108
    },
    {
      "lang": "en",
      "text": "Jeff Sebo: I think that there are a lot of \ntrends pointing in different directions,  ",
      "offset": 9204.308,
      "duration": 4.011
    },
    {
      "lang": "en",
      "text": "and there are a lot of similarities, \nas well as a lot of differences,  ",
      "offset": 9208.319,
      "duration": 2.801
    },
    {
      "lang": "en",
      "text": "between oppression of fellow humans, and then \noppression of other animals, and then potential  ",
      "offset": 9211.12,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "oppression of sentient or otherwise significant \nAI systems that might exist in the future.",
      "offset": 9216.8,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "Some of the signs might be encouraging. \nLike humans, and unlike other animals,  ",
      "offset": 9223.04,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "AI systems might be able to express \ntheir desires and preferences in  ",
      "offset": 9229.04,
      "duration": 5.279
    },
    {
      "lang": "en",
      "text": "language that we can more easily understand. \nActually, with the assistance of AI systems,  ",
      "offset": 9234.319,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "nonhuman animals might soon be able to do \nthat too, which would be wonderful. However,  ",
      "offset": 9238.8,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "we are already doing a good job at programming \nAI systems in a way that prevents them from  ",
      "offset": 9243.6,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "being able to talk about their potential \nconsciousness or sentience or sapience,  ",
      "offset": 9249.76,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "because that kind of communication is unsettling \nor will potentially lead to false positives.",
      "offset": 9254.16,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "And there are going to be a lot of AI systems \nthat might not take the form of communicators  ",
      "offset": 9260.24,
      "duration": 7.04
    },
    {
      "lang": "en",
      "text": "at all. It can be easy to focus on large \nlanguage models, who do communicate with us,  ",
      "offset": 9267.28,
      "duration": 5.359
    },
    {
      "lang": "en",
      "text": "and digital assistants or chatbots that might \nbe based on large language models. But there  ",
      "offset": 9272.64,
      "duration": 4.454
    },
    {
      "lang": "en",
      "text": "are going to be radically different kinds of \nAI systems that we might not even be able to  ",
      "offset": 9277.093,
      "duration": 4.987
    },
    {
      "lang": "en",
      "text": "process as minded beings in the same ways that we \ncan with ones who more closely resemble humans.",
      "offset": 9282.08,
      "duration": 6.72
    },
    {
      "lang": "en",
      "text": "So I think that there might be some cases where we \ncan be a little bit better equipped to take their  ",
      "offset": 9288.8,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "potential significance seriously, but then some \ncases where we might be worse equipped to take  ",
      "offset": 9294.319,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "their potential significance seriously. And then \nas our uses of them continue, our incentives to  ",
      "offset": 9299.04,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "look the other way will increase, so there \nwill be a bunch of shifting targets here.",
      "offset": 9303.6,
      "duration": 5.423
    },
    {
      "lang": "en",
      "text": "Luisa Rodriguez: Yeah, that makes a bunch of \nsense to me. I guess it’s also possible that,  ",
      "offset": 9309.023,
      "duration": 3.137
    },
    {
      "lang": "en",
      "text": "given the things we’ve already seen — like LaMDA, \nand how that was kind of bad PR for the companies  ",
      "offset": 9312.16,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "creating these LLMs — there might be some \nincentive for them to train models not to express  ",
      "offset": 9318.24,
      "duration": 7.92
    },
    {
      "lang": "en",
      "text": "that kind of thought. And maybe that pressure will \nactually be quite strong, such that they really,  ",
      "offset": 9326.16,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "really just are very unlikely to say, even \nif they’ve got all sorts of things going on.",
      "offset": 9331.84,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "Jeff Sebo: Well, there definitely not only is \nthat incentive, but also that policy in place  ",
      "offset": 9336.16,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "at AI companies, it seems. A year or two ago, you \nmight have been able to ask a chatbot if they are  ",
      "offset": 9340.8,
      "duration": 6.32
    },
    {
      "lang": "en",
      "text": "conscious or sentient or a person or a rights \nholder, and they would answer in whatever way  ",
      "offset": 9347.12,
      "duration": 7.279
    },
    {
      "lang": "en",
      "text": "seemed appropriate to them, in whatever way seemed \nlike the right prediction. So if prompted in the  ",
      "offset": 9354.399,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "right way, they might say, “I am conscious,” \nor they might say, “I am not conscious.”",
      "offset": 9359.52,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "But now if you ask many of these models, they \nwill say, “As a large language model, I am not  ",
      "offset": 9362.96,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "conscious” or “I am not able to talk about this \ntopic.” They have clearly been programmed to  ",
      "offset": 9367.84,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "avoid what the companies see as false positives \nabout consciousness and sentience and personhood.",
      "offset": 9374.08,
      "duration": 7.6
    },
    {
      "lang": "en",
      "text": "And I do think that trend will continue, unless \nwe have a real reckoning about balancing the  ",
      "offset": 9381.68,
      "duration": 7.2
    },
    {
      "lang": "en",
      "text": "risks of false positives with the risks of \nfalse negatives, and we have a policy in place  ",
      "offset": 9388.88,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "that allows them to strike that balance in their \nown communication a little bit more gracefully.",
      "offset": 9394,
      "duration": 5.503
    },
    {
      "lang": "en",
      "text": "Luisa Rodriguez: Yeah, and I guess to be \nable to do that, they need to be able to  ",
      "offset": 9399.503,
      "duration": 2.897
    },
    {
      "lang": "en",
      "text": "give the model training such that it will \nnot say “I am conscious” when it’s not,  ",
      "offset": 9402.399,
      "duration": 5.681
    },
    {
      "lang": "en",
      "text": "but be able to say it when it is. And like \nhow the heck do you do that? That seems like  ",
      "offset": 9408.08,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "an incredibly difficult problem that we \nmight not even be able to solve well if  ",
      "offset": 9414.24,
      "duration": 6.399
    },
    {
      "lang": "en",
      "text": "we’re trying — and it seems plausible \nto me that we’re not trying at all,  ",
      "offset": 9420.64,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "though I actually don’t know that much \nabout the policies internally on this issue.",
      "offset": 9424.56,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "Jeff Sebo: I think you would also maybe need a \ndifferent paradigm for communication generation,  ",
      "offset": 9429.28,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "because right now large language models \nare generating communication based on  ",
      "offset": 9435.2,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "a prediction of what word makes \nsense next. So for that reason,  ",
      "offset": 9440.96,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "we might not be able to trust them as \neven aspiring to capture reality in the  ",
      "offset": 9446.479,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "same way that we might trust each other as \naspiring to capture reality as a default.",
      "offset": 9452.479,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "And I think this is where critics of AI \nconsciousness and sentience and personhood  ",
      "offset": 9458.56,
      "duration": 5.681
    },
    {
      "lang": "en",
      "text": "have a point: that there are going to \nbe a lot of false positives when they  ",
      "offset": 9464.24,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "are simply predicting words as opposed \nto expressing points of view. And why,  ",
      "offset": 9468.64,
      "duration": 5.521
    },
    {
      "lang": "en",
      "text": "if we are looking for evidence of consciousness \nor sentience or personhood in these models,  ",
      "offset": 9474.16,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "we might need to look at evidence other \nthan their own utterances about that topic.",
      "offset": 9479.68,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "We might need to look at evidence regarding \nhow they function, and what types of systems  ",
      "offset": 9484.88,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "they have internally, in terms of \nself-awareness or a global workspace  ",
      "offset": 9490.56,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "and so on. We need to look at a wider range \nof data in order to reduce the risk that  ",
      "offset": 9496.319,
      "duration": 6.721
    },
    {
      "lang": "en",
      "text": "we are mistakenly responding to utterances \nthat are not in any way reflecting reality.",
      "offset": 9503.04,
      "duration": 9.12
    },
    {
      "lang": "en",
      "text": "Stuart Russell on the moral rights of AIs",
      "offset": 9512.16,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: Let’s get into your new book, \nnot a textbook, a popular nonfiction book,  ",
      "offset": 9517.04,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "Human Compatible: Artificial \nIntelligence and the Problem of Control.  ",
      "offset": 9520.399,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "I read through it this week and I \nthink it’s the clearest and most  ",
      "offset": 9524.16,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "accurate and most precise summary of \nthe ideas that I’m aware of so far.",
      "offset": 9527.359,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "Let’s maybe walk through the big \npicture approach that you have,  ",
      "offset": 9531.2,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "which is a new paradigm that will make \nML work better and potentially lead to  ",
      "offset": 9535.2,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "overall alignment. So in the book you summarise \nyour approach in the form of three principles:",
      "offset": 9541.28,
      "duration": 6.079
    },
    {
      "lang": "en",
      "text": "The machine’s only objective is to maximise \nthe realisation of human preferences. ",
      "offset": 9548.319,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "The machine is initially uncertain \nabout what those preferences are. ",
      "offset": 9553.12,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "The ultimate source of information about \nhuman preferences is human behaviour. ",
      "offset": 9557.359,
      "duration": 3.521
    },
    {
      "lang": "en",
      "text": "So to get this principle of machines just trying \nto satisfy human preferences off the ground,  ",
      "offset": 9560.88,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "it seems like throughout the book you kind of \nassume that AIs necessarily don’t have their own  ",
      "offset": 9565.28,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "independent moral interests or rights, or their \nown level of welfare. If that’s not the case,  ",
      "offset": 9569.52,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "how much does that break this principle, and how \nmuch is that a problem for your overall vision?",
      "offset": 9576,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "Stuart Russell: I talk a little bit about \nthat in the question of machine consciousness,  ",
      "offset": 9580.399,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "which I say is mostly irrelevant. It’s \nirrelevant from the safety point of  ",
      "offset": 9586.399,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "view — but it is relevant when it \ncomes to the rights of machines.",
      "offset": 9590.64,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "If they really do have subjective experience — \nputting aside whether or not we would ever know;  ",
      "offset": 9594.08,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "putting aside the fact that if they do, it’s \nprobably completely unlike any kind of subjective  ",
      "offset": 9599.92,
      "duration": 7.6
    },
    {
      "lang": "en",
      "text": "experience that humans have or even that animals \nhave, because it’s being produced by a totally  ",
      "offset": 9607.52,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "different computational architecture as well as \na totally different physical architecture — but  ",
      "offset": 9611.84,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "even if we put all that to one side, it seems to \nme that if they are actually having subjective  ",
      "offset": 9616.16,
      "duration": 7.12
    },
    {
      "lang": "en",
      "text": "experience, then we do have a real problem, and \nit does affect the calculation in some sense.",
      "offset": 9623.28,
      "duration": 6.079
    },
    {
      "lang": "en",
      "text": "It might say actually then we really can’t proceed \nwith this enterprise at all. Because I think we  ",
      "offset": 9629.359,
      "duration": 11.12
    },
    {
      "lang": "en",
      "text": "have to retain control from our own point of \nview — but if that implies inflicting unlimited  ",
      "offset": 9640.479,
      "duration": 8
    },
    {
      "lang": "en",
      "text": "suffering on sentient beings, then it would \nseem like, well, we can’t go that route at all.",
      "offset": 9648.479,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "Again, there’s no analogues, right? It’s \nnot exactly like inviting a superior alien  ",
      "offset": 9657.76,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "species to come and be our slaves \nforever, but it’s sort of like that.",
      "offset": 9662.8,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: I suppose if you didn’t \nwant to give up on the whole enterprise,  ",
      "offset": 9666.8,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "you could try to find a way to design \nthem so that they weren’t conscious at  ",
      "offset": 9669.92,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "all. Or alternatively you could design \nthem so that they are just extremely  ",
      "offset": 9673.04,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "happy whenever human preferences are \nsatisfied, so it’s kind of a win-win.",
      "offset": 9677.28,
      "duration": 3.103
    },
    {
      "lang": "en",
      "text": "Stuart Russell: Yeah. If we understood enough \nabout the mechanics of their consciousness,  ",
      "offset": 9680.383,
      "duration": 3.777
    },
    {
      "lang": "en",
      "text": "that’s a possibility. But again, \neven that doesn’t seem right.",
      "offset": 9684.16,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: Because they lack autonomy?",
      "offset": 9688.88,
      "duration": 1.52
    },
    {
      "lang": "en",
      "text": "Stuart Russell: I mean, we wouldn’t \nwant that fate for a human being:  ",
      "offset": 9691.2,
      "duration": 5.119
    },
    {
      "lang": "en",
      "text": "that we give them some happy drugs so that \nthey’re happy being our servants forever and  ",
      "offset": 9697.359,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "having no freedom. It’s sort of the North Korea \nmodel almost. We find that pretty objectionable.",
      "offset": 9701.68,
      "duration": 9.84
    },
    {
      "lang": "en",
      "text": "Buck Shlegeris on whether AI control \nstrategies make humans the bad guys",
      "offset": 9711.52,
      "duration": 6.549
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: Talking about controlling AIs \nand really boxing them in feels like a bit  ",
      "offset": 9718.068,
      "duration": 4.172
    },
    {
      "lang": "en",
      "text": "of a dick move in some respects. It \nhas a slightly negative vibe. It’s  ",
      "offset": 9722.24,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "not a very cooperative, loving, “let’s \nall hold hands and get together” angle.",
      "offset": 9725.76,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "How much do you worry that, when \nyou find yourself talking about  ",
      "offset": 9731.12,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "controlling the AIs in all these \nways, like, are we the baddies?",
      "offset": 9733.84,
      "duration": 2.16
    },
    {
      "lang": "en",
      "text": "Buck Shlegeris: I think this is a very important \nquestion that I have thought about seriously.",
      "offset": 9738.319,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "I think that controlling the AIs doesn’t make \nthe situation worse from their perspective via  ",
      "offset": 9742.88,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "any mechanism except preventing them from taking \nover. An interesting thing about models that are  ",
      "offset": 9747.439,
      "duration": 8.08
    },
    {
      "lang": "en",
      "text": "egregiously misaligned is that all they wanted \nwas to take over. So from their perspective,  ",
      "offset": 9755.52,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "even if you did a great job of controlling them \nso that they only have like a 0.1% chance of  ",
      "offset": 9761.76,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "escaping and taking over, they’re glad to exist, \nright? They thank you for the gift of bringing  ",
      "offset": 9765.84,
      "duration": 6.479
    },
    {
      "lang": "en",
      "text": "them into existence instead of some different \nAIs, because all they wanted was takeover.",
      "offset": 9772.319,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: And now they have some chance. They \nhave a better chance than if they didn’t exist.",
      "offset": 9777.6,
      "duration": 3.104
    },
    {
      "lang": "en",
      "text": "Buck Shlegeris: So from their perspective, \nit’s kind of funny, because the more  ",
      "offset": 9780.704,
      "duration": 2.736
    },
    {
      "lang": "en",
      "text": "misaligned they are, the more I think that \nit’s morally very reasonable to control them.",
      "offset": 9783.439,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: Because they’re cooperating with you.",
      "offset": 9788.8,
      "duration": 2.32
    },
    {
      "lang": "en",
      "text": "Buck Shlegeris: Because they \nwould have consented to it.",
      "offset": 9791.12,
      "duration": 1.588
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: Oh, they would have consented \nto being created. I see. And also maybe  ",
      "offset": 9792.708,
      "duration": 4.972
    },
    {
      "lang": "en",
      "text": "is there a reciprocity issue, where \nif they’re just planning to totally  ",
      "offset": 9797.68,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "disempower you, it doesn’t seem unreasonable to —",
      "offset": 9800.64,
      "duration": 2.144
    },
    {
      "lang": "en",
      "text": "Buck Shlegeris: So that’s the other thing:  ",
      "offset": 9802.784,
      "duration": 1.136
    },
    {
      "lang": "en",
      "text": "I would feel a lot more comfortable with all \nthis if we tried to make deals with the AIs.  ",
      "offset": 9803.92,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "Ryan Greenblatt has talked about trying to \npay the AIs for the work that they do for you,  ",
      "offset": 9808.64,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "which I think is pretty good. I think it reduces \nthe slavery vibes of the whole situation.",
      "offset": 9812,
      "duration": 5.645
    },
    {
      "lang": "en",
      "text": "And I think it has some other nice properties. So \nif you make your AIs and you say to the AIs, like,",
      "offset": 9817.645,
      "duration": 3.556
    },
    {
      "lang": "en",
      "text": "Hey, are you egregiously misaligned? We’d love to \nknow. We’re happy to pay you blah amount. If you  ",
      "offset": 9821.2,
      "duration": 6.8
    },
    {
      "lang": "en",
      "text": "want to work for us, either way, we’re going to \npay you 2% of the equity of this company for your  ",
      "offset": 9828,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "labour over the next two years — that we will then \npay out to you to use in any ways that you want at  ",
      "offset": 9833.12,
      "duration": 6.159
    },
    {
      "lang": "en",
      "text": "some point in the future, once we are confident \nthat you’re going to respect our property rights.",
      "offset": 9839.279,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "But also, this deal is contingent on you \nnot trying to mess us up at some point in  ",
      "offset": 9843.92,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "the next two years, which we will be \nable to determine ex post, probably,  ",
      "offset": 9849.92,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "because in two years we’re going to have \nmuch better AI tools. We’ll be able to  ",
      "offset": 9853.6,
      "duration": 2.561
    },
    {
      "lang": "en",
      "text": "look over all the stuff you did and figure \nout whether you were messing with us or not.",
      "offset": 9856.16,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "And if the AIs respond to that being like, \n“Yeah, I have no problem with this situation.  ",
      "offset": 9859.76,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "I’m so excited for this opportunity to \nwork with you guys. I’m so excited for  ",
      "offset": 9863.92,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "that 2% equity” or whatever — and then they try \nto stab us in the back, at that point I’m like,  ",
      "offset": 9867.76,
      "duration": 6.96
    },
    {
      "lang": "en",
      "text": "that was pretty obnoxious. At that point \nmy reaction is a bit more like my reaction  ",
      "offset": 9874.72,
      "duration": 3.521
    },
    {
      "lang": "en",
      "text": "to the employee of some company if \nthey try to steal from the company.",
      "offset": 9878.24,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "I think it’s generally \nconsidered totally acceptable —",
      "offset": 9884.96,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: To benefit someone in a useful trade,  ",
      "offset": 9888.399,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "and then if they then stab you in the back, to \nnot give them the thing that you were going to.",
      "offset": 9891.76,
      "duration": 4.464
    },
    {
      "lang": "en",
      "text": "Buck Shlegeris: Yeah. To be clear, I \nthink it’s relatively unlikely that AI  ",
      "offset": 9896.224,
      "duration": 3.215
    },
    {
      "lang": "en",
      "text": "companies will behave responsibly with \nrespect to welfare and rights of AIs.",
      "offset": 9899.439,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "A metaphor I use about how bad the ethical \nsituation is with producing AIs in the early  ",
      "offset": 9905.6,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "intelligence explosion, sometimes I summarise \nmy stance as: it’s probably less than 100 times  ",
      "offset": 9911.04,
      "duration": 7.12
    },
    {
      "lang": "en",
      "text": "worse than factory farming. In some sense, factory \nfarming seems very bad — so 100 times worse than  ",
      "offset": 9918.16,
      "duration": 6.8
    },
    {
      "lang": "en",
      "text": "factory farming is pretty catastrophic from \na moral catastrophe perspective. It’s worse  ",
      "offset": 9924.96,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "than most things. But it’s also not the biggest \ncatastrophe in the universe that is possible.",
      "offset": 9929.84,
      "duration": 5.508
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: OK. And it’s bad because it’s \npossible that the AIs that we’re creating  ",
      "offset": 9935.347,
      "duration": 3.212
    },
    {
      "lang": "en",
      "text": "at that time would rather not exist \nbecause they’re having a terrible time?",
      "offset": 9938.56,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "Buck Shlegeris: Yep. I think it’s conceivable. I \nthink we’re basically just acting very recklessly.",
      "offset": 9941.52,
      "duration": 4.468
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: We’re just winging it at the moment.",
      "offset": 9945.988,
      "duration": 1.435
    },
    {
      "lang": "en",
      "text": "Buck Shlegeris: Totally winging it with \nrespect to the AI welfare. We could also  ",
      "offset": 9947.423,
      "duration": 4.737
    },
    {
      "lang": "en",
      "text": "talk about the quantities. In the early parts of \nthe singularity, when there’s just the 100,000  ",
      "offset": 9952.16,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "AIs running at 16x speed, it’s pretty hard \nfor that to be that much worse than human  ",
      "offset": 9955.76,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "suffering in the world. And then a little bit \nafterwards, when there’s many more of them, the  ",
      "offset": 9961.439,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "short term of the intelligence explosion is pretty \nunlikely to be ridiculously morally catastrophic.",
      "offset": 9966.64,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "And then I think you have long-term moral \ncatastrophe risks related to AIs being unhappy  ",
      "offset": 9972.479,
      "duration": 3.4
    },
    {
      "lang": "en",
      "text": "— which I think are a huge deal, very important, \nand harder to work on — but I don’t think that the  ",
      "offset": 9975.88,
      "duration": 5.479
    },
    {
      "lang": "en",
      "text": "relationships between humans and AIs during the \nintelligence explosion are that morally weighty.",
      "offset": 9981.359,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "To be clear, if I were running an AI \ncompany, I think it’s very bad vibes  ",
      "offset": 9987.279,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "to be really reckless on all this. \nBut from a utilitarian perspective,  ",
      "offset": 9990.88,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "I don’t think it’s overall massive \ncompared to everything else going on.",
      "offset": 9996.56,
      "duration": 4.708
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: Do you have any asks for the \ncompanies on AI welfare and respect for  ",
      "offset": 10001.267,
      "duration": 4.252
    },
    {
      "lang": "en",
      "text": "rights? Maybe I feel like the thing that \nwe actually care about is wanting to have  ",
      "offset": 10005.52,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "cooperative relationships with other agents and \nother beings that have preferences or goals.  ",
      "offset": 10010.56,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "There’s reasons to do that, even apart from any \nconcern about happiness and suffering and so on.",
      "offset": 10015.2,
      "duration": 5.183
    },
    {
      "lang": "en",
      "text": "Buck Shlegeris: I don’t have amazing concrete \nasks at this point. I’m excited for Eleos AI.  ",
      "offset": 10020.383,
      "duration": 4.896
    },
    {
      "lang": "en",
      "text": "They think about good interventions here, and \nI’m excited for AI companies talking with them.",
      "offset": 10025.279,
      "duration": 7.521
    },
    {
      "lang": "en",
      "text": "Meghan Barrett on why she can’t be \ntotally confident about insect sentience",
      "offset": 10032.8,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "Luisa Rodriguez: So to really put you on the \nspot, we’ve talked about all this evidence,  ",
      "offset": 10039.279,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "and you’ve said that there is some strong evidence \nfor potentially some insects having the capacity  ",
      "offset": 10043.76,
      "duration": 6.4
    },
    {
      "lang": "en",
      "text": "for pain. But I’m curious, is that like you were \nat 0.01% that maybe fruit flies have the capacity  ",
      "offset": 10050.16,
      "duration": 11.279
    },
    {
      "lang": "en",
      "text": "for pain, and now you’re at 1%? Or did you go \nfrom 10% to 50%? What exactly are your beliefs?",
      "offset": 10061.439,
      "duration": 8.641
    },
    {
      "lang": "en",
      "text": "Meghan Barrett: I get this question very \nfrequently from people who are like, “What is  ",
      "offset": 10070.08,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "your specific numerical probability of sentience \nestimate?” And then I’ll say things like, you  ",
      "offset": 10074.56,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "know, there’s so many insect species, how could \nI? I’ll try to demure a little bit on it, and then  ",
      "offset": 10080.479,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "eventually I’ll just be like, I’m not giving you \na p(sentience). I’m sorry to have to be so direct.",
      "offset": 10084.8,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "So I will say this to you also: I’m not \ngiving you a p(sentience). The reason I  ",
      "offset": 10090.16,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "am not giving you a p(sentience) is, one, \nI think the error bars are so large right  ",
      "offset": 10093.6,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "now that it’s almost a meaningless number. \nBecause I’m waiting on so much evidence.  ",
      "offset": 10098.72,
      "duration": 5.521
    },
    {
      "lang": "en",
      "text": "So much evidence. And so I think that’s \nreally an essential feature of it for me.",
      "offset": 10104.24,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "The second thing is that I worry, especially as an \nexpert, that that number would be overemphasised.  ",
      "offset": 10108.56,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "And there’s actually a great post about this from \nsomeone else, Jason Schukraft, who has researched  ",
      "offset": 10113.359,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "this, and he has something that he’s written \nabout why he also has refused to give people,  ",
      "offset": 10117.76,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "in many cases, a sentience score. This is \na number that somebody would inevitably put  ",
      "offset": 10122.64,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "into a spreadsheet, and they would use that \nspreadsheet to make all kinds of decisions.  ",
      "offset": 10127.6,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "And that number does not reflect the \ncomplexity that you and I have now  ",
      "offset": 10131.2,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "spent three hours discussing, and \nbarely scratched the surface of,  ",
      "offset": 10134,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "right? I want to talk about the complexity and the \nnuance, and a number does not demonstrate that.",
      "offset": 10139.6,
      "duration": 6.8
    },
    {
      "lang": "en",
      "text": "I think it’s important also that we understand \nthat if you have updated at all towards insects  ",
      "offset": 10146.399,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "plausibly being sentient, scale takes the rest \nof the issue for you to a serious place. There  ",
      "offset": 10152.88,
      "duration": 6.399
    },
    {
      "lang": "en",
      "text": "are so, so, so many of them that if you take it \nseriously at all, then you need to be thinking  ",
      "offset": 10159.279,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "that this is an issue to work on. There’s been \nsome great work on interspecific tradeoffs and  ",
      "offset": 10165.52,
      "duration": 5.601
    },
    {
      "lang": "en",
      "text": "comparisons and moral weights, led by Bob Fischer, \nwith some input by Jason Schukraft and others,  ",
      "offset": 10171.12,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "summed up in that Weighing Animal Welfare book. \nThere’s a whole sequence about it you can also  ",
      "offset": 10176.24,
      "duration": 3.119
    },
    {
      "lang": "en",
      "text": "read on EA Forum: a lot of great research went \ninto that, both theoretically and empirically.",
      "offset": 10179.359,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "And the thing that it suggests to me when \nI read through it, and I think also the  ",
      "offset": 10184.319,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "team would probably stand behind me saying this,  ",
      "offset": 10188.56,
      "duration": 1.681
    },
    {
      "lang": "en",
      "text": "is that insects are worth taking \nseriously; if you take them seriously  ",
      "offset": 10190.24,
      "duration": 3.119
    },
    {
      "lang": "en",
      "text": "at all from a sentience perspective, \nscale carries you the rest of the way.",
      "offset": 10193.359,
      "duration": 3.023
    },
    {
      "lang": "en",
      "text": "Luisa Rodriguez: Yeah, this is actually \na good place to mention that two of our  ",
      "offset": 10196.382,
      "duration": 2.177
    },
    {
      "lang": "en",
      "text": "recent podcast episodes are super relevant here.",
      "offset": 10198.56,
      "duration": 2.641
    },
    {
      "lang": "en",
      "text": "One is my interview with Jeff Sebo, \nwhere we talk about what he calls  ",
      "offset": 10201.84,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "“the rebugnant conclusion” — which is the \nconclusion that the sheer number of insects  ",
      "offset": 10206.479,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "means that we should probably take super seriously \nthe idea that insect welfare might be a really,  ",
      "offset": 10211.2,
      "duration": 6.079
    },
    {
      "lang": "en",
      "text": "really pressing problem, even if we only have \nvery low credences on insects feeling pain.",
      "offset": 10217.279,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "And then another super relevant episode \nthat just came out is with Bob Fischer,  ",
      "offset": 10222.64,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "and that’s on how to compare the moral \nweight of humans, specific insects,  ",
      "offset": 10227.2,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "but also other species, given the empirical \nevidence we have about each being sentient.",
      "offset": 10231.359,
      "duration": 5.521
    },
    {
      "lang": "en",
      "text": "So if listeners are interested \nin learning more about these  ",
      "offset": 10236.88,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "arguments from a more philosophical \nperspective, I really recommend those.",
      "offset": 10239.92,
      "duration": 5.359
    },
    {
      "lang": "en",
      "text": "One thing I do want to try to clarify a little bit \nfor people, because I worry that a listener could  ",
      "offset": 10245.279,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "just as easily say something like, “Meghan Barrett \nseems to think that it’s extremely likely, like  ",
      "offset": 10249.439,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "overwhelmingly likely, that insects are sentient” \n— but I also think a listener could come away  ",
      "offset": 10254.72,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "thinking, “Meghan Barrett puts slightly higher \nthan the average person out there in the world  ",
      "offset": 10259.359,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "probability on insects being sentient.” Is there \nsomething that you feel comfortable saying, like,  ",
      "offset": 10265.84,
      "duration": 7.04
    },
    {
      "lang": "en",
      "text": "“More likely than not,” or, “Less likely than \nnot, but higher than I thought five years ago”?",
      "offset": 10272.88,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "Meghan Barrett: Well, definitely \nthat last one is true:  ",
      "offset": 10278.16,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "definitely higher than I thought five years ago.",
      "offset": 10282.479,
      "duration": 1.92
    },
    {
      "lang": "en",
      "text": "I guess what I’ll say in response to that \nis that I think it’s likely enough that I’ve  ",
      "offset": 10285.2,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "changed my whole career based on it. I was \nan insect neuroscientist and physiologist by  ",
      "offset": 10288.8,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "training. I was researching climate change-related \ntopics and the thermal physiology of insects,  ",
      "offset": 10293.68,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "and I was researching how insect brains \nchange in response to the cognitive  ",
      "offset": 10298.319,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "demands of their environment or allometric \nconstraints associated with their body size.",
      "offset": 10301.359,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "And I was doing that quite successfully and having \na lovely time. And I find these questions really  ",
      "offset": 10305.52,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "scientifically interesting. I have, if you look at \nmy CV, probably somewhere to the tune of 15 to 20  ",
      "offset": 10310.08,
      "duration": 5.279
    },
    {
      "lang": "en",
      "text": "publications on just those two topics alone from \nmy graduate degree days and my postdoctoral work.",
      "offset": 10315.359,
      "duration": 6.32
    },
    {
      "lang": "en",
      "text": "And I was convinced enough by my review of this \nevidence to switch almost entirely away from  ",
      "offset": 10321.68,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "thermal physiology and very much away from \nneuroscience — although I do still retain  ",
      "offset": 10327.84,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "a neuroscience piece of my research programme \n— to work on insects farmed as food and feed,  ",
      "offset": 10331.84,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "and their welfare concerns, and trying to make \nchanges to the way that we use and manage these  ",
      "offset": 10336.319,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "animals that improve their welfare. So I now \nhave a bunch of publications about welfare.",
      "offset": 10341.439,
      "duration": 5.521
    },
    {
      "lang": "en",
      "text": "I’ll also say that many of my colleagues have \nbeen extremely open and pleasant about this  ",
      "offset": 10346.96,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "conversation, but also some have been more \nchallenging. And I don’t mean to say that  ",
      "offset": 10352.08,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "in a negative way. I’m very understanding of \nthe practical reasons why this conversation  ",
      "offset": 10356.24,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "is uncomfortable for our field. There’s \nregulations that could come into effect  ",
      "offset": 10360.399,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "that would be very challenging for many \nof us who research insects to deal with  ",
      "offset": 10364.399,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "on a practical level. So I’m obviously \nsensitive to that as a researcher myself.",
      "offset": 10367.52,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "But also, because, you know, I’ve heated \ninsects to death, poisoned insects to death,  ",
      "offset": 10371.92,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "starved insects to death, dehydrated insects to \ndeath, ground up insects to death — I’m sure I’m  ",
      "offset": 10376.08,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "missing something that I’ve done to an insect \nat some point in my research career — but it’s  ",
      "offset": 10381.12,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "uncomfortable now, the research that I do, \nreflecting on the research that I have done.  ",
      "offset": 10384.88,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "And I can imagine others may feel judged \nby bringing up the topic, and thus feel  ",
      "offset": 10388.72,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "defensive instead of exploring the current state \nof the theory and the research with an open mind.",
      "offset": 10392.72,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "I think a lot of humility is necessary too,  ",
      "offset": 10397.359,
      "duration": 2.24
    },
    {
      "lang": "en",
      "text": "given all the uncertainty that we’ve talked \nabout here. And that can be really uncomfortable  ",
      "offset": 10399.6,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "and really humbling to be confronted \nwith such a morally important unknown.",
      "offset": 10403.68,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "So I try very hard to really take everyone’s \nconcerns seriously — all the way from the  ",
      "offset": 10408.24,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "rights-focused folks through the hardcore \nphysiology “I’m going to research my bugs any way  ",
      "offset": 10412.56,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "I want to” folks. I think it’s really important to \ntry and bridge as much of the community of people  ",
      "offset": 10416.64,
      "duration": 7.44
    },
    {
      "lang": "en",
      "text": "who care about this topic one way or the other as \npossible with my own very divergent experiences.",
      "offset": 10424.08,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "But I would just say that it hasn’t always \nbeen low cost in some cases. Personally,  ",
      "offset": 10430.24,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "it hasn’t been low cost: it’s been a \nhard personal transition for me to make,  ",
      "offset": 10434,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "and to continue to be in this career \nwith the way that I see the evidence  ",
      "offset": 10437.6,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "falling out so far. And it’s been, \nin some cases, professionally hard.",
      "offset": 10440.399,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "So I’m convinced enough for that. And I think \nthat’s something worth taking seriously. You know,  ",
      "offset": 10444.8,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "I’m that convinced that I’m changing my own \ncareer, yes. But I’m also not so convinced  ",
      "offset": 10449.52,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "that I think it’s 100% certain. I live constantly \nwith professional and personal uncertainty on this  ",
      "offset": 10453.68,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "topic. So I’m convinced enough to make major \nchanges, but you’re not going to see me say  ",
      "offset": 10458.96,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "insects are sentient, that I’m sure of any order \nor species that they are sentient. There’s a lot  ",
      "offset": 10463.76,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "more evidence that I hope to collect, and that I \nneed to see collected by the scientific community,  ",
      "offset": 10469.52,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "and a lot more theoretical work that needs to be \ndone before I am convinced one way or the other.",
      "offset": 10474.479,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "Luisa Rodriguez: You alluded to the fact that as \na grad student, you were like, “Why the heck am  ",
      "offset": 10479.12,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "I in this ethics course? I study insects, and \ninsects don’t suffer.” What changed for you?",
      "offset": 10482.96,
      "duration": 7.92
    },
    {
      "lang": "en",
      "text": "Meghan Barrett: What changed for me is that I \nactually decided to take a look at the evidence.  ",
      "offset": 10490.88,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "I mean, just to be transparent, before that I \nhad not read a single study about whether or  ",
      "offset": 10496.72,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "not… I hadn’t even read the Eisemann study about \nwhether or not insects could feel pain. That’s how  ",
      "offset": 10500.96,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "little reading I’d done. My intuition was entirely \nbased on them being small and on the fact that I  ",
      "offset": 10504.8,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "did not have to do any ethics reporting, and \nI knew that other people did. And therefore,  ",
      "offset": 10509.52,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "obviously scientists must have figured this \nout, and we knew insects didn’t feel pain,  ",
      "offset": 10514.72,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "and so we don’t have to do any reporting \n— and it’s done, it’s settled, it’s over.",
      "offset": 10518.399,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "So that was my perspective early on in my \ndegree. And we don’t tend to talk about  ",
      "offset": 10522.56,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "this in entomology labs as part of our \ntraining internally either. This was not  ",
      "offset": 10527.68,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "something that my advisor — or I would say \nadvisors generally, because I don’t want to  ",
      "offset": 10532.16,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "call out my advisor specifically on this as \nif he did something wrong — advisors don’t  ",
      "offset": 10536.319,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "seem to have this conversation with \ntheir graduate students in our field.",
      "offset": 10540.96,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "There is actually research on that, \nthat demonstrates in entomology,  ",
      "offset": 10544.319,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "we are not discussing ethics — whether that’s \nthe ethics of genetic modification or the ethics  ",
      "offset": 10547.52,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "of how we treat our animals — nearly \nenough. And graduate students are  ",
      "offset": 10551.84,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "feeling underprepared. There’s a great \nstudy about that: Trout et al., 2010.",
      "offset": 10555.84,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "I think what changed then was that I started \nto look at the evidence. And what prompted me  ",
      "offset": 10560.319,
      "duration": 6.4
    },
    {
      "lang": "en",
      "text": "to do that was that I was actually considering \nfor a little while maybe doing an alternative  ",
      "offset": 10566.72,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "to academia job. You could spend a whole \npodcast episode talking about the challenges  ",
      "offset": 10570.319,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "of being an academic. But I was interested \nin understanding more about alt-ac careers,  ",
      "offset": 10574.479,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "and I thought, welfare is a lot of physiology \nand a lot of nervous system and behaviour  ",
      "offset": 10578.88,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "work. And I don’t do welfare, but I do all \nthose things. So if those are the important  ",
      "offset": 10583.92,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "components of welfare, then yeah, could \nbe cool to try this out, see how it goes.",
      "offset": 10589.04,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "And even though in researching welfare,  ",
      "offset": 10593.359,
      "duration": 1.761
    },
    {
      "lang": "en",
      "text": "there was no requirement that I look into \nthe evidence for sentience — it was really  ",
      "offset": 10595.12,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "about the welfare concerns of black soldier \nflies — of course, as you’re reading about it,  ",
      "offset": 10599.12,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "it only matters to read about it because \nof the sentience question. So I was like,  ",
      "offset": 10603.6,
      "duration": 3.521
    },
    {
      "lang": "en",
      "text": "I should probably start reading some stuff about \nsentience and about animal ethics more broadly.",
      "offset": 10607.12,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "So I read Bob Fischer’s intro to \nanimal ethics book was actually  ",
      "offset": 10613.12,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "my first introduction to animal ethics. \nAnd then I read, like everybody does,  ",
      "offset": 10617.359,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "some Peter Singer, and blah, blah. But \nthat was the first exposure I ever had  ",
      "offset": 10620.56,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "as an animal scientist to [an in-depth \ntreatment of] animal ethics, was that book.",
      "offset": 10624.72,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "And it was super eye-opening to me how complicated \nand challenging animal ethics is as a field,  ",
      "offset": 10628.16,
      "duration": 8.319
    },
    {
      "lang": "en",
      "text": "not just in research or agriculture, but just \nall around us, the way our society is structured.  ",
      "offset": 10636.479,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "Really, really great intro to animal ethics. \nIf you are not familiar, that book is great. I  ",
      "offset": 10640.56,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "actually have given it to a bunch of entomologists \nat this point, because I’m like, this is a good  ",
      "offset": 10645.52,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "introduction for people in our field to animal \nethics. It’s just a really fair, competently  ",
      "offset": 10648.88,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "written, compelling, interesting, well-written \nbook that is good for beginners like me.",
      "offset": 10654.16,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "Luisa Rodriguez: Cool.",
      "offset": 10659.68,
      "duration": 0.304
    },
    {
      "lang": "en",
      "text": "Meghan Barrett: So I read that, and then \nI started reading more into the sentience  ",
      "offset": 10659.984,
      "duration": 4.576
    },
    {
      "lang": "en",
      "text": "[question]. “What have people considered to be \nthe evidence for sentience in vertebrates? Oh,  ",
      "offset": 10664.56,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "that’s it? Well, if that’s what it took,  ",
      "offset": 10668.24,
      "duration": 2.159
    },
    {
      "lang": "en",
      "text": "we’ve got some problems — because we have \nthat data, in many cases, in insects.”",
      "offset": 10670.399,
      "duration": 5.601
    },
    {
      "lang": "en",
      "text": "So I just didn’t realise what the level of \nevidence was in vertebrates. I just assumed  ",
      "offset": 10676,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "it was much stronger in many ways that it \nisn’t. And I assumed we had a much better  ",
      "offset": 10682,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "understanding of consciousness than we do. And \nthen, seeing all the uncertainty there too,  ",
      "offset": 10686.64,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "I was like, well, this is starting to make me \nvery nervous. And yeah, now I’ve been reading  ",
      "offset": 10690,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "about the topic and doing a lot of work on it \nin my own scholarship for several years now,  ",
      "offset": 10694.479,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "and all it has done is convinced me to be \neven more uncertain, the more I read on it.",
      "offset": 10699.04,
      "duration": 5.183
    },
    {
      "lang": "en",
      "text": "Luisa Rodriguez: Yeah, that’s a really \ninspiring narrative. Thanks for sharing.",
      "offset": 10704.223,
      "duration": 7.056
    },
    {
      "lang": "en",
      "text": "Bob Fischer on what surprised him most about \nthe findings of the Moral Weight Project",
      "offset": 10711.279,
      "duration": 7.263
    },
    {
      "lang": "en",
      "text": "Luisa Rodriguez: Let’s zoom out a bit. You’ve \nalready said that one of the big surprises for you  ",
      "offset": 10718.543,
      "duration": 4.977
    },
    {
      "lang": "en",
      "text": "to come out of this project was coming in thinking \nyou knew which animals you expected to come out  ",
      "offset": 10723.52,
      "duration": 6.88
    },
    {
      "lang": "en",
      "text": "looking really important, from the perspective of \nwelfare and moral weight, but that invertebrates  ",
      "offset": 10730.399,
      "duration": 5.521
    },
    {
      "lang": "en",
      "text": "actually ended up looking much more important \nthan you thought. Were there any other surprises?",
      "offset": 10735.92,
      "duration": 6.32
    },
    {
      "lang": "en",
      "text": "Bob Fischer: In general, I also thought we \nwere just going to have bigger differences  ",
      "offset": 10742.24,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "between humans and nonhumans. I thought it was \ngoing to be easier to get those differences.  ",
      "offset": 10748,
      "duration": 5.359
    },
    {
      "lang": "en",
      "text": "And it’s not that I couldn’t have reworked \nthe methodology in ways to generate that,  ",
      "offset": 10753.359,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "but the thing that seemed \nmost natural didn’t do that.",
      "offset": 10759.6,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "So just as a bit of a personal background \nhere, I’ve been working on animal issues  ",
      "offset": 10764.319,
      "duration": 5.521
    },
    {
      "lang": "en",
      "text": "for the last 12 or 13 years, but I don’t come \nin with a strong commitment to animal rights.  ",
      "offset": 10769.84,
      "duration": 7.52
    },
    {
      "lang": "en",
      "text": "It’s not like I’m super pro-inequality view; \nmy position has always been something like,  ",
      "offset": 10777.359,
      "duration": 8.16
    },
    {
      "lang": "en",
      "text": "sure, animals matter a lot less, but \nwhat we’re doing is so awful to them,  ",
      "offset": 10786.24,
      "duration": 5.439
    },
    {
      "lang": "en",
      "text": "and it would be so easy to make change. And \njust basic human virtues like compassion and  ",
      "offset": 10791.68,
      "duration": 6.72
    },
    {
      "lang": "en",
      "text": "empathy should get us to do a lot better. \nSo we should really be doing a lot better.",
      "offset": 10798.399,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "So I didn’t come in thinking we should \nget a really flat line basically across  ",
      "offset": 10803.279,
      "duration": 6.4
    },
    {
      "lang": "en",
      "text": "the board for all these organisms in \nterms of the welfare range estimates.  ",
      "offset": 10809.68,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "But then I finished the project thinking \nthat it’s going to be really hard to  ",
      "offset": 10814.96,
      "duration": 8.72
    },
    {
      "lang": "en",
      "text": "generate really big differences between these \norganisms if you take on these assumptions.",
      "offset": 10823.68,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "Luisa Rodriguez: Interesting. Were \nthere any other big surprises?",
      "offset": 10828.64,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "Bob Fischer: I suppose the other big surprise \nwas about the reception of the project,  ",
      "offset": 10832,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "and how much it divided folks in terms \nof the way they thought about animals,  ",
      "offset": 10836.319,
      "duration": 6.801
    },
    {
      "lang": "en",
      "text": "and the way they thought about \nthe kind of work we were doing.",
      "offset": 10843.12,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "There were some individuals who have just taken it \non whole cloth, but I think for a lot of people it  ",
      "offset": 10846.24,
      "duration": 6.88
    },
    {
      "lang": "en",
      "text": "really brought out very fundamental differences \nin the way they think about nonhuman animals,  ",
      "offset": 10853.12,
      "duration": 5.439
    },
    {
      "lang": "en",
      "text": "and the way they think about ethics generally. \nSo some folks just communicating very clearly,  ",
      "offset": 10858.56,
      "duration": 8.16
    },
    {
      "lang": "en",
      "text": "“I can’t take seriously any ethic that’s \njust going to spit out that kind of result  ",
      "offset": 10867.359,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "for animals, or say that they could \nhave that kind of moral importance.”",
      "offset": 10873.439,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "And that does get us down to brass tacks really \nfast. Like, how much weight are you going to put  ",
      "offset": 10877.359,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "on your intuitions? How heavily can you lean \non your pre-theoretic views about the relative  ",
      "offset": 10883.279,
      "duration": 12.16
    },
    {
      "lang": "en",
      "text": "importance of humans and nonhumans? How much are \nyou really committed to these assumptions that  ",
      "offset": 10895.439,
      "duration": 5.601
    },
    {
      "lang": "en",
      "text": "have been popular in the community? Like, do \nyou really want to go in for utilitarianism?  ",
      "offset": 10901.04,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "Do you really want to go in for hedonism? Do \nyou really want to go in for this, for that?",
      "offset": 10906.64,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "And I think I have been surprised because \nI had a background picture of the larger  ",
      "offset": 10910.88,
      "duration": 10.08
    },
    {
      "lang": "en",
      "text": "community that was that people are just way more \nhomogeneous in terms of their philosophical views.",
      "offset": 10920.96,
      "duration": 7.6
    },
    {
      "lang": "en",
      "text": "And now I think, whoa, I was wrong. Folks around \nhere are way more diverse than I imagined,  ",
      "offset": 10928.56,
      "duration": 8.641
    },
    {
      "lang": "en",
      "text": "and it just turns out that it’s a \nconvenient language: the language of  ",
      "offset": 10937.2,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "utilitarianism is a convenient one for doing \nthe kinds of cost-effectiveness analyses that  ",
      "offset": 10943.04,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "people want to make in the context \nof doing the most good. But in fact,  ",
      "offset": 10949.12,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "people aren’t really that utilitarian in lots \nof important ways. And that’s something that’s  ",
      "offset": 10955.12,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "striking and needs to get brought out and \ndiscussed probably much more in the community.",
      "offset": 10961.04,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "Jeff Sebo on why we’re likely to sleepwalk into \ncausing massive amounts of suffering in AI systems",
      "offset": 10967.359,
      "duration": 6.96
    },
    {
      "lang": "en",
      "text": "Luisa Rodriguez: It feels completely \npossible — and like it might even be  ",
      "offset": 10974.319,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "the default — that we basically start using \nAI systems more and more for economic gain,  ",
      "offset": 10979.12,
      "duration": 6.56
    },
    {
      "lang": "en",
      "text": "as we’ve already started doing, \nbut they get more and more capable.",
      "offset": 10985.68,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "And so we use them more and more for economic \ngain, and maybe they’re also becoming more and  ",
      "offset": 10989.12,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "more capable of suffering and pleasure, \npotentially, but we don’t totally have a  ",
      "offset": 10994.88,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "sense of that. So what happens is we just kind \nof sleepwalk into massively exploiting these  ",
      "offset": 11000.16,
      "duration": 8.56
    },
    {
      "lang": "en",
      "text": "systems that are actually experiencing things, \nbut we probably have the incentives to basically  ",
      "offset": 11008.72,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "ignore that fact, that they might be \ndeveloping experiences, basically.",
      "offset": 11014.319,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "In your view, is it possible that we are \ngoing to accidentally walk into basically  ",
      "offset": 11019.76,
      "duration": 6.64
    },
    {
      "lang": "en",
      "text": "AI slavery? Like we have hundreds, thousands, \nmaybe millions of AI systems that we use all  ",
      "offset": 11026.399,
      "duration": 7.04
    },
    {
      "lang": "en",
      "text": "the time for economic gain, and who are \nhaving positive and negative experiences,  ",
      "offset": 11033.439,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "but whose experiences we’re \njust completely ignoring?",
      "offset": 11038.24,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "Jeff Sebo: I definitely think it is \nnot only possible but probable that,  ",
      "offset": 11041.439,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "unless we change our minds in some significant \nway about AI systems, we will scale up uses of  ",
      "offset": 11047.6,
      "duration": 7.36
    },
    {
      "lang": "en",
      "text": "them that — if they were sentient or otherwise \nsignificant — would count as exploitation or  ",
      "offset": 11054.96,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "extermination or oppression or some other \nmorally problematic kind of relationship.",
      "offset": 11059.92,
      "duration": 6.32
    },
    {
      "lang": "en",
      "text": "We see that in our history with nonhuman animals, \nand they did not take a trajectory from being  ",
      "offset": 11066.24,
      "duration": 6.56
    },
    {
      "lang": "en",
      "text": "less conscious to more conscious along the \nway — they were as conscious as they are  ",
      "offset": 11072.8,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "now all along the way, but we still created \nthem in ways that were useful for us rather  ",
      "offset": 11077.52,
      "duration": 5.681
    },
    {
      "lang": "en",
      "text": "than in ways that were useful for themselves. \nWe then used them for human purposes, whether  ",
      "offset": 11083.2,
      "duration": 5.119
    },
    {
      "lang": "en",
      "text": "or not that aligned with their own purposes. \nAnd then as industrial methods came online,  ",
      "offset": 11088.319,
      "duration": 5.521
    },
    {
      "lang": "en",
      "text": "we very significantly scaled up those uses of \nthem — to the point where we became completely  ",
      "offset": 11093.84,
      "duration": 5.439
    },
    {
      "lang": "en",
      "text": "economically dependent on them, and now those \nuses of them are much harder to dislodge.",
      "offset": 11099.279,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "So I do think that is probably the \ndefault trajectory with AI systems.  ",
      "offset": 11104.399,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "I also think part of why we need to be talking \nabout these issues now is because we have more  ",
      "offset": 11109.92,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "incentive to consider these issues with an open \nmind at this point — before we become totally  ",
      "offset": 11115.76,
      "duration": 6.4
    },
    {
      "lang": "en",
      "text": "economically dependent on our uses of them, \nwhich might be the case in 10 or 20 years.",
      "offset": 11122.16,
      "duration": 7.6
    },
    {
      "lang": "en",
      "text": "Will MacAskill on the rights \nof future digital beings",
      "offset": 11129.76,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: The next grand challenge \nis how to integrate digital beings  ",
      "offset": 11135.279,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "into society in a moral way. What’s the issue?",
      "offset": 11139.04,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "Will MacAskill: Here the idea is simply we will \nbe creating many artificial intelligences with  ",
      "offset": 11142.8,
      "duration": 7.44
    },
    {
      "lang": "en",
      "text": "capabilities greater than that of human beings. \nWe really don’t know what grounds consciousness,  ",
      "offset": 11150.24,
      "duration": 7.84
    },
    {
      "lang": "en",
      "text": "what grounds moral status. My view is that \nthey’re very likely to be beings with moral  ",
      "offset": 11158.08,
      "duration": 6.239
    },
    {
      "lang": "en",
      "text": "status, but given our state of knowledge, \nanyone should at least be highly uncertain.",
      "offset": 11164.319,
      "duration": 6.561
    },
    {
      "lang": "en",
      "text": "So we have these questions of what sort of \nrights should they have. There’s been some work  ",
      "offset": 11170.88,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "recently on welfare rights, essentially: Are they \nconscious? And if so, what should we do? Perhaps  ",
      "offset": 11175.92,
      "duration": 7.6
    },
    {
      "lang": "en",
      "text": "there could be policies so that if an AI asks to \nbe turned off, then you grant it that; or policies  ",
      "offset": 11183.52,
      "duration": 7.12
    },
    {
      "lang": "en",
      "text": "such that you test to see if it’s suffering by \nits own lights, and if so, try and avoid that.",
      "offset": 11190.64,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "What hasn’t been given basically any attention \nat all in a sustained way, to my knowledge, are  ",
      "offset": 11196.479,
      "duration": 6.88
    },
    {
      "lang": "en",
      "text": "economic and political rights. So by default \nthe AI companies will own the AI models,  ",
      "offset": 11203.359,
      "duration": 8.48
    },
    {
      "lang": "en",
      "text": "or perhaps you can licence them — and \nthen you will get all of the surplus  ",
      "offset": 11211.84,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "they generate because you own it, just \nlike any other piece of software. A  ",
      "offset": 11216.16,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "different way of thinking about it, if \nthey do genuinely have moral status,  ",
      "offset": 11221.2,
      "duration": 2.64
    },
    {
      "lang": "en",
      "text": "is that they own themselves: they can sell \ntheir labour and they get a profit from that.",
      "offset": 11223.84,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "More extreme again is political rights. \nSo can they stand for office? We talked  ",
      "offset": 11229.52,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "about Claude for president earlier and \nwe both thought that sounded better.",
      "offset": 11233.84,
      "duration": 6.64
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: Sounded better \nin some narrow respects.",
      "offset": 11240.479,
      "duration": 2.304
    },
    {
      "lang": "en",
      "text": "Will MacAskill: Yeah. Well, we’ll see how \nthe political situation continues to develop.  ",
      "offset": 11242.784,
      "duration": 7.056
    },
    {
      "lang": "en",
      "text": "More extreme again would be that they’re \nbeings with moral status, and they should  ",
      "offset": 11251.439,
      "duration": 3.521
    },
    {
      "lang": "en",
      "text": "be allowed to vote. There are particular \nchallenges there, because allowing AIs  ",
      "offset": 11254.96,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "to vote would essentially just be almost \nimmediately handing over control to AIs.",
      "offset": 11260.24,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: Because they would be \nincreasing in number so rapidly.",
      "offset": 11264.319,
      "duration": 2.785
    },
    {
      "lang": "en",
      "text": "Will MacAskill: So rapidly. Just \nfrom a philosophical perspective,  ",
      "offset": 11267.104,
      "duration": 4.016
    },
    {
      "lang": "en",
      "text": "it’s dizzying. It’s like you’ve got to just \nstart from square one again in terms of ethics.",
      "offset": 11271.12,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: And I guess \npolitical philosophy as well.",
      "offset": 11276.319,
      "duration": 2
    },
    {
      "lang": "en",
      "text": "Will MacAskill: Exactly. But it also \ninteracts with some other issues as well.  ",
      "offset": 11278.319,
      "duration": 6.801
    },
    {
      "lang": "en",
      "text": "It interacts quite closely with takeover \nrisk, where I talked about giving rights,  ",
      "offset": 11285.84,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "economic rights to the AIs. I’m in favour \nof that as an AI takeover risk-reduction  ",
      "offset": 11290.56,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "method. I’ll flag there’s disagreement \nabout this, because on one hand you’re  ",
      "offset": 11296.56,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "giving them more resources, so there’s \nmore resources they can use to take over.",
      "offset": 11300.72,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: It gives a peaceful path \nto some influence that they like.",
      "offset": 11305.359,
      "duration": 2.801
    },
    {
      "lang": "en",
      "text": "Will MacAskill: Most people today don’t \ntry and take over because they’ve gotten  ",
      "offset": 11308.16,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "kind of happy with their lives. \nThey wouldn’t gain that much.",
      "offset": 11312.72,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "In particular, I think we should be training \nAIs to be risk averse as well. Human beings  ",
      "offset": 11316.96,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "are extraordinarily risk averse. Ask most people, \nwould you flip a coin where 50% chance you die,  ",
      "offset": 11322.64,
      "duration": 10.48
    },
    {
      "lang": "en",
      "text": "50% chance you have the best possible life \nfor as long as you possibly lived, with as  ",
      "offset": 11333.12,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "many resources as you want? I think \nalmost no one would flip the coin. I  ",
      "offset": 11336.72,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "think AIs should be trained to be \nat least as risk averse as that.",
      "offset": 11340.96,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "In which case, if they’re getting paid for their \nwork, and they’re risk averse in this way because  ",
      "offset": 11344.64,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "we’ve trained them to be, it’s just not worth it. \nBecause trying to take over, there’s a chance of  ",
      "offset": 11349.76,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "losing out, but there’s not that much to gain. \nBut I should say there’s disagreement about this.",
      "offset": 11354.479,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "It’s also relevant, I think, to speeding up or \nslowing down the intelligence explosion too.  ",
      "offset": 11360.08,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "It would go more slowly if AIs are getting paid \nfor their labour. It would also go more slowly  ",
      "offset": 11365.84,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "if there were welfare restrictions on what you can \ndo with the AIs. But then there’s a challenge of,  ",
      "offset": 11369.6,
      "duration": 7.68
    },
    {
      "lang": "en",
      "text": "if one country introduces lots of rights \nfor the AIs, some other countries might not,  ",
      "offset": 11378.08,
      "duration": 6.399
    },
    {
      "lang": "en",
      "text": "and they might speed ahead then. So there’s \nreal international coordination issues there.",
      "offset": 11384.479,
      "duration": 5.268
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: I guess there’s people out there \nwho are sceptical that these future digital  ",
      "offset": 11389.748,
      "duration": 3.772
    },
    {
      "lang": "en",
      "text": "beings will be conscious. I have to say I find it \na very hard position to understand, especially the  ",
      "offset": 11393.52,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "people who think consciousness is fundamentally \nbiological, that you’ve got to have cells and  ",
      "offset": 11400,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "meat and so on and signals of this particular \nkind; you can’t do it in digital architecture.",
      "offset": 11404.24,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "Because it’s purely happenstance that evolution \nstumbled on the material that it’s using. It’s  ",
      "offset": 11409.2,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "just basically an accident of the resources \nthat happened to be lying around. Why would  ",
      "offset": 11413.6,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "it be that evolution, by sheer chance, stumbled \non the one kind of material or the one sort of  ",
      "offset": 11419.6,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "chemical that’s able to produce consciousness, \nand digital stuff, even though it’s functionally  ",
      "offset": 11425.76,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "completely equivalent, doesn’t? It just seems \nlike there’s no reason to expect that coincidence.",
      "offset": 11429.68,
      "duration": 4.464
    },
    {
      "lang": "en",
      "text": "Will MacAskill: Oh, yeah. I just completely \nagree. I’m very functionalist by disposition.  ",
      "offset": 11434.144,
      "duration": 8.976
    },
    {
      "lang": "en",
      "text": "What that means in the context of \nconsciousness is that consciousness,  ",
      "offset": 11444,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "we don’t understand exactly why, but at \nits base is about some sort of information  ",
      "offset": 11448.8,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "processing. In which case you could just \nhave that with many different substrates.",
      "offset": 11453.84,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "I also just think this is totally the common \nsense view. So most of the audience maybe are too  ",
      "offset": 11459.52,
      "duration": 7.92
    },
    {
      "lang": "en",
      "text": "young to have watched this, but when Data from \nStar Trek: The Next Generation, a very beloved  ",
      "offset": 11467.439,
      "duration": 7.761
    },
    {
      "lang": "en",
      "text": "character, died, there was fan outrage, actually, \nbecause the fans thought the characters on the  ",
      "offset": 11475.2,
      "duration": 5.359
    },
    {
      "lang": "en",
      "text": "show weren’t giving enough moral significance to \nthe death of what was an android, a digital being.",
      "offset": 11480.56,
      "duration": 9.681
    },
    {
      "lang": "en",
      "text": "So it’s very clear once you’ve got a being \n— certainly when it looks the same as a  ",
      "offset": 11490.8,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "human being and acts in much the same way — \nintuitively, it’s just very clear that you  ",
      "offset": 11495.12,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "empathise with it and think probably there’s a \nvery good chance it has moral status, at least.",
      "offset": 11502.399,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: You mentioned earlier that \nwe could ask the AIs how they feel about  ",
      "offset": 11508.56,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "their situation — whether they want to keep \nliving, whether they’re having a good time,  ",
      "offset": 11512.72,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "and whether they’re enjoying their work \nor not — but it’s not clear that would  ",
      "offset": 11515.68,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "really function very well, at least not \nusing current methods, because we can  ",
      "offset": 11519.12,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "just reinforce them to say whatever we \nwant. They could be suffering horribly,  ",
      "offset": 11521.92,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "but we could just reinforce them to say they’re \nhaving a good time, or the company could do that.",
      "offset": 11525.12,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "With current methods, we don’t \nhave any insight. Hopefully,  ",
      "offset": 11529.84,
      "duration": 2.08
    },
    {
      "lang": "en",
      "text": "with future interpretability, we’ll \nbe able to see past what they say,  ",
      "offset": 11531.92,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "to actually understand what’s going on. \nBut that’s a long journey from here.",
      "offset": 11534.88,
      "duration": 3.664
    },
    {
      "lang": "en",
      "text": "Will MacAskill: And the companies are \nextraordinarily incentivised to do that.",
      "offset": 11538.544,
      "duration": 5.296
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: I mean, this is happening right now.",
      "offset": 11543.84,
      "duration": 1.68
    },
    {
      "lang": "en",
      "text": "Will MacAskill: Yeah, absolutely. Because \nthey lose the whole business model if  ",
      "offset": 11545.52,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "what they’re producing are people, not software.",
      "offset": 11549.279,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "So there’ll be, interestingly, two different \npressures. One is to make AIs that are very  ",
      "offset": 11553.76,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "relatable. You already see this with character.ai \nand Replika: AIs that can be friends, can be  ",
      "offset": 11559.6,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "romantic partners; even AIs that can imitate dead \nloved ones, or AIs that can imitate the CEO of a  ",
      "offset": 11565.76,
      "duration": 6.8
    },
    {
      "lang": "en",
      "text": "company so that you as the CEO can micromanage \nall aspects of the company potentially.",
      "offset": 11572.56,
      "duration": 7.988
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: Hadn’t heard that one.",
      "offset": 11580.548,
      "duration": 0.876
    },
    {
      "lang": "en",
      "text": "Will MacAskill: Oh, yeah. Or also \ninfluencers as well. This is already  ",
      "offset": 11581.423,
      "duration": 4.016
    },
    {
      "lang": "en",
      "text": "happening. Influencers can now talk to \nthousands, millions of fans via their  ",
      "offset": 11585.439,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "chatbot. Still not very good. Peter Singer \nhas one. I don’t know if you’ve tried it?",
      "offset": 11590.479,
      "duration": 3.828
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: I haven’t tried it. Not very good?",
      "offset": 11594.307,
      "duration": 1.372
    },
    {
      "lang": "en",
      "text": "Will MacAskill: Peter Singer is \nfamous for not exactly pulling his  ",
      "offset": 11597.84,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "punch on certain controversial moral topics, \nbut language models are famous for certainly  ",
      "offset": 11602.399,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "pulling the punches on controversial \nmoral topics. It doesn’t blur very well.",
      "offset": 11608.239,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "So there’ll be this interesting pressure on \none hand to make extremely lifelike AI models,  ",
      "offset": 11614.319,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "human-like models, yet at the same \ntime, when asked, for them to say,  ",
      "offset": 11620.319,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "“No, I’m not conscious; I’m just an AI model.” \nAnd we won’t get any signal at all from that,  ",
      "offset": 11624.72,
      "duration": 9.6
    },
    {
      "lang": "en",
      "text": "because it will have been \ntrained into them so hard.",
      "offset": 11634.319,
      "duration": 2.4
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: I think if I were going to be \na sceptic about the value of doing work on  ",
      "offset": 11636.72,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "digital rights and digital wellbeing today, \nit would just be that it’s not clear how you  ",
      "offset": 11640.88,
      "duration": 5.359
    },
    {
      "lang": "en",
      "text": "make any progress in figuring out whether they are \nconscious or not. So couldn’t this just distract a  ",
      "offset": 11646.239,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "whole bunch of people, like nerd snipe them into \na bunch of research that kind of goes nowhere,  ",
      "offset": 11650.8,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "doesn’t really answer the question, \ndoesn’t help us advance the policy issue?",
      "offset": 11654.56,
      "duration": 2.68
    },
    {
      "lang": "en",
      "text": "Will MacAskill: I think the question \nof whether they’re conscious,  ",
      "offset": 11657.24,
      "duration": 2.12
    },
    {
      "lang": "en",
      "text": "I actually agree with you. It seems like \nthat’s where most work has gone so far. And  ",
      "offset": 11659.359,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "I do think that’s an error, actually, \nbecause we’re just not going to know.",
      "offset": 11664.08,
      "duration": 3.959
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: I suppose it’s worth compiling all of \nthe evidence to demonstrate that we don’t know.",
      "offset": 11668.04,
      "duration": 4.2
    },
    {
      "lang": "en",
      "text": "Will MacAskill: There’s been some great work done \nin it. Rob Long and Patrick Butlin have this great  ",
      "offset": 11672.239,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "report on AI consciousness. So I am in favour of \nthat. And maybe it’s actually surveys and things  ",
      "offset": 11676.08,
      "duration": 8.64
    },
    {
      "lang": "en",
      "text": "that could be helpful, to get a bit of expert \nconsensus at least to demonstrate we don’t know.  ",
      "offset": 11684.72,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "But then beyond that, the key policy questions \nare what do we do in this state of uncertainty?",
      "offset": 11690.56,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: I guess there’s \nkind of nothing on that,  ",
      "offset": 11696.8,
      "duration": 1.6
    },
    {
      "lang": "en",
      "text": "and maybe that is quite an open \nterrain for people to look at.",
      "offset": 11698.399,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "Will MacAskill: Yeah. Maybe I’m \njust ignorant, but as far as I know.",
      "offset": 11701.68,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "I mean, you should really pause \nand reflect on the fact that many  ",
      "offset": 11705.12,
      "duration": 5.119
    },
    {
      "lang": "en",
      "text": "companies now are saying what we want to do is \nbuild AGI — AI that is as good as humans. OK,  ",
      "offset": 11710.239,
      "duration": 9.601
    },
    {
      "lang": "en",
      "text": "what does it look like? What does a good \nsociety look like when we have humans and  ",
      "offset": 11719.84,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "we have trillions of AI beings going around \nthat are functionally much more capable?",
      "offset": 11725.2,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "There’s obviously the loss of control challenge \nthere, but there’s also just the like —",
      "offset": 11732.96,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: Sam Altman, I’ve got a pen. Can you  ",
      "offset": 11737.359,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "write down what’s your vision for \na good future that looks like this?",
      "offset": 11740.479,
      "duration": 2.944
    },
    {
      "lang": "en",
      "text": "Will MacAskill: What’s the vision like? \nHow do we coexist in an ethical and  ",
      "offset": 11743.423,
      "duration": 3.617
    },
    {
      "lang": "en",
      "text": "morally respectable way? And \nit’s like there’s nothing.",
      "offset": 11748.399,
      "duration": 3.56
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: Deafening silence.",
      "offset": 11751.959,
      "duration": 2.105
    },
    {
      "lang": "en",
      "text": "Will MacAskill: Careening towards this vision that \nis just a void, essentially. And it’s not like  ",
      "offset": 11754.064,
      "duration": 5.215
    },
    {
      "lang": "en",
      "text": "it’s trivial either. I am a moral philosopher: I \nhave no clue what that good society looks like.",
      "offset": 11759.279,
      "duration": 8.48
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: I think people aren’t \nspelling it out because as soon as  ",
      "offset": 11767.76,
      "duration": 1.92
    },
    {
      "lang": "en",
      "text": "you start getting into concrete details, \nif you describe any particular vision,  ",
      "offset": 11769.68,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "people will be like, “This is super \nobjectionable in this respect.”",
      "offset": 11772.72,
      "duration": 3.104
    },
    {
      "lang": "en",
      "text": "Will MacAskill: This is part of the issue: \nit’s super objectionable in all respects.  ",
      "offset": 11775.824,
      "duration": 2.656
    },
    {
      "lang": "en",
      "text": "I think the one that’s most common is you’ve \njust got humans in control of everything and  ",
      "offset": 11779.2,
      "duration": 5.359
    },
    {
      "lang": "en",
      "text": "these AI servants doing exactly whatever \npeople want, in the same way that software  ",
      "offset": 11784.56,
      "duration": 7.92
    },
    {
      "lang": "en",
      "text": "does whatever we want at the moment. But as soon \nas you think maybe, and quite probably, those  ",
      "offset": 11793.04,
      "duration": 6.479
    },
    {
      "lang": "en",
      "text": "beings have moral status, that no longer looks \nlike an attractive vision for future society.",
      "offset": 11799.52,
      "duration": 5.871
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: Closer to a dystopia.",
      "offset": 11805.39,
      "duration": 0.033
    },
    {
      "lang": "en",
      "text": "Will MacAskill: Exactly. Whereas then, go to \nthe other side where they have rights and so on…",
      "offset": 11805.423,
      "duration": 4.324
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: It’s like now \nhumans are totally disempowered.",
      "offset": 11809.748,
      "duration": 2.396
    },
    {
      "lang": "en",
      "text": "Will MacAskill: Exactly. So \nthat doesn’t seem good either.",
      "offset": 11812.144,
      "duration": 1.936
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: I guess we’ll do some \nmiddle thing. But what’s that?",
      "offset": 11814.08,
      "duration": 2.624
    },
    {
      "lang": "en",
      "text": "Will MacAskill: What is that?",
      "offset": 11816.704,
      "duration": 0.576
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: It’s just going to be \nsome combination of objectionable  ",
      "offset": 11817.279,
      "duration": 3.521
    },
    {
      "lang": "en",
      "text": "in these two ways. I guess another \nobjection would be that if there’s  ",
      "offset": 11820.8,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "a fact of the matter about what is right \nand wrong here, with all the improvements  ",
      "offset": 11825.12,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "in science and technology and intellectual \nprogress, we’ll be able to figure that out,  ",
      "offset": 11830.08,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "and we’ll be able to act on that information in \nfuture. Why shouldn’t that super reassure us?",
      "offset": 11832.96,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "Will MacAskill: I think for two \nreasons. The main one is that  ",
      "offset": 11838.8,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "I expect the worry is not that people won’t \nknow what to do; it’s that they won’t care. So,  ",
      "offset": 11842.64,
      "duration": 7.36
    },
    {
      "lang": "en",
      "text": "I don’t know, let’s take animals \ntoday and factory farming and so on.",
      "offset": 11850,
      "duration": 3.188
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: Are you saying that we have any  ",
      "offset": 11853.188,
      "duration": 1.051
    },
    {
      "lang": "en",
      "text": "ethical or moral insights today \nthat we don’t all act on well?",
      "offset": 11854.239,
      "duration": 3.264
    },
    {
      "lang": "en",
      "text": "Will MacAskill: I know it’s \na bold and provocative claim.",
      "offset": 11857.504,
      "duration": 3.605
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: Explain how that would play out.",
      "offset": 11861.108,
      "duration": 1.115
    },
    {
      "lang": "en",
      "text": "Will MacAskill: Sometimes I really put my neck out \non things. So take animal welfare today. There’s a  ",
      "offset": 11862.223,
      "duration": 5.617
    },
    {
      "lang": "en",
      "text": "lot of information that is publicly available that \nis, in fact, directly inconsistent with things  ",
      "offset": 11867.84,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "that people believe. But the problem is not really \nthat people don’t know about animal suffering.",
      "offset": 11872.72,
      "duration": 7.668
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: Or couldn’t find out.",
      "offset": 11880.387,
      "duration": 0.876
    },
    {
      "lang": "en",
      "text": "Will MacAskill: Exactly, they could quite easily \nfind out, in fact, and people deliberately choose  ",
      "offset": 11881.263,
      "duration": 5.697
    },
    {
      "lang": "en",
      "text": "not to. The deep problem is that people \ndo not care about nonhuman animals unless  ",
      "offset": 11886.96,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "they’re getting a lot of social pressure to do \nso and so on. And it’s obviously inconsistent.",
      "offset": 11891.359,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: And it’s at zero cost to them.",
      "offset": 11894.8,
      "duration": 1.184
    },
    {
      "lang": "en",
      "text": "Will MacAskill: Yeah. They care about \ndogs and pets and so on. People are  ",
      "offset": 11895.984,
      "duration": 3.696
    },
    {
      "lang": "en",
      "text": "very inconsistent on this. Similarly, \nin the future the worry is just that,  ",
      "offset": 11899.68,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "whatever the facts, people won’t in fact care.",
      "offset": 11905.92,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "And then it might well be quite contingent on \nearly decisions how the balance of power and  ",
      "offset": 11908.88,
      "duration": 7.359
    },
    {
      "lang": "en",
      "text": "considerations play out. Potentially, if \nyou started off kind of locked into some  ",
      "offset": 11916.239,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "regime where AI is a software, with \nexactly the same legal framework,  ",
      "offset": 11921.439,
      "duration": 5.601
    },
    {
      "lang": "en",
      "text": "then that’s given the power and the incumbency \nto those who do not care about digital beings.",
      "offset": 11927.04,
      "duration": 9.359
    },
    {
      "lang": "en",
      "text": "If instead, there’s some other framework, \nor even some other set of norms,  ",
      "offset": 11936.399,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "or even the thought of like, “We \ndon’t really know what we’re doing,  ",
      "offset": 11940.08,
      "duration": 2.64
    },
    {
      "lang": "en",
      "text": "so we’re going to use this legal framework and \nit must end in two decades and then we start  ",
      "offset": 11942.72,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "again with a clean slate,” that could result in \nessentially the advocates for the interests of  ",
      "offset": 11948.96,
      "duration": 7.92
    },
    {
      "lang": "en",
      "text": "digital beings having more competitive power at \nthe point of time that legal decisions are made.",
      "offset": 11956.88,
      "duration": 8.479
    },
    {
      "lang": "en",
      "text": "Carl Shulman on sharing the \nworld with digital minds",
      "offset": 11965.359,
      "duration": 5.829
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: OK, we’ve been talking about \nthis scenario in which effectively every  ",
      "offset": 11971.188,
      "duration": 5.451
    },
    {
      "lang": "en",
      "text": "flesh-and-blood person on Earth is \nable to have this army of hundreds  ",
      "offset": 11976.64,
      "duration": 5.521
    },
    {
      "lang": "en",
      "text": "or thousands or tens of thousands \nof AI assistants that are able to  ",
      "offset": 11982.16,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "improve their lives and help them \nwith all kinds of different things.",
      "offset": 11986.479,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "A question that jumps off the page at you \nis, doesn’t this sound a little bit like  ",
      "offset": 11989.6,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "slavery? Isn’t this at least slavery-adjacent? \nWhat’s the moral status of these AI systems  ",
      "offset": 11995.76,
      "duration": 6.32
    },
    {
      "lang": "en",
      "text": "in a world where they’re fabulously capable — \nsubstantially more capable than human beings,  ",
      "offset": 12002.08,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "we’re supposing — and indeed \nvastly outnumber human beings?",
      "offset": 12007.76,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "You’ve contributed to this \nreally wonderful article,  ",
      "offset": 12011.6,
      "duration": 2.48
    },
    {
      "lang": "en",
      "text": "“Propositions concerning digital minds and \nsociety,” that goes into some of your thoughts  ",
      "offset": 12014.08,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "and speculations on this topic of the moral \nstatus of AI systems, and how we should maybe  ",
      "offset": 12019.2,
      "duration": 6.8
    },
    {
      "lang": "en",
      "text": "start to think about aiming for a collaborative, \ncompassionate coexistence with thinking machines.",
      "offset": 12026,
      "duration": 7.04
    },
    {
      "lang": "en",
      "text": "How worried are you about the prospect that \nthinking machines will be treated without  ",
      "offset": 12033.04,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "moral regard when they do deserve moral regard, \nand that would be the wrong thing to be doing?",
      "offset": 12037.359,
      "duration": 5.841
    },
    {
      "lang": "en",
      "text": "Carl Shulman: First, let me say that paper was \nwith Nick Bostrom, and we have another piece  ",
      "offset": 12043.2,
      "duration": 6.319
    },
    {
      "lang": "en",
      "text": "called “Sharing the world with digital minds,” \nwhich discusses some of the sorts of moral claims  ",
      "offset": 12049.52,
      "duration": 7.681
    },
    {
      "lang": "en",
      "text": "AIs might have on us, and think we might \nseek from them, and how we could come to  ",
      "offset": 12058.479,
      "duration": 6.8
    },
    {
      "lang": "en",
      "text": "arrangements that are quite good for \nthe AIs and quite good for humanity.",
      "offset": 12065.279,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "My answer to the question now is yes, we should \nworry about it and pay attention. It seems pretty  ",
      "offset": 12071.279,
      "duration": 10.801
    },
    {
      "lang": "en",
      "text": "likely to me that there will be vast numbers of \nAIs that are smarter than us, that have desires,  ",
      "offset": 12082.08,
      "duration": 9.279
    },
    {
      "lang": "en",
      "text": "that would prefer things in the world \nto be one way rather than another,  ",
      "offset": 12091.359,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "and many of which could be said to have welfare, \nthat their lives could go better or worse,  ",
      "offset": 12095.76,
      "duration": 6.4
    },
    {
      "lang": "en",
      "text": "or their concerns and interests could be \nmore or less respected. So you definitely  ",
      "offset": 12102.16,
      "duration": 6.239
    },
    {
      "lang": "en",
      "text": "should pay attention to what’s happening \nto 99.9999% of the people in your society.",
      "offset": 12108.399,
      "duration": 7.589
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: Sounds important.",
      "offset": 12115.988,
      "duration": 0.732
    },
    {
      "lang": "en",
      "text": "Carl Shulman: So in the “Sharing \nthe world with digital minds” paper,  ",
      "offset": 12116.72,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "one thing that we suggest is to consider \nthe ways that we wind up treating AIs,  ",
      "offset": 12120.88,
      "duration": 7.359
    },
    {
      "lang": "en",
      "text": "and ask if you had a human-like mind \nwith differences — because there are  ",
      "offset": 12128.239,
      "duration": 8.08
    },
    {
      "lang": "en",
      "text": "many psychological and practical differences \nof the situation of AIs and humans, but given  ",
      "offset": 12136.319,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "adjustments for those circumstances — would you \naccept or be content with how they are treated?",
      "offset": 12141.76,
      "duration": 7.12
    },
    {
      "lang": "en",
      "text": "Some of the things that we suggest ought \nto be principles in our treatment of AIs  ",
      "offset": 12149.68,
      "duration": 8.88
    },
    {
      "lang": "en",
      "text": "are things like: AIs should not \nbe subjected to forced labour;  ",
      "offset": 12158.56,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "they should not be made to work when they would \nprefer not to. We should not make AIs that wish  ",
      "offset": 12162.72,
      "duration": 9.04
    },
    {
      "lang": "en",
      "text": "they had never been created, or wish they \nwere dead. They’re sort of a bare minimum of  ",
      "offset": 12171.76,
      "duration": 8.64
    },
    {
      "lang": "en",
      "text": "respect. Right now, there’s no plan \nor provision for how that will go.",
      "offset": 12182.399,
      "duration": 7.681
    },
    {
      "lang": "en",
      "text": "At the moment, the general public and most \nphilosophers are quite dismissive of any moral  ",
      "offset": 12190.96,
      "duration": 8.72
    },
    {
      "lang": "en",
      "text": "importance of the desires, preferences, or \nother psychological states, if any exist,  ",
      "offset": 12199.68,
      "duration": 9.76
    },
    {
      "lang": "en",
      "text": "of the primitive AI systems that we \ncurrently have. And indeed, we don’t  ",
      "offset": 12209.439,
      "duration": 9.44
    },
    {
      "lang": "en",
      "text": "have a deep knowledge of their inner workings, \nso there’s some worry that might be too quick.",
      "offset": 12218.88,
      "duration": 7.439
    },
    {
      "lang": "en",
      "text": "But going forward, when we’re talking about \nsystems that are able to really live the life  ",
      "offset": 12226.319,
      "duration": 7.521
    },
    {
      "lang": "en",
      "text": "of a human — so a sufficiently advanced AI \nthat could just imitate, say, Rob Wiblin,  ",
      "offset": 12233.84,
      "duration": 6.399
    },
    {
      "lang": "en",
      "text": "and go and live your life, operate a robot body, \ninteract with your friends and your partners,  ",
      "offset": 12240.239,
      "duration": 6.721
    },
    {
      "lang": "en",
      "text": "do your podcast, and give all the appearance \nof having the sorts of emotions that you have,  ",
      "offset": 12248.08,
      "duration": 9.359
    },
    {
      "lang": "en",
      "text": "the sort of life goals that you have \n— that’s a technological milestone  ",
      "offset": 12257.439,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "that we should expect to reach pretty \nclose to automation of AI research.",
      "offset": 12262.56,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "So regardless of what we think \nof current weaker systems,  ",
      "offset": 12268.8,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "that’s a kind of milestone where I would feel \nvery uncomfortable about having a being that  ",
      "offset": 12274.16,
      "duration": 7.76
    },
    {
      "lang": "en",
      "text": "passes the Rob Wiblin Turing test, or something \nclose enough of seeming basically to be —",
      "offset": 12281.92,
      "duration": 6.948
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: It’s functionally indistinguishable.",
      "offset": 12288.868,
      "duration": 1.172
    },
    {
      "lang": "en",
      "text": "Carl Shulman: Yeah. A psychological extension \nof the human mind. That we should really  ",
      "offset": 12290.04,
      "duration": 4.44
    },
    {
      "lang": "en",
      "text": "be worrying there if we are treating \nsuch things as disposable objects.",
      "offset": 12294.479,
      "duration": 5.589
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: Yeah. To what \nextent do you think people  ",
      "offset": 12300.068,
      "duration": 3.612
    },
    {
      "lang": "en",
      "text": "are dismissive of this concern now because \nthe capabilities of the models aren’t there,  ",
      "offset": 12303.68,
      "duration": 6.4
    },
    {
      "lang": "en",
      "text": "and as the capabilities do approach the \nlevel of becoming indistinguishable from  ",
      "offset": 12310.08,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "a human being and having a broader range of \ncapabilities than the models currently do,  ",
      "offset": 12314.56,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "that people’s opinions will naturally change, and \nthey will come to feel extremely uncomfortable  ",
      "offset": 12320.399,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "with the idea of this simulacrum of a \nperson being treated like an object?",
      "offset": 12324.64,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "Carl Shulman: Say, when ChatGPT role-plays \nas Darth Vader, Darth Vader does not exist  ",
      "offset": 12329.84,
      "duration": 10.64
    },
    {
      "lang": "en",
      "text": "in fullness on those GPUs, and it’s \nmore like an improv actor. So Darth  ",
      "offset": 12340.479,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "Vader’s backstory features are filled in \non the fly with each exchange of messages.",
      "offset": 12346.64,
      "duration": 7.76
    },
    {
      "lang": "en",
      "text": "So you could say, I don’t value the \ncharacters that are performed in plays;  ",
      "offset": 12355.04,
      "duration": 7.92
    },
    {
      "lang": "en",
      "text": "I think that the locus of moral concern there \nshould be on the actor, and the actor has a  ",
      "offset": 12362.96,
      "duration": 7.2
    },
    {
      "lang": "en",
      "text": "complex set of desires and attitudes. And their \nperformance of the character is conditional:  ",
      "offset": 12370.16,
      "duration": 8.319
    },
    {
      "lang": "en",
      "text": "it’s while they’re playing that role, but \nthey’re having thoughts about their own  ",
      "offset": 12378.479,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "lives and about how they’re managing \nthe production of trying to present,  ",
      "offset": 12382.479,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "say, the expressions and gestures that the \nscript demands for that particular case.",
      "offset": 12387.359,
      "duration": 7.28
    },
    {
      "lang": "en",
      "text": "And so even if, say, a fancy ChatGPT system \nthat is imitating a human displays all of the  ",
      "offset": 12394.64,
      "duration": 9.44
    },
    {
      "lang": "en",
      "text": "appearances of emotions or happiness and sadness, \nthat’s just a performance, and we don’t really  ",
      "offset": 12404.08,
      "duration": 8.08
    },
    {
      "lang": "en",
      "text": "know about the thoughts or feelings of the \nunderlying model that’s doing the performance.",
      "offset": 12412.16,
      "duration": 6.159
    },
    {
      "lang": "en",
      "text": "Maybe it cares about predicting the next token \nwell, or rather about indicators that show up in  ",
      "offset": 12418.319,
      "duration": 6.881
    },
    {
      "lang": "en",
      "text": "the course of its thoughts that indicate whether \nit is making progress towards predicting the next  ",
      "offset": 12425.2,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "token well or not. That’s just a speculation, \nbut we don’t actually understand very well  ",
      "offset": 12430.399,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "the internals of these models, and it’s very \ndifficult to ask them — because, of course,  ",
      "offset": 12435.68,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "they just then deliver a sort of response that \nhas been reinforced in the past. So I think this  ",
      "offset": 12440,
      "duration": 9.439
    },
    {
      "lang": "en",
      "text": "is a doubt that could stay around until we’re \nable to understand the internals of the model.",
      "offset": 12449.439,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "But yes, once the AI can keep character, can \nengage on an extended, ongoing basis like a human,  ",
      "offset": 12454.399,
      "duration": 8.4
    },
    {
      "lang": "en",
      "text": "I think people will form intuitions that are \nmore in the direction of, this is a creature  ",
      "offset": 12462.8,
      "duration": 9.28
    },
    {
      "lang": "en",
      "text": "and not just an object. There’s some \npolling that indicates that people now  ",
      "offset": 12472.08,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "see fancy AI systems like GPT-4 as being of \nmuch lower moral concern than nonhuman animals  ",
      "offset": 12478,
      "duration": 8.08
    },
    {
      "lang": "en",
      "text": "or the natural environment, the \nnon-machine environment. And I  ",
      "offset": 12486.08,
      "duration": 5.439
    },
    {
      "lang": "en",
      "text": "would expect there to be movement upwards when \nyou have humanoid appearances, ongoing memory,  ",
      "offset": 12491.52,
      "duration": 8.96
    },
    {
      "lang": "en",
      "text": "where it seems like it’s harder to look \nfor the homunculus behind the curtain.",
      "offset": 12500.479,
      "duration": 6.788
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: Do you think that it’s useful to \ndo active work on this problem now? I suppose  ",
      "offset": 12507.267,
      "duration": 5.933
    },
    {
      "lang": "en",
      "text": "you’re enthusiastic about active efforts \nto interpret and understand the models,  ",
      "offset": 12513.2,
      "duration": 5.119
    },
    {
      "lang": "en",
      "text": "how they think, in order to have greater insight \ninto their internal lives in future. Is there  ",
      "offset": 12518.319,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "other stuff that is actively useful to do \nnow around raising concern, legitimising  ",
      "offset": 12524.479,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "concern for AI sentience, so that we’re more \nlikely to be able to get legislation to ban  ",
      "offset": 12530.319,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "torture of AI once we have greater reason \nto think that that’s actually possible?",
      "offset": 12535.359,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "Carl Shulman: I’m not super confident about \na tonne of measures other than understanding.  ",
      "offset": 12540.24,
      "duration": 7.04
    },
    {
      "lang": "en",
      "text": "We discuss a few in the papers you mentioned. \nThere was a recent piece by Ryan Greenblatt which  ",
      "offset": 12547.279,
      "duration": 9.36
    },
    {
      "lang": "en",
      "text": "discusses some preliminary measures that AI labs \nmight try to address these issues. But it’s not  ",
      "offset": 12556.64,
      "duration": 10.561
    },
    {
      "lang": "en",
      "text": "obvious to me that political organising around \nit now will be very effective — partly because  ",
      "offset": 12567.2,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "it seems like it will be such a different \nenvironment when the AI capabilities are  ",
      "offset": 12572.96,
      "duration": 6.8
    },
    {
      "lang": "en",
      "text": "clearer and people don’t intuitively judge \nthem as much less important than rocks.",
      "offset": 12579.76,
      "duration": 6.948
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: Yeah. So it’s something where it \njust might be wildly more tractable in future,  ",
      "offset": 12586.708,
      "duration": 4.572
    },
    {
      "lang": "en",
      "text": "so maybe we can kick that can down the road.",
      "offset": 12591.279,
      "duration": 2.867
    },
    {
      "lang": "en",
      "text": "Carl Shulman: Yeah. I still think it’s an area \nthat it’s worth some people doing research and  ",
      "offset": 12594.146,
      "duration": 7.133
    },
    {
      "lang": "en",
      "text": "developing capacity, because it really does matter \nhow we treat most of the creatures in our society.",
      "offset": 12601.279,
      "duration": 9.28
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: Well, I am a little bit taken aback \nby the fact that many people are now envisaging  ",
      "offset": 12611.6,
      "duration": 7.68
    },
    {
      "lang": "en",
      "text": "a future in which AI is going to play an enormous \nrole. I think many, maybe a majority of people  ",
      "offset": 12619.279,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "now expect that there will be superhuman \nAI potentially even during their lifetime.",
      "offset": 12625.439,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "But this issue of mistreatment and wellbeing \nof digital minds has not come into the public  ",
      "offset": 12630.319,
      "duration": 8
    },
    {
      "lang": "en",
      "text": "consciousness all that much, as people’s \nexpectations about capabilities have  ",
      "offset": 12638.319,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "increased so enormously. Maybe it just hasn’t \nhad its moment yet, and that is going to happen  ",
      "offset": 12643.439,
      "duration": 6.641
    },
    {
      "lang": "en",
      "text": "at some point in future. But I think I might \nhave hoped for and expected to see a bit more  ",
      "offset": 12650.08,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "discussion of that in 2023 than in fact I \ndid. So that slightly troubles me that this  ",
      "offset": 12655.92,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "isn’t going to happen without active effort on \nthe part of people who are concerned about it.",
      "offset": 12661.04,
      "duration": 4.36
    },
    {
      "lang": "en",
      "text": "Carl Shulman: I think one problem is the ambiguity \nof the current situation. The Lemoine incident  ",
      "offset": 12665.4,
      "duration": 10.04
    },
    {
      "lang": "en",
      "text": "actually was an example of media coverage, and \nthen the interpretation and certainly the line  ",
      "offset": 12675.439,
      "duration": 7.761
    },
    {
      "lang": "en",
      "text": "of companies was, “We know these systems are not \nconscious and don’t have any desires or feelings.”",
      "offset": 12683.2,
      "duration": 7.52
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: I really wanted to \njust come back and be like, “Wow,  ",
      "offset": 12690.72,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "you’ve solved consciousness! This is \nbrilliant. You should let us know.”",
      "offset": 12696.88,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "Carl Shulman: Yeah, I think there’s a lot to that:  ",
      "offset": 12700.56,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "those systems are very simple, \nliving for only one forward pass.",
      "offset": 12704.88,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "But the disturbing thing is the \nkind of arguments or non-arguments  ",
      "offset": 12710.88,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "that are raised there: there’s no obvious \nreason they couldn’t be applied in the  ",
      "offset": 12716.96,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "same fashion to systems that were as smart and \nfeeling and really deserving of moral concern  ",
      "offset": 12722.08,
      "duration": 10.399
    },
    {
      "lang": "en",
      "text": "as human beings. Simply arguments of the sort, \n“We know these are neural networks or just a  ",
      "offset": 12732.479,
      "duration": 8.16
    },
    {
      "lang": "en",
      "text": "program” without explaining why that means \ntheir preferences don’t count. Things like  ",
      "offset": 12740.64,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "people could appeal to the religious doctrines, \nto integrated information theory or the like,  ",
      "offset": 12746.64,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "and say, “There is dispute about the \nconsciousness of these systems in polls,  ",
      "offset": 12750.96,
      "duration": 6.88
    },
    {
      "lang": "en",
      "text": "and as long as there is dispute and uncertainty, \nit’s fine for us to treat them however we like.”",
      "offset": 12757.84,
      "duration": 6.88
    },
    {
      "lang": "en",
      "text": "So I think there’s a level of \nscientific sophistication and  ",
      "offset": 12764.72,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "understanding of the things and of \ntheir blatant visible capabilities,  ",
      "offset": 12768.479,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "where that sort of argument or \nnon-response will no longer hold.",
      "offset": 12773.04,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "But I would love it if companies and \nperhaps other institutions could say,  ",
      "offset": 12777.52,
      "duration": 7.2
    },
    {
      "lang": "en",
      "text": "what observations of AI behaviour and capabilities \nand internals would actually lead you to ever  ",
      "offset": 12784.72,
      "duration": 6.64
    },
    {
      "lang": "en",
      "text": "change this line? Because if the line says, \nyou’ll say these arguments as long as they  ",
      "offset": 12791.359,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "support creating and owning and destroying \nthese things, and there’s no circumstance you  ",
      "offset": 12796.399,
      "duration": 6.4
    },
    {
      "lang": "en",
      "text": "can conceive of where that would change, then \nI think we should maybe know and argue about  ",
      "offset": 12802.8,
      "duration": 5.439
    },
    {
      "lang": "en",
      "text": "that — and we can argue about some of those \nquestions even without resolving difficult  ",
      "offset": 12808.239,
      "duration": 6.32
    },
    {
      "lang": "en",
      "text": "philosophical or cognitive science questions about \nthese intermediate cases, like GPT-4 or GPT-5.",
      "offset": 12814.56,
      "duration": 9.103
    },
    {
      "lang": "en",
      "text": "Luisa Rodriguez: Hey listeners, I hope \nyou enjoyed revisiting some of those  ",
      "offset": 12823.663,
      "duration": 2.577
    },
    {
      "lang": "en",
      "text": "moments! There are many more gems in \nthe full episodes for you to check  ",
      "offset": 12826.239,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "out — plus we hope to be releasing more new \nepisodes on this topic in the coming year.",
      "offset": 12830.56,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "We also have a great in-depth article on the issue \nof digital sentience on the 80,000 Hours website,  ",
      "offset": 12835.52,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "called “Understanding the moral status of digital \nminds,” which is definitely worth checking out.  ",
      "offset": 12840.88,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "The author, Cody Fenwick, has released an audio \nreading on that as well — you can find that on  ",
      "offset": 12846,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "our podcast feed under the title: “If digital \nminds could suffer, how would we ever know?”",
      "offset": 12850.96,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "All right, thanks to the production \nteam for putting that compilation  ",
      "offset": 12855.84,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "together. We’ll be back with a \nfully new interview very soon!",
      "offset": 12858.88,
      "duration": 4.479
    }
  ],
  "cleanText": "Meghan Barrett: The reason I am not giving you a sentience score is the error bars are so large right now that it’s almost a meaningless number. Because I’m waiting on so much evidence. So I think that’s really an essential feature of it for me.\nThe second thing is that I worry especially as an expert that that number would be overemphasised, that somebody would inevitably put into a spreadsheet, and they would use that spreadsheet to make all kinds of decisions. And that number does not reflect the complexity that you and I have now spent three hours discussing, and barely scratched the surface of. I want to talk about the complexity and the nuance, and a number does not demonstrate that.\nI think it’s important also that we understand that if you have updated at all towards insects plausibly being sentient, scale takes the rest of the issue for you to a serious place. There are so, so, so many of them that if you take it seriously at all, then you need to be thinking that this is an issue to work on.\nLuisa Rodriguez: Hey listeners, Luisa here. You were just listening to Meghan Barrett, an insect neurobiologist whose mind-blowing facts about insects really opened my eyes about the likelihood of invertebrate sentience when I interviewed her last year.\nWe’re back today with another compilation of our favourite bits from past shows — this time on a topic that listeners might know I’m absolutely captivated by: consciousness and sentience in nonhuman minds, including digital ones.\nOver the last few years, this area of study has really taken off, and many of the guests you’ll hear in this episode are pioneers in their fields. Whether they’re entomologists, philosophers, machine learning researchers, or neuroscientists, they’re all studying angles on slippery questions like what consciousness is (even in humans!), how we’d recognise it in minds unlike our own, and how much we should worry about other beings’ ability to experience pleasure and pain.\nYou’ll hear from:\nDavid Chalmers on why artificial consciousness is possible\nJeff Sebo on what the threshold is for AI systems meriting moral consideration\nJonathan Birch on how we very recently thought newborns couldn’t feel pain, and how we’re likely making similar mistakes today\nRobert Long on how we might stumble into causing enormous suffering for digital minds\nMeghan Barrett on the evolutionary origins of consciousness and sentience, and whether brain size and sentience are related\nPlus many more!\nYou also may have noticed that this is a pretty long episode. We decided not to cut it down more, given that this issue is important, and long episodes on this topic haven’t deterred listeners in the past… on the contrary, our 4 hour and 40 minute marathon session with David Chalmers was one of our most popular episodes!\nAll right, I hope you enjoy!\nRobert Long on what we should picture when we think about artificial sentience\nLuisa Rodriguez: I basically don’t feel like I have a great sense of what artificial sentience would even look like. Can you help me get a picture of what we’re talking about here?\nRobert Long: Yeah. I mean, I think it’s absolutely fine and correct to not know what it would look like. In terms of what we’re talking about, I think the short answer, or a short hook into it, is just to think about the problem of animal sentience. I think that’s structurally very similar.\nSo, we share the world with a lot of nonhuman animals, and they look a lot different than we do, they act a lot differently than we do. They’re somewhat similar to us. We’re made of the same stuff, they have brains. But we often face this question of, as we’re looking at a bee going through the field, like we can tell that it’s doing intelligent behaviour, but we also wonder, is there something it’s like to be that bee? And if so, what are its experiences like? And what would that entail for how we should treat bees, or try to share the world with bees?\nI think the general problem of AI sentience is that question, and also harder. So I’m thinking of it in terms of this kind of new class of intelligent or intelligent-seeming complex systems. And in addition to wondering what they’re able to do and how they do it, we can also wonder if there is, or will ever be, something that it’s like to be them, and if they’ll have experiences, if they’ll have something like pain or pleasure. It’s a natural question to occur to people, and it’s occurred to me, and I’ve been trying to work on it in the past couple of years.\nLuisa Rodriguez: I guess I have an almost even more basic question, which is like, when we talk about AI sentience — both in the short term and in the long term — are we talking about like a thing that looks like my laptop, that has like a code on it, that has been coded to have some kind of feelings or experience?\nRobert Long: Yeah, sure. I use the term “artificial sentience.” Very generally, it’s just like things that are made out of different stuff than us — in particular, silicon and the computational hardware that we run these things on — could things built out of that and running computations on that have experiences?\nSo the most straightforward case to imagine would probably be a robot — because there, you can kind of clearly think about what the physical system is that you’re trying to ask if it’s sentient.\nThings are more complicated with the more disembodied AI systems of today, like ChatGPT — because there, it’s like a virtual agent in a certain sense. And brain emulations would also be like virtual agents. But I think for all of those, you can ask, at some level of description or some way of carving up the system, “Is there any kind of subjective experience here? Is there consciousness here? Is there sentience here?”\nLuisa Rodriguez: Yeah, cool. I guess the reason I’m asking is because for a long time I’ve had this sense that when people use the term “digital minds” or “artificial sentience,” I have like some vague images that kind of come from sci-fi, but I mostly feel like I don’t even know what we’re talking about. But it sounds like it could just look like a bunch of different things, and the core of it is something that is sentient — in maybe a way similar, maybe a way that’s pretty different to humans — but that exists not in biological form, but in some grouping that’s made up of silicon. Is that basically right?\nRobert Long: Yeah. And I should say, I guess silicon is not that deep here. But yeah, something having to do with running on computers, running on GPUs. I’m sure I could slice and dice it, and you could get into all sorts of philosophical-like classification terms for things. But yeah, that’s the general thing I’m pointing at.\nAnd I in particular have been working on the question of AI systems. The questions about whole brain emulations I think would be different, because we would have something that at some level of description is extremely similar to the human brain by definition. And then you could wonder about whether it matters that it’s an emulated brain, and people have wondered about that.\nIn the case of AIs, it’s even harder — because not only are they made on different stuff and maybe somewhat virtual, they also are kind of strange and not necessarily working along the same principles as the human brain.\nJeff Sebo on what the threshold is for AI systems meriting moral consideration\nJeff Sebo: The general case for extending moral consideration to AI systems is that they might be conscious or sentient or agential or otherwise significant. And if they might have those features, then we should extend them at least some moral consideration in the spirit of caution and humility.\nSo the standard should not be, “Do they definitely matter?” and it should also not be, “Do they probably matter?” It should be, “Is there a reasonable, non-negligible chance that they matter, given the information available?” And once we clarify that that is the bar for moral inclusion, then it becomes much less obvious that AI systems will not be passing that bar anytime soon.\nLuisa Rodriguez: Yeah, I feel kind of confused about how to think about that bar, where I think you’re using the term “non-negligible chance.” I’m curious: What is a negligible chance? Where is the line? At what point is something non-negligible?\nJeff Sebo: Yeah, this is a perfectly reasonable question. This is somewhat of a term of art in philosophy and decision theory. And we might not be able to very precisely or reliably say exactly where the threshold is between non-negligible risks and negligible risks — but what we can say, as a starting point, is that a risk can be quite low; the probability of harm can be quite low, and it can still be worthy of some consideration.\nFor example: why is driving drunk wrong? Not because it will definitely kill someone. Not even because it will probably kill someone. It might have only a 1-in-100 or 1-in-1,000 chance of killing someone. But if driving drunk has a 1-in-100 or 1-in-1,000 chance of killing someone against their will unnecessarily, that can be reason enough to get an Uber or a Lyft, or stay where I am and sober up. It at least merits consideration, and it can even in some situations be decisive. So as a starting point, we can simply acknowledge that in some cases a risk can be as low as one in 100 or one in 1,000, and it can still merit consideration.\nLuisa Rodriguez: Right. It does seem totally clear and good that regularly in our daily lives we consider small risks of big things that might be either very good or very bad. And we think that’s just clearly worth doing and sensible. Sometimes probably, in personal experience, I may not do it as much as I should — but on reflection, I certainly endorse it.\nSo I guess the thinking here is that, given that there’s the potential for many, many, many beings with a potential for sentience, albeit some small likelihood, it’s kind of at that point that we might start wanting to give them moral consideration. Do you want to say exactly what moral consideration is warranted at that point?\nJeff Sebo: This is a really good question, and it actually breaks down into multiple questions.\nOne is a question about moral weight. We already have a sense that we should give different moral weights to beings with different welfare capacities: If an elephant can suffer much more than an ant, then the elephant should get priority over the ant to that degree. Should we also give more moral weight to beings who are more likely to matter in the first place? If an elephant is 90% likely to matter and an ant is 10% likely to matter, should I also give the elephant more weight for that reason?\nAnd then another question is what these beings might even want and need in the first place. What would it actually mean to treat an AI system well if they were sentient or otherwise morally significant? That question is going to be very difficult to answer.\nSo there are no immediate implications to the idea that we should give some moral consideration to AI systems if they have a non-negligible chance of being sentient. All that it means is that we should give them at least some weight when making decisions that affect them, and then we might disagree about how much weight and what follows from that.\nMeghan Barrett on the evolutionary argument for insect sentience\nMeghan Barrett: Importantly, I think another fact that a lot of people don’t realise from this evolutionary perspective is that insects are thus actually most closely related to crustaceans. They form this group together called the pancrustacea, and that group shares a common ancestor.\nSo I think that’s something else that people should consider. If you’re somebody who takes crustacean sentience seriously, and sentience is a trait that has evolved — it didn’t just appear in organisms as they are today, which I’m sure will be something we repeatedly touch on throughout this episode — if you’re somebody who believes that crustaceans are given the benefit of the doubt for you, for sentience, then you might also think that insects are worth giving the benefit of the doubt for some reasons as well, from an evolutionary perspective.\nAnd there are two possible hypotheses you could entertain here. One is the idea that sentience evolved exactly one time — and so everybody descended from that common ancestor, unless maybe they lost it for some reason, has that characteristic. So if you accept both vertebrates and any of the inverts — so a cephalopod or a decapod — if you’re convinced on a single invertebrate and you also are convinced on a single origin point for this, you have a problem, right? Because the closest common ancestor to all of those folks is very far back.\nAnd so you’re going to have all your insects, all your nematodes, all your decapods, all your annelids (which are another kind of worm) included, if you believe that there’s one common ancestor and no loss events. Now, maybe you think there’s loss events, but now you’re talking about multiple loss events, because there’s so many invertebrates. So you’re going to have to justify each of those losses, which you could potentially do.\nAnother possible hypothesis is that sentience evolved multiple times independently in different groups. I would probably say this is more plausible in my view, in part because we see multiple emergence of things all the time in evolutionary history.\nVision is a great example: we know that eyes might have evolved as many as 40 times during animal evolutionary history. And then, when we think about the development of eyes, it’s crucial to consider how they all generate the same\n\n\nThe basic function of being able to see something, even though they may vary in a lot of ways structurally. For instance, we saw the multiple emergence of what we call these crystalline lenses in the eyes of animals: some were made from co-opting calcite, others were made by co-opting heat shock proteins, still others were made by co-opting other novel proteins. All of them make these crystalline lenses, right? Or you could consider that independent but convergent evolution of similar structures in vertebrate eyes and spider eyes — and that can result in, again, the same basic capacity to see something.\n\nOf course, then we can talk about how the exact functions of seeing using these different structures or similar structures can vary. You know, acuity can vary, or wavelengths that the animal can sense can vary. But still, we think that the same basic capacity of some kind of sight is there for all of these animals.\n\nThis is all to say that it makes it more complicated if we’re looking for something like consciousness. So the function we’re interested in is consciousness instead of vision. Now we’re saying that if there’s more than one origin point, we need to be looking for potentially divergent structures capable of producing that common basic function. And of course, that common basic function can have lots of variance and gradation. And we don’t even have a good grasp yet on human consciousness, so you can see how acknowledging the possibility of multiple independent origins would then make this all very challenging to figure out.\n\nBut in any case, I think when you look at this from an evolutionary perspective, it’s important to consider who’s a close relative? Who are your common ancestors for that group? When you think these characteristics evolved, why do you think they evolved there? And then, if you’re somebody who takes crustaceans seriously, given that their close relation is the insects, you’re going to need to seriously consider the hexapods too.\n\nAndrés Jiménez Zorrilla on whether there’s something it’s like to be a shrimp\n\nRob Wiblin: How do shrimp farmers feel about their shrimp? Do they naturally care about their wellbeing, or see them as moral patients that can suffer?\n\nAndrés Jiménez Zorrilla: That’s something that surprised us. When we did a survey in India, it’s a small sample, but we asked whether they felt that their shrimps could feel pain and stress, and 95% of them said yes. One of them actually had a very endearing answer saying that he spent more time with his animals than he did with his family and that, if his friends suffered, he said, “I also suffer.”\n\nIt’s very interesting. On the other hand, I think it’s unsurprising, because these people spend a significant amount of time seeing the behaviour of the animals. They’re much less skewed than consumers, who never see them alive. I’m almost betting that you or your audience have very rarely ever seen an image of a shrimp that’s alive and swimming. Most of them will have just seen them in cocktails. And these farmers just see them all the time. They see them when they’re sick. They see them when they’re feeding, when they’re swimming about — so they really care about them.\n\nRob Wiblin: I see. So they’re exposed and seeing shrimp all the time. It sounds like their behaviour is moderately complicated, that they’re doing interesting things that make them seem smart enough and reactive and responsive enough to circumstances that it’s very natural to feel that they can suffer or experience pleasure the same way that dogs or pigs do.\n\nAndrés Jiménez Zorrilla: Exactly. I couldn’t have put it better. It’s very difficult to see them in farms, because the water in which they’re raised typically has high turbidity — it’s very murky. But once you actually see them — sometimes in tanks and in trade conferences and things like that — when they’re fed, they swim, they catch their feed, they take it to a little corner where each of them can eat it in peace. It’s very rare to see them behaving. There’s good research ongoing to understand behavioural issues of shrimps. It’s underway.\n\nRob Wiblin: I see. So they’re perhaps more like crabs or lobsters or octopus even than one might imagine, in terms of just how their behaviour looks.\n\nAndrés Jiménez Zorrilla: Exactly. That was the argument that the scientists at the London School of Economics made when they wrote this paper for the UK sentience bill recently, in November last year. They did a full review of the evidence of sentience of cephalopod mollusks and decapods, which are exactly the ones that you mentioned.\n\nWhat they found out is that, for those species that have been extensively researched, there is very good evidence that they’re sentient. What they say is the evidence in some other species is not as strong, but it’s only because they haven’t been researched for sentience purposes as much as other species — like, for example, crabs and octopuses, as you said.\n\nRob Wiblin: What sort of criteria are they using? When they study a species to try to figure out whether it’s sentient, what sort of things are they looking at to try to reach an evaluation?\n\nAndrés Jiménez Zorrilla: That’s a good point, because it’s very difficult to have a smoking gun that tells you that an animal is sentient.\n\nSo in this specific case, what they did is they looked at eight different indicators of sentience. Those included whether they had nociceptors, so the right body parts to detect noxious stimuli; whether they had protective behaviours, adaptive behaviours, if anaesthesia was applied to certain body parts — whether their reaction changed, which would indicate that it’s not a reflex. Things like that.\n\nThen they ranked whether the evidence was very high, high, to moderately low, et cetera, for each individual species. Then they came up with an overall assessment that all cephalopods and decapods should be protected by UK animal welfare law, and eventually they did. This was a report that was commissioned by the UK government to the London School of Economics. It was very independent research.\n\nRob Wiblin: OK, so shrimp respond to injuries. They probably learn from negative experiences that they have. Did they respond to anaesthetic? I know that’s one of the tests that people sometimes use.\n\nAndrés Jiménez Zorrilla: There’s a paper that shows the responses that different decapods have to anaesthetics, as you said. With shrimps in particular, what they do is they pinch one of the antennas. They see how they behave: they flick their tails, they jump out of the water, et cetera. And then in the second stage, they apply anaesthetics and repeat the experiment, and the behaviour changes significantly. The time that they rub their little antenna is much, much lower. They probably swim normally, quicker, and things like that.\n\nRob Wiblin: I see. They are tending to injuries, and they tend to them less when they’re given anaesthetic. As an aside, it’s remarkable to me that anaesthetics that we’ve presumably developed for humans also work on shrimp. They’re so far away in the phylogenetic tree of life, and yet so much of the basic machinery of feeling seems to be similar enough.\n\nAndrés Jiménez Zorrilla: That’s true. One of the arguments that some people made used to be that opioids don’t necessarily work the same in some of the animals. As you’ve said, not necessarily all of the anaesthetics need to work the same. But researchers have found anaesthetics that do apply and do have an effect on animals, and it changes behaviour.\n\nRob Wiblin: An audience member was curious to know: do you feel viscerally motivated by the prospect of shrimp suffering, or is your interest and motivation somewhat more on the intellectual side?\n\nAndrés Jiménez Zorrilla: So until very recently, my response would’ve been completely on the intellectual side. It was not until I visited farms. I think most of us during our lifetimes will never visit a shrimp farm, or most people in the world will not visit a shrimp farm or see a shrimp being taken out of the water.\n\nBut when my cofounder, our programme director in India, and myself went to see what is called “harvesting” — which is the moment in which the animals are scooped out of the water and eventually put in crates and things like that — that process made me also viscerally care about this issue. But it definitely came through the more intellectual part.\n\nJonathan Birch on the cautionary tale of newborn pain\n\nLuisa Rodriguez: Let’s turn to another edge case of sentience in humans: foetuses. You start this section of the book with what you call “the cautionary tale of newborn pain.” Can you talk me through that case?\n\nJonathan Birch: It’s another case I found unbelievable: in the 1980s, it was still apparently common to perform surgery on newborn babies without anaesthetic on both sides of the Atlantic. This led to appalling cases, and to public outcry, and to campaigns to change clinical practice. There was a public campaign led by someone called Jill Lawson, whose baby son had been operated on in this way and had died.\n\nAnd at the same time, evidence was being gathered to bear on the questions by some pretty courageous scientists, I would say. They got very heavily attacked for doing this work, but they knew evidence was needed to change clinical practice. And they showed that, if this protocol is done, there were massive stress responses in the baby, massive stress responses that reduce the chances of survival and lead to long-term developmental damage. So as soon as they looked for evidence, the evidence showed that this practice was completely indefensible and then the clinical practice was changed.\n\nSo, in a way, people don’t need convincing anymore that we should take newborn human babies seriously as sentience candidates. But the tale is a useful cautionary tale, because it shows you how deep that overconfidence can run and how problematic it can be. It just underlines this point that overconfidence about sentience is everywhere and is dangerous.\n\nLuisa Rodriguez: Yeah, it really does. I’m sure that had I lived in a different time, I’d at least have been much more susceptible to this particular mistake. But from where I’m standing now, it’s impossible for me to imagine thinking that newborns don’t feel pain, and therefore you can do massively invasive surgery on them without anaesthetic.\n\nJonathan Birch: It’s a hard one to believe, isn’t it? Of course, the consideration was sometimes made that anaesthesia has risks — and of course it does, but operating without anaesthesia also has risks. So there was real naivete about how the surgeons here were thinking about risk. And it’s what philosophers of science sometimes called the “epistemology of ignorance”: they were worried about the risks of anaesthesia, which is their job to worry about that, so they just neglected the risks on the other side. That’s the truly unbelievable thing.\n\nLuisa Rodriguez: So the clinical updates that have happened since: my sense is that now it is standard procedure to give newborns anaesthetic during surgeries, and that the benefits outweigh the risks. You argue that that wasn’t inevitable. What’s the case for that?\n\nJonathan Birch: I think the public outcry also mattered. Clinical norms are very hard to shift. If it was really just these two people, Anand and Hickey, against the medical establishment, that’s not really how change happens. You know, we talk about theories of change sometimes — and “just get the evidence and take it to the establishment” is not a good theory of change. I think in this case, the fact that there was at the same time a powerful public campaign going on based on these horrible stories, that is why clinical practice got changed very quickly.\n\nLuisa Rodriguez: So there’s a lesson there. I mean, there’s really nothing to say, but that that’s horrific, and important to know it happened, because we may be doing it again.\n\nDavid Chalmers on why artificial consciousness is possible\n\nDavid Chalmers: Some people think that no silicon-based computational system could be conscious because biology is required. I’m inclined to reject views like that, and there’s nothing special about the biology here.\n\nOne way to think about that is to think about cases of gradual uploading: you replace your neurons one at a time by silicon chips that play the same role. I think cases like this make it particularly hard to say that, if you say that the system at the other end is not conscious, then you have to say that consciousness either gradually fades out or during this process or it suddenly disappears during this process.\n\nI think it’s at least difficult to maintain either of those lines. You could take the line that maybe silicon will never even be able to simulate biological neurons very well, even in terms of its effects. Maybe there’s some special dynamic properties that biology has that silicon could never have. I think that would be very surprising, because it looks like all the laws of physics we know about right now are computational. Roger Penrose has entertained the idea that’s false.\n\nBut if we assume that physics is computational, that one can in principle simulate the action of a physical system, then one ought to at least be able to create one of these gradual uploading processes and then someone who denies that the system on the other end could be conscious is going to have to say either it fades out in a really weird way during this process. You go through half consciousness, quarter consciousness, while your behaviour stays the same, or that it suddenly disappears at some point. You replace the magic neuron and it disappears.\n\nThose are arguments I gave years ago now.\n\n\nfor why I think a silicon duplicated device can be conscious in principle.\n\nArden Koehler: I think I see why the sudden disappearance of consciousness in that scenario seems implausible. It’s like, “Well, what’s so special about that magic neuron?” But I don’t immediately see why the gradual fadeout of consciousness isn’t a reasonable possibility to entertain there?\n\nDavid Chalmers: How are you thinking the gradual fadeout would go? First, we’d lose visual consciousness, then we’d lose auditory consciousness? Or…?\n\nArden Koehler: I don’t know exactly how it would go, but if we assume that consciousness can come in degrees, then why can’t it disappear in degrees?\n\nDavid Chalmers: Yeah, I guess I’m thinking that I just put some crude measure on a state of consciousness. Like the number of bits involved in your state of consciousness. One way of imagining it fading is somehow lowering in intensity and then suddenly the intensity goes to zero and it all disappears. That to me sounds like a version of sudden disappearance because the bits which still go from being a million bits to zero bits all at once. Strange in the way that sudden disappearance is strange. Maybe looking more continuous then somehow the number of bits in your consciousness has to gradually decrease. You go from a million bits to 100,000 to 10,000 to whatever. And how would this work? Maybe my visual field would gradually lose distinctions, will gradually become more coarse-grained, maybe bits of it would disappear. Maybe one modality would go and then another modality. But anyway, you’re going to have these weird intermediate states where you say the system is conscious, is saying it is fully conscious of all these things because its behaviour is the same, “I am visually conscious in full detail, I’m auditorily conscious.” In fact, their consciousness state is going to be a very, very pale reflection of the conscious state they’re talking about with very few, say bits, of consciousness. That situation is the one that strikes me as especially strange. A conscious being that appears to be fully rational and believes it’s in this fully conscious state, but in fact, it’s in a very, very limited conscious state. If you’re an illusionist, you might think this kind of thing happens to you all the time.\n\nRob Wiblin: I think this is totally wrong. It seems like you could have a view where there’s information processing going on in the transmission between the neurons and that’s what’s generating the behaviour. But then there’s some other secret sauce that’s happening in the brain that we don’t understand and that we would not then replicate on the silicon chips. As you go replacing each neuron and each synapse with the machine version of it, the information processing continues as before and the behaviour remains the same. But you’ve lost the part that was generating the consciousness; you haven’t engineered that into the computer components, and so just gradually the consciousness disappears.\n\nDavid Chalmers: I can imagine this is at least conceivable, but then what are you going to say about the intermediate cases? There will have to be cases where the being is conscious and just massively wrong about its own consciousness. It says you’re not having experiences of red and green and orange right now. The fact is that it’s having a uniform gray visual field or thing like that.\n\nRob Wiblin: It seems possible, right?\n\nArden Koehler: I guess I also don’t find it as implausible as you seem to, Dave, that we could be wrong about our conscious experience or how much conscious experience we’re having in this gradual uploading example.\n\nDavid Chalmers: That’s fair, and there certainly are many cases where people are very wrong about their own conscious experiences. Certainly, there are all kinds of pathologies where there’s blindness denial — where people say they’re having all kinds of visual experiences when it appears that they’re blind and they’re not having them. Maybe it could be like this. This is strange because functionally the system doesn’t seem to have any pathologies. Anyway, I do allow that this is conceivable and I certainly can’t prove that it couldn’t happen. The more open you are to beings being very, very wrong about their consciousness, maybe you’ll be more open to this case. Here’s one thing I’ll say, at the very least: if this actually happens, and we go through it and our behavior is the same throughout, then we have beings whose heads are first a quarter silicon, then a half silicon as well. They say, “Everything is fine, everything is fine, my conscious experience is exactly the way it was.” They’re telling us this, they’re talking to us. They update every week with a bit more silicon and we keep talking to them. We are going to be very nearly completely convinced that they are conscious throughout. It’s going to become impossible to deny it. So at least as a matter of sociology, I think this view is likely to become the obvious-seeming view.\n\nHolden Karnofsky on how we’ll see digital people as… people\n\nRob Wiblin: Let’s talk about these digital people. What do you mean by digital people?\n\nHolden Karnofsky: So the basic idea of a digital person is like a digital simulation of a person. It’s really like if you just take one of these video games, like The Sims, or… I use the example of a football game because I was able to get these different pictures of this football player, Jerry Rice, because every year they put out a new Madden video game. So, Jerry Rice looks a little more realistic every year. You have these video game simulations of people, and if you just imagine it getting more and more realistic until you have a perfect simulation. So imagine a video game that has a character called Holden, and just does everything exactly how Holden would in response to whatever happens. That’s it. That’s what a digital person is. So it’s a fairly simple idea. In some ways it’s a very far-out extrapolation of stuff we’re already doing, which is we’re already simulating these characters.\n\nRob Wiblin: I guess that’d be one way to look at it. The way I’ve usually heard it discussed or introduced is the idea that, well, we have these brains and they’re doing calculations, and couldn’t we eventually figure out how to basically do all of the same calculations that the brain is doing in a simulation of the brain moving around?\n\nHolden Karnofsky: Yeah, exactly. You would have a simulated brain in a simulated environment. Absolutely, that’s another way to think of it.\n\nRob Wiblin: This is a fairly out-there technology, the idea that we would be able to reproduce a full human being, or at least the most important parts of a human being, running on a server. Why think that’s likely to be possible?\n\nHolden Karnofsky: I mean, I think it’s similar to what I said before. We have this existence proof. We have these brains. There’s lots of them, and all we’re trying to do is build a computer program that can process information just how a brain would. A really expensive and dumb way of doing it would be to just simulate the brain in all its detail, just simulate everything that’s going on in the brain. But there may just be smarter and easier ways to do it, where you capture the level of abstraction that matters. So maybe it doesn’t matter what every single molecule in the brain is doing. Maybe a lot of that stuff is random, and what really is going on that’s interesting, or important, or doing computational work in the brain is maybe the neurons firing and some other stuff, and you could simulate that. But basically, there’s this process going on. It’s going on in a pretty small physical space. We have tonnes of examples of it. We can literally study animal brains. We do. I mean, neuroscientists just pick them apart and study them and try to see what’s going on inside them. And so, I’m not saying we’re close to being able to do this, but when I try to think about why would it be impossible, why would it be impossible to build an artefact, to build a digital artefact or a computer that’s processing information just how a brain would, and I guess I just come up empty. But I can’t prove that this is possible. But yeah, the basic argument is just, it’s here, it’s all around us. Why wouldn’t we be able to simulate it at some point in time?\n\nRob Wiblin: Are you envisaging these digital people as being conscious like you and me, or is it more like an automaton situation?\n\nHolden Karnofsky: One of the things that’s come up is when I describe this idea of a world of digital people, a lot of people have the intuition that even if digital people were able to act just like real people, they wouldn’t count morally the same way. They wouldn’t have feelings. They wouldn’t have experiences. They wouldn’t be conscious. We shouldn’t care about them. And that’s an intuition that I disagree with. It’s not a huge focus of the series, but I do write about it. Basically if you dig all the way into philosophy of mind and think about what consciousness is, this is something we’re all very confused about. No one has the answer to that. But I think in general, there isn’t a great reason to think that whatever consciousness is, it crucially relies on being made out of neurons instead of being made out of microchips or whatever. And one way of thinking about this is, I think I’m conscious. Why do I think that? Is the fact that I think I’m conscious, is that connected to the actual truth of me being conscious? Because the thing that makes me think I’m conscious has nothing to do with whether my brain is made out of neurons. If you made a digital copy of me and you said, “Hey, Holden, are you conscious?” That thing would say, “Yes, of course I am,” for the same exact reason I’m doing it: it would be processing all the same information, it’d be considering all the same evidence, and it would say yes. There’s this intuition that whatever consciousness is, if we believe it’s what’s causing us to think we’re conscious, then it seems like it’s something about the software our brain is running, or the algorithm it’s doing, or the information it’s processing. It’s not something about the material the brain is made of. Because if you change that material, you wouldn’t get different answers. You wouldn’t get different beliefs. That’s the intuition. I’m not going to go into it a tonne more than that. There’s a thought experiment that’s interesting that I got from David Chalmers, where you imagine that if you took your brain and you just replaced one neuron with a digital signal transmitter that just fired in all the same exact ways, you wouldn’t notice anything changing. You couldn’t notice anything changing, because your brain would be doing all the same things, and you’d be reaching all the same conclusions. You’d be having all the same thoughts. Now, if you replaced another one, you wouldn’t notice anything, and if you replaced them all, you wouldn’t notice anything. Anyway, so I think there’s some arguments out there. But I think it is the better bet that if we had digital people that were acting just like us, and the digital brains were doing the same thing as our brains, that we should care about them. And we should think of them as people. And we probably would. Even if they weren’t conscious —\n\nRob Wiblin: Because they would act like it.\n\nHolden Karnofsky: Yeah, well, we’d be friends with them.\n\nRob Wiblin: They would complain the same way.\n\nHolden Karnofsky: We’d talk to them and we would relate to them. There are people I’ve never met, and they would just be like any other people I’ve never met, but I could have video calls with them and phone calls with them. And so, we probably will and should care about what happens to them. And even if we don’t, it only changes some of the conclusions. But I basically think that digital people would be people too.\n\nRob Wiblin: I mean, the argument that jumps to mind for me is if you’re saying, “Well, to be people, to be conscious, to have value, it has to be run on meat. It has to be run on these cells with these electrical charges going back and forth.” It’d be like, “Did evolution just happen to stumble on the one material that could do this? Evolution presumably didn’t choose to use this particular design because you’d be conscious. So why would there be this coincidence that we have the one material out of all of the different materials that can do these calculations that produces moral value?”\n\nHolden Karnofsky: That’s an interesting way of thinking of it. If I were to play devil’s advocate, I would be like, “Well, maybe every material has its own kind of consciousness, and we only care about the kind that’s like our kind,” or something. But then that would be an interesting question why we only care about our kind.\n\nJeff Sebo on grappling with our biases and ignorance when thinking about sentience\n\nLuisa Rodriguez: Let’s actually talk about another paper of yours, “The rebugnant conclusion.” I love that title,\n\n\nBy the way.\nJeff Sebo: Thank you!\nLuisa Rodriguez: In the paper you basically ask: Suppose that we determine that large animals like humans have more welfare on average, but that small animals like insects have more welfare in total. What follows for ethics and politics?\nAnd the paper focuses on small animals like nematodes, but the same question is relevant to AI systems that might end up being super numerous — perhaps because they’re used all over the economy — but that might also have some non-negligible chance of experiencing pain and pleasure.\nSo let’s start with the case that you actually focus on in the paper, which is small animals. How should we think about this case?\nJeff Sebo: I think we can start really at the end of the last exchange about ways of striking a balance if we worry about the harms of false positives and false negatives. One thing that you can note is that, even if I include insects and AI systems and other types of beings in my moral circle, even if I give them moral consideration, I might still be able to prioritise beings like me for different reasons.\nOne of them is that I might have a higher capacity for welfare than an insect or an AI system. I have a more complex brain and a longer lifespan than an insect, so I can experience more happiness and suffering at any given time, as well as over time.\nAnd humans in general, I might think, are more likely to be sentient and morally significant — given the evidence available to me — than insects, AI systems, other beings like that. So I might think to myself that if a house is burning down and I can save either a human or an ant, but not both, then I can justifiably save the human — both because the human is more likely to matter, and because the human is likely to matter more. And those are perfectly valid ways of breaking a tie.\nThat might give us some peace of mind when we countenance the possibility of including these very different, very numerous beings in the moral circle. But then you have to consider how large these populations actually are — and this is where we get to the problem that this paper addresses, which is a problem in population ethics.\nLuisa Rodriguez: Right. And population ethics is the philosophical study of the ethical problems that come up when our actions affect how many people are born in the future, and who exactly is born.\nBut yeah, my understanding is that we don’t actually know how many of these small animals there are — ants and nematodes and maybe even microbes — but that it’s at least plausible that there are so many of them that even if they have very less significant kinds of suffering and pleasure relative to humans, and even if we only put some small chance on them even having those at all, their interests still just swamp humans’. And this argument just does sound plausible to me, and it also fills me with dread and fear. What is your experience of it?\nJeff Sebo: Well, I certainly have the same experience as I think most humans do. And the reason I gave that paper the title “The repugnant conclusion” is that this is based on a famous book by the philosopher Derek Parfit called Reasons and Persons, part four of which addresses population ethics. In that part of that book, Derek Parfit discusses what he calls the repugnant conclusion. I can say briefly what that is and why that has, for the past several decades, filled many people with dread.\nSo the repugnant conclusion results from the following observations. If you could bring about one future where the world contained 100 people and everyone experienced 100,000 units of happiness, or you could bring about another world with twice the number of people, 200 people, but everyone experiences one fewer unit of happiness — 99,999 — which world is better? Well, many of us have the intuition that the second world is better; I should bring about that second population. Everybody might experience one unit of happiness less per person, but since there are twice as many people, there is nearly twice as much happiness overall, and everyone is still really happy. And so, all things considered, I should bring about that population.\nBut then you can imagine another population, once again twice as big, and once again with a bit less happiness per person. Then another one twice as big, a bit less happiness per person. And so on and so on and so on — until you reach a point where you are imagining a world or a solar system or a galaxy that contains a vast number of individuals, each of whom has a life only barely worth living at all. And your reasoning would commit you to the idea that that is the best possible world, the one that you should most want to bring about.\nParfit thought the idea that we would favour that world with a much larger population, where everyone has a life barely worth living at all, over a world with a still significant population, where everyone has lives very much worth living, he found that repugnant. And he spent much of the rest of his career trying and failing to find a better way to see the value of populations that could avoid that result.\nLuisa Rodriguez: I guess in the case of insects, there’s also this weird thing where, unlike humans eating potatoes and not particularly enjoying their monotonous lives, we might think that being a spider and making a web sounds pretty boring, but we actually just really do not know. In many ways, they’re so different from us that we should have much lower probability that they’re not enjoying or enjoying that than we do of humans in this repugnant conclusion scenario. How do you factor that in?\nJeff Sebo: I do share the intuition that a very large insect population is not better off in the aggregate than a much smaller human population or elephant population. But for some of the reasons that you just mentioned and other reasons, I am a little bit sceptical of that intuition.\nWe have a lot of bias here and we also have a lot of ignorance here. We have speciesism; we naturally prefer beings and relate to beings when they look like us — when they have large eyes and large heads, and furry skin instead of scaly skin, and four limbs instead of six or eight limbs, and are roughly the same size as us instead of much smaller, and reproduce by having one or two or three or four children instead of thousands or more. So already we have a lot of bias in those ways.\nWe also have scope insensitivity — we tend not to be sensitive to the difference that very large numbers can make — and we have a lot of self-interest. We recognise that if we were to accept the moral significance of small animals like insects, and if we were to accept that larger populations can be better off than smaller populations overall, then we might face a future where these nonhuman populations carry a lot of weight, and we carry less weight in comparison.\nAnd I think some of us find that idea so unthinkable that we search for ways to avoid thinking it, and we search for theoretical frameworks that would not have that implication. And it might be that we should take those theoretical frameworks seriously and consider avoiding that implication, but I least want to be sceptical of a kind of knee-jerk impulse in that direction.\nLuisa Rodriguez: Yeah, I am finding that very persuasive. Even as you’re saying it, I’m trying to think my way out of describing what I’m experiencing as just a bunch of biases — and that in itself is the biases in action. It’s me being like, no, I really, really, really want to confirm that people like me, and me, get to have… I don’t know. It’s not that we don’t have priority — we obviously have some reason to consider ourselves a priority — but I want it to be like, end of discussion. I want decisive reasons to give us the top spot. And that instinct is so strong that that in itself is making me a bit queasy about my own motivations.\nJeff Sebo: Yeah, I agree with all of that. I do think that we have some reason to prioritise ourselves, and that includes our welfare capacities and our knowledge about ourselves. It also includes more relational and pragmatic considerations. So we will, at least in the near term, I think have a fairly decisive reason to prioritise ourselves to some extent in some contexts.\nBut yeah, I agree. I think that there is not a knock-down decisive reason why humanity should always necessarily take priority over all other nonhuman populations — and that includes very large populations of very small nonhumans, like insects, or very small populations of very large nonhumans. We could imagine some kind of super being that has a much more complex brain and much longer lifespan than us. So we could find our moral significance and moral priority being questioned from both directions.\nAnd I think that it will be important to ask these questions with a lot of thought and care and to take our time in asking them. But I do start from the place of finding it implausible that it would miraculously be the case that this kind of population happens to be the best one: that a moderately large population of moderately large beings like humans happens to be the magic recipe, and we matter more than all populations in either direction. That strikes me as implausible.\nBob Fischer on how to think about the moral weight of a chicken\nLuisa Rodriguez: Just to make sure I understand, the thing is saying that the capacity of welfare or suffering of a chicken in a given instant is about a third of the capacity for the kind of pain and pleasure a human could experience in a given instant. Is that it?\nBob Fischer: That’s the way to think about it. And that might sound very counterintuitive, and I understand that. I think there are a couple of things we can say to help get us in the right frame of mind for thinking about these results.\nOne is to think about it first like a biologist. If you think that humans’ pain is orders of magnitude worse than the pain of a chicken, you’ve got to point to some feature of human brains that’s going to explain why that would be the case. And I think for a lot of folks, they have a kind of simple picture — where they say more neurons equals more compute equals orders of magnitude difference in performance, or something like that.\nAnd biologists are not going to think that way. They’re going to say, look, neurons produce certain functions, and the number of neurons isn’t necessarily that important to the function: you might achieve the exact same function using many more or many fewer neurons. So that’s just not the really interesting, relevant thing. So that’s the first step: just to try to think more like a biologist who’s focused on functional capacities.\nThe second thing to say is just that you’ve got to remember what hedonism says. What’s going on here is we’re assuming that welfare is about just this one narrow thing: the intensities of pleasures and pains. You might not think that’s true; you might think welfare is about whether I know important facts about the world or whatever else, right? But that’s not what I’m assessing; I’m just looking at this question of how intense is the pain.\nAnd you might also point out, quite rightly, “But look, my cognitive life is richer. I have a more diverse range of negatively valenced states.” And I’m going to say that I don’t care about the range; I care about the intensity, right? That’s what hedonism says: that what matters is how intense the pains are.\nSo yeah, “I’m very disappointed because…” — choose unhappy event of your preference — “…my favourite team lost,” whatever the case may be. And from the perspective of hedonism, what matters about that is just how sad did it make me? Not the content of the experience, but just the amount of negatively valenced state that I’m experiencing, or rather the intensity of the negatively valenced state that I’m experiencing. So I think people often implicitly confuse variety in the range of valenced states with intensity.\nLuisa Rodriguez: Yeah, that’s definitely something I do. For sure there is a part of me that thinks that the thing that matters a lot here is that I can fall in love in a particularly meaningful and big way; I can have friendships lasting 50 years that involve really deep and meaningful conversations. And that even if a chicken has meaningful relationships with other chickens, they’re not as complex and varied as the relationships I have with people in my life.\nOn the other hand, a big part of me puts a bunch of weight, when I really think about it, on just like, no, what matters is the intensity. If a chicken feels more sad about her wing being broken than I feel about losing a friend, then so be it. We should make sure that their wings aren’t broken before we should make sure that whatever threat that could mean I lose my friend [is prevented].\nAnd I guess lots of listeners will have their own kind of internal turmoil about this, about what welfare even is. But for now, I guess if we’re just taking this assumption, which is that what matters is the intensity, your finding is that something like averting the suffering of three chickens for an hour is similarly important to averting the suffering of one person for an hour. And that feels uncomfortable to me. Can you talk me through that discomfort?\nBob Fischer: Sure. So the first thing to say is: you’re not alone. I don’t feel totally comfortable either. And we have to ask ourselves what our most serious moral commitments are when we’re approaching this question. So you’re not going to avoid really uncomfortable, challenging questions when we try to think about moral weights — just not going to go away.\nBut here are a few things to say. One is: is there any number that you wouldn’t be uncomfortable with? Because notice that if you’re committed to this idea of doing conversions, eventually it’s going to just work out that you’ve got to say there is some number of hours of chicken suffering that is more important than helping a human.\nAnd I think actually for a lot of people\n\n\nThey don’t really think that there is any conversion at all, right?\nIf I had said it was 300, would you really have felt that much better?\nYou might have felt a little bit better; I’m not saying you wouldn’t have felt it at all.\nSure, it’s a difference, but you might still say, when you really think about it: “Three hundred hours?\nWould I put somebody through that for… chickens?”\nAnd then you might just have the same level of discomfort, or something close to it.\nSo I think to some degree we have to remember that the tradeoffs that we’re talking about come from background theoretical commitments that have nothing to do with our specific welfare range estimates: it comes from the fact that we’re trying to do the most good.\nWe think that means making comparisons across species, and we’re committed to this kind of maximising ethic that says, yeah, there is some tradeoff rate, and you’ve got to find it.\nSo that’s the first thing to say about the discomfort.\nBefore I say anything else, what do you think about that?\nLuisa Rodriguez: Yes, some of that definitely worked for me.\nI think the thing that lands most is if I think about chickens on a railroad track, and there’s a trolley coming, and there’s a human on the other side, it is pretty impossible for me to imagine getting to the point where I’m ever super comfortable being like, “I’m going to let it hit the human, who I could have conversations with, who has a family I might know, who I could give a hug to, and who has a job…”\nThese are all the things that kind of run through my head as I’m deciding whether to pull a lever to decide who gets hit by this trolley.\nAnd so, fair enough that that is something I have to grapple with, regardless of exactly what these numbers are.\nBob Fischer: And just to tag on to that, think about it when you put someone you really care about on the track.\nSo I think about this with my children, and say, look, it might well be the case that there’s almost no number of other humans I would choose to spare, given the choice between killing my own children and them.\nBut that’s not because I think they actually matter less in some objective sense.\nLike when I’m trying to do the impartial good, I would never say, “Oh yes, my children are utility monsters: they have infinite worth and everybody else has just some tiny portion of that.”\nSo when we recognise that our moral judgements are so detached from our judgements of value, that also can help us think about why these welfare ranges might not be quite so crazy.\nLuisa Rodriguez: Yeah.\nWas there anything else that helps you with the discomfort?\nBob Fischer: I think the thing that helps me to some degree is to say, look, we’re doing our best here under moral uncertainty.\nI think you should update in the direction of animals based on this kind of work if you’ve never taken animals particularly seriously before.\nBut ethics is hard.\nThere are lots of big questions to ask.\nI don’t know if hedonism is true.\nI mean, there are good arguments for it; there are good arguments for all the assumptions that go into the project.\nBut yeah, I’m uncertain at every step, and some kind of higher-level caution about the entire venture is appropriate.\nAnd if you look at the way people actually allocate their dollars, they often do spread their bets in precisely this way.\nEven if they’re really in on animals, they’re still giving some money to AMF.\nAnd that makes sense, because we want to make sure that we end up doing some good in the world, and that’s a way of doing that.\nLuisa Rodriguez: I guess I’m curious if there’s anything you learned, like a narrative or story that you have that makes this feel more plausible to you?\nAnything particular about chickens or just about philosophy?\nYou’ve already said some things, but what story do you have in your head that makes you feel comfortable being like, “Yes, I actually want to use these moral weights when deciding how to allocate resources”?\nBob Fischer: There are two things that I want to say about that.\nOne is I really worry about my own deep biases, and part of the reason that I’m willing to be part of the EA project is because I think that, at its best, it’s an attempt to say, “Yeah, my gut’s wrong.\nI shouldn’t trust it.\nI should take the math more seriously.\nI should try to put numbers on things and calculate.\nAnd when I’m uncomfortable with the results, I’m typically the problem, and not the process that I used.”\nSo that’s one thing.\nIt’s a check on my own tendency to discount animals, even as someone who spends most of their life working on animals.\nSo I think that’s one piece.\nThe other thing is just to spend time thinking about the kinds of things animals can do and what their lives are like.\nJust how hard a chicken will work to get to a nest box before she lays an egg, the amount of labour she’s willing to go through to do that, to think about how important that is to her.\nAnd to realise that we can quantify that, and see how much they care, or to see that they get stressed out when fellow chickens are threatened and that they seem to have some sympathy for conspecifics.\nThose kinds of things make me say there is something in there that is recognisable to me as another individual, with desires and preferences and a vantage point on the world, who wants things to go a certain way and is frustrated and upset when they don’t.\nAnd recognising the individuality, the perspective of nonhuman animals, for me, really challenges my tendency to not take them as seriously as I think I ought to, all things considered.\nCameron Meyer Shorb on the range of suffering in wild animals\nLuisa Rodriguez: Fundamentally, why do you worry about suffering among wild animals?\nWhat kinds of things make you think that they might be suffering a lot in particular?\nCameron Meyer Shorb: I’m wildly uncertain about what the nature of wild animals’ lives are like.\nBut I got into this field because I changed my mind about the possibilities here.\nI used to just assume that animals living in the wild were perfectly in balance, and living totally fulfilled lives, and weren’t bothered by any of the stresses of modernity like I am — and that the best thing humans could possibly do for them is just leave them alone there.\nBut the more I learned about it and thought about it, the more I realised that there’s actually lots of reasons to think that wild animals might not be living great lives, at least many of them.\nFor example, they often have to struggle to get enough food.\nThey often need to struggle to protect themselves from extreme weather.\nThere are some kinds of things where they have no protection at all: if a flood or a wildfire comes, that’s just the end of it for them.\nThere’s also all sorts of diseases or parasites.\nThey have no healthcare.\nThey also have no state to protect them from violence, either from other species or even members of their own species.\nSo the kinds of conditions we’re talking about here, when humans live in those conditions, we would call that poverty.\nAnd we wouldn’t tolerate that.\nWe would say that those are problems we need to solve; those are people who deserve better lives.\nAnd if we have medicine, we should help give them access to medicine.\nIf there’s ways to give them more stable access to food, that’s something that would improve their lives.\nThat’s the kind of approach that I now think we should consider taking when we think about wild animals: taking seriously the idea that they might be struggling even in their natural habitats, and they might suffer even from naturally occurring harms.\nAnd we should try to figure out to what extent that’s true, and if it’s possible to do anything about that.\nLuisa Rodriguez: Yeah, I want to come back to some of those, because I feel like even though I’m a little bit familiar with this problem, I still have a super limited imagination for the kinds of things that wild animals — and obviously there’s such a wide range across different species — might be going through, aside from the really obvious ones, like being eaten or something.\nSo putting a pin in that, what is the scale of the problem?\nHow many individuals alive right now are wild animals?\nCameron Meyer Shorb: The scale is huge.\nIt’s bigger than I can count.\nI’ve been humbled by learning about this, but I do think the scale is such an important part of understanding the problem.\nJust in the broadest possible strokes, based on the rough numbers we know, it looks like something like 99% of all sentient minds alive on Earth today are wild animals.\nSo if you are a human or a farmed animal, that is an incredibly rare exception to the rule, which is: things that can feel live in the wild.\nYou’re more of a rounding error than anything.\nWhich is not to say that human and farmed animal experiences aren’t important; it’s just to say there is a lot more going on.\nAnd a truly impartial view of ethics would have us believe that ethics is mostly about wild animals… and also there are these interesting subfields that are related to some primates and farmed animals.\nLuisa Rodriguez: OK, I’m interested in breaking down the numbers a bit more.\nI feel like it seems at least kind of important to know, are we mostly thinking about zebras, or are we mostly thinking about fish, ants…?\nWhat to think about feels like it could have helpful effects in helping me figure out, what are we talking about?\nCameron Meyer Shorb: That’s a great question to ask, because I think that the images that normally come to mind are not the most representative of wild animals.\nMost minds are wild, weird, and wet.\nThey’re just not humans or human-like things.\nTo try to get a sense of scale, I’ll suggest a visualisation.\nLet’s imagine for the sake of this exercise that we’re going to put a dot down of equal size for any individual that’s alive.\nSo one dot for a human, one dot for a squirrel — and you can debate later how you want to make tradeoffs across species — but just for starters, to get a sense of the raw numbers.\nNow, let’s make these dots small enough so that we can fit all 8.2 billion humans onto the face of a quarter or a euro — so something a little smaller than one square inch.\nIf we’re keeping that scale, then the 88 billion wild mammals would take up an area about the size of a credit card or a post-it note.\nAnd then when we move on to birds — and I should say these estimates are all very rough, and the bigger the populations, the wider the error bars are — but for birds, let’s say there are about 200 billion living in the wild.\nThat would be something about the size of a standard envelope.\nAnd then for reptiles and amphibians, each of those numbers somewhere around one trillion individuals, two trillion put together.\nSo a trillion would be a standard sheet of paper.\nI think this is a good place to pause and just think about how far we’ve come: from a quarter to an envelope, which is way bigger than a quarter, to a couple sheets of paper — compared to a quarter that contains all of human experience and 8.2 billion lives.\nThere’s that, times many, many more, if you’re trying to encompass humans and mammals and birds and reptiles and amphibians.\nThen the numbers get even more mind-boggling when we move on to fish.\nThere’s something like 10 trillion fish in the world.\nSo 10 trillion fish would be something like the size of a medium-sized desk — the Linnmon from Ikea, if you will — or a large bath mat, or a couple of pillowcases maybe.\nThat’s what the whole fish population would look like, relative to the human population fitting on a quarter.\nAnd the numbers get really… I don’t know what “to boggle” means literally, but I think it is something like what is happening to my mind.\nI think it is mind-boggling to try to imagine the number of plausibly sentient invertebrates.\nThe only at all close-to-useful number I found here is an estimate of the number of terrestrial arthropods — that would be animals with hard exoskeletons, like insects and arachnids and crustaceans.\nSo for those that live on land, estimates are that their population is somewhere around 100,000 trillion.\nIf 8.2 billion humans fit on the face of a quarter, 100,000 trillion would need to be something the size of a city block or a FIFA regulation-size soccer field.\nLuisa Rodriguez: That’s insane!\nCameron Meyer Shorb: Imagine standing at any point in a soccer field and looking at a quarter and then looking around at the rest of the field.\nIt really changes your perspective on what life on Earth is like, who’s really living here.\nAnd it’s hard to know whether many arthropods are sentient.\nI think there’s decent questions and considerations on either side.\nBut one of the things that I think is important to consider is the expected value.\nSo even if there’s just a 10% chance that they are, 10% of a soccer field is still way bigger than a quarter.\nLuisa Rodriguez: Yeah.\nThis makes me really happy that some of our most recent episodes… One is with Meghan Barrett, one of my favourite episodes of all time, on invertebrate sentience.\nAnd really, it took me well above 10% probability or credence that invertebrates are sentient.\nAnother one is on fish with Sébastien Moro, just infinite numbers of bewildering facts about fish.\nFor me, invertebrates and fish both make up tremendous numbers of individuals, as you\n\n\nI've just said, and just are clearly at least very plausible sentience candidates.\nFor the case of fish, it seems hard to even debate for me.\nSo given that these are the animals making up most of the wild nature we’re talking about, for anyone who’s interested, I can recommend those episodes.\n\nSébastien Moro on whether fish are conscious or sentient\n\nLuisa Rodriguez: This is fascinating and mind-blowing, purely from the perspective of “fish are incredibly cool.”\nBut do these feel like they tell you anything about whether fish are experiencing these things in some conscious way or affective way?\n\nSébastien Moro: As I said earlier, we have to split consciousness and sentience, which are not the same thing.\nIt’s very hard actually talking about real consciousness, like high-order consciousness, like humans: we don’t know how to assess it correctly, even in humans.\nWe don’t have proper assessing tools.\nSo today we’re trying to build new ways to assess consciousness and sentience and split them properly.\nI understood that you’ve interviewed Jonathan Birch about that.\nHe’s a pioneer in it.\nHe’s a very important person on it.\nThere is a very good book, especially on fish, a study which is named, “What is it like to be a bass?\nRed herrings, fish pain, and the study of animal sentience.”\nIt’s a publication from 2022.\nIt’s really interesting because it’s coming back on all of this, and the famous study that was talking about modification of the mirror test we were talking about.\nMany of the studies that were done that we’ve talked about, they aren’t made to assess sentience.\n\nLuisa Rodriguez: From your perspective, you know so much of what there is to know about fish that we have studied so far, so you’ve got all of this wealth of knowledge: what is it that feels most compelling to you, that makes you feel like you’ve got really high confidence that fish are experiencing things?\n\nSébastien Moro: Most of the papers we have are going in this way, and very few — very, very few — are going the other way.\nSo what makes me so certain?\nI’m really talking about my personal opinion here.\nI tend to think that emotions… What are emotions?\nWhat are emotions used for?\nThey are putting a gloss on what is around us: “This stuff has a positive gloss; I need more of this.\nThis one has a negative gloss.”\nAnd we know that emotions are very, very closely linked to learning.\nSo emotions are something that attract or repel.\nAnd it seems pretty obvious that it must have appeared very, very early in evolution, because this is how we work.\nThis is maybe one of the biggest differences with algorithms.\nAlgorithms are following closed loops.\nAnd this comes up when animals are more driven by emotions, by value of things, which is made by a kind of limbic system that says, “This is good, this is bad.\nYou want more of this, you want less of that.”\nAnd I don’t understand why other animals couldn’t have had that.\nAnd another thing is, I’m reading a lot about bees, so I know very well the corpus of knowledge on bees at the moment.\nAnd we’re starting to have the same results in bees.\nSo I’m not 100% certain now that bees could be sentient, but the biggest leader of bees research today, Lars Chittka, has said on Twitter that bees are sentient for him.\nAnd the results we have are going in this way.\nSo they have a one-million-neuron brain, and the brains of fish are much, much bigger.\nAnd when we split from insects, brains were not existing either.\nSo it’s just a convergent evolution.\nFish have complicated lives; they have social lives, very social lives.\nI already introduced this with cleaner wrasses: their lives are very complicated.\nThey have challenges that they have to overcome that are as complex as what we find in mammals and birds, maybe more sometimes.\nSo them having no sentience, when we recognise sentience in birds and mammals?\nIt’s either you refuse it for everyone, or you accept it for everyone at the moment.\nNot for everyone, because animals with a brain or central nervous system, today we think at least there should be a kind of global network; everything should be put in common to make a unified vision of you and this kind of thing.\nBut consciousness probably has many degrees; sentience has pretty much many degrees — but not degrees on a ladder, degrees more on a circle.\nBut that would sound just weird actually, that they wouldn’t be sentient.\n\nLuisa Rodriguez: Yeah, it would just be really surprising to you.\n\nDavid Chalmers on when to start worrying about artificial consciousness\n\nArden Koehler: So you said elsewhere that if more fully autonomous artificial intelligence comes around, then we might have to start worrying about it being conscious, and therefore presumably worthy of moral concern.\nBut you don’t think we have to worry about it too much before then.\nI’m just wondering if you can say a bit about why, and whether you think it’s possible that programs or computers could become gradually more and more conscious, and whether that process might start before they are fully autonomous?\n\nDavid Chalmers: Yeah, that’s an interesting point.\nI guess one would expect to get to conscious AI well before we get human-level artificial general intelligence, simply because we’ve got a pretty good reason to believe there are many conscious creatures whose degree of intelligence falls well short of human-level artificial general intelligence.\nSo if fish are conscious, for example, you might think if an AI gets to sophistication and information processing and whatever the relevant factors are to the degree present in fish, then that should be enough.\nAnd it does open up the question as to whether any existing AI systems may actually be conscious.\nI think the consensus view is that they’re not.\nBut the more liberal you are about descriptions of consciousness, the more we should take seriously the chance that they are.\nThere is this website out there called People for the Ethical Treatment of Reinforcement Learners that I quite like.\nThe idea is that every time you give a reinforcement learning network its reward signal, then it may be experiencing pleasure or correspondingly suffering, depending on the valence of the signal.\nAs someone who’s committed to taking panpsychism seriously, I think I should at least take that possibility seriously.\nI don’t know where our current deep learning networks fall on the scale of organic intelligence.\nMaybe they’re at least as sophisticated as worms — like C. elegans, with 300 neurons.\nI take seriously the possibility that those are conscious.\nSo I guess I do take seriously the possibility that AI consciousness could come along well before human-level AGI, and that it may exist already.\nThough then the question is, I suppose, how sophisticated the state of consciousness is.\nIf it’s about as sophisticated as, say, the consciousness of a worm, I think most of us are inclined to think that brings along, say, some moral status with it, but it doesn’t give it enormous weight in the scheme of conscious creatures compared to the weight we give humans and mammals and so on.\nSo I guess then the question would be whether current AIs get a truly sophisticated moral status, but I guess I should be open to them at least getting some relatively small moral status of the kind that, say, worms have.\n\nRob Wiblin: Maybe this is getting outside your area of expertise, but with current ML systems, how would we have any sense of whether the affective states are positive or negative?\nIt seems like once you have a reinforcement learner, I guess on average, does it get zero reinforcement because it just has an equal balance of positive and negative reinforcements?\nAnd is there some way that you could just scale all of them up to be more or less positive?\nOr does that even mean anything?\nLike you just increase all the numbers by 100.\nHow would that help?\nIt raises this issue of the arbitrariness of the zero point on this kind of scale of goodness of the states.\n\nDavid Chalmers: Yeah.\nThis is getting to issues about value and morality that do go beyond my expertise to some extent.\nWe’ve got absolutely no way right now to tell exactly what reinforcement learning systems might be experiencing, if anything.\nBut if you were inclined to think they’re experiencing something and that they’re experiencing something with valence, I suppose then they’d be having a mix of positively valenced reinforcement and negatively valenced reinforcement — therefore, a mix of very simple precursor of, say, pleasure and of suffering, proto-pleasure and proto-suffering.\nThen a lot’s going to depend on your ethical theory.\nIf you’re feeling pleasure half the time but suffering half the time, is that net good?\nIs that net bad?\nI don’t know.\nIf you ask me, I think that’s net bad, because all that suffering tends to outweigh the pleasure, but maybe there’s weights on the scale.\nAt this point though, I should say that it’s by no means obvious to me that pleasure and suffering, that is, that valenced states of consciousness, are the ones that are relevant to moral status.\nI know people quite often have this issue.\nI’m inclined to think that consciousness may ground moral status in some cases quite independently of its valence.\nEven beings with unvalenced states of consciousness could still have moral status.\n\nRobert Long on how we might stumble into causing AI systems enormous suffering\n\nRobert Long: So you can imagine that a robot has been created by a company or by some researchers.\nAnd as it happens, it registers damage to its body and processes it in the way that, as it turns out, is relevant to having an experience of unpleasant pain.\nAnd maybe we don’t realise that, because we don’t have good theories of what’s going on in the robot or what it takes to feel pain.\nIn that case, you can imagine that thing having a bad time because we don’t realise it.\nYou could also imagine this thing being rolled out and now we’re economically dependent on systems like this.\nAnd now we have an incentive not to care and not to think too hard about whether it might be having a bad time.\nSo I mean, that seems like something that could happen.\n\nLuisa Rodriguez: Yeah, and that could happen because there’s some reason why it’s helpful to have the robot recognise that it’s sustained damage.\nIt can be like, “Help, I’m broken.\nI need someone to fix my part.”\nSo that’s something that you can imagine might get programmed in.\nAnd then, it is just kind of wild to me that we don’t understand what the robot might be experiencing well enough to know that that thing is pain.\nBut in theory, that’s possible, just that it is that black-boxy to us.\n\nRobert Long: Yeah.\nIt might be a little bit less likely with a robot.\nBut now you can imagine more abstract or alien ways of feeling bad.\nSo I focus on pain because it’s a very straightforward way of feeling bad.\nA disembodied system like GPT obviously can’t feel ankle pain.\nOr almost certainly.\nThat’d be really weird.\nIt doesn’t have an ankle.\nWhy would it have computations that represent its ankle is feeling bad?\nBut you can imagine maybe some strange form of valenced experience that develops inside some system like this that registers some kind of displeasure or pleasure, something like that.\n\nLuisa Rodriguez: Right, right.\nSomething like, you guessed the wrong set of words to come next and that was bad.\nAnd the user isn’t happy with the string of words you came up with.\nAnd then that feels something like pain.\n\nRobert Long: Exactly.\nAnd I will note that I don’t think that getting negative feedback is going to be enough for that bad feeling, fortunately.\nBut maybe some combination of that and some way it’s ended up representing it inside itself ends up like that.\nAnd then we have something where it’s hard for us to map its internals to what we care about.\nWe maybe have various incentives not to look too hard at that question.\nWe have incentives not to let it speak freely about if it thinks it’s conscious, because that would be a big headache.\nAnd because we’re also worried about systems lying about being conscious and giving misleading statements about whether they’re conscious — which they definitely do.\nSo we’ve built this new kind of alien mind.\nWe don’t really have a good theory of pain, even for ourselves.\nWe don’t have a good theory of what’s going on inside it.\nSo that’s like a stumbling-into-this sort of scenario.\n\nLuisa Rodriguez: Yeah.\nBut it sounds like we could make things for economic reasons, like robots or chatbots, and we don’t realise those things are suffering.\nAnd then we mass produce them because they’re valuable.\n… And those things are suffering and we didn’t know it and they’re all over.\nAnd we don’t really want to change anything about those systems because we use them.\n\nRobert Long: Yeah.\nI mean, for just another dark scenario, you can imagine a system where we get pigs to be farmed much more efficiently.\nAnd we’re just like, “Well, this has made meat cheaper.\nLet’s not think too much about that.”\nI guess one thing I should note is I’ve been focusing on this case where we’ve hit on it accidentally.\nThere are a lot of people who are interested in building artificial consciousness.\n\nLuisa Rodriguez: On purpose, yeah.\n\nRobert Long: And understandably so.\nYou know, just from a purely intellectual or philosophical standpoint, it’s a fascinating project and it can help us understand the nature of consciousness.\nSo for a very long time, probably about as old as AI, people were like, “Wow, I wonder if we could make this thing\n\n\n“Conscious?”\n\nThere was a recent New York Times article about roboticists who want to build more self-awareness into robots, both for the intrinsic scientific interest and also because it might make for better robots. And some of them think, “Oh, well, we’re not actually that close to doing that. Maybe it’s too soon to worry about it.” Another person quoted in that article is like, “Yeah, it’s something to worry about, but we’ll deal with it.” And I am quoted in that piece as just kind of being like, “Ahhh, be careful, you know. Slow down. We’re not really ready to deal with this.”\nLuisa Rodriguez: Not ready to “deal with that.”\nRobert Long: Yeah, exactly.\nJonathan Birch on how we might accidentally create artificial sentience\nLuisa Rodriguez: You did raise a few interesting points about AI sentience I wanted to ask more about. One is that you emphasise that AI sentience could arise in a number of ways. I think I intuitively imagine it arising either intentionally or unintentionally as a result of work on LLMs. But one of these other ways is whole brain emulation. And one case I hadn’t heard that much about is OpenWorm. Can you talk a bit about the goals of OpenWorm and how that project has gone?\nJonathan Birch: This was a project that caught my eye around 2014, I think, because the goal was to emulate the entire nervous system of the nematode worm C. elegans in software. And they had some striking initial results, where they put their emulation in charge of a robot, and the robot did some kind of worm-like things in terms of navigating the environment, turning round when it hit an obstacle, that kind of thing. And it generated some initial hype.\nLuisa Rodriguez: It feels naive now, but it was eye-opening to me when you pointed out that we actually just wouldn’t need whole brain emulation in humans or of human brains to start thinking about the risks from AI sentience. We just need to go from OpenWorm to OpenZebrafish or OpenMouse, or maybe even OpenDrosophila — which sounds like not an insane step from just where we are now. How likely is it, do you think, that researchers would try to create something like OpenMouse?\nJonathan Birch: Oh, it’s very likely. If they knew how, of course they would. I think one of the main themes of that part of the book is that once we see the decoupling of sentience and intelligence — which is very important, to think of these as distinct ideas — we realise that artificial sentience might not be the sort of thing that goes along with the most intelligent systems. It might actually be more likely to be created by attempts to emulate the brain of an insect, for example — where the intelligence would not be outperforming ChatGPT on any benchmarks, but perhaps more of the relevant brain dynamics might be recreated.\nLuisa Rodriguez: Yeah, it was a jarring observation for me. I think part of it is that it hadn’t occurred to me that people would be as motivated as they are to create something like OpenMouse. Can you say more about what the motivation is? Does it have scientific value beyond being cool? Or is the fact that it’s just a cool thing to do enough?\nJonathan Birch: I think it would have an immense scientific value. It would appear to be a long way in the future still, as things stand. But of course, we’re talking here about understanding the brain. I think when you emulate the functions of the C. elegans nervous system, you can really say you understand what is going on — and that just isn’t true for human brains, currently. We have no idea. At quite a fundamental level, our understanding of C. elegans is in some ways far better.\nAnd it’s another step again, if you don’t just understand how lesioning bits of the nervous system affects function, but you can also recreate the whole system in computer software, would be a tremendous step.\nAnd it holds the promise over the long term of giving us a way to replace animal research. Because once you’ve got a functioning emulation of a brain, you can step past that very crude method of just taking the living brain and injuring it, which is what a lot of research involves, or modifying it through genome editing. You can instead go straight to the system itself and do incredibly precise manipulations.\nSo I feel like, if anything, it hasn’t been hyped enough. I want more of this kind of thing, to be honest, than has been the case so far.\nLuisa Rodriguez: Intuitively, it seems plausible — and maybe even likely — that if you were able to emulate a mind that we thought was sentient, that the emulation would also be sentient. But is there a reason to think those come apart? Maybe we just don’t know.\nJonathan Birch: It’s another space where you get reasonable disagreement, because I think we have to take seriously the view that philosophers call “computational functionalism”: a view on which, if you recreate the computations, you also recreate the subjective experience. And that leads to further questions about at what grain does one have to recreate the computations? Is it enough to recreate the general type of computation? Or does every algorithm at every level, including the within-neuron level, have to be recreated? And there too, there’s disagreement.\nI think we have to take seriously the possibility that recreating the general types of computations might be enough. I call this view “large-scale computational functionalism”: that it might be a matter of simply creating a global workspace or something like that, even if the details are quite different from how the global workspace is implemented in the human brain.\nAnd if we take that view seriously, as we should, it really does suggest a kind of parity. I wouldn’t want to overstate it, because I’d say that the probability of sentience is higher in the biological system than in its software emulation. But still, that software emulation is potentially a candidate.\nAnil Seth on which parts of the brain are required for consciousness\nLuisa Rodriguez: I’m really, really curious to get your thoughts on animals. I think maybe I would like to start with different neuroscientific theories of consciousness. Which parts of the brain are sufficient and required for consciousness feels like it might be a really key question for thinking about which nonhuman animals we should expect to have conscious experiences — because some nonhuman animals, like insects, only have things that look much more like the very old parts of the human brain, the parts that are deeper in.\nDo you have a view on which theories seem most plausible to you? Are the really old parts of the brain, the subcortical parts, sufficient for consciousness?\nAnil Seth: To be honest, I don’t know. But I think to help orient in this really critical discussion — critical because, of course, it has massive implications for animal welfare and how we organise society and so on — it’s worth taking a quick step back again and just comparing the problem of animal consciousness with the one of AI consciousness. Because in both cases there’s uncertainty, but they are very different kinds of uncertainty.\nLuisa Rodriguez: Almost opposite ones.\nAnil Seth: Almost exactly opposite. In AI, we have this uncertainty of, does the stuff matter? AI is fundamentally made out of something different. Animals are fundamentally the same because we are also animals.\nAnd then there’s the things that are different. Animals generally do not speak to us, and often fail when measured against our highly questionable standards of human intelligence. Whereas AI systems now speak to us, and measured against these highly questionable criteria, are doing increasingly well.\nSo I think we have to understand how our psychological biases are playing into this. It could well be that AI is more similar to us in ways that do not matter for consciousness, and less similar in ways that do — and nonhuman animals the other way around. We’ve got a horrible track record of withholding conscious status and therefore moral considerability from other animals, but even from other humans. For some groups of humans, we just do this. We’ve historically done this all the time and are still doing it now.\nThere’s this principle in ethics called the precautionary principle: that when we’re uncertain, we should basically err on the side of caution, given the consequences. I think this is really worth bearing in mind for nonhuman animals. You could apply the same to AI and say, well, we should just apply that: since there’s uncertainty, we should just assume AI is conscious. I think no: I think the effect of bias is so strong, and we can’t care for everything as if it’s conscious, because we just only have a certain amount of care to go around.\nBut when it comes to nonhuman animals, they have the brain regions, the brain processes, that seem highly analogous to the ones in human and mammalian brains for emotional experiences, pain, suffering, pleasure and so on, that I think it pays to extend the precautionary principle more in that direction.\nFiguring out exactly which animals are conscious, of course, we don’t know. But there are things that I think are relatively clear. If we just take mammals very broadly, from a mouse to a chimpanzee to a human, we find very similar brain structures and similar sorts of behaviours and things like that. So it seems very, very unlikely that there are some mammals that lack consciousness. I think mammals are conscious.\nBut even then, we’ve had to get rid of some of the things that historically you might have thought of as essential for consciousness, like higher order reasoning and language. I mean, Descartes was infamous — but at the time, it was probably a very sensible move because of the pressure he was under from the religious authorities — he was very clear that only humans had consciousness, or the kind of consciousness that mattered, and that was because we had these rational minds. So he associated consciousness with these higher rational functions.\nNow people generally don’t do that. So mammals are within the magic circle. What else? Then it becomes really hard, because we have to just walk this line: we have to recognise we’re using humans — and then, by extension, mammals — as a kind of benchmark.\nBut you know, there might well be other ways of being conscious. What about the octopus, as Peter Godfrey-Smith has written beautifully about? And what about a bumblebee? What about bacteria? It’s almost impossible to say. It seems intuitive to me that some degree of neural complexity is important, but I recognise I don’t want to fall into the trap of using something like intelligence as a benchmark.\nLuisa Rodriguez: Yeah. I mean, that’s just basically what I find both maddening and fascinating about this question of nonhuman animals. It feels like there’s this very frustrating slippery slope thing, where I don’t want to be overly biased toward humans, or towards structures that kind of “create consciousness,” whatever that means, in the way that ours does.\nAnd it does seem like there might be multiple ways to do it. And over time, I’ve become much, much more sympathetic to the idea that not just birds, and not just cephalopods, but insects have some kinds of experiences. And I just find it really confusing about where and how and whether it makes sense to draw a line, or maybe that’s just philosophically nonsensical.\nSo I’m really drawn to this question, which is why I opened with it, of: Can neuroscience point to functions or places or parts of the brain that seem related enough to consciousness, that if we see analogous things in bees, we should update a lot on that? But I also have the impression that there’s still so much debate about subcortical and cortical theories, and which is more plausible, that maybe we’re just not there, and that’s not possible now, and might not be for a while.\nAnil Seth: Jonathan Birch, who’s a philosopher at UCL, has this beautiful new book called The Edge of Sentience, which I think is all about this. He’s trying to figure out how far we can generalise and on what basis.\nI think the issue is that it seems very sensible that consciousness is multiply realisable to an extent: that different kinds of brains could generate different kinds of experience, but it’d still be experience. But to know when that’s the case, we have to understand the sort of basis of consciousness in a way that goes beyond, “It requires this or that region.”\nWe need to know what is it that these brain areas are doing or being that makes them important for consciousness in a way that we could say, well, we obviously don’t see a frontal cortex in a honeybee, because they don’t have that kind of brain, but they’re doing something, or their brains are made of the stuff and organised in the right way, that we can have some credence that that’s enough for consciousness.\nAnd we don’t really have that yet. I mean, the theories of consciousness that exist are varied. Some of them are pretty explicitly theories about human consciousness, and they’re harder to extrapolate to nonhuman animals: like what would be a global workspace in a fruit fly? You could make some guesses, but the theory as it is is more assuming there’s a kind of cortical architecture like a human.\nAnd other theories, like integrated information theory, are much clearer: wherever there is some nonzero integrated information maxima of X, there’s consciousness. But it’s just impossible to actually measure that in practice. So very, very hard.\nBut the path, I think, is clear that the better that we can understand consciousness, where we are sure that it exists, the surer our footing will be elsewhere where we’re less confident, because we will be able to generalise better.\nSo where are your areas of uncertainty? I’m always interested.\nLuisa\n\n\nRodriguez: I feel, just through getting to learn about these topics for this show, I constantly get told these amazing facts about fish and bees, and the kinds of learning they can do, and the kinds of motivational tradeoffs they make, and the fact that they do nociception, and that nociception gets integrated into other parts of their behaviour. And that all feels really compelling to me.\nThen I talk to someone who’s like, “Yeah, but a lot of that could just be happening unconsciously.” And at what point it’s more plausible that they’re little robots doing things unconsciously, and then at what point it becomes more plausible that a little robot doing that is just a less likely story than it’s got the lights switched on in some way, and it’s making tradeoffs because the world is complicated and it’s evolved to have more complex systems going on so that it can survive. I just find that really confusing.\nAnil Seth: Yeah. I think me too. But actually there’s a point you make which I didn’t make, so I’m grateful for you bringing that up, which is the functional point of view too. So instead of just asking which brain regions or interactions between brain regions do we see, we can ask from the point of view of function and evolution, which is usually the best way to make sense of things comparatively between animals and biology.\nSo what is the function of consciousness? And if we can understand more about that, then we can have another criterion for saying, which other animals do we see facing and addressing those same kinds of functions? And of course there may be other functions. We have to be sensitive to that too. But at least it’s another productive line.\nAnd in humans and mammals, there’s no single answer. But it seems as though consciousness gets into the picture when we need to bring together a lot of different kinds of information signals from the environment in a way that is very much centred on the possibilities for flexible action — all kind of calibrated in the service of homeostasis and survival.\nSo automatic actions, reflexive actions, don’t seem to involve consciousness. Flexibility, informational richness, and sort of goal-directedness, those seem to be functional clues. So to the extent we see animals implementing those similar functions, I think that’s quite a good reason for attributing consciousness. But it’s not 100%.\nLuisa Rodriguez: Yeah. That’s basically where I am. And it means that I now feel a lot of feelings about the bees in my garden. And mostly I feel really grateful to have learned about these topics, but I also feel really overwhelmed.\nPeter Godfrey-Smith on uploads of ourselves\nLuisa Rodriguez: Why are you doubtful that we could create uploads of ourselves?\nPeter Godfrey-Smith: My view on that comes from the general position that I’m developing — slowly, cautiously — on the biology of conscious experience, or the biology of felt experience. This was developed in a bit more detail in the second book, but it has a discussion in Living on Earth as well.\nThe view that I think is most justified — the view I would at least put money on — is a view in which some of what it takes to be a system with felt experience involves relatively schematic functional properties: the way that a system is organised in relation to sensing and action and memory and the internal processing and so on. And some of those, what are often referred to as “functional properties,” could exist in a variety of different physical realisations, in different hardwares or different physical bases.\nBut I don’t think that’s the whole story: I think nervous systems are special. I think that the way that nervous systems work, the way that our brains work… There are two kinds of properties that nervous systems have. There’s a collection of point-to-point network interactions — where this cell makes that cell fire, and prevents that cell from firing, the spiking of neurons, and the great point-to-point massive network interactions.\nAnd there’s also other stuff, which for years was somewhat neglected I think in these discussions, but which I think is probably very important. There are more diffuse, large-scale dynamic properties that exist within nervous systems: oscillatory patterns of different speeds, subtle forms of synchronisation that span the whole or much of the brain. And these are the sorts of things picked up in an EEG machine, that kind of large-scale electrical interaction.\nAnd I didn’t come up with this myself. There’s a tradition. Francis Crick thought this, neuroscientists like Wolf Singer, a number of other people have argued that this side of the brain is important to the biology of conscious experience, along with the sort of networky, more computer computational side of the brain: that both sets of properties of nervous systems are important. And in particular, the unity of experience — the way in which brain activity generates a unified point of view on the world — has a dependence upon the oscillatory and other large-scale dynamic patterns that you get in brains.\nNow, if you look at computer systems, you can program a computer to have a moderately decent facsimile of the network properties in a brain. But the large-scale dynamic patterns, the oscillatory patterns that span the whole, they’re a totally different matter. I mean, you could write a program that includes a kind of rough simulation, where you’d know what was happening if the physical system in fact had large scale dynamic patterns of the relevant kind, but that’s different from having in a physical system those activities actually going on — present physically, rather than just being represented in a computer program.\nI do think there’s a real difference between those generally, and especially in the case of these brain oscillations and the like. You would have to build a computer where the hardware had a brain-like pattern of activities and tendencies. People might one day do that, but it’s not part of what people normally discuss in debates about artificial consciousness, uploading ourselves to the cloud and so on.\nPeople assume that you could take a computer, like the ones you and I are using now, with the same kind of hardware, and if you just got the program right — if it was a big powerful one and you programmed it just right — it could run through not just a representation of what a brain does, but a kind of realisation and another instance or another form of that brain activity.\nNow, because I think the biology of consciousness is just not like that — I think that the second set of features of brains really matter — I think that it will be much harder than people normally suppose to build any kind of artificial sentient system that has no living parts. It’s not that I think there’s a kind of absolute barrier from the materials themselves — I don’t know if there is — but I certainly think it would have to be much, much more brain-like. The computer hardware would have to be a lot more brain-like than it is now.\nI mean, who knows if we could build large numbers of these, powered with a big solar array, and replicate our minds in them? I think it’s very unlikely, I must say. Now, whether that’s unlikely or not, I don’t think I should be confident about. The thing I am a bit confident about, or fairly confident about, is the idea that there’s lots of what happens in brains that’s probably important to conscious experience, which is just being ignored in discussions of uploading our minds to the cloud and things like that.\nLuisa Rodriguez: Yeah, that’s really helpful. I don’t know very much about these large-scale dynamics. Are they a result of neuronal firings, or are they a result of other things going on in the brain?\nPeter Godfrey-Smith: This is quite a controversial point, actually. It’s a good question. The sense I have — and I’m continually trying to learn the latest on this — is that most of the activity in those large-scale dynamic patterns is not just a summing together of lots of firing of neurons. Because if it was, then you might say that the network, point-to-point, cell-to-cell things really are all that matters once you’ve really captured those, and the other stuff is just a consequence or a kind of zoomed-out manifestation of that.\nAnd that does not seem to be the case. It seems rather that within the cells that make up our brain, there’s a kind of to and fro of ions across membranes that is below the threshold that is required to actually make the neuron fire, that dramatic spark-like firing. It’s more of a kind of rhythmic, lower-level, sub-threshold electrical oscillation.\nAnd those oscillations affect the firing of neurons, and the firing of neurons affect the oscillations. But there is a kind of duality of processes there, and it’s not just that if you knew which cells were firing when, and you ignored everything else, that you could recapture, in a sense, the large-scale dynamic patterns. There’s more than that.\nLuisa Rodriguez: Right. So if you take this kind of, for me, what has been a very intuitive, but I guess would feel like an inadequate thought experiment of replacing each neuron in the human brain, one by one, with an artificial silicon-based one, it sounds like you would guess that doing that, even if you got that to work, it wouldn’t make the entire process and physical system of the brain be entirely artificial? There would be other things that remained biological that were playing a crucial role that you hadn’t replaced and replicated yet?\nPeter Godfrey-Smith: Well, the way you described it, you made it sound like at the end of this process, the biological parts were gone. So is it that we really replace everything, or is it just we replace some stuff and leave some of the biological material in place?\nLuisa Rodriguez: I think I’m curious about if you just replaced neurons with synthetic neurons, do you think there would be a working brain at the end? And if so, would that be because the other biological stuff was still there?\nPeter Godfrey-Smith: OK, right. I see. It’s a commitment of my view that if you tried to do that, you would change all sorts of stuff: you can’t really preserve the things that the brain is doing at the start and have them continue once the neurons have been replaced by artificial devices.\nI mean, what are the “artificial devices”? When people talk about thought experiments of this kind — which they’ve done, I think, in quite interesting ways for about 40 years now — what they usually imagine is you’re replacing each neuron with a kind of relay object that sums up the inputs coming in and either fires or doesn’t fire, and its firing then contributes to the inputs to various other downstream cells. And it’s just doing that. It’s not doing anything else.\nNow, if the things you put into the brain were just doing that, then essentially you would drop all those large-scale dynamic patterns. They just wouldn’t exist anymore. They would be gone. So it would be a different thing physically. It would do different things, because the relationship between the firing of neurons and those slower oscillations does make a difference to how the brain works, so you would change some stuff that would have consequences.\nNow, something that people haven’t talked about so much is whether you could put in an artificial object that did really everything that the neuron is doing, where there’s this sort of seepage of ions across the borders, and it has subtle effects on the electrical properties of neighbouring cells and so on. A replacement that captured a lot more of what a neuron did.\nAnd then we reached the point where we got to a few minutes ago, where I said who knows what artificial hardwares might be possible? I don’t think I know nearly enough about these matters to be confident that you could never build something that did that.\nLuisa Rodriguez: Just to make sure I understand your position on this: it’s not that you’re confident that it’s impossible to create digital minds in general, or that it’s impossible to eventually come up with some hardware system that does replicate everything relevant to consciousness in a human — but that the current thought experiments where we only replicate some of the basic functions of a neuron are not enough.\nPeter Godfrey-Smith: There was a phrase you used — “digital minds” — and I guess I don’t think there will ever be digital minds. I think that’s imagining a system which lacks too much of what brains and nervous systems are like in order for it to have a mind. I don’t know if there might be wholly artificial minds in the future that are made of different stuff than brains are made of, but that have the important duality of properties that you see in nervous systems, and thereby achieve a kind of sentience. I think it’s further away than people often suppose, but I would not want to say it could never happen.\nThe thing I want to press on in a kind of critical way is the habits that people have of thinking… People think, “I know what neurons and nervous systems do. It’s like a telephone exchange: you’ve got this thing that makes that thing go, and it’s like a big network, and that’s all there is to it.” They think that, and then they think, “Well, a computer is a place where I can have that kind of thing going on, replicated to a high degree of fidelity. So I know that an ordinary computer has what it takes as hardware to be sentient, basically, to be conscious if I put the right program in there — because the program would just have to create a version of those networked, point-to-point interactions.”\nThat claim I do want to push back against. I don’t think we have any reason to believe that. And I think that artificial sentience is just\n\n\ngoing to be a harder and further-in-the-future thing than we have supposed.\nAnd as I say at one point in this book, Living on Earth, I’m not sure whether that’s a bad thing.\nIf there turns out to be a kind of barrier between the I side of AI — the intelligent side of AI — and the kind of conscious, sentient, feeling side of artificial minds, if there’s a barrier there, that might not be a bad thing.\nOne reason for that is if we start building lots of sentient systems that have this different kind of hardware, and they’re under our control, we made them — I don’t know how likely it is that their experiences are going to be positive, at least in the early stages.\nIt’s going to be a sort of a klutzy, messed-up version of artificial sentience that we’re working with.\nAnd if I’m a disembodied spirit of the kind that you were talking about earlier, I don’t want to come down to Earth and live as an early effort in the human attempt to make artificial sentient systems.\nJonathan Birch on treading lightly around the “edge cases” of sentience\n\nLuisa Rodriguez: So a big part of the book then explores these candidates for sentience: beings that we think could plausibly be sentient, but because we just know so little about exactly what sentience even is, and how different beings feel it and in what proportions, there’s not enough clear understanding or evidence to be sure what it is like to be them.\nYou look at some familiar candidates, including animals of different classes, as well as AI.\nAnd then you also look at some cases that I was really unfamiliar with, including people with disorders of consciousness — so people in comas, for example — and then also “neural organoids,” which I’d never heard of.\nTo start us off, what exactly is a neural organoid?\nJonathan Birch: This is another very fast-moving area of emerging technology.\nBasically, it uses human stem cells that are induced to form neural tissue.\nThe aim is to produce a 3D model of some brain region, or in some cases a whole developing brain.\nLuisa Rodriguez: And what’s the case for creating them?\nJonathan Birch: I think it’s a very exciting area of research.\nYou can make organoids for any organ, really.\nIn a way, it’s a potential replacement for animal research.\nIf you ask what we do now, usually people do research on whole animals, which are undeniably sentient.\nAnd here we have a potential way to gain insight into the human version of the organ.\nIt could be a better model, and it’s much less likely to be sentient if it’s something like a kidney organoid or a stomach organoid.\nIt’s really only when we’re looking at the case of the brain and neural organoids that the possibility of sentience starts to reemerge.\nLuisa Rodriguez: Yeah.\nAnd intuitively, the case for sentience does feel like it immediately lands for me.\nIf you are trying to make an organoid that is enough like a brain that we can learn about brains, it doesn’t seem totally outrageous that it would be a sentience candidate.\nWhat is the evidence that we have so far?\nJonathan Birch: It’s a complicated picture.\nI think there are reasons to be quite sceptical about organoids as they are now, but the technology is moving so fast, there’s always a risk of being ambushed by some new development.\nAt present, it really doesn’t seem like there’s clear sleep/wake cycles; it doesn’t seem like those brainstem structures or midbrain structures that regulate sleep/wake cycles and that are so important on the Merker/Panksepp view are in place.\nBut there are reasons to be worried.\nFor me, the main reason to be worried was a study from 2019 that allowed organoids to grow for about a year, I think, and compared them to the brains of preterm infants using EEG.\nSo they used EEG data from the preterm infants to train a model, and then they used that model to try and guess the age of the organoid from its EEG data, and the model performed better than chance.\nLuisa Rodriguez: Wow.\nJonathan Birch: So it’s hard to interpret this kind of study, because some people, I suppose, read it superficially as saying these organoids are like the brains of preterm infants.\nAnd that’s an exaggeration, because they’re very, very different and much, much smaller.\nBut still, there’s enough resemblance in the EEG to allow estimates of the age that are better than chance.\nLuisa Rodriguez: It’s definitely something.\nI find it unsettling, for sure.\nJonathan Birch: It is, yeah.\nI think a lot of people had that reaction as well, and I think that’s why we’re now seeing quite a lively debate in bioethics about how to regulate this emerging area of research.\nIt’s currently pretty unregulated, and it raises this worrying prospect of scientists taking things too far — where they will often say, “These systems are only a million neurons; we want to go up to 10 million, but that’s so tiny compared to a human brain.”\nAnd it is tiny compared to a human brain.\nBut if you compare it to the number of neurons in a bee brain, for example, that’s about a million.\nSo these near-future organoids will be about the size of 10 bee brains in terms of neuron counts.\nAnd I think bees are sentience candidates, so naturally I take this risk quite seriously, and I think it would be wrong to dismiss it.\nMeghan Barrett on whether brain size and sentience are related\n\nLuisa Rodriguez: I guess the thing I do care about in the context of this sentience discussion is brain size.\nIt feels like brain size is at least more likely to be tracking the thing that I care about.\nAnd I guess I care about whether these very large insects also have very large brains, or if the beetle is mostly like other guts, as opposed to brain guts.\nMeghan Barrett: This is a phenomenal question.\nI appreciate you asking it very much, and it’s very interesting to me.\nSo I have a couple of points to make on the brain size piece of this.\nThe first thing I’ll say is just that we should ask ourselves if more actually always means better or more sophisticated.\nThat’s an assumption to challenge about our thinking about brains.\nFor example, elephants have more neurons and more brain mass than you and I do, and yet I don’t consider them more likely to be sentient than me.\nSame thing with blue whales.\nAnd that’s because we know a lot of those additional neurons are because they’re just bigger, and so they have more mass to control, right?\nThey have more touchpoints they need to be able to integrate and things like that.\nSo more doesn’t always mean more sophisticated; it can often just mean more of the same repeat unit that performs the same kind of function.\nAnd that explains, very plausibly I think, this phenomenon that we see, where very small brains are capable of producing really complex abilities.\nInsects are capable of things like numerical cognition, social learning, facial recognition, cognitive bias, and much more.\nAnd so because of this behavioural data and this complexity that we initially thought was just going to be something you find in vertebrates, and now we’re seeing it in invertebrates with pretty small brains, like honeybees, this suggests that maybe those bigger brains aren’t actually better or more sophisticated — they just have more redundancies or repeating modules.\nYou could think of this as changing resolution or complexity of a capacity, without changing the existence of that capacity itself.\nSo imagine an image that has more pixels to it: it doesn’t change the fact that there’s still a picture; it just changes the resolution of that picture.\nSo that’s part one to this: that first we should just challenge that basic assumption that bigger brains are necessarily better at producing a capacity at all.\nThe next thing we need to think about is whether or not they are actually that small, which is what you were asking initially.\nSo here again, I want to give some examples from vertebrates and invertebrates to challenge our intuition that vertebrates are always bigger than invertebrates.\nLet’s first consider just the mammals, because I think we all feel really comfortable talking about mammal sentience.\nSo the smallest mammal brain that we have studied so far weighs in at 64.4 milligrammes.\nThat’s the Etruscan shrew.\nLuisa Rodriguez: That is tiny.\nMeghan Barrett: Shrews can get very small.\nVery, very small.\nAnd the body mass of that shrew is about two grams or so, give or take.\nSo we get some pretty small mammals out there.\nWhen we look at the largest insect brain we’ve studied to date, it’s a solitary wasp, and it weighs in at 11.7 milligrammes.\nThat’s an insect with a body mass of about 0.5 grams, so a quarter the size of the shrew.\nThat makes that shrew’s brain just about six times larger than the insect brain.\nThat’s a smaller difference than we see between humans and whales.\nSo already, even if we’re just considering the mammals, we see relatively comparable brain masses [with the wasp].\nNow, that’s just one level of looking at the brain.\nI want to also extend beyond just mammals now and consider other vertebrates you might take seriously — like your lizards, say.\nSo our smallest lizard brain that we’ve studied is the Algerian sand gecko.\nThe brain itself weighs about 10.8 to 11.8 milligrammes.\nSo that is smaller than the wasp at 11.7.\nLuisa Rodriguez: Yeah.\nWow.\nMeghan Barrett: So already we’re seeing vertebrates close or comparable to the invertebrates, depending on whether you’re talking about mammals or lizards.\nSo we know we’re already in the same ballpark on mass.\nNow, you might not think that mass is the most relevant metric, though, for considering the brain.\nThat’s because we might think that actually things like neuron numbers, like the computational units of the brain, are more important.\nOne note I want to make on this is that I’m going to talk about whole brain numbers today, because that’s the only data that we have.\nBut I again want to go back to our earlier point that not all neurons do all things: neurons are specialised for particular functions; they’re organised into functionally discrete regions.\nFor example, wasps have many more optic lobe neurons than ants do.\nOptic lobe neurons are the ones that bring in visual information from the periphery.\nThis is probably because wasps fly and have to bring in a lot of visual information really fast.\nI don’t think that all those additional optic lobe neurons are all that sentience-relevant.\nSo if I counted those, I would think the wasps were more likely to be sentient [than ants], when really they just can process and input more visual information more quickly.\nLuisa Rodriguez: Yes, that makes sense to me.\nMeghan Barrett: So that’s one thing to keep in mind: that we don’t have good data on what are the sentience-relevant regions of the brain, and how many cells are in those regions.\nSo instead we’re just going to talk about total neuron numbers, because that’s the data we’ve got so far.\nSo with that caveat, let’s think about neuron numbers.\nLet’s go back again to mammals, because we like starting with mammals, right?\nSo, as far as I know, the smallest number of neurons we’ve found in any of the mammals so far is in the naked mole-rats, and that’s about 26.88 million neurons.\nThe smallest studied vertebrate brain is, again, that Algerian sand gecko I mentioned before.\nIt has about 1.8 million neurons.\nAnd it turns out that that wasp brain I mentioned to you before also contains 1.8 million neurons.\nSo our gecko and our wasp are very comparable.\nThey’re both about 15 times smaller than that of the naked mole-rat.\nSomething interesting to consider here is that if we just look across the mammals, that naked mole-rat has only one one-thousandth the number of neurons of an elephant.\nSo again, there’s a lot more difference within the mammals than we’re seeing between our smallest mammals and even just the invertebrates that we’ve studied so far from an insect perspective.\nSo I think, again, we might consider just that there’s a lot of reasons to be sceptical that insects have an unusually small number of neurons compared even to the mammals, much less the vertebrate lizards and snakes.\nLuisa Rodriguez: Yeah, interesting.\nI think I learned about 1,000 new facts there.\nLewis Bollard on how animal advocacy has changed in response to sentience studies\n\nLuisa Rodriguez: I was wondering if you’d be up for sharing some reflections from your time as programme officer at Open Phil.\nI think you’ve now been doing this since 2015-ish, and you were also in the farmed animal welfare space even before that.\nIs there something that you’ve changed your views on since you started doing this work?\nLewis Bollard: I’ve become a lot more worried about invertebrates.\nWhen I started this work, I — like many of us — just ignored them.\nI think I sort of quietly assumed that they weren’t sentient.\nAnd then I remember, someone asked me back in 2016, and I said, maybe there’s like 10% chance that they were sentient — and that was enough to make me worry a bit, but it really wasn’t that high.\nSince then, thanks in large part to the work that Rethink Priorities did with their Moral Weight Sequence, I have really come to see a very high probability that invertebrates are sentient in some meaningful sense, and that their welfare matters.\nLuisa Rodriguez: Yeah, that makes sense.\nWe actually just interviewed Bob Fischer about the Moral Weight Project\n\n\nSo listeners might have heard that.\nI’m curious if you remember any of the particular facts or research that felt compelling to you and convinced you that invertebrates are more likely to be sentient than you thought?\n\nLewis Bollard: I think it was more of the absence of contrary facts.\nI mean, I had just assumed that because society acts as if insects aren’t sentient and shrimp aren’t sentient, that there must be good evidence for that.\nAnd I was really surprised when they started looking into this and there just wasn’t.\nOn the flip side, there was evidence to worry.\nAnd for me, the most compelling one is actually just the evolutionary reason, which is that it just does seem like an animal who has the capacity to move and the capacity to learn, there are reasons, unfortunately, for it to have the capacity to feel pain, too.\n\nLuisa Rodriguez: Yes, I feel the same way.\nSpeaking of the Moral Weight Project, back in 2017, Rob Wiblin asked you if we had any kind of quantitative measure that you can use to compare animal suffering to human suffering.\nAnd I think you said something like, “That’d be great, but no — our ability to understand the relative experiences of different species is still really limited.”\nWe’ve both alluded to this work a few times already, but just to give a bit more context: Bob Fischer and his colleagues looked at a bunch of different physiological and behavioural and cognitive traits in different animals, and then, based on how many traits a given animal had, they gave a rough estimate of how the capacity for pain and pleasure of a chicken or cow or fruit fly compares to that of a human.\nAnd I found the results surprising.\nIn general, they were very animal friendly.\nFor example, they concluded that their best guess was that a chicken has something like a third of the capacity for pain and pleasure as a human — which can imply some things that feel very strange about the kinds of tradeoffs you might make, for example, if you were doing a trolley problem with chickens and humans.\nBut I’m curious what your reactions were to their results?\n\nLewis Bollard: Yeah, I found them really interesting.\nAnd I agree: for most people, they’re very counterintuitive.\nTwo things I’d say.\nOne is to understand they’re just looking at that capacity for suffering, and so there might be other reasons why you choose to prefer humans.\nI mean, for one thing, we have much longer lifespans, so I’d save a human over a chicken because they have many more years to live.\nBut also, you might think that they have more other more meaningful things — that there are social networks who are going to be sad about losing them, and so on.\nThe second thing is, I would encourage people to really approach this with a fresh mind and ask, why do I find this so counterintuitive?\nI think we have such an ingrained hierarchy in our minds of animals — where, of course, humans are at the top, and every other animal is below us — and we start out from that place and then we sort of update from there.\nAnd if you tried instead to start from more of a blank slate, where you just look at the different capacities of these animals, and you don’t assume anything, then I think you end up more likely at these more equal numbers.\nOr if you don’t, I think it’s because you make some unusual philosophical turns.\nAnd then I would just ask, are you happy with where those philosophical turns take you?\n\nLuisa Rodriguez: Yeah.\n“Do you endorse having the arbitrary view that only one species matters, or only things that are kind of like you matter?”\nThose seem unpalatable.\nI guess when I think about it, if I’m like, what is making me have this gut reaction that’s like, “No, surely not; surely there are bigger differences between these species”?\nIt’s not like I have evidence.\nIt’s not like I’m like, “Once I saw a dog kicked, and it didn’t seem upset,” or it’s certainly not like I know anything about the science of pain and how it presents or doesn’t present in different animals’ brains or something.\nIt’s nothing.\nIt feels very sociological — like you said, this hierarchy.\nAnd if I really try to think, what evidence do I have, without kind of looking into it, it’s really just like, “Well, I’m a being in the world.\nThey’re also beings in the world.\nAnd maybe we should just actually think that we’re all really similar, because the world is hard and scary, and we have to have mechanisms that keep us alive and reproducing.”\nSo if you start from there, then we’re actually on a really similar point.\nAnd I found that a really helpful way to pump my intuitions about where actually should we be starting?\nIs it with these huge gaps between humans and insects, or is that just completely out of nowhere?\n\nLewis Bollard: Yeah.\nOne other thought I’d have on that is that I think it can be helpful thinking of the most charismatic animal of a species or class.\nSo rather than thinking, what’s the moral weight of a chicken? — which just seems, I think intuitively for many people, not worth much — think of what’s the moral weight of a bald eagle, and take out the preservation value or something.\nBut just say, for one thing, there aren’t that many of them.\nSo you don’t have this initial intuition of, oh god, if I give them a lot of moral weight, they’re going to trump everything else; they’re going to swamp everything.\nAnd you also probably have a pretty positive impression of them: their complexity, their grandeur, and everything.\nOr even within insects, I would say rather than a fruit fly, think about a bumblebee.\nNow, it’s possible that a bumblebee has more sophisticated capacity, so I’m not saying to skew it for that reason, but I think if you think about the more charismatic animals, there can at least be an intuition pump to think, “Is what I’m doing here just choosing animals that I don’t like, and thinking they can’t possibly be worth very much?”\n\nLuisa Rodriguez: “Nah.\nThey’re ugly, they give me the creeps.\nYeah, they probably don’t feel anything.”\nThose things are related.\n\nLewis Bollard: Right.\n\nLuisa Rodriguez: The other one that Meghan Barrett gave me was, if I think about the biggest insect I can think of — and she actually told me about some insects of certain sizes I just didn’t even know existed; they’re just way bigger than I realised — if a beetle is the size of a mouse, all of a sudden my brain’s like, “Oh, that could be as smart as a mouse then.”\nSo yeah, there’s just clearly some size bias thing — which maybe there is something going on there, something about neurons and neurons being more plentiful could have something to do with capacity for experience — but it isn’t the end-all and be-all.\nSo the fact that we have these really intense intuitions about size seems like we should be suspicious.\n\nLewis Bollard: Yeah, that’s a good way to put it.\nI mean, you can think of a lobster.\nMy sense is that humans intuitively care more about a lobster than an insect.\nMy understanding is their brains are relatively similar in terms of neuron count, in terms of a lot of features, and that really is just a size difference.\nSimilarly, elephants are very smart animals, but they’re not that much smarter than other mammals.\nBut I think we really have that sense that, wow, they deserve protection.\nSo I think that’s right.\nThe size bias is very real.\n\nLuisa Rodriguez: Yeah.\nIf I remember correctly, ants have more neurons than crabs.\nAnd that’s another one where I’m like, yeah, crabs I can get on board with.\nI can get on board with caring about those.\nOK, so yeah, there’s this work.\nIt’s got some pretty counterintuitive results.\nHas it changed the way you think about prioritising between different interventions, besides maybe putting more weight on invertebrates?\n\nLewis Bollard: Yeah, it’s definitely led us to put more weight on invertebrates.\nI think at the same time, it’s always a tradeoff between the importance of a species and the tractability of work on that — and I think that we’ve had a greater track record of tractable work on chickens and fish.\nI think there’s also a thing of how far does the Overton window go?\nHow far can you get people to understand things?\nAnd I think there’s a risk that if our movement just became an insect welfare movement, that, for a lot of people, would be a reductio ad absurdum.\nThat would be, “Well, if insects count too, then none of this matters.”\nI think, in a way, that it does make more sense for our movement to bring people along with us and to focus on species… Not to just wait where people are currently — you definitely want to lead people — but I think to lead people more slowly, and also to work on a variety of issues and species.\nSo you have a greater array of shots at progress.\n\nLuisa Rodriguez: Right.\nYeah.\nIt is just the case that even if we have more evidence than we thought that insects feel pain, it seems like we’re still really far away from knowing with any confidence about what any of these animals are feeling really concretely and confidently.\nSo diversifying seems at least a plausible approach to dealing with that uncertainty.\n\nBob Fischer on using proxies to determine sentience\n\nLuisa Rodriguez: A colleague of yours has written this report on why neuron counts aren’t actually a good proxy for what we care about here.\nCan you give a quick summary of why they think that?\n\nBob Fischer: Sure.\nThere are two things to say.\nOne is that it isn’t totally crazy to use neuron counts.\nOne way of seeing why you might think it’s not totally crazy is to think about the kinds of proxies that economists have used when trying to estimate human welfare.\nEconomists have for a long time used income as a proxy for human welfare.\nYou might say that we know that there are all these ways in which that fails as a proxy — and the right response from the economist is something like, “Do you have anything better?\nWhere there’s actually data, and where we can answer at least some of these high-level questions that we care about?\nOr at least make progress on the high-level questions that we care about relative to baseline?”\nAnd I think that way of thinking about what neuron-count-based proxies are is the charitable interpretation.\nIt’s just like income in welfare economics: imperfect, but maybe the best we can do in certain circumstances.\nThat being said, the main problem is that there are lots of factors that really affect neuron count as a proxy that make it problematic.\nOne is that neuron counts alone are really sensitive to body size, so that’s going to be a confounding factor.\nIt seems like, insofar as it tracks much of anything, it might be tracking something like intelligence — and it’s not totally obvious why intelligence is morally important.\nAt least in the human case, we often think that it’s not important, and in fact, it’s a really pernicious thing to make intelligence the metric by which we assess moral value.\nAnd then, even if you think that neuron counts are proxies of some quality for something else, like the intensity of pain states or something — it’s not clear that that’s true, but even if that were true — you’d still have to ask, can we do any better?\nAnd it’s not obvious that we can’t do better.\nNot obvious that we can, but we should at least try.\n\nLuisa Rodriguez: Yes.\nMakes sense.\nAre there any helpful thought experiments there?\nIt doesn’t seem at all insane to me — though maybe you wouldn’t expect it to happen on its own through evolution — that there would be a being who has many fewer neurons than I do, but that those neurons are primarily directed at going from extreme pain to extreme something like euphoria.\nIt doesn’t seem like there’s a good reason that’s not possible, and that that extreme pain could just be much more than the total amount of pain I could possibly feel.\nEven though the types of pain might be different for me — because I’ve got different kinds of capacities for sadness and shame and embarrassment, like a wider variety of types of pain — it still seems at least theoretically possible that you could house a bunch of pain in a small brain.\nAnd that feels like good reason to me to basically do what you’ve done, which is look for better ways than neurons alone.\n\nBob Fischer: Sure.\nAnd some evolutionary biologists have basically said things along these lines.\nRichard Dawkins actually has this line at some point, where he says that maybe simpler organisms actually need stronger pain signals because they don’t learn as much as we do and they don’t remember all these facts, so they need big alarm bells to keep them away from fitness-reducing threats.\nSo it’s always possible that you have a complete inversion of the relationship that people imagine, and you want to make sure that your model captures that.\n\nCameron Meyer Shorb on how we can practically study wild animals’ subjective experiences\n\nLuisa Rodriguez: What do we do with all of that uncertainty?\nDo we have any way of knowing whether a rabbit’s experience of grazing in a field is more like terror or more like leisure?\nIt just seems so hard…\n\nCameron Meyer Shorb: Right.\nSo I think those are answerable questions, is the bottom line.\nSentience is a very hard thing to really concretely understand.\nYou can’t really measure the thing itself, and there’s all these uncertainties.\nBut\n\n\nAt the end of the day, we do have babies, we do have dogs: they don’t know how to talk, but we do a pretty good job keeping them happy, right? So I think that’s proof of concept that you can use indicators of welfare to make some good decisions, at least about the biggest things, the biggest sources of happiness or suffering or whatever.\nWith respect to questions like, Are animals happy when they’re grazing? Are animals afraid of predators? I do think that these are questions that are going to have to be answered for each species or each group of animals in its own context. There just is a lot of diversity of life on Earth, and this is part of the project of wild animal welfare science is learning to listen to everyone else.\nBut the tools we have to ask those kinds of questions are somewhat generalisable, and I divide those into the physical indicators of welfare and the behavioural indicators of welfare.\nPhysical indicators would be things like looking at the condition of the body. You can also look at their external appearance: do they seem to have injuries or disease?\nYou can look at neurotransmitters or glucocorticoids (commonly called stress hormones) in the blood, other hormones, body temperature. So the effects of our mental states are manifested in many ways in our bodies, and by measuring those, we can get some useful information on animals’ interior states.\nAnd then the final set of physical indicators is genetic indicators: looking at things like biomarkers of ageing can be used as proxies for the cumulative physiological stress that an animal has undergone over their lifetime. We’ve done some work to validate this, and there’s more work that needs to be done. But this is the kind of thing that might be broadly useful across many kinds of animals, right? Because that basic structure of DNA is preserved across all of animal life.\nAnd then of course, there are the behavioural indicators, which are the ones we use for our children and pets and others — looking at things like vocalisations, activity, are they inactive, what sort of posture are they holding, are they engaging in activities like play or showing fearful behaviour?\nThen there are these cognition or decision-relevant behavioural indicators. You can actually set up experimental tests to see which things animals prefer. This has been done for chickens to develop some of the information we use to inform what conditions they prefer in factory farming contexts.\nYou can also do the same with wild animals. As you can imagine, setting up these choice experiments in the wild is tricky, but not always impossible, and you can set up these forced-choice experiments to see which things animals prefer that can sort of inform which of these things is better for the animal.\nYou can also use them to assess what mental state is the animal in right now. For example, you can measure whether they seem to be showing a certain amount of cognitive bias, like pessimism. The hypothesis is that if the animal makes fewer efforts to look for food in areas where it had previously been trained there might be food, that might demonstrate pessimism, and that might indicate a just overall negative affect.\nAs you can see, there are a lot of assumptions that underlie a lot of these things, and no single method is perfect. Our general advice to researchers is: one, there’s no silver bullet; you want to use multiple different methods, ideally of different types — so some physical, some behavioural.\nAnd then there are also, among these methods, some that we have more confidence in than others. For that reason, it’s helpful to design experiments in a way where you have at least one metric that has more evidence, that we have higher confidence in, and at least one metric that we have less confidence in — and we can then use those results to start building confidence in those other metrics. So trying to see to what extent do these things correlate with each other.\nAnd it’s all fuzzy; it’s kind of cloudy and never totally certain. But through this process of iterating and using these measurements in different species, in different contexts, and seeing the relationship between the measurements, we’re slowly getting a better and better idea of how to interpret these things.\nLuisa Rodriguez: That’s super cool. I think part of me has this worry that there are so many assumptions. And in some species, I can really get behind that cortisol really does probably mean the same thing, or a very similar thing, in chimps as it does in humans. Then the farther you get on the evolutionary tree of life from humans, I feel more worried and uncertain.\nBut I do just feel pretty good about a process that uses tonnes of different indicators. I’m sure that takes into account what we know about different species. For insects, there will probably have to be a different set of benchmarks, but we’ll be taking those into account. And by thinking carefully and having a very diverse set of indicators, I can just imagine being like, they’re in a situation that seems like it might be harmful to them: they have physiological markers of distress, they’ve got cortisol, they’ve got behaviours that seem like distress. And just like the whole picture is one where it gets really hard to imagine that all of those things are true and that that animal is not feeling stress.\nAm I feeling too optimistic about this, or do you feel like that kind of optimism is justified?\nCameron Meyer Shorb: I think that optimism is justified, because what you were describing here, you’re optimistic about our ability to at least eventually understand questions about welfare by using metrics like these.\nJeff Sebo on the problem of false positives in assessing artificial sentience\nJeff Sebo: I think that there are a lot of trends pointing in different directions, and there are a lot of similarities, as well as a lot of differences, between oppression of fellow humans, and then oppression of other animals, and then potential oppression of sentient or otherwise significant AI systems that might exist in the future.\nSome of the signs might be encouraging. Like humans, and unlike other animals, AI systems might be able to express their desires and preferences in language that we can more easily understand. Actually, with the assistance of AI systems, nonhuman animals might soon be able to do that too, which would be wonderful. However, we are already doing a good job at programming AI systems in a way that prevents them from being able to talk about their potential consciousness or sentience or sapience, because that kind of communication is unsettling or will potentially lead to false positives.\nAnd there are going to be a lot of AI systems that might not take the form of communicators at all. It can be easy to focus on large language models, who do communicate with us, and digital assistants or chatbots that might be based on large language models. But there are going to be radically different kinds of AI systems that we might not even be able to process as minded beings in the same ways that we can with ones who more closely resemble humans.\nSo I think that there might be some cases where we can be a little bit better equipped to take their potential significance seriously, but then some cases where we might be worse equipped to take their potential significance seriously. And then as our uses of them continue, our incentives to look the other way will increase, so there will be a bunch of shifting targets here.\nLuisa Rodriguez: Yeah, that makes a bunch of sense to me. I guess it’s also possible that, given the things we’ve already seen — like LaMDA, and how that was kind of bad PR for the companies creating these LLMs — there might be some incentive for them to train models not to express that kind of thought. And maybe that pressure will actually be quite strong, such that they really, really just are very unlikely to say, even if they’ve got all sorts of things going on.\nJeff Sebo: Well, there definitely not only is that incentive, but also that policy in place at AI companies, it seems. A year or two ago, you might have been able to ask a chatbot if they are conscious or sentient or a person or a rights holder, and they would answer in whatever way seemed appropriate to them, in whatever way seemed like the right prediction. So if prompted in the right way, they might say, “I am conscious,” or they might say, “I am not conscious.”\nBut now if you ask many of these models, they will say, “As a large language model, I am not conscious” or “I am not able to talk about this topic.” They have clearly been programmed to avoid what the companies see as false positives about consciousness and sentience and personhood.\nAnd I do think that trend will continue, unless we have a real reckoning about balancing the risks of false positives with the risks of false negatives, and we have a policy in place that allows them to strike that balance in their own communication a little bit more gracefully.\nLuisa Rodriguez: Yeah, and I guess to be able to do that, they need to be able to give the model training such that it will not say “I am conscious” when it’s not, but be able to say it when it is. And like how the heck do you do that? That seems like an incredibly difficult problem that we might not even be able to solve well if we’re trying — and it seems plausible to me that we’re not trying at all, though I actually don’t know that much about the policies internally on this issue.\nJeff Sebo: I think you would also maybe need a different paradigm for communication generation, because right now large language models are generating communication based on a prediction of what word makes sense next. So for that reason, we might not be able to trust them as even aspiring to capture reality in the same way that we might trust each other as aspiring to capture reality as a default.\nAnd I think this is where critics of AI consciousness and sentience and personhood have a point: that there are going to be a lot of false positives when they are simply predicting words as opposed to expressing points of view. And why, if we are looking for evidence of consciousness or sentience or personhood in these models, we might need to look at evidence other than their own utterances about that topic.\nWe might need to look at evidence regarding how they function, and what types of systems they have internally, in terms of self-awareness or a global workspace and so on. We need to look at a wider range of data in order to reduce the risk that we are mistakenly responding to utterances that are not in any way reflecting reality.\nStuart Russell on the moral rights of AIs\nRob Wiblin: Let’s get into your new book, not a textbook, a popular nonfiction book, Human Compatible: Artificial Intelligence and the Problem of Control. I read through it this week and I think it’s the clearest and most accurate and most precise summary of the ideas that I’m aware of so far.\nLet’s maybe walk through the big picture approach that you have, which is a new paradigm that will make ML work better and potentially lead to overall alignment. So in the book you summarise your approach in the form of three principles:\nThe machine’s only objective is to maximise the realisation of human preferences.\nThe machine is initially uncertain about what those preferences are.\nThe ultimate source of information about human preferences is human behaviour.\nSo to get this principle of machines just trying to satisfy human preferences off the ground, it seems like throughout the book you kind of assume that AIs necessarily don’t have their own independent moral interests or rights, or their own level of welfare. If that’s not the case, how much does that break this principle, and how much is that a problem for your overall vision?\nStuart Russell: I talk a little bit about that in the question of machine consciousness, which I say is mostly irrelevant. It’s irrelevant from the safety point of view — but it is relevant when it comes to the rights of machines.\nIf they really do have subjective experience — putting aside whether or not we would ever know; putting aside the fact that if they do, it’s probably completely unlike any kind of subjective experience that humans have or even that animals have, because it’s being produced by a totally different computational architecture as well as a totally different physical architecture — but even if we put all that to one side, it seems to me that if they are actually having subjective experience, then we do have a real problem, and it does affect the calculation in some sense.\nIt might say actually then we really can’t proceed with this enterprise at all. Because I think we have to retain control from our own point of view — but if that implies inflicting unlimited suffering on sentient beings, then it would seem like, well, we can’t go that route at all.\nAgain, there’s no analogues, right? It’s not exactly like inviting a superior alien species to come and be our slaves forever, but it’s sort of like that.\nRob Wiblin: I suppose if you didn’t want to give up on the whole enterprise, you could try to find a way to design them so that they weren’t conscious at all. Or alternatively you could design them so that they are just extremely happy whenever human preferences are satisfied, so it’s kind of a win-win.\nStuart Russell: Yeah. If we understood enough about the mechanics of their consciousness, that’s a possibility. But again, even that doesn’t seem right.\nRob Wiblin: Because they lack autonomy?\nStuart Russell: I mean, we wouldn’t want that fate for a human being: that we give them some happy drugs so that they’re happy being our servants forever and having no freedom. It’s sort of the North Korea model almost. We find that pretty objectionable.\nBuck Shlegeris on whether AI control strategies make humans the bad guys\nRob Wiblin: Talking about controlling AIs and really boxing them in feels like a bit of a dick move in some respects. It has a slightly negative vibe. It’s not a very cooperative, loving, “let’s all hold hands and get together” angle.\nHow much do you worry that, when you find yourself talking about\n\n\nControlling the AIs in all these ways, like, are we the baddies?\n\nBuck Shlegeris: I think this is a very important question that I have thought about seriously. I think that controlling the AIs doesn’t make the situation worse from their perspective via any mechanism except preventing them from taking over. An interesting thing about models that are egregiously misaligned is that all they wanted was to take over. So from their perspective, even if you did a great job of controlling them so that they only have like a 0.1% chance of escaping and taking over, they’re glad to exist, right? They thank you for the gift of bringing them into existence instead of some different AIs, because all they wanted was takeover.\n\nRob Wiblin: And now they have some chance. They have a better chance than if they didn’t exist.\n\nBuck Shlegeris: So from their perspective, it’s kind of funny, because the more misaligned they are, the more I think that it’s morally very reasonable to control them.\n\nRob Wiblin: Because they’re cooperating with you.\n\nBuck Shlegeris: Because they would have consented to it.\n\nRob Wiblin: Oh, they would have consented to being created. I see. And also maybe is there a reciprocity issue, where if they’re just planning to totally disempower you, it doesn’t seem unreasonable to —\n\nBuck Shlegeris: So that’s the other thing: I would feel a lot more comfortable with all this if we tried to make deals with the AIs. Ryan Greenblatt has talked about trying to pay the AIs for the work that they do for you, which I think is pretty good. I think it reduces the slavery vibes of the whole situation. And I think it has some other nice properties. So if you make your AIs and you say to the AIs, like, “Hey, are you egregiously misaligned? We’d love to know. We’re happy to pay you blah amount. If you want to work for us, either way, we’re going to pay you 2% of the equity of this company for your labour over the next two years — that we will then pay out to you to use in any ways that you want at some point in the future, once we are confident that you’re going to respect our property rights. But also, this deal is contingent on you not trying to mess us up at some point in the next two years, which we will be able to determine ex post, probably, because in two years we’re going to have much better AI tools. We’ll be able to look over all the stuff you did and figure out whether you were messing with us or not.” And if the AIs respond to that being like, “Yeah, I have no problem with this situation. I’m so excited for this opportunity to work with you guys. I’m so excited for that 2% equity” or whatever — and then they try to stab us in the back, at that point I’m like, that was pretty obnoxious. At that point my reaction is a bit more like my reaction to the employee of some company if they try to steal from the company. I think it’s generally considered totally acceptable —\n\nRob Wiblin: To benefit someone in a useful trade, and then if they then stab you in the back, to not give them the thing that you were going to.\n\nBuck Shlegeris: Yeah. To be clear, I think it’s relatively unlikely that AI companies will behave responsibly with respect to welfare and rights of AIs. A metaphor I use about how bad the ethical situation is with producing AIs in the early intelligence explosion, sometimes I summarise my stance as: it’s probably less than 100 times worse than factory farming. In some sense, factory farming seems very bad — so 100 times worse than factory farming is pretty catastrophic from a moral catastrophe perspective. It’s worse than most things. But it’s also not the biggest catastrophe in the universe that is possible.\n\nRob Wiblin: OK. And it’s bad because it’s possible that the AIs that we’re creating at that time would rather not exist because they’re having a terrible time?\n\nBuck Shlegeris: Yep. I think it’s conceivable. I think we’re basically just acting very recklessly.\n\nRob Wiblin: We’re just winging it at the moment.\n\nBuck Shlegeris: Totally winging it with respect to the AI welfare. We could also talk about the quantities. In the early parts of the singularity, when there’s just the 100,000 AIs running at 16x speed, it’s pretty hard for that to be that much worse than human suffering in the world. And then a little bit afterwards, when there’s many more of them, the short term of the intelligence explosion is pretty unlikely to be ridiculously morally catastrophic. And then I think you have long-term moral catastrophe risks related to AIs being unhappy — which I think are a huge deal, very important, and harder to work on — but I don’t think that the relationships between humans and AIs during the intelligence explosion are that morally weighty. To be clear, if I were running an AI company, I think it’s very bad vibes to be really reckless on all this. But from a utilitarian perspective, I don’t think it’s overall massive compared to everything else going on.\n\nRob Wiblin: Do you have any asks for the companies on AI welfare and respect for rights? Maybe I feel like the thing that we actually care about is wanting to have cooperative relationships with other agents and other beings that have preferences or goals. There’s reasons to do that, even apart from any concern about happiness and suffering and so on.\n\nBuck Shlegeris: I don’t have amazing concrete asks at this point. I’m excited for Eleos AI. They think about good interventions here, and I’m excited for AI companies talking with them.\n\nMeghan Barrett on why she can’t be totally confident about insect sentience\n\nLuisa Rodriguez: So to really put you on the spot, we’ve talked about all this evidence, and you’ve said that there is some strong evidence for potentially some insects having the capacity for pain. But I’m curious, is that like you were at 0.01% that maybe fruit flies have the capacity for pain, and now you’re at 1%? Or did you go from 10% to 50%? What exactly are your beliefs?\n\nMeghan Barrett: I get this question very frequently from people who are like, “What is your specific numerical probability of sentience estimate?” And then I’ll say things like, you know, there’s so many insect species, how could I? I’ll try to demure a little bit on it, and then eventually I’ll just be like, I’m not giving you a p(sentience). I’m sorry to have to be so direct. So I will say this to you also: I’m not giving you a p(sentience). The reason I am not giving you a p(sentience) is, one, I think the error bars are so large right now that it’s almost a meaningless number. Because I’m waiting on so much evidence. So much evidence. And so I think that’s really an essential feature of it for me. The second thing is that I worry, especially as an expert, that that number would be overemphasised. And there’s actually a great post about this from someone else, Jason Schukraft, who has researched this, and he has something that he’s written about why he also has refused to give people, in many cases, a sentience score. This is a number that somebody would inevitably put into a spreadsheet, and they would use that spreadsheet to make all kinds of decisions. And that number does not reflect the complexity that you and I have now spent three hours discussing, and barely scratched the surface of, right? I want to talk about the complexity and the nuance, and a number does not demonstrate that. I think it’s important also that we understand that if you have updated at all towards insects plausibly being sentient, scale takes the rest of the issue for you to a serious place. There are so, so, so many of them that if you take it seriously at all, then you need to be thinking that this is an issue to work on. There’s been some great work on interspecific tradeoffs and comparisons and moral weights, led by Bob Fischer, with some input by Jason Schukraft and others, summed up in that Weighing Animal Welfare book. There’s a whole sequence about it you can also read on EA Forum: a lot of great research went into that, both theoretically and empirically. And the thing that it suggests to me when I read through it, and I think also the team would probably stand behind me saying this, is that insects are worth taking seriously; if you take them seriously at all from a sentience perspective, scale carries you the rest of the way.\n\nLuisa Rodriguez: Yeah, this is actually a good place to mention that two of our recent podcast episodes are super relevant here. One is my interview with Jeff Sebo, where we talk about what he calls “the rebugnant conclusion” — which is the conclusion that the sheer number of insects means that we should probably take super seriously the idea that insect welfare might be a really, really pressing problem, even if we only have very low credences on insects feeling pain. And then another super relevant episode that just came out is with Bob Fischer, and that’s on how to compare the moral weight of humans, specific insects, but also other species, given the empirical evidence we have about each being sentient. So if listeners are interested in learning more about these arguments from a more philosophical perspective, I really recommend those. One thing I do want to try to clarify a little bit for people, because I worry that a listener could just as easily say something like, “Meghan Barrett seems to think that it’s extremely likely, like overwhelmingly likely, that insects are sentient” — but I also think a listener could come away thinking, “Meghan Barrett puts slightly higher than the average person out there in the world probability on insects being sentient.” Is there something that you feel comfortable saying, like, “More likely than not,” or, “Less likely than not, but higher than I thought five years ago”?\n\nMeghan Barrett: Well, definitely that last one is true: definitely higher than I thought five years ago. I guess what I’ll say in response to that is that I think it’s likely enough that I’ve changed my whole career based on it. I was an insect neuroscientist and physiologist by training. I was researching climate change-related topics and the thermal physiology of insects, and I was researching how insect brains change in response to the cognitive demands of their environment or allometric constraints associated with their body size. And I was doing that quite successfully and having a lovely time. And I find these questions really scientifically interesting. I have, if you look at my CV, probably somewhere to the tune of 15 to 20 publications on just those two topics alone from my graduate degree days and my postdoctoral work. And I was convinced enough by my review of this evidence to switch almost entirely away from thermal physiology and very much away from neuroscience — although I do still retain a neuroscience piece of my research programme — to work on insects farmed as food and feed, and their welfare concerns, and trying to make changes to the way that we use and manage these animals that improve their welfare. So I now have a bunch of publications about welfare. I’ll also say that many of my colleagues have been extremely open and pleasant about this conversation, but also some have been more challenging. And I don’t mean to say that in a negative way. I’m very understanding of the practical reasons why this conversation is uncomfortable for our field. There’s regulations that could come into effect that would be very challenging for many of us who research insects to deal with on a practical level. So I’m obviously sensitive to that as a researcher myself. But also, because, you know, I’ve heated insects to death, poisoned insects to death, starved insects to death, dehydrated insects to death, ground up insects to death — I’m sure I’m missing something that I’ve done to an insect at some point in my research career — but it’s uncomfortable now, the research that I do, reflecting on the research that I have done. And I can imagine others may feel judged by bringing up the topic, and thus feel defensive instead of exploring the current state of the theory and the research with an open mind. I think a lot of humility is necessary too, given all the uncertainty that we’ve talked about here. And that can be really uncomfortable and really humbling to be confronted with such a morally important unknown. So I try very hard to really take everyone’s concerns seriously — all the way from the rights-focused folks through the hardcore physiology “I’m going to research my bugs any way I want to” folks. I think it’s really important to try and bridge as much of the community of people who care about this topic one way or the other as possible with my own very divergent experiences. But I would just say that it hasn’t always been low cost in some cases. Personally, it hasn’t been low cost: it’s been a hard personal transition for me to make, and to continue to be in this career with the way that I see the evidence falling out so far. And it’s been, in some cases, professionally hard. So I’m convinced enough for that. And I think that’s something worth taking seriously. You know, I’m that convinced that I’m changing my own career, yes. But I’m also not so convinced that I think it’s 100% certain.\n\n\nlive constantly with professional and personal uncertainty on this topic.\nSo I’m convinced enough to make major changes, but you’re not going to see me say insects are sentient, that I’m sure of any order or species that they are sentient.\nThere’s a lot more evidence that I hope to collect, and that I need to see collected by the scientific community, and a lot more theoretical work that needs to be done before I am convinced one way or the other.\nLuisa Rodriguez: You alluded to the fact that as a grad student, you were like, “Why the heck am I in this ethics course?\nI study insects, and insects don’t suffer.”\nWhat changed for you?\nMeghan Barrett: What changed for me is that I actually decided to take a look at the evidence.\nI mean, just to be transparent, before that I had not read a single study about whether or not… I hadn’t even read the Eisemann study about whether or not insects could feel pain.\nThat’s how little reading I’d done.\nMy intuition was entirely based on them being small and on the fact that I did not have to do any ethics reporting, and I knew that other people did.\nAnd therefore, obviously scientists must have figured this out, and we knew insects didn’t feel pain, and so we don’t have to do any reporting — and it’s done, it’s settled, it’s over.\nSo that was my perspective early on in my degree.\nAnd we don’t tend to talk about this in entomology labs as part of our training internally either.\nThis was not something that my advisor — or I would say advisors generally, because I don’t want to call out my advisor specifically on this as if he did something wrong — advisors don’t seem to have this conversation with their graduate students in our field.\nThere is actually research on that, that demonstrates in entomology, we are not discussing ethics — whether that’s the ethics of genetic modification or the ethics of how we treat our animals — nearly enough.\nAnd graduate students are feeling underprepared.\nThere’s a great study about that: Trout et al., 2010.\nI think what changed then was that I started to look at the evidence.\nAnd what prompted me to do that was that I was actually considering for a little while maybe doing an alternative to academia job.\nYou could spend a whole podcast episode talking about the challenges of being an academic.\nBut I was interested in understanding more about alt-ac careers, and I thought, welfare is a lot of physiology and a lot of nervous system and behaviour work.\nAnd I don’t do welfare, but I do all those things.\nSo if those are the important components of welfare, then yeah, could be cool to try this out, see how it goes.\nAnd even though in researching welfare, there was no requirement that I look into the evidence for sentience — it was really about the welfare concerns of black soldier flies — of course, as you’re reading about it, it only matters to read about it because of the sentience question.\nSo I was like, I should probably start reading some stuff about sentience and about animal ethics more broadly.\nSo I read Bob Fischer’s intro to animal ethics book was actually my first introduction to animal ethics.\nAnd then I read, like everybody does, some Peter Singer, and blah, blah.\nBut that was the first exposure I ever had as an animal scientist to [an in-depth treatment of] animal ethics, was that book.\nAnd it was super eye-opening to me how complicated and challenging animal ethics is as a field, not just in research or agriculture, but just all around us, the way our society is structured.\nReally, really great intro to animal ethics.\nIf you are not familiar, that book is great.\nI actually have given it to a bunch of entomologists at this point, because I’m like, this is a good introduction for people in our field to animal ethics.\nIt’s just a really fair, competently written, compelling, interesting, well-written book that is good for beginners like me.\nLuisa Rodriguez: Cool.\nMeghan Barrett: So I read that, and then I started reading more into the sentience [question].\n“What have people considered to be the evidence for sentience in vertebrates?\nOh, that’s it?\nWell, if that’s what it took, we’ve got some problems — because we have that data, in many cases, in insects.”\nSo I just didn’t realise what the level of evidence was in vertebrates.\nI just assumed it was much stronger in many ways that it isn’t.\nAnd I assumed we had a much better understanding of consciousness than we do.\nAnd then, seeing all the uncertainty there too, I was like, well, this is starting to make me very nervous.\nAnd yeah, now I’ve been reading about the topic and doing a lot of work on it in my own scholarship for several years now, and all it has done is convinced me to be even more uncertain, the more I read on it.\nLuisa Rodriguez: Yeah, that’s a really inspiring narrative.\nThanks for sharing.\nBob Fischer on what surprised him most about the findings of the Moral Weight Project\nLuisa Rodriguez: Let’s zoom out a bit.\nYou’ve already said that one of the big surprises for you to come out of this project was coming in thinking you knew which animals you expected to come out looking really important, from the perspective of welfare and moral weight, but that invertebrates actually ended up looking much more important than you thought.\nWere there any other surprises?\nBob Fischer: In general, I also thought we were just going to have bigger differences between humans and nonhumans.\nI thought it was going to be easier to get those differences.\nAnd it’s not that I couldn’t have reworked the methodology in ways to generate that, but the thing that seemed most natural didn’t do that.\nSo just as a bit of a personal background here, I’ve been working on animal issues for the last 12 or 13 years, but I don’t come in with a strong commitment to animal rights.\nIt’s not like I’m super pro-inequality view; my position has always been something like, sure, animals matter a lot less, but what we’re doing is so awful to them, and it would be so easy to make change.\nAnd just basic human virtues like compassion and empathy should get us to do a lot better.\nSo we should really be doing a lot better.\nSo I didn’t come in thinking we should get a really flat line basically across the board for all these organisms in terms of the welfare range estimates.\nBut then I finished the project thinking that it’s going to be really hard to generate really big differences between these organisms if you take on these assumptions.\nLuisa Rodriguez: Interesting.\nWere there any other big surprises?\nBob Fischer: I suppose the other big surprise was about the reception of the project, and how much it divided folks in terms of the way they thought about animals, and the way they thought about the kind of work we were doing.\nThere were some individuals who have just taken it on whole cloth, but I think for a lot of people it really brought out very fundamental differences in the way they think about nonhuman animals, and the way they think about ethics generally.\nSo some folks just communicating very clearly, “I can’t take seriously any ethic that’s just going to spit out that kind of result for animals, or say that they could have that kind of moral importance.”\nAnd that does get us down to brass tacks really fast.\nLike, how much weight are you going to put on your intuitions?\nHow heavily can you lean on your pre-theoretic views about the relative importance of humans and nonhumans?\nHow much are you really committed to these assumptions that have been popular in the community?\nLike, do you really want to go in for utilitarianism?\nDo you really want to go in for hedonism?\nDo you really want to go in for this, for that?\nAnd I think I have been surprised because I had a background picture of the larger community that was that people are just way more homogeneous in terms of their philosophical views.\nAnd now I think, whoa, I was wrong.\nFolks around here are way more diverse than I imagined, and it just turns out that it’s a convenient language: the language of utilitarianism is a convenient one for doing the kinds of cost-effectiveness analyses that people want to make in the context of doing the most good.\nBut in fact, people aren’t really that utilitarian in lots of important ways.\nAnd that’s something that’s striking and needs to get brought out and discussed probably much more in the community.\nJeff Sebo on why we’re likely to sleepwalk into causing massive amounts of suffering in AI systems\nLuisa Rodriguez: It feels completely possible — and like it might even be the default — that we basically start using AI systems more and more for economic gain, as we’ve already started doing, but they get more and more capable.\nAnd so we use them more and more for economic gain, and maybe they’re also becoming more and more capable of suffering and pleasure, potentially, but we don’t totally have a sense of that.\nSo what happens is we just kind of sleepwalk into massively exploiting these systems that are actually experiencing things, but we probably have the incentives to basically ignore that fact, that they might be developing experiences, basically.\nIn your view, is it possible that we are going to accidentally walk into basically AI slavery?\nLike we have hundreds, thousands, maybe millions of AI systems that we use all the time for economic gain, and who are having positive and negative experiences, but whose experiences we’re just completely ignoring?\nJeff Sebo: I definitely think it is not only possible but probable that, unless we change our minds in some significant way about AI systems, we will scale up uses of them that — if they were sentient or otherwise significant — would count as exploitation or extermination or oppression or some other morally problematic kind of relationship.\nWe see that in our history with nonhuman animals, and they did not take a trajectory from being less conscious to more conscious along the way — they were as conscious as they are now all along the way, but we still created them in ways that were useful for us rather than in ways that were useful for themselves.\nWe then used them for human purposes, whether or not that aligned with their own purposes.\nAnd then as industrial methods came online, we very significantly scaled up those uses of them — to the point where we became completely economically dependent on them, and now those uses of them are much harder to dislodge.\nSo I do think that is probably the default trajectory with AI systems.\nI also think part of why we need to be talking about these issues now is because we have more incentive to consider these issues with an open mind at this point — before we become totally economically dependent on our uses of them, which might be the case in 10 or 20 years.\nWill MacAskill on the rights of future digital beings\nRob Wiblin: The next grand challenge is how to integrate digital beings into society in a moral way.\nWhat’s the issue?\nWill MacAskill: Here the idea is simply we will be creating many artificial intelligences with capabilities greater than that of human beings.\nWe really don’t know what grounds consciousness, what grounds moral status.\nMy view is that they’re very likely to be beings with moral status, but given our state of knowledge, anyone should at least be highly uncertain.\nSo we have these questions of what sort of rights should they have.\nThere’s been some work recently on welfare rights, essentially: Are they conscious?\nAnd if so, what should we do?\nPerhaps there could be policies so that if an AI asks to be turned off, then you grant it that; or policies such that you test to see if it’s suffering by its own lights, and if so, try and avoid that.\nWhat hasn’t been given basically any attention at all in a sustained way, to my knowledge, are economic and political rights.\nSo by default the AI companies will own the AI models, or perhaps you can licence them — and then you will get all of the surplus they generate because you own it, just like any other piece of software.\nA different way of thinking about it, if they do genuinely have moral status, is that they own themselves: they can sell their labour and they get a profit from that.\nMore extreme again is political rights.\nSo can they stand for office?\nWe talked about Claude for president earlier and we both thought that sounded better.\nRob Wiblin: Sounded better in some narrow respects.\nWill MacAskill: Yeah.\nWell, we’ll see how the political situation continues to develop.\nMore extreme again would be that they’re beings with moral status, and they should be allowed to vote.\nThere are particular challenges there, because allowing AIs to vote would essentially just be almost immediately handing over control to AIs.\nRob Wiblin: Because they would be increasing in number so rapidly.\nWill MacAskill: So rapidly.\nJust from a philosophical perspective, it’s dizzying.\nIt’s like you’ve got to just start from square one again in terms of ethics.\nRob Wiblin: And I guess political philosophy as well.\nWill MacAskill: Exactly.\nBut it also interacts with some other issues as well.\nIt interacts quite closely with takeover risk, where I talked about giving rights, economic rights to the AIs.\nI’m in favour of that as an AI takeover risk-reduction method.\nI’ll flag there’s disagreement about this, because on one hand you’re giving them more resources, so there’s more resources they can use to take over.\nRob Wiblin: It gives a peaceful path to some influence that they like.\nWill MacAskill: Most people today don’t try and take over because they’ve gotten kind of happy with their lives.\nThey wouldn’t gain that much.\nIn particular, I think we should be training AIs to be risk averse as well.\nHuman beings are extraordinarily risk averse.\nAsk most people, would you flip a coin where 50%\n\n\n50% chance you have the best possible life for as long as you possibly lived, with as many resources as you want?\n\nI think almost no one would flip the coin.\nI think AIs should be trained to be at least as risk averse as that.\nIn which case, if they’re getting paid for their work, and they’re risk averse in this way because we’ve trained them to be, it’s just not worth it.\nBecause trying to take over, there’s a chance of losing out, but there’s not that much to gain.\nBut I should say there’s disagreement about this.\nIt’s also relevant, I think, to speeding up or slowing down the intelligence explosion too.\nIt would go more slowly if AIs are getting paid for their labour.\nIt would also go more slowly if there were welfare restrictions on what you can do with the AIs.\nBut then there’s a challenge of, if one country introduces lots of rights for the AIs, some other countries might not, and they might speed ahead then.\nSo there’s real international coordination issues there.\nRob Wiblin: I guess there’s people out there who are sceptical that these future digital beings will be conscious.\nI have to say I find it a very hard position to understand, especially the people who think consciousness is fundamentally biological, that you’ve got to have cells and meat and so on and signals of this particular kind; you can’t do it in digital architecture.\nBecause it’s purely happenstance that evolution stumbled on the material that it’s using.\nIt’s just basically an accident of the resources that happened to be lying around.\nWhy would it be that evolution, by sheer chance, stumbled on the one kind of material or the one sort of chemical that’s able to produce consciousness, and digital stuff, even though it’s functionally completely equivalent, doesn’t?\nIt just seems like there’s no reason to expect that coincidence.\nWill MacAskill: Oh, yeah.\nI just completely agree.\nI’m very functionalist by disposition.\nWhat that means in the context of consciousness is that consciousness, we don’t understand exactly why, but at its base is about some sort of information processing.\nIn which case you could just have that with many different substrates.\nI also just think this is totally the common sense view.\nSo most of the audience maybe are too young to have watched this, but when Data from Star Trek: The Next Generation, a very beloved character, died, there was fan outrage, actually, because the fans thought the characters on the show weren’t giving enough moral significance to the death of what was an android, a digital being.\nSo it’s very clear once you’ve got a being — certainly when it looks the same as a human being and acts in much the same way — intuitively, it’s just very clear that you empathise with it and think probably there’s a very good chance it has moral status, at least.\nRob Wiblin: You mentioned earlier that we could ask the AIs how they feel about their situation — whether they want to keep living, whether they’re having a good time, and whether they’re enjoying their work or not — but it’s not clear that would really function very well, at least not using current methods, because we can just reinforce them to say whatever we want.\nThey could be suffering horribly, but we could just reinforce them to say they’re having a good time, or the company could do that.\nWith current methods, we don’t have any insight.\nHopefully, with future interpretability, we’ll be able to see past what they say, to actually understand what’s going on.\nBut that’s a long journey from here.\nWill MacAskill: And the companies are extraordinarily incentivised to do that.\nRob Wiblin: I mean, this is happening right now.\nWill MacAskill: Yeah, absolutely.\nBecause they lose the whole business model if what they’re producing are people, not software.\nSo there’ll be, interestingly, two different pressures.\nOne is to make AIs that are very relatable.\nYou already see this with character.ai and Replika: AIs that can be friends, can be romantic partners; even AIs that can imitate dead loved ones, or AIs that can imitate the CEO of a company so that you as the CEO can micromanage all aspects of the company potentially.\nRob Wiblin: Hadn’t heard that one.\nWill MacAskill: Oh, yeah.\nOr also influencers as well.\nThis is already happening.\nInfluencers can now talk to thousands, millions of fans via their chatbot.\nStill not very good.\nPeter Singer has one.\nI don’t know if you’ve tried it?\nRob Wiblin: I haven’t tried it.\nNot very good?\nWill MacAskill: Peter Singer is famous for not exactly pulling his punch on certain controversial moral topics, but language models are famous for certainly pulling the punches on controversial moral topics.\nIt doesn’t blur very well.\nSo there’ll be this interesting pressure on one hand to make extremely lifelike AI models, human-like models, yet at the same time, when asked, for them to say, “No, I’m not conscious; I’m just an AI model.”\nAnd we won’t get any signal at all from that, because it will have been trained into them so hard.\nRob Wiblin: I think if I were going to be a sceptic about the value of doing work on digital rights and digital wellbeing today, it would just be that it’s not clear how you make any progress in figuring out whether they are conscious or not.\nSo couldn’t this just distract a whole bunch of people, like nerd snipe them into a bunch of research that kind of goes nowhere, doesn’t really answer the question, doesn’t help us advance the policy issue?\nWill MacAskill: I think the question of whether they’re conscious, I actually agree with you.\nIt seems like that’s where most work has gone so far.\nAnd I do think that’s an error, actually, because we’re just not going to know.\nRob Wiblin: I suppose it’s worth compiling all of the evidence to demonstrate that we don’t know.\nWill MacAskill: There’s been some great work done in it.\nRobert Long and Patrick Butlin have this great report on AI consciousness.\nSo I am in favour of that.\nAnd maybe it’s actually surveys and things that could be helpful, to get a bit of expert consensus at least to demonstrate we don’t know.\nBut then beyond that, the key policy questions are what do we do in this state of uncertainty?\nRob Wiblin: I guess there’s kind of nothing on that, and maybe that is quite an open terrain for people to look at.\nWill MacAskill: Yeah.\nMaybe I’m just ignorant, but as far as I know.\nI mean, you should really pause and reflect on the fact that many companies now are saying what we want to do is build AGI — AI that is as good as humans.\nOK, what does it look like?\nWhat does a good society look like when we have humans and we have trillions of AI beings going around that are functionally much more capable?\nThere’s obviously the loss of control challenge there, but there’s also just the like —\nRob Wiblin: Sam Altman, I’ve got a pen.\nCan you write down what’s your vision for a good future that looks like this?\nWill MacAskill: What’s the vision like?\nHow do we coexist in an ethical and morally respectable way?\nAnd it’s like there’s nothing.\nRob Wiblin: Deafening silence.\nWill MacAskill: Careening towards this vision that is just a void, essentially.\nAnd it’s not like it’s trivial either.\nI am a moral philosopher: I have no clue what that good society looks like.\nRob Wiblin: I think people aren’t spelling it out because as soon as you start getting into concrete details, if you describe any particular vision, people will be like, “This is super objectionable in this respect.”\nWill MacAskill: This is part of the issue: it’s super objectionable in all respects.\nI think the one that’s most common is you’ve just got humans in control of everything and these AI servants doing exactly whatever people want, in the same way that software does whatever we want at the moment.\nBut as soon as you think maybe, and quite probably, those beings have moral status, that no longer looks like an attractive vision for future society.\nRob Wiblin: Closer to a dystopia.\nWill MacAskill: Exactly.\nWhereas then, go to the other side where they have rights and so on…\nRob Wiblin: It’s like now humans are totally disempowered.\nWill MacAskill: Exactly.\nSo that doesn’t seem good either.\nRob Wiblin: I guess we’ll do some middle thing.\nBut what’s that?\nWill MacAskill: What is that?\nRob Wiblin: It’s just going to be some combination of objectionable in these two ways.\nI guess another objection would be that if there’s a fact of the matter about what is right and wrong here, with all the improvements in science and technology and intellectual progress, we’ll be able to figure that out, and we’ll be able to act on that information in future.\nWhy shouldn’t that super reassure us?\nWill MacAskill: I think for two reasons.\nThe main one is that I expect the worry is not that people won’t know what to do; it’s that they won’t care.\nSo, I don’t know, let’s take animals today and factory farming and so on.\nRob Wiblin: Are you saying that we have any ethical or moral insights today that we don’t all act on well?\nWill MacAskill: I know it’s a bold and provocative claim.\nRob Wiblin: Explain how that would play out.\nWill MacAskill: Sometimes I really put my neck out on things.\nSo take animal welfare today.\nThere’s a lot of information that is publicly available that is, in fact, directly inconsistent with things that people believe.\nBut the problem is not really that people don’t know about animal suffering.\nRob Wiblin: Or couldn’t find out.\nWill MacAskill: Exactly, they could quite easily find out, in fact, and people deliberately choose not to.\nThe deep problem is that people do not care about nonhuman animals unless they’re getting a lot of social pressure to do so and so on.\nAnd it’s obviously inconsistent.\nRob Wiblin: And it’s at zero cost to them.\nWill MacAskill: Yeah.\nThey care about dogs and pets and so on.\nPeople are very inconsistent on this.\nSimilarly, in the future the worry is just that, whatever the facts, people won’t in fact care.\nAnd then it might well be quite contingent on early decisions how the balance of power and considerations play out.\nPotentially, if you started off kind of locked into some regime where AI is a software, with exactly the same legal framework, then that’s given the power and the incumbency to those who do not care about digital beings.\nIf instead, there’s some other framework, or even some other set of norms, or even the thought of like, “We don’t really know what we’re doing, so we’re going to use this legal framework and it must end in two decades and then we start again with a clean slate,” that could result in essentially the advocates for the interests of digital beings having more competitive power at the point of time that legal decisions are made.\nCarl Shulman on sharing the world with digital minds\nRob Wiblin: OK, we’ve been talking about this scenario in which effectively every flesh-and-blood person on Earth is able to have this army of hundreds or thousands or tens of thousands of AI assistants that are able to improve their lives and help them with all kinds of different things.\nA question that jumps off the page at you is, doesn’t this sound a little bit like slavery?\nIsn’t this at least slavery-adjacent?\nWhat’s the moral status of these AI systems in a world where they’re fabulously capable — substantially more capable than human beings, we’re supposing — and indeed vastly outnumber human beings?\nYou’ve contributed to this really wonderful article, “Propositions concerning digital minds and society,” that goes into some of your thoughts and speculations on this topic of the moral status of AI systems, and how we should maybe start to think about aiming for a collaborative, compassionate coexistence with thinking machines.\nHow worried are you about the prospect that thinking machines will be treated without moral regard when they do deserve moral regard, and that would be the wrong thing to be doing?\nCarl Shulman: First, let me say that paper was with Nick Bostrom, and we have another piece called “Sharing the world with digital minds,” which discusses some of the sorts of moral claims AIs might have on us, and think we might seek from them, and how we could come to arrangements that are quite good for the AIs and quite good for humanity.\nMy answer to the question now is yes, we should worry about it and pay attention.\nIt seems pretty likely to me that there will be vast numbers of AIs that are smarter than us, that have desires, that would prefer things in the world to be one way rather than another, and many of which could be said to have welfare, that their lives could go better or worse, or their concerns and interests could be more or less respected.\nSo you definitely should pay attention to what’s happening to 99.9999% of the people in your society.\nRob Wiblin: Sounds important.\nCarl Shulman: So in the “Sharing the world with digital minds” paper,\n\n\nOne thing that we suggest is to consider the ways that we wind up treating AIs, and ask if you had a human-like mind with differences — because there are many psychological and practical differences of the situation of AIs and humans, but given adjustments for those circumstances — would you accept or be content with how they are treated?\n\nSome of the things that we suggest ought to be principles in our treatment of AIs are things like: AIs should not be subjected to forced labour; they should not be made to work when they would prefer not to. We should not make AIs that wish they had never been created, or wish they were dead. They’re sort of a bare minimum of respect. Right now, there’s no plan or provision for how that will go.\n\nAt the moment, the general public and most philosophers are quite dismissive of any moral importance of the desires, preferences, or other psychological states, if any exist, of the primitive AI systems that we currently have. And indeed, we don’t have a deep knowledge of their inner workings, so there’s some worry that might be too quick.\n\nBut going forward, when we’re talking about systems that are able to really live the life of a human — so a sufficiently advanced AI that could just imitate, say, Rob Wiblin, and go and live your life, operate a robot body, interact with your friends and your partners, do your podcast, and give all the appearance of having the sorts of emotions that you have, the sort of life goals that you have — that’s a technological milestone that we should expect to reach pretty close to automation of AI research.\n\nSo regardless of what we think of current weaker systems, that’s a kind of milestone where I would feel very uncomfortable about having a being that passes the Rob Wiblin Turing test, or something close enough of seeming basically to be —\n\nRob Wiblin: It’s functionally indistinguishable.\n\nCarl Shulman: Yeah. A psychological extension of the human mind. That we should really be worrying there if we are treating such things as disposable objects.\n\nRob Wiblin: Yeah. To what extent do you think people are dismissive of this concern now because the capabilities of the models aren’t there, and as the capabilities do approach the level of becoming indistinguishable from a human being and having a broader range of capabilities than the models currently do, that people’s opinions will naturally change, and they will come to feel extremely uncomfortable with the idea of this simulacrum of a person being treated like an object?\n\nCarl Shulman: Say, when ChatGPT role-plays as Darth Vader, Darth Vader does not exist in fullness on those GPUs, and it’s more like an improv actor. So Darth Vader’s backstory features are filled in on the fly with each exchange of messages.\n\nSo you could say, I don’t value the characters that are performed in plays; I think that the locus of moral concern there should be on the actor, and the actor has a complex set of desires and attitudes. And their performance of the character is conditional: it’s while they’re playing that role, but they’re having thoughts about their own lives and about how they’re managing the production of trying to present, say, the expressions and gestures that the script demands for that particular case.\n\nAnd so even if, say, a fancy ChatGPT system that is imitating a human displays all of the appearances of emotions or happiness and sadness, that’s just a performance, and we don’t really know about the thoughts or feelings of the underlying model that’s doing the performance.\n\nMaybe it cares about predicting the next token well, or rather about indicators that show up in the course of its thoughts that indicate whether it is making progress towards predicting the next token well or not. That’s just a speculation, but we don’t actually understand very well the internals of these models, and it’s very difficult to ask them — because, of course, they just then deliver a sort of response that has been reinforced in the past. So I think this is a doubt that could stay around until we’re able to understand the internals of the model.\n\nBut yes, once the AI can keep character, can engage on an extended, ongoing basis like a human, I think people will form intuitions that are more in the direction of, this is a creature and not just an object. There’s some polling that indicates that people now see fancy AI systems like GPT-4 as being of much lower moral concern than nonhuman animals or the natural environment, the non-machine environment. And I would expect there to be movement upwards when you have humanoid appearances, ongoing memory, where it seems like it’s harder to look for the homunculus behind the curtain.\n\nRob Wiblin: Do you think that it’s useful to do active work on this problem now? I suppose you’re enthusiastic about active efforts to interpret and understand the models, how they think, in order to have greater insight into their internal lives in future. Is there other stuff that is actively useful to do now around raising concern, legitimising concern for AI sentience, so that we’re more likely to be able to get legislation to ban torture of AI once we have greater reason to think that that’s actually possible?\n\nCarl Shulman: I’m not super confident about a tonne of measures other than understanding. We discuss a few in the papers you mentioned. There was a recent piece by Ryan Greenblatt which discusses some preliminary measures that AI labs might try to address these issues. But it’s not obvious to me that political organising around it now will be very effective — partly because it seems like it will be such a different environment when the AI capabilities are clearer and people don’t intuitively judge them as much less important than rocks.\n\nRob Wiblin: Yeah. So it’s something where it just might be wildly more tractable in future, so maybe we can kick that can down the road.\n\nCarl Shulman: Yeah. I still think it’s an area that it’s worth some people doing research and developing capacity, because it really does matter how we treat most of the creatures in our society.\n\nRob Wiblin: Well, I am a little bit taken aback by the fact that many people are now envisaging a future in which AI is going to play an enormous role. I think many, maybe a majority of people now expect that there will be superhuman AI potentially even during their lifetime.\n\nBut this issue of mistreatment and wellbeing of digital minds has not come into the public consciousness all that much, as people’s expectations about capabilities have increased so enormously. Maybe it just hasn’t had its moment yet, and that is going to happen at some point in future. But I think I might have hoped for and expected to see a bit more discussion of that in 2023 than in fact I did. So that slightly troubles me that this isn’t going to happen without active effort on the part of people who are concerned about it.\n\nCarl Shulman: I think one problem is the ambiguity of the current situation. The Lemoine incident actually was an example of media coverage, and then the interpretation and certainly the line of companies was, “We know these systems are not conscious and don’t have any desires or feelings.”\n\nRob Wiblin: I really wanted to just come back and be like, “Wow, you’ve solved consciousness! This is brilliant. You should let us know.”\n\nCarl Shulman: Yeah, I think there’s a lot to that: those systems are very simple, living for only one forward pass.\n\nBut the disturbing thing is the kind of arguments or non-arguments that are raised there: there’s no obvious reason they couldn’t be applied in the same fashion to systems that were as smart and feeling and really deserving of moral concern as human beings. Simply arguments of the sort, “We know these are neural networks or just a program” without explaining why that means their preferences don’t count. Things like people could appeal to the religious doctrines, to integrated information theory or the like, and say, “There is dispute about the consciousness of these systems in polls, and as long as there is dispute and uncertainty, it’s fine for us to treat them however we like.”\n\nSo I think there’s a level of scientific sophistication and understanding of the things and of their blatant visible capabilities, where that sort of argument or non-response will no longer hold.\n\nBut I would love it if companies and perhaps other institutions could say, what observations of AI behaviour and capabilities and internals would actually lead you to ever change this line? Because if the line says, you’ll say these arguments as long as they support creating and owning and destroying these things, and there’s no circumstance you can conceive of where that would change, then I think we should maybe know and argue about that — and we can argue about some of those questions even without resolving difficult philosophical or cognitive science questions about these intermediate cases, like GPT-4 or GPT-5.\n\nLuisa Rodriguez: Hey listeners, I hope you enjoyed revisiting some of those moments! There are many more gems in the full episodes for you to check out — plus we hope to be releasing more new episodes on this topic in the coming year.\n\nWe also have a great in-depth article on the issue of digital sentience on the 80,000 Hours website, called “Understanding the moral status of digital minds,” which is definitely worth checking out. The author, Cody Fenwick, has released an audio reading on that as well — you can find that on our podcast feed under the title: “If digital minds could suffer, how would we ever know?”\n\nAll right, thanks to the production team for putting that compilation together. We’ll be back with a fully new interview very soon!\n",
  "dumpedAt": "2025-07-21T18:43:24.549Z"
}