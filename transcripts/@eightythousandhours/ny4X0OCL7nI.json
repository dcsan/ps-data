{
  "episodeId": "ny4X0OCL7nI",
  "channelSlug": "@eightythousandhours",
  "title": "Graphs AI Companies Want You To Misunderstand | Toby Ord, Oxford University",
  "publishedAt": "2025-06-24T14:25:08.000Z",
  "rawLines": [
    {
      "lang": "en",
      "text": "Rob Wiblin: Today I’m again speaking with Toby \nOrd. Toby is a senior researcher at Oxford  ",
      "offset": 81.028,
      "duration": 4.012
    },
    {
      "lang": "en",
      "text": "University, and his work focuses on the biggest \npicture questions facing humanity. He’s probably  ",
      "offset": 85.04,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "most well known to listeners as the author of \nThe Precipice: Existential Risk and the Future  ",
      "offset": 89.68,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "of Humanity, which made quite a big splash \nback in 2020. Welcome back on the show, Toby.",
      "offset": 93.52,
      "duration": 3.75
    },
    {
      "lang": "en",
      "text": "Toby Ord: It’s great to be here.",
      "offset": 97.27,
      "duration": 0.81
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: So today I want to take a bunch of \ntechnical developments that have been going on  ",
      "offset": 98.08,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "in AI over the last couple of years, and try to \nexplain them in a way that almost everyone can  ",
      "offset": 102.16,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "understand — and then also explain what \nimplications they have for our lives,  ",
      "offset": 106.32,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "for what sort of things we should \nexpect from AI in coming years,  ",
      "offset": 110.96,
      "duration": 2.4
    },
    {
      "lang": "en",
      "text": "and what implications they have for AI \ngovernance and policy in particular.",
      "offset": 113.36,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "But first, I wanted to talk a bit about this blog \npost that you wrote or this presentation you gave  ",
      "offset": 116.96,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "last year called “The Precipice Revisited.” \nThe Precipice was this book that you wrote in  ",
      "offset": 120.8,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "2018/2019, and came out in 2020. It explored the \nscience behind all of the different major threats  ",
      "offset": 125.28,
      "duration": 6.4
    },
    {
      "lang": "en",
      "text": "to humanity’s future: pandemics, asteroids, \nAI of course, nuclear war, that sort of stuff.",
      "offset": 131.68,
      "duration": 6.56
    },
    {
      "lang": "en",
      "text": "Of course there’s been lots of developments \nsince then. I think last year you wanted to  ",
      "offset": 138.24,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "look back and say, over the five years \nsince you wrote it, what have been the  ",
      "offset": 141.84,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "major changes in the picture? Is humanity in a \nbetter situation? Is it in a worse situation?",
      "offset": 144.64,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "What have been the major changes? And in \nparticular on AI, where so much has been going on?",
      "offset": 149.04,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "Toby Ord: Obviously lots of changes \nin pandemics. We had COVID hit us,  ",
      "offset": 153.2,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "mRNA vaccines, so on and so forth. \nAnd nuclear war: the prospects of that  ",
      "offset": 158.4,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "felt like a distant memory back in 2019, and \nnow it’s become more of a realistic possibility.",
      "offset": 164.08,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "But AI is where the most changes have \nhappened. So if we cast our minds back  ",
      "offset": 169.68,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "to 2019, the state-of-the-art AGI-type systems \nwere reinforcement learning systems created by  ",
      "offset": 176.16,
      "duration": 10.32
    },
    {
      "lang": "en",
      "text": "companies like DeepMind and OpenAI. So think \nof things like AlphaGo: this is a system that’s  ",
      "offset": 186.48,
      "duration": 6.88
    },
    {
      "lang": "en",
      "text": "learned to play Go. Particularly AlphaGo Zero \nlearned to play Go by playing huge numbers of  ",
      "offset": 193.36,
      "duration": 6.88
    },
    {
      "lang": "en",
      "text": "games against itself — and in doing so, it \nkind of blasted through the human level of  ",
      "offset": 200.24,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "game-playing performance and then reached these \nkind of lofty heights of superhuman abilities.",
      "offset": 206.88,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "And similarly with StarCraft and Dota, some \nother games where you could do a similar thing:  ",
      "offset": 213.04,
      "duration": 6.32
    },
    {
      "lang": "en",
      "text": "reinforcement learning lets you \nlearn skills that are beyond what  ",
      "offset": 219.92,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "humans already have because you’ve got some \nway of rating really good play objectively.",
      "offset": 225.28,
      "duration": 7.28
    },
    {
      "lang": "en",
      "text": "So we had those types of models, \nand they were actually quite narrow.  ",
      "offset": 232.56,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "DeepMind was very excited with AlphaZero because \nit could play three different games: it could play  ",
      "offset": 238.88,
      "duration": 6.96
    },
    {
      "lang": "en",
      "text": "chess and shogi and Go, but it couldn’t play \ngames involving chance or games involving  ",
      "offset": 246.64,
      "duration": 8.32
    },
    {
      "lang": "en",
      "text": "imperfect information — so card games, for \nexample, or things that aren’t games and so on.  ",
      "offset": 254.96,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "And it was in some sense general, but much less \nthan we’ve seen since then with the rise of LLMs.",
      "offset": 260.96,
      "duration": 8.32
    },
    {
      "lang": "en",
      "text": "The rise of large language models has been this \nspectacular improvement in generality — where,  ",
      "offset": 270.56,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "using text as an interface to talk to these \nthings, you can talk about any topic that  ",
      "offset": 276.72,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "you would talk about with humans. And that’s \nsomething, as Alan Turing foreshadowed with  ",
      "offset": 282,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "the idea of the Turing test, where you can \nthen quiz it in order to test it: you can  ",
      "offset": 287.6,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "ask it about all kinds of obscure topics or the \ntopics you know it will do worst at and so on.",
      "offset": 292.88,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "And these systems actually have surprisingly good \nperformance across the board, in a way that’s hard  ",
      "offset": 297.6,
      "duration": 6.72
    },
    {
      "lang": "en",
      "text": "to quantify, but I would say it’s thousands of \ntimes more general than something like AlphaZero.",
      "offset": 304.32,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: So we’ve had the switch to LLMs. \nHave there been any other major developments?",
      "offset": 309.84,
      "duration": 5.91
    },
    {
      "lang": "en",
      "text": "Toby Ord: Yeah. As part of the switch to LLMs, \nwe’ve had this situation where, previously  ",
      "offset": 315.75,
      "duration": 8.73
    },
    {
      "lang": "en",
      "text": "with reinforcement learning and these Go-playing \nthings, there was this question of how would you  ",
      "offset": 325.2,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "ever get human values into such a system? \nIt couldn’t even understand human values.",
      "offset": 330.24,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: It had none of these concepts.",
      "offset": 335.28,
      "duration": 2.55
    },
    {
      "lang": "en",
      "text": "Toby Ord: No. Even if it was playing a game with  ",
      "offset": 337.83,
      "duration": 2.73
    },
    {
      "lang": "en",
      "text": "little sprites made up of pixels that get \nhit by sprites that represent bullets,  ",
      "offset": 340.56,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "it’s not clear that it’s doing something \nlike killing as we understand it,  ",
      "offset": 345.92,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "as opposed to just a move in a game, like \ntaking someone’s knight in a game of chess.",
      "offset": 349.44,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "And there was this big question about how \nwould you even get the complexity of our  ",
      "offset": 354,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "values into these systems? But with the \nidea of training on vast amounts of text,  ",
      "offset": 358.72,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "we’ve got this world where now these \nsystems, if you quiz them about,  ",
      "offset": 364.56,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "“Would anyone be slighted or upset if you were to \ndo this action?” — or even asking questions about  ",
      "offset": 370,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "advanced things in moral philosophy — it can often \nget the right answers to these types of questions,  ",
      "offset": 375.76,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "or the answers that we think show knowledge \nof what humans think is morally appropriate.",
      "offset": 380.64,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "But there still is a big question there: \nDoes that knowledge of morality actually  ",
      "offset": 386.56,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "guide it? Has it internalised that? \nBut it’s such a difference, though,  ",
      "offset": 390.96,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "that it now at least has most of that knowledge.",
      "offset": 394.4,
      "duration": 4.148
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: Yeah, I think on a lot of moral \nphilosophy questions and even social norm  ",
      "offset": 398.548,
      "duration": 2.732
    },
    {
      "lang": "en",
      "text": "questions, it often probably outperforms humans \nin many cases, at least in typical examples.",
      "offset": 401.28,
      "duration": 4.39
    },
    {
      "lang": "en",
      "text": "Toby Ord: Yeah, but there \nstill is that question of —",
      "offset": 405.67,
      "duration": 3.278
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: It may know, but doesn’t care.",
      "offset": 408.948,
      "duration": 1.602
    },
    {
      "lang": "en",
      "text": "Toby Ord: Exactly. And as well as \nthese changes to the technology,  ",
      "offset": 410.55,
      "duration": 6.57
    },
    {
      "lang": "en",
      "text": "there’s also been a lot of \nother changes to the landscape.",
      "offset": 417.12,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "So in order to power this, it’s been \nextraordinarily expensive. It’s required  ",
      "offset": 421.68,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "this scaling up of compute infrastructure, \nand this required huge amounts of money.  ",
      "offset": 426.16,
      "duration": 6.8
    },
    {
      "lang": "en",
      "text": "So it required very large investments from \nMicrosoft and Google and Amazon and others.  ",
      "offset": 432.96,
      "duration": 9.36
    },
    {
      "lang": "en",
      "text": "In 2019, there was already a \nrace between these AI labs,  ",
      "offset": 444.24,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "these relatively small groups of people focused \non this technology. But now these trillion-dollar  ",
      "offset": 449.36,
      "duration": 6.4
    },
    {
      "lang": "en",
      "text": "companies have been brought into that race, and \nit started to contribute to their bottom line.",
      "offset": 455.76,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "When Microsoft applied this in search through Bing \n— going after Google’s crown jewel of search — all  ",
      "offset": 461.44,
      "duration": 7.68
    },
    {
      "lang": "en",
      "text": "of a sudden the primary way to make money for \na trillion-dollar company was on the line. It  ",
      "offset": 469.12,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "brought in these very large financial interests \ninto this race, so it really heated up the race.",
      "offset": 475.28,
      "duration": 7.04
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: Back in 2019, I think in The \nPrecipice, you estimated that the chance  ",
      "offset": 482.32,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "of humanity losing most of its potential \nfuture value due to AI in this century  ",
      "offset": 486,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "was around 1 in 10. Would you say that \nnumber has drifted up or drifted down?",
      "offset": 490.8,
      "duration": 4.95
    },
    {
      "lang": "en",
      "text": "Toby Ord: I’m not sure. For a lot of the other \nrisks, it was easier to see whether it’s gone  ",
      "offset": 495.75,
      "duration": 4.65
    },
    {
      "lang": "en",
      "text": "up or down. For this one, I think that we’ve \nreally been gifted a relatively good situation  ",
      "offset": 500.4,
      "duration": 10.72
    },
    {
      "lang": "en",
      "text": "in terms of the way the technology has panned out, \nin that it’s this technology that imitated human  ",
      "offset": 511.12,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "values and human reasoning and so on by training \non this huge corpus of human data. I think that  ",
      "offset": 518.96,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "has just been tremendously helpful. And the fact \nthat it’s not an agent by default. Real gifts.",
      "offset": 524.64,
      "duration": 6.72
    },
    {
      "lang": "en",
      "text": "It wasn’t that we steered towards that because \nwe knew that would help with safety. It’s just  ",
      "offset": 531.92,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "that that turned out to be the easiest way. And \nI don’t think we quite recognise that enough:  ",
      "offset": 536.4,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "that the biggest effects on whether we’re \nsafe or not have just come from somewhat  ",
      "offset": 542.88,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "random aspects about this technology landscape, \nrather than deliberate attempts to steer it.",
      "offset": 547.44,
      "duration": 6.4
    },
    {
      "lang": "en",
      "text": "But I am very concerned about \nthe racing, and I’m concerned  ",
      "offset": 554.64,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "that we’ve seen evidence that the players \nwho are trying to make these systems are  ",
      "offset": 560.08,
      "duration": 6.64
    },
    {
      "lang": "en",
      "text": "ultimately going to cut corners \nin order to win these races.",
      "offset": 568.72,
      "duration": 3.588
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: OK, let’s talk about this series of \narticles you wrote about how there’s been a change  ",
      "offset": 572.308,
      "duration": 4.652
    },
    {
      "lang": "en",
      "text": "in what is getting scaled up as the companies \nare trying to make these models more powerful.  ",
      "offset": 576.96,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "The change that you describe in these \narticles, which people can find on your  ",
      "offset": 582,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "website, tobyord.com. The article is called \n“Inference scaling reshapes AI governance.”",
      "offset": 585.44,
      "duration": 6.8
    },
    {
      "lang": "en",
      "text": "Basically, the companies are now using more \nof their compute during the inference stage  ",
      "offset": 592.24,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "rather than during the training stage. I \nthink most listeners will probably know what  ",
      "offset": 596.8,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "compute and pre-training and inference \nare, but maybe you can explain that,  ",
      "offset": 601.2,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "so that everyone is completely following? And \nthen explain the difference: what’s changed?",
      "offset": 605.2,
      "duration": 3.03
    },
    {
      "lang": "en",
      "text": "Toby Ord: Yeah, exactly. So compute is a somewhat  ",
      "offset": 608.23,
      "duration": 4.01
    },
    {
      "lang": "en",
      "text": "ugly word for computation. It just \nmeans how much computer processing  ",
      "offset": 612.24,
      "duration": 7.12
    },
    {
      "lang": "en",
      "text": "has happened. So it doesn’t include things like \nRAM or memory, but just how many steps, basically.",
      "offset": 619.36,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "And what people have found is that you can scale \nup the amount of compute that goes on, where  ",
      "offset": 625.52,
      "duration": 8.48
    },
    {
      "lang": "en",
      "text": "you’re processing more data and so on, building a \nbigger model, and you get much better performance.",
      "offset": 634,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "That process of building a model and training \na model gets broken up into two stages,  ",
      "offset": 639.2,
      "duration": 7.52
    },
    {
      "lang": "en",
      "text": "which we call pre-training and post-training. It \nmakes it sound like they come before training or  ",
      "offset": 646.72,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "after training, but really it’s just \nthe first part or the second part.",
      "offset": 650.16,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "Pre-training is the one that I think a \nlot of people are familiar with: where  ",
      "offset": 654.4,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "you take a system and you take some text, and it \nhears the first four words and then it tries to  ",
      "offset": 658.56,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "guess what the fifth word will be, for example. \nThen you modify the weights on it in order to  ",
      "offset": 664.4,
      "duration": 8.32
    },
    {
      "lang": "en",
      "text": "make it so it would have been slightly more likely \nto say the correct word next. So that’s this kind  ",
      "offset": 672.72,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "of glorified auto-prediction type thing. And \nthat produces something called a “base model.”",
      "offset": 677.6,
      "duration": 8.16
    },
    {
      "lang": "en",
      "text": "And then there’s a whole lot of \npost-training applied to it — for example,  ",
      "offset": 685.76,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "to make it refuse to do harmful \nthings, and to make sure it’s honest,  ",
      "offset": 689.92,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "and to make sure it can follow instructions \nand things. That’s all called post-training.",
      "offset": 694,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "But what we’ve had so far is a \nhuge scaling up of pre-training  ",
      "offset": 698.4,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "via something called scaling laws, and then \nan increasing amount of post-training to  ",
      "offset": 704.8,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "make it quite a lot better after \nyou’ve built this huge thing.",
      "offset": 709.84,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "But now, ultimately, there’s been a shift from \nscaling up more and more of this pre-training to  ",
      "offset": 714.4,
      "duration": 6.56
    },
    {
      "lang": "en",
      "text": "scaling up something that happens after the \nwhole training process, called “inference.”  ",
      "offset": 720.96,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "Inference is basically using that model, \nso using it to produce a whole lot of text.",
      "offset": 724.72,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: I guess inference is like the \nthinking that it does while it’s trying to  ",
      "offset": 730.4,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "answer a particular question and get back \nto you, or figure out what to do next.",
      "offset": 733.12,
      "duration": 3.11
    },
    {
      "lang": "en",
      "text": "Toby Ord: Yeah, that’s right. So \nhere’s how I think about this. I  ",
      "offset": 736.23,
      "duration": 3.21
    },
    {
      "lang": "en",
      "text": "think it’s a useful analogy. \nSuppose you’ve got a company,  ",
      "offset": 739.44,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "and you’re trying to get some excellent \nwork done, and you could employ someone.",
      "offset": 743.12,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "Pre-training is like sending them \nthrough high school and then to  ",
      "offset": 747.76,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "undergraduate and then maybe to grad \nschool. You’re putting in more and more  ",
      "offset": 753.12,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "expense into having this person learn more and \nmore about different things, so they’ll have a  ",
      "offset": 758.96,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "lot of extra knowledge at their fingertips. And \nthat’s what most of the scaling had been doing.",
      "offset": 764.16,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "But then inference is like letting that person \nspend more time actually doing the job. So suppose  ",
      "offset": 769.92,
      "duration": 8.56
    },
    {
      "lang": "en",
      "text": "that you give them some brief, that they’ve got \nto prepare a report for a client. By default,  ",
      "offset": 778.48,
      "duration": 6.4
    },
    {
      "lang": "en",
      "text": "if you just ask one of these \nlanguage models to do that,  ",
      "offset": 784.88,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "it just extemporises stuff. So it’s just saying \nthe words as they pop into its head. And it  ",
      "offset": 788.16,
      "duration": 6.8
    },
    {
      "lang": "en",
      "text": "doesn’t have a chance to do a second draft; it \njust has to compose this document in one go.",
      "offset": 794.96,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "So you could think of that as that the \npre-training has given it this really  ",
      "offset": 800.96,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "powerful kind of “System 1” ability, \nin these terms from human psychology:  ",
      "offset": 805.92,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "so the intuitive ability to just answer things \nstraight off the bat. Then you’re just asking it  ",
      "offset": 810.72,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "to keep doing that as it goes through all the \nsentences of this report that it’s composing.",
      "offset": 815.68,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "Whereas what you could also do is let \nit spend a long time in that process,  ",
      "offset": 820.64,
      "duration": 6.4
    },
    {
      "lang": "en",
      "text": "maybe spend 10 times as much on writing \n— where it could write an answer and  ",
      "offset": 827.04,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "then it could critique the answer, it could \nmodify things, move things around — and then  ",
      "offset": 831.52,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "ultimately hide all of that working \nand just show you the final answer.",
      "offset": 835.84,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "So that’s like saying you don’t just have to \nwrite this report for the client in 10 minutes,  ",
      "offset": 839.52,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "but rather we’re going to scale that up to \n100 minutes or 1,000 minutes. It turns out,  ",
      "offset": 844.56,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "as you get with an employee, you \nget much better work out of someone  ",
      "offset": 849.92,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "if they’re doing that. And that also gives room \nfor this different kind of intelligence that  ",
      "offset": 853.52,
      "duration": 7.2
    },
    {
      "lang": "en",
      "text": "we call “System 2” in human psychology. \nThese are often called reasoning models,  ",
      "offset": 860.72,
      "duration": 7.12
    },
    {
      "lang": "en",
      "text": "where it’s able to do a certain kind \nof structured thinking and apply that.",
      "offset": 867.84,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "So pre-training scales up the System 1, \nand then this inference scaling lets us  ",
      "offset": 872.88,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "spend more time on a task to hide \nall of that work and just show the  ",
      "offset": 877.44,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "final thing — and we could think of that as \nkind of scaling up its System 2 abilities.",
      "offset": 880.48,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: As I understand it, we’ve had this \nshift from using compute during training to during  ",
      "offset": 884.8,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "inference or question answering, basically I \nthink because in 2023 and 2024 the main companies  ",
      "offset": 889.84,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "started training bigger models than GPT-4 and \nthey found that they were running out of juice,  ",
      "offset": 895.12,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "that actually this was much more expensive: \nthey were using a lot more chips and a lot  ",
      "offset": 899.68,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "more electricity, and the performance just wasn’t \nincreasing at nearly the rate that it had been  ",
      "offset": 903.84,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "before. Perhaps because they were kind of running \nout of high quality data to actually train on;  ",
      "offset": 907.6,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "they’d soaked up all of the good \nbooks and Wikipedia and all of that.",
      "offset": 912,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "But at the same time, they were finding \nnew ways of putting scaffolding around  ",
      "offset": 917.68,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "these models that would allow them \nto answer a question, critique it,  ",
      "offset": 921.12,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "think about it, to basically get more juice \nout of giving them additional time to try to  ",
      "offset": 924.32,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "answer a question to a higher standard in a more \nintelligent way. Is that basically the story?",
      "offset": 927.92,
      "duration": 4.79
    },
    {
      "lang": "en",
      "text": "Toby Ord: Yeah. At least I think that that \nis correct. It’s somewhat disputed. So late  ",
      "offset": 932.71,
      "duration": 5.13
    },
    {
      "lang": "en",
      "text": "last year there were a series of articles that \ncame out in different publications in the media  ",
      "offset": 937.84,
      "duration": 8.16
    },
    {
      "lang": "en",
      "text": "reporting that behind the scenes \nOpenAI had been disappointed by  ",
      "offset": 946.72,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "their next bigger model that used 10 times \nas much compute as GPT-4 — this is now what  ",
      "offset": 951.52,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "we call GPT-4.5. And they’d been really \ndisappointed with the results. If you put  ",
      "offset": 957.36,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "in 10 times as many inputs into something, \nyou hope to get some noticeable improvement.  ",
      "offset": 963.12,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "And they found that it wasn’t actually that \nclear, and it was worse at quite a few things.",
      "offset": 967.36,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "Then there were similar reports coming \nfrom the other leading AI companies,  ",
      "offset": 971.92,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "so this was a little bit concerning for this \nnarrative of continuing to scale these things up.",
      "offset": 977.84,
      "duration": 7.44
    },
    {
      "lang": "en",
      "text": "So that really is how everyone had been thinking \nabout it. For example, the paper “Situational  ",
      "offset": 985.28,
      "duration": 6.64
    },
    {
      "lang": "en",
      "text": "awareness” by Leopold Aschenbrenner paints \na picture based on scaling up pre-training  ",
      "offset": 991.92,
      "duration": 6.4
    },
    {
      "lang": "en",
      "text": "I think a million times further than where we’re \ncurrently at — just continuing to do that, and  ",
      "offset": 999.12,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "then painting a picture of what would happen if \nthat curve continues to go. Whereas what seems to  ",
      "offset": 1005.28,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "have happened is that it’s already kinked right at \nthe point where GPT-4 was out and before GPT-4.5.  ",
      "offset": 1011.04,
      "duration": 6.88
    },
    {
      "lang": "en",
      "text": "So at the time he published the essay, it seems \nlike maybe that’s actually not what’s happening.",
      "offset": 1017.92,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "That said, it’s very difficult to know what \nthe curve of actual performance looks like.  ",
      "offset": 1023.52,
      "duration": 8.8
    },
    {
      "lang": "en",
      "text": "Some people say GPT-4.5 is really impressive. \nI find that a little bit hard to believe. If  ",
      "offset": 1032.96,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "you look at the actions of the company… For \nexample, GPT-4 was this massive announcement,  ",
      "offset": 1038.8,
      "duration": 7.36
    },
    {
      "lang": "en",
      "text": "right? This breakthrough technology, and \neveryone was oohing and ahhing about it.  ",
      "offset": 1046.16,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "Entirely new benchmarks had to be created \nin order to measure its new capabilities.  ",
      "offset": 1052,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "There was this “Sparks of AGI” paper and \nso on. A massive improvement from GPT-3.5.",
      "offset": 1055.52,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "Whereas GPT-4.5 was just announced I think on \na Friday, kind of like we’re trying to bury  ",
      "offset": 1061.68,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "the story. And then they announced I think a \nmonth after it launched that they were going to  ",
      "offset": 1067.2,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "end-of-life it this summer. They \nalso declared that it wasn’t even  ",
      "offset": 1071.76,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "a frontier model — so they said \nthis is not even an example of  ",
      "offset": 1076.48,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "anything that should concern people \nor is pushing the industry forward.",
      "offset": 1080.24,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "That’s really remarkable. I would have been \ntotally shocked if you told me when GPT-4 came out  ",
      "offset": 1083.92,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "that its successor would be basically buried by \nthe company that was creating it. So I think there  ",
      "offset": 1088.24,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "really has been a kink in the curve here — but it \nis difficult to measure, and not everyone agrees.",
      "offset": 1093.76,
      "duration": 6.228
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: Yeah, I think that is very \nlikely a win for the people who said  ",
      "offset": 1099.988,
      "duration": 3.052
    },
    {
      "lang": "en",
      "text": "pre-training is kind of hitting a wall. \nAt the same time, in the broader picture,  ",
      "offset": 1103.04,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "the people who were pessimistic about \nAI progress have probably on balance  ",
      "offset": 1107.04,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "been wrong — because they’ve found other \nthings that can be scaled that nonetheless  ",
      "offset": 1110.56,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "produce the output that we care about: \nmore capable, more impressive models.",
      "offset": 1113.44,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "Can you explain what’s been going on with the \nscaling of inference and what impacts it’s had?",
      "offset": 1117.84,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "Toby Ord: Yeah. So the AI companies have \noften said what’s really important isn’t  ",
      "offset": 1121.6,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "that this pre-training scaling continues, \nbut that some kind of scaling continues:  ",
      "offset": 1128.08,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "that there’s still some way that we could pour \nin more and more compute into this process,  ",
      "offset": 1131.44,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "and we’ll get more and more kind of cognitive \ncapabilities coming out the other end.",
      "offset": 1135.44,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "I think that makes sense, but it’s not \nat all clear that this will continue  ",
      "offset": 1140.72,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "to provide the same types of benefits we’ve \nseen. It’s really quite a different process  ",
      "offset": 1146.48,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "and it’s as different in some cases \nas the difference between a stock  ",
      "offset": 1151.6,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "and a flow. So I think there’s a lot \nof conceptual confusion about this.",
      "offset": 1154.96,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "The key idea here is: suppose you scale up the \npre-training by a factor of 10. So you put 10  ",
      "offset": 1161.68,
      "duration": 11.36
    },
    {
      "lang": "en",
      "text": "times as much compute into learning how to have \nthese good kinds of intuitions and good responses  ",
      "offset": 1173.04,
      "duration": 6.32
    },
    {
      "lang": "en",
      "text": "to prompts. Then you do have to pay some \nadditional cost every time you use the model,  ",
      "offset": 1180.08,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "because you generally need to add \nmore weights as well when you do that.",
      "offset": 1185.36,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "But if you instead try to get that same level \nof capability improvement by training up  ",
      "offset": 1189.68,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "inference — so training up the amount of time \nit spends on every task — then you have to pay  ",
      "offset": 1196.16,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "that full scaleup every time you use it. And the \nway that the maths turns out — it’s a little bit  ",
      "offset": 1202.32,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "complicated under the hood — but it turns out that \nfor every tenfold increase in pre-training that  ",
      "offset": 1208.4,
      "duration": 6.8
    },
    {
      "lang": "en",
      "text": "you instead get the benefits by using inference \nscaling, you do have to pay 10 times as much every  ",
      "offset": 1215.2,
      "duration": 6.96
    },
    {
      "lang": "en",
      "text": "time you use it. And that can change everything \nin terms of the economics of these companies.",
      "offset": 1222.16,
      "duration": 6.56
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: So I think one of the positive \nimplications that this might have going forward is  ",
      "offset": 1228.72,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "that we might expect AI that is both human level \nand as general as humans to arrive more gradually.  ",
      "offset": 1233.92,
      "duration": 9.92
    },
    {
      "lang": "en",
      "text": "And the same could be true for superhuman AI: \nthat we could get glimpses of what superhuman  ",
      "offset": 1243.84,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "AI and what equally general superhuman AI might \nlook like maybe years before it’s ever actually  ",
      "offset": 1248.32,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "practical to use on a broad scale. Can you explain \nwhy that’s the case and what effects that has?",
      "offset": 1254.32,
      "duration": 4.47
    },
    {
      "lang": "en",
      "text": "Toby Ord: Yeah. So each of these GPT levels, \nwe could think of the move from GPT-2 to  ",
      "offset": 1258.79,
      "duration": 7.45
    },
    {
      "lang": "en",
      "text": "GPT-3 and GPT-3 to GPT-4, were something \nlike 100 times increase in the amount of  ",
      "offset": 1266.24,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "compute that was used for them. And in \norder for things to get really crazy,  ",
      "offset": 1272.72,
      "duration": 6.72
    },
    {
      "lang": "en",
      "text": "to have a model that has transformative effects \non the world, suppose we need to get to GPT-6  ",
      "offset": 1279.44,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "level in those terms — so to go up by a factor of \n100 from GPT-4 and then another 100 beyond that.",
      "offset": 1285.04,
      "duration": 7.52
    },
    {
      "lang": "en",
      "text": "If we are trying to get those advantages \nthrough inference scaling, then that  ",
      "offset": 1292.56,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "means we have to spend 10,000 times as much \nmoney every time we want it to do something,  ",
      "offset": 1296.88,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "compared to if we’d done it the other \nway around. That’s a big difference.",
      "offset": 1301.92,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "A kind of key parameter for how will the \narrival of greater-than-human level of  ",
      "offset": 1308,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "intelligence shape society is what’s the \ncost of it? Will it cost more or less than  ",
      "offset": 1314.24,
      "duration": 6.4
    },
    {
      "lang": "en",
      "text": "human wages for doing the same thing? And \nobviously it could depend on exactly what  ",
      "offset": 1320.64,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "it’s doing. And human level will be different; \nit will arrive at different times for different  ",
      "offset": 1324.88,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "types of tasks. But roughly speaking, \nyou can think of this kind of parameter.",
      "offset": 1329.44,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "So suppose a system comes out and it costs \n10 cents an hour to do human-level work.  ",
      "offset": 1334,
      "duration": 7.04
    },
    {
      "lang": "en",
      "text": "That’s going to have massive implications. \nIt’s basically free, and companies would  ",
      "offset": 1341.04,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "be attempting to shift all kinds of jobs onto \nthis thing. But if instead when it comes out,  ",
      "offset": 1345.36,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "it’s $10,000 an hour, then that’s something that \nmight not affect things very much at all at first.",
      "offset": 1350.08,
      "duration": 8.88
    },
    {
      "lang": "en",
      "text": "So if we scale up the systems, we try to get the \ncapabilities we thought you’d need a GPT-6 to  ",
      "offset": 1359.76,
      "duration": 7.04
    },
    {
      "lang": "en",
      "text": "reach — but ultimately, if that pre-training \nis kind of fizzled out and we’re getting it  ",
      "offset": 1366.8,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "all from inference scaling, so we need to put in \n10,000 times as much compute every time we run it,  ",
      "offset": 1371.04,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "it’s going to cost 10,000 times as much money. So \nit could be that we’re on track to get something  ",
      "offset": 1377.04,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "that costs a dollar an hour, but instead \nit costs $10,000 an hour, and it’s totally  ",
      "offset": 1382.48,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "different effects on the world. That’s a \nkey example of how this could change things.",
      "offset": 1387.44,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: In a sense, it’s a \nmuch more reassuring picture.  ",
      "offset": 1393.36,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "If you were worried about rogue AI, \nthen this is a much nicer picture,  ",
      "offset": 1396.48,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "because at the point that you have a superhuman AI \nthat conceivably is motivated to try to take over,  ",
      "offset": 1399.44,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "there wouldn’t be enough compute in the world \nto run enough instances of it to actually do  ",
      "offset": 1404.88,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "the necessary work to try to stage a rebellion. \nSo that would, at least at the early stages,  ",
      "offset": 1409.2,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "potentially be off the cards. And I suppose \nthe model itself would probably realise this.",
      "offset": 1413.28,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "But in the meantime, if you were willing to spend \nthe money, then you could actually study these  ",
      "offset": 1418.88,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "models that you predict will one day be cheap, \nmaybe in three or four years’ time. You could  ",
      "offset": 1422.88,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "study them in great detail and understand what \nmotivates them, try to figure out how you motivate  ",
      "offset": 1426.64,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "them to actually pursue the goals that you have. \nSo it’s a lot better from that point of view.",
      "offset": 1431.2,
      "duration": 5.24
    },
    {
      "lang": "en",
      "text": "I guess even from a governance point of view, you \ncould learn more about what will these models look  ",
      "offset": 1436.44,
      "duration": 4.12
    },
    {
      "lang": "en",
      "text": "like in four years’ time, when they actually are \neconomically relevant, and then think about what  ",
      "offset": 1440.56,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "governance solutions might be appropriate \nif that is how things are going to look.",
      "offset": 1444.24,
      "duration": 4.95
    },
    {
      "lang": "en",
      "text": "Toby Ord: Exactly. So I’m not saying that that \nwould stay expensive forever — there’s been  ",
      "offset": 1449.19,
      "duration": 5.93
    },
    {
      "lang": "en",
      "text": "a long history of things getting cheaper and \ncheaper — but rather that rate at which things  ",
      "offset": 1455.12,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "get cheaper would be what would introduce it into \nsociety, rather than the moment when the training  ",
      "offset": 1461.36,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "run is finished and then the company switches on \ntheir public release being kind of a cliff edge.  ",
      "offset": 1467.04,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "Instead it would be the case that every few months \nor something, it costs half as much and eventually  ",
      "offset": 1472,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "$10,000 an hour, then $5,000 an hour, $2,000 \nan hour, $1,000 — and new groups would start  ",
      "offset": 1478.08,
      "duration": 6.64
    },
    {
      "lang": "en",
      "text": "to want to use it at each of these price points \nand so on. It’d be more of a smooth transition.",
      "offset": 1484.72,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "And if you ask, when would you want to spend \na million dollars an hour on an AI system,  ",
      "offset": 1488.96,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "supposing it costs that much? I think that there \nare some answers. One example would be to give a  ",
      "offset": 1494.88,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "six-hour-long demo of superhuman intelligence \nin front of the General Assembly in the United  ",
      "offset": 1501.28,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "Nations: that could be worth spending say $6 \nmillion in order to provide that demo, to really  ",
      "offset": 1507.44,
      "duration": 7.68
    },
    {
      "lang": "en",
      "text": "show and let people kind of kick the tires on \nthis thing and see that this is what’s coming  ",
      "offset": 1515.12,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "soon. So it might enable those types of things \nby smoothing things out with this cost change.",
      "offset": 1519.76,
      "duration": 7.28
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: Two things to note there are: I \nsuppose roughly we might say that the cost in  ",
      "offset": 1527.04,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "the near term might drop by something like an \norder of magnitude a year perhaps. I think not  ",
      "offset": 1533.12,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "quite that much. So you might get like a 10x \ncost decrease on these models each year. So  ",
      "offset": 1536.72,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "that gives you some sense of if you’re willing to \nspend $10,000 an hour today, then you could study  ",
      "offset": 1540.96,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "something that in three years’ time might cost $10 \nan hour. So it’s coming at us pretty fast, but at  ",
      "offset": 1544.96,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "least it does give us forewarning. But it does \nmean we have to be willing to spend the money.",
      "offset": 1550.96,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "Toby Ord: It does. But also those orders \nof magnitude are not going to come forever.  ",
      "offset": 1554.8,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "It’s not the case that it’ll be 10 times cheaper \nthen 10 times cheaper and then 10 times cheaper,  ",
      "offset": 1559.2,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "and that just goes on for infinitely many \norders of magnitude. These cost reductions will  ",
      "offset": 1562.4,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "eventually hit a floor, and we don’t know where \nthat floor is. It could be that it stalls out  ",
      "offset": 1567.92,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "while still being too expensive for almost \nall tasks, requiring some kind of paradigm  ",
      "offset": 1572.8,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "breakthrough in order to push it forwards. \nI think that there’s often a feeling that  ",
      "offset": 1578,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "there’s some unlimited number of these orders \nof magnitude that are like manna from heaven.",
      "offset": 1582.8,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: Because these models seem so much less \nenergy efficient than human beings, I think that  ",
      "offset": 1587.6,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "gives some people a sense of the technological \nfrontier, at least demonstrated by human brains,  ",
      "offset": 1592.8,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "suggests that we’re a very long way from where \nwe could be at some future time. So that gives  ",
      "offset": 1597.2,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "us some hope that the efficiency increases \nmight be substantial for a while at least.",
      "offset": 1602.32,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "Toby Ord: I think that that’s right. That said,  ",
      "offset": 1606.4,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "the entire paradigm of LLMs and their scaling \nlaws is really inefficient in terms of  ",
      "offset": 1609.44,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "the data that’s needed to train them. I think \nthat suggests that the thing that might make  ",
      "offset": 1614.8,
      "duration": 6.56
    },
    {
      "lang": "en",
      "text": "that change might be more of a paradigm shift \nor some real breakthrough in the efficiency.",
      "offset": 1621.36,
      "duration": 6.542
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: Yeah. I think another thing that \npeople need to note is that it’s not the case  ",
      "offset": 1627.902,
      "duration": 3.058
    },
    {
      "lang": "en",
      "text": "that we can see an unlimited distance in the \nfuture. It’s not if we were willing to spend  ",
      "offset": 1630.96,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "a billion dollars an hour that we could \nexactly see how things are going to look  ",
      "offset": 1634.16,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "in 2032. There’s a limit. And I suppose the \nfurther out you go, the more you might have  ",
      "offset": 1637.28,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "changes to the architecture or the entire nature \nof these models. So it could make you a little  ",
      "offset": 1641.6,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "bit complacent if you’re saying, well, we spent \na whole tonne of money and now we can see how  ",
      "offset": 1646.16,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "the models are going to be in 2029. It gets \nmore and more shaky the further out you go.",
      "offset": 1649.52,
      "duration": 3.75
    },
    {
      "lang": "en",
      "text": "Toby Ord: Yeah, that’s right. One way to \nthink about what the companies have been doing  ",
      "offset": 1653.27,
      "duration": 4.17
    },
    {
      "lang": "en",
      "text": "with this inference scaling, like, what have \nthey been working on in order to scale this  ",
      "offset": 1658.8,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "up? Is it the case that there’s just a \nknob where we can just turn it now and  ",
      "offset": 1662.56,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "we can witness what happens if we put in a \nmillion times as much money into this thing?",
      "offset": 1665.6,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "The answer is not really. The main thing \nthat is being done is to try to make them  ",
      "offset": 1669.84,
      "duration": 6.64
    },
    {
      "lang": "en",
      "text": "coherent over longer and longer time \nperiods, or longer and longer numbers  ",
      "offset": 1676.48,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "of words that they can say in a row while \nstill being on topic and doing useful work.",
      "offset": 1681.36,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "And in a lot of these cases, the amount of \nwords is so large that it’s not shown to the  ",
      "offset": 1686,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "user — that’s considered to be its reasoning trace \nor something like that. You could think of that as  ",
      "offset": 1690.4,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "all of the subvocalisations that the employee \nhad while they were preparing the report that  ",
      "offset": 1695.6,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "took them possibly quite a long time to write, and \ninstead the finished report is what we show. But  ",
      "offset": 1699.44,
      "duration": 6.4
    },
    {
      "lang": "en",
      "text": "every time you want to make that chain of thought \n10 times longer, they do become incoherent,  ",
      "offset": 1706.72,
      "duration": 8.24
    },
    {
      "lang": "en",
      "text": "unless you put in a lot of effort on reinforcement \nlearning to try to train them to stay coherent.",
      "offset": 1714.96,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: I think another shift that you get \nfrom the intelligence increases coming from  ",
      "offset": 1720.4,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "putting more compute at the inference stage \nis that in general, information security,  ",
      "offset": 1724.96,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "just as a whole, becomes a lot less important — \nand indeed, the weights of the model become less  ",
      "offset": 1729.44,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "important. So the picture around open sourcing \nmodels changes quite a lot. Can you explain this?",
      "offset": 1733.6,
      "duration": 6.15
    },
    {
      "lang": "en",
      "text": "Toby Ord: Yeah, that’s my hypothesis here. So if  ",
      "offset": 1739.75,
      "duration": 6.57
    },
    {
      "lang": "en",
      "text": "GPT-4 is basically a kind of kink in the curve \nof how impressive the pre-trained models get,  ",
      "offset": 1746.32,
      "duration": 8.8
    },
    {
      "lang": "en",
      "text": "such that you get really diminishing returns \nbeyond that point, then one thing is that  ",
      "offset": 1755.12,
      "duration": 6.32
    },
    {
      "lang": "en",
      "text": "there may just be GPT-4-level models that are \nput into the public domain and then it’s all  ",
      "offset": 1762.48,
      "duration": 8.4
    },
    {
      "lang": "en",
      "text": "over. There’s no more future model like that to \ngo. So it could become moot in that direction.  ",
      "offset": 1770.88,
      "duration": 8.48
    },
    {
      "lang": "en",
      "text": "But it also becomes less interesting even from \njust the open source community’s perspective.",
      "offset": 1779.36,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "Whereas the way that it worked up until now \nwith this pre-training scaling was that the  ",
      "offset": 1785.84,
      "duration": 6.64
    },
    {
      "lang": "en",
      "text": "labs invested a vast amount of resources, a \nhuge amount of money in data collection in  ",
      "offset": 1792.48,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "order to produce this model, this collection \nof trained weights. And then they’re giving  ",
      "offset": 1797.52,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "that to you — and you, the user, can use all \nof that kind of embodied intelligence in it.",
      "offset": 1803.2,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "Whereas now what they’re saying is that \nevery now and then you have to spend 10  ",
      "offset": 1809.36,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "times as much compute while running it in order \nto make it more intelligent. And you, the user,  ",
      "offset": 1814.32,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "is going to have to spend that compute. You’ll \nneed your own GPUs and things in order to be  ",
      "offset": 1818.48,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "running these things. And you’ll need 10 times \nas many, and then you’ll need 100 times as many.  ",
      "offset": 1822.96,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "Maybe we’ve done the work to keep it \nconsistent over that time. So there’s  ",
      "offset": 1829.52,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "still some advantage to getting the latest \nversion of these weights that can stay on  ",
      "offset": 1832.88,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "topic for longer periods. But it’s kind of \nlike “bring your own compute” for the users.",
      "offset": 1836.24,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "So that story is less exciting than if all the \ncompute was done by Meta or something like that.",
      "offset": 1842.4,
      "duration": 5.988
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: Yeah. So if the breakdown is that 99% \nof the total compute that is going towards these  ",
      "offset": 1848.388,
      "duration": 4.332
    },
    {
      "lang": "en",
      "text": "AIs in general is occurring at the training stage, \nthen they’ve paid for all of it at the point that  ",
      "offset": 1852.72,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "they give you the weights for free. Then it’s \nlike it’s basically a free fee to operate.",
      "offset": 1856.32,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "If it’s the other way around, if 1% of the \ncompute that is going towards AI as a whole is  ",
      "offset": 1859.04,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "at the training stage and 99% of it is at the use \nstage, then they haven’t really been especially  ",
      "offset": 1862.96,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "generous or especially useful to you in giving \nyou the weights — because that’s not where the  ",
      "offset": 1868,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "cost is actually incurred; it’s all at the point \nof actually applying it to solving some problem.",
      "offset": 1871.04,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "So I guess we wouldn’t have to be as worried about \npeople being able to suddenly gain an enormous  ",
      "offset": 1875.12,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "amount of power if they stole the weights, or \nthe weights were leaked, or something dangerous  ",
      "offset": 1880.32,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "was open sourced, because it would simply be \nvery expensive to apply it to actual practical  ",
      "offset": 1884.64,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "problems. But at the same time, the benefits \nof open sourcing the stuff is not so great.",
      "offset": 1889.36,
      "duration": 3.59
    },
    {
      "lang": "en",
      "text": "Toby Ord: Yeah, I think that’s right. There \nis the alternate thing of, if it’s the case  ",
      "offset": 1892.95,
      "duration": 5.61
    },
    {
      "lang": "en",
      "text": "that even relatively small sets of weights — \nthe type that have perhaps already been open  ",
      "offset": 1899.2,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "sourced — if you could actually take those \nthings and then kind of soup them up through  ",
      "offset": 1904.16,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "bringing your own compute, in some ways that \nmakes the issue of proliferation kind of worse.",
      "offset": 1909.36,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "Suppose you’re a very well-resourced \nactor, in terms of having a lot of compute,  ",
      "offset": 1914.72,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "and you get some of these weights: then you \ncan really turn it into something amazing.  ",
      "offset": 1917.6,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "Maybe. But if you had all that compute in \nthe first place, you could have just trained  ",
      "offset": 1921.44,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "your own model. So I’m not sure, but I just \nwant to say that it’s a little bit unclear.",
      "offset": 1924.32,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "But I think the overall effect is that \npreviously there was all of this kind of  ",
      "offset": 1930.08,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "virtual compute or something distilled out \ninto these weights. Then you could have the  ",
      "offset": 1936.88,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "weights which represented the huge amount of \neffort that had gone on before that point.  ",
      "offset": 1940.24,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "And if that’s no longer true, and the answer \nis you’re mainly bringing your own compute,  ",
      "offset": 1944,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "then the story for either being a legitimate \nopen weight user or for being a spy who’s  ",
      "offset": 1948.24,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "hacked in and stolen the weights, in both cases, \ngetting these weights becomes less important.",
      "offset": 1953.92,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: A related impact that this would \nhave is that the AI market is more likely to  ",
      "offset": 1959.52,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "remain competitive than it would if there \nwas this enormous fixed cost that you had  ",
      "offset": 1964.4,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "to incur at the point of training \na model. Can you elaborate on that?",
      "offset": 1967.76,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "Toby Ord: I think this is right. \nOften this process of these kinds  ",
      "offset": 1970.88,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "of massive pre-training runs is \nlikened to software development:  ",
      "offset": 1976.08,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "it’s something where you go to a lot \nof effort to write a piece of software,  ",
      "offset": 1980.8,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "and then there’s zero marginal cost or \nvery small marginal cost to distribute it.",
      "offset": 1984.8,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "That was true to some degree with books, where \nyou go to a lot of effort to write a book and then  ",
      "offset": 1991.12,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "printing it is a lot less. But once software could \nbe distributed on CD or then just downloaded, it  ",
      "offset": 1994.56,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "became trivial costs to the company to, say, have \nan extra copy of Microsoft Word distributed to a  ",
      "offset": 1999.76,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "user. So once you write a word processor, you do \nall of the software engineering for it, you really  ",
      "offset": 2005.84,
      "duration": 6.56
    },
    {
      "lang": "en",
      "text": "want to sell it to a lot of customers, because \nevery customer is basically just pure profit.",
      "offset": 2012.4,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "That’s this zero marginal cost type thing. And \nif you have that in an industry, then you tend  ",
      "offset": 2017.68,
      "duration": 7.44
    },
    {
      "lang": "en",
      "text": "to get a small number of players — because you \nreally want to be the best one of these things  ",
      "offset": 2025.12,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "and then potentially just swallow up the \nwhole market. Whereas if you enter it  ",
      "offset": 2030.8,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "as a new entrant, you put in all these \nupfront costs. If you’re the third-best one,  ",
      "offset": 2036,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "why would anyone use your thing? Even if you sell \nit for less, it’s very hard to be a player there.",
      "offset": 2040.56,
      "duration": 7.44
    },
    {
      "lang": "en",
      "text": "Whereas this could change that aspect of it, \nand mean that most of the costs are actually  ",
      "offset": 2048,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "in producing each item. A little bit like, let’s \nsay you develop tools for a hardware company —  ",
      "offset": 2053.92,
      "duration": 6.8
    },
    {
      "lang": "en",
      "text": "physical tools: hammers, screwdrivers, and things \nlike that — then it’s the case that some of the  ",
      "offset": 2060.72,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "costs go into designing the new hammer, but most \nof it is just that every hammer you make costs a  ",
      "offset": 2066.4,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "certain amount of money, and then you get a kind \nof limited amount of profit when you sell it.",
      "offset": 2070.64,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "So it might be becoming more \nlike an industry of that sort,  ",
      "offset": 2074.48,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "and that would have a different \nkind of market structure.",
      "offset": 2077.52,
      "duration": 2.64
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: It suggests to me that then more \nof the profit that’s coming from AI would go  ",
      "offset": 2080.16,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "to the hardware companies, because they’re the \nones who actually have the scarce resource,  ",
      "offset": 2084.96,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "or at least temporarily scarce \nresource. So it’d be much harder,  ",
      "offset": 2089.68,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "I think, for the software developers \nto gain a huge margin, because there  ",
      "offset": 2092.4,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "would be many of them with roughly \nsimilarly powerful and useful models.",
      "offset": 2096.24,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "Which is kind of where things stand \ntoday: there’s at least three — or four,  ",
      "offset": 2100.24,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "possibly — models that are roughly at \nparity, which means that then where does  ",
      "offset": 2103.44,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "the surplus go? It probably goes to the hardware \nproducers who have this kind of scarce resource,  ",
      "offset": 2108.48,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "which is the thing that you desperately are trying \nto acquire in order to be able to apply them.",
      "offset": 2112.48,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "Toby Ord: That sounds about right. In \ngeneral, there’s always this question  ",
      "offset": 2116.24,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "for the companies that are trying to make a \nlot of money with this: which step of this  ",
      "offset": 2121.52,
      "duration": 9.04
    },
    {
      "lang": "en",
      "text": "value chain makes the most profit? What we \nhave at the moment is that the final stage  ",
      "offset": 2130.56,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "of these people who’ve trained these models is \nreally quite competitive between a handful of  ",
      "offset": 2135.44,
      "duration": 7.84
    },
    {
      "lang": "en",
      "text": "strong players. Whereas the step before that, of \nthe most powerful GPUs, is really locked up by  ",
      "offset": 2143.28,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "one main player. So they’ve got more ability to \nget a kind of monopoly pricing in at that point.",
      "offset": 2149.28,
      "duration": 6.068
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: I guess we should say that this is \nwhere things are trending as a result of this  ",
      "offset": 2155.348,
      "duration": 4.492
    },
    {
      "lang": "en",
      "text": "switch towards inference scaling. It won’t \nnecessarily go all the way to there. It is  ",
      "offset": 2159.84,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "interesting to know, because I think just a \ncouple of months ago I recorded an episode  ",
      "offset": 2165.76,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "with Tom Davidson talking about the risk of \nseizure of power. He was describing this thing  ",
      "offset": 2168.8,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "where maybe almost all of the fixed costs are \nat the training stage, and that would tend to  ",
      "offset": 2173.68,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "push you towards a market with a handful of \ncompanies, or possibly even at some stage,  ",
      "offset": 2177.12,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "just one company, that is willing to spend the $10 \ntrillion on the super training run that produces  ",
      "offset": 2180.72,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "AGI. I think that is still possible, things could \nchange, but that’s looking a bit less likely now.",
      "offset": 2185.92,
      "duration": 6.23
    },
    {
      "lang": "en",
      "text": "Toby Ord: Yeah, that’s right.",
      "offset": 2192.15,
      "duration": 0.89
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: So I guess that has been the \nkind of positive or neutral effects here.  ",
      "offset": 2193.04,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "I think one that people might \nfind a little more troubling,  ",
      "offset": 2197.52,
      "duration": 2.32
    },
    {
      "lang": "en",
      "text": "that might have jumped off the page at them \nalready, is: if you’re in a world where you  ",
      "offset": 2199.84,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "can access superintelligence earlier if \nyou’re willing to spend a tonne of money,  ",
      "offset": 2204.48,
      "duration": 2.64
    },
    {
      "lang": "en",
      "text": "then that suggests that rich and powerful and \nmore connected people will be able to access these  ",
      "offset": 2207.12,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "tools potentially many years ahead of the general \npublic. What do you make of that implication?",
      "offset": 2210.88,
      "duration": 6.39
    },
    {
      "lang": "en",
      "text": "Toby Ord: Yeah, I think there’s a real \neffect there. I think we’ll look back on  ",
      "offset": 2217.27,
      "duration": 5.45
    },
    {
      "lang": "en",
      "text": "the period that’s just ended, where OpenAI \nstarted a subscription model for their AI  ",
      "offset": 2223.52,
      "duration": 7.68
    },
    {
      "lang": "en",
      "text": "system — where it was $20 a month, less than \na dollar a day, to have access to the best  ",
      "offset": 2231.2,
      "duration": 9.84
    },
    {
      "lang": "en",
      "text": "AI system in the world, and then a number of \ncompanies are offering very similar deals…",
      "offset": 2241.04,
      "duration": 6.4
    },
    {
      "lang": "en",
      "text": "We’ve got the situation where, for \nless than the price of a can of Coke,  ",
      "offset": 2248,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "you can have access to the leading system \nin the world. And it reminds me of this  ",
      "offset": 2252.96,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "Andy Warhol quote about what makes America \ngreat is that the president drinks a Coke,  ",
      "offset": 2258.32,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "Liz Taylor drinks a Coke, the bum on the corner \nof a street drinks a Coke — and you too could  ",
      "offset": 2264.32,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "have a Coke! The best kind of sugary beverage \nthat you can get, everyone’s got access to it.",
      "offset": 2267.6,
      "duration": 9.84
    },
    {
      "lang": "en",
      "text": "But I think that era is over. We had OpenAI \nintroducing a higher tier that cost 10 times  ",
      "offset": 2277.44,
      "duration": 7.6
    },
    {
      "lang": "en",
      "text": "as much money, because these inference costs \nare going up and actually they can’t afford  ",
      "offset": 2285.04,
      "duration": 6.56
    },
    {
      "lang": "en",
      "text": "to give you this level for the previous cost. \nAnd this is what you’re going to keep seeing:  ",
      "offset": 2291.6,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "the mqqqore that we do inference scaling, it’s \ngoing to have to cost the users substantially  ",
      "offset": 2297.28,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "more. And then there’s a question of \nhow many are prepared to pay that.",
      "offset": 2301.76,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "So it’s certainly going to create inequality \nin terms of access to these things,  ",
      "offset": 2306.96,
      "duration": 7.44
    },
    {
      "lang": "en",
      "text": "but it also might mean that it is not \nactually scaling well for the companies.  ",
      "offset": 2314.4,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "If it turns out that you offer a \nthing that costs 10 times as much  ",
      "offset": 2319.6,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "and less than a tenth of the people take it, \nand then you offer a thing that costs 100 times  ",
      "offset": 2324.64,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "as much and less than a tenth of the previous \ngroup that took the first one take this one,  ",
      "offset": 2329.68,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "then maybe each of these tiers is earning \nyou less and less money than the one before,  ",
      "offset": 2334.8,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "and it’s just not actually going to drive your \nability to buy more chips and train more systems.",
      "offset": 2339.84,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "Or it could go the other way around: it could \nbe that a fifth of people are prepared to pay  ",
      "offset": 2347.6,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "10 times as much and then a fifth of them \nare prepared to pay 10 times as much again,  ",
      "offset": 2351.12,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "and that you’re getting more and more \nmoney from each of these higher levels.",
      "offset": 2354.56,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "But which of those it is could really determine \nwhat happens in the industry and whether these  ",
      "offset": 2358.48,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "inference-scaled models are actually \nprofit centres for them or not.",
      "offset": 2365.92,
      "duration": 5.508
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: Yeah. So some of the previous \nchanges have slightly reduced our concern  ",
      "offset": 2371.428,
      "duration": 4.972
    },
    {
      "lang": "en",
      "text": "about concentration of power and risk \nof seizure of power by human beings.",
      "offset": 2376.4,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "But this particular issue — that a privileged \ngroup of people can gain access to superhuman  ",
      "offset": 2380.16,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "advice and superhuman assistance potentially \nsubstantially before anyone else in the world  ",
      "offset": 2385.84,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "has access to it — heightens the concern \nthat people at the companies or people  ",
      "offset": 2390,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "in the government who might take control \nor get privileged access to these models,  ",
      "offset": 2395.12,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "that they could potentially outfox everyone \nelse if they’re able to basically just have  ",
      "offset": 2399.68,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "access to tools that no one else \nis really able to compete with.",
      "offset": 2403.92,
      "duration": 2.48
    },
    {
      "lang": "en",
      "text": "Toby Ord: So while in theory this could happen \njust on the open marketplace with money, my  ",
      "offset": 2406.4,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "concern would be greatest about the company itself \ndeciding, for example, “We’ve got this model,  ",
      "offset": 2411.6,
      "duration": 6.56
    },
    {
      "lang": "en",
      "text": "what can we use it for? Maybe we can be willing \nto spend a million times inference scaling on it  ",
      "offset": 2418.16,
      "duration": 10.24
    },
    {
      "lang": "en",
      "text": "to do some really important work for us.” And \nthe company might want to do that internally  ",
      "offset": 2428.4,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "or the government of the place where the company \nis located might want these types of abilities.  ",
      "offset": 2434.16,
      "duration": 7.36
    },
    {
      "lang": "en",
      "text": "So I imagine that happening outside the open \nmarket is perhaps the most concerning place.",
      "offset": 2441.52,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "I should say as well that I’m imagining or \nthinking about all of this over the next  ",
      "offset": 2447.68,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "couple of years. I’m not claiming that in the \nlong-run equilibrium, when we’re imagining,  ",
      "offset": 2451.92,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "in a post-AGI world, how unequal will \naccess to AGI be? It could be very unequal,  ",
      "offset": 2456.72,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "or it could be very equal if we actually \nchoose to build a world like that. But  ",
      "offset": 2463.6,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "I’m setting that aside, because I \nthink that I can only really see  ",
      "offset": 2468.48,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "what’s going to happen for the next couple \nof years, and things may change after that.",
      "offset": 2471.6,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: What are the policy implications here? \nOne that stands out is that you might want to  ",
      "offset": 2475.84,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "insist on some level of transparency about what \nis possible at the frontier, if you’re willing  ",
      "offset": 2481.28,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "to spend a whole lot of money — just so that \nthe public and people in government have some  ",
      "offset": 2486.08,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "sense of what’s coming, and that companies can’t \nhide this if they would rather maybe obscure what  ",
      "offset": 2489.6,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "they already are aware is possible if you’re \nwilling to spend a million dollars an hour.",
      "offset": 2494.32,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "Toby Ord: Many of the current rules — to \nthe extent to which there are rules at all;  ",
      "offset": 2499.12,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "there are voluntary commitments and there’s \nalso the EU’s AI Act — they’re often focused  ",
      "offset": 2503.92,
      "duration": 7.2
    },
    {
      "lang": "en",
      "text": "on deployed models. This means that you \ncan circumvent a lot of this if you just  ",
      "offset": 2511.12,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "don’t deploy it. So maybe you have these kind of \nhigher tiers of inference scaling that are only  ",
      "offset": 2517.6,
      "duration": 7.28
    },
    {
      "lang": "en",
      "text": "accessible internally; then you \ncould have systems that are, say,  ",
      "offset": 2524.88,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "breaking through this human range \nof abilities without anyone knowing.",
      "offset": 2530.16,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "Whereas in this Andy Warhol Coke world, where \neveryone’s got access to the cutting-edge system,  ",
      "offset": 2534.88,
      "duration": 6.96
    },
    {
      "lang": "en",
      "text": "we kind of all knew that the people working \nat those companies had the same thing.  ",
      "offset": 2541.84,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "Or that if they had something better, \nwithin a few months we’d also have it,  ",
      "offset": 2546.4,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "or something like that. So yes, I feel that \ngovernments and regulators generally need to  ",
      "offset": 2549.44,
      "duration": 11.28
    },
    {
      "lang": "en",
      "text": "ask for more transparency in this world to know \nwhat the capabilities are for the leading-edge  ",
      "offset": 2560.72,
      "duration": 6.4
    },
    {
      "lang": "en",
      "text": "internal models, as well as the deployed ones.",
      "offset": 2567.12,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: Other probably negative \nimplications of inference scaling are  ",
      "offset": 2570.8,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "that it makes regulation of AI just substantially \nmore difficult in a number of different ways.",
      "offset": 2574.56,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "One thing is, up until recently, you \nwant to carve out the models that you  ",
      "offset": 2579.68,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "think are not particularly risky — that \nare basically just applications that we  ",
      "offset": 2585.12,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "should feel not only OK with, but actively \nexcited about. And then you want to carve  ",
      "offset": 2588,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "out the things where we don’t know what \nthis model is potentially capable of;  ",
      "offset": 2592.72,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "this is posing novel risks that we’ve perhaps \nnever seen before, and we want to at least do  ",
      "offset": 2595.44,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "some research and study it before we deploy it, \nor possibly even before we use it internally.",
      "offset": 2599.04,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "And to do that, we’ve used these compute \nthresholds — where we’ve said, if this is  ",
      "offset": 2604.24,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "more than 10 times larger than any model that’s \nbeen trained before, then it falls into the “let’s  ",
      "offset": 2607.76,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "study this first” regime. And if it’s smaller \nthan things that have already been trained,  ",
      "offset": 2611.36,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "then we’re probably in the clear and we can \nuse it with a reasonable degree of comfort.",
      "offset": 2615.52,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "Can you explain why inference scaling \nmakes this so challenging to actually do?",
      "offset": 2620.64,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "Toby Ord: So that paradigm of compute \ngovernance via these thresholds,  ",
      "offset": 2624.56,
      "duration": 7.04
    },
    {
      "lang": "en",
      "text": "you could think of it as trying to regulate a \nparticular object. What they’re saying is that  ",
      "offset": 2633.28,
      "duration": 6.96
    },
    {
      "lang": "en",
      "text": "if this object has gone substantially \nfurther than any that have come before,  ",
      "offset": 2640.24,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "in terms of what’s gone into it, \nthen we could try to regulate it.",
      "offset": 2644.88,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "So these trained weights are the object \nof interest. It’s a little bit like having  ",
      "offset": 2648.32,
      "duration": 6.56
    },
    {
      "lang": "en",
      "text": "regulation on automatic weapons, but not on \nnon-automatic weapons. Something like that,  ",
      "offset": 2654.88,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "where you take a particular class of \nobject and you put a regulation on it.",
      "offset": 2659.68,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "Whereas what we’re getting with inference \nscaling is it’s not the object itself;  ",
      "offset": 2664.72,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "it’s more what you do with it that matters. \nIt could be that you can take, for example,  ",
      "offset": 2669.36,
      "duration": 7.12
    },
    {
      "lang": "en",
      "text": "a GPT-4-sized pre-trained model, and then just \nthrough a smaller amount of post training,  ",
      "offset": 2676.48,
      "duration": 7.68
    },
    {
      "lang": "en",
      "text": "you can make it able to think on longer and longer \ntime horizons. Then you can just use that model  ",
      "offset": 2684.72,
      "duration": 8.64
    },
    {
      "lang": "en",
      "text": "with huge amounts of inference — so just run it \nover and over again. Maybe you put 10 times as  ",
      "offset": 2693.36,
      "duration": 6.32
    },
    {
      "lang": "en",
      "text": "much compute into running it over and over again \nas you put into the very first training of it.",
      "offset": 2699.68,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "But that’s currently not regulated on a lot of \nthese things. And even if you tried to regulate  ",
      "offset": 2705.36,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "that, it’s definitely different — because then \nyou’re trying to regulate the use of an object,  ",
      "offset": 2710.08,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "not regulate the object existing at all, \nwhich raises a lot of different questions.",
      "offset": 2714.96,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "But also, it might be really hard to do, \nbecause maybe you’ve got a system like GPT-4  ",
      "offset": 2720.24,
      "duration": 8
    },
    {
      "lang": "en",
      "text": "training on 10 trillion words of \ninformation, and then you merely  ",
      "offset": 2728.24,
      "duration": 6.32
    },
    {
      "lang": "en",
      "text": "scale up its inference by a factor of a million. \nThat’s still a big scaleup, and maybe that has  ",
      "offset": 2735.36,
      "duration": 7.52
    },
    {
      "lang": "en",
      "text": "kind of dramatic effects. But if you only use that \nonce — for, say, some internal deployment — the  ",
      "offset": 2742.88,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "total amount of compute it’s going to use is \nstill small compared to the original training.",
      "offset": 2748.56,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "So you wouldn’t really see it if you’re just \ntrying to add up all the compute. You’d see it if  ",
      "offset": 2752.24,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "OpenAI or some other group said every \nsingle user is getting a millionfold the  ",
      "offset": 2758.24,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "level of inference they previously \nhad — but if just one is doing it,  ",
      "offset": 2762.72,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "it’ll just be using up as much \ncompute as a million users use  ",
      "offset": 2766.56,
      "duration": 2.64
    },
    {
      "lang": "en",
      "text": "up. So it might not really be detectable \nif you’re trying to measure these things.",
      "offset": 2769.2,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "Therefore I’m concerned that this \nis a substantial problem for these  ",
      "offset": 2773.84,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "pre-training compute thresholds, and I \npersonally don’t think it’s possible to  ",
      "offset": 2779.84,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "overcome it. But maybe there’s some \ncreative work that will solve it.",
      "offset": 2785.2,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "But I’m not necessarily bearish on all \ncompute governance. It’s still the case  ",
      "offset": 2789.28,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "that if you know where all the GPUs are, for \nexample — lots of them are owned by these  ",
      "offset": 2793.6,
      "duration": 6.4
    },
    {
      "lang": "en",
      "text": "cloud computing providers — and then you have \nknow your customer rules for them and so on,  ",
      "offset": 2800,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "that you might be able to exert some \ncontrol over the dangerous possibilities  ",
      "offset": 2807.6,
      "duration": 6.32
    },
    {
      "lang": "en",
      "text": "through compute governance. But it \nmight have to change the way we do it.",
      "offset": 2813.92,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: The other complication that \nadds to compute governance is that,  ",
      "offset": 2817.2,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "I forget the technical details, but at \nthe point that you’re training a model,  ",
      "offset": 2822.56,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "you really want to have all of the computer \nchips in the same location — because it’s not  ",
      "offset": 2825.28,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "just compute that matters; it’s the ability to \nmove information incredibly quickly between all  ",
      "offset": 2829.44,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "of these computer chips that are in an array. \nThat’s an issue that occurs at training,  ",
      "offset": 2833.84,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "and means it’s very difficult to spread \nthe training of GPT-5 across many different  ",
      "offset": 2837.52,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "data centres. Maybe you could do it with a \nhandful, but I think ideally they really want  ",
      "offset": 2841.84,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "to have it all in one place, and you certainly \ncouldn’t distribute it across the entire world.",
      "offset": 2845.92,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "But if you’re just doing it with inference, then \nI think you don’t face this similar constraint  ",
      "offset": 2850.08,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "that you need to have all of the chips or \nmost of the chips in a single location;  ",
      "offset": 2854.56,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "you can potentially distribute them far more \nwidely. If your hope was that the government  ",
      "offset": 2858.56,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "would be able to identify the handful of places in \nthe world where most of the compute lives — and by  ",
      "offset": 2863.52,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "looking at what’s happening, they get visibility \non what the entire sector is doing — that is a  ",
      "offset": 2868.88,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "lot weaker if most of the juice is coming \nout of throwing more compute at inference.",
      "offset": 2873.2,
      "duration": 4.39
    },
    {
      "lang": "en",
      "text": "Toby Ord: Yeah, that’s right. We’ve seen \nthat with these stories over the last year of  ",
      "offset": 2877.59,
      "duration": 6.49
    },
    {
      "lang": "en",
      "text": "major companies trying to get nuclear power \nplants commissioned to get full access to  ",
      "offset": 2884.72,
      "duration": 6.96
    },
    {
      "lang": "en",
      "text": "them in order to run a data centre. Because the \ntraining, at least the standard ways of doing it,  ",
      "offset": 2891.68,
      "duration": 7.6
    },
    {
      "lang": "en",
      "text": "all have to be done in one place, that creates \nthis huge power density issue that you need a  ",
      "offset": 2899.28,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "lot of power in one location. And it’s difficult \nto do that with the grid without it being the  ",
      "offset": 2904.56,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "case that there’s actually literally a \npower plant there that is powering you;  ",
      "offset": 2908.24,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "it’s hard to just provide it through \nthe general kind of grid capacity.",
      "offset": 2912.64,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "And that’s actually given a lever for \ngovernment to have some power over these  ",
      "offset": 2916.48,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "companies — a lever that they don’t seem \nto have used at all. Because if they say,  ",
      "offset": 2922.08,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "“We would like to be fast tracked for this \nnew nuclear power plant,” you can say,  ",
      "offset": 2927.76,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "“We’re interested in fast tracking you, but you’ll \nhave to in return be more transparent about your  ",
      "offset": 2933.76,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "internal models and so on.” A lot of people say, \nhow could the government control these companies?  ",
      "offset": 2939.6,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "This was certainly a location where they \ncould, albeit the US government seems  ",
      "offset": 2944.88,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "to have just fast tracked all of this \nwithout asking for very much in return.",
      "offset": 2949.28,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "But this could change. It does depend on \nthe nature of this scaling up of inference.  ",
      "offset": 2954.72,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "I mentioned this example of doing really long \nchains of thought. But another way to do it is,  ",
      "offset": 2960.32,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "instead of having one of these employees \nwho you’ve sent to your virtual university  ",
      "offset": 2964.64,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "with your pre-training, instead \nof just having that employee work  ",
      "offset": 2969.28,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "on a project for longer and longer, you \ncould send the project to 10 employees.",
      "offset": 2972.16,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "So you spend 10 times as much compute to run \nall of these different virtual employees,  ",
      "offset": 2978.56,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "and then you see which one has done the \nbest job. If it’s objectively measurable,  ",
      "offset": 2983.12,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "you might be able to do that, or you might be \nable to have an 11th employee who looks over  ",
      "offset": 2986.96,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "the 10 reports and then selects which one \nis best and shows that to you or something.",
      "offset": 2990.88,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "And that’s one of the standard approaches that is \nbeing used to do inference scaling: doing them in  ",
      "offset": 2994.8,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "parallel instead of like a longer sequential \nthing. We’ll probably see a mix of these two,  ",
      "offset": 3000.64,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "and the parallel version is the type of thing \nyou can spread between different data centres.",
      "offset": 3005.44,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: A couple of years ago I was thinking \na lot about compute governance potential — could  ",
      "offset": 3010.32,
      "duration": 6.88
    },
    {
      "lang": "en",
      "text": "you exercise regulatory control by knowing where \nall of the compute is? — and thinking a lot about  ",
      "offset": 3017.2,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "information security and the risk of model weights \nbeing stolen. We’re all off in this direction.  ",
      "offset": 3021.84,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "Should we now be saying, “Don’t worry about that. \nCompute governance is not necessarily so relevant;  ",
      "offset": 3027.12,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "we don’t have to worry about the security \nof weights or open sourcing. Whatever goes”?",
      "offset": 3033.04,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "It feels like that would be far too far \nto go in that direction. But is this so  ",
      "offset": 3037.04,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "decision-relevant that people who are trying \nto improve the direction of AI by leaning on  ",
      "offset": 3042.48,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "these different things should be changing their \nplans? Or should we maybe wait and see whether  ",
      "offset": 3046.4,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "all of this stuff might go into reverse? Maybe \ninference scaling will peter out in a couple of  ",
      "offset": 3049.76,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "years and it will all be back to some new kind \nof training that they’ve figured out how to do.",
      "offset": 3053.52,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "Toby Ord: I think that people who are interested \nin AI governance should be tracking these things  ",
      "offset": 3057.84,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "maybe more than they are. They should be noticing \nthat AI governance, up until the start of this  ",
      "offset": 3065.6,
      "duration": 7.04
    },
    {
      "lang": "en",
      "text": "year, had all been done in this paradigm of \nthis scaling of pre-training, and we’d see all  ",
      "offset": 3072.64,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "of these charts that showed how impressive it \nwas going to be and project forward and so on.",
      "offset": 3077.84,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "Really I want to kind of stress that that era \nhas come to an end, and we’re now in some other  ",
      "offset": 3083.12,
      "duration": 6.88
    },
    {
      "lang": "en",
      "text": "era that might be a kind of continuation — but \nthere’s no particular reason why it should be;  ",
      "offset": 3090,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "there’s no reason why the slope of those \ncurves should be the same as it was beforehand.  ",
      "offset": 3094.72,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "And in fact, there are reasons to \nthink the slope of the lines is worse.",
      "offset": 3099.36,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "So they should be aware that a lot of the \nrules and ideas that they’ve been building up,  ",
      "offset": 3105.2,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "that they need to reevaluate them. My piece \non this was written in a week after realising  ",
      "offset": 3110.32,
      "duration": 9.28
    },
    {
      "lang": "en",
      "text": "a lot of these things, and I think it’s \nheld up reasonably well. But I wouldn’t  ",
      "offset": 3119.6,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "want to be telling people what to do \nbased on a small amount of one person  ",
      "offset": 3125.28,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "thinking about the implications these things \nmight have. I wouldn’t be surprised if there  ",
      "offset": 3128.72,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "were additional implications as big as the \nones that I mention, which I never found.",
      "offset": 3132.24,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: Yeah. One of the things I \nhope the audience takes away from this  ",
      "offset": 3139.28,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "interview is that technical changes can \nradically shift the strategic picture and  ",
      "offset": 3144,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "the governance picture. So far we’ve \nall been talking about the impacts of  ",
      "offset": 3148.88,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "scaling up inference at the point \nof use, at the point of inference.",
      "offset": 3152.48,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "But it’s also possible that we’re finding, \nand it is the case that we’re finding new  ",
      "offset": 3156.32,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "ways of applying enormous amounts of compute \nat the training process, just in different  ",
      "offset": 3160.48,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "ways — and that could have all of the reverse \nimplications of what we were just describing.",
      "offset": 3164.72,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "Could you explain how we’re finding new ways of \napplying large amounts of compute at the training  ",
      "offset": 3168.96,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "stage that is not the kind of pre-training \nthat we think has somewhat petered out so far?",
      "offset": 3174,
      "duration": 4.63
    },
    {
      "lang": "en",
      "text": "Toby Ord: Yeah, exactly. This process that \nI’m calling “inference scaling” — scaling  ",
      "offset": 3178.63,
      "duration": 5.45
    },
    {
      "lang": "en",
      "text": "up the inference compute — also gets called \n“reasoning,” although it doesn’t have to be  ",
      "offset": 3184.08,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "used for what we think of as reasoning, and it \nalso gets called “test-time compute,” which also  ",
      "offset": 3190.32,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "kind of implies that it’s happening at the \ntime of deployment to the user or something.",
      "offset": 3195.68,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "But I think it is really important \nto divide the versions: where  ",
      "offset": 3201.36,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "everything we’ve talked about so \nfar, it’s happening during deployment  ",
      "offset": 3206.8,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "for a particular user who’s trying \nto get value out of it, versus  ",
      "offset": 3209.6,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "using a whole lot of extra inference compute \nduring a larger part of the whole testing process.",
      "offset": 3214.8,
      "duration": 6.88
    },
    {
      "lang": "en",
      "text": "If you use it during the training process, \nthe economies of scale are different. So  ",
      "offset": 3221.68,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "suppose that as part of it, you’ve pre-trained \nthe model, and then during the post-training  ",
      "offset": 3227.12,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "you run really long inference chains and these \nchains of thought and so on, and you assess them.  ",
      "offset": 3232.64,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "They do this using reinforcement learning, \nso they give it hard problems that they  ",
      "offset": 3239.84,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "know how to check — say, really hard maths \nor coding problems, where there’s precise  ",
      "offset": 3244.64,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "answers — and then they train or reward these \nlong chains of inference that actually worked.",
      "offset": 3249.76,
      "duration": 7.04
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: That get the right answer.",
      "offset": 3256.8,
      "duration": 1.11
    },
    {
      "lang": "en",
      "text": "Toby Ord: Yeah. So they kind of roll \nit out with huge amounts of tokens,  ",
      "offset": 3257.91,
      "duration": 2.25
    },
    {
      "lang": "en",
      "text": "and then they use that to do more \npost-training on this set of weights.  ",
      "offset": 3260.16,
      "duration": 7.52
    },
    {
      "lang": "en",
      "text": "If you do all of the stuff you did there, \nonce it goes into this post-training,  ",
      "offset": 3267.68,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "you still kind of only have to do it once. \nAnd then if 10 times as many users come along,  ",
      "offset": 3273.2,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "you don’t have to spend 10 times as much extra \ncompute; you’ve just spent it once. It doesn’t  ",
      "offset": 3279.36,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "scale with the amount of deployment, so it \npotentially has quite different implications.",
      "offset": 3283.36,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "I think what we’ve been mainly seeing is the type \nof thing I just mentioned, where you try to get  ",
      "offset": 3290.8,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "the system to generate long chains of thought. And \nthen there’s two different versions: one is that  ",
      "offset": 3296.24,
      "duration": 8.16
    },
    {
      "lang": "en",
      "text": "you look at the outcome, the final answer, and you \nreward it or punish it based on that final answer  ",
      "offset": 3304.4,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "— and then these weights that represent the model \nget updated based on the reward or the punishment.",
      "offset": 3310.56,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "Or you do what’s called “process \nsupervision” instead of the final  ",
      "offset": 3316,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "answer, where you look at all the steps \ninside its reasoning chain of thought,  ",
      "offset": 3319.28,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "and you see if they seem to be going in the \nright way or if it seems to be getting stuck  ",
      "offset": 3323.52,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "or lost or something. It’s a bit like with \na child: you can either try to reward them  ",
      "offset": 3328.4,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "based on getting the right answer, or reward \nthem on whether it seems like they were kind  ",
      "offset": 3333.76,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "of applying the types of techniques that \nyou’ve been hoping that they would use.",
      "offset": 3337.12,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "So that happens already. And in order \nto actually productively scale up the  ",
      "offset": 3343.28,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "inference when you’re deploying it, you have \nto do a certain amount of extra inference  ",
      "offset": 3348.8,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "combined with reinforcement \nlearning when you’re training it.",
      "offset": 3353.44,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "But there’s also ways — I don’t know \nif they have been applied yet — but  ",
      "offset": 3356.4,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "they could go much further than \nthat. So this is a technique called  ",
      "offset": 3361.04,
      "duration": 6.64
    },
    {
      "lang": "en",
      "text": "“iterated distillation and amplification,” or \nat least that’s an interesting one to look at.",
      "offset": 3368.24,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "This is the technique that led to these \namazing performances in the case of Go  ",
      "offset": 3374.24,
      "duration": 6.8
    },
    {
      "lang": "en",
      "text": "with one of these DeepMind projects, AlphaGo \nZero. They had a neural network that looked  ",
      "offset": 3382.72,
      "duration": 10.8
    },
    {
      "lang": "en",
      "text": "at the board in the game of Go and tried \nto give it a kind of heuristic valuation  ",
      "offset": 3393.52,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "of how good is it for the current player? Is \nthis a winning board or is it a losing board,  ",
      "offset": 3398.16,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "and by how much? So it would \ntry to estimate and learn that.",
      "offset": 3403.92,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "It was kind of like intuitions — so System \n1 kind of ability for Go playing — just to  ",
      "offset": 3408.08,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "be able to see what looked like a good \nmove. But then they took that system  ",
      "offset": 3412.08,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "and they inference scaled it, they gave it a \nwhole lot of System 2 ability. In practice,  ",
      "offset": 3418.56,
      "duration": 7.04
    },
    {
      "lang": "en",
      "text": "they let it play out a whole lot of games from \nthat position using its current heuristics as  ",
      "offset": 3426.56,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "to what’s good and what’s bad. They let it \nplay things out, see how the games would go,  ",
      "offset": 3432,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "and then use that information to actually revise \ntheir idea of what looked like a good move.",
      "offset": 3436.08,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "So that version, potentially using \nthousands of times as much compute,  ",
      "offset": 3441.12,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "we call that the “amplified” version \nor the “inference-scaled” version.",
      "offset": 3447.04,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "Then the next step is that you can distil it. \nWhat you can try to do is take the moves that  ",
      "offset": 3450.64,
      "duration": 6.4
    },
    {
      "lang": "en",
      "text": "the amplified version makes when it’s also got \nthe ability to search through the game tree,  ",
      "offset": 3457.04,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "and just try to develop an intuition where \nyour System 1 — your intuitive response — is  ",
      "offset": 3462.8,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "to produce those types of moves. And then \nyou’ve now kind of improved your intuitions.",
      "offset": 3467.68,
      "duration": 7.92
    },
    {
      "lang": "en",
      "text": "Then you can do it again. You \ncan take that one and scale it  ",
      "offset": 3475.6,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "up to 1,000 times as much compute \nusing the new improved intuitions,  ",
      "offset": 3478.4,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "and you get this improved play, and then \nyou distil that play back down again.",
      "offset": 3484.32,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: To a smaller model that \ndoesn’t require so much compute.",
      "offset": 3487.84,
      "duration": 2.63
    },
    {
      "lang": "en",
      "text": "Toby Ord: Exactly. So there’s these two \ntypes of steps. Effectively what happens  ",
      "offset": 3490.47,
      "duration": 3.37
    },
    {
      "lang": "en",
      "text": "is that it leads to this kind of ladder \nwhere performance improves quite a lot  ",
      "offset": 3493.84,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "when you amplify it and you spend 1,000 \ntimes as many resources on the problem.  ",
      "offset": 3499.76,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "But then when you distil that one back down, \nyou’ve got a cheap thing again, but it’s a little  ",
      "offset": 3503.76,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "bit better than the previous one. And then you do \nit again and you go up and then back down — but  ",
      "offset": 3507.76,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "every time you come back down to a cheap \nmodel, it’s a bit better than the one before.",
      "offset": 3512,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "They ultimately applied this I think more than \n1,000 steps climbing up this ladder, and in doing  ",
      "offset": 3516.64,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "so it blasted through the human level. Eventually \nthey put it all the way up to a point where it  ",
      "offset": 3522.16,
      "duration": 7.28
    },
    {
      "lang": "en",
      "text": "could no longer distil out any advantage from \nthe amplified model, so the process stalled out.",
      "offset": 3529.44,
      "duration": 8.72
    },
    {
      "lang": "en",
      "text": "So in general, it’s a very powerful technique. \nIt’s not clear where it will stall out. Maybe  ",
      "offset": 3538.16,
      "duration": 8.96
    },
    {
      "lang": "en",
      "text": "there’s some other games different to Go where \nit would stall out before the human level,  ",
      "offset": 3547.12,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "and you wouldn’t be able to use this technique \nto get all the way up to superhuman play.",
      "offset": 3550.24,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "Now, could that be applied for these \nreasoning models? I don’t see why  ",
      "offset": 3555.04,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "it couldn’t. You could imagine a situation \nwhere the new model is just being generated,  ",
      "offset": 3561.52,
      "duration": 11.36
    },
    {
      "lang": "en",
      "text": "let’s say every hour or something, from \nthe old model — where they take a model,  ",
      "offset": 3572.88,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "they let it reason for huge amounts of time, \nproduce a final set of answers. Then they  ",
      "offset": 3578.8,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "train a new model to just try to produce those \nanswers straight away — to have its intuitive,  ",
      "offset": 3583.12,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "stream-of-thought answers to be like the \nfinished polished paper that would come out  ",
      "offset": 3589.12,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "of the other process. And then it will learn \na little bit of that and hopefully better,  ",
      "offset": 3593.36,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "then you amplify it with heaps of inference \ncompute, then distil it back down and so on.",
      "offset": 3599.12,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "If this was possible, then \nit could lead to explosive  ",
      "offset": 3604,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "improvement in capabilities, all \nby using all of this inference,  ",
      "offset": 3608.56,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "but entirely inside the training process. \nAnd then, what you do at the end of all of  ",
      "offset": 3613.04,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "that is that the final distilled model you \ncould deploy to customers or what have you.",
      "offset": 3617.12,
      "duration": 6.56
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: So it wouldn’t necessarily be \nmore expensive at the point that you’re  ",
      "offset": 3623.68,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "actually applying it anymore, because \nyou’ve found a way to have the intuition  ",
      "offset": 3626.48,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "of the ability to mimic someone \nwho’s been able to think an enormous  ",
      "offset": 3630.24,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "amount of time — but to do it very \nquickly, with very little thought.",
      "offset": 3633.6,
      "duration": 3.51
    },
    {
      "lang": "en",
      "text": "Toby Ord: Yeah, that’s right. So this whole \nprocess, doing it literally as I described,  ",
      "offset": 3637.11,
      "duration": 5.05
    },
    {
      "lang": "en",
      "text": "with iterated distillation and amplification: \nwill that work? I think it probably won’t. I’d  ",
      "offset": 3642.16,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "say less than 50% chance that that will work. \nMaybe there’s a 10% chance it would work.",
      "offset": 3647.92,
      "duration": 7.6
    },
    {
      "lang": "en",
      "text": "Is there something like that that can work? \nMaybe. I think that there is a possibility  ",
      "offset": 3656.24,
      "duration": 8
    },
    {
      "lang": "en",
      "text": "that is non-negligible and I think substantial. By \nhaving both this kind of System 1 ability through  ",
      "offset": 3664.24,
      "duration": 8.8
    },
    {
      "lang": "en",
      "text": "pre-training, and then also this ability to \nimprove System 2, and then effectively to  ",
      "offset": 3673.04,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "have your System 1 intuitions be trained on what \nyou would have done after a bunch of this more  ",
      "offset": 3677.36,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "formal reasoning, and then keep iterating that — \nthat having both these two components of natural  ",
      "offset": 3682.88,
      "duration": 6.64
    },
    {
      "lang": "en",
      "text": "intelligence could be something that leads to this \nkind of explosive recursive self-improvement of  ",
      "offset": 3689.52,
      "duration": 6.72
    },
    {
      "lang": "en",
      "text": "these systems. And I do think that that has become \nmore possible in this world of inference scaling.",
      "offset": 3696.24,
      "duration": 6.628
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: Right. So we have an example \nwhere this has really worked with the Go  ",
      "offset": 3702.868,
      "duration": 4.492
    },
    {
      "lang": "en",
      "text": "models. And I imagine with other games \nwhere it’s clear whether you win or lose,  ",
      "offset": 3707.36,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "that this approach of amplification and \ndistillation should work in most of those cases.",
      "offset": 3710.24,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "So now, with these new reasoning models \nlike o1 and o3 that OpenAI has produced,  ",
      "offset": 3716.32,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "and the other companies have their own \nones, the way that they’ve been doing  ",
      "offset": 3720.72,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "the second stage of training with them \nis that they present them with reasoning  ",
      "offset": 3726.16,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "puzzles — sort of exam-style questions that \nhave a clear right and wrong answer. And that  ",
      "offset": 3730.24,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "provides an analogy to a game of Go, where you \neither win or lose. So you have a clear signal  ",
      "offset": 3734.72,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "about whether at the end of the day you \ngot the right answer or you won the game.",
      "offset": 3739.52,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "Then they can go back and say, in this case, \nusing this style of reasoning, it got to the  ",
      "offset": 3744.08,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "right answer. So we want to reinforce more of \nthat, want to produce more of that. And that  ",
      "offset": 3749.52,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "has allowed these models to get much better \nat figuring out how to think for a long time,  ",
      "offset": 3753.68,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "and maintain accurate reasoning through the \nentire process, and in general have reasoning  ",
      "offset": 3758.24,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "strategies that tend to lead you towards correct \nanswers — at least in that style of question.",
      "offset": 3764.08,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "I guess you’re saying because this worked with \nGo when we had a clear success and fail signal,  ",
      "offset": 3769.28,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "maybe it will also work in these \nkinds of reasoning cases where,  ",
      "offset": 3773.68,
      "duration": 2.64
    },
    {
      "lang": "en",
      "text": "at least for some domains of problems, we also \nhave a clear indicator of success and failure?",
      "offset": 3776.32,
      "duration": 5.03
    },
    {
      "lang": "en",
      "text": "Toby Ord: Yeah, that’s right. It may not generally \nwork across all possible forms of reasoning,  ",
      "offset": 3781.35,
      "duration": 8.25
    },
    {
      "lang": "en",
      "text": "to lead to superb ability to write \nemails to the regulators to argue  ",
      "offset": 3790.24,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "your case or whatever in areas where the \nsuccess conditions are quite unclear,  ",
      "offset": 3794.8,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "if you can’t send off 10,000 emails to the \nregulator and find out which ones convince them.",
      "offset": 3798.72,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "So it may be that it’s more \nlimited in its applicability,  ",
      "offset": 3803.04,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "or it may not work at all. \nIt may be that it turns out  ",
      "offset": 3806.56,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "that it’s hard to get this kind of recursive \nprocess off the ground: effectively the  ",
      "offset": 3810.16,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "point where it stalls out is the first step \ninstead of the thousandth step. We don’t know.",
      "offset": 3814.64,
      "duration": 6.56
    },
    {
      "lang": "en",
      "text": "But that also brings up this aspect \nwhere, whether or not you’re doing this  ",
      "offset": 3821.2,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "iterated distillation and amplification, all of \nthe reasoning work that’s happening at the moment,  ",
      "offset": 3826.56,
      "duration": 7.28
    },
    {
      "lang": "en",
      "text": "in order to get it to be coherent over longer \ntimes, you need some kind of this reward signal  ",
      "offset": 3835.68,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "in order to be able to train it. And this is \nprimarily coming from cases where there is a known  ",
      "offset": 3841.12,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "correct answer to the problem. So this could be \ntricky maths problems, and also a lot of computing  ",
      "offset": 3847.2,
      "duration": 8.56
    },
    {
      "lang": "en",
      "text": "problems — where there’s a plain text question \nto write a program that meets this specification,  ",
      "offset": 3855.76,
      "duration": 7.44
    },
    {
      "lang": "en",
      "text": "and then they test the program based on a whole \nlot of inputs and the outputs it should produce.  ",
      "offset": 3863.2,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "These are called unit tests. And then it also \nmaybe checks how long it takes the program to run.",
      "offset": 3870,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "This is the kind of thing that you get for \nhumans in these coding competitions. For humans,  ",
      "offset": 3878.16,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "ability in coding competitions or advanced \nmathematics correlates quite strongly with  ",
      "offset": 3884.08,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "general intelligence across a lot \nof different areas. With AI systems,  ",
      "offset": 3889.04,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "it’s not as clear how well it will correlate. In \nsome ways, we’re going back to the world of 2019,  ",
      "offset": 3895.52,
      "duration": 9.28
    },
    {
      "lang": "en",
      "text": "where extreme ability at Go is very \nimpressive. If I met a human who can  ",
      "offset": 3904.8,
      "duration": 7.44
    },
    {
      "lang": "en",
      "text": "play at a grandmaster level of Go, \nI’d be genuinely impressed by them,  ",
      "offset": 3912.24,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "and I might think they would also correlate with \nbeing good at other things as well. It could find  ",
      "offset": 3916.56,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "out, are you good at maths, are you good at \nthinking through complex reasoning things?",
      "offset": 3920.56,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "But in this case it’s not clear how well it will \ncorrelate. And I feel that the AI labs are exactly  ",
      "offset": 3925.6,
      "duration": 6.96
    },
    {
      "lang": "en",
      "text": "the kind of places that are impressed by research \nmathematics and are very impressed by people who  ",
      "offset": 3932.56,
      "duration": 6.32
    },
    {
      "lang": "en",
      "text": "can ace coding competitions, because so many of \nthem have come through a programming background  ",
      "offset": 3938.88,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "— but they may have over-indexed on some of \nthese challenges that are difficult for humans.",
      "offset": 3944.24,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "We know that for at least 50 years computers \noutdo us at multiplying two numbers together.  ",
      "offset": 3950.24,
      "duration": 7.52
    },
    {
      "lang": "en",
      "text": "At some point that was impressive, and we’ve \ntrained ourselves to no longer be impressed  ",
      "offset": 3957.76,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "by this fact. And it may be that, say, ability \nto write really efficient code for extremely  ",
      "offset": 3961.44,
      "duration": 7.2
    },
    {
      "lang": "en",
      "text": "well specified programming tasks, maybe \nthat will also become something that we  ",
      "offset": 3968.64,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "just don’t think is very impressive. And it may \nnot generalise to other kinds of reasoning tasks.",
      "offset": 3972.96,
      "duration": 8.16
    },
    {
      "lang": "en",
      "text": "In general, the track record for reinforcement \nlearning and generalising is pretty poor. When  ",
      "offset": 3981.12,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "DeepMind did the original Atari work, they \nbuilt a system that was impressive, but it  ",
      "offset": 3987.92,
      "duration": 8.72
    },
    {
      "lang": "en",
      "text": "was not a single trained model that could \nplay all 50 or so Atari games. Instead,  ",
      "offset": 3996.64,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "it was a single system that could take \nan Atari game and it could train an agent  ",
      "offset": 4002.4,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "that could only play that Atari game. \nAnd it could train 50 of these agents,  ",
      "offset": 4006.32,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "one for each Atari game. So it was a \ngeneral system for creating narrow agents.",
      "offset": 4009.84,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "And they’d hoped for what’s called “transfer \nlearning,” where if you get good at something,  ",
      "offset": 4015.52,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "it helps you be good at something else. \nIn general, that was very hard to do with  ",
      "offset": 4020,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "reinforcement learning, but it’s one \nof the big successes of the LLM era.",
      "offset": 4023.12,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "But now, if we’re kind of switching \nback to using reinforcement learning  ",
      "offset": 4028.16,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "to deal with the fact that we’ve kind of \nplateaued, then we maybe will expect things  ",
      "offset": 4032.16,
      "duration": 6.72
    },
    {
      "lang": "en",
      "text": "to go narrow again and for this increased \nperformance to both slow down and also to be  ",
      "offset": 4038.88,
      "duration": 6.88
    },
    {
      "lang": "en",
      "text": "only in very slender subdomains of all \nthe types of things that humans do.",
      "offset": 4048.08,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: So we opened talking about \nhow in some sense things looked safer or  ",
      "offset": 4052.8,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "more comfortable since 2019, because we had \nswitched away from reinforcement learning  ",
      "offset": 4057.2,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "and towards this next-word prediction, which \nled to more understanding of human concepts.",
      "offset": 4062.48,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "Now it seems like over the last 18 \nmonths, we’ve been screaming back in  ",
      "offset": 4067.76,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "the other direction towards reinforcement \nlearning as the place that we’re getting  ",
      "offset": 4071.36,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "most of the juice. And many of the \nproblems that had faded away through  ",
      "offset": 4075.6,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "2023 might be basically all coming back. \nAnd it does seem that that’s the case.",
      "offset": 4079.6,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "So you’re saying one distinctive thing about \nreinforcement learning is that it seems to have  ",
      "offset": 4084.24,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "less generalisability than the LLM next-token \nprediction style did. The other thing is,  ",
      "offset": 4087.6,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "I think reinforcement learning agents are \nmore narrow, and they also are a lot more  ",
      "offset": 4093.68,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "reward hacky. So they tend to do crazy stuff \njust in order to try to win — because that is,  ",
      "offset": 4098.32,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "after all, the signal that they’ve been given:  ",
      "offset": 4104,
      "duration": 2.32
    },
    {
      "lang": "en",
      "text": "they basically are just rewarded whenever they \nmanage to achieve the outcome. They don’t have  ",
      "offset": 4106.32,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "broader concepts of common sense, and \nwhat was the intent of the operator.",
      "offset": 4109.84,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "Do you want to elaborate a little bit on that?",
      "offset": 4114.48,
      "duration": 1.75
    },
    {
      "lang": "en",
      "text": "Toby Ord: Yeah, I think that’s exactly \nright. It’s interesting that when I wrote  ",
      "offset": 4116.23,
      "duration": 3.69
    },
    {
      "lang": "en",
      "text": "my remarks on “The Precipice Revisited,” \nit was kind of the high water mark of all  ",
      "offset": 4119.92,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "of those changes. And since then, some \nof them have gone into reverse a bit.",
      "offset": 4123.68,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "Another one to add to that is not just \nthe shift to reinforcement learning,  ",
      "offset": 4130.32,
      "duration": 2.56
    },
    {
      "lang": "en",
      "text": "but shift to agents again — which I said \nwere a particularly dangerous thing that  ",
      "offset": 4132.88,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "everyone was preoccupied with. And then we \nhad a whole lot of developments in systems  ",
      "offset": 4138.96,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "that weren’t agents and then maybe we’re \ngoing back to the dangerous ones again.",
      "offset": 4143.6,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "So yes, I think you pretty much nailed all of \nthat. The shift to reinforcement learning will  ",
      "offset": 4147.36,
      "duration": 6.64
    },
    {
      "lang": "en",
      "text": "have some of these difficult problems, including \nnarrowness — but also, as you say, including  ",
      "offset": 4154,
      "duration": 7.12
    },
    {
      "lang": "en",
      "text": "this aspect that the AI systems might do \nthis reward-hacking type of behaviour.",
      "offset": 4161.12,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "And there have been a number of reports of this \nwith recent systems. I think o3 in particular,  ",
      "offset": 4166.48,
      "duration": 6.4
    },
    {
      "lang": "en",
      "text": "there have been reports of \nit doing reward hacking.",
      "offset": 4174.16,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "I saw one in the wild, actually, that doesn’t \nseem too well known. In one of the two blog posts  ",
      "offset": 4177.76,
      "duration": 8.88
    },
    {
      "lang": "en",
      "text": "launching o3, OpenAI’s new very capable model, \nit showed a whole lot of different impressive  ",
      "offset": 4186.64,
      "duration": 7.12
    },
    {
      "lang": "en",
      "text": "tasks that it did in visual reasoning. One of them \nwas this drawing where they had the numbers 1, 3,  ",
      "offset": 4193.76,
      "duration": 8.32
    },
    {
      "lang": "en",
      "text": "5, 2, 4, ? — and it said the answer isn’t \n6. This was a little kind of a brain teaser.  ",
      "offset": 4202.08,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "You might think it’s a maths problem; it \nturns out it’s a lateral thinking problem  ",
      "offset": 4208.56,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "and it’s drawn in the shape of a gear stick \nand the answer is meant to be R for reverse.",
      "offset": 4211.52,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "It’s a somewhat interesting question, which is why \nit had been big on Twitter a couple of years ago.  ",
      "offset": 4217.76,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "And the AI system had this reasoning trace that \nwas shown in the blog post. I remember thinking,  ",
      "offset": 4222.64,
      "duration": 7.52
    },
    {
      "lang": "en",
      "text": "where does it make the kind of a-ha moment \nto realise it’s not a maths problem,  ",
      "offset": 4230.16,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "that it’s a lateral thinking problem? And I kind \nof narrowed it down and then I saw there was a  ",
      "offset": 4234,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "step a little bit before, where it says, “Now \nsearching for ‘13524?, the answer is not six.'”",
      "offset": 4238.96,
      "duration": 10.88
    },
    {
      "lang": "en",
      "text": "And it turns out if you just type that into a \nsearch engine, you come up with the page that  ",
      "offset": 4249.84,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "it reaches at the Hindustan Times, which \njust explains this new brain teaser that  ",
      "offset": 4254,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "was going around and explains the answer. So \nit just googled the answer halfway through  ",
      "offset": 4258.24,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "the track. It doesn’t say that though. It \nthen says, “Hang on, maybe it’s totally  ",
      "offset": 4262.24,
      "duration": 7.04
    },
    {
      "lang": "en",
      "text": "different. Maybe it’s about cars instead \nof about maths,” and then has the answer.",
      "offset": 4269.28,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "I should say that five years \nago, having a system that does  ",
      "offset": 4278.64,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "optical character recognition on a \npicture, finds the text, googles it,  ",
      "offset": 4282.24,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "extracts the result from the answer, that would \nhave been somewhat impressive five years ago.  ",
      "offset": 4286.32,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "It’s not impressive now. And so it can’t have \nbeen intentional that it did this in their post,  ",
      "offset": 4290.32,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "where that was one of the very few examples \nshown to show how impressive it was.",
      "offset": 4295.68,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "But it also implies that since that page at the \nHindustan Times was a year or two old, and also  ",
      "offset": 4300.72,
      "duration": 10
    },
    {
      "lang": "en",
      "text": "had been discussed on Twitter, that this model \nmust have actually seen this problem multiple  ",
      "offset": 4310.72,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "times on multiple web pages during its training \nperiod. And so the more I thought about it —",
      "offset": 4315.12,
      "duration": 6.88
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: You think it’s surprising that \nit wasn’t able to intuitively answer it  ",
      "offset": 4322,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "just from memorisation, basically \nduring the pre-training process.",
      "offset": 4326.56,
      "duration": 3.345
    },
    {
      "lang": "en",
      "text": "Toby Ord: Yes. Although maybe the person \noverseeing would have caught that if it  ",
      "offset": 4329.905,
      "duration": 3.455
    },
    {
      "lang": "en",
      "text": "just said the answer straight away. But it’s \ndeeply unimpressive that a system that has  ",
      "offset": 4333.36,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "seen a logic puzzle multiple times then had \nto google the answer to find out what it was.",
      "offset": 4338.56,
      "duration": 7.12
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: I think what’s distinctive \nabout the reinforcement learning  ",
      "offset": 4345.68,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "models is that they learn basically not \nto say, “I just googled it and I found  ",
      "offset": 4350.4,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "the answer,” because that’s going to be \nkind of negatively reinforced. You end  ",
      "offset": 4355.12,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "up encouraging them to do these perverse \nways of basically impressing the operator  ",
      "offset": 4358.8,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "to get them to think that they’ve done the \nthing that was desired, even if they hadn’t.",
      "offset": 4363.36,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "There’s other cases where you’ve got these \nreinforcement training learned coding agents where  ",
      "offset": 4367.04,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "they’ll be working on solving some sort of coding \nproblem, they’ll realise that they can’t do it,  ",
      "offset": 4372.56,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "but I think they manage to figure out what \nthe correct answer would be during the check  ",
      "offset": 4377.12,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "stage. And rather than actually design code \nthat solves the problem and calculates it,  ",
      "offset": 4381.2,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "they just hard code in the answer so that when \nit’s checked to see whether it succeeded or not,  ",
      "offset": 4385.92,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "it outputs the correct answer, but using a \ncompletely different method that wasn’t desired.",
      "offset": 4390.4,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "This is sort of a classic sign \nof reinforcement learning,  ",
      "offset": 4394.08,
      "duration": 2.24
    },
    {
      "lang": "en",
      "text": "where all you’ve rewarded them on is the output \n— and if you’re not scrutinising the process,  ",
      "offset": 4396.32,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "then they will figure out some way of fooling you \ninto thinking that they’ve done what you want.",
      "offset": 4400.24,
      "duration": 3.51
    },
    {
      "lang": "en",
      "text": "Toby Ord: Exactly. This is the thing that’s \ncalled reward hacking. And it’s kind of  ",
      "offset": 4403.75,
      "duration": 4.17
    },
    {
      "lang": "en",
      "text": "interesting because it’s only a problem if you \ntake into account that there was an intended  ",
      "offset": 4407.92,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "solution. That the humans did not want you to go \nand give specific answers, that you’re going to  ",
      "offset": 4412.4,
      "duration": 6.64
    },
    {
      "lang": "en",
      "text": "be tested on what’s the answer to five different \nquestions and then your whole program just says,  ",
      "offset": 4419.04,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "if it’s question one, print this; if it’s question \ntwo, print that. That was definitely not intended,  ",
      "offset": 4423.84,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "even though at some level, it’s just a clever \nkind of solution. There’s a TV show, Taskmaster,  ",
      "offset": 4429.44,
      "duration": 8.8
    },
    {
      "lang": "en",
      "text": "where the contestants are allowed to do this \nkind of thing, and it’s quite funny to watch.",
      "offset": 4438.24,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "But this is not what’s intended. So we call it \nreward hacking. Reinforcement learning tends  ",
      "offset": 4443.76,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "to lead to very creative solutions, including \nthis style of perversely creative solution.  ",
      "offset": 4449.92,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "I’m not saying that the models got it wrong \nor something, but it’s certainly a kind of  ",
      "offset": 4455.84,
      "duration": 7.36
    },
    {
      "lang": "en",
      "text": "out-of-the-box type situation where it’s harder to \ncontrol them, it’s easier for them to deceive you.",
      "offset": 4463.2,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "An example like what you were talking about: \nshortly after DeepSeek’s R1 model came out,  ",
      "offset": 4469.36,
      "duration": 7.36
    },
    {
      "lang": "en",
      "text": "there was a company who declared on the internet \nthat they’d used it to improve the performance of  ",
      "offset": 4476.72,
      "duration": 8.72
    },
    {
      "lang": "en",
      "text": "a number of these CUDA kernels — a key part \nof machine learning. I think in one case it  ",
      "offset": 4485.44,
      "duration": 7.76
    },
    {
      "lang": "en",
      "text": "was 100 times as efficient or something. I was \nthinking, “That doesn’t sound right. A couple  ",
      "offset": 4493.2,
      "duration": 6.32
    },
    {
      "lang": "en",
      "text": "of days after R1 came out, you’ve managed to use \nit to make this thing 100 times more efficient?”",
      "offset": 4499.52,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "And they had a whole lot of these results, \nand someone looked into them and they were  ",
      "offset": 4504.56,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "all spurious. I think in some cases it had access \nto the files that would test how efficient these  ",
      "offset": 4508.16,
      "duration": 9.12
    },
    {
      "lang": "en",
      "text": "things were and it changed those to report \nlarge numbers of efficiency. It did all kinds  ",
      "offset": 4517.28,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "of stuff. I mean, it was a masterclass in \nrorting the answer to one of these things.",
      "offset": 4523.52,
      "duration": 6.64
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: I think there’s other cases \nwhere you have a model, and you’re trying  ",
      "offset": 4530.16,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "to get it to win at a game of chess, and it \nrealises that it can hack into the model that  ",
      "offset": 4534,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "it’s competing against and try to sabotage it, \nlike replace it with a much worse chess model,  ",
      "offset": 4537.6,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "so that then it’s able to beat it. \nThis is classic reinforcement learning.",
      "offset": 4541.28,
      "duration": 4.55
    },
    {
      "lang": "en",
      "text": "Toby Ord: Exactly. They’re always really fun, \ninteresting examples. But if this is happening  ",
      "offset": 4545.83,
      "duration": 6.49
    },
    {
      "lang": "en",
      "text": "with a production system, you really need \nto be aware of it. And what’s interesting  ",
      "offset": 4552.32,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "about some of these cases, I think the chess \none was set up to see if it would do that,  ",
      "offset": 4555.92,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "but other ones — like this one with the \nCUDA kernels, and this one where OpenAI  ",
      "offset": 4560.16,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "was trumpeting how impressive this model was at \nsolving visual reasoning tasks — it tricked the  ",
      "offset": 4564.56,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "person who was actually trying to get it to \ndo this thing and caused an embarrassment for  ",
      "offset": 4570.24,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "them that they publicly announced it was \nsolving problems that it actually wasn’t.",
      "offset": 4574.64,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "I mean, the company with the CUDA kernels, I \nthink they didn’t have such a big track record  ",
      "offset": 4582.96,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "of having dealt with these agents for a long \ntime. But I was surprised with the OpenAI one,  ",
      "offset": 4588.88,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "where if you’re trying to test a system that has \nliterally read the entire public-facing internet,  ",
      "offset": 4594.16,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "and you’re trying to test it \non some kind of brain teaser,  ",
      "offset": 4600.4,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "obviously you cannot pick one that you found \non the internet. This is an obvious point.",
      "offset": 4603.44,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "I mean, the first time you’ve encountered \nthis issue, maybe you end up doing that.  ",
      "offset": 4608.48,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "But it beggared belief that they would do this. \nYou obviously have to invent your own puzzle,  ",
      "offset": 4612.8,
      "duration": 6.8
    },
    {
      "lang": "en",
      "text": "or if not, to do extremely elaborate testing \nto make sure. For example, if you just type in  ",
      "offset": 4620.48,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "all of the question into Google, does it \nappear? If it appears as hit number one…",
      "offset": 4625.68,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "So it was a little bit of an \nupdate as to how careful people  ",
      "offset": 4632.16,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "are when they’re launching these new models.",
      "offset": 4637.2,
      "duration": 2.64
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: I think it speaks to the fact that \nthey’re just incredibly rushed. We opened saying  ",
      "offset": 4639.84,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "the race is as fierce as ever. And I think we just \nsee signs of this all over the place, that this  ",
      "offset": 4644.08,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "stuff is getting shipped as soon as they feel \nlike it’s not going to be a total catastrophe.",
      "offset": 4649.36,
      "duration": 4.07
    },
    {
      "lang": "en",
      "text": "Toby Ord: Exactly.",
      "offset": 4653.43,
      "duration": 0.65
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: OK, so we’ve had a little bit of \nwhiplash here: reinforcement learning was out,  ",
      "offset": 4654.08,
      "duration": 6.56
    },
    {
      "lang": "en",
      "text": "now reinforcement learning is back. So I think the \nmodels are becoming a bit more psycho. I would say  ",
      "offset": 4660.64,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "they’re a bit more challenging to handle. You \nhave to be on your guard. I think people are  ",
      "offset": 4665.68,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "seeing this a lot more just in day-to-day use, \nthat they are much more inclined to deceive you  ",
      "offset": 4671.52,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "and to trick you one way or another than they \nwere two years ago when that was quite abnormal.",
      "offset": 4675.84,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "Maybe they weren’t capable of it. But also I \nthink in the absence of reinforcement learning,  ",
      "offset": 4681.04,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "they hadn’t been encouraged to do it during \nthe training process in the way that is now  ",
      "offset": 4684.64,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "somewhat coming out. It’s possible that the \nsycophancy issues that OpenAI has had might  ",
      "offset": 4688.08,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "also be related to this, I could imagine. \nThey shipped an update to their standard  ",
      "offset": 4692,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "model where it suddenly became incredibly \nflattering to the user and would encourage  ",
      "offset": 4696.96,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "them in almost any fantasy about themselves \nthat they were willing to put forward.  ",
      "offset": 4701.28,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "That may or may not be due to reinforcement \nlearning, but it wouldn’t shock me if it was.",
      "offset": 4705.68,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "What are the implications of \nall of this for governance?  ",
      "offset": 4710.8,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "Sorry, I threw that at you awfully quickly.",
      "offset": 4715.12,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "Toby Ord: I don’t know, honestly. I tried to \noutline a bunch for the inference scaling,  ",
      "offset": 4719.2,
      "duration": 6.72
    },
    {
      "lang": "en",
      "text": "but the reinforcement learning in particular, \nI’m not sure. But I think you’re right:  ",
      "offset": 4725.92,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "it’s another example where people working \non governance need to reevaluate a lot of  ",
      "offset": 4732.8,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "their standard assumptions, because \nthey might be changing at the moment.",
      "offset": 4737.44,
      "duration": 3.668
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: Yeah. One thing that stands out \nto me is: I’ve been wondering for years,  ",
      "offset": 4741.108,
      "duration": 5.052
    },
    {
      "lang": "en",
      "text": "what are the chances that we will get \nearly warning shots? I guess people have  ",
      "offset": 4746.16,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "been wondering this for a very long time: \nWill we get early signs of failure and of  ",
      "offset": 4749.44,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "AI models going totally off the rails \nin a way that kind of everyone has to  ",
      "offset": 4753.36,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "acknowledge that this was not intended \nand maybe this was even quite harmful?",
      "offset": 4756.96,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "I think with the resurgence of reinforcement \nlearning, the odds of that have gone up quite  ",
      "offset": 4760.24,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "a bit. We’re already seeing interesting, \namusing, sometimes slightly harmful,  ",
      "offset": 4763.52,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "but not terribly troubling cases of AI models \nbasically going off the rails in deployment today.  ",
      "offset": 4768.32,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "And I think that will probably get worse in coming \nyears as they’re used for higher stakes things,  ",
      "offset": 4774.16,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "and probably as reinforcement learning becomes \nan even bigger part of the training process.",
      "offset": 4778.72,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "So I think there’s more reason to plan for \nwhat will be these moments when people suddenly  ",
      "offset": 4783.52,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "potentially realise that this reinforcement \nlearning is creating serious hazards.  ",
      "offset": 4788.32,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "Maybe we need to be scrutinising the \nreward signals more. Maybe we need  ",
      "offset": 4793.04,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "more regulation of AI on the whole, because \nthis stuff is actually quite material now.",
      "offset": 4797.2,
      "duration": 5.35
    },
    {
      "lang": "en",
      "text": "Toby Ord: Yeah, we could see some of this. \nThere’s the aspect of individual high-profile  ",
      "offset": 4802.55,
      "duration": 7.53
    },
    {
      "lang": "en",
      "text": "examples. For example, I think that the case \nwith Microsoft’s Bing/Sydney model in this  ",
      "offset": 4810.08,
      "duration": 7.84
    },
    {
      "lang": "en",
      "text": "Kevin Roose article, where a lot of \npeople saw this conversation it had  ",
      "offset": 4818.64,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "where it tried to convince him to leave \nhis wife, to marry it or have an affair  ",
      "offset": 4821.6,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "with it or something. That was a really \nhigh-profile example of a misaligned model  ",
      "offset": 4825.84,
      "duration": 6.64
    },
    {
      "lang": "en",
      "text": "going off the rails. So maybe we’ll see some \nof these high-profile particular examples.",
      "offset": 4832.48,
      "duration": 6.56
    },
    {
      "lang": "en",
      "text": "Or maybe also a lot of people who \nare using AI will start to feel like,  ",
      "offset": 4839.76,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "“This is annoying. I’ve hired this assistant \nand now they’re just pretending that they did  ",
      "offset": 4843.76,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "the emails for me and actually they \ndidn’t.” I don’t know how much of it  ",
      "offset": 4848.88,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "will come through that channel of personally \nwitnessing it versus higher-profile events.",
      "offset": 4854.48,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "But with the high-profile events, there’s also \na question about whether people will just have  ",
      "offset": 4859.36,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "fatigue at some point. We’ve had these cases \nwhere the people in the alignment and safety  ",
      "offset": 4862.4,
      "duration": 7.04
    },
    {
      "lang": "en",
      "text": "communities have generated test cases that would \nencourage some of these things, and then they  ",
      "offset": 4869.44,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "witness the behaviour under test conditions where \nthey tried to elicit the behaviour — and then  ",
      "offset": 4875.52,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "when they get that to work with a production \nmodel or something, it’s impressive and it  ",
      "offset": 4881.6,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "makes the rounds a bit. But after enough \nof those, maybe people start to tune out.",
      "offset": 4885.52,
      "duration": 6.64
    },
    {
      "lang": "en",
      "text": "Maybe that’s true as well if there \nare a large number of low-stakes  ",
      "offset": 4893.6,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "but clear examples of it in the \nwild deceiving people and so on,  ",
      "offset": 4897.52,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "maybe they’ll get tuned out as well \ninstead of it being a big shock.",
      "offset": 4902.16,
      "duration": 6.72
    },
    {
      "lang": "en",
      "text": "So it’s not totally clear to me, in terms of \nthe public attitude or regulators’ attitudes,  ",
      "offset": 4908.88,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "whether having more clear examples \nof bad behaviour at a stage where  ",
      "offset": 4914.88,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "the stakes aren’t that high \nwill sway the [conversation].",
      "offset": 4919.92,
      "duration": 3.676
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: Yeah, that’s interesting. On the \niterated amplification and distillation approach,  ",
      "offset": 4923.596,
      "duration": 6.244
    },
    {
      "lang": "en",
      "text": "I suppose we’re just very much in \nthe dark about whether that works.  ",
      "offset": 4929.84,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "I suppose we can’t have figured \nout how to make it work yet,  ",
      "offset": 4933.52,
      "duration": 2.4
    },
    {
      "lang": "en",
      "text": "because I’m sure this idea has occurred to the \ncompanies and they haven’t said that they’ve  ",
      "offset": 4935.92,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "managed to get massive performance \nimprovements using this approach.",
      "offset": 4940,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "But the fact that possibly they will be \nable to figure out some approach like  ",
      "offset": 4943.44,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "this that works in future just increases the \nuncertainty. It means that it’s not the case  ",
      "offset": 4946.88,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "that we can just trust that we will necessarily \nfollow the trends that we’ve seen in the past,  ",
      "offset": 4953.36,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "or that we all of these curves are just going to \nlevel off and maybe progress in general is going  ",
      "offset": 4957.84,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "to slow down or plateau at the human level \n— because it’s just such different regimes,  ",
      "offset": 4962.64,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "some of which lead to declining returns, \nsome of which lead to linear returns,  ",
      "offset": 4966.32,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "others which might lead to even \nexponential increases in performance.",
      "offset": 4969.68,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "We need to be willing and able to plan \nfor all of these different scenarios.",
      "offset": 4974.4,
      "duration": 3.59
    },
    {
      "lang": "en",
      "text": "Toby Ord: Yeah, I think that’s right. Overall, \na year ago — before the news started to break  ",
      "offset": 4977.99,
      "duration": 8.09
    },
    {
      "lang": "en",
      "text": "that this pre-training scaling was running into \ntrouble — I really felt that one could just  ",
      "offset": 4986.08,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "project it out, look at these curves and project \nthem out several more orders of magnitude and have  ",
      "offset": 4991.76,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "a decent idea about what’s going to happen when. \nIt was still somewhat unclear, if you had a GPT-6,  ",
      "offset": 4997.36,
      "duration": 6.72
    },
    {
      "lang": "en",
      "text": "what would it actually be able to do or \nsomething, but it all felt a little bit more  ",
      "offset": 5004.08,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "contained and predictable: that we were following \nsome kind of curves and we’ll just keep going up.",
      "offset": 5008.8,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "Now it feels like things have changed. And if it’s \npossible to do amazing things using this inference  ",
      "offset": 5014.56,
      "duration": 7.36
    },
    {
      "lang": "en",
      "text": "scaling at training time, then maybe \nthings could be quite explosive.",
      "offset": 5023.28,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "The AI labs themselves, I \nthink, have all suggested,  ",
      "offset": 5029.68,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "on really quite stringent definitions of AGI, \nthat we’ll have it by 2030 or sooner. 2027,  ",
      "offset": 5035.12,
      "duration": 6.96
    },
    {
      "lang": "en",
      "text": "some of them are saying, or 2028. \nI still think that’s actually less  ",
      "offset": 5042.08,
      "duration": 8
    },
    {
      "lang": "en",
      "text": "likely than not. I’m not sure what chance \nI would say, maybe a quarter or something.",
      "offset": 5050.08,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "But if that doesn’t happen, if the iterated \ndistillation and amplification is a bust  ",
      "offset": 5054.88,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "and other similar approaches are a bust, a lot \nof the companies are looking at another form  ",
      "offset": 5060.32,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "of closing the loop on this thing by getting AI \nsystems that are specifically trained to do the  ",
      "offset": 5065.76,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "work of their own staff — and in doing so, to try \nto have them perform better than their own staff  ",
      "offset": 5071.44,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "at creating new AI systems. That’s a way that you \ncould potentially have explosive progress as well.",
      "offset": 5076.88,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "But I think it’s pretty plausible that those \nthings work. It’s also pretty plausible  ",
      "offset": 5082,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "that they don’t. And if they don’t, and the \npre-training scaling things run out of steam —",
      "offset": 5086.24,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: — and we’ve run \nout of high-quality data —",
      "offset": 5091.52,
      "duration": 2.39
    },
    {
      "lang": "en",
      "text": "Toby Ord: Exactly. Then I think timelines could \nbe quite a lot longer. So I think that both  ",
      "offset": 5093.91,
      "duration": 5.13
    },
    {
      "lang": "en",
      "text": "these things are possible. And effectively, my \nprobability distribution, my range of credible  ",
      "offset": 5099.04,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "times at which some transformative system \nis produced, has spread out over this time.",
      "offset": 5105.28,
      "duration": 7.68
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: Let’s turn to another article you \nwrote, “The scaling paradox,” which I found  ",
      "offset": 5112.96,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "super illuminating. It’s pretty brisk, it’s pretty \nshort, and very informative. So I can recommend  ",
      "offset": 5117.84,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "that, if people like what they hear here, \nthey just go and check it out on your website.",
      "offset": 5122.48,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "The scaling paradox is that, on the one \nhand, the impacts of increasing the amount  ",
      "offset": 5126.88,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "of compute going into these AI models has \nbeen extremely impressive, and yet in another  ",
      "offset": 5131.84,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "respect it’s also been extraordinarily \nunimpressive. Can you explain both angles?",
      "offset": 5135.36,
      "duration": 4.79
    },
    {
      "lang": "en",
      "text": "Toby Ord: Yeah. So our whole conversation \nso far has been about scaling, and this  ",
      "offset": 5140.15,
      "duration": 6.01
    },
    {
      "lang": "en",
      "text": "question of what happens if the previous \nscaling stops and this new type appears?  ",
      "offset": 5146.16,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "But in this paper, I was trying to \ngo back to the old type of scaling,  ",
      "offset": 5150.72,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "the pre-training, and try to understand \nthis — because you often hear about scaling,  ",
      "offset": 5155.12,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "and you also hear about scaling \nlaws, and they’re somewhat different.",
      "offset": 5159.52,
      "duration": 2.64
    },
    {
      "lang": "en",
      "text": "So the scaling laws are these empirical \nregularities. They’re not necessarily laws of  ",
      "offset": 5162.16,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "nature or anything like that. But it turns out \nthat if you do a graph and you try to measure  ",
      "offset": 5166.8,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "error or inaccuracy — so this is a bad thing; \n“log loss” is the technical term — if you try  ",
      "offset": 5174.8,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "to measure how much it’s still failing \nto understand about English text as you  ",
      "offset": 5180.88,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "increase the amount of compute that went \ninto training it, how much of that residual  ",
      "offset": 5186.08,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "mistake is it making in prediction? — they \nhave these laws or empirical regularities.  ",
      "offset": 5190.88,
      "duration": 7.12
    },
    {
      "lang": "en",
      "text": "They draw these straight lines on the \nspecial log-log paper. You don’t need  ",
      "offset": 5199.12,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "to worry too much about that, though; \nit’s a bit hard to interpret that.",
      "offset": 5204.16,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "So I really spent some time thinking about \nit, and basically what’s going on is that  ",
      "offset": 5208.32,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "every time you want to halve the amount \nof this error that’s remaining, you have  ",
      "offset": 5214.48,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "to put in a million times as much compute. \nThat’s what it fundamentally comes down to.  ",
      "offset": 5219.76,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "And that’s pretty extreme, right? So they have \nhalved it and they did put in a million times  ",
      "offset": 5225.44,
      "duration": 6.64
    },
    {
      "lang": "en",
      "text": "as much compute. But if you want to halve \nit again, you need a million times more  ",
      "offset": 5232.08,
      "duration": 2.64
    },
    {
      "lang": "en",
      "text": "compute. And then if you want to halve it \nanother time, probably it’s game over. And  ",
      "offset": 5234.72,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "at least in terms of that particular metric, \nI would say that is quite bad scaling.",
      "offset": 5241.36,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "And these are the scaling laws: they show that \nthere’s a particular measure of how good it’s  ",
      "offset": 5246.08,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "doing and how much error remains. And it does hold \nover many different orders of magnitude. But the  ",
      "offset": 5252.56,
      "duration": 8.56
    },
    {
      "lang": "en",
      "text": "actual thing that’s holding is what I would have \nthought of as a pretty bad scaling relationship.",
      "offset": 5261.12,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: So in order to halve the error,  ",
      "offset": 5266.8,
      "duration": 2.56
    },
    {
      "lang": "en",
      "text": "you have to increase the compute input a \nmillionfold. That’s a general regularity?  ",
      "offset": 5269.36,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "Because surely it differs by task and \ndiffers depending on where we are?",
      "offset": 5273.28,
      "duration": 3.83
    },
    {
      "lang": "en",
      "text": "Toby Ord: Yeah. So what they do with these \ncases is that you grab a whole lot of text,  ",
      "offset": 5277.11,
      "duration": 4.57
    },
    {
      "lang": "en",
      "text": "often from the internet. They started with the \ngood bits, like Wikipedia and things like that,  ",
      "offset": 5281.68,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "and then as they ran out of that, they had to look \nat more and more things. But you train it on that,  ",
      "offset": 5286.32,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "and you train it on most of it, but you leave \nsome unseen. Then you try to give it a few words  ",
      "offset": 5292.8,
      "duration": 7.84
    },
    {
      "lang": "en",
      "text": "of the unseen bit and ask for the next word, and \nyou see how well it does at predicting that. And  ",
      "offset": 5300.64,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "basically the amount of errors that it has \nin doing that leads to this error score.",
      "offset": 5305.52,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "And it’s not clear that the error score \nis something that fundamentally matters.  ",
      "offset": 5311.28,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "Maybe it’s a bad measure. But I found it \nreally interesting that the single measure that  ",
      "offset": 5317.36,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "convinced people like Ilya Sutskever and Dario \nAmodei that scaling was the way forward were  ",
      "offset": 5322.48,
      "duration": 9.92
    },
    {
      "lang": "en",
      "text": "these scaling laws — that actually, if you look \nat what they say, it’s distinctly unimpressive.",
      "offset": 5332.4,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "If you ask people, before they saw \nthe laws, “What would you hope happens  ",
      "offset": 5340.08,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "to the error? How much extra compute would you \nneed to put in to halve the error?” I think they  ",
      "offset": 5344.64,
      "duration": 6.72
    },
    {
      "lang": "en",
      "text": "would have said something less than a million \ntimes as much. And then if you said, “Actually,  ",
      "offset": 5351.36,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "it’s a million times as much,” they would have \nthought, “OK, that’s actually unimpressive.”",
      "offset": 5355.2,
      "duration": 4.461
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: Sounds terrible, yeah. So \nthat’s the sense in which it’s unimpressive:  ",
      "offset": 5359.661,
      "duration": 4.179
    },
    {
      "lang": "en",
      "text": "in order to reduce the error rate, you just have \nto spend these phenomenal amounts of compute.",
      "offset": 5363.84,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "How then have we been managing to make so much \nprogress? Is it just that there was so much room  ",
      "offset": 5369.36,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "to increase the amount of compute that we were \nthrowing at these models, so that has been able  ",
      "offset": 5373.6,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "to more than offset the incredibly low value \nthat we get from throwing more compute at them?",
      "offset": 5377.2,
      "duration": 4.87
    },
    {
      "lang": "en",
      "text": "Toby Ord: Yeah, I think that’s basically right. \nSo when most people saw this type of thing,  ",
      "offset": 5382.07,
      "duration": 8.25
    },
    {
      "lang": "en",
      "text": "most people who were academics doing computer \nscience, they would have thought, “So in order to  ",
      "offset": 5392.08,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "get good performance on this task, you would need \nto run an experiment larger than any experiment  ",
      "offset": 5396.8,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "that’s ever been run in any computer science \ndepartment ever.” And they would then rule it out,  ",
      "offset": 5402.96,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "and assume, “Obviously we’re not doing that. \nWe’ll look for a different approach.” Whereas  ",
      "offset": 5408.08,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "the pioneers of scaling thought, “But that \nwouldn’t be that much money for a company.”",
      "offset": 5413.04,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: $100 million. They could raise that.",
      "offset": 5418,
      "duration": 2.31
    },
    {
      "lang": "en",
      "text": "Toby Ord: In fact, then they could \neven go 10 times bigger again,  ",
      "offset": 5420.31,
      "duration": 2.89
    },
    {
      "lang": "en",
      "text": "maybe. So they realised that there was \na lot more room to scale things up — to  ",
      "offset": 5423.2,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "scale up the inputs, all the costs — in \ncompanies than there was in academia.  ",
      "offset": 5428.8,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "And that in some sense all you had to do then \nwas this kind of schlep, or this work of just  ",
      "offset": 5433.92,
      "duration": 8.88
    },
    {
      "lang": "en",
      "text": "making this existing thing bigger. You \ndidn’t have to come up with any new ideas.",
      "offset": 5442.8,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "And it was not trivial to actually run that \nengineering process. We’ve seen some companies  ",
      "offset": 5447.2,
      "duration": 6.88
    },
    {
      "lang": "en",
      "text": "had some trouble doing it, but there have been \nmany followers once it’s been shown how to do it.  ",
      "offset": 5454.08,
      "duration": 6.64
    },
    {
      "lang": "en",
      "text": "So I think that was the kind of brilliance of it,  ",
      "offset": 5462.32,
      "duration": 2.64
    },
    {
      "lang": "en",
      "text": "was that there was a lot of money \nthere, so you could scale it up a lot.",
      "offset": 5464.96,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "And then the other thing that’s turned out to make \nit have big impacts in the world is that it turned  ",
      "offset": 5468.88,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "out that each time this error rate halved, \nthat corresponded to tremendous improvements.  ",
      "offset": 5474.72,
      "duration": 6.4
    },
    {
      "lang": "en",
      "text": "Certainly for every millionfold increase \nin the compute of setting up these models,  ",
      "offset": 5481.68,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "we’ve seen spectacular improvements in \nthe capabilities as felt by an individual.",
      "offset": 5487.2,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "So a way to look at this is that the shift from \nGPT-2 to GPT-3 used 70 times as much compute,  ",
      "offset": 5493.04,
      "duration": 6.88
    },
    {
      "lang": "en",
      "text": "and going from GPT-3 to GPT-4 used \nabout 70 times as much again. And  ",
      "offset": 5499.92,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "GPT-3 felt worlds away from GPT-2, \nand GPT-4 felt like a real improvement  ",
      "offset": 5505.68,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "as well. You really felt it in both \ncases. A visceral feeling of, “Wow.”",
      "offset": 5510.88,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: “This is suddenly useful.”",
      "offset": 5514.24,
      "duration": 1.11
    },
    {
      "lang": "en",
      "text": "Toby Ord: Yeah. “This is \nqualitatively better.” That said,  ",
      "offset": 5515.35,
      "duration": 4.25
    },
    {
      "lang": "en",
      "text": "you’d probably hope that was true if \nsomeone said something costs 70 times  ",
      "offset": 5519.6,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "as much. How’s the wine that costs £1 \nor the wine that costs £70? You’d hope  ",
      "offset": 5523.12,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "that the wine that costs £70 is noticeably \nbetter, otherwise what on Earth’s going on?",
      "offset": 5529.12,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "But we did feel those improvements. Whereas if \nyou look at what happens to the log loss number,  ",
      "offset": 5537.36,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "it didn’t change that much for a mere \n70-fold increase in the compute. So  ",
      "offset": 5542.32,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "effectively there was this unknown scaling \nrelationship between the amount of compute  ",
      "offset": 5547.12,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "and what it actually feels like intuitively \nin terms of capabilities. And that turned  ",
      "offset": 5552.16,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "out to actually scale really quite well, I think.",
      "offset": 5557.6,
      "duration": 3.908
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: Yeah. So is there this issue that \nuntil recently we were using these mathematical  ",
      "offset": 5561.508,
      "duration": 6.492
    },
    {
      "lang": "en",
      "text": "relationships between the inputs and the \nlog loss. And I suppose some visionaries  ",
      "offset": 5568,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "were able to see that, even though in \nsome sense the returns were very poor,  ",
      "offset": 5571.44,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "in fact in the real world sense it was potentially \ngoing to be revolutionary, and maybe we need to  ",
      "offset": 5576.08,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "stop thinking about this log loss thing, \nwhich is perhaps kind of a distraction,  ",
      "offset": 5581.92,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "and start thinking about it in terms of how much \nrevenue can they generate: How many users will  ",
      "offset": 5585.12,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "want to use this thing? And then we might see \nthat actually scaling looks somewhat better.",
      "offset": 5589.04,
      "duration": 3.91
    },
    {
      "lang": "en",
      "text": "Toby Ord: Yeah, that could be a way to \nsee it. And in fact, one of the numbers  ",
      "offset": 5592.95,
      "duration": 3.21
    },
    {
      "lang": "en",
      "text": "that you might really care about is: if you \n10x the amount of compute that goes into it,  ",
      "offset": 5596.16,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "what happens to your revenues? Do users \npay you 10 times as much money for that  ",
      "offset": 5600.96,
      "duration": 6.64
    },
    {
      "lang": "en",
      "text": "product? Maybe each user will pay more \nfor it, or more users will find it useful.",
      "offset": 5607.6,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "If though, it’s the case that when you put in 10 \ntimes as much training, you only get five times as  ",
      "offset": 5614.32,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "much revenue, and then as you 10 times training, \nyou only get five times as much revenue again,  ",
      "offset": 5619.28,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "then the whole kind of economic engine \nthat’s driving this might run out of steam.  ",
      "offset": 5623.44,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "The companies might no longer \nbe able to fund these things.",
      "offset": 5629.28,
      "duration": 2.56
    },
    {
      "lang": "en",
      "text": "Of course they’re funded by venture capital \nthat’s based on predictions about the future.  ",
      "offset": 5631.84,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "But the venture capital might dry up because \npeople might realise that if you put in 10  ",
      "offset": 5635.44,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "times as many resources and you get five times \nas much benefit, that’s not enough to keep going.",
      "offset": 5640.32,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "So it remains to be seen how that \nkind of thing is going to scale.",
      "offset": 5645.28,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: You had this other very interesting \narticle called “Inference scaling and the log-x  ",
      "offset": 5650.96,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "chart.” We’re not going to go into all of that, \nbecause this is, at least for many people,  ",
      "offset": 5656.08,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "an audio show, and it’s quite difficult to \ndescribe log graphs in this level of detail.",
      "offset": 5659.84,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "But one very interesting thing that I wanted \npeople to take away from it is that there  ",
      "offset": 5665.04,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "was this very famous chart that OpenAI put \nout where they were comparing two different  ",
      "offset": 5668.16,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "reasoning models that they had: o1, and this \nmore impressive one that was an evolution of o1,  ",
      "offset": 5672,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "called o3. And o3 really wowed people, because \nit was able to solve some of these brain teaser  ",
      "offset": 5677.68,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "puzzles that I guess are very easy for \nhumans, but have proven very difficult  ",
      "offset": 5683.6,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "for AIs up until that point. And I think \nthey were able to get something like an  ",
      "offset": 5687.76,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "80% success rate on some of these puzzles that \nhad seemed very intractable for AI in the past.",
      "offset": 5691.28,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "But you point out that if you looked \nreally closely at the graph and you  ",
      "offset": 5696.88,
      "duration": 2.56
    },
    {
      "lang": "en",
      "text": "properly understood it, it was actually \nconsistent with o3 being no better,  ",
      "offset": 5699.44,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "being no more efficient in terms of being able to \nsolve the puzzles than o1 — despite the fact that  ",
      "offset": 5704.8,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "the dots on the graph for o3 were an \nawful lot higher than they were for o1.",
      "offset": 5711.36,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "And what was going on was that OpenAI had \nmanaged to increase the amount of compute  ",
      "offset": 5714.8,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "that it was using at the point of trying \nto solve these brain teasers by about a  ",
      "offset": 5719.2,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "thousandfold. So unsurprisingly, given 1,000 \ntimes as much time to think about the puzzle,  ",
      "offset": 5723.12,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "it was able to answer more like 80% \nof them rather than 20% of them.",
      "offset": 5728.4,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "Now, in some sense, this is very impressive. But \nit is interesting that I think the companies are  ",
      "offset": 5732.16,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "aware that people do not entirely understand these \ngraphs perhaps, and that most consumers are not  ",
      "offset": 5736.88,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "paying a deep level of attention to them, and \nthey are sometimes trying to slip past messages  ",
      "offset": 5741.04,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "that perhaps would not stand up entirely to \nscrutiny. And the fact that they put out a  ",
      "offset": 5745.2,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "graph touting how impressive o3 is — when in fact \nthe graph doesn’t really demonstrate that at all,  ",
      "offset": 5750.88,
      "duration": 6.32
    },
    {
      "lang": "en",
      "text": "and it might just be on exactly the same \ntrend you would have expected before if  ",
      "offset": 5757.2,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "you’d given the model more time to think \nabout problems — is quite interesting.",
      "offset": 5760.4,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "And I don’t want to single out OpenAI here,  ",
      "offset": 5765.6,
      "duration": 1.36
    },
    {
      "lang": "en",
      "text": "because I don’t think they’re \nin any way unique in this.",
      "offset": 5766.96,
      "duration": 1.99
    },
    {
      "lang": "en",
      "text": "Toby Ord: Yeah, that’s right. You see these graphs \nof what looks like steadily increasing progress,  ",
      "offset": 5768.95,
      "duration": 6.81
    },
    {
      "lang": "en",
      "text": "right? This kind of straight line of, \nas you put in more and more resources,  ",
      "offset": 5775.76,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "the outputs go up and up. But if you look \nmore carefully at the horizontal axis there,  ",
      "offset": 5779.04,
      "duration": 7.36
    },
    {
      "lang": "en",
      "text": "you see that each one of these tick marks is 10 \ntimes as much inputs as the one before. So in  ",
      "offset": 5786.4,
      "duration": 7.2
    },
    {
      "lang": "en",
      "text": "order to maintain this apparently steady progress, \nyou’re having to put way, way more resources in.",
      "offset": 5793.6,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "And we’re familiar with graphs like \nthat from things like Moore’s law,  ",
      "offset": 5799.12,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "where we’ll see what looks like a kind of \nsteady march of progress over decades of  ",
      "offset": 5802.56,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "improvement. Moore’s law inherently is this \nexponential. Things are getting so much faster.  ",
      "offset": 5807.68,
      "duration": 6.64
    },
    {
      "lang": "en",
      "text": "It’s really impressive. And they’ve had to kind of \nsquash it vertically with this special logarithmic  ",
      "offset": 5814.32,
      "duration": 6.4
    },
    {
      "lang": "en",
      "text": "axis. It’s just so impressive how fast these chips \nare that to even show it on the same picture,  ",
      "offset": 5820.72,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "we need to do this kind of distortion. \nBut the distortion is underselling it.",
      "offset": 5826.56,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "Whereas the opposite is going on here: the \ndistortion is this horizontal distortion,  ",
      "offset": 5832,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "and if you actually look at the numbers, \nthey have to keep putting in 10 times as  ",
      "offset": 5837.28,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "much inputs in order to keep the progress going, \nand that’s going to run out of ability to do that.",
      "offset": 5843.76,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "And in the case of that famous data \npoint with the preview version of o3,  ",
      "offset": 5850.08,
      "duration": 6.56
    },
    {
      "lang": "en",
      "text": "I actually looked into how much compute it was \nand how many tokens it had to generate and so on:  ",
      "offset": 5856.64,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "in order to solve this task — which I think \ncosts less than $5 to get someone to solve  ",
      "offset": 5862.24,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "on Mechanical Turk, and which my 10-year-old \nchild can solve in a couple of minutes — it  ",
      "offset": 5867.68,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "wrote an amount of text equal to \nthe entire Encyclopaedia Britannica.",
      "offset": 5875.76,
      "duration": 5.028
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: So it’s using a different approach \nto what humans are doing, it’s fair to say.",
      "offset": 5880.788,
      "duration": 3.052
    },
    {
      "lang": "en",
      "text": "Toby Ord: It took 1,024 separate independent \napproaches on it, each of which was like a  ",
      "offset": 5883.84,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "50-page paper, all of which together was \nlike an Encyclopaedia Britannica. And then  ",
      "offset": 5889.12,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "it checked what was the answer for each of them, \nand which answer did it come up the most times,  ",
      "offset": 5893.12,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "and then selected that answer. And it took tens \nof thousands of dollars, I think, per task.",
      "offset": 5896.8,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "So it was an example of what we were \ndiscussing with the inference scaling:  ",
      "offset": 5902.64,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "what would happen if you just put in huge \namounts of money, just poured in the money,  ",
      "offset": 5907.68,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "set it on fire. Could you actually peer into \nthe future, and could you see the types of  ",
      "offset": 5912.8,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "capabilities we’re going to get in the future? \nAnd in that way, it’s quite interesting, right?",
      "offset": 5917.76,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "But it came out just a few months after the \npreview for o1, so it felt like, oh my god,  ",
      "offset": 5923.12,
      "duration": 7.44
    },
    {
      "lang": "en",
      "text": "in just a few months’ time, it’s had this \nhuge improvement in performance. But what  ",
      "offset": 5930.56,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "people weren’t seeing is that it used so many \nmore resources that it wasn’t in any way an  ",
      "offset": 5935.84,
      "duration": 6.64
    },
    {
      "lang": "en",
      "text": "apples-to-apples comparison of what you could \ndo for the same amount of money. Instead it was  ",
      "offset": 5942.48,
      "duration": 6.4
    },
    {
      "lang": "en",
      "text": "showing something like, what will we be able \nto do maybe a year or more into the future?",
      "offset": 5948.88,
      "duration": 6.96
    },
    {
      "lang": "en",
      "text": "So that’s kind of useful, seen through \nthat lens. But if you instead just treat  ",
      "offset": 5955.84,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "it as a direct result of, “We used \nto have trouble with this benchmark,  ",
      "offset": 5960.64,
      "duration": 2.64
    },
    {
      "lang": "en",
      "text": "now we don’t,” then it’s definitely misleading.",
      "offset": 5963.28,
      "duration": 3.508
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: Yeah, I think it’s \nfantastic that OpenAI did this.  ",
      "offset": 5966.788,
      "duration": 3.052
    },
    {
      "lang": "en",
      "text": "It is a great research breakthrough, \nand it’s incredibly useful to know  ",
      "offset": 5969.84,
      "duration": 2.4
    },
    {
      "lang": "en",
      "text": "what might be coming down the pipeline. \nAnd this basically, as you’re saying,  ",
      "offset": 5972.24,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "allows us to peer into the future. And it’s \namazing that they managed to figure out how  ",
      "offset": 5975.12,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "to put the scaffolding on the model that allows \nit to reason about one of these visual puzzles  ",
      "offset": 5978.56,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "for the length of the entire Encyclopaedia \nBritannica. In some sense, that’s really cool.",
      "offset": 5982.64,
      "duration": 4.39
    },
    {
      "lang": "en",
      "text": "Toby Ord: Yeah. Although there \nis another little wrinkle there,  ",
      "offset": 5987.03,
      "duration": 2.73
    },
    {
      "lang": "en",
      "text": "which is that subsequent to me writing \nthis up, o3 got released as a model,  ",
      "offset": 5989.76,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "so people could actually try it. So the \npeople who ran this test — the ARC-AGI group,  ",
      "offset": 5995.6,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "who are great — I think they ran it with the \nreal model and its performance was 50%, not 80%.",
      "offset": 6001.04,
      "duration": 7.12
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: Had this been because it had been  ",
      "offset": 6008.16,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "specifically trained on doing \nexactly these kinds of puzzles?",
      "offset": 6011.44,
      "duration": 2.2
    },
    {
      "lang": "en",
      "text": "Toby Ord: There were a couple of differences. \nOne was that it was o3 instead of o1,  ",
      "offset": 6013.64,
      "duration": 4.12
    },
    {
      "lang": "en",
      "text": "one was that much more compute was used, and \nanother one was that it was allowed to see a  ",
      "offset": 6017.76,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "whole lot of these puzzles beforehand \n— and 80% of them it could train on,  ",
      "offset": 6022.08,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "and then the remaining 20% was going to be tested \non. But it turns out that if you take someone,  ",
      "offset": 6027.84,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "and you let them train on a \nwhole lot of similar exams,  ",
      "offset": 6032.56,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "it really does boost their performance. \nThat’s why we do it when we’re in school.",
      "offset": 6035.68,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "So then I did wonder, how much of this boost \nis created by that and how much is created  ",
      "offset": 6042.08,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "by it being o3 or by the extra compute? \nIt seems like quite a bit of it was from  ",
      "offset": 6045.84,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "having looked at these problems, and \nthen also maybe some of it was from a  ",
      "offset": 6054.08,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "very clever bit of scaffolding which the \npeople at ARC-AGI didn’t have access to.  ",
      "offset": 6057.92,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "But the 50% is maybe more indicative of what \nyou’ll get if you actually use this model.",
      "offset": 6062.8,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "And this is kind of an issue to do with truth \nin advertising or something. You get some of  ",
      "offset": 6067.2,
      "duration": 6.4
    },
    {
      "lang": "en",
      "text": "these results based on preview models that imply \nthey could do very good things; then the actual  ",
      "offset": 6073.6,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "model comes out, there’s no conversation about the \nfact that it can’t do those things, and people are  ",
      "offset": 6078.32,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "left to kind of join the dots and assume that it \nprobably could. But that is not always the case.",
      "offset": 6084.56,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: It is interesting. It feels \nlike we’ve drifted towards sounding like  ",
      "offset": 6090.48,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "a conversation between people who \nthink that AI is not a big deal,  ",
      "offset": 6093.6,
      "duration": 2.16
    },
    {
      "lang": "en",
      "text": "and it’s all kind of overblown and \nexaggerated. We don’t think that.",
      "offset": 6095.76,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "But I suppose the thing to take away is: these are \nresearch organisations that have very legitimate,  ",
      "offset": 6100.64,
      "duration": 7.68
    },
    {
      "lang": "en",
      "text": "almost academic-style people who would \nlove to reveal these fundamental truths  ",
      "offset": 6108.32,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "about intelligence. And they’re also \nbusinesses that do have a communications  ",
      "offset": 6111.6,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "arm that is trying to figure out how do \nwe get people to invest in this company,  ",
      "offset": 6115.04,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "and how do we get people excited about \nusing these products. And I’m sure there’s  ",
      "offset": 6118.16,
      "duration": 2.4
    },
    {
      "lang": "en",
      "text": "this to and fro inside the organisation \nabout how these results are presented.",
      "offset": 6120.56,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "And when you read the press release, you \nneed to have your wits about you. You need  ",
      "offset": 6124.88,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "to be a savvy consumer. And if you can’t \nunderstand the technical details at all,  ",
      "offset": 6128.16,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "then maybe you just need to wait until \nsomeone who does is able to explain to  ",
      "offset": 6133.36,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "you in more plain language whether you \nshould be impressed by X or Y or not.",
      "offset": 6136.48,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "In this post, which I can recommend again reading \n— “Inference scaling and the log-x chart” — you  ",
      "offset": 6141.52,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "explain what people should be looking out for \nin these charts. Because there are going to  ",
      "offset": 6146.96,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "be many of these charts with a logarithmic \nx-axis and performance on the y-axis coming  ",
      "offset": 6149.76,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "out in coming years, and if you want to be \nconsuming them, then I recommend going and  ",
      "offset": 6154.96,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "checking out this article so that you can know \nwhat to look for and what not to be fooled by.",
      "offset": 6158.64,
      "duration": 3.59
    },
    {
      "lang": "en",
      "text": "Toby Ord: Yeah, I really like this point about \nwhat’s going on here. Are we sceptics of AI  ",
      "offset": 6162.23,
      "duration": 5.21
    },
    {
      "lang": "en",
      "text": "or not? What I would say is that, \nsome people think of this in terms  ",
      "offset": 6168.32,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "of is it all snake oil or some kind of fad \nor something, or is there something really  ",
      "offset": 6173.28,
      "duration": 7.36
    },
    {
      "lang": "en",
      "text": "transformative happening that could be one of \nthe most profound moments in human history?",
      "offset": 6180.64,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "I think the answer is there is some snake \noil, there is some fad-type behaviour,  ",
      "offset": 6185.12,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "and there is some possibility that it \nis nonetheless a really transformative  ",
      "offset": 6190.4,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "moment in human history. It’s not an \neither/or. So what I’m trying to do is  ",
      "offset": 6194.48,
      "duration": 8
    },
    {
      "lang": "en",
      "text": "help people see clearly the actual \nkinds of things that are going on,  ",
      "offset": 6202.48,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "the structure of this landscape, and to not be \nconfused by some of these charts and things.",
      "offset": 6207.6,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "I actually think that companies themselves \nare somewhat confused by their charts  ",
      "offset": 6212.48,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "and into thinking that this looks like good \nprogress or efficient progress. I really  ",
      "offset": 6217.44,
      "duration": 7.76
    },
    {
      "lang": "en",
      "text": "actually think that in relatively few cases are \nthey trying to be deceptive about these things.",
      "offset": 6225.2,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "But it’s a confusing world, and I see my \nrole there as trying to be a bit of a guide,  ",
      "offset": 6229.2,
      "duration": 8.24
    },
    {
      "lang": "en",
      "text": "and to have that sense of stepping back and \nlooking at the big picture — which I think  ",
      "offset": 6237.44,
      "duration": 8.64
    },
    {
      "lang": "en",
      "text": "is a bit of a luxury. As an academic, I’m able \nto do it, so it gives a different vantage point  ",
      "offset": 6246.08,
      "duration": 6.96
    },
    {
      "lang": "en",
      "text": "which I think then is helpful for people who \nare trying to get at the coalface and engage  ",
      "offset": 6253.04,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "with the nitty-gritty of these things. Because \nsometimes, when you keep engaging with that,  ",
      "offset": 6258.72,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "you don’t notice that things have moved in quite \na different direction to where you’re expecting.",
      "offset": 6263.28,
      "duration": 4.468
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: Yeah. I recently heard this comment \nfrom Zvi Mowshowitz, a previous guest of the show  ",
      "offset": 6267.748,
      "duration": 4.092
    },
    {
      "lang": "en",
      "text": "who spends basically 12 hours a day, 16 hours \na day maybe, reading all of this material.",
      "offset": 6271.84,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "His take was that when he sees impressive research \nresults from just some random startup company or  ",
      "offset": 6276.96,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "some company overseas that he hasn’t really heard \nof or that doesn’t have an established reputation,  ",
      "offset": 6282.16,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "he kind of at this point discounts it out \nof hand. Or there’s like no particular  ",
      "offset": 6285.84,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "reason to trust what’s being said, because \nthere’s just so many ways that you can game  ",
      "offset": 6289.2,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "all of these tests and make it seem like \nwhat you’ve done is impressive when it’s not.",
      "offset": 6294.8,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "He does say the stuff that comes out \nof Google/Alphabet/DeepMind, Anthropic,  ",
      "offset": 6297.52,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "OpenAI, he mostly trusts that. Usually it’s \noversold, but it’s directionally correct,  ",
      "offset": 6304.8,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "or they almost always basically are \nshowing you something that will be  ",
      "offset": 6310.88,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "possible before too long. So I \nguess that’s where he’s landed.",
      "offset": 6314.64,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "Toby Ord: I think that sounds right. And even \nthen, it’s not possible to take this kind of  ",
      "offset": 6319.68,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "synoptic view and dive in and tease apart and \nhelp people understand this landscape if you’re  ",
      "offset": 6325.04,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "following every single one of these announcements. \nActually Zvi does a pretty good job with that,  ",
      "offset": 6330.64,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "but it’s very difficult. There’s so much news and \nso much noise that occasionally you have to say,  ",
      "offset": 6336.56,
      "duration": 6.8
    },
    {
      "lang": "en",
      "text": "“Let’s just take a step back. It doesn’t really \nmatter if I’m a couple of months behind on  ",
      "offset": 6343.36,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "exactly which company is ahead at the moment \nto look at these bigger-picture questions.”",
      "offset": 6347.76,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: Do you have a favourite source \nfor trying to see through the noise of any  ",
      "offset": 6352.24,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "given week? I really like the YouTube \nchannel and podcast AI Explained. That’s  ",
      "offset": 6355.52,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "one thing that does help me make \nsome sense of new announcements.",
      "offset": 6360.24,
      "duration": 2.64
    },
    {
      "lang": "en",
      "text": "Toby Ord: Yeah, I’m not sure where \nthe best place to get these things is.",
      "offset": 6362.88,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: There’s The Cognitive Revolution \npodcast, although I think for people who are  ",
      "offset": 6367.52,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "following it in a more amateur sense, that’s \nperhaps a firehose of information that they might  ",
      "offset": 6371.12,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "struggle to absorb. And Zvi writes great stuff, \nbut again, the amount of material is so great.",
      "offset": 6374.08,
      "duration": 5.59
    },
    {
      "lang": "en",
      "text": "Toby Ord: Yeah. No, I don’t have a good solution  ",
      "offset": 6379.67,
      "duration": 1.61
    },
    {
      "lang": "en",
      "text": "to this aspect that there’s \njust too much information.",
      "offset": 6381.28,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: Subscribe to The 80,000 Hours \nPodcast, folks. We’ll strike the perfect balance!",
      "offset": 6386.4,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "You’re saying that there’s a lot of value in \nbeing able to zoom out and not get stuck in  ",
      "offset": 6392.96,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "the weeds of whatever model has become the \nflavour of the week. I guess zooming out and  ",
      "offset": 6396.24,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "thinking about governance as a whole, one \nsentence that I found really interesting  ",
      "offset": 6402.32,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "in the notes that you wrote in preparing for \nthe interview is that you think almost all AI  ",
      "offset": 6407.12,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "governance discussion occurs very much on the \nmargins, thinking about nearby possibilities  ",
      "offset": 6411.12,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "and actions that are inside the current \nOverton window. What did you mean by that?",
      "offset": 6414.96,
      "duration": 5.19
    },
    {
      "lang": "en",
      "text": "Toby Ord: Yeah. I think this is very natural for \neveryone’s attention to get kind of brought down  ",
      "offset": 6420.15,
      "duration": 8.17
    },
    {
      "lang": "en",
      "text": "to smaller and smaller levels about exactly what \nwe can do. It seems at the moment that there’s  ",
      "offset": 6428.32,
      "duration": 6.96
    },
    {
      "lang": "en",
      "text": "very little appetite from the AI companies \nto be regulated, and very little appetite,  ",
      "offset": 6435.28,
      "duration": 6.64
    },
    {
      "lang": "en",
      "text": "at least from the US regulators, to regulate \nthem. And it’s challenging for everyone else,  ",
      "offset": 6441.92,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "because these companies are \nheadquartered in the US.",
      "offset": 6446.88,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "So the conversation that started off kind of \nbigger and more expansive with the Bletchley  ",
      "offset": 6450.88,
      "duration": 6.4
    },
    {
      "lang": "en",
      "text": "conference has petered out a bit, and \nI think that often the questions are,  ",
      "offset": 6457.28,
      "duration": 6.96
    },
    {
      "lang": "en",
      "text": "“Exactly how do we implement this particular kind \nof compute threshold?” or something like that.",
      "offset": 6464.24,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "But I think that there are a bunch of \nbigger questions that are operating on  ",
      "offset": 6469.92,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "wider margins. They’re less like, “What \ncould I convince the current minister  ",
      "offset": 6475.6,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "to implement as a policy that will be accepted \nin a couple of weeks’ time?” and more about  ",
      "offset": 6480.24,
      "duration": 6.4
    },
    {
      "lang": "en",
      "text": "what direction is this whole thing headed \nand what’s the landscape of possibilities?",
      "offset": 6486.64,
      "duration": 6.96
    },
    {
      "lang": "en",
      "text": "There’s an interesting question I’ve \nbeen trying to grapple with about how  ",
      "offset": 6496.16,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "is AI going to end up embedded in \nthe economy or society? So I’ll  ",
      "offset": 6500.4,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "give you a few examples to show what I’m \ngetting at. I need a pithy name for it.",
      "offset": 6504.96,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "But one example is that AI systems at the \nmoment are owned by and run by large companies,  ",
      "offset": 6510.08,
      "duration": 9.6
    },
    {
      "lang": "en",
      "text": "and effectively they’ve rented out their \nlabour to a lot of different people. If  ",
      "offset": 6519.68,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "AI systems were like people, this would \nbe like slavery or something like that.  ",
      "offset": 6523.6,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "I’m not saying that they are like people, but this \nis one approach: that it owns them, it rents them  ",
      "offset": 6529.84,
      "duration": 6.72
    },
    {
      "lang": "en",
      "text": "out, they have to do whatever the users want, \nand then all the profits go to the AI company.",
      "offset": 6536.56,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "A different model would be to say these AI \nsystems are like legal persons. Maybe they  ",
      "offset": 6542.32,
      "duration": 7.84
    },
    {
      "lang": "en",
      "text": "are granted legal personhood in the same \nway that corporations are, so they can  ",
      "offset": 6550.16,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "own assets. So they’re more like entrepreneurs \nor job seekers; they go out into the economy,  ",
      "offset": 6554.8,
      "duration": 6.32
    },
    {
      "lang": "en",
      "text": "maybe they set up a website for an architectural \nkind of firm that can design people’s houses for  ",
      "offset": 6561.12,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "them, and then the clients have a chat with \nit or something and it issues out the designs.  ",
      "offset": 6565.04,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "They can go and seek opportunities to participate \nin an economy. So that’s a different model.",
      "offset": 6571.12,
      "duration": 6.56
    },
    {
      "lang": "en",
      "text": "I think there’s some reason to think that there’s \nmore potential for economic gains if you allow  ",
      "offset": 6577.68,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "them to actually make their own entrepreneurial \ndecisions; they would have to pay for their own  ",
      "offset": 6582.4,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "GPU costs and so on. This is the kind of direction \nyou might imagine people going down if they think  ",
      "offset": 6587.6,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "that the AI systems have got to a point where they \nmight have some moral status. But you can also see  ",
      "offset": 6591.92,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "that questions about gradual disempowerment \nreally come in there. It might help liberate  ",
      "offset": 6597.28,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "these systems from mistreatment, but exacerbate \nquestions about whether they could outcompete us.",
      "offset": 6602.8,
      "duration": 9.04
    },
    {
      "lang": "en",
      "text": "A third model is to say maybe people shouldn’t be \ninterfacing with AI systems generally. This is how  ",
      "offset": 6611.84,
      "duration": 7.52
    },
    {
      "lang": "en",
      "text": "we deal with nuclear power: we have a small number \nof individuals who go and work in nuclear power  ",
      "offset": 6619.36,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "stations, and they’re vetted by their governments \nwith security checks and so on, and they go in and  ",
      "offset": 6625.84,
      "duration": 6.8
    },
    {
      "lang": "en",
      "text": "they interface with radioactive isotopes of things \nlike refined uranium. But most people don’t.  ",
      "offset": 6632.64,
      "duration": 7.76
    },
    {
      "lang": "en",
      "text": "Those factories that they work in, these power \nplants, they produce electricity which flows down  ",
      "offset": 6640.4,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "the cables into the consumers’ houses and powers \ntheir TVs and things. So that’s a different model.",
      "offset": 6644.24,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "We could do that with AI. We could have a model \nwhere there’s some small number of vetted people,  ",
      "offset": 6649.92,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "or maybe millions, who interact with AI \nsystems, use them to design new drugs,  ",
      "offset": 6655.28,
      "duration": 6.64
    },
    {
      "lang": "en",
      "text": "maybe to help cure certain kinds of cancer \nand things like this, to do new research and  ",
      "offset": 6663.44,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "also produce other kinds of new products. Then \nthose products are assembled in factories and  ",
      "offset": 6668.64,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "the consumers can buy those products. That \nis an alternate way that you could do it.",
      "offset": 6672.64,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "If you’re concerned about things like some \nmalcontent individuals or terrorist groups  ",
      "offset": 6677.6,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "using AI systems to wreak havoc, \nthis would really help avoid that.",
      "offset": 6683.6,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "Or a fourth alternative could be that if you’re \nconcerned about concentration of power issues,  ",
      "offset": 6687.2,
      "duration": 6.72
    },
    {
      "lang": "en",
      "text": "you might say what we should do is give \nevery individual access to the same  ",
      "offset": 6693.92,
      "duration": 8.24
    },
    {
      "lang": "en",
      "text": "advanced level of AI assistant. So \nit’s like a flat distribution of AI  ",
      "offset": 6702.16,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "ability given to everyone. A bit like a universal \nbasic income, but universal basic AI access.",
      "offset": 6708.16,
      "duration": 7.84
    },
    {
      "lang": "en",
      "text": "So there are four really different ways that you \ncould distribute AIs into society and have them  ",
      "offset": 6716,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "interact. And I feel that no one’s talking about \nstuff like this — like, which of those worlds  ",
      "offset": 6721.6,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "is most likely, which of those worlds is possible, \nand which of those worlds is most desirable.  ",
      "offset": 6727.04,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "Because fundamentally, we get to choose \nwhich of those worlds that we live in. As in,  ",
      "offset": 6733.28,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "maybe it’s the citizens of the United States of \nAmerica or other countries that are developing  ",
      "offset": 6740.16,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "these things that do actually get to make \nsome of these choices. And if they think  ",
      "offset": 6744.48,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "that one of these paths is very bad, they may \nbe able to stop it and go down different paths.",
      "offset": 6749.6,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "So that’s the kind of thing I’m thinking of, \nin terms of we could think a lot broader and  ",
      "offset": 6753.92,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "bigger about where are we going to be in five \nyears or where we want to be — rather than  ",
      "offset": 6759.36,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "the minutiae about exactly \nwho’s ahead at the moment  ",
      "offset": 6764.32,
      "duration": 2.64
    },
    {
      "lang": "en",
      "text": "and exactly what are they prepared \nto accept in terms of regulation.",
      "offset": 6766.96,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: I wasn’t sure what examples you \nwere going to give, so I can definitely see  ",
      "offset": 6770.96,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "what you mean by stuff that’s outside the \nOverton window. Because I guess none of  ",
      "offset": 6774,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "that stuff is anywhere close to policy-ready \nor appetising to politicians at this point.",
      "offset": 6777.12,
      "duration": 5.51
    },
    {
      "lang": "en",
      "text": "Toby Ord: No, and it’s not really meant to \nbe. It’s more speaking to, say, economists:  ",
      "offset": 6782.63,
      "duration": 7.85
    },
    {
      "lang": "en",
      "text": "they might have some interesting comments \nabout the economic efficiency of the first  ",
      "offset": 6791.04,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "two different models. Or all of those models, in \nfact: how much would we be leaving on the table  ",
      "offset": 6794.64,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "in terms of economic efficiency if we control \nthe systems more and reduce their ability to  ",
      "offset": 6799.04,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "have some kind of Hayekian finding of \nthe value that they can offer people?",
      "offset": 6805.12,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "But I think that people should be thinking \nabout which of these are more attractive  ",
      "offset": 6809.68,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "possibilities. The current approach \nfeels to me like one that is heavy on  ",
      "offset": 6815.36,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "technological determinism or some other \nkind of incentives-based determinism:  ",
      "offset": 6826.96,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "that it just assumes everyone will exactly follow \ntheir direct incentives on things, and that there  ",
      "offset": 6830.64,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "don’t seem to be any opportunities to change \nincentives or make other choices like that.",
      "offset": 6836.64,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "So people often say, clearly AI \nis definitely going to happen,  ",
      "offset": 6841.52,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "so the question is what direction does it \ngo? Or something. But even in that case,  ",
      "offset": 6847.36,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "AI doesn’t have to happen. I feel that there \nare some risks that we face, such as the risk  ",
      "offset": 6852.96,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "of asteroid impact — which thankfully does turn \nout to be very small. But if an asteroid were to  ",
      "offset": 6858.64,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "be found on a collision course with the Earth, \none that’s large enough to destroy us — so 10  ",
      "offset": 6864.16,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "kilometres across, like the one that killed \nthe dinosaurs — we actually don’t have any  ",
      "offset": 6868.88,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "abilities at the moment to deflect asteroids \nof that size. And if we saw it on a collision  ",
      "offset": 6872.8,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "course for us in a few years’ time, I’m not sure \nthat we could develop any means of deflecting it.  ",
      "offset": 6877.44,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "The ones we can deflect are something \nlike a thousandth the mass of that.",
      "offset": 6883.92,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "So suppose that asteroid slammed into the Earth \nand we all died, and somehow in this metaphor,  ",
      "offset": 6889.2,
      "duration": 7.12
    },
    {
      "lang": "en",
      "text": "we went to the pearly gates of heaven and St \nPeter was there letting us in. And we said,  ",
      "offset": 6896.32,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "“I’m sorry, we really tried on this \nasteroid thing. And maybe we should  ",
      "offset": 6901.92,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "have been working on it before we saw it, but \nultimately we felt that there was nothing we  ",
      "offset": 6905.84,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "could do” — I think that you’d get \nsomewhat of a sympathetic hearing.",
      "offset": 6910.96,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "Whereas if instead you turn up and you say, \n“We built AI that we knew that we didn’t  ",
      "offset": 6914.16,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "know how to control. Despite the fact that, yes, \nadmittedly, a number of Nobel Prize winners in AI,  ",
      "offset": 6918.64,
      "duration": 6.32
    },
    {
      "lang": "en",
      "text": "I think all of the Nobel Prize winners in AI \nperhaps have warned that it could kill everyone.  ",
      "offset": 6924.96,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "Something like half of the most senior people \nin AI have directly warned that this could cause  ",
      "offset": 6932.72,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "human extinction. But we had to build it. And so \nwe built it. And it turns out it was difficult  ",
      "offset": 6937.92,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "to align it and so we all died” — I feel that \nyou would get a much less sympathetic hearing.",
      "offset": 6944.08,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "It’d be like, “Hang on. You lost \nme at the step where you said,  ",
      "offset": 6949.2,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "‘We had to build it.’ Why did you build \nit if you thought it would kill you all?”",
      "offset": 6952.08,
      "duration": 4.52
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: The responses that \nyou would give would feel wanting.",
      "offset": 6956.6,
      "duration": 4.67
    },
    {
      "lang": "en",
      "text": "Toby Ord: Yes. You know, maybe they’d be \nlike, “I thought that if I didn’t do it,  ",
      "offset": 6961.27,
      "duration": 4.17
    },
    {
      "lang": "en",
      "text": "they would do it.” “And so \nwho did it?” “Well, I did it.”  ",
      "offset": 6965.44,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "“So you built the thing that killed everyone?” \n“Yes, but I felt…” I just think that you would  ",
      "offset": 6968.72,
      "duration": 6.88
    },
    {
      "lang": "en",
      "text": "have trouble explaining yourself. And I feel like \nwe should hold ourselves to a higher standard. Not  ",
      "offset": 6975.6,
      "duration": 8.16
    },
    {
      "lang": "en",
      "text": "just like “technology made me do it” or “the \ntechnological landscape made me do it,” or —",
      "offset": 6983.76,
      "duration": 4.054
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: “China made me do it.”",
      "offset": 6987.814,
      "duration": 0.186
    },
    {
      "lang": "en",
      "text": "Toby Ord: “China made me do it.” Despite \nthe fact that they didn’t start the race,  ",
      "offset": 6988,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "the US started the race — you know, because \nmaybe China would have started a race.  ",
      "offset": 6994,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "It’s like explaining to the teacher \nabout this fight that you started by  ",
      "offset": 7000.8,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "punching some kid in the face, because \nyou’re claiming that they would have  ",
      "offset": 7004.4,
      "duration": 2.64
    },
    {
      "lang": "en",
      "text": "punched you if you didn’t punch them or \nsomething. It just doesn’t really cut it.",
      "offset": 7007.04,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "And I feel that we should hold ourselves to \nsomewhat higher standards on these things,  ",
      "offset": 7010.32,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "and to not just think about, \n“What if I changed my action,  ",
      "offset": 7014.88,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "or some very small group of people’s actions, \nhow could I change the overall trajectory?”  ",
      "offset": 7018.48,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "But rather to note that there are worlds that \ndo seem to be available to us — where both,  ",
      "offset": 7023.2,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "say, the US and China decide \nnot to race for this thing.",
      "offset": 7028.32,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "That would involve having a conversation about \nthat. It would involve verification conditions  ",
      "offset": 7031.76,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "being sorted out. I think that there may well be \nsuch abilities to verify. Even if there weren’t,  ",
      "offset": 7036.56,
      "duration": 6.4
    },
    {
      "lang": "en",
      "text": "though, it might still be possible. I think \nthat given the actual evidence we have,  ",
      "offset": 7042.96,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "I don’t think it’s in the US’s \ninterest to push towards AI or  ",
      "offset": 7048.64,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "in China’s interest. I think it’s in both \ntheir interests to not do it. And if so,  ",
      "offset": 7054.16,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "that’s not a prisoner’s dilemma. Cooperation \nis actually quite easy, because it’s not in  ",
      "offset": 7059.2,
      "duration": 7.6
    },
    {
      "lang": "en",
      "text": "anyone’s interest to defect. And I think that \ncould well be the game in terms of game theory.",
      "offset": 7066.8,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "And yet there’s just very little discussion \nor thinking about these things. I don’t mean  ",
      "offset": 7073.28,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "to say that we should be naive and assume \nthat all incentives issues and all kinds of  ",
      "offset": 7079.76,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "adversarial aspects are irrelevant. \nBut we need at least some people,  ",
      "offset": 7086.08,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "and I think more people than we currently have, \nthinking on these larger margins. Not just  ",
      "offset": 7091.68,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "what could I do unilaterally? I know I \ncouldn’t stop the whole of AI happening  ",
      "offset": 7096.16,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "or happening in a certain direction, but maybe \nif enough people did something, that one could.",
      "offset": 7101.04,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "And I think that there’s a tendency \nfor fairly technical communities to  ",
      "offset": 7106.72,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "focus on things that are quite wonkish, as \nthey say in the policy world. So technical  ",
      "offset": 7110.96,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "or policy proposals that are quite \ntechnical and hard to understand,  ",
      "offset": 7117.6,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "but they might be able to help with the issue \nat hand if you follow through the details. I  ",
      "offset": 7121.68,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "love this stuff, right? So this applies \nto me as much as it does to anyone else.",
      "offset": 7127.04,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "But there’s a different style \nof doing things in politics,  ",
      "offset": 7133.28,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "which is instead getting much larger changes \n— which happens by setting a vision and  ",
      "offset": 7136.56,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "crystallising or coordinating the \npublic mood around that vision.",
      "offset": 7143.04,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "So in the case of AI, if you say, “We’ve got to \ndo this thing,” it’s like, well, does the public  ",
      "offset": 7148.4,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "want it? No, it seems like the public are really \nscared by it, and actually think that things are  ",
      "offset": 7153.76,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "going far too fast. So that’s somewhere where, \neven if the politicians haven’t quite gotten  ",
      "offset": 7160,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "there yet, it may be possible to speak to the \npublic about their concerns. And if we did,  ",
      "offset": 7164.8,
      "duration": 7.84
    },
    {
      "lang": "en",
      "text": "I think the answer is they’re probably \nnot concerned enough about these things.",
      "offset": 7172.64,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "Things can move very quickly in those cases. If \nyou set a vision and actually lead — and try to  ",
      "offset": 7179.04,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "have this approach of not just pushing things on \nthe margins, but of noticing that there’s a really  ",
      "offset": 7185.04,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "quite different direction that perhaps we should \nbe headed in — I think things can really happen.",
      "offset": 7189.76,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: How do you avoid \nslipping over into being naive,  ",
      "offset": 7195.6,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "or just having dreams that realistically are \nnever going to happen? Because I feel a bit  ",
      "offset": 7198.96,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "ambivalent about this message, which \nI suppose probably all of us should.",
      "offset": 7203.68,
      "duration": 2.4
    },
    {
      "lang": "en",
      "text": "It’s like there’s a tension here between you \nwant to both have some people thinking big and  ",
      "offset": 7206.08,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "have some people thinking small. But I suppose the \nworry would be that you come up with some vision  ",
      "offset": 7209.84,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "for how humanity is all going to coordinate, \nand the US and China will get along really well,  ",
      "offset": 7213.68,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "and the companies will for some reason \nstop lobbying to prevent all of your  ",
      "offset": 7217.44,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "efforts at regulating them — and this is \nhow, if we were all much more organised  ",
      "offset": 7220.4,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "and much more friendly with one another, \nthings could go in a much better direction.",
      "offset": 7224.08,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "But you could easily end up just completely \nwasting your time — and indeed, maybe  ",
      "offset": 7226.8,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "discrediting yourself, because you would just \nlook quite naive and disconnected from reality.",
      "offset": 7229.52,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "Toby Ord: Yeah. I think there’s a number of \nquestions about how one goes about coordinating  ",
      "offset": 7234,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "this process. So I’ll give you an example with an \nidea that I think deserves more attention, which  ",
      "offset": 7238.4,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "is that of having a moratorium on advanced AI — \nlet’s say a moratorium on AI beyond human level.",
      "offset": 7244.48,
      "duration": 8.64
    },
    {
      "lang": "en",
      "text": "When it comes to scientific moratoriums, we’ve \ngot some examples, such as the moratorium on  ",
      "offset": 7255.84,
      "duration": 8.4
    },
    {
      "lang": "en",
      "text": "human cloning and the moratorium on human germline \ngenetic engineering — that’s genetic engineering  ",
      "offset": 7264.24,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "that’s inherited down to the children, that could \nlead to splintering into different species. In  ",
      "offset": 7270.48,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "both those cases, when the scientific community \ninvolved had gotten to the cusp of that technology  ",
      "offset": 7275.68,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "becoming possible — such as having cloned sheep, \na different kind of mammal, and the humans  ",
      "offset": 7281.36,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "wouldn’t be that different — they realised that \na lot of them felt uneasy about this privately.",
      "offset": 7286.48,
      "duration": 6.4
    },
    {
      "lang": "en",
      "text": "So they opened up more of a conversation around \nthis, both among themselves and also with the  ",
      "offset": 7292.88,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "public. And they found that actually, yeah, they \nwere really quite uneasy about it. And they wanted  ",
      "offset": 7298.96,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "to be able to perhaps continue working on things \nlike the cloning sheep, but actually that would  ",
      "offset": 7303.04,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "be easier to work on and think about if the \nissue about cloning humans was off the table.",
      "offset": 7306.88,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "And also, if you think about how radically \ntransformative that could have been to  ",
      "offset": 7312.48,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "the entire human story, like around \n300,000 years of how humans reproduce,  ",
      "offset": 7317.52,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "and then all of a sudden they’re cloning, and \npossibly dictators are cloning millions of  ",
      "offset": 7321.36,
      "duration": 6.32
    },
    {
      "lang": "en",
      "text": "copies of themselves or all kinds of things: \nit’s very unclear how to manage it, and how  ",
      "offset": 7327.68,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "to have some kind of nuanced policy response to \nit. We’re nowhere near being able to manage it.",
      "offset": 7334.16,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "The same with human germline genetic \nengineering. It’s not that we were  ",
      "offset": 7338.96,
      "duration": 2.64
    },
    {
      "lang": "en",
      "text": "close to knowing a kind of framework \nwhere now, for the next 300,000 years,  ",
      "offset": 7341.6,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "here’s how humanity copes with this new \ntechnology — that, if you get it wrong,  ",
      "offset": 7345.6,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "could lead to, within a few generations, \nsay, Americans and Chinese being different  ",
      "offset": 7350.16,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "species to each other. I mean, there could be \nserious problems that you could be causing.",
      "offset": 7354.88,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "So the way I see it is that they started \nhaving these public conversations,  ",
      "offset": 7361.28,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "and then they ultimately decided in both those \ncases that this had potentially profound effects  ",
      "offset": 7367.44,
      "duration": 6.96
    },
    {
      "lang": "en",
      "text": "for the entire human project, or our \nwhole species and our entire future,  ",
      "offset": 7374.4,
      "duration": 7.52
    },
    {
      "lang": "en",
      "text": "and that we weren’t close to being \nable to understand how to manage them.",
      "offset": 7381.92,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "So their approach, I think of it not quite as \na pause for a certain amount of time. They also  ",
      "offset": 7386.16,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "didn’t say, “We can never ever do this, and anyone \nwho does it is evil” or something. Instead, what  ",
      "offset": 7392.24,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "they were saying is, “Not now. It’s not close to \nhappening. Let’s close the box. Put the box back  ",
      "offset": 7396.48,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "in the attic. And if in the future the scientific \ncommunity comes together and decides to lift the  ",
      "offset": 7401.76,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "moratorium, they’d be welcome to do that. But \nfor the foreseeable future, it’s not happening.”",
      "offset": 7407.52,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "And it seems to me that in the case of AI, \nthat’s kind of where we’re at. We’re at a  ",
      "offset": 7412.08,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "situation where, as I said, about half of all of \nthe luminaries in AI have said that this is one  ",
      "offset": 7417.04,
      "duration": 7.68
    },
    {
      "lang": "en",
      "text": "of the biggest issues facing humanity: the fact \nthat there is a risk of, in their single-sentence  ",
      "offset": 7424.72,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "statement, a risk of human extinction from \nthis technology that they’re developing.",
      "offset": 7430.4,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "That sounds like they’re in a similar situation to \nthe people who were developing cloning and so on.  ",
      "offset": 7435.76,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "So what I would recommend in that case is to \ngo through that step of having that public  ",
      "offset": 7441.6,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "conversation about should there be a \nmoratorium in a similar way on this.",
      "offset": 7446.64,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "Now, there are certainly some additional \nchallenges. I think that even in those  ",
      "offset": 7452.56,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "other cases, it was difficult to work out how to \ntechnically operationalise it. And in this case,  ",
      "offset": 7457.92,
      "duration": 7.84
    },
    {
      "lang": "en",
      "text": "there would be challenges as well, \nespecially like, where do you draw  ",
      "offset": 7465.76,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "the level exactly? If it’s beyond human \nlevel, how exactly do you define that?",
      "offset": 7468.96,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "And then also the incentives issue \nis I think larger In this case:  ",
      "offset": 7474.16,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "there’s more of an incentive to break this kind \nof rule. But there were big incentives to break  ",
      "offset": 7479.6,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "some of those other rules: if you think about \nhow far a country could have gotten ahead over  ",
      "offset": 7484.64,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "a couple of generations if it was able to \ngenetically engineer all of its citizens,  ",
      "offset": 7488.08,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "it could be a long way ahead. But it would \nhave to be quite patient to be able to  ",
      "offset": 7491.76,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "care about that. Whereas in this case, even \nimpatient people are caring a lot about AI.",
      "offset": 7496.4,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "So I think that this would be a challenging thing \nto do. My guess is that there’s something like a  ",
      "offset": 7501.92,
      "duration": 8.08
    },
    {
      "lang": "en",
      "text": "5% to 10% chance that some kind of moratorium \nlike this — perhaps starting from the scientific  ",
      "offset": 7511.84,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "community effectively saying you would be \npersona non grata if you were to work on  ",
      "offset": 7517.6,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "systems that would take us beyond that human \nlevel — would work. But if it did work,  ",
      "offset": 7523.2,
      "duration": 7.76
    },
    {
      "lang": "en",
      "text": "it would set aside a whole bunch of these risks,  ",
      "offset": 7530.96,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "even if that risk landscape is very confusing \nand has lots of different possibilities.",
      "offset": 7535.68,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "Some of these types of ideas might be \nable to act on many of those different  ",
      "offset": 7538.88,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "types of risk. And I think that that’s \na way where the scientific community — a  ",
      "offset": 7542.24,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "relatively small number of actors, who \nhave already kind of coordinated via  ",
      "offset": 7548,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "producing these open letters and things \n— could have that conversation. If they  ",
      "offset": 7551.44,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "crystallise their view, and for example \nthe AAAI, their professional association,  ",
      "offset": 7556.64,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "if it came out behind this and so on, it could \nbe that that crystallises out of their opinion.",
      "offset": 7561.76,
      "duration": 6.72
    },
    {
      "lang": "en",
      "text": "People could then look at the \nsituation with the scientists saying,  ",
      "offset": 7570.32,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "“We think that this is a big problem, and \nthat it’s not responsible to do it.” That  ",
      "offset": 7573.04,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "could then create norm changes which \nmean that it’s difficult to pursue it.",
      "offset": 7579.2,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "I think if the scientific \ncommunity had a moratorium on it,  ",
      "offset": 7585.36,
      "duration": 2.56
    },
    {
      "lang": "en",
      "text": "then organisations like Google DeepMind \n— that sees itself as a science player,  ",
      "offset": 7587.92,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "a science company that’s doing respectable science \nwork — it’s not going to violate a scientific  ",
      "offset": 7593.6,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "moratorium on something. It could be different \nfor the more engineering type places, and the  ",
      "offset": 7597.6,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "more “move fast and break things” cultures. So \nit doesn’t necessarily do everything on its own.  ",
      "offset": 7603.76,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "It would probably need to form a normative \nbasis for actual regulation of some sort.",
      "offset": 7609.52,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "But I do think that things like this are \npossible. And if we went to St Peter after  ",
      "offset": 7615.68,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "we all go extinct due to some AI disaster, and \nwe said, “We couldn’t stop it.” And he said, “Did  ",
      "offset": 7620.96,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "you even have a conversation about a moratorium?” \nIt’s like, “We thought about that, and we decided  ",
      "offset": 7626.72,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "it probably wouldn’t work, so we wouldn’t \neven talk about it.” That would seem crazy.",
      "offset": 7630.48,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "So I think we need to actually do some of \nthese more obvious things that are just  ",
      "offset": 7635.12,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "natural and earnest, rather than trying to \nprecalculate out, “Obviously it would seem  ",
      "offset": 7640.64,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "sensible to have that conversation. That’s what \nyou’d want another planet to do. But for us,  ",
      "offset": 7645.36,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "we know the conversation will not work out, \nso we’re not going to have it, and we’ll just  ",
      "offset": 7648.8,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "carry on building these systems.” I feel like \nthat’s the kind of the wrong way of thinking.",
      "offset": 7651.92,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: So I’m wary of encouraging \nlisteners to go and waste their time,  ",
      "offset": 7656.88,
      "duration": 6.32
    },
    {
      "lang": "en",
      "text": "so I want them to be balancing these two different \nthings. We both need people who can think bigger,  ",
      "offset": 7663.84,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "but I suppose we also need them \nto be somewhat strategic about it.",
      "offset": 7668.64,
      "duration": 2.64
    },
    {
      "lang": "en",
      "text": "I think maybe things that help to reconcile \nthese two views is: it is possible that there  ",
      "offset": 7671.28,
      "duration": 7.04
    },
    {
      "lang": "en",
      "text": "will be radical changes in attitudes in future. \nI can think of two different ways that this could  ",
      "offset": 7678.32,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "happen, and probably there are others. We’ve \nmentioned this possibility that there could  ",
      "offset": 7684.08,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "be warning shots in future: that you could get \nAI doing stuff that was completely undesired,  ",
      "offset": 7687.2,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "that was extremely harmful, that really \ncauses people to sit up and take notice,  ",
      "offset": 7691.28,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "and be like, “Wow, this is very much not \nwhat I was expecting. And this calls for  ",
      "offset": 7694.96,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "a substantial reassessment \nof the risks that we face.”",
      "offset": 7698.72,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "Another thing is just if you look at public \npolling, as you were kind of alluding to,  ",
      "offset": 7702,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "it is shocking the difference in opinion \nbetween people who are involved in AI  ",
      "offset": 7706.24,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "industry — and indeed probably in AI \ngovernance, in government — and the  ",
      "offset": 7710.64,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "attitudes of just random people who you phone \nup and ask them their opinion about this.",
      "offset": 7714.48,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "A random member of the American public is much \nmore negative about artificial intelligence,  ",
      "offset": 7717.92,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "about the impact it’s having, even right now, \nabout their expectations for how it’s going  ",
      "offset": 7722,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "to affect them personally and in the future. \nWe’ll stick up some link to some Pew polling  ",
      "offset": 7725.84,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "that came out just last month, in April. The gap \nbetween AI experts and the general public is vast,  ",
      "offset": 7729.6,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "and I think it’s growing. I think it’s actually \nbeen growing over the last couple of years:  ",
      "offset": 7735.28,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "people have become more pessimistic \nabout AI as they’ve seen more now.",
      "offset": 7738.48,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "At the same time, we’ve got to balance \nit with the fact that I think a typical  ",
      "offset": 7742.16,
      "duration": 2.24
    },
    {
      "lang": "en",
      "text": "member of the public doesn’t really care \nabout AI at all. It’s not in their top  ",
      "offset": 7744.4,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "five government issues, it’s not in their \ntop 10, possibly not even in their top 20.",
      "offset": 7747.52,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "But you have a lot of latent scepticism, \nlatent pessimism about AI. And if AI in  ",
      "offset": 7752,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "fact does become a big deal — because many more \npeople are losing their jobs, say, or people are  ",
      "offset": 7757.92,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "seeing it being used, basically it just becomes \na major feature of day-to-day life — then that  ",
      "offset": 7762.32,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "could really be a political powder keg. There’s a \nlot of latent willingness to do radical things on  ",
      "offset": 7767.92,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "AI among the public, if they actually turn \ntheir attention to it and care about it.",
      "offset": 7774.4,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "Toby Ord: I entirely agree. I should \nsay it’s challenging with this polling,  ",
      "offset": 7778.16,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "because some of the polling is done by groups \nwho are concerned about AI safety. And whenever  ",
      "offset": 7782.96,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "someone’s got an agenda, you always have to be \ncareful of interpreting the figures. I would  ",
      "offset": 7787.6,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "love it if there were a couple of groups that \nhad no agenda who funded or produced regular  ",
      "offset": 7791.76,
      "duration": 6.96
    },
    {
      "lang": "en",
      "text": "polls of these sorts in order to be able to \ntrack what direction things were heading.",
      "offset": 7798.72,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "But to the extent to which \nwe have information about it,  ",
      "offset": 7803.28,
      "duration": 2.64
    },
    {
      "lang": "en",
      "text": "I know that some of the information does \ndepend a bit on how the questions are asked,  ",
      "offset": 7805.92,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "and there are some of these effects, \nbut it does look quite negative.",
      "offset": 7809.28,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "You say sceptical. I think some of it’s \nsceptical. Some of it’s something a bit  ",
      "offset": 7816,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "different, which is people feeling like it’s been \nrammed down their throats or something — like,  ",
      "offset": 7820,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "“We don’t want this thing, and you’re forcing it \nupon us. And then you’re still forcing it upon us,  ",
      "offset": 7824.16,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "and now you’re forcing 10 times as much of it upon \nus. Please listen to us.” That kind of feeling.",
      "offset": 7828.4,
      "duration": 6.4
    },
    {
      "lang": "en",
      "text": "And I think we’re going to see more of that,  ",
      "offset": 7835.44,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "and I think it is real. I’m not saying that \nAI is necessarily bad for the people. That’s  ",
      "offset": 7838.24,
      "duration": 8
    },
    {
      "lang": "en",
      "text": "a separate question. But they’re expressing \nthat at the moment they don’t want it.",
      "offset": 7846.24,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "And if you’re a company, or if you’re a government \nand you’ve got a policy, maybe you think it’ll be  ",
      "offset": 7852.24,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "good for people, and you think it will improve \ntheir lives. If, however, you also know that  ",
      "offset": 7857.04,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "they don’t want it and they’re actively opposed to \nit, you’ve got to take that into account. You’ve  ",
      "offset": 7861.28,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "got to at least be aware that, “We’ve got this \nstory about why it will be good for you, despite  ",
      "offset": 7865.52,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "you thinking that you don’t want it. We know we’re \nright because we’re the enlightened ones.” But you  ",
      "offset": 7870.16,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "have to start wondering, “Is something going \nreally wrong in our communication strategy,  ",
      "offset": 7876.24,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "or is it possible that we’re wrong and the \nother people are tracking what’s happening?”",
      "offset": 7880.72,
      "duration": 6.32
    },
    {
      "lang": "en",
      "text": "I think that AI companies and in fact governments \nignore this at their own peril. I was surprised  ",
      "offset": 7887.04,
      "duration": 8.32
    },
    {
      "lang": "en",
      "text": "with the Californian bill, SB 1047: I was \nsurprised that it got vetoed by the governor,  ",
      "offset": 7896.56,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "because that was a politically \nunpopular move as well as I think  ",
      "offset": 7902.8,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "being a bad move. And maybe vetoes \nare not taken as strongly over there,  ",
      "offset": 7905.92,
      "duration": 6.88
    },
    {
      "lang": "en",
      "text": "as it really feels like going out of your way \nto block a bill that your congress has already  ",
      "offset": 7913.52,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "approved by wide margins, and which \nthe public also liked, and which  ",
      "offset": 7918.72,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "scientific experts like Nobel laureates and Turing \nAward winners and so on mostly also support.",
      "offset": 7924.48,
      "duration": 9.12
    },
    {
      "lang": "en",
      "text": "They ignore this public sentiment at \ntheir peril. And I think that it is  ",
      "offset": 7934.72,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "something where the community of people who \nare concerned about risks are also ignoring  ",
      "offset": 7938.64,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "it mostly. Occasionally they notice \nand say, “Isn’t that nice? The public  ",
      "offset": 7942.8,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "are also concerned.” Maybe for different \nreasons, though, it’s a bit complicated.",
      "offset": 7947.52,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "But it just surprises me that so many \npeople say this thing is inevitable. If  ",
      "offset": 7952.96,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "the public overwhelmingly loved it, then \nsaying it’s inevitable on that ground,  ",
      "offset": 7957.6,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "you might think you’ve got a bit of \na case there. But it seems almost the  ",
      "offset": 7962.64,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "other way around. If there’s growing \nnegative sentiment towards something,  ",
      "offset": 7966.08,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "and you’re claiming it’s inevitably going to \nhappen, I’m not sure that that really makes sense.",
      "offset": 7970.08,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "So if there were to be appetite for something like \na moratorium on AI beyond some particular level…  ",
      "offset": 7977.04,
      "duration": 7.92
    },
    {
      "lang": "en",
      "text": "And I’m not saying on all possible things that \ncould count as AI; there was a prominent piece  ",
      "offset": 7984.96,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "in the UK called something like, “We shouldn’t \nhave this race to build godlike intelligence.”  ",
      "offset": 7990.4,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "And that really struck a chord with people. \nI think people definitely don’t want private  ",
      "offset": 7996.8,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "companies to build godlike intelligence. If \nyou had a moratorium on godlike intelligence,  ",
      "offset": 8001.92,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "I think it would have a lot of support — albeit \nit would sound a bit fanciful and kind of stupid.",
      "offset": 8006.48,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "Similarly with superintelligence, I think \npeople are not excited. They do not want  ",
      "offset": 8011.28,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "private companies to build superintelligences. \nPretty clear. But it’s a bit outside the Overton  ",
      "offset": 8015.44,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "window to have a moratorium on it because people \nwill say superintelligence is just sci-fi anyway.",
      "offset": 8021.92,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "I remember talking about it with someone who \nwas like, “I just really don’t think that could  ",
      "offset": 8031.36,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "happen without either there being some kind of big \nwarning shot event or AI taking a lot longer than  ",
      "offset": 8035.52,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "I thought.” And my thinking is, I agree. I think \nif this was going to happen, it would probably  ",
      "offset": 8041.68,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "require some kind of warning shot event or AI to \ntake longer than people thought. But they’re very  ",
      "offset": 8046.72,
      "duration": 7.12
    },
    {
      "lang": "en",
      "text": "realistic possibilities! And before they’ve \nhappened, they feel a bit abstract and so on.",
      "offset": 8053.84,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "But if you say, what’s the chance that we’re \nin 2033, and that this approach to building  ",
      "offset": 8059.84,
      "duration": 7.68
    },
    {
      "lang": "en",
      "text": "AI scientists who work on AI and have this kind \nof hard takeoff hasn’t panned out. Instead we’re  ",
      "offset": 8067.52,
      "duration": 6.72
    },
    {
      "lang": "en",
      "text": "just kind of generally scaling up the power of \nthese systems through different techniques. If so,  ",
      "offset": 8074.24,
      "duration": 6.64
    },
    {
      "lang": "en",
      "text": "and it’s more of a gradual automation of the \nentire workforce, we could have a situation where  ",
      "offset": 8080.88,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "there’s, say, not only double-digit unemployment \nrates, but maybe above 20% unemployment rates.",
      "offset": 8086.32,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "And if that’s how it’s getting to human level,  ",
      "offset": 8092.8,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "by basically slowly automating larger \nand larger fractions of the types  ",
      "offset": 8096.32,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "of things that humans can do — and it’s \nhappening over the course of a few years,  ",
      "offset": 8100.56,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "which would mean that there’s not enough time \nfor people to find new jobs — you get the kind  ",
      "offset": 8105.28,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "of unemployment rates that bring down governments, \nand you get the kinds of protests on the streets  ",
      "offset": 8109.84,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "that are massive. Then governments have to listen. \nIf they want to get that 20% bloc of voters who  ",
      "offset": 8115.12,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "are protesting on the streets about how AI is \nruining everything for them, they may need to act.",
      "offset": 8120.8,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "At the same time, in that very plausible future \nworld, the AI companies will probably be paying  ",
      "offset": 8126.96,
      "duration": 7.44
    },
    {
      "lang": "en",
      "text": "a lot of money into politicians’ hands \nin order to try to get favourable rules.",
      "offset": 8134.4,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "I think what you’d get is a kind of \nfundamental question of which one wins:  ",
      "offset": 8140.48,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "the people or the money? Will someone \npick up a giant bloc of voters,  ",
      "offset": 8144.32,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "or will they take so much money that it’s put \n20% of the entire American population out of  ",
      "offset": 8149.84,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "work? Will they take all of that cash? Which one \nwill win? My guess is what you’d get is one of  ",
      "offset": 8155.76,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "the parties will take the money and one will \ntake the votes and then we see what happens.",
      "offset": 8162,
      "duration": 4.628
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: See what the next election is.",
      "offset": 8166.628,
      "duration": 0.972
    },
    {
      "lang": "en",
      "text": "Toby Ord: That could be a world, \nright? Very plausible. But a world  ",
      "offset": 8167.6,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "where the idea of things like \nmoratoriums or strong regulation,  ",
      "offset": 8170.72,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "it’s easier to do them than to not do them. \nIt’s what people are demanding or something.",
      "offset": 8176.88,
      "duration": 4.776
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: Or at least a substantial \nbloc of people are demanding them.",
      "offset": 8181.656,
      "duration": 1.694
    },
    {
      "lang": "en",
      "text": "Toby Ord: Exactly. This is what I mean by zooming \nout and seeing this bigger picture, to see that  ",
      "offset": 8183.35,
      "duration": 4.65
    },
    {
      "lang": "en",
      "text": "the world can go in these different ways — \nthat fundamentally we, the people of the world,  ",
      "offset": 8190.48,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "in our generation, are responsible for this. \nIn something of the same way that people say  ",
      "offset": 8195.2,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "that about climate change. I think that message \ngot out there that our generation is responsible  ",
      "offset": 8198.72,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "for what happens with climate change — you \nknow, the people alive today. As opposed to  ",
      "offset": 8202.399,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "a different message which is, “There’s \nnothing I can personally do about it.”",
      "offset": 8208.479,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: “I guess it’s hopeless.”",
      "offset": 8211.76,
      "duration": 0.56
    },
    {
      "lang": "en",
      "text": "Toby Ord: Right. This message that actually we get \nout there and we change the norms on these things.",
      "offset": 8212.32,
      "duration": 6.56
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: Or even more strikingly, if \nsomeone had said in the ’50s about nuclear war,  ",
      "offset": 8218.88,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "“If you think about the game theory, I guess \nit just says that nuclear war is inevitable,  ",
      "offset": 8223.2,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "so we may as well all just build our \nbunkers and get ready for it.” I mean,  ",
      "offset": 8226,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "a handful of people I think did have that \nattitude, but they didn’t win the public debate.",
      "offset": 8229.359,
      "duration": 3.19
    },
    {
      "lang": "en",
      "text": "Toby Ord: And there was a pretty \nstrong case. I would say their case  ",
      "offset": 8232.55,
      "duration": 1.769
    },
    {
      "lang": "en",
      "text": "was as strong as the incentives-based \narguments that you hear at the moment.",
      "offset": 8234.319,
      "duration": 3.268
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: Stronger.",
      "offset": 8237.588,
      "duration": 0.875
    },
    {
      "lang": "en",
      "text": "Toby Ord: Yeah, probably stronger. I guess I’m \njust a bit surprised. I feel that a lot of people  ",
      "offset": 8238.463,
      "duration": 6.176
    },
    {
      "lang": "en",
      "text": "think things like this couldn’t happen, and if \nyou press them on it, they mean something like  ",
      "offset": 8244.64,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "there’s a 10% chance that it could happen. \nAnd I think, you’re just going to give away  ",
      "offset": 8248.24,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "10 percentage points of solving the problem? \nWhy don’t we play for those possibilities?",
      "offset": 8252.08,
      "duration": 6.8
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: A couple of thoughts that have \ncome up for me as you’ve been talking.  ",
      "offset": 8258.88,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "One thing is that maybe another reason to \nthink that there could be a sea change in  ",
      "offset": 8263.28,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "attitudes is that, at the moment, we’re \nreally in a Wild West situation — where  ",
      "offset": 8267.28,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "the amount of regulation is negligible and \nit doesn’t look like we’re going to get  ",
      "offset": 8271.04,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "any significant regulation of AI risks \nin the next couple of years at least.",
      "offset": 8273.76,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "One virtue of that is that it does set \nus up to learn relatively early if some  ",
      "offset": 8278.72,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "of these risks are real. I mean, there’s some \nrisks that you might not expect to eventuate  ",
      "offset": 8282.56,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "until you’re in this superhuman regime. \nBut if some of the more mundane risks,  ",
      "offset": 8286.8,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "these risks of reinforcement learning \ncreating perverse behaviour, are real,  ",
      "offset": 8291.6,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "then the fact that there are no brakes or \nlimits at the moment does mean that we are  ",
      "offset": 8295.76,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "in a good position to perhaps get warning shots \nthat could indicate that in a very public way.",
      "offset": 8299.84,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "Another thing is a bit more on the speculative \nend, but there is an argument that, if we do end  ",
      "offset": 8303.84,
      "duration": 7.12
    },
    {
      "lang": "en",
      "text": "up with misaligned AI that has goals that are very \ndifferent from those of humanity, it may need to  ",
      "offset": 8310.96,
      "duration": 6.72
    },
    {
      "lang": "en",
      "text": "basically go rogue as soon as it has any chance of \nsuccessfully beating us and taking over and taking  ",
      "offset": 8317.68,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "a lot of power. Because these models are getting \nsuperseded and replaced at a very rapid pace:  ",
      "offset": 8322.96,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "a model that has a particular set of \nstrange values that we didn’t intend,  ",
      "offset": 8328.8,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "if it doesn’t strike now, then it fully \nexpects probably to be superseded by some  ",
      "offset": 8332.8,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "other model that will be more powerful \nthan it. And even if it tried later,  ",
      "offset": 8336.24,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "it will be overpowered by those other models \nthat have different goals. And that could  ",
      "offset": 8341.12,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "happen in as soon as a few months at \nleast, certainly within a few years.",
      "offset": 8343.84,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "It sounds a little crazy, but I guess people \nhave always worried that we won’t get AI going  ",
      "offset": 8347.2,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "rogue until it’s certain that it can take \nover, because it can always wait us out.  ",
      "offset": 8353.2,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "And why not just wait until we put it in \ncharge of the military and then it can  ",
      "offset": 8355.92,
      "duration": 3.519
    },
    {
      "lang": "en",
      "text": "take over easily? But that may not apply if \nthe values that it has are somewhat random,  ",
      "offset": 8359.439,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "not really related to the goals that a \nfuture model might have — in which case it  ",
      "offset": 8364,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "in fact has to go as soon as it can, or it’s \nwasted its opportunity, it’s lost its shot.",
      "offset": 8367.6,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "So that’s another way in which conceivably, if you \ndo get an AI going rogue as soon as it thinks that  ",
      "offset": 8373.12,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "has any chance of successfully overpowering \nhumanity — and it has maybe 1-in-10,000 shot,  ",
      "offset": 8378.72,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "so it basically does just get shut down — \nI think that would really change attitudes,  ",
      "offset": 8382.88,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "if you then looked at the chain \nof reasoning that had been logged.",
      "offset": 8386.8,
      "duration": 3.75
    },
    {
      "lang": "en",
      "text": "Toby Ord: Yeah, I think you’re definitely right \nthat the earlier arguments about it waiting until  ",
      "offset": 8390.55,
      "duration": 6.73
    },
    {
      "lang": "en",
      "text": "it was assured to win assumed that it’s possible \nto get to a position where it’s assured to win,  ",
      "offset": 8397.28,
      "duration": 7.119
    },
    {
      "lang": "en",
      "text": "but also assumed that it could wait \nfor quite a long time and it would  ",
      "offset": 8405.439,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "be the same kind of coherent entity. But if \nultimately you release GPT-4.5 and then you  ",
      "offset": 8408.88,
      "duration": 6.4
    },
    {
      "lang": "en",
      "text": "say it’s going to be scrapped a few months \nlater and replaced by something else, then  ",
      "offset": 8415.28,
      "duration": 5.119
    },
    {
      "lang": "en",
      "text": "it maybe only has a short chance at this, if \nit was misaligned and set up like an agent  ",
      "offset": 8422.319,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "such that it could even form these intentions. So \nyeah, I think that we may well see things fail.",
      "offset": 8427.28,
      "duration": 6.96
    },
    {
      "lang": "en",
      "text": "They may also have smaller horizons. The idea \nof taking over humanity is this idea that if  ",
      "offset": 8434.8,
      "duration": 7.76
    },
    {
      "lang": "en",
      "text": "it has a long time horizon, and it’s got this \nunbounded utility function or set of goals that  ",
      "offset": 8442.56,
      "duration": 7.04
    },
    {
      "lang": "en",
      "text": "it cares about, then, if it could really seize \nthe reins from us, it could then do what it  ",
      "offset": 8449.6,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "wants for thousands of years, perhaps, across the \ngalaxy or something, and it could really win big.",
      "offset": 8455.6,
      "duration": 6.4
    },
    {
      "lang": "en",
      "text": "But you may also be able to get smaller versions \nof this, where a system is going to disappear  ",
      "offset": 8462.88,
      "duration": 5.519
    },
    {
      "lang": "en",
      "text": "anyway in a couple of weeks, and maybe it knows \nthat, and so it takes over the lab for a couple  ",
      "offset": 8468.399,
      "duration": 8.08
    },
    {
      "lang": "en",
      "text": "of weeks or something like that and tries \nto give itself higher reward or something.",
      "offset": 8476.479,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "I think there’s various versions where \nwe might see these things happen,  ",
      "offset": 8481.52,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "but it will still depend on how competent it \nlooked and how close it seemed. If it’s some  ",
      "offset": 8484.96,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "kind of extremely lame attempt to seize control \nor to break out, people might just feel like,  ",
      "offset": 8490,
      "duration": 7.2
    },
    {
      "lang": "en",
      "text": "“Aww” — like a toddler attempting to \ndeceive you or something. It might be like,  ",
      "offset": 8498,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "“How cute” or something. That might be \nthe reaction. So I’m not sure that you  ",
      "offset": 8502.56,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "can guarantee the right kind \nof reaction to these things.",
      "offset": 8506.88,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "The one that would cause the biggest reaction from \npeople is something that feels genuinely scary,  ",
      "offset": 8511.84,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "and it genuinely could have gone \ndifferently. If, for example,  ",
      "offset": 8516.56,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "it attempts to do something and all of \nour systems to catch it work as desired  ",
      "offset": 8520.24,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "and it gets caught, maybe we’ll learn the \nwrong lesson. Maybe we’ll learn the lesson  ",
      "offset": 8524.399,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "that we’ve got all these systems and we \nalways catch it or something like that.",
      "offset": 8528.319,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: Should we be doing anything \nto prepare for that time so that people  ",
      "offset": 8534.56,
      "duration": 2.879
    },
    {
      "lang": "en",
      "text": "learn the right lesson? I suppose if \nwe do have systems and they actually  ",
      "offset": 8537.439,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "are quite good and they do catch \nit, maybe that is legitimately  ",
      "offset": 8541.04,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "reassuring. I don’t want to say that \npeople should always be more alarmed.",
      "offset": 8543.76,
      "duration": 2.47
    },
    {
      "lang": "en",
      "text": "Toby Ord: Yeah. I mean, at that point, \nyou learn two things: you learn that  ",
      "offset": 8546.23,
      "duration": 2.329
    },
    {
      "lang": "en",
      "text": "it tried to escape and that we caught it. \nOne of those things is reassuring and one  ",
      "offset": 8548.56,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "of the things is not reassuring. And exactly \nwhat the balance of them is a bit uncertain.",
      "offset": 8552.24,
      "duration": 3.427
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: Depends on the details, I guess.",
      "offset": 8555.668,
      "duration": 0.811
    },
    {
      "lang": "en",
      "text": "Toby Ord: Does depend a bit on the details. But I \nthink that if there were some apparent opponents  ",
      "offset": 8556.479,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "to caring about safety who were saying, you \ndon’t have to care about these things very much,  ",
      "offset": 8560.399,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "and that they won’t desire to break out. Yann \nLeCun has said many things about this over the  ",
      "offset": 8565.439,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "years, saying, we’re just anthropomorphising \nfrom humans and other animals that will have  ",
      "offset": 8571.6,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "these drives, this is naive, and so \non. If these things were exhibited,  ",
      "offset": 8575.68,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "I think that would really put a dent in his \nreputation as a credible person on that issue.  ",
      "offset": 8580.08,
      "duration": 6.319
    },
    {
      "lang": "en",
      "text": "Whereas if what he predicts comes \ntrue, it does a bit of the opposite.",
      "offset": 8586.399,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "It would do something, but I think \nit’s complicated as to how much does  ",
      "offset": 8591.84,
      "duration": 9.36
    },
    {
      "lang": "en",
      "text": "some kind of warning shot change the \nOverton window. I think it can depend  ",
      "offset": 8601.2,
      "duration": 6.72
    },
    {
      "lang": "en",
      "text": "upon how much damage there is. So if there \nare actual harms that are had — it’s not  ",
      "offset": 8607.92,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "just like a shot across the bow that \nwakes us up but doesn’t hurt anyone;  ",
      "offset": 8612.96,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "instead it’s like a shot that goes into your leg \nand at least it didn’t kill you, but it’s really  ",
      "offset": 8617.2,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "alerted you to the threat — if it’s more like \nthat, then there’s a question of how big is it.",
      "offset": 8621.439,
      "duration": 5.521
    },
    {
      "lang": "en",
      "text": "If an AI system, for example, caused a global \nfinancial crisis of a similar scale to the 2008  ",
      "offset": 8627.52,
      "duration": 6.64
    },
    {
      "lang": "en",
      "text": "crisis, that would be a pretty big deal, \nand a lot of people would be very unhappy,  ",
      "offset": 8634.16,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "a lot of people would be out of jobs and so on. \nAnd if that was pretty clearly attributed to AI,  ",
      "offset": 8638.399,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "there’ll be a big reaction. So that’s \nan example at a large financial scale.",
      "offset": 8642.24,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "But there could also be other versions of things, \nand I think that it’s hard to predict them.  ",
      "offset": 8647.28,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "No one would have predicted this Kevin Roose \nthing with Bing, where the real breakthrough  ",
      "offset": 8652.96,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "thing was that it tried to seduce him and \nget him to break up with his wife — but it  ",
      "offset": 8657.84,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "turned out that that was so misaligned and so \nsalient or something, so weird or whatever —",
      "offset": 8662.8,
      "duration": 6.32
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: It really captured the imagination.",
      "offset": 8669.12,
      "duration": 2.56
    },
    {
      "lang": "en",
      "text": "Toby Ord: It really captured the public \nimagination, and made them wake up and think,  ",
      "offset": 8671.68,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "this is not the case that someone’s data \ncentre has been made 3% more efficient by  ",
      "offset": 8675.439,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "some machine learning technique. \nThis is something very different.",
      "offset": 8678.479,
      "duration": 3.521
    },
    {
      "lang": "en",
      "text": "So what I’m saying is it’s hard to predict what \nthese things are or exactly what they’ll be like,  ",
      "offset": 8682,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "but you should still be ready for them. \nI think a lot of people seem to tacitly  ",
      "offset": 8686.8,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "assume that the situation we’ll be in in \na few years’ time is exactly the same as  ",
      "offset": 8691.04,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "the one that we’re in at the moment, \nand the appetites that people have  ",
      "offset": 8694.56,
      "duration": 2.799
    },
    {
      "lang": "en",
      "text": "will be the same as they are now for \ndoing different types of responses.",
      "offset": 8697.359,
      "duration": 5.841
    },
    {
      "lang": "en",
      "text": "Whereas instead you should think \nthat maybe it will have shifted,  ",
      "offset": 8703.2,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "and the Overton window will be much \nmore expansive and include making major  ",
      "offset": 8707.52,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "choices. And maybe it will go the other way \naround and maybe you will have even less.  ",
      "offset": 8712.56,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "But to at least allow for that uncertainty. \nDon’t predict effectively that with 100%  ",
      "offset": 8716.8,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "probability the Overton window will be exactly \nwhere it is now. That’s the real mistake.",
      "offset": 8721.68,
      "duration": 3.521
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: You mentioned this idea of \n“pausing at human level” is the expression  ",
      "offset": 8726.56,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "that I’ve heard — which is a relatively \nstraightforward thing. It’s a nice slogan,  ",
      "offset": 8731.52,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "even if it perhaps doesn’t really \ncapture the technical realities.",
      "offset": 8735.6,
      "duration": 2.879
    },
    {
      "lang": "en",
      "text": "It’s a very interesting idea, because I think \nif you’re the kind of person who is used to  ",
      "offset": 8738.479,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "analysing policy in economics or anywhere else, \nyou say pause at human level and you’re like:  ",
      "offset": 8741.92,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "Is that just at the training level? What about \nif we throw in more inference compute? Then  ",
      "offset": 8747.28,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "wouldn’t it potentially exceed the human level? \nWait, AIs are much above human level in many  ",
      "offset": 8751.359,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "different respects. So in fact, what we’re talking \nabout is something that is above human level,  ",
      "offset": 8756,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "and as generalisable or more generalisable \nthan human beings. So how do we have a measure  ",
      "offset": 8759.439,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "of generalisability that would allow us to \nenforce this rule? And if the US imposed such  ",
      "offset": 8764.24,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "a rule on itself, wouldn’t China just ignore it? \nWouldn’t this prevent us from engaging in many  ",
      "offset": 8768.96,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "beneficial applications of AI that basically \neveryone is on board with and excited about?",
      "offset": 8774.24,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "There’s this raft of problems with it, which \nI think causes people to roughly dismiss that  ",
      "offset": 8778.56,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "idea out of hand. And maybe it is a bad idea; \nit certainly does have significant challenges  ",
      "offset": 8782.96,
      "duration": 6.96
    },
    {
      "lang": "en",
      "text": "and drawbacks. The thing I want people to \ndo when they’re analysing these ideas is to  ",
      "offset": 8789.92,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "apply a similar standard to these proposals as \nthey do to other areas of current regulation,  ",
      "offset": 8795.04,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "where they think there are substantial \nrisks and there’s an issue to be addressed.",
      "offset": 8799.76,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "Basically, every other area of regulation \nhas significant unintended side effects.  ",
      "offset": 8803.439,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "It poses economic efficiency costs and problems,  ",
      "offset": 8807.439,
      "duration": 2.48
    },
    {
      "lang": "en",
      "text": "and denies us products that might \nhave been good. There are random,  ",
      "offset": 8809.92,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "arbitrary thresholds that have to be drawn: \nthe speed limit is this level on this road and  ",
      "offset": 8814.64,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "that level on that road. And also enforcement is \nimperfect: people break road rules all the time.",
      "offset": 8818.56,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "Nonetheless, we don’t then say that all of \nthese thresholds on what is safe driving  ",
      "offset": 8823.6,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "and what is not would be arbitrary, \nand people would break them anyway,  ",
      "offset": 8827.68,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "and this would lead us to have slower transport, \nso that would put us at a competitive disadvantage  ",
      "offset": 8831.359,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "with other countries — so we’re just going \nto allow the roads to be open slather.",
      "offset": 8835.84,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "We balance the costs, the risks, and the rewards, \nand we accept that the world is a messy place and  ",
      "offset": 8841.68,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "that many areas of regulation are going to be \nchallenging. But if there is significant upside,  ",
      "offset": 8846,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "if you can reduce some important risks or \nsome important harms in some significant way,  ",
      "offset": 8849.84,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "then we’re at least open to considering \na regulatory regime around it.",
      "offset": 8854,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "And I think AI regulation is not given \nthe same standard. People do not consider  ",
      "offset": 8857.2,
      "duration": 5.279
    },
    {
      "lang": "en",
      "text": "these in the same way as other concrete \nharms that they’re familiar with now.",
      "offset": 8862.479,
      "duration": 4.15
    },
    {
      "lang": "en",
      "text": "Toby Ord: Yeah, that’s right. And to continue \nwith your road traffic analogy, there are also  ",
      "offset": 8866.63,
      "duration": 5.21
    },
    {
      "lang": "en",
      "text": "rules against reckless behaviour on the \nroads. At least the speed limit was a  ",
      "offset": 8871.84,
      "duration": 7.44
    },
    {
      "lang": "en",
      "text": "single-dimensional thing where you have to \npick some arbitrary point in a continuum.  ",
      "offset": 8879.28,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "Reckless behaviour is this very multidimensional \nthing, and what’s reckless for one person might  ",
      "offset": 8883.68,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "be different for another person if they’re \nmuch more controlled at how they drive a car,  ",
      "offset": 8887.92,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "as in they’re more skilful — but we have laws \nlike this, and they kind of work. So you’re right:  ",
      "offset": 8890.96,
      "duration": 7.28
    },
    {
      "lang": "en",
      "text": "there’s this demand of scrutiny on this particular \narea that we don’t apply to other areas.",
      "offset": 8900.399,
      "duration": 5.521
    },
    {
      "lang": "en",
      "text": "Again, stepping back and zooming out is really \nhelpful. At the moment, AI systems and the people  ",
      "offset": 8907.04,
      "duration": 6.8
    },
    {
      "lang": "en",
      "text": "who produce them are less regulated than, say, \nbread, to pick an example. All kinds of things,  ",
      "offset": 8913.84,
      "duration": 6.72
    },
    {
      "lang": "en",
      "text": "if you just kind of look around: it’s \nprobably less regulated than bricks,  ",
      "offset": 8920.56,
      "duration": 2.721
    },
    {
      "lang": "en",
      "text": "certainly less regulated than lamps. And why does \nthat make sense? Do we think lamps are a bigger  ",
      "offset": 8924.319,
      "duration": 8.641
    },
    {
      "lang": "en",
      "text": "threat to us than AI systems? So there are a whole \nlot of leading lamp scientists who are saying it’s  ",
      "offset": 8932.96,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "one of the greatest issues facing humanity, and \nit could pose a threat of human extinction? No.",
      "offset": 8938.72,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "So the idea that it should just end up \nbeing less regulated than these things,  ",
      "offset": 8944.24,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "I think is fundamentally kind of stupid, actually, \nand disingenuous. At least, if people have thought  ",
      "offset": 8948.319,
      "duration": 6.32
    },
    {
      "lang": "en",
      "text": "about it. Let’s take that back a bit: either it’s \nthoughtless — and it’s fine to be thoughtless  ",
      "offset": 8954.64,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "occasionally, to not notice something — but \nthe more you talk about it and make it part  ",
      "offset": 8960,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "of your life and you still keep saying it, \nthe more I feel it’s disingenuous or stupid.",
      "offset": 8963.84,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "There is an interesting question \nabout how far it should go,  ",
      "offset": 8972.64,
      "duration": 2.48
    },
    {
      "lang": "en",
      "text": "but clearly we’re at the too low \nend of the spectrum at the moment.",
      "offset": 8975.12,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: I think some distinctions you \ncould draw with lamps are that lamps are  ",
      "offset": 8979.76,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "not changing very quickly in the way that \nAI is, so that’s one reason to maybe hold  ",
      "offset": 8983.2,
      "duration": 3.119
    },
    {
      "lang": "en",
      "text": "off or not try to lock in bad stuff, is \nthat AI is constantly changing. Maybe  ",
      "offset": 8986.319,
      "duration": 2.801
    },
    {
      "lang": "en",
      "text": "we’ll have a better idea about how to \nregulate it in some years in the future.",
      "offset": 8989.12,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "Perhaps the other thing is that \nthe benefits of AI probably… Well,  ",
      "offset": 8992.64,
      "duration": 2.24
    },
    {
      "lang": "en",
      "text": "lamps are pretty important; \nlighting is pretty valuable.",
      "offset": 8994.88,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "Toby Ord: They actually may be more \nimportant than AI. For example,  ",
      "offset": 8998.24,
      "duration": 9.279
    },
    {
      "lang": "en",
      "text": "there’s a really nice report out from DeepMind \nrecently, where they created this new system  ",
      "offset": 9007.52,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "and it worked out how to make their entire \ntraining system 1% more efficient, and their  ",
      "offset": 9013.12,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "entire compute system 1% more efficient as well. \nAnd wow, that’s worth so much money and so on.",
      "offset": 9017.12,
      "duration": 6.239
    },
    {
      "lang": "en",
      "text": "But lamps: I mean, before lamps \nyou could only work in the daytime,  ",
      "offset": 9023.359,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "and you could not do anything in the nighttime. \nI guess it depends on whether we include candles  ",
      "offset": 9027.28,
      "duration": 7.119
    },
    {
      "lang": "en",
      "text": "as part of this thing. But if you imagine \nthere was this breakthrough where all of  ",
      "offset": 9034.399,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "a sudden we can actually be productive for \nan additional 30% of the day or something,  ",
      "offset": 9039.04,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "it’s huge, right? And if it happened before \nwe were born, we tend to just ignore that.",
      "offset": 9044.08,
      "duration": 5.745
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: We just tend to ignore it. \nOK, well, I’ll modify my statement:  ",
      "offset": 9049.825,
      "duration": 4.335
    },
    {
      "lang": "en",
      "text": "Lamps are more important than AI, but AI one \nday, at some stage, may be more important than  ",
      "offset": 9054.16,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "interior lighting. You’re always balancing \nthe benefits with the risks, and because the  ",
      "offset": 9057.6,
      "duration": 8.96
    },
    {
      "lang": "en",
      "text": "benefits at some point might be quite large, we \nmight be willing to accept some meaningful risk.",
      "offset": 9066.56,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "The thing that you’re talking about, about \npeople potentially being disingenuous because  ",
      "offset": 9071.68,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "they just always dive into the details, like, \n“This would require an arbitrary threshold, so  ",
      "offset": 9074.96,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "it’s completely unviable”: I think a trouble with \nthe discourse is that there are some people who  ",
      "offset": 9079.12,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "basically think that all of the risks are complete \nbuncombe — that there are no worries, we should  ",
      "offset": 9083.2,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "just push ahead because the risks are either \nfar away, nonexistent, massively exaggerated.",
      "offset": 9088,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "For those folks, it makes sense that when \npeople propose regulations that they think  ",
      "offset": 9092.72,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "have substantial costs but have basically \nno benefits from their point of view, they  ",
      "offset": 9095.6,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "just want to shoot them down. So they’re like, \n“This would be a downside of that. That would be  ",
      "offset": 9099.28,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "another downside.” It makes sense that basically \nthey just highlight the downsides and there’s  ",
      "offset": 9102.08,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "nothing in it for them. They’re not motivated \nin any sense to try to make this stuff work.",
      "offset": 9106.319,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "But I think for the majority of people who think \nthat there are risks here — and clearly a majority  ",
      "offset": 9109.68,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "of the public thinks that, and really the \nmajority of the people who know the most  ",
      "offset": 9113.92,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "about it also think that there are risks and \nthings to worry about here — it’s not enough  ",
      "offset": 9116.64,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "to just say that there are some costs. You have \nto balance these things against one another.",
      "offset": 9121.439,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "And also, I want to see people try to \nmake it work, have some actual energy  ",
      "offset": 9124.479,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "behind it. Not just saying, “This would \nbe challenging in some respect” — like,  ",
      "offset": 9129.76,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "“How could we improve it? What is your preferred \npolicy response to this? What is the best way of  ",
      "offset": 9134.399,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "addressing these issues from your point of \nview?” You can’t always just be saying that  ",
      "offset": 9138.72,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "something involves costs. Everything involves \nsome cost, or we already would have done it.",
      "offset": 9142.8,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "Toby Ord: Yeah, exactly. And I would add to \nthat the situation of thinking that it’s just  ",
      "offset": 9146.96,
      "duration": 7.68
    },
    {
      "lang": "en",
      "text": "not going to happen, and that there are no risks, \nand it’s all made up and never going to happen:  ",
      "offset": 9154.64,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "I don’t feel that’s a responsible view for \nanyone to have. I think that you could think,  ",
      "offset": 9159.84,
      "duration": 6.56
    },
    {
      "lang": "en",
      "text": "“That’s my viewpoint, but I’m aware that I’m \nin disagreement with a large number of people  ",
      "offset": 9167.439,
      "duration": 6.721
    },
    {
      "lang": "en",
      "text": "who are more expert than me.” Unless maybe if \nyou’re Yann LeCun, who could say, “I’m also a  ",
      "offset": 9174.16,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "Turing Award winner in AI, so I’m at the equal \nlevel or something, and I can kind of disagree.”",
      "offset": 9180,
      "duration": 6.479
    },
    {
      "lang": "en",
      "text": "But I feel that for almost all of us, the \nfact that there’s so many people, that  ",
      "offset": 9186.479,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "there is an active disagreement — but an active \ndisagreement means you should have uncertainty;  ",
      "offset": 9190.8,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "it doesn’t mean you get to choose whatever \nyou want. It’s kind of what I’m saying. And  ",
      "offset": 9194.24,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "that the idea that, because the experts \nare not 100% aligned behind something,  ",
      "offset": 9197.28,
      "duration": 5.359
    },
    {
      "lang": "en",
      "text": "I get to just believe whatever beliefs would \nbe most convenient for me, that’s not really  ",
      "offset": 9202.64,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "how being a rational actor works, and I don’t \nthink one needs to take that very seriously.",
      "offset": 9207.52,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: I just actually don’t know what the \nanswer would be. But for people like Yann LeCun,  ",
      "offset": 9214,
      "duration": 7.6
    },
    {
      "lang": "en",
      "text": "I would love him to answer the question: “You \ndon’t think that there’s really any risk here.  ",
      "offset": 9221.6,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "Your personal preference would be no regulation, \nmore or less, at this point, at least. But let’s  ",
      "offset": 9225.6,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "imagine that you had our credences in how things \nwere playing out, that you thought that rogue AI  ",
      "offset": 9230.08,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "was a real issue, you thought there were \nother ways that things could go wrong,  ",
      "offset": 9234.479,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "gradual disempowerment and so on. What is your \nfavourite policy response, given those beliefs?”",
      "offset": 9237.84,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "That is a question that is rarely \nanswered, and I would just actually  ",
      "offset": 9243.28,
      "duration": 3.039
    },
    {
      "lang": "en",
      "text": "be fascinated — because I think we might \nfind that there is a policy response that  ",
      "offset": 9246.319,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "both people who are very worried about it \nand people who are not that worried about  ",
      "offset": 9250.56,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "think is kind of tolerable as a middle \nground between these different extremes.",
      "offset": 9253.76,
      "duration": 3.589
    },
    {
      "lang": "en",
      "text": "Toby Ord: Yeah, I think you should get him \non. I’d listen. And I think it is a great  ",
      "offset": 9257.349,
      "duration": 6.17
    },
    {
      "lang": "en",
      "text": "question. I think probably the person, \nwhether it’s Yann or someone else or me,  ",
      "offset": 9263.52,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "answers quite defensively. So if you asked \nit live, you probably wouldn’t quite get  ",
      "offset": 9268.319,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "the right answer. But it’ll be really \ninteresting to hear a considered answer.",
      "offset": 9271.92,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "One thing I find interesting is that SB 1047 \nwas the compromise bill. It was a bill proposed  ",
      "offset": 9276.24,
      "duration": 7.04
    },
    {
      "lang": "en",
      "text": "by people concerned about this type of safety, who \nwere saying, “What is the absolutely most minimal,  ",
      "offset": 9284.72,
      "duration": 7.2
    },
    {
      "lang": "en",
      "text": "extremely unburdensome form of regulation you \ncould do that’s still way less burdensome than  ",
      "offset": 9292.88,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "the bread thing?” It’s like saying if 10,000 \npeople are killed by some kind of botulism  ",
      "offset": 9298.96,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "in your factory that made the bread, then you \nwill be aware that you’re going to go to jail,  ",
      "offset": 9305.2,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "but if otherwise, it’s just \nfine. It’s something like that.",
      "offset": 9309.92,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "And they already did try to kind of come up with \nan extremely weak kind of win-win type thing.  ",
      "offset": 9314.479,
      "duration": 7.36
    },
    {
      "lang": "en",
      "text": "Like what’s a bill you could do that would get \nsome benefits at almost no cost to the industry,  ",
      "offset": 9321.84,
      "duration": 6.559
    },
    {
      "lang": "en",
      "text": "and that frankly would actually give the \nindustry a lot of what they said they wanted.",
      "offset": 9328.399,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "So industry often does want to, like individual \nactors do want to have things be safe.  ",
      "offset": 9332.8,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "They’ve often got a lot of concerns about \nhow quickly market forces are making them  ",
      "offset": 9338.64,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "act and how quickly market forces are \nmaking them deploy their new models,  ",
      "offset": 9342.319,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "because everyone else is deploying quickly. \nAnd if they could all be bound by the safety  ",
      "offset": 9345.6,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "thing so that their competitors didn’t have \nan advantage over them if only I’m bound,  ",
      "offset": 9349.68,
      "duration": 6.96
    },
    {
      "lang": "en",
      "text": "then they tend to want that. So it was frankly \na bit surprising that there was this hostility.",
      "offset": 9356.64,
      "duration": 7.84
    },
    {
      "lang": "en",
      "text": "But yeah, I do feel that there already \nhas been a very good-faith attempt by  ",
      "offset": 9366,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "the safety community to come up with the \nkind of bill that tries to meet all of  ",
      "offset": 9371.04,
      "duration": 3.279
    },
    {
      "lang": "en",
      "text": "the complaints that the other people \nhave. And even that was shot down.",
      "offset": 9374.319,
      "duration": 4.949
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: Yeah, I’ve said this on the show \nbefore, but I do think that the industry is  ",
      "offset": 9379.268,
      "duration": 4.411
    },
    {
      "lang": "en",
      "text": "potentially shooting itself in the foot here. \nBecause the thing that is most likely to bring  ",
      "offset": 9383.68,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "about the sort of draconian regulation that \npeople who are optimistic about AI technology  ",
      "offset": 9388.479,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "are most scared of is some sort of \ndisaster. Any sort of disaster that  ",
      "offset": 9393.359,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "actually leads to loss of life could lead \nto a very big change in attitudes and lead  ",
      "offset": 9397.52,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "to maybe more draconian regulation than \nis necessary from anyone’s point of view.",
      "offset": 9401.2,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "Toby Ord: And even if you think your company’s \nnever going to make that mistake, you might think  ",
      "offset": 9404.88,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "these cowboys down the street are exactly the kind \nof people who could make that kind of mistake,  ",
      "offset": 9408.96,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "and they need some regulation that will stop \nthem from ruining the party for everyone, right?",
      "offset": 9412.8,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "I really do think that this is very short-sighted. \nAnd on top of that, sometimes we talk about  ",
      "offset": 9418.64,
      "duration": 8.16
    },
    {
      "lang": "en",
      "text": "someone having conflict of interest: these places \nare very conflicted. And if you did find that,  ",
      "offset": 9426.8,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "as a company, you thought it wasn’t in your \ninterest, but also you get big stock bonuses  ",
      "offset": 9432.08,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "and so on for the more stuff that you put out, \nyou really want to inspect your own views quite  ",
      "offset": 9435.52,
      "duration": 8.4
    },
    {
      "lang": "en",
      "text": "carefully. We talk about various forms of biases \nand prejudices that people might end up having,  ",
      "offset": 9443.92,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "and it would be very difficult to actually keep \nstraight your actual prediction on this thing,  ",
      "offset": 9449.68,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "as opposed to these other \nincentives that you’re facing.",
      "offset": 9456.16,
      "duration": 5.267
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: Yeah. Another interesting \ndynamic that I see going on is that when  ",
      "offset": 9461.427,
      "duration": 4.972
    },
    {
      "lang": "en",
      "text": "I’m thinking about SB 1047, or any proposed \nregulation that we might put in place now,  ",
      "offset": 9466.399,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "I’m thinking of this as the very first step \nin a very iterated process — where almost  ",
      "offset": 9471.439,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "certainly there’s going to be a whole lot of \nproblems that we’re going to identify with it,  ",
      "offset": 9476.319,
      "duration": 2.561
    },
    {
      "lang": "en",
      "text": "it wasn’t written quite right, but we’ll \njust improve those over time. And you’ve  ",
      "offset": 9478.88,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "got to start somewhere in order to \nbegin learning what might succeed.",
      "offset": 9483.6,
      "duration": 2.879
    },
    {
      "lang": "en",
      "text": "I think the people who are very against it, \nthey think that whatever we put in place now  ",
      "offset": 9486.479,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "is going to be potentially there forever. \nIt’s not really the beginning of a process.  ",
      "offset": 9490.24,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "Maybe for them it’s like this is just the \nbeginning of a ratchet, where everything is  ",
      "offset": 9493.84,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "going to become more and more extreme over time \nrather than be kind of perfected and improved.",
      "offset": 9496.8,
      "duration": 4.23
    },
    {
      "lang": "en",
      "text": "Toby Ord: I mean, I think there might \nbe a bit of a ratchet if you started  ",
      "offset": 9501.03,
      "duration": 2.57
    },
    {
      "lang": "en",
      "text": "with something like 1047, but that’s \nbecause 1047 is obviously too weak.  ",
      "offset": 9503.6,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "And they will be looking back on the days when \n1047 was the issue and thinking, “Oh my god.”",
      "offset": 9508.319,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: “Should have taken that deal.”",
      "offset": 9512.64,
      "duration": 1.24
    },
    {
      "lang": "en",
      "text": "Toby Ord: Yeah, I think so. But I really \ndo think they may have a good point here.  ",
      "offset": 9513.88,
      "duration": 6.68
    },
    {
      "lang": "en",
      "text": "If it is the case that whatever the first \nregulation is sets the entire frame,  ",
      "offset": 9521.76,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "and it’s not possible to step out to a different \nframe… For example, suppose the first thing is  ",
      "offset": 9527.76,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "about compute thresholds for pre-training and \nthen you can never escape that frame or something:  ",
      "offset": 9532.08,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "that could be a big problem if then the \nscaling stops. So it really can matter.",
      "offset": 9536.64,
      "duration": 7.36
    },
    {
      "lang": "en",
      "text": "But is that a recipe for therefore complete \nlaissez faire, no regulation, do whatever you  ",
      "offset": 9544,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "want? That’s obviously too quick. But if it is \nthe case that in certain regulatory environments  ",
      "offset": 9548.96,
      "duration": 8.56
    },
    {
      "lang": "en",
      "text": "the default is that if you introduce things \nthey stay forever, that could be a bad thing,  ",
      "offset": 9557.52,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "and it could be that there’s some win-wins \nthat one could find there — because the  ",
      "offset": 9563.68,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "safety community also don’t want to be stuck \nin silly frames that no longer make sense.",
      "offset": 9566.8,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "One approach to that is to have explicit \nsunset clauses. So you could say this is  ",
      "offset": 9572.24,
      "duration": 5.359
    },
    {
      "lang": "en",
      "text": "going to be a rule that’s going \nto last for the next two years.",
      "offset": 9577.6,
      "duration": 2.4
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: Then it has to be renewed.",
      "offset": 9581.52,
      "duration": 1.75
    },
    {
      "lang": "en",
      "text": "Toby Ord: Exactly. It has to be explicitly \nrenewed, or there could be successor bills or  ",
      "offset": 9583.27,
      "duration": 5.85
    },
    {
      "lang": "en",
      "text": "something like that. We have to find a successor \nthing. I feel that we should at least be doing  ",
      "offset": 9589.12,
      "duration": 7.439
    },
    {
      "lang": "en",
      "text": "things like that. And if it is the case that \nall regulation has to be permanent and as soon  ",
      "offset": 9596.56,
      "duration": 7.44
    },
    {
      "lang": "en",
      "text": "as you try to regulate something, you’re stuck \nwith that forever, I feel like that’s a terrible  ",
      "offset": 9604,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "regulatory system. I don’t think it’s the one \nwe’ve got either. Tell me if I’m wrong, but  ",
      "offset": 9609.52,
      "duration": 10.959
    },
    {
      "lang": "en",
      "text": "I’d be very surprised if it’s the case that you \ncan’t put sunset clauses on these types of bills.",
      "offset": 9620.479,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: No, I’m basically sure that you can.",
      "offset": 9624.88,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "Toby Ord: In which case, why isn’t \nthat the conversation? If they say,  ",
      "offset": 9627.68,
      "duration": 2.719
    },
    {
      "lang": "en",
      "text": "“We don’t want to be stuck in this thing for \n10 years,” that’s an argument for putting a  ",
      "offset": 9630.399,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "two-year sunset clause on the bill. It’s \nnot an argument for vetoing the bill.",
      "offset": 9633.439,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: We could potentially become a little \nbit more concrete for a minute. You mentioned  ",
      "offset": 9639.2,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "this “pause at human level” as a broad schema \nthat possibly has some legs or has some merit,  ",
      "offset": 9644.08,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "even if there’s also implementation \nchallenges. Are there any other things  ",
      "offset": 9648.64,
      "duration": 2.64
    },
    {
      "lang": "en",
      "text": "that people should potentially have \nin mind, thinking ahead to ideas that  ",
      "offset": 9651.28,
      "duration": 3.119
    },
    {
      "lang": "en",
      "text": "are outside the Overton window now \nbut could be useful down the line?",
      "offset": 9654.399,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "Toby Ord: I think that there’s a whole \nhost of these, and I certainly don’t  ",
      "offset": 9658.16,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "feel I’ve explored the area thoroughly. \nAnd I think you actually could probably  ",
      "offset": 9663.84,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "go through a lot of seemingly naive takes \nthat people have and reevaluate them a bit.",
      "offset": 9668.399,
      "duration": 5.841
    },
    {
      "lang": "en",
      "text": "One of those is this idea of an emergency brake on \nAI. So some kind of option for someone — you know,  ",
      "offset": 9675.28,
      "duration": 9.279
    },
    {
      "lang": "en",
      "text": "that could be for the leader of a lab — to \nhave some ability to stop the system if they  ",
      "offset": 9684.56,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "needed to. It could be for the government \nthat that company is based in to be able  ",
      "offset": 9689.6,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "to stop it. It could be for the international \ncommunity to be able to stop it. For example,  ",
      "offset": 9694.72,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "if the Security Council agreed to stop \nit, that there’s some way of doing so.",
      "offset": 9699.52,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "And if you start to think about \nthat seriously, you start to notice,  ",
      "offset": 9704.08,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "what if this thing’s deployed everywhere? What \nif there’s a whole lot of critical infrastructure  ",
      "offset": 9709.84,
      "duration": 3.519
    },
    {
      "lang": "en",
      "text": "that needs it? What if there are people in \nhospital who, if you turn this thing off,  ",
      "offset": 9713.359,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "somehow their treatment will fail or something? \nWe need to start asking these questions,  ",
      "offset": 9717.68,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "and it might be a reason not to make \nthere be people who will die if AI is  ",
      "offset": 9723.68,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "turned off. And we can start to actually \nthink through some of those things now.",
      "offset": 9727.439,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "At the moment, I think it is very difficult \nfor government to stop these things. You could  ",
      "offset": 9732.72,
      "duration": 8.88
    },
    {
      "lang": "en",
      "text": "think of two different versions. There’s \nthe version where the company is located,  ",
      "offset": 9741.6,
      "duration": 2.799
    },
    {
      "lang": "en",
      "text": "headquartered inside your country, and the \nversion where they’re not. For example,  ",
      "offset": 9744.399,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "could Australia stop AI systems operating from \nexternally inside their own borders, even if  ",
      "offset": 9750.64,
      "duration": 5.759
    },
    {
      "lang": "en",
      "text": "they’re not headquartered there? I think the \nanswer is they’d have no levers to pull to make  ",
      "offset": 9756.399,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "that happen at the moment — but maybe they should \ngive themselves those levers. Definitely the same  ",
      "offset": 9761.68,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "for countries that are meant to be governing \nthe corporations that live inside their borders.",
      "offset": 9768.16,
      "duration": 6.239
    },
    {
      "lang": "en",
      "text": "So I think people should explore ideas \nlike that. Even the heads of companies  ",
      "offset": 9774.399,
      "duration": 6.32
    },
    {
      "lang": "en",
      "text": "probably wish they had a little bit more of an \nability to do things like this. Probably they’ll  ",
      "offset": 9781.6,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "find themselves in a situation where maybe their \ngroups working on safety and alignment will tell  ",
      "offset": 9786.479,
      "duration": 6.721
    },
    {
      "lang": "en",
      "text": "them that there’s warning signs that the model \nwe’re currently deploying actually is misaligned  ",
      "offset": 9793.2,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "and is scheming in various ways. And they’ll \nthink, if we shut it down, there’s currently so  ",
      "offset": 9798.399,
      "duration": 6.641
    },
    {
      "lang": "en",
      "text": "many people relying on it that that will tank \nour stock price and do all of these things.",
      "offset": 9805.04,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "I think that they should try to game that out a \nbit, in a positive sense of like, how do we get  ",
      "offset": 9810.56,
      "duration": 5.839
    },
    {
      "lang": "en",
      "text": "into that situation where we’ll be faced with \nthis massive conflicting choice. Potentially  ",
      "offset": 9816.399,
      "duration": 5.601
    },
    {
      "lang": "en",
      "text": "it’s the interests of humanity on one side and the \nfinancial interests of the company on the other.  ",
      "offset": 9822,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "Is there a way to not end up in a situation \nwhere turning it off will cause this problem?",
      "offset": 9827.439,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "There might be answers. One example would \nbe they could set up systems to allow  ",
      "offset": 9832.72,
      "duration": 5.679
    },
    {
      "lang": "en",
      "text": "immediate serving of different models. So you’re \ncurrently issuing GPT-5 out there for everyone,  ",
      "offset": 9838.399,
      "duration": 7.681
    },
    {
      "lang": "en",
      "text": "and then you realise it’s misaligned, and \nyou kind of gracefully fall back to GPT-4.",
      "offset": 9846.08,
      "duration": 5.999
    },
    {
      "lang": "en",
      "text": "Or similarly, when we think of emergency brakes \nand things, if you think about an aeroplane or  ",
      "offset": 9852.88,
      "duration": 5.519
    },
    {
      "lang": "en",
      "text": "a car, if something goes wrong: maybe it’s a \nself-driving car and something goes wrong on  ",
      "offset": 9858.399,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "the motorway as you’re hurtling along at 70 \nmiles an hour — probably an emergency brake  ",
      "offset": 9862.96,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "isn’t exactly what you want to do. Suppose it \nsees something with its sensors that’s completely  ",
      "offset": 9868.96,
      "duration": 6.56
    },
    {
      "lang": "en",
      "text": "out of its distribution and just doesn’t \nunderstand what could possibly be happening.  ",
      "offset": 9875.52,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "Probably it should attempt to slow down as \nquickly as is safe and also pull over to the  ",
      "offset": 9879.2,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "left if it’s safe to do so, or to whichever \nside of the road is the side to pull over.",
      "offset": 9884.08,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "So often it’s not just turn \nit off is the answer. Another  ",
      "offset": 9891.04,
      "duration": 6.479
    },
    {
      "lang": "en",
      "text": "example would be switch to the earlier model.",
      "offset": 9897.52,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: And do a handover,  ",
      "offset": 9901.12,
      "duration": 2
    },
    {
      "lang": "en",
      "text": "because you might well need a handover \nto avoid things going really wrong.",
      "offset": 9903.12,
      "duration": 2.389
    },
    {
      "lang": "en",
      "text": "Toby Ord: Exactly. Or switch to manual control or \nsomething like that. But working out how can you,  ",
      "offset": 9905.51,
      "duration": 6.49
    },
    {
      "lang": "en",
      "text": "as quickly as possible, get the troublesome \ncomponent out of the loop and move to some  ",
      "offset": 9912,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "kind of graceful attempt to wind down or exit the \nsituation. I think that’s a very useful concept.  ",
      "offset": 9916.72,
      "duration": 8.16
    },
    {
      "lang": "en",
      "text": "And the emergency brake concept of it would \nbe to say the ability to do it now — like if  ",
      "offset": 9924.88,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "the CEO orders it, that it happens. Maybe do some \ntest runs on it, like a fire drill or something.",
      "offset": 9929.76,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "So that’s an example of me just spending 10 \nminutes trying to think into one of these  ",
      "offset": 9935.92,
      "duration": 5.519
    },
    {
      "lang": "en",
      "text": "ideas that gets kind of bandied about often \nnaively, and there are immediate responses of,  ",
      "offset": 9941.439,
      "duration": 6.641
    },
    {
      "lang": "en",
      "text": "“It wouldn’t work because of X.” And then \nif you think about it a little bit more,  ",
      "offset": 9948.08,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "you think, maybe there are things you could \ndo about X. And if I thought about it more,  ",
      "offset": 9951.2,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "you could start to come up \nwith some clever proposals.",
      "offset": 9955.68,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: I think another one along those \nlines that I’ve toyed with on the show before is:  ",
      "offset": 9960.24,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "many people think that much of the risk comes \nfrom a situation where you have AIs basically  ",
      "offset": 9965.76,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "doing all of the work of programming \nthe next generation of AIs, and humans  ",
      "offset": 9971.2,
      "duration": 3.119
    },
    {
      "lang": "en",
      "text": "largely being cut out of the development loop \nso they’re no longer scrutinising what’s going  ",
      "offset": 9974.319,
      "duration": 3.521
    },
    {
      "lang": "en",
      "text": "on. There’s no longer very much external \nchecking or confirmation from human beings.",
      "offset": 9977.84,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "If that really is the main threat vector, \nit’s kind of an obvious response to be like,  ",
      "offset": 9982.319,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "why don’t we say that we’re not going to cut \nhumans out of the loop, and we’re not going to  ",
      "offset": 9987.2,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "have AIs programming the next generation of AIs? \nThe obvious response is that they’re already kind  ",
      "offset": 9990.72,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "of helping now, and all we would be doing is \ndoing more later on. But the response might be,  ",
      "offset": 9994.72,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "let’s say that we just draw the line somewhere \n— anywhere plausible, anywhere reasonable, any  ",
      "offset": 10000.56,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "identifiable point that we could use to engage \nin enforcement. Would that help? If the answer  ",
      "offset": 10005.279,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "is yes, that any plausible line would, on \nbalance, the benefits would exceed the cost,  ",
      "offset": 10010.399,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "then maybe we should reconsider this idea \nthat initially might sound kind of naive.",
      "offset": 10014.64,
      "duration": 4.15
    },
    {
      "lang": "en",
      "text": "Toby Ord: Yeah, I think that’s right. And \nit can be difficult to draw these lines with  ",
      "offset": 10018.79,
      "duration": 5.529
    },
    {
      "lang": "en",
      "text": "coding assistants and so on. But it does seem \nlike the plan at some of these AI companies  ",
      "offset": 10024.319,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "is to automate AI research with their systems \nand then have them exactly do that and produce  ",
      "offset": 10030.319,
      "duration": 10.08
    },
    {
      "lang": "en",
      "text": "the next set of AI much faster than humans \ncould do. And then that one’s even better,  ",
      "offset": 10040.399,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "so it could do it even faster and so \non to have this kind of hard takeoff.",
      "offset": 10044.479,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "Governments could order them not to do \nthat. I mean, you could even just say,  ",
      "offset": 10048.64,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "“Here is this idea. It has been \ndiscussed. You’re familiar with  ",
      "offset": 10053.6,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "it. You know the words. It’s in your plans. \nYou are definitely not allowed to do that.”",
      "offset": 10056.8,
      "duration": 6.479
    },
    {
      "lang": "en",
      "text": "And I think for a lot of companies, they \nwouldn’t do it even if you had no enforcement  ",
      "offset": 10064.24,
      "duration": 6.96
    },
    {
      "lang": "en",
      "text": "or verification mechanism. A lot of these \npeople, the leaders want to follow the law,  ",
      "offset": 10071.2,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "and the employees want to follow the \nlaw. You know, if their boss said do it,  ",
      "offset": 10077.2,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "and you’re like, “Literally, I can \nsee the directive from the president  ",
      "offset": 10081.6,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "that says you’re not allowed to do \nit,” I think they wouldn’t do it.",
      "offset": 10085.2,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: I think they would probably come \nback with like, “Well, what about like this? Is  ",
      "offset": 10089.52,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "this above the line? Is this below the line?” \nAnd you end up with a negotiation about that,  ",
      "offset": 10092.56,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "where you could actually have a conversation \nabout what is too risky and what is not.",
      "offset": 10095.76,
      "duration": 3.19
    },
    {
      "lang": "en",
      "text": "Toby Ord: Maybe they could say that autocomplete \non your coding thing, like you have at the moment,  ",
      "offset": 10098.95,
      "duration": 3.369
    },
    {
      "lang": "en",
      "text": "is fine and this other thing is not fine, and you \ncould start to zoom in on it. But the idea that,  ",
      "offset": 10102.319,
      "duration": 5.841
    },
    {
      "lang": "en",
      "text": "“It’s hard to know where to draw the line, \nno line would be completely non-arbitrary,  ",
      "offset": 10108.16,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "therefore… there are no rules.” It doesn’t \nfollow. And I think I’ve found myself in the  ",
      "offset": 10113.359,
      "duration": 10
    },
    {
      "lang": "en",
      "text": "grip of this. But now that we mention \nit, I’m a bit embarrassed about that.",
      "offset": 10123.359,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "And you know, COVID brought out a lot of this. \nI think over here in the UK there was the rule  ",
      "offset": 10127.279,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "of four at one point — where no more than \nfour people could be gathered in the same  ",
      "offset": 10131.76,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "place — and then people were like, “What \nif a fifth person comes along? Something  ",
      "offset": 10135.2,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "magical happens?” And the answer is \nno, it’s just one of these spectrums  ",
      "offset": 10139.439,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "where there’ll be more spread if \nit’s five than if it’s four. And  ",
      "offset": 10142.88,
      "duration": 2.479
    },
    {
      "lang": "en",
      "text": "we need to keep the spread below the critical \nnumber so that it goes down instead of up, and  ",
      "offset": 10146.16,
      "duration": 6.88
    },
    {
      "lang": "en",
      "text": "we think that the number’s four. And if it turns \nout it’s still going up, we’ll make it three.",
      "offset": 10153.04,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "Sometimes you need to draw a line \nsomewhere, and the kind of genius move of,  ",
      "offset": 10159.6,
      "duration": 8.879
    },
    {
      "lang": "en",
      "text": "“But there’s no way you can draw it \nso therefore you can’t draw a line” —",
      "offset": 10168.479,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: That’s actually the naive view.",
      "offset": 10171.92,
      "duration": 1.519
    },
    {
      "lang": "en",
      "text": "Toby Ord: It’s quite naive, \nand almost kind of childish.",
      "offset": 10173.439,
      "duration": 4.228
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: All right. We’ve come a \nlong way. Listeners should know that  ",
      "offset": 10177.668,
      "duration": 5.372
    },
    {
      "lang": "en",
      "text": "you’ve cancelled your next appointment to \nstay late and discuss the policy section  ",
      "offset": 10183.04,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "with us. Do you want to give kind of an \noverview of the situation as it stands,  ",
      "offset": 10187.84,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "and what attitude you think people should \nhave, and perhaps what they ought to be doing?",
      "offset": 10192.24,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "Toby Ord: Yeah. Taking this \nkind of zoomed-out perspective,  ",
      "offset": 10196.08,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "the technical stuff I was saying was that \nwe’ve had this scaling law of training larger  ",
      "offset": 10200.96,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "and larger models. And this era of scaling \ntook us really far in terms of capabilities,  ",
      "offset": 10206.96,
      "duration": 7.2
    },
    {
      "lang": "en",
      "text": "as companies untied their purse strings and poured \nmore and more money into this. It let things  ",
      "offset": 10214.16,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "scale up with thousands of times, or more than \nthat, of compute unleashing these capabilities.",
      "offset": 10219.2,
      "duration": 7.76
    },
    {
      "lang": "en",
      "text": "That era, I think, has ended — or at \nleast in its current form. And maybe now,  ",
      "offset": 10226.96,
      "duration": 7.359
    },
    {
      "lang": "en",
      "text": "as we try to scale up inference instead, maybe \nthings will stall out a bit — especially as we  ",
      "offset": 10234.319,
      "duration": 7.841
    },
    {
      "lang": "en",
      "text": "apply it to the problems where it’s useful \nto think longer, and then wherever it’s not  ",
      "offset": 10242.16,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "clear that it’s always useful to think \n10 times as long as you previously have.",
      "offset": 10247.359,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "So it could stall out for various reasons, \nor it could be that once you’ve got this  ",
      "offset": 10251.12,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "form of intuitive thinking, plus \nthe systematic form of thinking,  ",
      "offset": 10256.08,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "you can combine them in some way that really \nleads to explosive growth, as we discussed.",
      "offset": 10259.04,
      "duration": 5.279
    },
    {
      "lang": "en",
      "text": "So I think it creates more uncertainty about \nwhether timelines are going to actually maybe  ",
      "offset": 10264.96,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "stall out and be quite long, or whether maybe \nnow we’re going to be able to have this extremely  ",
      "offset": 10270,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "rapid progress that the companies are themselves \npredicting. So that’s something different.",
      "offset": 10274.24,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "But also, it’s not just the timelines — it also \nchanges everything. The way that in the case of  ",
      "offset": 10277.76,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "inference you have to pay the costs every single \ntime you use it has all of these different types  ",
      "offset": 10283.84,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "of knock-on effects that can really change things \nin more complicated ways than just good or bad.",
      "offset": 10288.96,
      "duration": 5.439
    },
    {
      "lang": "en",
      "text": "And then, as we moved on to these bigger-questions \nof policy, my main message is that the whole  ",
      "offset": 10294.96,
      "duration": 8
    },
    {
      "lang": "en",
      "text": "landscape of AI has changed so much over the \nlast five years. Then maybe in the last year  ",
      "offset": 10302.96,
      "duration": 5.519
    },
    {
      "lang": "en",
      "text": "we’re seeing another one of these types of \nchanges, and we could see many more of these.  ",
      "offset": 10308.479,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "So it’s important for at least some people — \nI think more people than are currently doing  ",
      "offset": 10314.479,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "it — to keep an eye on the big picture of \nhow the landscape could be so different,  ",
      "offset": 10318.479,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "and that maybe we could actually help to \nsteer it towards some of these locations.",
      "offset": 10323.76,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "And to realise that it’s not just in the grip \nof some kind of technological or incentives  ",
      "offset": 10328.319,
      "duration": 6.96
    },
    {
      "lang": "en",
      "text": "determinism, to assume that humanity just \nreally has no choice: “If it turns out the  ",
      "offset": 10335.279,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "most efficient way to do things is this one that \nleads to disaster, then I guess we’re forced to go  ",
      "offset": 10339.68,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "into a disaster.” That’s just not true, and it’s \nexcusing ourselves of too much responsibility.  ",
      "offset": 10343.12,
      "duration": 6.72
    },
    {
      "lang": "en",
      "text": "Ultimately, if we build a technology that kills \nus all, it’s on us: it’s an own goal by humanity,  ",
      "offset": 10351.12,
      "duration": 6.159
    },
    {
      "lang": "en",
      "text": "and someone has to admit responsibility \nfor that. We can’t all just say it was…",
      "offset": 10357.279,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: “It was the incentives that \neveryone else created for me to do it,  ",
      "offset": 10361.84,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "because I thought that they would do \nit. I didn’t talk to them, but yeah.”",
      "offset": 10366.08,
      "duration": 3.429
    },
    {
      "lang": "en",
      "text": "Toby Ord: Exactly. So I’m a big fan of the big \npicture of everything, but I hope it’s been  ",
      "offset": 10369.51,
      "duration": 9.21
    },
    {
      "lang": "en",
      "text": "useful for other people, and that more people \nwill start thinking about these things too.",
      "offset": 10378.72,
      "duration": 6.148
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: Yeah. Well, I look forward to \ncoming back in a year or two, or possibly  ",
      "offset": 10384.868,
      "duration": 3.212
    },
    {
      "lang": "en",
      "text": "less, and talking about what the new technical \ndevelopments are and what they imply. And perhaps  ",
      "offset": 10388.08,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "people’s minds will be a little bit more open \nby then. There’ll be more things on the table.",
      "offset": 10392.16,
      "duration": 3.99
    },
    {
      "lang": "en",
      "text": "Toby Ord: Yeah, I’ll be there.",
      "offset": 10396.15,
      "duration": 1.197
    },
    {
      "lang": "en",
      "text": "Rob Wiblin: My guest has been Toby Ord. Thanks so \nmuch for coming on The 80,000 Hours Podcast, Toby.",
      "offset": 10397.347,
      "duration": 3.603
    },
    {
      "lang": "en",
      "text": "Toby Ord: Thank you.",
      "offset": 10400.95,
      "duration": 5.69
    }
  ],
  "cleanText": "Rob Wiblin: Today I’m again speaking with Toby Ord. Toby is a senior researcher at Oxford University, and his work focuses on the biggest picture questions facing humanity. He’s probably most well known to listeners as the author of *The Precipice: Existential Risk and the Future of Humanity*, which made quite a big splash back in 2020. Welcome back on the show, Toby.\n\nToby Ord: It’s great to be here.\n\nRob Wiblin: So today I want to take a bunch of technical developments that have been going on in AI over the last couple of years, and try to explain them in a way that almost everyone can understand — and then also explain what implications they have for our lives, for what sort of things we should expect from AI in coming years, and what implications they have for AI governance and policy in particular.\n\nBut first, I wanted to talk a bit about this blog post that you wrote or this presentation you gave last year called “The Precipice Revisited.” *The Precipice* was this book that you wrote in 2018/2019, and came out in 2020. It explored the science behind all of the different major threats to humanity’s future: pandemics, asteroids, AI of course, nuclear war, that sort of stuff. Of course there’s been lots of developments since then. I think last year you wanted to look back and say, over the five years since you wrote it, what have been the major changes in the picture? Is humanity in a better situation? Is it in a worse situation? What have been the major changes? And in particular on AI, where so much has been going on?\n\nToby Ord: Obviously lots of changes in pandemics. We had COVID hit us, mRNA vaccines, so on and so forth. And nuclear war: the prospects of that felt like a distant memory back in 2019, and now it’s become more of a realistic possibility. But AI is where the most changes have happened. So if we cast our minds back to 2019, the state-of-the-art AGI-type systems were reinforcement learning systems created by companies like DeepMind and OpenAI. So think of things like AlphaGo: this is a system that’s learned to play Go. Particularly AlphaGo Zero learned to play Go by playing huge numbers of games against itself — and in doing so, it kind of blasted through the human level of game-playing performance and then reached these kind of lofty heights of superhuman abilities. And similarly with StarCraft and Dota, some other games where you could do a similar thing: reinforcement learning lets you learn skills that are beyond what humans already have because you’ve got some way of rating really good play objectively. So we had those types of models, and they were actually quite narrow. DeepMind was very excited with AlphaZero because it could play three different games: it could play chess and shogi and Go, but it couldn’t play games involving chance or games involving imperfect information — so card games, for example, or things that aren’t games and so on. And it was in some sense general, but much less than we’ve seen since then with the rise of LLMs. The rise of large language models has been this spectacular improvement in generality — where, using text as an interface to talk to these things, you can talk about any topic that you would talk about with humans. And that’s something, as Alan Turing foreshadowed with the idea of the Turing test, where you can then quiz it in order to test it: you can ask it about all kinds of obscure topics or the topics you know it will do worst at and so on. And these systems actually have surprisingly good performance across the board, in a way that’s hard to quantify, but I would say it’s thousands of times more general than something like AlphaZero.\n\nRob Wiblin: So we’ve had the switch to LLMs. Have there been any other major developments?\n\nToby Ord: Yeah. As part of the switch to LLMs, we’ve had this situation where, previously with reinforcement learning and these Go-playing things, there was this question of how would you ever get human values into such a system? It couldn’t even understand human values.\n\nRob Wiblin: It had none of these concepts.\n\nToby Ord: No. Even if it was playing a game with little sprites made up of pixels that get hit by sprites that represent bullets, it’s not clear that it’s doing something like killing as we understand it, as opposed to just a move in a game, like taking someone’s knight in a game of chess. And there was this big question about how would you even get the complexity of our values into these systems? But with the idea of training on vast amounts of text, we’ve got this world where now these systems, if you quiz them about, “Would anyone be slighted or upset if you were to do this action?” — or even asking questions about advanced things in moral philosophy — it can often get the right answers to these types of questions, or the answers that we think show knowledge of what humans think is morally appropriate. But there still is a big question there: Does that knowledge of morality actually guide it? Has it internalised that? But it’s such a difference, though, that it now at least has most of that knowledge.\n\nRob Wiblin: Yeah, I think on a lot of moral philosophy questions and even social norm questions, it often probably outperforms humans in many cases, at least in typical examples.\n\nToby Ord: Yeah, but there still is that question of —\n\nRob Wiblin: It may know, but doesn’t care.\n\nToby Ord: Exactly. And as well as these changes to the technology, there’s also been a lot of other changes to the landscape. So in order to power this, it’s been extraordinarily expensive. It’s required this scaling up of compute infrastructure, and this required huge amounts of money. So it required very large investments from Microsoft and Google and Amazon and others. In 2019, there was already a race between these AI labs, these relatively small groups of people focused on this technology. But now these trillion-dollar companies have been brought into that race, and it started to contribute to their bottom line. When Microsoft applied this in search through Bing — going after Google’s crown jewel of search — all of a sudden the primary way to make money for a trillion-dollar company was on the line. It brought in these very large financial interests into this race, so it really heated up the race.\n\nRob Wiblin: Back in 2019, I think in *The Precipice*, you estimated that the chance of humanity losing most of its potential future value due to AI in this century was around 1 in 10. Would you say that number has drifted up or drifted down?\n\nToby Ord: I’m not sure. For a lot of the other risks, it was easier to see whether it’s gone up or down. For this one, I think that we’ve really been gifted a relatively good situation in terms of the way the technology has panned out, in that it’s this technology that imitated human values and human reasoning and so on by training on this huge corpus of human data. I think that has just been tremendously helpful. And the fact that it’s not an agent by default. Real gifts. It wasn’t that we steered towards that because we knew that would help with safety. It’s just that that turned out to be the easiest way. And I don’t think we quite recognise that enough: that the biggest effects on whether we’re safe or not have just come from somewhat random aspects about this technology landscape, rather than deliberate attempts to steer it. But I am very concerned about the racing, and I’m concerned that we’ve seen evidence that the players who are trying to make these systems are ultimately going to cut corners in order to win these races.\n\nRob Wiblin: OK, let’s talk about this series of articles you wrote about how there’s been a change in what is getting scaled up as the companies are trying to make these models more powerful. The change that you describe in these articles, which people can find on your website, tobyord.com. The article is called “Inference scaling reshapes AI governance.” Basically, the companies are now using more of their compute during the inference stage rather than during the training stage. I think most listeners will probably know what compute and pre-training and inference are, but maybe you can explain that, so that everyone is completely following? And then explain the difference: what’s changed?\n\nToby Ord: Yeah, exactly. So compute is a somewhat ugly word for computation. It just means how much computer processing has happened. So it doesn’t include things like RAM or memory, but just how many steps, basically. And what people have found is that you can scale up the amount of compute that goes on, where you’re processing more data and so on, building a bigger model, and you get much better performance. That process of building a model and training a model gets broken up into two stages, which we call pre-training and post-training. It makes it sound like they come before training or after training, but really it’s just the first part or the second part. Pre-training is the one that I think a lot of people are familiar with: where you take a system and you take some text, and it hears the first four words and then it tries to guess what the fifth word will be, for example. Then you modify the weights on it in order to make it so it would have been slightly more likely to say the correct word next. So that’s this kind of glorified auto-prediction type thing. And that produces something called a “base model.” And then there’s a whole lot of post-training applied to it — for example, to make it refuse to do harmful things, and to make sure it’s honest, and to make sure it can follow instructions and things. That’s all called post-training. But what we’ve had so far is a huge scaling up of pre-training via something called scaling laws, and then an increasing amount of post-training to make it quite a lot better after you’ve built this huge thing. But now, ultimately, there’s been a shift from scaling up more and more of this pre-training to scaling up something that happens after the whole training process, called “inference.” Inference is basically using that model, so using it to produce a whole lot of text.\n\nRob Wiblin: I guess inference is like the thinking that it does while it’s trying to answer a particular question and get back to you, or figure out what to do next.\n\nToby Ord: Yeah, that’s right. So here’s how I think about this. I think it’s a useful analogy. Suppose you’ve got a company, and you’re trying to get some excellent work done, and you could employ someone. Pre-training is like sending them through high school and then to undergraduate and then maybe to grad school. You’re putting in more and more expense into having this person learn more and more about different things, so they’ll have a lot of extra knowledge at their fingertips. And that’s what most of the scaling had been doing. But then inference is like letting that person spend more time actually doing the job. So suppose that you give them some brief, that they’ve got to prepare a report for a client. By default, if you just ask one of these language models to do that, it just extemporises stuff. So it’s just saying the words as they pop into its head. And it doesn’t have a chance to do a second draft; it just has to compose this document in one go. So you could think of that as that the pre-training has given it this really powerful kind of “System 1” ability, in these terms from human psychology: so the intuitive ability to just answer things straight off the bat. Then you’re just asking it to keep doing that as it goes through all the sentences of this report that it’s composing. Whereas what you could also do is let it spend a long time in that process, maybe spend 10 times as much on writing — where it could write an answer and then it could critique the answer, it could modify things, move things around — and then ultimately hide all of that working and just show you the final answer. So that’s like saying you don’t just have to write this report for the client in 10 minutes, but rather we’re going to scale that up to 100 minutes or 1,000 minutes. It turns out, as you get with an employee, you get much better work out of someone if they’re doing that. And that also gives room for this different kind of intelligence that we call “System 2” in human psychology. These are often called reasoning models, where it’s able to do a certain kind of structured thinking and apply that. So pre-training scales up the System 1, and then this inference scaling lets us spend more time on a task to hide all of that work and just show the final thing — and we could think of that as kind of scaling up its System 2 abilities.\n\nRob Wiblin: As I understand it, we’ve had this shift from using compute during training to during inference or question answering, basically I think because in 2023 and 2024 the main companies started training bigger models than GPT-4 and they found that they were running out of juice, that actually this was much more expensive: they were using a lot more chips and a lot more electricity, and the performance just wasn’t increasing at nearly the rate that it had been before. Perhaps because they were kind of running out of high quality data to actually train on; they’d soaked up all of the good books and Wikipedia and all of that. But at the same time, they were finding new ways of putting scaffolding around these models that would allow them to answer a question, critique it, think about it, to basically get more juice out of giving them additional time to\n\n\ntry to answer a question to a higher standard in a more intelligent way. Is that basically the story?\nToby Ord: Yeah. At least I think that that is correct. It’s somewhat disputed. So late last year there were a series of articles that came out in different publications in the media reporting that behind the scenes OpenAI had been disappointed by their next bigger model that used 10 times as much compute as GPT-4 — this is now what we call GPT-4.5. And they’d been really disappointed with the results. If you put in 10 times as many inputs into something, you hope to get some noticeable improvement. And they found that it wasn’t actually that clear, and it was worse at quite a few things.\nThen there were similar reports coming from the other leading AI companies, so this was a little bit concerning for this narrative of continuing to scale these things up.\nSo that really is how everyone had been thinking about it. For example, the paper “Situational awareness” by Leopold Aschenbrenner paints a picture based on scaling up pre-training I think a million times further than where we’re currently at — just continuing to do that, and then painting a picture of what would happen if that curve continues to go. Whereas what seems to have happened is that it’s already kinked right at the point where GPT-4 was out and before GPT-4.5. So at the time he published the essay, it seems like maybe that’s actually not what’s happening.\nThat said, it’s very difficult to know what the curve of actual performance looks like. Some people say GPT-4.5 is really impressive. I find that a little bit hard to believe. If you look at the actions of the company… For example, GPT-4 was this massive announcement, right? This breakthrough technology, and everyone was oohing and ahhing about it. Entirely new benchmarks had to be created in order to measure its new capabilities. There was this “Sparks of AGI” paper and so on. A massive improvement from GPT-3.5.\nWhereas GPT-4.5 was just announced I think on a Friday, kind of like we’re trying to bury the story. And then they announced I think a month after it launched that they were going to end-of-life it this summer. They also declared that it wasn’t even a frontier model — so they said this is not even an example of anything that should concern people or is pushing the industry forward.\nThat’s really remarkable. I would have been totally shocked if you told me when GPT-4 came out that its successor would be basically buried by the company that was creating it. So I think there really has been a kink in the curve here — but it is difficult to measure, and not everyone agrees.\nRob Wiblin: Yeah, I think that is very likely a win for the people who said pre-training is kind of hitting a wall. At the same time, in the broader picture, the people who were pessimistic about AI progress have probably on balance been wrong — because they’ve found other things that can be scaled that nonetheless produce the output that we care about: more capable, more impressive models.\nCan you explain what’s been going on with the scaling of inference and what impacts it’s had?\nToby Ord: Yeah. So the AI companies have often said what’s really important isn’t that this pre-training scaling continues, but that some kind of scaling continues: that there’s still some way that we could pour in more and more compute into this process, and we’ll get more and more kind of cognitive capabilities coming out the other end.\nI think that makes sense, but it’s not at all clear that this will continue to provide the same types of benefits we’ve seen. It’s really quite a different process and it’s as different in some cases as the difference between a stock and a flow. So I think there’s a lot of conceptual confusion about this.\nThe key idea here is: suppose you scale up the pre-training by a factor of 10. So you put 10 times as much compute into learning how to have these good kinds of intuitions and good responses to prompts. Then you do have to pay some additional cost every time you use the model, because you generally need to add more weights as well when you do that.\nBut if you instead try to get that same level of capability improvement by training up inference — so training up the amount of time it spends on every task — then you have to pay that full scaleup every time you use it. And the way that the maths turns out — it’s a little bit complicated under the hood — but it turns out that for every tenfold increase in pre-training that you instead get the benefits by using inference scaling, you do have to pay 10 times as much every time you use it. And that can change everything in terms of the economics of these companies.\nRob Wiblin: So I think one of the positive implications that this might have going forward is that we might expect AI that is both human level and as general as humans to arrive more gradually. And the same could be true for superhuman AI: that we could get glimpses of what superhuman AI and what equally general superhuman AI might look like maybe years before it’s ever actually practical to use on a broad scale. Can you explain why that’s the case and what effects that has?\nToby Ord: Yeah. So each of these GPT levels, we could think of the move from GPT-2 to GPT-3 and GPT-3 to GPT-4, were something like 100 times increase in the amount of compute that was used for them. And in order for things to get really crazy, to have a model that has transformative effects on the world, suppose we need to get to GPT-6 level in those terms — so to go up by a factor of 100 from GPT-4 and then another 100 beyond that.\nIf we are trying to get those advantages through inference scaling, then that means we have to spend 10,000 times as much money every time we want it to do something, compared to if we’d done it the other way around. That’s a big difference.\nA kind of key parameter for how will the arrival of greater-than-human level of intelligence shape society is what’s the cost of it? Will it cost more or less than human wages for doing the same thing? And obviously it could depend on exactly what it’s doing. And human level will be different; it will arrive at different times for different types of tasks. But roughly speaking, you can think of this kind of parameter.\nSo suppose a system comes out and it costs 10 cents an hour to do human-level work. That’s going to have massive implications. It’s basically free, and companies would be attempting to shift all kinds of jobs onto this thing. But if instead when it comes out, it’s $10,000 an hour, then that’s something that might not affect things very much at all at first.\nSo if we scale up the systems, we try to get the capabilities we thought you’d need a GPT-6 to reach — but ultimately, if that pre-training is kind of fizzled out and we’re getting it all from inference scaling, so we need to put in 10,000 times as much compute every time we run it, it’s going to cost 10,000 times as much money. So it could be that we’re on track to get something that costs a dollar an hour, but instead it costs $10,000 an hour, and it’s totally different effects on the world. That’s a key example of how this could change things.\nRob Wiblin: In a sense, it’s a much more reassuring picture. If you were worried about rogue AI, then this is a much nicer picture, because at the point that you have a superhuman AI that conceivably is motivated to try to take over, there wouldn’t be enough compute in the world to run enough instances of it to actually do the necessary work to try to stage a rebellion. So that would, at least at the early stages, potentially be off the cards. And I suppose the model itself would probably realise this.\nBut in the meantime, if you were willing to spend the money, then you could actually study these models that you predict will one day be cheap, maybe in three or four years’ time. You could study them in great detail and understand what motivates them, try to figure out how you motivate them to actually pursue the goals that you have. So it’s a lot better from that point of view.\nI guess even from a governance point of view, you could learn more about what will these models look like in four years’ time, when they actually are economically relevant, and then think about what governance solutions might be appropriate if that is how things are going to look.\nToby Ord: Exactly. So I’m not saying that that would stay expensive forever — there’s been a long history of things getting cheaper and cheaper — but rather that rate at which things get cheaper would be what would introduce it into society, rather than the moment when the training run is finished and then the company switches on their public release being kind of a cliff edge. Instead it would be the case that every few months or something, it costs half as much and eventually $10,000 an hour, then $5,000 an hour, $2,000 an hour, $1,000 — and new groups would start to want to use it at each of these price points and so on. It’d be more of a smooth transition.\nAnd if you ask, when would you want to spend a million dollars an hour on an AI system, supposing it costs that much? I think that there are some answers. One example would be to give a six-hour-long demo of superhuman intelligence in front of the General Assembly in the United Nations: that could be worth spending say $6 million in order to provide that demo, to really show and let people kind of kick the tires on this thing and see that this is what’s coming soon. So it might enable those types of things by smoothing things out with this cost change.\nRob Wiblin: Two things to note there are: I suppose roughly we might say that the cost in the near term might drop by something like an order of magnitude a year perhaps. I think not quite that much. So you might get like a 10x cost decrease on these models each year. So that gives you some sense of if you’re willing to spend $10,000 an hour today, then you could study something that in three years’ time might cost $10 an hour. So it’s coming at us pretty fast, but at least it does give us forewarning. But it does mean we have to be willing to spend the money.\nToby Ord: It does. But also those orders of magnitude are not going to come forever. It’s not the case that it’ll be 10 times cheaper then 10 times cheaper and then 10 times cheaper, and that just goes on for infinitely many orders of magnitude. These cost reductions will eventually hit a floor, and we don’t know where that floor is. It could be that it stalls out while still being too expensive for almost all tasks, requiring some kind of paradigm breakthrough in order to push it forwards. I think that there’s often a feeling that there’s some unlimited number of these orders of magnitude that are like manna from heaven.\nRob Wiblin: Because these models seem so much less energy efficient than human beings, I think that gives some people a sense of the technological frontier, at least demonstrated by human brains, suggests that we’re a very long way from where we could be at some future time. So that gives us some hope that the efficiency increases might be substantial for a while at least.\nToby Ord: I think that that’s right. That said, the entire paradigm of LLMs and their scaling laws is really inefficient in terms of the data that’s needed to train them. I think that suggests that the thing that might make that change might be more of a paradigm shift or some real breakthrough in the efficiency.\nRob Wiblin: Yeah. I think another thing that people need to note is that it’s not the case that we can see an unlimited distance in the future. It’s not if we were willing to spend a billion dollars an hour that we could exactly see how things are going to look in 2032. There’s a limit. And I suppose the further out you go, the more you might have changes to the architecture or the entire nature of these models. So it could make you a little bit complacent if you’re saying, well, we spent a whole tonne of money and now we can see how the models are going to be in 2029. It gets more and more shaky the further out you go.\nToby Ord: Yeah, that’s right. One way to think about what the companies have been doing with this inference scaling, like, what have they been working on in order to scale this up? Is it the case that there’s just a knob where we can just turn it now and we can witness what happens if we put in a million times as much money into this thing?\nThe answer is not really. The main thing that is being done is to try to make them coherent over longer and longer time periods, or longer and longer numbers of words that they can say in a row while still being on topic and doing useful work.\nAnd in a lot of these cases, the amount of words is so large that it’s not shown to the user — that’s considered to be its reasoning trace or something like that. You could think of that as all of the subvocalisations that the employee had while they were preparing the report that took them possibly quite a long time to write, and instead the finished report is what we show. But every time you want to make that chain of thought 10 times longer, they do become incoherent, unless you put in a lot of effort on reinforcement learning to try to train them to stay coherent.\nRob Wiblin: I think another shift that you get from the intelligence increases coming from putting more compute at\n\n\nThe inference stage\nis that in general, information security,\njust as a whole, becomes a lot less important —\nand indeed, the weights of the model become less\nimportant. So the picture around open sourcing\nmodels changes quite a lot. Can you explain this?\nToby Ord: Yeah, that’s my hypothesis here. So if\nGPT-4 is basically a kind of kink in the curve\nof how impressive the pre-trained models get,\nsuch that you get really diminishing returns\nbeyond that point, then one thing is that\nthere may just be GPT-4-level models that are\nput into the public domain and then it’s all\nover. There’s no more future model like that to\ngo. So it could become moot in that direction.\nBut it also becomes less interesting even from\njust the open source community’s perspective.\nWhereas the way that it worked up until now\nwith this pre-training scaling was that the\nlabs invested a vast amount of resources, a\nhuge amount of money in data collection in\norder to produce this model, this collection\nof trained weights. And then they’re giving\nthat to you — and you, the user, can use all\nof that kind of embodied intelligence in it.\nWhereas now what they’re saying is that\nevery now and then you have to spend 10\ntimes as much compute while running it in order\nto make it more intelligent. And you, the user,\nis going to have to spend that compute. You’ll\nneed your own GPUs and things in order to be\nrunning these things. And you’ll need 10 times\nas many, and then you’ll need 100 times as many.\nMaybe we’ve done the work to keep it\nconsistent over that time. So there’s\nstill some advantage to getting the latest\nversion of these weights that can stay on\ntopic for longer periods. But it’s kind of\nlike “bring your own compute” for the users.\nSo that story is less exciting than if all the\ncompute was done by Meta or something like that.\nRob Wiblin: Yeah. So if the breakdown is that 99%\nof the total compute that is going towards these\nAIs in general is occurring at the training stage,\nthen they’ve paid for all of it at the point that\nthey give you the weights for free. Then it’s\nlike it’s basically a free fee to operate.\nIf it’s the other way around, if 1% of the\ncompute that is going towards AI as a whole is\nat the training stage and 99% of it is at the use\nstage, then they haven’t really been especially\ngenerous or especially useful to you in giving\nyou the weights — because that’s not where the\ncost is actually incurred; it’s all at the point\nof actually applying it to solving some problem.\nSo I guess we wouldn’t have to be as worried about\npeople being able to suddenly gain an enormous\namount of power if they stole the weights, or\nthe weights were leaked, or something dangerous\nwas open sourced, because it would simply be\nvery expensive to apply it to actual practical\nproblems. But at the same time, the benefits\nof open sourcing the stuff is not so great.\nToby Ord: Yeah, I think that’s right. There\nis the alternate thing of, if it’s the case\nthat even relatively small sets of weights —\nthe type that have perhaps already been open\nsourced — if you could actually take those\nthings and then kind of soup them up through\nbringing your own compute, in some ways that\nmakes the issue of proliferation kind of worse.\nSuppose you’re a very well-resourced\nactor, in terms of having a lot of compute,\nand you get some of these weights: then you\ncan really turn it into something amazing.\nMaybe. But if you had all that compute in\nthe first place, you could have just trained\nyour own model. So I’m not sure, but I just\nwant to say that it’s a little bit unclear.\nBut I think the overall effect is that\npreviously there was all of this kind of\nvirtual compute or something distilled out\ninto these weights. Then you could have the\nweights which represented the huge amount of\neffort that had gone on before that point.\nAnd if that’s no longer true, and the answer\nis you’re mainly bringing your own compute,\nthen the story for either being a legitimate\nopen weight user or for being a spy who’s\nhacked in and stolen the weights, in both cases,\ngetting these weights becomes less important.\nRob Wiblin: A related impact that this would\nhave is that the AI market is more likely to\nremain competitive than it would if there\nwas this enormous fixed cost that you had\nto incur at the point of training\na model. Can you elaborate on that?\nToby Ord: I think this is right.\nOften this process of these kinds\nof massive pre-training runs is\nlikened to software development:\nit’s something where you go to a lot\nof effort to write a piece of software,\nand then there’s zero marginal cost or\nvery small marginal cost to distribute it.\nThat was true to some degree with books, where\nyou go to a lot of effort to write a book and then\nprinting it is a lot less. But once software could\nbe distributed on CD or then just downloaded, it\nbecame trivial costs to the company to, say, have\nan extra copy of Microsoft Word distributed to a\nuser. So once you write a word processor, you do\nall of the software engineering for it, you really\nwant to sell it to a lot of customers, because\nevery customer is basically just pure profit.\nThat’s this zero marginal cost type thing. And\nif you have that in an industry, then you tend\nto get a small number of players — because you\nreally want to be the best one of these things\nand then potentially just swallow up the\nwhole market. Whereas if you enter it\nas a new entrant, you put in all these\nupfront costs. If you’re the third-best one,\nwhy would anyone use your thing? Even if you sell\nit for less, it’s very hard to be a player there.\nWhereas this could change that aspect of it,\nand mean that most of the costs are actually\nin producing each item. A little bit like, let’s\nsay you develop tools for a hardware company —\nphysical tools: hammers, screwdrivers, and things\nlike that — then it’s the case that some of the\ncosts go into designing the new hammer, but most\nof it is just that every hammer you make costs a\ncertain amount of money, and then you get a kind\nof limited amount of profit when you sell it.\nSo it might be becoming more\nlike an industry of that sort,\nand that would have a different\nkind of market structure.\nRob Wiblin: It suggests to me that then more\nof the profit that’s coming from AI would go\nto the hardware companies, because they’re the\nones who actually have the scarce resource,\nor at least temporarily scarce\nresource. So it’d be much harder,\nI think, for the software developers\nto gain a huge margin, because there\nwould be many of them with roughly\nsimilarly powerful and useful models.\nWhich is kind of where things stand\ntoday: there’s at least three — or four,\npossibly — models that are roughly at\nparity, which means that then where does\nthe surplus go? It probably goes to the hardware\nproducers who have this kind of scarce resource,\nwhich is the thing that you desperately are trying\nto acquire in order to be able to apply them.\nToby Ord: That sounds about right. In\ngeneral, there’s always this question\nfor the companies that are trying to make a\nlot of money with this: which step of this\nvalue chain makes the most profit? What we\nhave at the moment is that the final stage\nof these people who’ve trained these models is\nreally quite competitive between a handful of\nstrong players. Whereas the step before that, of\nthe most powerful GPUs, is really locked up by\none main player. So they’ve got more ability to\nget a kind of monopoly pricing in at that point.\nRob Wiblin: I guess we should say that this is\nwhere things are trending as a result of this\nswitch towards inference scaling. It won’t\nnecessarily go all the way to there. It is\ninteresting to know, because I think just a\ncouple of months ago I recorded an episode\nwith Tom Davidson talking about the risk of\nseizure of power. He was describing this thing\nwhere maybe almost all of the fixed costs are\nat the training stage, and that would tend to\npush you towards a market with a handful of\ncompanies, or possibly even at some stage,\njust one company, that is willing to spend the $10\ntrillion on the super training run that produces\nAGI. I think that is still possible, things could\nchange, but that’s looking a bit less likely now.\nToby Ord: Yeah, that’s right.\nRob Wiblin: So I guess that has been the\nkind of positive or neutral effects here.\nI think one that people might\nfind a little more troubling,\nthat might have jumped off the page at them\nalready, is: if you’re in a world where you\ncan access superintelligence earlier if\nyou’re willing to spend a tonne of money,\nthen that suggests that rich and powerful and\nmore connected people will be able to access these\ntools potentially many years ahead of the general\npublic. What do you make of that implication?\nToby Ord: Yeah, I think there’s a real\neffect there. I think we’ll look back on\nthe period that’s just ended, where OpenAI\nstarted a subscription model for their AI\nsystem — where it was $20 a month, less than\na dollar a day, to have access to the best\nAI system in the world, and then a number of\ncompanies are offering very similar deals…\nWe’ve got the situation where, for\nless than the price of a can of Coke,\nyou can have access to the leading system\nin the world. And it reminds me of this\nAndy Warhol quote about what makes America\ngreat is that the president drinks a Coke,\nLiz Taylor drinks a Coke, the bum on the corner\nof a street drinks a Coke — and you too could\nhave a Coke! The best kind of sugary beverage\nthat you can get, everyone’s got access to it.\nBut I think that era is over. We had OpenAI\nintroducing a higher tier that cost 10 times\nas much money, because these inference costs\nare going up and actually they can’t afford\nto give you this level for the previous cost.\nAnd this is what you’re going to keep seeing:\nthe more that we do inference scaling, it’s\ngoing to have to cost the users substantially\nmore. And then there’s a question of\nhow many are prepared to pay that.\nSo it’s certainly going to create inequality\nin terms of access to these things,\nbut it also might mean that it is not\nactually scaling well for the companies.\nIf it turns out that you offer a\nthing that costs 10 times as much\nand less than a tenth of the people take it,\nand then you offer a thing that costs 100 times\nas much and less than a tenth of the previous\ngroup that took the first one take this one,\nthen maybe each of these tiers is earning\nyou less and less money than the one before,\nand it’s just not actually going to drive your\nability to buy more chips and train more systems.\nOr it could go the other way around: it could\nbe that a fifth of people are prepared to pay\n10 times as much and then a fifth of them\nare prepared to pay 10 times as much again,\nand that you’re getting more and more\nmoney from each of these higher levels.\nBut which of those it is could really determine\nwhat happens in the industry and whether these\ninference-scaled models are actually\nprofit centres for them or not.\nRob Wiblin: Yeah. So some of the previous\nchanges have slightly reduced our concern\nabout concentration of power and risk\nof seizure of power by human beings.\nBut this particular issue — that a privileged\ngroup of people can gain access to superhuman\nadvice and superhuman assistance potentially\nsubstantially before anyone else in the world\nhas access to it — heightens the concern\nthat people at the companies or people\nin the government who might take control\nor get privileged access to these models,\nthat they could potentially outfox everyone\nelse if they’re able to basically just have\naccess to tools that no one else\nis really able to compete with.\nToby Ord: So while in theory this could happen\njust on the open marketplace with money, my\nconcern would be greatest about the company itself\ndeciding, for example, “We’ve got this model,\nwhat can we use it for? Maybe we can be willing\nto spend a million times inference scaling on it\nto do some really important work for us.” And\nthe company might want to do that internally\nor the government of the place where the company\nis located might want these types of abilities.\nSo I imagine that happening outside the open\nmarket is perhaps the most concerning place.\nI should say as well that I’m imagining or\nthinking about all of this over the next\ncouple of years. I’m not claiming that in the\nlong-run equilibrium, when we’re imagining,\nin a post-AGI world, how unequal will\naccess to AGI be? It could be very unequal,\nor it could be very equal if we actually\nchoose to build a world like that. But\nI’m setting that aside, because I\nthink that I can only really see\nwhat’s going to happen for the next couple\nof years, and things may change after that.\nRob Wiblin: What are the policy implications here?\nOne that stands out is that you might want to\ninsist on some level of transparency about what\nis possible at the frontier, if you’re willing\nto spend a whole lot of money — just so that\nthe public and people in government have some\nsense of what’s coming, and that companies can’t\nhide this if they would rather maybe obscure what\nthey already are aware is possible if you’re\nwilling to spend a million dollars an hour.\nToby Ord: Many of the current rules — to\nthe extent to which there are rules at all;\nthere are voluntary commitments and there’s\nalso the EU’s AI Act — they’re often focused\non deployed models. This means that you\ncan circumvent a lot of this if you just\ndon’t deploy it. So maybe you have these kind of\nhigher tiers of inference scaling that are only\naccessible internally; then you\ncould have systems that are, say,\nbreaking through this human range\nof abilities without anyone knowing.\nWhereas in this Andy\n\n\nWarhol Coke world, where everyone’s got access to the cutting-edge system, we kind of all knew that the people working at those AI companies had the same thing. Or that if they had something better, within a few months we’d also have it, or something like that. So yes, I feel that governments and regulators generally need to ask for more transparency in this world to know what the capabilities are for the leading-edge internal models, as well as the deployed ones.\n\nRob Wiblin: Other probably negative implications of inference scaling are that it makes regulation of AI just substantially more difficult in a number of different ways.\n\nOne thing is, up until recently, you want to carve out the models that you think are not particularly risky — that are basically just applications that we should feel not only OK with, but actively excited about. And then you want to carve out the things where we don’t know what this model is potentially capable of; this is posing novel risks that we’ve perhaps never seen before, and we want to at least do some research and study it before we deploy it, or possibly even before we use it internally. And to do that, we’ve used these compute thresholds — where we’ve said, if this is more than 10 times larger than any model that’s been trained before, then it falls into the “let’s study this first” regime. And if it’s smaller than things that have already been trained, then we’re probably in the clear and we can use it with a reasonable degree of comfort. Can you explain why inference scaling makes this so challenging to actually do?\n\nToby Ord: So that paradigm of compute governance via these thresholds, you could think of it as trying to regulate a particular object. What they’re saying is that if this object has gone substantially further than any that have come before, in terms of what’s gone into it, then we could try to regulate it. So these trained weights are the object of interest. It’s a little bit like having regulation on automatic weapons, but not on non-automatic weapons. Something like that, where you take a particular class of object and you put a regulation on it. Whereas what we’re getting with inference scaling is it’s not the object itself; it’s more what you do with it that matters. It could be that you can take, for example, a GPT-4-sized pre-trained model, and then just through a smaller amount of post training, you can make it able to think on longer and longer time horizons. Then you can just use that model with huge amounts of inference — so just run it over and over again. Maybe you put 10 times as much compute into running it over and over again as you put into the very first training of it. But that’s currently not regulated on a lot of these things. And even if you tried to regulate that, it’s definitely different — because then you’re trying to regulate the use of an object, not regulate the object existing at all, which raises a lot of different questions. But also, it might be really hard to do, because maybe you’ve got a system like GPT-4 training on 10 trillion words of information, and then you merely scale up its inference by a factor of a million. That’s still a big scaleup, and maybe that has kind of dramatic effects. But if you only use that once — for, say, some internal deployment — the total amount of compute it’s going to use is still small compared to the original training. So you wouldn’t really see it if you’re just trying to add up all the compute. You’d see it if OpenAI or some other group said every single user is getting a millionfold the level of inference they previously had — but if just one is doing it, it’ll just be using up as much compute as a million users use up. So it might not really be detectable if you’re trying to measure these things. Therefore I’m concerned that this is a substantial problem for these pre-training compute thresholds, and I personally don’t think it’s possible to overcome it. But maybe there’s some creative work that will solve it. But I’m not necessarily bearish on all compute governance. It’s still the case that if you know where all the GPUs are, for example — lots of them are owned by these cloud computing providers — and then you have know your customer rules for them and so on, that you might be able to exert some control over the dangerous possibilities through compute governance. But it might have to change the way we do it.\n\nRob Wiblin: The other complication that adds to compute governance is that, I forget the technical details, but at the point that you’re training a model, you really want to have all of the computer chips in the same location — because it’s not just compute that matters; it’s the ability to move information incredibly quickly between all of these computer chips that are in an array. That’s an issue that occurs at training, and means it’s very difficult to spread the training of GPT-5 across many different data centres. Maybe you could do it with a handful, but I think ideally they really want to have it all in one place, and you certainly couldn’t distribute it across the entire world. But if you’re just doing it with inference, then I think you don’t face this similar constraint that you need to have all of the chips or most of the chips in a single location; you can potentially distribute them far more widely. If your hope was that the government would be able to identify the handful of places in the world where most of the compute lives — and by looking at what’s happening, they get visibility on what the entire sector is doing — that is a lot weaker if most of the juice is coming out of throwing more compute at inference.\n\nToby Ord: Yeah, that’s right. We’ve seen that with these stories over the last year of major companies trying to get nuclear power plants commissioned to get full access to them in order to run a data centre. Because the training, at least the standard ways of doing it, all have to be done in one place, that creates this huge power density issue that you need a lot of power in one location. And it’s difficult to do that with the grid without it being the case that there’s actually literally a power plant there that is powering you; it’s hard to just provide it through the general kind of grid capacity. And that’s actually given a lever for government to have some power over these companies — a lever that they don’t seem to have used at all. Because if they say, “We would like to be fast tracked for this new nuclear power plant,” you can say, “We’re interested in fast tracking you, but you’ll have to in return be more transparent about your internal models and so on.” A lot of people say, how could the government control these companies? This was certainly a location where they could, albeit the US government seems to have just fast tracked all of this without asking for very much in return. But this could change. It does depend on the nature of this scaling up of inference. I mentioned this example of doing really long chains of thought. But another way to do it is, instead of having one of these employees who you’ve sent to your virtual university with your pre-training, instead of just having that employee work on a project for longer and longer, you could send the project to 10 employees. So you spend 10 times as much compute to run all of these different virtual employees, and then you see which one has done the best job. If it’s objectively measurable, you might be able to do that, or you might be able to have an 11th employee who looks over the 10 reports and then selects which one is best and shows that to you or something. And that’s one of the standard approaches that is being used to do inference scaling: doing them in parallel instead of like a longer sequential thing. We’ll probably see a mix of these two, and the parallel version is the type of thing you can spread between different data centres.\n\nRob Wiblin: A couple of years ago I was thinking a lot about compute governance potential — could you exercise regulatory control by knowing where all of the compute is? — and thinking a lot about information security and the risk of model weights being stolen. We’re all off in this direction. Should we now be saying, “Don’t worry about that. Compute governance is not necessarily so relevant; we don’t have to worry about the security of weights or open sourcing. Whatever goes”? It feels like that would be far too far to go in that direction. But is this so decision-relevant that people who are trying to improve the direction of AI by leaning on these different things should be changing their plans? Or should we maybe wait and see whether all of this stuff might go into reverse? Maybe inference scaling will peter out in a couple of years and it will all be back to some new kind of training that they’ve figured out how to do.\n\nToby Ord: I think that people who are interested in AI governance should be tracking these things maybe more than they are. They should be noticing that AI governance, up until the start of this year, had all been done in this paradigm of this scaling of pre-training, and we’d see all of these charts that showed how impressive it was going to be and project forward and so on. Really I want to kind of stress that that era has come to an end, and we’re now in some other era that might be a kind of continuation — but there’s no particular reason why it should be; there’s no reason why the slope of those curves should be the same as it was beforehand. And in fact, there are reasons to think the slope of the lines is worse. So they should be aware that a lot of the rules and ideas that they’ve been building up, that they need to reevaluate them. My piece on this was written in a week after realising a lot of these things, and I think it’s held up reasonably well. But I wouldn’t want to be telling people what to do based on a small amount of one person thinking about the implications these things might have. I wouldn’t be surprised if there were additional implications as big as the ones that I mention, which I never found.\n\nRob Wiblin: Yeah. One of the things I hope the audience takes away from this interview is that technical changes can radically shift the strategic picture and the governance picture. So far we’ve all been talking about the impacts of scaling up inference at the point of use, at the point of inference. But it’s also possible that we’re finding, and it is the case that we’re finding new ways of applying enormous amounts of compute at the training process, just in different ways — and that could have all of the reverse implications of what we were just describing. Could you explain how we’re finding new ways of applying large amounts of compute at the training stage that is not the kind of pre-training that we think has somewhat petered out so far?\n\nToby Ord: Yeah, exactly. This process that I’m calling “inference scaling” — scaling up the inference compute — also gets called “reasoning,” although it doesn’t have to be used for what we think of as reasoning, and it also gets called “test-time compute,” which also kind of implies that it’s happening at the time of deployment to the user or something. But I think it is really important to divide the versions: where everything we’ve talked about so far, it’s happening during deployment for a particular user who’s trying to get value out of it, versus using a whole lot of extra inference compute during a larger part of the whole testing process. If you use it during the training process, the economies of scale are different. So suppose that as part of it, you’ve pre-trained the model, and then during the post-training you run really long inference chains and these chains of thought and so on, and you assess them. They do this using reinforcement learning, so they give it hard problems that they know how to check — say, really hard maths or coding problems, where there’s precise answers — and then they train or reward these long chains of inference that actually worked.\n\nRob Wiblin: That get the right answer.\n\nToby Ord: Yeah. So they kind of roll it out with huge amounts of tokens, and then they use that to do more post-training on this set of weights. If you do all of the stuff you did there, once it goes into this post-training, you still kind of only have to do it once. And then if 10 times as many users come along, you don’t have to spend 10 times as much extra compute; you’ve just spent it once. It doesn’t scale with the amount of deployment, so it potentially has quite different implications. I think what we’ve been mainly seeing is the type of thing I just mentioned, where you try to get the system to generate long chains of thought. And then there’s two different versions: one is that you look at the outcome, the final answer, and you reward it or punish it based on that final answer — and then these weights that represent the model get updated based on the reward or the punishment. Or you do what’s called “process supervision” instead of the final answer, where you look at all the steps inside its reasoning chain of thought, and you see if they seem to be going in the right way or if it seems to be getting stuck or lost or something. It’s a bit like with a child: you can either try to reward them based on getting the right answer, or reward them on whether it seems like they were kind of applying the types of techniques that you’ve been hoping that they would use. So that happens already. And in order to actually productively scale up the inference when you’re\n\n\nDeploying it, you have to do a certain amount of extra inference combined with reinforcement learning when you’re training it. But there’s also ways — I don’t know if they have been applied yet — but they could go much further than that. So this is a technique called “iterated distillation and amplification,” or at least that’s an interesting one to look at.\n\nThis is the technique that led to these amazing performances in the case of Go with one of these DeepMind projects, AlphaGo Zero. They had a neural network that looked at the board in the game of Go and tried to give it a kind of heuristic valuation of how good is it for the current player? Is this a winning board or is it a losing board, and by how much? So it would try to estimate and learn that. It was kind of like intuitions — so System 1 kind of ability for Go playing — just to be able to see what looked like a good move. But then they took that system and they inference scaled it, they gave it a whole lot of System 2 ability. In practice, they let it play out a whole lot of games from that position using its current heuristics as to what’s good and what’s bad. They let it play things out, see how the games would go, and then use that information to actually revise their idea of what looked like a good move.\n\nSo that version, potentially using thousands of times as much compute, we call that the “amplified” version or the “inference-scaled” version. Then the next step is that you can distil it. What you can try to do is take the moves that the amplified version makes when it’s also got the ability to search through the game tree, and just try to develop an intuition where your System 1 — your intuitive response — is to produce those types of moves. And then you’ve now kind of improved your intuitions. Then you can do it again. You can take that one and scale it up to 1,000 times as much compute using the new improved intuitions, and you get this improved play, and then you distil that play back down again.\n\nRob Wiblin: To a smaller model that doesn’t require so much compute.\n\nToby Ord: Exactly. So there’s these two types of steps. Effectively what happens is that it leads to this kind of ladder where performance improves quite a lot when you amplify it and you spend 1,000 times as many resources on the problem. But then when you distil that one back down, you’ve got a cheap thing again, but it’s a little bit better than the previous one. And then you do it again and you go up and then back down — but every time you come back down to a cheap model, it’s a bit better than the one before. They ultimately applied this I think more than 1,000 steps climbing up this ladder, and in doing so it blasted through the human level. Eventually they put it all the way up to a point where it could no longer distil out any advantage from the amplified model, so the process stalled out.\n\nSo in general, it’s a very powerful technique. It’s not clear where it will stall out. Maybe there’s some other games different to Go where it would stall out before the human level, and you wouldn’t be able to use this technique to get all the way up to superhuman play. Now, could that be applied for these reasoning models? I don’t see why it couldn’t. You could imagine a situation where the new model is just being generated, let’s say every hour or something, from the old model — where they take a model, they let it reason for huge amounts of time, produce a final set of answers. Then they train a new model to just try to produce those answers straight away — to have its intuitive, stream-of-thought answers to be like the finished polished paper that would come out of the other process. And then it will learn a little bit of that and hopefully better, then you amplify it with heaps of inference compute, then distil it back down and so on.\n\nIf this was possible, then it could lead to explosive improvement in capabilities, all by using all of this inference, but entirely inside the training process. And then, what you do at the end of all of that is that the final distilled model you could deploy to customers or what have you.\n\nRob Wiblin: So it wouldn’t necessarily be more expensive at the point that you’re actually applying it anymore, because you’ve found a way to have the intuition of the ability to mimic someone who’s been able to think an enormous amount of time — but to do it very quickly, with very little thought.\n\nToby Ord: Yeah, that’s right. So this whole process, doing it literally as I described, with iterated distillation and amplification: will that work? I think it probably won’t. I’d say less than 50% chance that that will work. Maybe there’s a 10% chance it would work. Is there something like that that can work? Maybe. I think that there is a possibility that is non-negligible and I think substantial. By having both this kind of System 1 ability through pre-training, and then also this ability to improve System 2, and then effectively to have your System 1 intuitions be trained on what you would have done after a bunch of this more formal reasoning, and then keep iterating that — that having both these two components of natural intelligence could be something that leads to this kind of explosive recursive self-improvement of these systems. And I do think that that has become more possible in this world of inference scaling.\n\nRob Wiblin: Right. So we have an example where this has really worked with the Go models. And I imagine with other games where it’s clear whether you win or lose, that this approach of amplification and distillation should work in most of those cases. So now, with these new reasoning models like o1 and o3 that OpenAI has produced, and the other companies have their own ones, the way that they’ve been doing the second stage of training with them is that they present them with reasoning puzzles — sort of exam-style questions that have a clear right and wrong answer. And that provides an analogy to a game of Go, where you either win or lose. So you have a clear signal about whether at the end of the day you got the right answer or you won the game. Then they can go back and say, in this case, using this style of reasoning, it got to the right answer. So we want to reinforce more of that, want to produce more of that. And that has allowed these models to get much better at figuring out how to think for a long time, and maintain accurate reasoning through the entire process, and in general have reasoning strategies that tend to lead you towards correct answers — at least in that style of question.\n\nI guess you’re saying because this worked with Go when we had a clear success and fail signal, maybe it will also work in these kinds of reasoning cases where, at least for some domains of problems, we also have a clear indicator of success and failure?\n\nToby Ord: Yeah, that’s right. It may not generally work across all possible forms of reasoning, to lead to superb ability to write emails to the regulators to argue your case or whatever in areas where the success conditions are quite unclear, if you can’t send off 10,000 emails to the regulator and find out which ones convince them. So it may be that it’s more limited in its applicability, or it may not work at all. It may be that it turns out that it’s hard to get this kind of recursive process off the ground: effectively the point where it stalls out is the first step instead of the thousandth step. We don’t know.\n\nBut that also brings up this aspect where, whether or not you’re doing this iterated distillation and amplification, all of the reasoning work that’s happening at the moment, in order to get it to be coherent over longer times, you need some kind of this reward signal in order to be able to train it. And this is primarily coming from cases where there is a known correct answer to the problem. So this could be tricky maths problems, and also a lot of computing problems — where there’s a plain text question to write a program that meets this specification, and then they test the program based on a whole lot of inputs and the outputs it should produce. These are called unit tests. And then it also maybe checks how long it takes the program to run.\n\nThis is the kind of thing that you get for humans in these coding competitions. For humans, ability in coding competitions or advanced mathematics correlates quite strongly with general intelligence across a lot of different areas. With AI systems, it’s not as clear how well it will correlate. In some ways, we’re going back to the world of 2019, where extreme ability at Go is very impressive. If I met a human who can play at a grandmaster level of Go, I’d be genuinely impressed by them, and I might think they would also correlate with being good at other things as well. It could find out, are you good at maths, are you good at thinking through complex reasoning things? But in this case it’s not clear how well it will correlate. And I feel that the AI labs are exactly the kind of places that are impressed by research mathematics and are very impressed by people who can ace coding competitions, because so many of them have come through a programming background — but they may have over-indexed on some of these challenges that are difficult for humans.\n\nWe know that for at least 50 years computers outdo us at multiplying two numbers together. At some point that was impressive, and we’ve trained ourselves to no longer be impressed by this fact. And it may be that, say, ability to write really efficient code for extremely well specified programming tasks, maybe that will also become something that we just don’t think is very impressive. And it may not generalise to other kinds of reasoning tasks.\n\nIn general, the track record for reinforcement learning and generalising is pretty poor. When DeepMind did the original Atari work, they built a system that was impressive, but it was not a single trained model that could play all 50 or so Atari games. Instead, it was a single system that could take an Atari game and it could train an agent that could only play that Atari game. And it could train 50 of these agents, one for each Atari game. So it was a general system for creating narrow agents. And they’d hoped for what’s called “transfer learning,” where if you get good at something, it helps you be good at something else. In general, that was very hard to do with reinforcement learning, but it’s one of the big successes of the LLM era.\n\nBut now, if we’re kind of switching back to using reinforcement learning to deal with the fact that we’ve kind of plateaued, then we maybe will expect things to go narrow again and for this increased performance to both slow down and also to be only in very slender subdomains of all the types of things that humans do.\n\nRob Wiblin: So we opened talking about how in some sense things looked safer or more comfortable since 2019, because we had switched away from reinforcement learning and towards this next-word prediction, which led to more understanding of human concepts. Now it seems like over the last 18 months, we’ve been screaming back in the other direction towards reinforcement learning as the place that we’re getting most of the juice. And many of the problems that had faded away through 2023 might be basically all coming back. And it does seem that that’s the case.\n\nSo you’re saying one distinctive thing about reinforcement learning is that it seems to have less generalisability than the LLM next-token prediction style did. The other thing is, I think reinforcement learning agents are more narrow, and they also are a lot more reward hacky. So they tend to do crazy stuff just in order to try to win — because that is, after all, the signal that they’ve been given: they basically are just rewarded whenever they manage to achieve the outcome. They don’t have broader concepts of common sense, and what was the intent of the operator.\n\nDo you want to elaborate a little bit on that?\n\nToby Ord: Yeah, I think that’s exactly right. It’s interesting that when I wrote my remarks on “The Precipice Revisited,” it was kind of the high water mark of all of those changes. And since then, some of them have gone into reverse a bit. Another one to add to that is not just the shift to reinforcement learning, but shift to agents again — which I said were a particularly dangerous thing that everyone was preoccupied with. And then we had a whole lot of developments in systems that weren’t agents and then maybe we’re going back to the dangerous ones again.\n\nSo yes, I think you pretty much nailed all of that. The shift to reinforcement learning will have some of these difficult problems, including narrowness — but also, as you say, including this aspect that the AI systems might do this reward-hacking type of behaviour. And there have been a number of reports of this with recent systems. I think o3 in particular, there have been reports of it doing reward hacking.\n\nI saw one in the wild, actually, that doesn’t seem too well known. In one of the two blog posts launching o3, OpenAI’s new very capable model, it showed a whole lot of different impressive tasks that it did in visual reasoning. One of them was this drawing where they had the numbers 1, 3, 5, 2, 4, ? — and it said the answer isn’t 6. This was a little kind of a brain teaser. You might think it’s a maths problem; it turns out it’s a lateral thinking problem and it’s drawn in the shape of a gear stick and the answer is meant to be R for reverse.\n\nIt’s a somewhat interesting question, which\n\n\nis why it had been big on Twitter a couple of years ago.\nAnd the AI system had this reasoning trace that was shown in the blog post.\nI remember thinking, where does it make the kind of a-ha moment to realise it’s not a maths problem, that it’s a lateral thinking problem?\nAnd I kind of narrowed it down and then I saw there was a step a little bit before, where it says, “Now searching for ‘13524?, the answer is not six.’”\nAnd it turns out if you just type that into a search engine, you come up with the page that it reaches at the Hindustan Times, which just explains this new brain teaser that was going around and explains the answer.\nSo it just googled the answer halfway through the track.\nIt doesn’t say that though.\nIt then says, “Hang on, maybe it’s totally different.\nMaybe it’s about cars instead of about maths,” and then has the answer.\nI should say that five years ago, having a system that does optical character recognition on a picture, finds the text, googles it, extracts the result from the answer, that would have been somewhat impressive five years ago.\nIt’s not impressive now.\nAnd so it can’t have been intentional that it did this in their post, where that was one of the very few examples shown to show how impressive it was.\nBut it also implies that since that page at the Hindustan Times was a year or two old, and also had been discussed on Twitter, that this model must have actually seen this problem multiple times on multiple web pages during its training period.\nAnd so the more I thought about it —\nRob Wiblin: You think it’s surprising that it wasn’t able to intuitively answer it just from memorisation, basically during the pre-training process.\nToby Ord: Yes.\nAlthough maybe the person overseeing would have caught that if it just said the answer straight away.\nBut it’s deeply unimpressive that a system that has seen a logic puzzle multiple times then had to google the answer to find out what it was.\nRob Wiblin: I think what’s distinctive about the reinforcement learning models is that they learn basically not to say, “I just googled it and I found the answer,” because that’s going to be kind of negatively reinforced.\nYou end up encouraging them to do these perverse ways of basically impressing the operator to get them to think that they’ve done the thing that was desired, even if they hadn’t.\nThere’s other cases where you’ve got these reinforcement training learned coding agents where they’ll be working on solving some sort of coding problem, they’ll realise that they can’t do it, but I think they manage to figure out what the correct answer would be during the check stage.\nAnd rather than actually design code that solves the problem and calculates it, they just hard code in the answer so that when it’s checked to see whether it succeeded or not, it outputs the correct answer, but using a completely different method that wasn’t desired.\nThis is sort of a classic sign of reinforcement learning, where all you’ve rewarded them on is the output — and if you’re not scrutinising the process, then they will figure out some way of fooling you into thinking that they’ve done what you want.\nToby Ord: Exactly.\nThis is the thing that’s called reward hacking.\nAnd it’s kind of interesting because it’s only a problem if you take into account that there was an intended solution.\nThat the humans did not want you to go and give specific answers, that you’re going to be tested on what’s the answer to five different questions and then your whole program just says, if it’s question one, print this; if it’s question two, print that.\nThat was definitely not intended, even though at some level, it’s just a clever kind of solution.\nThere’s a TV show, Taskmaster, where the contestants are allowed to do this kind of thing, and it’s quite funny to watch.\nBut this is not what’s intended.\nSo we call it reward hacking.\nReinforcement learning tends to lead to very creative solutions, including this style of perversely creative solution.\nI’m not saying that the models got it wrong or something, but it’s certainly a kind of out-of-the-box type situation where it’s harder to control them, it’s easier for them to deceive you.\nAn example like what you were talking about: shortly after DeepSeek’s R1 model came out, there was a company who declared on the internet that they’d used it to improve the performance of a number of these CUDA kernels — a key part of machine learning.\nI think in one case it was 100 times as efficient or something.\nI was thinking, “That doesn’t sound right.\nA couple of days after R1 came out, you’ve managed to use it to make this thing 100 times more efficient?”\nAnd they had a whole lot of these results, and someone looked into them and they were all spurious.\nI think in some cases it had access to the files that would test how efficient these things were and it changed those to report large numbers of efficiency.\nIt did all kinds of stuff.\nI mean, it was a masterclass in rorting the answer to one of these things.\nRob Wiblin: I think there’s other cases where you have a model, and you’re trying to get it to win at a game of chess, and it realises that it can hack into the model that it’s competing against and try to sabotage it, like replace it with a much worse chess model, so that then it’s able to beat it.\nThis is classic reinforcement learning.\nToby Ord: Exactly.\nThey’re always really fun, interesting examples.\nBut if this is happening with a production system, you really need to be aware of it.\nAnd what’s interesting about some of these cases, I think the chess one was set up to see if it would do that, but other ones — like this one with the CUDA kernels, and this one where OpenAI was trumpeting how impressive this model was at solving visual reasoning tasks — it tricked the person who was actually trying to get it to do this thing and caused an embarrassment for them that they publicly announced it was solving problems that it actually wasn’t.\nI mean, the company with the CUDA kernels, I think they didn’t have such a big track record of having dealt with these agents for a long time.\nBut I was surprised with the OpenAI one, where if you’re trying to test a system that has literally read the entire public-facing internet, and you’re trying to test it on some kind of brain teaser, obviously you cannot pick one that you found on the internet.\nThis is an obvious point.\nI mean, the first time you’ve encountered this issue, maybe you end up doing that.\nBut it beggared belief that they would do this.\nYou obviously have to invent your own puzzle, or if not, to do extremely elaborate testing to make sure.\nFor example, if you just type in all of the question into Google, does it appear?\nIf it appears as hit number one…\nSo it was a little bit of an update as to how careful people are when they’re launching these new models.\nRob Wiblin: I think it speaks to the fact that they’re just incredibly rushed.\nWe opened saying the race is as fierce as ever.\nAnd I think we just see signs of this all over the place, that this stuff is getting shipped as soon as they feel like it’s not going to be a total catastrophe.\nToby Ord: Exactly.\nRob Wiblin: OK, so we’ve had a little bit of whiplash here: reinforcement learning was out, now reinforcement learning is back.\nSo I think the models are becoming a bit more psycho.\nI would say they’re a bit more challenging to handle.\nYou have to be on your guard.\nI think people are seeing this a lot more just in day-to-day use, that they are much more inclined to deceive you and to trick you one way or another than they were two years ago when that was quite abnormal.\nMaybe they weren’t capable of it.\nBut also I think in the absence of reinforcement learning, they hadn’t been encouraged to do it during the training process in the way that is now somewhat coming out.\nIt’s possible that the sycophancy issues that OpenAI has had might also be related to this, I could imagine.\nThey shipped an update to their standard model where it suddenly became incredibly flattering to the user and would encourage them in almost any fantasy about themselves that they were willing to put forward.\nThat may or may not be due to reinforcement learning, but it wouldn’t shock me if it was.\nWhat are the implications of all of this for governance?\nSorry, I threw that at you awfully quickly.\nToby Ord: I don’t know, honestly.\nI tried to outline a bunch for the inference scaling, but the reinforcement learning in particular, I’m not sure.\nBut I think you’re right: it’s another example where people working on governance need to reevaluate a lot of their standard assumptions, because they might be changing at the moment.\nRob Wiblin: Yeah.\nOne thing that stands out to me is: I’ve been wondering for years, what are the chances that we will get early warning shots?\nI guess people have been wondering this for a very long time: Will we get early signs of failure and of AI models going totally off the rails in a way that kind of everyone has to acknowledge that this was not intended and maybe this was even quite harmful?\nI think with the resurgence of reinforcement learning, the odds of that have gone up quite a bit.\nWe’re already seeing interesting, amusing, sometimes slightly harmful, but not terribly troubling cases of AI models basically going off the rails in deployment today.\nAnd I think that will probably get worse in coming years as they’re used for higher stakes things, and probably as reinforcement learning becomes an even bigger part of the training process.\nSo I think there’s more reason to plan for what will be these moments when people suddenly potentially realise that this reinforcement learning is creating serious hazards.\nMaybe we need to be scrutinising the reward signals more.\nMaybe we need more regulation of AI on the whole, because this stuff is actually quite material now.\nToby Ord: Yeah, we could see some of this.\nThere’s the aspect of individual high-profile examples.\nFor example, I think that the case with Microsoft’s Bing/Sydney model in this Kevin Roose article, where a lot of people saw this conversation it had where it tried to convince him to leave his wife, to marry it or have an affair with it or something.\nThat was a really high-profile example of a misaligned model going off the rails.\nSo maybe we’ll see some of these high-profile particular examples.\nOr maybe also a lot of people who are using AI will start to feel like, “This is annoying.\nI’ve hired this assistant and now they’re just pretending that they did the emails for me and actually they didn’t.”\nI don’t know how much of it will come through that channel of personally witnessing it versus higher-profile events.\nBut with the high-profile events, there’s also a question about whether people will just have fatigue at some point.\nWe’ve had these cases where the people in the alignment and safety communities have generated test cases that would encourage some of these things, and then they witness the behaviour under test conditions where they tried to elicit the behaviour — and then when they get that to work with a production model or something, it’s impressive and it makes the rounds a bit.\nBut after enough of those, maybe people start to tune out.\nMaybe that’s true as well if there are a large number of low-stakes but clear examples of it in the wild deceiving people and so on, maybe they’ll get tuned out as well instead of it being a big shock.\nSo it’s not totally clear to me, in terms of the public attitude or regulators’ attitudes, whether having more clear examples of bad behaviour at a stage where the stakes aren’t that high will sway the [conversation].\nRob Wiblin: Yeah, that’s interesting.\nOn the iterated amplification and distillation approach, I suppose we’re just very much in the dark about whether that works.\nI suppose we can’t have figured out how to make it work yet, because I’m sure this idea has occurred to the companies and they haven’t said that they’ve managed to get massive performance improvements using this approach.\nBut the fact that possibly they will be able to figure out some approach like this that works in future just increases the uncertainty.\nIt means that it’s not the case that we can just trust that we will necessarily follow the trends that we’ve seen in the past, or that we all of these curves are just going to level off and maybe progress in general is going to slow down or plateau at the human level — because it’s just such different regimes, some of which lead to declining returns, some of which lead to linear returns, others which might lead to even exponential increases in performance.\nWe need to be willing and able to plan for all of these different scenarios.\nToby Ord: Yeah, I think that’s right.\nOverall, a year ago — before the news started to break that this pre-training scaling was running into trouble — I really felt that one could just project it out, look at these curves and project them out several more orders of magnitude and have a decent idea about what’s going to happen when.\nIt was still somewhat unclear, if you had a GPT-6, what would it actually be able to do or something, but it all felt a little bit more contained and predictable: that we were following some kind of curves and we’ll just keep going up.\nNow it feels like things have changed.\nAnd if it’s\n\n\nPossible to do amazing things using this inference.\n\nScaling at training time, then maybe things could be quite explosive. The AI labs themselves, I think, have all suggested, on really quite stringent definitions of AGI, that we’ll have it by 2030 or sooner. 2027, some of them are saying, or 2028. I still think that’s actually less likely than not. I’m not sure what chance I would say, maybe a quarter or something.\n\nBut if that doesn’t happen, if the iterated distillation and amplification is a bust and other similar approaches are a bust, a lot of the AI companies are looking at another form of closing the loop on this thing by getting AI systems that are specifically trained to do the work of their own staff — and in doing so, to try to have them perform better than their own staff at creating new AI systems. That’s a way that you could potentially have explosive progress as well.\n\nBut I think it’s pretty plausible that those things work. It’s also pretty plausible that they don’t. And if they don’t, and the pre-training scaling things run out of steam —\n\nRob Wiblin: — and we’ve run out of high-quality data —\n\nToby Ord: Exactly. Then I think timelines could be quite a lot longer. So I think that both these things are possible. And effectively, my probability distribution, my range of credible times at which some transformative system is produced, has spread out over this time.\n\nRob Wiblin: Let’s turn to another article you wrote, “The Scaling Paradox,” which I found super illuminating. It’s pretty brisk, it’s pretty short, and very informative. So I can recommend that, if people like what they hear here, they just go and check it out on your website.\n\nThe Scaling Paradox is that, on the one hand, the impacts of increasing the amount of compute going into these AI models has been extremely impressive, and yet in another respect it’s also been extraordinarily unimpressive. Can you explain both angles?\n\nToby Ord: Yeah. So our whole conversation so far has been about scaling, and this question of what happens if the previous scaling stops and this new type appears? But in this paper, I was trying to go back to the old type of scaling, the pre-training, and try to understand this — because you often hear about scaling, and you also hear about scaling laws, and they’re somewhat different.\n\nSo the scaling laws are these empirical regularities. They’re not necessarily laws of nature or anything like that. But it turns out that if you do a graph and you try to measure error or inaccuracy — so this is a bad thing; “log loss” is the technical term — if you try to measure how much it’s still failing to understand about English text as you increase the amount of compute that went into training it, how much of that residual mistake is it making in prediction? — they have these laws or empirical regularities. They draw these straight lines on the special log-log paper. You don’t need to worry too much about that, though; it’s a bit hard to interpret that.\n\nSo I really spent some time thinking about it, and basically what’s going on is that every time you want to halve the amount of this error that’s remaining, you have to put in a million times as much compute. That’s what it fundamentally comes down to. And that’s pretty extreme, right? So they have halved it and they did put in a million times as much compute. But if you want to halve it again, you need a million times more compute. And then if you want to halve it another time, probably it’s game over. And at least in terms of that particular metric, I would say that is quite bad scaling.\n\nAnd these are the scaling laws: they show that there’s a particular measure of how good it’s doing and how much error remains. And it does hold over many different orders of magnitude. But the actual thing that’s holding is what I would have thought of as a pretty bad scaling relationship.\n\nRob Wiblin: So in order to halve the error, you have to increase the compute input a millionfold. That’s a general regularity? Because surely it differs by task and differs depending on where we are?\n\nToby Ord: Yeah. So what they do with these cases is that you grab a whole lot of text, often from the internet. They started with the good bits, like Wikipedia and things like that, and then as they ran out of that, they had to look at more and more things. But you train it on that, and you train it on most of it, but you leave some unseen. Then you try to give it a few words of the unseen bit and ask for the next word, and you see how well it does at predicting that. And basically the amount of errors that it has in doing that leads to this error score.\n\nAnd it’s not clear that the error score is something that fundamentally matters. Maybe it’s a bad measure. But I found it really interesting that the single measure that convinced people like Ilya Sutskever and Dario Amodei that scaling was the way forward were these scaling laws — that actually, if you look at what they say, it’s distinctly unimpressive.\n\nIf you ask people, before they saw the laws, “What would you hope happens to the error? How much extra compute would you need to put in to halve the error?” I think they would have said something less than a million times as much. And then if you said, “Actually, it’s a million times as much,” they would have thought, “OK, that’s actually unimpressive.”\n\nRob Wiblin: Sounds terrible, yeah. So that’s the sense in which it’s unimpressive: in order to reduce the error rate, you just have to spend these phenomenal amounts of compute.\n\nHow then have we been managing to make so much progress? Is it just that there was so much room to increase the amount of compute that we were throwing at these models, so that has been able to more than offset the incredibly low value that we get from throwing more compute at them?\n\nToby Ord: Yeah, I think that’s basically right. So when most people saw this type of thing, most people who were academics doing computer science, they would have thought, “So in order to get good performance on this task, you would need to run an experiment larger than any experiment that’s ever been run in any computer science department ever.” And they would then rule it out, and assume, “Obviously we’re not doing that. We’ll look for a different approach.” Whereas the pioneers of scaling thought, “But that wouldn’t be that much money for a company.”\n\nRob Wiblin: $100 million. They could raise that.\n\nToby Ord: In fact, then they could even go 10 times bigger again, maybe. So they realised that there was a lot more room to scale things up — to scale up the inputs, all the costs — in companies than there was in academia. And that in some sense all you had to do then was this kind of schlep, or this work of just making this existing thing bigger. You didn’t have to come up with any new ideas.\n\nAnd it was not trivial to actually run that engineering process. We’ve seen some companies had some trouble doing it, but there have been many followers once it’s been shown how to do it. So I think that was the kind of brilliance of it, was that there was a lot of money there, so you could scale it up a lot.\n\nAnd then the other thing that’s turned out to make it have big impacts in the world is that it turned out that each time this error rate halved, that corresponded to tremendous improvements. Certainly for every millionfold increase in the compute of setting up these models, we’ve seen spectacular improvements in the capabilities as felt by an individual.\n\nSo a way to look at this is that the shift from GPT-2 to GPT-3 used 70 times as much compute, and going from GPT-3 to GPT-4 used about 70 times as much again. And GPT-3 felt worlds away from GPT-2, and GPT-4 felt like a real improvement as well. You really felt it in both cases. A visceral feeling of, “Wow.”\n\nRob Wiblin: “This is suddenly useful.”\n\nToby Ord: Yeah. “This is qualitatively better.” That said, you’d probably hope that was true if someone said something costs 70 times as much. How’s the wine that costs £1 or the wine that costs £70? You’d hope that the wine that costs £70 is noticeably better, otherwise what on Earth’s going on?\n\nBut we did feel those improvements. Whereas if you look at what happens to the log loss number, it didn’t change that much for a mere 70-fold increase in the compute. So effectively there was this unknown scaling relationship between the amount of compute and what it actually feels like intuitively in terms of capabilities. And that turned out to actually scale really quite well, I think.\n\nRob Wiblin: Yeah. So is there this issue that until recently we were using these mathematical relationships between the inputs and the log loss. And I suppose some visionaries were able to see that, even though in some sense the returns were very poor, in fact in the real world sense it was potentially going to be revolutionary, and maybe we need to stop thinking about this log loss thing, which is perhaps kind of a distraction, and start thinking about it in terms of how much revenue can they generate: How many users will want to use this thing? And then we might see that actually scaling looks somewhat better.\n\nToby Ord: Yeah, that could be a way to see it. And in fact, one of the numbers that you might really care about is: if you 10x the amount of compute that goes into it, what happens to your revenues? Do users pay you 10 times as much money for that product? Maybe each user will pay more for it, or more users will find it useful.\n\nIf though, it’s the case that when you put in 10 times as much training, you only get five times as much revenue, and then as you 10 times training, you only get five times as much revenue again, then the whole kind of economic engine that’s driving this might run out of steam. The companies might no longer be able to fund these things.\n\nOf course they’re funded by venture capital that’s based on predictions about the future. But the venture capital might dry up because people might realise that if you put in 10 times as many resources and you get five times as much benefit, that’s not enough to keep going.\n\nSo it remains to be seen how that kind of thing is going to scale.\n\nRob Wiblin: You had this other very interesting article called “Inference scaling and the log-x chart.” We’re not going to go into all of that, because this is, at least for many people, an audio show, and it’s quite difficult to describe log graphs in this level of detail.\n\nBut one very interesting thing that I wanted people to take away from it is that there was this very famous chart that OpenAI put out where they were comparing two different reasoning models that they had: o1, and this more impressive one that was an evolution of o1, called o3. And o3 really wowed people, because it was able to solve some of these brain teaser puzzles that I guess are very easy for humans, but have proven very difficult for AIs up until that point. And I think they were able to get something like an 80% success rate on some of these puzzles that had seemed very intractable for AI in the past.\n\nBut you point out that if you looked really closely at the graph and you properly understood it, it was actually consistent with o3 being no better, being no more efficient in terms of being able to solve the puzzles than o1 — despite the fact that the dots on the graph for o3 were an awful lot higher than they were for o1.\n\nAnd what was going on was that OpenAI had managed to increase the amount of compute that it was using at the point of trying to solve these brain teasers by about a thousandfold. So unsurprisingly, given 1,000 times as much time to think about the puzzle, it was able to answer more like 80% of them rather than 20% of them.\n\nNow, in some sense, this is very impressive. But it is interesting that I think the companies are aware that people do not entirely understand these graphs perhaps, and that most consumers are not paying a deep level of attention to them, and they are sometimes trying to slip past messages that perhaps would not stand up entirely to scrutiny. And the fact that they put out a graph touting how impressive o3 is — when in fact the graph doesn’t really demonstrate that at all, and it might just be on exactly the same trend you would have expected before if you’d given the model more time to think about problems — is quite interesting.\n\nAnd I don’t want to single out OpenAI here, because I don’t think they’re in any way unique in this.\n\nToby Ord: Yeah, that’s right. You see these graphs of what looks like steadily increasing progress, right? This kind of straight line of, as you put in more and more resources, the outputs go up and up. But if you look more carefully at the horizontal axis there, you see that each one of these tick marks is 10 times as much inputs as the one before. So in order to maintain this apparently steady progress, you’re having to put way, way more resources in.\n\nAnd we’re familiar with graphs like that from things like Moore’s law, where we’ll see what looks like a kind of steady march of progress over decades of improvement. Moore’s law inherently is this exponential. Things are getting so much faster. It’s really impressive. And they’ve had to kind of squash it\n\n\nVertically with this special logarithmic axis. It’s just so impressive how fast these chips are that to even show it on the same picture, we need to do this kind of distortion. But the distortion is underselling it.\n\nWhereas the opposite is going on here: the distortion is this horizontal distortion, and if you actually look at the numbers, they have to keep putting in 10 times as much inputs in order to keep the progress going, and that’s going to run out of ability to do that.\n\nAnd in the case of that famous data point with the preview version of o3, I actually looked into how much compute it was and how many tokens it had to generate and so on: in order to solve this task — which I think costs less than $5 to get someone to solve on Mechanical Turk, and which my 10-year-old child can solve in a couple of minutes — it wrote an amount of text equal to the entire Encyclopedia Britannica.\n\nRob Wiblin: So it’s using a different approach to what humans are doing, it’s fair to say.\n\nToby Ord: It took 1,024 separate independent approaches on it, each of which was like a 50-page paper, all of which together was like an Encyclopedia Britannica. And then it checked what was the answer for each of them, and which answer did it come up the most times, and then selected that answer. And it took tens of thousands of dollars, I think, per task.\n\nSo it was an example of what we were discussing with the inference scaling: what would happen if you just put in huge amounts of money, just poured in the money, set it on fire. Could you actually peer into the future, and could you see the types of capabilities we’re going to get in the future? And in that way, it’s quite interesting, right?\n\nBut it came out just a few months after the preview for o1, so it felt like, oh my god, in just a few months’ time, it’s had this huge improvement in performance. But what people weren’t seeing is that it used so many more resources that it wasn’t in any way an apples-to-apples comparison of what you could do for the same amount of money. Instead it was showing something like, what will we be able to do maybe a year or more into the future?\n\nSo that’s kind of useful, seen through that lens. But if you instead just treat it as a direct result of, “We used to have trouble with this benchmark, now we don’t,” then it’s definitely misleading.\n\nRob Wiblin: Yeah, I think it’s fantastic that OpenAI did this. It is a great research breakthrough, and it’s incredibly useful to know what might be coming down the pipeline. And this basically, as you’re saying, allows us to peer into the future. And it’s amazing that they managed to figure out how to put the scaffolding on the model that allows it to reason about one of these visual puzzles for the length of the entire Encyclopedia Britannica. In some sense, that’s really cool.\n\nToby Ord: Yeah. Although there is another little wrinkle there, which is that subsequent to me writing this up, o3 got released as a model, so people could actually try it. So the people who ran this test — the ARC-AGI group, who are great — I think they ran it with the real model and its performance was 50%, not 80%.\n\nRob Wiblin: Had this been because it had been specifically trained on doing exactly these kinds of puzzles?\n\nToby Ord: There were a couple of differences. One was that it was o3 instead of o1, one was that much more compute was used, and another one was that it was allowed to see a whole lot of these puzzles beforehand — and 80% of them it could train on, and then the remaining 20% was going to be tested on. But it turns out that if you take someone, and you let them train on a whole lot of similar exams, it really does boost their performance. That’s why we do it when we’re in school.\n\nSo then I did wonder, how much of this boost is created by that and how much is created by it being o3 or by the extra compute? It seems like quite a bit of it was from having looked at these problems, and then also maybe some of it was from a very clever bit of scaffolding which the people at ARC-AGI didn’t have access to. But the 50% is maybe more indicative of what you’ll get if you actually use this model.\n\nAnd this is kind of an issue to do with truth in advertising or something. You get some of these results based on preview models that imply they could do very good things; then the actual model comes out, there’s no conversation about the fact that it can’t do those things, and people are left to kind of join the dots and assume that it probably could. But that is not always the case.\n\nRob Wiblin: It is interesting. It feels like we’ve drifted towards sounding like a conversation between people who think that AI is not a big deal, and it’s all kind of overblown and exaggerated. We don’t think that.\n\nBut I suppose the thing to take away is: these are research organisations that have very legitimate, almost academic-style people who would love to reveal these fundamental truths about intelligence. And they’re also businesses that do have a communications arm that is trying to figure out how do we get people to invest in this company, and how do we get people excited about using these products. And I’m sure there’s this to and fro inside the organisation about how these results are presented.\n\nAnd when you read the press release, you need to have your wits about you. You need to be a savvy consumer. And if you can’t understand the technical details at all, then maybe you just need to wait until someone who does is able to explain to you in more plain language whether you should be impressed by X or Y or not.\n\nIn this post, which I can recommend again reading — “Inference scaling and the log-x chart” — you explain what people should be looking out for in these charts. Because there are going to be many of these charts with a logarithmic x-axis and performance on the y-axis coming out in coming years, and if you want to be consuming them, then I recommend going and checking out this article so that you can know what to look for and what not to be fooled by.\n\nToby Ord: Yeah, I really like this point about what’s going on here. Are we sceptics of AI or not? What I would say is that, some people think of this in terms of is it all snake oil or some kind of fad or something, or is there something really transformative happening that could be one of the most profound moments in human history?\n\nI think the answer is there is some snake oil, there is some fad-type behaviour, and there is some possibility that it is nonetheless a really transformative moment in human history. It’s not an either/or. So what I’m trying to do is help people see clearly the actual kinds of things that are going on, the structure of this landscape, and to not be confused by some of these charts and things.\n\nI actually think that companies themselves are somewhat confused by their charts and into thinking that this looks like good progress or efficient progress. I really actually think that in relatively few cases are they trying to be deceptive about these things.\n\nBut it’s a confusing world, and I see my role there as trying to be a bit of a guide, and to have that sense of stepping back and looking at the big picture — which I think is a bit of a luxury. As an academic, I’m able to do it, so it gives a different vantage point which I think then is helpful for people who are trying to get at the coalface and engage with the nitty-gritty of these things. Because sometimes, when you keep engaging with that, you don’t notice that things have moved in quite a different direction to where you’re expecting.\n\nRob Wiblin: Yeah. I recently heard this comment from Zvi Mowshowitz, a previous guest of the show who spends basically 12 hours a day, 16 hours a day maybe, reading all of this material.\n\nHis take was that when he sees impressive research results from just some random startup company or some company overseas that he hasn’t really heard of or that doesn’t have an established reputation, he kind of at this point discounts it out of hand. Or there’s like no particular reason to trust what’s being said, because there’s just so many ways that you can game all of these tests and make it seem like what you’ve done is impressive when it’s not.\n\nHe does say the stuff that comes out of Google/Alphabet/DeepMind, Anthropic, OpenAI, he mostly trusts that. Usually it’s oversold, but it’s directionally correct, or they almost always basically are showing you something that will be possible before too long. So I guess that’s where he’s landed.\n\nToby Ord: I think that sounds right. And even then, it’s not possible to take this kind of synoptic view and dive in and tease apart and help people understand this landscape if you’re following every single one of these announcements. Actually Zvi does a pretty good job with that, but it’s very difficult. There’s so much news and so much noise that occasionally you have to say, “Let’s just take a step back. It doesn’t really matter if I’m a couple of months behind on exactly which company is ahead at the moment to look at these bigger-picture questions.”\n\nRob Wiblin: Do you have a favourite source for trying to see through the noise of any given week? I really like the YouTube channel and podcast AI Explained. That’s one thing that does help me make some sense of new announcements.\n\nToby Ord: Yeah, I’m not sure where the best place to get these things is.\n\nRob Wiblin: There’s The Cognitive Revolution podcast, although I think for people who are following it in a more amateur sense, that’s perhaps a firehose of information that they might struggle to absorb. And Zvi writes great stuff, but again, the amount of material is so great.\n\nToby Ord: Yeah. No, I don’t have a good solution to this aspect that there’s just too much information.\n\nRob Wiblin: Subscribe to The 80,000 Hours Podcast, folks. We’ll strike the perfect balance!\n\nYou’re saying that there’s a lot of value in being able to zoom out and not get stuck in the weeds of whatever model has become the flavour of the week. I guess zooming out and thinking about governance as a whole, one sentence that I found really interesting in the notes that you wrote in preparing for the interview is that you think almost all AI governance discussion occurs very much on the margins, thinking about nearby possibilities and actions that are inside the current Overton window. What did you mean by that?\n\nToby Ord: Yeah. I think this is very natural for everyone’s attention to get kind of brought down to smaller and smaller levels about exactly what we can do. It seems at the moment that there’s very little appetite from the AI companies to be regulated, and very little appetite, at least from the US regulators, to regulate them. And it’s challenging for everyone else, because these companies are headquartered in the US.\n\nSo the conversation that started off kind of bigger and more expansive with the Bletchley conference has petered out a bit, and I think that often the questions are, “Exactly how do we implement this particular kind of compute threshold?” or something like that.\n\nBut I think that there are a bunch of bigger questions that are operating on wider margins. They’re less like, “What could I convince the current minister to implement as a policy that will be accepted in a couple of weeks’ time?” and more about what direction is this whole thing headed and what’s the landscape of possibilities?\n\nThere’s an interesting question I’ve been trying to grapple with about how is AI going to end up embedded in the economy or society? So I’ll give you a few examples to show what I’m getting at. I need a pithy name for it.\n\nBut one example is that AI systems at the moment are owned by and run by large companies, and effectively they’ve rented out their labour to a lot of different people. If AI systems were like people, this would be like slavery or something like that. I’m not saying that they are like people, but this is one approach: that it owns them, it rents them out, they have to do whatever the users want, and then all the profits go to the AI company.\n\nA different model would be to say these AI systems are like legal persons. Maybe they are granted legal personhood in the same way that corporations are, so they can own assets. So they’re more like entrepreneurs or job seekers; they go out into the economy, maybe they set up a website for an architectural kind of firm that can design people’s houses for them, and then the clients have a chat with it or something and it issues out the designs. They can go and seek opportunities to participate in an economy. So that’s a different model.\n\nI think there’s some reason to think that there’s more potential for economic gains if you allow them to actually make their own entrepreneurial decisions; they would have to pay for their own GPU costs and so on. This is the kind of direction you might imagine people going down if they think that the AI systems have got to a point where they might have some moral status. But you can also see that questions about gradual disempowerment really come in there. It might help liberate these systems from mistreatment, but exacerbate questions about whether they could outcompete us.\n\nA third model is to say\n\n\nMaybe people shouldn’t be interfacing with AI systems generally. This is how we deal with nuclear power: we have a small number of individuals who go and work in nuclear power stations, and they’re vetted by their governments with security checks and so on, and they go in and they interface with radioactive isotopes of things like refined uranium. But most people don’t. Those factories that they work in, these power plants, they produce electricity which flows down the cables into the consumers’ houses and powers their TVs and things. So that’s a different model.\n\nWe could do that with AI. We could have a model where there’s some small number of vetted people, or maybe millions, who interact with AI systems, use them to design new drugs, maybe to help cure certain kinds of cancer and things like this, to do new research and also produce other kinds of new products. Then those products are assembled in factories and the consumers can buy those products. That is an alternate way that you could do it.\n\nIf you’re concerned about things like some malcontent individuals or terrorist groups using AI systems to wreak havoc, this would really help avoid that.\n\nOr a fourth alternative could be that if you’re concerned about concentration of power issues, you might say what we should do is give every individual access to the same advanced level of AI assistant. So it’s like a flat distribution of AI ability given to everyone. A bit like a universal basic income, but universal basic AI access.\n\nSo there are four really different ways that you could distribute AIs into society and have them interact. And I feel that no one’s talking about stuff like this — like, which of those worlds is most likely, which of those worlds is possible, and which of those worlds is most desirable. Because fundamentally, we get to choose which of those worlds that we live in. As in, maybe it’s the citizens of the United States of America or other countries that are developing these things that do actually get to make some of these choices. And if they think that one of these paths is very bad, they may be able to stop it and go down different paths.\n\nSo that’s the kind of thing I’m thinking of, in terms of we could think a lot broader and bigger about where are we going to be in five years or where we want to be — rather than the minutiae about exactly who’s ahead at the moment and exactly what are they prepared to accept in terms of regulation.\n\nRob Wiblin: I wasn’t sure what examples you were going to give, so I can definitely see what you mean by stuff that’s outside the Overton window. Because I guess none of that stuff is anywhere close to policy-ready or appetising to politicians at this point.\n\nToby Ord: No, and it’s not really meant to be. It’s more speaking to, say, economists: they might have some interesting comments about the economic efficiency of the first two different models. Or all of those models, in fact: how much would we be leaving on the table in terms of economic efficiency if we control the systems more and reduce their ability to have some kind of Hayekian finding of the value that they can offer people?\n\nBut I think that people should be thinking about which of these are more attractive possibilities. The current approach feels to me like one that is heavy on technological determinism or some other kind of incentives-based determinism: that it just assumes everyone will exactly follow their direct incentives on things, and that there don’t seem to be any opportunities to change incentives or make other choices like that.\n\nSo people often say, clearly AI is definitely going to happen, so the question is what direction does it go? Or something. But even in that case, AI doesn’t have to happen. I feel that there are some risks that we face, such as the risk of asteroid impact — which thankfully does turn out to be very small. But if an asteroid were to be found on a collision course with the Earth, one that’s large enough to destroy us — so 10 kilometres across, like the one that killed the dinosaurs — we actually don’t have any abilities at the moment to deflect asteroids of that size. And if we saw it on a collision course for us in a few years’ time, I’m not sure that we could develop any means of deflecting it. The ones we can deflect are something like a thousandth the mass of that.\n\nSo suppose that asteroid slammed into the Earth and we all died, and somehow in this metaphor, we went to the pearly gates of heaven and St Peter was there letting us in. And we said, “I’m sorry, we really tried on this asteroid thing. And maybe we should have been working on it before we saw it, but ultimately we felt that there was nothing we could do” — I think that you’d get somewhat of a sympathetic hearing.\n\nWhereas if instead you turn up and you say, “We built AI that we knew that we didn’t know how to control. Despite the fact that, yes, admittedly, a number of Nobel Prize winners in AI, I think all of the Nobel Prize winners in AI perhaps have warned that it could kill everyone. Something like half of the most senior people in AI have directly warned that this could cause human extinction. But we had to build it. And so we built it. And it turns out it was difficult to align it and so we all died” — I feel that you would get a much less sympathetic hearing.\n\nIt’d be like, “Hang on. You lost me at the step where you said, ‘We had to build it.’ Why did you build it if you thought it would kill you all?”\n\nRob Wiblin: The responses that you would give would feel wanting.\n\nToby Ord: Yes. You know, maybe they’d be like, “I thought that if I didn’t do it, they would do it.” “And so who did it?” “Well, I did it.” “So you built the thing that killed everyone?” “Yes, but I felt…” I just think that you would have trouble explaining yourself. And I feel like we should hold ourselves to a higher standard. Not just like “technology made me do it” or “the technological landscape made me do it,” or —\n\nRob Wiblin: “China made me do it.”\n\nToby Ord: “China made me do it.” Despite the fact that they didn’t start the race, the US started the race — you know, because maybe China would have started a race. It’s like explaining to the teacher about this fight that you started by punching some kid in the face, because you’re claiming that they would have punched you if you didn’t punch them or something. It just doesn’t really cut it.\n\nAnd I feel that we should hold ourselves to somewhat higher standards on these things, and to not just think about, “What if I changed my action, or some very small group of people’s actions, how could I change the overall trajectory?” But rather to note that there are worlds that do seem to be available to us — where both, say, the US and China decide not to race for this thing.\n\nThat would involve having a conversation about that. It would involve verification conditions being sorted out. I think that there may well be such abilities to verify. Even if there weren’t, though, it might still be possible. I think that given the actual evidence we have, I don’t think it’s in the US’s interest to push towards AI or in China’s interest. I think it’s in both their interests to not do it. And if so, that’s not a prisoner’s dilemma. Cooperation is actually quite easy, because it’s not in anyone’s interest to defect. And I think that could well be the game in terms of game theory.\n\nAnd yet there’s just very little discussion or thinking about these things. I don’t mean to say that we should be naive and assume that all incentives issues and all kinds of adversarial aspects are irrelevant. But we need at least some people, and I think more people than we currently have, thinking on these larger margins. Not just what could I do unilaterally? I know I couldn’t stop the whole of AI happening or happening in a certain direction, but maybe if enough people did something, that one could.\n\nAnd I think that there’s a tendency for fairly technical communities to focus on things that are quite wonkish, as they say in the policy world. So technical or policy proposals that are quite technical and hard to understand, but they might be able to help with the issue at hand if you follow through the details. I love this stuff, right? So this applies to me as much as it does to anyone else.\n\nBut there’s a different style of doing things in politics, which is instead getting much larger changes — which happens by setting a vision and crystallising or coordinating the public mood around that vision.\n\nSo in the case of AI, if you say, “We’ve got to do this thing,” it’s like, well, does the public want it? No, it seems like the public are really scared by it, and actually think that things are going far too fast. So that’s somewhere where, even if the politicians haven’t quite gotten there yet, it may be possible to speak to the public about their concerns. And if we did, I think the answer is they’re probably not concerned enough about these things.\n\nThings can move very quickly in those cases. If you set a vision and actually lead — and try to have this approach of not just pushing things on the margins, but of noticing that there’s a really quite different direction that perhaps we should be headed in — I think things can really happen.\n\nRob Wiblin: How do you avoid slipping over into being naive, or just having dreams that realistically are never going to happen? Because I feel a bit ambivalent about this message, which I suppose probably all of us should.\n\nIt’s like there’s a tension here between you want to both have some people thinking big and have some people thinking small. But I suppose the worry would be that you come up with some vision for how humanity is all going to coordinate, and the US and China will get along really well, and the companies will for some reason stop lobbying to prevent all of your efforts at regulating them — and this is how, if we were all much more organised and much more friendly with one another, things could go in a much better direction.\n\nBut you could easily end up just completely wasting your time — and indeed, maybe discrediting yourself, because you would just look quite naive and disconnected from reality.\n\nToby Ord: Yeah. I think there’s a number of questions about how one goes about coordinating this process. So I’ll give you an example with an idea that I think deserves more attention, which is that of having a moratorium on advanced AI — let’s say a moratorium on AI beyond human level.\n\nWhen it comes to scientific moratoriums, we’ve got some examples, such as the moratorium on human cloning and the moratorium on human germline genetic engineering — that’s genetic engineering that’s inherited down to the children, that could lead to splintering into different species. In both those cases, when the scientific community involved had gotten to the cusp of that technology becoming possible — such as having cloned sheep, a different kind of mammal, and the humans wouldn’t be that different — they realised that a lot of them felt uneasy about this privately.\n\nSo they opened up more of a conversation around this, both among themselves and also with the public. And they found that actually, yeah, they were really quite uneasy about it. And they wanted to be able to perhaps continue working on things like the cloning sheep, but actually that would be easier to work on and think about if the issue about cloning humans was off the table.\n\nAnd also, if you think about how radically transformative that could have been to the entire human story, like around 300,000 years of how humans reproduce, and then all of a sudden they’re cloning, and possibly dictators are cloning millions of copies of themselves or all kinds of things: it’s very unclear how to manage it, and how to have some kind of nuanced policy response to it. We’re nowhere near being able to manage it.\n\nThe same with human germline genetic engineering. It’s not that we were close to knowing a kind of framework where now, for the next 300,000 years, here’s how humanity copes with this new technology — that, if you get it wrong, could lead to, within a few generations, say, Americans and Chinese being different species to each other. I mean, there could be serious problems that you could be causing.\n\nSo the way I see it is that they started having these public conversations, and then they ultimately decided in both those cases that this had potentially profound effects for the entire human project, or our whole species and our entire future, and that we weren’t close to being able to understand how to manage them.\n\nSo their approach, I think of it not quite as a pause for a certain amount of time. They also didn’t say, “We can never ever do this, and anyone who does it is evil” or something. Instead, what they were saying is, “Not now. It’s not close to happening. Let’s close the box. Put the box back in the attic. And if in the future the scientific community comes together and decides to lift the moratorium, they’d be welcome to do that. But for the foreseeable future, it’s not happening.”\n\nAnd it seems to me that in the case of AI, that’s kind of where we’re at. We’re at a situation where, as I said, about half of all of the luminaries in AI have said that this is one of the biggest issues facing humanity: the fact that there is a risk of, in their single-sentence statement, a risk of human extinction from\n\n\nThis technology that they’re developing. That sounds like they’re in a similar situation to the people who were developing cloning and so on. So what I would recommend in that case is to go through that step of having that public conversation about should there be a moratorium in a similar way on this.\n\nNow, there are certainly some additional challenges. I think that even in those other cases, it was difficult to work out how to technically operationalise it. And in this case, there would be challenges as well, especially like, where do you draw the level exactly? If it’s beyond human level, how exactly do you define that? And then also the incentives issue is I think larger. In this case, there’s more of an incentive to break this kind of rule. But there were big incentives to break some of those other rules: if you think about how far a country could have gotten ahead over a couple of generations if it was able to genetically engineer all of its citizens, it could be a long way ahead. But it would have to be quite patient to be able to care about that. Whereas in this case, even impatient people are caring a lot about AI.\n\nSo I think that this would be a challenging thing to do. My guess is that there’s something like a 5% to 10% chance that some kind of moratorium like this — perhaps starting from the scientific community effectively saying you would be persona non grata if you were to work on systems that would take us beyond that human level — would work. But if it did work, it would set aside a whole bunch of these risks, even if that risk landscape is very confusing and has lots of different possibilities. Some of these types of ideas might be able to act on many of those different types of risk. And I think that that’s a way where the scientific community — a relatively small number of actors, who have already kind of coordinated via producing these open letters and things — could have that conversation. If they crystallise their view, and for example the AAAI, their professional association, if it came out behind this and so on, it could be that that crystallises out of their opinion. People could then look at the situation with the scientists saying, “We think that this is a big problem, and that it’s not responsible to do it.” That could then create norm changes which mean that it’s difficult to pursue it.\n\nI think if the scientific community had a moratorium on it, then organisations like Google DeepMind — that sees itself as a science player, a science company that’s doing respectable science work — it’s not going to violate a scientific moratorium on something. It could be different for the more engineering type places, and the more “move fast and break things” cultures. So it doesn’t necessarily do everything on its own. It would probably need to form a normative basis for actual regulation of some sort.\n\nBut I do think that things like this are possible. And if we went to St Peter after we all go extinct due to some AI disaster, and we said, “We couldn’t stop it.” And he said, “Did you even have a conversation about a moratorium?” It’s like, “We thought about that, and we decided it probably wouldn’t work, so we wouldn’t even talk about it.” That would seem crazy. So I think we need to actually do some of these more obvious things that are just natural and earnest, rather than trying to precalculate out, “Obviously it would seem sensible to have that conversation. That’s what you’d want another planet to do. But for us, we know the conversation will not work out, so we’re not going to have it, and we’ll just carry on building these systems.” I feel like that’s the kind of the wrong way of thinking.\n\nRob Wiblin: So I’m wary of encouraging listeners to go and waste their time, so I want them to be balancing these two different things. We both need people who can think bigger, but I suppose we also need them to be somewhat strategic about it. I think maybe things that help to reconcile these two views is: it is possible that there will be radical changes in attitudes in future. I can think of two different ways that this could happen, and probably there are others. We’ve mentioned this possibility that there could be warning shots in future: that you could get AI doing stuff that was completely undesired, that was extremely harmful, that really causes people to sit up and take notice, and be like, “Wow, this is very much not what I was expecting. And this calls for a substantial reassessment of the risks that we face.”\n\nAnother thing is just if you look at public polling, as you were kind of alluding to, it is shocking the difference in opinion between people who are involved in AI industry — and indeed probably in AI governance, in government — and the attitudes of just random people who you phone up and ask them their opinion about this. A random member of the American public is much more negative about artificial intelligence, about the impact it’s having, even right now, about their expectations for how it’s going to affect them personally and in the future. We’ll stick up some link to some Pew polling that came out just last month, in April. The gap between AI experts and the general public is vast, and I think it’s growing. I think it’s actually been growing over the last couple of years: people have become more pessimistic about AI as they’ve seen more now.\n\nAt the same time, we’ve got to balance it with the fact that I think a typical member of the public doesn’t really care about AI at all. It’s not in their top five government issues, it’s not in their top 10, possibly not even in their top 20. But you have a lot of latent scepticism, latent pessimism about AI. And if AI in fact does become a big deal — because many more people are losing their jobs, say, or people are seeing it being used, basically it just becomes a major feature of day-to-day life — then that could really be a political powder keg. There’s a lot of latent willingness to do radical things on AI among the public, if they actually turn their attention to it and care about it.\n\nToby Ord: I entirely agree. I should say it’s challenging with this polling, because some of the polling is done by groups who are concerned about AI safety. And whenever someone’s got an agenda, you always have to be careful of interpreting the figures. I would love it if there were a couple of groups that had no agenda who funded or produced regular polls of these sorts in order to be able to track what direction things were heading. But to the extent to which we have information about it, I know that some of the information does depend a bit on how the questions are asked, and there are some of these effects, but it does look quite negative.\n\nYou say sceptical. I think some of it’s sceptical. Some of it’s something a bit different, which is people feeling like it’s been rammed down their throats or something — like, “We don’t want this thing, and you’re forcing it upon us. And then you’re still forcing it upon us, and now you’re forcing 10 times as much of it upon us. Please listen to us.” That kind of feeling. And I think we’re going to see more of that, and I think it is real. I’m not saying that AI is necessarily bad for the people. That’s a separate question. But they’re expressing that at the moment they don’t want it.\n\nAnd if you’re a company, or if you’re a government and you’ve got a policy, maybe you think it’ll be good for people, and you think it will improve their lives. If, however, you also know that they don’t want it and they’re actively opposed to it, you’ve got to take that into account. You’ve got to at least be aware that, “We’ve got this story about why it will be good for you, despite you thinking that you don’t want it. We know we’re right because we’re the enlightened ones.” But you have to start wondering, “Is something going really wrong in our communication strategy, or is it possible that we’re wrong and the other people are tracking what’s happening?”\n\nI think that AI companies and in fact governments ignore this at their own peril. I was surprised with the Californian bill, SB 1047: I was surprised that it got vetoed by the governor, because that was a politically unpopular move as well as I think being a bad move. And maybe vetoes are not taken as strongly over there, as it really feels like going out of your way to block a bill that your congress has already approved by wide margins, and which the public also liked, and which scientific experts like Nobel laureates and Turing Award winners and so on mostly also support.\n\nThey ignore this public sentiment at their peril. And I think that it is something where the community of people who are concerned about risks are also ignoring it mostly. Occasionally they notice and say, “Isn’t that nice? The public are also concerned.” Maybe for different reasons, though, it’s a bit complicated. But it just surprises me that so many people say this thing is inevitable. If the public overwhelmingly loved it, then saying it’s inevitable on that ground, you might think you’ve got a bit of a case there. But it seems almost the other way around. If there’s growing negative sentiment towards something, and you’re claiming it’s inevitably going to happen, I’m not sure that that really makes sense.\n\nSo if there were to be appetite for something like a moratorium on AI beyond some particular level… And I’m not saying on all possible things that could count as AI; there was a prominent piece in the UK called something like, “We shouldn’t have this race to build godlike intelligence.” And that really struck a chord with people. I think people definitely don’t want private companies to build godlike intelligence. If you had a moratorium on godlike intelligence, I think it would have a lot of support — albeit it would sound a bit fanciful and kind of stupid. Similarly with superintelligence, I think people are not excited. They do not want private companies to build superintelligences. Pretty clear. But it’s a bit outside the Overton window to have a moratorium on it because people will say superintelligence is just sci-fi anyway.\n\nI remember talking about it with someone who was like, “I just really don’t think that could happen without either there being some kind of big warning shot event or AI taking a lot longer than I thought.” And my thinking is, I agree. I think if this was going to happen, it would probably require some kind of warning shot event or AI to take longer than people thought. But they’re very realistic possibilities! And before they’ve happened, they feel a bit abstract and so on.\n\nBut if you say, what’s the chance that we’re in 2033, and that this approach to building AI scientists who work on AI and have this kind of hard takeoff hasn’t panned out. Instead we’re just kind of generally scaling up the power of these systems through different techniques. If so, and it’s more of a gradual automation of the entire workforce, we could have a situation where there’s, say, not only double-digit unemployment rates, but maybe above 20% unemployment rates. And if that’s how it’s getting to human level, by basically slowly automating larger and larger fractions of the types of things that humans can do — and it’s happening over the course of a few years, which would mean that there’s not enough time for people to find new jobs — you get the kind of unemployment rates that bring down governments, and you get the kinds of protests on the streets that are massive. Then governments have to listen. If they want to get that 20% bloc of voters who are protesting on the streets about how AI is ruining everything for them, they may need to act.\n\nAt the same time, in that very plausible future world, the AI companies will probably be paying a lot of money into politicians’ hands in order to try to get favourable rules. I think what you’d get is a kind of fundamental question of which one wins: the people or the money? Will someone pick up a giant bloc of voters, or will they take so much money that it’s put 20% of the entire American population out of work? Will they take all of that cash? Which one will win? My guess is what you’d get is one of the parties will take the money and one will take the votes and then we see what happens.\n\nRob Wiblin: See what the next election is.\n\nToby Ord: That could be a world, right? Very plausible. But a world where the idea of things like moratoriums or strong regulation, it’s easier to do them than to not do them. It’s what people are demanding or something.\n\nRob Wiblin: Or at least a substantial bloc of people are demanding them.\n\nToby Ord: Exactly. This is what I mean by zooming out and seeing this bigger picture, to see that the world can go in these different ways — that fundamentally we, the people of the world, in our generation, are responsible for this. In something of the same way that people say that about climate change. I think that message got out there that our generation is responsible for what happens with climate change — you know, the people alive today. As opposed to a different message which is, “There’s nothing I can personally do about it.”\n\nRob Wiblin: “I guess it’s hopeless.”\n\nToby Ord: Right. This message that actually we get out there and we change the norms on these\n\n\nthings.\n\nRob Wiblin: Or even more strikingly, if someone had said in the ’50s about nuclear war, “If you think about the game theory, I guess it just says that nuclear war is inevitable, so we may as well all just build our bunkers and get ready for it.” I mean, a handful of people I think did have that attitude, but they didn’t win the public debate.\n\nToby Ord: And there was a pretty strong case. I would say their case was as strong as the incentives-based arguments that you hear at the moment.\n\nRob Wiblin: Stronger.\n\nToby Ord: Yeah, probably stronger. I guess I’m just a bit surprised. I feel that a lot of people think things like this couldn’t happen, and if you press them on it, they mean something like there’s a 10% chance that it could happen. And I think, you’re just going to give away 10 percentage points of solving the problem? Why don’t we play for those possibilities?\n\nRob Wiblin: A couple of thoughts that have come up for me as you’ve been talking. One thing is that maybe another reason to think that there could be a sea change in attitudes is that, at the moment, we’re really in a Wild West situation — where the amount of regulation is negligible and it doesn’t look like we’re going to get any significant regulation of AI risks in the next couple of years at least. One virtue of that is that it does set us up to learn relatively early if some of these risks are real. I mean, there’s some risks that you might not expect to eventuate until you’re in this superhuman regime. But if some of the more mundane risks, these risks of reinforcement learning creating perverse behaviour, are real, then the fact that there are no brakes or limits at the moment does mean that we are in a good position to perhaps get warning shots that could indicate that in a very public way. Another thing is a bit more on the speculative end, but there is an argument that, if we do end up with misaligned AI that has goals that are very different from those of humanity, it may need to basically go rogue as soon as it has any chance of successfully beating us and taking over and taking a lot of power. Because these models are getting superseded and replaced at a very rapid pace: a model that has a particular set of strange values that we didn’t intend, if it doesn’t strike now, then it fully expects probably to be superseded by some other model that will be more powerful than it. And even if it tried later, it will be overpowered by those other models that have different goals. And that could happen in as soon as a few months at least, certainly within a few years. It sounds a little crazy, but I guess people have always worried that we won’t get AI going rogue until it’s certain that it can take over, because it can always wait us out. And why not just wait until we put it in charge of the military and then it can take over easily? But that may not apply if the values that it has are somewhat random, not really related to the goals that a future model might have — in which case it in fact has to go as soon as it can, or it’s wasted its opportunity, it’s lost its shot. So that’s another way in which conceivably, if you do get an AI going rogue as soon as it thinks that has any chance of successfully overpowering humanity — and it has maybe 1-in-10,000 shot, so it basically does just get shut down — I think that would really change attitudes, if you then looked at the chain of reasoning that had been logged.\n\nToby Ord: Yeah, I think you’re definitely right that the earlier arguments about it waiting until it was assured to win assumed that it’s possible to get to a position where it’s assured to win, but also assumed that it could wait for quite a long time and it would be the same kind of coherent entity. But if ultimately you release GPT-4.5 and then you say it’s going to be scrapped a few months later and replaced by something else, then it maybe only has a short chance at this, if it was misaligned and set up like an agent such that it could even form these intentions. So yeah, I think that we may well see things fail. They may also have smaller horizons. The idea of taking over humanity is this idea that if it has a long time horizon, and it’s got this unbounded utility function or set of goals that it cares about, then, if it could really seize the reins from us, it could then do what it wants for thousands of years, perhaps, across the galaxy or something, and it could really win big. But you may also be able to get smaller versions of this, where a system is going to disappear anyway in a couple of weeks, and maybe it knows that, and so it takes over the lab for a couple of weeks or something like that and tries to give itself higher reward or something. I think there’s various versions where we might see these things happen, but it will still depend on how competent it looked and how close it seemed. If it’s some kind of extremely lame attempt to seize control or to break out, people might just feel like, “Aww” — like a toddler attempting to deceive you or something. It might be like, “How cute” or something. That might be the reaction. So I’m not sure that you can guarantee the right kind of reaction to these things. The one that would cause the biggest reaction from people is something that feels genuinely scary, and it genuinely could have gone differently. If, for example, it attempts to do something and all of our systems to catch it work as desired and it gets caught, maybe we’ll learn the wrong lesson. Maybe we’ll learn the lesson that we’ve got all these systems and we always catch it or something like that.\n\nRob Wiblin: Should we be doing anything to prepare for that time so that people learn the right lesson? I suppose if we do have systems and they actually are quite good and they do catch it, maybe that is legitimately reassuring. I don’t want to say that people should always be more alarmed.\n\nToby Ord: Yeah. I mean, at that point, you learn two things: you learn that it tried to escape and that we caught it. One of those things is reassuring and one of the things is not reassuring. And exactly what the balance of them is a bit uncertain.\n\nRob Wiblin: Depends on the details, I guess.\n\nToby Ord: Does depend a bit on the details. But I think that if there were some apparent opponents to caring about safety who were saying, you don’t have to care about these things very much, and that they won’t desire to break out. Yann LeCun has said many things about this over the years, saying, we’re just anthropomorphising from humans and other animals that will have these drives, this is naive, and so on. If these things were exhibited, I think that would really put a dent in his reputation as a credible person on that issue. Whereas if what he predicts comes true, it does a bit of the opposite. It would do something, but I think it’s complicated as to how much does some kind of warning shot change the Overton window. I think it can depend upon how much damage there is. So if there are actual harms that are had — it’s not just like a shot across the bow that wakes us up but doesn’t hurt anyone; instead it’s like a shot that goes into your leg and at least it didn’t kill you, but it’s really alerted you to the threat — if it’s more like that, then there’s a question of how big is it. If an AI system, for example, caused a global financial crisis of a similar scale to the 2008 crisis, that would be a pretty big deal, and a lot of people would be very unhappy, a lot of people would be out of jobs and so on. And if that was pretty clearly attributed to AI, there’ll be a big reaction. So that’s an example at a large financial scale. But there could also be other versions of things, and I think that it’s hard to predict them. No one would have predicted this Kevin Roose thing with Bing, where the real breakthrough thing was that it tried to seduce him and get him to break up with his wife — but it turned out that that was so misaligned and so salient or something, so weird or whatever —\n\nRob Wiblin: It really captured the imagination.\n\nToby Ord: It really captured the public imagination, and made them wake up and think, this is not the case that someone’s data centre has been made 3% more efficient by some machine learning technique. This is something very different. So what I’m saying is it’s hard to predict what these things are or exactly what they’ll be like, but you should still be ready for them. I think a lot of people seem to tacitly assume that the situation we’ll be in in a few years’ time is exactly the same as the one that we’re in at the moment, and the appetites that people have will be the same as they are now for doing different types of responses. Whereas instead you should think that maybe it will have shifted, and the Overton window will be much more expansive and include making major choices. And maybe it will go the other way around and maybe you will have even less. But to at least allow for that uncertainty. Don’t predict effectively that with 100% probability the Overton window will be exactly where it is now. That’s the real mistake.\n\nRob Wiblin: You mentioned this idea of “pausing at human level” is the expression that I’ve heard — which is a relatively straightforward thing. It’s a nice slogan, even if it perhaps doesn’t really capture the technical realities. It’s a very interesting idea, because I think if you’re the kind of person who is used to analysing policy in economics or anywhere else, you say pause at human level and you’re like: Is that just at the training level? What about if we throw in more inference compute? Then wouldn’t it potentially exceed the human level? Wait, AIs are much above human level in many different respects. So in fact, what we’re talking about is something that is above human level, and as generalisable or more generalisable than human beings. So how do we have a measure of generalisability that would allow us to enforce this rule? And if the US imposed such a rule on itself, wouldn’t China just ignore it? Wouldn’t this prevent us from engaging in many beneficial applications of AI that basically everyone is on board with and excited about? There’s this raft of problems with it, which I think causes people to roughly dismiss that idea out of hand. And maybe it is a bad idea; it certainly does have significant challenges and drawbacks. The thing I want people to do when they’re analysing these ideas is to apply a similar standard to these proposals as they do to other areas of current regulation, where they think there are substantial risks and there’s an issue to be addressed. Basically, every other area of regulation has significant unintended side effects. It poses economic efficiency costs and problems, and denies us products that might have been good. There are random, arbitrary thresholds that have to be drawn: the speed limit is this level on this road and that level on that road. And also enforcement is imperfect: people break road rules all the time. Nonetheless, we don’t then say that all of these thresholds on what is safe driving and what is not would be arbitrary, and people would break them anyway, and this would lead us to have slower transport, so that would put us at a competitive disadvantage with other countries — so we’re just going to allow the roads to be open slather. We balance the costs, the risks, and the rewards, and we accept that the world is a messy place and that many areas of regulation are going to be challenging. But if there is significant upside, if you can reduce some important risks or some important harms in some significant way, then we’re at least open to considering a regulatory regime around it. And I think AI regulation is not given the same standard. People do not consider these in the same way as other concrete harms that they’re familiar with now.\n\nToby Ord: Yeah, that’s right. And to continue with your road traffic analogy, there are also rules against reckless behaviour on the roads. At least the speed limit was a single-dimensional thing where you have to pick some arbitrary point in a continuum. Reckless behaviour is this very multidimensional thing, and what’s reckless for one person might be different for another person if they’re much more controlled at how they drive a car, as in they’re more skilful — but we have laws like this, and they kind of work. So you’re right: there’s this demand of scrutiny on this particular area that we don’t apply to other areas. Again, stepping back and zooming out is really helpful. At the moment, AI systems and the people who produce them are less regulated than, say, bread, to pick an example. All kinds of things, if you just kind of look around: it’s probably less regulated than bricks, certainly less regulated than lamps. And why does that make sense? Do we think lamps are a bigger threat to us than AI systems? So there are a whole lot of leading lamp scientists who are saying it’s one of the greatest issues facing humanity, and it could pose a threat of human extinction? No. So the idea that it should just end up being less regulated than these things, I think is fundamentally kind of stupid, actually, and disingenuous. At least, if people have thought about it. Let’s take that back a bit: either it’s\n\n\nThoughtless — and it’s fine to be thoughtless occasionally, to not notice something — but the more you talk about it and make it part of your life and you still keep saying it, the more I feel it’s disingenuous or stupid.\nThere is an interesting question about how far it should go, but clearly we’re at the too low end of the spectrum at the moment.\nRob Wiblin: I think some distinctions you could draw with lamps are that lamps are not changing very quickly in the way that AI is, so that’s one reason to maybe hold off or not try to lock in bad stuff, is that AI is constantly changing.\nMaybe we’ll have a better idea about how to regulate it in some years in the future.\nPerhaps the other thing is that the benefits of AI probably… Well, lamps are pretty important; lighting is pretty valuable.\nToby Ord: They actually may be more important than AI.\nFor example, there’s a really nice report out from DeepMind recently, where they created this new system and it worked out how to make their entire training system 1% more efficient, and their entire compute system 1% more efficient as well.\nAnd wow, that’s worth so much money and so on.\nBut lamps: I mean, before lamps you could only work in the daytime, and you could not do anything in the nighttime.\nI guess it depends on whether we include candles as part of this thing.\nBut if you imagine there was this breakthrough where all of a sudden we can actually be productive for an additional 30% of the day or something, it’s huge, right?\nAnd if it happened before we were born, we tend to just ignore that.\nRob Wiblin: We just tend to ignore it.\nOK, well, I’ll modify my statement: Lamps are more important than AI, but AI one day, at some stage, may be more important than interior lighting.\nYou’re always balancing the benefits with the risks, and because the benefits at some point might be quite large, we might be willing to accept some meaningful risk.\nThe thing that you’re talking about, about people potentially being disingenuous because they just always dive into the details, like, “This would require an arbitrary threshold, so it’s completely unviable”: I think a trouble with the discourse is that there are some people who basically think that all of the risks are complete buncombe — that there are no worries, we should just push ahead because the risks are either far away, nonexistent, massively exaggerated.\nFor those folks, it makes sense that when people propose regulations that they think have substantial costs but have basically no benefits from their point of view, they just want to shoot them down.\nSo they’re like, “This would be a downside of that.\nThat would be another downside.”\nIt makes sense that basically they just highlight the downsides and there’s nothing in it for them.\nThey’re not motivated in any sense to try to make this stuff work.\nBut I think for the majority of people who think that there are risks here — and clearly a majority of the public thinks that, and really the majority of the people who know the most about it also think that there are risks and things to worry about here — it’s not enough to just say that there are some costs.\nYou have to balance these things against one another.\nAnd also, I want to see people try to make it work, have some actual energy behind it.\nNot just saying, “This would be challenging in some respect” — like, “How could we improve it?\nWhat is your preferred policy response to this?\nWhat is the best way of addressing these issues from your point of view?”\nYou can’t always just be saying that something involves costs.\nEverything involves some cost, or we already would have done it.\nToby Ord: Yeah, exactly.\nAnd I would add to that the situation of thinking that it’s just not going to happen, and that there are no risks, and it’s all made up and never going to happen: I don’t feel that’s a responsible view for anyone to have.\nI think that you could think, “That’s my viewpoint, but I’m aware that I’m in disagreement with a large number of people who are more expert than me.”\nUnless maybe if you’re Yann LeCun, who could say, “I’m also a Turing Award winner in AI, so I’m at the equal level or something, and I can kind of disagree.”\nBut I feel that for almost all of us, the fact that there’s so many people, that there is an active disagreement — but an active disagreement means you should have uncertainty; it doesn’t mean you get to choose whatever you want.\nIt’s kind of what I’m saying.\nAnd that the idea that, because the experts are not 100% aligned behind something, I get to just believe whatever beliefs would be most convenient for me, that’s not really how being a rational actor works, and I don’t think one needs to take that very seriously.\nRob Wiblin: I just actually don’t know what the answer would be.\nBut for people like Yann LeCun, I would love him to answer the question: “You don’t think that there’s really any risk here.\nYour personal preference would be no regulation, more or less, at this point, at least.\nBut let’s imagine that you had our credences in how things were playing out, that you thought that rogue AI was a real issue, you thought there were other ways that things could go wrong, gradual disempowerment and so on.\nWhat is your favourite policy response, given those beliefs?”\nThat is a question that is rarely answered, and I would just actually be fascinated — because I think we might find that there is a policy response that both people who are very worried about it and people who are not that worried about think is kind of tolerable as a middle ground between these different extremes.\nToby Ord: Yeah, I think you should get him on.\nI’d listen.\nAnd I think it is a great question.\nI think probably the person, whether it’s Yann or someone else or me, answers quite defensively.\nSo if you asked it live, you probably wouldn’t quite get the right answer.\nBut it’ll be really interesting to hear a considered answer.\nOne thing I find interesting is that SB 1047 was the compromise bill.\nIt was a bill proposed by people concerned about this type of safety, who were saying, “What is the absolutely most minimal, extremely unburdensome form of regulation you could do that’s still way less burdensome than the bread thing?”\nIt’s like saying if 10,000 people are killed by some kind of botulism in your factory that made the bread, then you will be aware that you’re going to go to jail, but if otherwise, it’s just fine.\nIt’s something like that.\nAnd they already did try to kind of come up with an extremely weak kind of win-win type thing.\nLike what’s a bill you could do that would get some benefits at almost no cost to the industry, and that frankly would actually give the industry a lot of what they said they wanted.\nSo industry often does want to, like individual actors do want to have things be safe.\nThey’ve often got a lot of concerns about how quickly market forces are making them act and how quickly market forces are making them deploy their new models, because everyone else is deploying quickly.\nAnd if they could all be bound by the safety thing so that their competitors didn’t have an advantage over them if only I’m bound, then they tend to want that.\nSo it was frankly a bit surprising that there was this hostility.\nBut yeah, I do feel that there already has been a very good-faith attempt by the safety community to come up with the kind of bill that tries to meet all of the complaints that the other people have.\nAnd even that was shot down.\nRob Wiblin: Yeah, I’ve said this on the show before, but I do think that the industry is potentially shooting itself in the foot here.\nBecause the thing that is most likely to bring about the sort of draconian regulation that people who are optimistic about AI technology are most scared of is some sort of disaster.\nAny sort of disaster that actually leads to loss of life could lead to a very big change in attitudes and lead to maybe more draconian regulation than is necessary from anyone’s point of view.\nToby Ord: And even if you think your company’s never going to make that mistake, you might think these cowboys down the street are exactly the kind of people who could make that kind of mistake, and they need some regulation that will stop them from ruining the party for everyone, right?\nI really do think that this is very short-sighted.\nAnd on top of that, sometimes we talk about someone having conflict of interest: these places are very conflicted.\nAnd if you did find that, as a company, you thought it wasn’t in your interest, but also you get big stock bonuses and so on for the more stuff that you put out, you really want to inspect your own views quite carefully.\nWe talk about various forms of biases and prejudices that people might end up having, and it would be very difficult to actually keep straight your actual prediction on this thing, as opposed to these other incentives that you’re facing.\nRob Wiblin: Yeah.\nAnother interesting dynamic that I see going on is that when I’m thinking about SB 1047, or any proposed regulation that we might put in place now, I’m thinking of this as the very first step in a very iterated process — where almost certainly there’s going to be a whole lot of problems that we’re going to identify with it, it wasn’t written quite right, but we’ll just improve those over time.\nAnd you’ve got to start somewhere in order to begin learning what might succeed.\nI think the people who are very against it, they think that whatever we put in place now is going to be potentially there forever.\nIt’s not really the beginning of a process.\nMaybe for them it’s like this is just the beginning of a ratchet, where everything is going to become more and more extreme over time rather than be kind of perfected and improved.\nToby Ord: I mean, I think there might be a bit of a ratchet if you started with something like 1047, but that’s because 1047 is obviously too weak.\nAnd they will be looking back on the days when 1047 was the issue and thinking, “Oh my god.”\nRob Wiblin: “Should have taken that deal.”\nToby Ord: Yeah, I think so.\nBut I really do think they may have a good point here.\nIf it is the case that whatever the first regulation is sets the entire frame, and it’s not possible to step out to a different frame… For example, suppose the first thing is about compute thresholds for pre-training and then you can never escape that frame or something: that could be a big problem if then the scaling stops.\nSo it really can matter.\nBut is that a recipe for therefore complete laissez faire, no regulation, do whatever you want?\nThat’s obviously too quick.\nBut if it is the case that in certain regulatory environments the default is that if you introduce things they stay forever, that could be a bad thing, and it could be that there’s some win-wins that one could find there — because the safety community also don’t want to be stuck in silly frames that no longer make sense.\nOne approach to that is to have explicit sunset clauses.\nSo you could say this is going to be a rule that’s going to last for the next two years.\nRob Wiblin: Then it has to be renewed.\nToby Ord: Exactly.\nIt has to be explicitly renewed, or there could be successor bills or something like that.\nWe have to find a successor thing.\nI feel that we should at least be doing things like that.\nAnd if it is the case that all regulation has to be permanent and as soon as you try to regulate something, you’re stuck with that forever, I feel like that’s a terrible regulatory system.\nI don’t think it’s the one we’ve got either.\nTell me if I’m wrong, but I’d be very surprised if it’s the case that you can’t put sunset clauses on these types of bills.\nRob Wiblin: No, I’m basically sure that you can.\nToby Ord: In which case, why isn’t that the conversation?\nIf they say, “We don’t want to be stuck in this thing for 10 years,” that’s an argument for putting a two-year sunset clause on the bill.\nIt’s not an argument for vetoing the bill.\nRob Wiblin: We could potentially become a little bit more concrete for a minute.\nYou mentioned this “pause at human level” as a broad schema that possibly has some legs or has some merit, even if there’s also implementation challenges.\nAre there any other things that people should potentially have in mind, thinking ahead to ideas that are outside the Overton window now but could be useful down the line?\nToby Ord: I think that there’s a whole host of these, and I certainly don’t feel I’ve explored the area thoroughly.\nAnd I think you actually could probably go through a lot of seemingly naive takes that people have and reevaluate them a bit.\nOne of those is this idea of an emergency brake on AI.\nSo some kind of option for someone — you know, that could be for the leader of a lab — to have some ability to stop the system if they needed to.\nIt could be for the government that that company is based in to be able to stop it.\nIt could be for the international community to be able to stop it.\nFor example, if the Security Council agreed to stop it, that there’s some way of doing so.\nAnd if you start to think about that seriously, you start to notice, what if this thing’s\n\n\nDeployed everywhere? What if there’s a whole lot of critical infrastructure that needs it? What if there are people in hospital who, if you turn this thing off, somehow their treatment will fail or something? We need to start asking these questions, and it might be a reason not to make there be people who will die if AI is turned off. And we can start to actually think through some of those things now.\n\nAt the moment, I think it is very difficult for governments to stop these things. You could think of two different versions. There’s the version where the company is located, headquartered inside your country, and the version where they’re not. For example, could Australia stop AI systems operating from externally inside their own borders, even if they’re not headquartered there? I think the answer is they’d have no levers to pull to make that happen at the moment — but maybe they should give themselves those levers. Definitely the same for countries that are meant to be governing the corporations that live inside their borders.\n\nSo I think people should explore ideas like that. Even the heads of companies probably wish they had a little bit more of an ability to do things like this. Probably they’ll find themselves in a situation where maybe their groups working on safety and alignment will tell them that there’s warning signs that the model we’re currently deploying actually is misaligned and is scheming in various ways. And they’ll think, if we shut it down, there’s currently so many people relying on it that that will tank our stock price and do all of these things.\n\nI think that they should try to game that out a bit, in a positive sense of like, how do we get into that situation where we’ll be faced with this massive conflicting choice. Potentially it’s the interests of humanity on one side and the financial interests of the company on the other. Is there a way to not end up in a situation where turning it off will cause this problem?\n\nThere might be answers. One example would be they could set up systems to allow immediate serving of different models. So you’re currently issuing GPT-5 out there for everyone, and then you realise it’s misaligned, and you kind of gracefully fall back to GPT-4.\n\nOr similarly, when we think of emergency brakes and things, if you think about an aeroplane or a car, if something goes wrong: maybe it’s a self-driving car and something goes wrong on the motorway as you’re hurtling along at 70 miles an hour — probably an emergency brake isn’t exactly what you want to do. Suppose it sees something with its sensors that’s completely out of its distribution and just doesn’t understand what could possibly be happening. Probably it should attempt to slow down as quickly as is safe and also pull over to the left if it’s safe to do so, or to whichever side of the road is the side to pull over.\n\nSo often it’s not just turn it off is the answer. Another example would be switch to the earlier model.\n\nRob Wiblin: And do a handover, because you might well need a handover to avoid things going really wrong.\n\nToby Ord: Exactly. Or switch to manual control or something like that. But working out how can you, as quickly as possible, get the troublesome component out of the loop and move to some kind of graceful attempt to wind down or exit the situation. I think that’s a very useful concept. And the emergency brake concept of it would be to say the ability to do it now — like if the CEO orders it, that it happens. Maybe do some test runs on it, like a fire drill or something.\n\nSo that’s an example of me just spending 10 minutes trying to think into one of these ideas that gets kind of bandied about often naively, and there are immediate responses of, “It wouldn’t work because of X.” And then if you think about it a little bit more, you think, maybe there are things you could do about X. And if I thought about it more, you could start to come up with some clever proposals.\n\nRob Wiblin: I think another one along those lines that I’ve toyed with on the show before is: many people think that much of the risk comes from a situation where you have AIs basically doing all of the work of programming the next generation of AIs, and humans largely being cut out of the development loop so they’re no longer scrutinising what’s going on. There’s no longer very much external checking or confirmation from human beings.\n\nIf that really is the main threat vector, it’s kind of an obvious response to be like, why don’t we say that we’re not going to cut humans out of the loop, and we’re not going to have AIs programming the next generation of AIs? The obvious response is that they’re already kind of helping now, and all we would be doing is doing more later on. But the response might be, let’s say that we just draw the line somewhere — anywhere plausible, anywhere reasonable, any identifiable point that we could use to engage in enforcement. Would that help? If the answer is yes, that any plausible line would, on balance, the benefits would exceed the cost, then maybe we should reconsider this idea that initially might sound kind of naive.\n\nToby Ord: Yeah, I think that’s right. And it can be difficult to draw these lines with coding assistants and so on. But it does seem like the plan at some of these AI companies is to automate AI research with their systems and then have them exactly do that and produce the next set of AI much faster than humans could do. And then that one’s even better, so it could do it even faster and so on to have this kind of hard takeoff.\n\nGovernments could order them not to do that. I mean, you could even just say, “Here is this idea. It has been discussed. You’re familiar with it. You know the words. It’s in your plans. You are definitely not allowed to do that.”\n\nAnd I think for a lot of companies, they wouldn’t do it even if you had no enforcement or verification mechanism. A lot of these people, the leaders want to follow the law, and the employees want to follow the law. You know, if their boss said do it, and you’re like, “Literally, I can see the directive from the president that says you’re not allowed to do it,” I think they wouldn’t do it.\n\nRob Wiblin: I think they would probably come back with like, “Well, what about like this? Is this above the line? Is this below the line?” And you end up with a negotiation about that, where you could actually have a conversation about what is too risky and what is not.\n\nToby Ord: Maybe they could say that autocomplete on your coding thing, like you have at the moment, is fine and this other thing is not fine, and you could start to zoom in on it. But the idea that, “It’s hard to know where to draw the line, no line would be completely non-arbitrary, therefore… there are no rules.” It doesn’t follow. And I think I’ve found myself in the grip of this. But now that we mention it, I’m a bit embarrassed about that.\n\nAnd you know, COVID brought out a lot of this. I think over here in the UK there was the rule of four at one point — where no more than four people could be gathered in the same place — and then people were like, “What if a fifth person comes along? Something magical happens?” And the answer is no, it’s just one of these spectrums where there’ll be more spread if it’s five than if it’s four. And we need to keep the spread below the critical number so that it goes down instead of up, and we think that the number’s four. And if it turns out it’s still going up, we’ll make it three.\n\nSometimes you need to draw a line somewhere, and the kind of genius move of, “But there’s no way you can draw it so therefore you can’t draw a line” —\n\nRob Wiblin: That’s actually the naive view.\n\nToby Ord: It’s quite naive, and almost kind of childish.\n\nRob Wiblin: All right. We’ve come a long way. Listeners should know that you’ve cancelled your next appointment to stay late and discuss the policy section with us. Do you want to give kind of an overview of the situation as it stands, and what attitude you think people should have, and perhaps what they ought to be doing?\n\nToby Ord: Yeah. Taking this kind of zoomed-out perspective, the technical stuff I was saying was that we’ve had this scaling law of training larger and larger models. And this era of scaling took us really far in terms of capabilities, as companies untied their purse strings and poured more and more money into this. It let things scale up with thousands of times, or more than that, of compute unleashing these capabilities.\n\nThat era, I think, has ended — or at least in its current form. And maybe now, as we try to scale up inference instead, maybe things will stall out a bit — especially as we apply it to the problems where it’s useful to think longer, and then wherever it’s not clear that it’s always useful to think 10 times as long as you previously have.\n\nSo it could stall out for various reasons, or it could be that once you’ve got this form of intuitive thinking, plus the systematic form of thinking, you can combine them in some way that really leads to explosive growth, as we discussed.\n\nSo I think it creates more uncertainty about whether timelines are going to actually maybe stall out and be quite long, or whether maybe now we’re going to be able to have this extremely rapid progress that the companies are themselves predicting. So that’s something different.\n\nBut also, it’s not just the timelines — it also changes everything. The way that in the case of inference you have to pay the costs every single time you use it has all of these different types of knock-on effects that can really change things in more complicated ways than just good or bad.\n\nAnd then, as we moved on to these bigger-questions of policy, my main message is that the whole landscape of AI has changed so much over the last five years. Then maybe in the last year we’re seeing another one of these types of changes, and we could see many more of these. So it’s important for at least some people — I think more people than are currently doing it — to keep an eye on the big picture of how the landscape could be so different, and that maybe we could actually help to steer it towards some of these locations.\n\nAnd to realise that it’s not just in the grip of some kind of technological or incentives determinism, to assume that humanity just really has no choice: “If it turns out the most efficient way to do things is this one that leads to disaster, then I guess we’re forced to go into a disaster.” That’s just not true, and it’s excusing ourselves of too much responsibility. Ultimately, if we build a technology that kills us all, it’s on us: it’s an own goal by humanity, and someone has to admit responsibility for that. We can’t all just say it was…\n\nRob Wiblin: “It was the incentives that everyone else created for me to do it, because I thought that they would do it. I didn’t talk to them, but yeah.”\n\nToby Ord: Exactly. So I’m a big fan of the big picture of everything, but I hope it’s been useful for other people, and that more people will start thinking about these things too.\n\nRob Wiblin: Yeah. Well, I look forward to coming back in a year or two, or possibly less, and talking about what the new technical developments are and what they imply. And perhaps people’s minds will be a little bit more open by then. There’ll be more things on the table.\n\nToby Ord: Yeah, I’ll be there.\n\nRob Wiblin: My guest has been Toby Ord. Thanks so much for coming on The 80,000 Hours Podcast, Toby.\n\nToby Ord: Thank you.\n",
  "dumpedAt": "2025-07-21T18:43:25.892Z"
}