{
  "episodeId": "WWoyWNhx2XU",
  "channelSlug": "@lennyspodcast",
  "title": "Anthropic co-founder: AGI predictions, leaving OpenAI, what keeps him up at night | Ben Mann",
  "publishedAt": "2025-07-20T11:00:52.000Z",
  "rawLines": [
    {
      "lang": "en",
      "text": "You wrote somewhere that creating powerful \nAI might be the last invention humanity ever  ",
      "offset": 0.08,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "needs to make. How much time do we have, Ben?\n ",
      "offset": 4.16,
      "duration": 2.16
    },
    {
      "lang": "en",
      "text": "I think 50th percentile chance of hitting some \nkind of superintelligence is now like 2028.\n ",
      "offset": 6.32,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "What is it that you saw at OpenAI? What'd you \nexperience there that made you feel like, okay,  ",
      "offset": 12.4,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "we got to go do our own thing?\nWe felt like safety wasn't the top  ",
      "offset": 15.84,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "priority there. The case for safety \nhas gotten a lot more concrete,  ",
      "offset": 18.72,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "so superintelligence is a lot about how do we \nkeep God in a box and not let the God out?\n ",
      "offset": 21.6,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "What are the odds that we align AI correctly?\nOnce we get to superintelligence, it will be too  ",
      "offset": 26.48,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "late to align the models. My best granularity \nforecast for could we have an X-risk or extremely  ",
      "offset": 32,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "bad outcome is somewhere between 0 and 10%.\nSomething that's in the news right now is this  ",
      "offset": 38.16,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "whole Zuck coming after all \nthe top AI researchers,\n ",
      "offset": 42.24,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "We've been much less affected because people \nhere, they get these offers and then they say,  ",
      "offset": 45.52,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "well, of course I'm not going to leave because my \nbest case scenario at Meta is that we make money  ",
      "offset": 49.6,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "and my best case scenario at Anthropic \nis we affect the future of humanity.\n ",
      "offset": 54.24,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "Dario, your CEO recently talked about how \nunemployment might go up to something like 20%.\n ",
      "offset": 59.2,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "If you just think about 20 years in the \nfuture where we're way past the singularity,  ",
      "offset": 64.4,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "it's hard for me to imagine that even capitalism \nwill look at all like it looks today.\n ",
      "offset": 68.72,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "Do you have any advice for folks that \nwant to try to get ahead of this?\n ",
      "offset": 72.96,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "I'm not immune to job replacement either. \nAt some point it's coming for all of us.\n ",
      "offset": 75.84,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "Today, my guest is Benjamin Mann. Holy moly. \nWhat a conversation. Ben is the co-founder of  ",
      "offset": 80.8,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "Anthropic. He serves as tech lead for product \nengineering. He focuses most of his time  ",
      "offset": 86.88,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "and energy on aligning AI to be helpful, \nharmless, and honest. Prior to Anthropic,  ",
      "offset": 91.52,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "he was one of the architects of GPT-3 at OpenAI. \nIn our conversation, we cover a lot of ground,  ",
      "offset": 96.56,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "including his thoughts on the recruiting battle \nfor top AI researchers, why he left OpenAI to  ",
      "offset": 102.24,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "start Anthropic, how soon he expects we'll see \nAGI. Also, his economic touring test for knowing  ",
      "offset": 107.52,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "when we've hit AGI, why scaling laws have not \nslowed down and are in fact accelerating and  ",
      "offset": 113.52,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "what the current biggest bottlenecks are. Why he's \nso deeply concerned with AI safety and how he and  ",
      "offset": 118.88,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "Anthropic operationalize safety and alignment \ninto the models that they build and into their  ",
      "offset": 124.64,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "ways of working. Also, how the existential risk \nfrom AI has impacted his own perspectives on the  ",
      "offset": 129.04,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "world and his own life and what he's encouraging \nhis kids to learn to succeed in an AI future.\n ",
      "offset": 135.12,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "A huge thank you to Steve Mnich, Danielle \nGhiglieri, Raph Lee, and my newsletter community  ",
      "offset": 140.32,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "for suggesting topics for this conversation. If \nyou enjoy this podcast, don't forget to subscribe  ",
      "offset": 145.52,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "and follow it in your favorite podcasting app or \nYouTube. Also, if you become an annual subscriber  ",
      "offset": 149.76,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "of my newsletter, you get a year free of a \nbunch of amazing products including Bolt,  ",
      "offset": 154.24,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "Linear, Superhuman, Notion, Granola, and more. \nCheck it out at Lennysnewsletter.com and click  ",
      "offset": 159.28,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "bundle with that I bring you Benjamin Mann.\nThis episode is brought to you by Sauce.  ",
      "offset": 164.56,
      "duration": 6.4
    },
    {
      "lang": "en",
      "text": "The way teams turn feedback into product \nimpact is stuck in the past. Vague reports,  ",
      "offset": 170.96,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "static taxonomies, unactionable insights that \ndon't move business metrics. The results churn,  ",
      "offset": 175.92,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "lost deals, misgrowth. Sauce is the AI product \nco-pilot that helps CPOs and product teams uncover  ",
      "offset": 181.44,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "business impact and act faster. It listens to \nyour sales calls, support tickets, turn reasons,  ",
      "offset": 187.36,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "and lost deals, surfacing the biggest product \nissues and opportunities in real time.\n ",
      "offset": 192.24,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "It then routes them to the right teams to turn \nsignals into PRDs, prototypes, and even code  ",
      "offset": 196.32,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "that drives revenue retention and adoption. \nThat's why Whatnot, Linktree, Incident.io,  ",
      "offset": 201.44,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "and Zip use Sauce. One enterprise uncovered a \nproduct gap that unlocked $16 million ARR, another  ",
      "offset": 206.48,
      "duration": 6.72
    },
    {
      "lang": "en",
      "text": "caught a spiking issue and prevented millions in \nchurn. You can too at sauce.app/lenny. Sauce built  ",
      "offset": 213.2,
      "duration": 7.2
    },
    {
      "lang": "en",
      "text": "for AI product teams. Don't get left behind.\nThis episode is brought to you by LucidLink,  ",
      "offset": 220.4,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "the storage collaboration platform. You've built a \ngreat product, but how you show it through video,  ",
      "offset": 225.68,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "design, and storytelling is what brings it to \nlife. If your team works with large media files,  ",
      "offset": 230.88,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "videos, design assets, layer project \nfiles, you know how painful it can be  ",
      "offset": 235.84,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "to stay organized across locations, files live \nin different places. You're constantly asking,  ",
      "offset": 239.44,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "is this the latest version? Creative work slows \ndown while people wait for files to transfer.  ",
      "offset": 244.32,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "LucidLink fixes this. It gives your team a shared \nspace in the cloud that works like a local drive.  ",
      "offset": 249.04,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "Files are instantly accessible for anywhere, no \ndownloading, no syncing, and always up to date.  ",
      "offset": 254.88,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "That means producers, editors, designers, and \nmarketers can open massive files in their native  ",
      "offset": 260.08,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "apps, work directly from the cloud, and stay \naligned wherever they are. Teams at Adobe,  ",
      "offset": 264.64,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "Shopify, and top creative agencies use LucidLink \nto keep their content engine running fast and  ",
      "offset": 269.6,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "smooth. Try it for free at lucidlink.com/lenny. \nThat's L-U-C-I-D-L-I-N-K dot com slash Lenny.\n ",
      "offset": 274.72,
      "duration": 12.4
    },
    {
      "lang": "en",
      "text": "Ben, thank you so much for being \nhere. Welcome to the podcast.\n ",
      "offset": 287.12,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "Thanks for having me. Great to be here, Lenny.\nI have a billion and one questions for you. I'm  ",
      "offset": 291.04,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "really excited to be chatting. I want to \nstart with something that's very timely,  ",
      "offset": 295.76,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "something that's happening this week. Something \nthat's in the news right now is this whole Zuck  ",
      "offset": 299.28,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "coming after all the top AI researchers offering \nthem $100 million signing bonuses, $100 million  ",
      "offset": 304.56,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "comp. He's poaching from all the top AI labs. I \nimagine this something you're dealing with. I'm  ",
      "offset": 310.56,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "just curious, what are you seeing inside Anthropic \nand just what's your take on the strategy? Where  ",
      "offset": 315.6,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "do you think things go from here?\nYeah, I mean I think this is a sign of  ",
      "offset": 320.4,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "the times. The technology that we're developing is \nextremely valuable. Our company is growing super,  ",
      "offset": 325.52,
      "duration": 6.8
    },
    {
      "lang": "en",
      "text": "super fast. Many of the other companies in the \nspace are growing really fast. And at Anthropic,  ",
      "offset": 332.32,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "I think we've been maybe much less affected \nthan many of the other companies in the space  ",
      "offset": 338.48,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "because people here are so mission oriented \nand they stay because... They get these offers  ",
      "offset": 343.2,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "and then they say, \"Well, of course I'm not \ngoing to leave because my best case scenario  ",
      "offset": 348.96,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "at Meta is that we make money and my best case at \nAnthropic is we affect the future of humanity and  ",
      "offset": 352.8,
      "duration": 9.12
    },
    {
      "lang": "en",
      "text": "try to make AI flourish and human flourishing go \nwell.\" To me, it's not a hard choice. Other people  ",
      "offset": 362.64,
      "duration": 8.4
    },
    {
      "lang": "en",
      "text": "have different life circumstances and it makes it \na much harder decision for them. For anybody who  ",
      "offset": 371.04,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "does get those mega offers and accepts them, \nI can't say I hold it against them when they  ",
      "offset": 376.08,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "accept it, but it's definitely not something that \nI would want to take myself if it came to me.\n ",
      "offset": 380.96,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "Yeah. We're going to talk about a lot of this \nstuff that you've mentioned. In terms of the  ",
      "offset": 386.32,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "offers do you think, is this a real number that \nyou're seeing this $100 million signing bonus,  ",
      "offset": 390,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "is that a real thing? I don't know \nif you've actually seen that.\n ",
      "offset": 394.08,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "I'm pretty sure it's real.\nWow.\n ",
      "offset": 396.96,
      "duration": 2.4
    },
    {
      "lang": "en",
      "text": "If you just think about the amount of impact that \nindividuals can have on a company's trajectory,  ",
      "offset": 399.92,
      "duration": 6.72
    },
    {
      "lang": "en",
      "text": "in our case, we are selling hotcakes and if \nwe get a 1 or 10 or 5% efficiency bonus on  ",
      "offset": 406.64,
      "duration": 10.8
    },
    {
      "lang": "en",
      "text": "our inference stack, that is worth an incredible \namount of money. And so to pay individuals like  ",
      "offset": 417.44,
      "duration": 8.08
    },
    {
      "lang": "en",
      "text": "$100 million over four year package, that's \nactually pretty cheap compared to the value  ",
      "offset": 425.52,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "created for the business. I think we're just in \nan unprecedented era of scale and it's only going  ",
      "offset": 430.72,
      "duration": 6.4
    },
    {
      "lang": "en",
      "text": "to get crazier actually. If you extrapolate the \nexponential on how much companies are spending,  ",
      "offset": 437.12,
      "duration": 6.8
    },
    {
      "lang": "en",
      "text": "it's like 2X a year roughly in terms of \nCapEx, and today we're maybe in the globally  ",
      "offset": 443.92,
      "duration": 7.92
    },
    {
      "lang": "en",
      "text": "$300 billion range, the entire industry spending \non this, and so numbers like 100 million are a  ",
      "offset": 451.84,
      "duration": 6.56
    },
    {
      "lang": "en",
      "text": "drop in the bucket. But if you go a few years \nout, a couple more doublings, we're talking  ",
      "offset": 458.4,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "about trillions of dollars and at that point it's \njust really hard to think about these numbers.\n ",
      "offset": 462.64,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "Along these lines, something that a lot of \npeople feel with AI progress is that we're  ",
      "offset": 468.32,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "hitting plateaus in many ways that it feels like \nnewer models are just not as smart as previous  ",
      "offset": 472.88,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "leaps. But I know you don't believe this. I know \nyou don't believe that we've hit plateaus on  ",
      "offset": 478.08,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "scaling loss. Talk about just what you're seeing \nthere and what you think people are missing.\n ",
      "offset": 483.04,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "It's kind of funny because this narrative comes \nout every six months or so and it's never been  ",
      "offset": 486.24,
      "duration": 6.88
    },
    {
      "lang": "en",
      "text": "true, and so I kind of wish people would have \na little bit of a bullshit detector in their  ",
      "offset": 493.12,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "heads when they see this. I think progress has \nactually been accelerating where if you look  ",
      "offset": 498.64,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "at the cadence of model releases, it used to be \nonce a year and now with the improvements in our  ",
      "offset": 502.24,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "post-training techniques, we're seeing releases \nevery month or three months, and so I would say  ",
      "offset": 508.24,
      "duration": 6.96
    },
    {
      "lang": "en",
      "text": "progress is actually accelerating in many ways, \nbut there's this weird time compression effect.  ",
      "offset": 515.2,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "Dario compared it to being in a near light \nspeed journey where a day that passes for  ",
      "offset": 520.24,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "you is like five days back on earth and we're \naccelerating. The time dilation is increasing.\n ",
      "offset": 526.48,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "And I think that's part of what's causing people \nto say that progress is slowing down, but if you  ",
      "offset": 532,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "look at the scaling laws, they're continuing to \nhold true. We did kind of need this transition  ",
      "offset": 537.2,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "from normal pre-training to reinforcement \nlearning scaling up to continue the scaling laws,  ",
      "offset": 542.32,
      "duration": 6.88
    },
    {
      "lang": "en",
      "text": "but I think it's kind of like for semiconductors \nwhere it's less about the density of transistors  ",
      "offset": 549.2,
      "duration": 6.72
    },
    {
      "lang": "en",
      "text": "that you can fit on a chip and more about how many \nflops can you fit in a data center or something.  ",
      "offset": 555.92,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "You have to change the definition around a little \nbit to keep your eye on the prize. But yeah,  ",
      "offset": 562.16,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "this is one of the few phenomena in the world that \nhas held across so many orders of magnitude. It's  ",
      "offset": 569.36,
      "duration": 6.64
    },
    {
      "lang": "en",
      "text": "actually pretty surprising that it is continuing \nto hold. To me, if you look at fundamental laws  ",
      "offset": 576,
      "duration": 6.32
    },
    {
      "lang": "en",
      "text": "of physics, many of them don't hold across 15 \norders of magnitude, so it's pretty surprising.\n ",
      "offset": 582.32,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "It boggles the the mind. What you're saying \nessentially is we're seeing newer models being  ",
      "offset": 587.6,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "released more often, and so we're comparing it \nto the last version and we're just not seeing  ",
      "offset": 591.76,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "as much advance. But if you go back and \nit was like a model released once a year,  ",
      "offset": 595.44,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "it was a huge leap, and so people are missing \nthat. We're just seeing many more iterations.\n ",
      "offset": 599.52,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "I guess, to be a little bit more generous to \nthe people saying things are slowing down. I  ",
      "offset": 604,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "think that for some tasks we are saturating the \namount of intelligence needed for that task,  ",
      "offset": 608,
      "duration": 6.96
    },
    {
      "lang": "en",
      "text": "maybe to extract information from a simple \ndocument that already has form fields on it or  ",
      "offset": 614.96,
      "duration": 7.12
    },
    {
      "lang": "en",
      "text": "something like it's just so easy that okay, yeah, \nwe're already at 100% and there's this great chart  ",
      "offset": 622.08,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "on Our World in Data that shows that when you \nrelease a new benchmark within six to 12 months,  ",
      "offset": 628.24,
      "duration": 7.04
    },
    {
      "lang": "en",
      "text": "it immediately gets saturated. And so maybe \nthe real constraint is how can we come up  ",
      "offset": 635.28,
      "duration": 6.4
    },
    {
      "lang": "en",
      "text": "with better benchmarks and better ambition of \nusing the tools that then reveals the bumps in  ",
      "offset": 641.68,
      "duration": 8
    },
    {
      "lang": "en",
      "text": "intelligence that we're seeing now.\nThat's a good segue to you have a very  ",
      "offset": 649.68,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "specific way of thinking about AGI \nand defining what AGI means.\n ",
      "offset": 654.08,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "I think AGI is kind of a loaded term, and so I \ntend not to use it very much anymore internally.  ",
      "offset": 657.52,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "Instead, I like the term transformative AI \nbecause it's less about can it do as much as  ",
      "offset": 663.6,
      "duration": 6.32
    },
    {
      "lang": "en",
      "text": "people do? Can it do literally everything and more \nabout objectively is it causing transformation in  ",
      "offset": 669.92,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "society and the economy? A very concrete way of \nmeasuring that is the Economic Turing Test. I  ",
      "offset": 676.16,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "didn't come up with this, but I really like it. \nIt's this idea that if you contract an agent for  ",
      "offset": 682,
      "duration": 6.72
    },
    {
      "lang": "en",
      "text": "a month or three months on a particular job, if \nyou decide to hire that agent and it turns out  ",
      "offset": 688.72,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "to be a machine rather than a person, then it's \npassed the Economic Turing Test for that role.\n ",
      "offset": 694.88,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "And then you can sort of expand that out in the \nsame way that for measuring purchasing power  ",
      "offset": 700.48,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "parity or inflation, there's a basket of goods. \nYou can have a market basket of jobs, and if the  ",
      "offset": 705.2,
      "duration": 6.56
    },
    {
      "lang": "en",
      "text": "agent can pass the Economic Turing Test for 50% of \nmoney-weighted jobs, then we have transformative  ",
      "offset": 711.76,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "AI and the exact thresholds don't really matter \nthat much, but it's kind of illustrative to say  ",
      "offset": 717.68,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "if we pass that threshold, then we would expect \nmassive effects on world GDP increases and  ",
      "offset": 723.68,
      "duration": 7.04
    },
    {
      "lang": "en",
      "text": "societal change and how many people are employed \nand things like that because societal institutions  ",
      "offset": 731.44,
      "duration": 7.12
    },
    {
      "lang": "en",
      "text": "and organizations are sticky, it's slow to have \nchange, but once these things are possible you  ",
      "offset": 738.56,
      "duration": 6.88
    },
    {
      "lang": "en",
      "text": "know that it's the start of a new era.\nAlong these lines, Dario, your CO recently  ",
      "offset": 745.44,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "talked about how AI is going to take a huge part \nof, I don't know, half of white-collar jobs,  ",
      "offset": 751.6,
      "duration": 6.56
    },
    {
      "lang": "en",
      "text": "that unemployment might go up to something \nlike 20%. I know you're even more vocal and  ",
      "offset": 758.16,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "opinionated about just how much impact AI is \nalready having in the workplace that people may  ",
      "offset": 764.08,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "not even be realizing. Talk about just what you \nthink people are missing about the impact AI is  ",
      "offset": 768,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "going to have on jobs and is already having.\nYeah, so from an economic standpoint, there's a  ",
      "offset": 773.12,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "couple different kinds of unemployment, and one \nis because the workers just don't have the skills  ",
      "offset": 779.04,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "to do the kinds of jobs that the economy needs. \nAnd another kind is where those jobs are just  ",
      "offset": 785.2,
      "duration": 6.96
    },
    {
      "lang": "en",
      "text": "completely eliminated, and I think it's going \nto be actually a combination of these things,  ",
      "offset": 792.16,
      "duration": 7.6
    },
    {
      "lang": "en",
      "text": "but if you just think about 20 years in the \nfuture where we're way past the singularity,  ",
      "offset": 799.76,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "it's hard for me to imagine that even \ncapitalism will look at all it looks today.  ",
      "offset": 805.2,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "If we do our jobs, we will have safe aligned \nsuperintelligence, we'll have, as Dario says,  ",
      "offset": 811.6,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "in Machines of Love and Grace, a country of \ngeniuses in a data center, and the ability to  ",
      "offset": 817.36,
      "duration": 6.64
    },
    {
      "lang": "en",
      "text": "accelerate positive change in science, technology, \neducation, mathematics, it's going to be amazing.\n ",
      "offset": 824,
      "duration": 8.64
    },
    {
      "lang": "en",
      "text": "But that also means in a world of abundance where \nlabor is almost free and anything you want to do,  ",
      "offset": 832.64,
      "duration": 6.96
    },
    {
      "lang": "en",
      "text": "you can just ask an expert to do for you, \nthen what do jobs even look like? And so I  ",
      "offset": 839.6,
      "duration": 7.76
    },
    {
      "lang": "en",
      "text": "guess there's this scary transition period from \nwhere we are today where people have jobs and  ",
      "offset": 847.36,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "capitalism works and the world of 20 years from \nnow where everything is completely different, but  ",
      "offset": 853.2,
      "duration": 6.4
    },
    {
      "lang": "en",
      "text": "part of the reason they call it the singularity \nis that it's a point beyond which you can't easily  ",
      "offset": 859.6,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "forecast what's going to happen. It's just such \na fast rate of change and so different that it's  ",
      "offset": 865.2,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "hard to even imagine. I guess taking the view from \nthe limit, it's pretty easy to say hopefully we'll  ",
      "offset": 869.68,
      "duration": 9.6
    },
    {
      "lang": "en",
      "text": "have figured it out. And in a world of abundance, \nmaybe the jobs themselves, it's not that scary,  ",
      "offset": 879.28,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "and I think making sure that that transition \ntime goes well is pretty important.\n ",
      "offset": 884.4,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "There's a couple of threads I want to follow \nthere. One is people hear this, there's a lot of  ",
      "offset": 889.28,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "headlines around this. Most people probably don't \nactually feel this yet or see this happening and  ",
      "offset": 893.2,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "so there's always this, I guess, I don't know, \nmaybe, but I don't know it's hard to believe,  ",
      "offset": 898.08,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "my job seems fine. Nothing's changed. What \nare you seeing just happening today already  ",
      "offset": 902.72,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "that you think people don't see or misunderstand \nin terms of the impact AI is having on jobs?\n ",
      "offset": 906.96,
      "duration": 7.12
    },
    {
      "lang": "en",
      "text": "I think part of this is that people are \nreally bad at modeling exponential progress.  ",
      "offset": 914.08,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "And if you look at an exponential on a graph, it \nlooks flat and almost zero at the beginning of it,  ",
      "offset": 919.44,
      "duration": 7.28
    },
    {
      "lang": "en",
      "text": "and then suddenly you hit the knee of the curve \nand things are changing real fast and then it  ",
      "offset": 926.72,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "goes vertical. That's the plot that we've been \non for a long time. I guess I started feeling it  ",
      "offset": 931.76,
      "duration": 9.04
    },
    {
      "lang": "en",
      "text": "in 2019 maybe when GPT-2 came out and I was like, \n\"Oh, this is how we're going to get to AGI.\" But  ",
      "offset": 941.36,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "I think that was pretty early compared to a lot \nof people where when they saw ChatGPT, they were  ",
      "offset": 947.28,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "like, \"Wow, something is different and changing.\" \nAnd so I guess I wouldn't expect widespread  ",
      "offset": 952.72,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "transformation in a lot of parts of society, \nand I would expect this skepticism reaction.  ",
      "offset": 958.72,
      "duration": 6.56
    },
    {
      "lang": "en",
      "text": "I think it's very reasonable and it's exactly \nwhat is the standard linear view of progress.\n ",
      "offset": 965.28,
      "duration": 7.76
    },
    {
      "lang": "en",
      "text": "But I guess to cite a couple of areas where \nI think things are changing quite quickly. In  ",
      "offset": 973.04,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "customer service we're seeing with things like Fin \nand Intercom, they're a great partner of ours, 82%  ",
      "offset": 978,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "customer service resolution rates automatically \nwithout a human involved. And in terms of software  ",
      "offset": 984.48,
      "duration": 7.28
    },
    {
      "lang": "en",
      "text": "engineering, our Claude Code team, like 95% of \nthe code is written by Claude. But I think a  ",
      "offset": 991.76,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "different way to phrase that is that we write \n10X more code or 20X more code, and so a much,  ",
      "offset": 997.6,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "much smaller team can just be much, much more \nimpactful. And similarly for the customer service,  ",
      "offset": 1003.2,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "yes, you can phrase it as 82% customer service \nresolution rates, but that nets out in the humans  ",
      "offset": 1008.64,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "doing those tasks, able to focus on the harder \nparts of those tasks. And for the more tricky  ",
      "offset": 1014.48,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "situations that in a normal world like five years \nago, they would've had to just drop those tickets  ",
      "offset": 1020.4,
      "duration": 7.68
    },
    {
      "lang": "en",
      "text": "because it was too much effort for them to \nactually go do the investigation. There were  ",
      "offset": 1028.08,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "too many other tickets for them to worry about.\nI think in the immediate term, there will be a  ",
      "offset": 1032.32,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "massive expansion of the pie and the amount \nof labor that people can do. I've never met  ",
      "offset": 1036.96,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "a hiring manager at a growth company and heard \nthem say, \"I don't want to hire more people.\"  ",
      "offset": 1043.44,
      "duration": 7.04
    },
    {
      "lang": "en",
      "text": "That's the hopeful version of it. But with things \nthat are lower skill jobs or less headroom on how  ",
      "offset": 1051.12,
      "duration": 6.8
    },
    {
      "lang": "en",
      "text": "good they can be, I think there will be a lot of \ndisplacement. It is just something we as a society  ",
      "offset": 1057.92,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "need to get ahead of and work on.\nOkay. I want to talk more about that,  ",
      "offset": 1063.44,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "but something that I also want to help people with \nis how do they get a leg up in this future world?  ",
      "offset": 1067.36,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "They listen to this, they're like, \"Oh, this \ndoesn't sound great. I need to think ahead.\"  ",
      "offset": 1074.72,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "I know you won't have all the answers, but just \ndo you have any advice for folks that want to try  ",
      "offset": 1080.8,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "to get ahead of this and kind of future-proof \ntheir career and their life to not be replaced  ",
      "offset": 1085.28,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "by AI? Anything you've seen people do, anything \nyou recommend they start trying to do more of?\n ",
      "offset": 1090.16,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "Even for me and being in the center of a lot \nof this transformation, I'm not immune to job  ",
      "offset": 1096.08,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "replacement either. Just some vulnerability there \nof at some point it's coming for all of us.\n ",
      "offset": 1101.28,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "Even you, Ben, now.\nAnd you, Lenny.\n ",
      "offset": 1107.28,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "And me.\nSorry.\n ",
      "offset": 1111.04,
      "duration": 1.16
    },
    {
      "lang": "en",
      "text": "Oh, wait, we've gone too far now. Okay.\nBut in terms of the transition period, yeah,  ",
      "offset": 1112.2,
      "duration": 7.48
    },
    {
      "lang": "en",
      "text": "I think there are things that we can do, and I \nthink a big part of it is just being ambitious and  ",
      "offset": 1119.68,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "how you use the tools and being willing to learn \nnew tools. People who use the new tools as if they  ",
      "offset": 1124,
      "duration": 6.32
    },
    {
      "lang": "en",
      "text": "were old tools tend to not succeed. As an example \nof that, when you're coding, people are very  ",
      "offset": 1130.32,
      "duration": 7.28
    },
    {
      "lang": "en",
      "text": "familiar with autocomplete, people are familiar \nwith SimpleChat where they can ask questions  ",
      "offset": 1137.6,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "about the code base, but the difference between \npeople who use Claude Code very effectively and  ",
      "offset": 1142.72,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "people who use it not so effectively is like are \nthey asking for the ambitious change? And if it  ",
      "offset": 1148.48,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "doesn't work the first time, asking three more \ntimes because our success rate when you just  ",
      "offset": 1154.4,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "completely start over and try again is much, much \nhigher than if you just try once and then just  ",
      "offset": 1159.12,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "keep banging on the same thing that didn't work.\nAnd even though that's a coding example and coding  ",
      "offset": 1165.04,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "is one of the areas that's taking off most \ndramatically, we have seen internally that our  ",
      "offset": 1171.2,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "legal team and our finance team are getting a ton \nof value out of using Claude Code itself. We're  ",
      "offset": 1176.8,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "going to be making better interfaces so that they \nwill have an easier time and require a little bit  ",
      "offset": 1182.4,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "less jumping in the deep end of using Claude Code \nin the terminal. But yeah, we're seeing them use  ",
      "offset": 1188.08,
      "duration": 7.28
    },
    {
      "lang": "en",
      "text": "it to redline documents and use it to run BigQuery \nanalyses of our customers and our revenue metrics.  ",
      "offset": 1195.36,
      "duration": 8.64
    },
    {
      "lang": "en",
      "text": "I guess it's about taking that risk and even if \nit feels like a scary thing, trying it out.\n ",
      "offset": 1204.8,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "Okay, so the advice here is use the tools. \nThat's something everyone's always saying,  ",
      "offset": 1210.72,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "just actually use these tools. It's like sit \nin Claude Code. And your point about being more  ",
      "offset": 1214.72,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "ambitious than you naturally feel like being \nbecause maybe it'll actually accomplish the  ",
      "offset": 1220.24,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "thing. This tip of trying it three times so the \nidea there is it may not get it right the first  ",
      "offset": 1225.68,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "time. Is the tip there ask it in different \nways or is it just try harder, try again?\n ",
      "offset": 1231.04,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "Yeah, I mean you can just literally ask the exact \nsame question. These things are stochastic and  ",
      "offset": 1235.76,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "sometimes they'll figure it out and sometimes \nthey won't. In every one of these model cards,  ",
      "offset": 1241.76,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "it always shows pass it one versus pass it in. \nAnd that's exactly the thing where they try the  ",
      "offset": 1246.32,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "exact same prompt, sometimes it gets it, sometimes \nit doesn't. That's the dumbest advice. But yeah,  ",
      "offset": 1251.44,
      "duration": 7.36
    },
    {
      "lang": "en",
      "text": "I think if you want to be a little bit smarter \nabout it, there can be gains there of saying,  ",
      "offset": 1258.8,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "\"Here's what you already tried and it \ndidn't work, so don't try that. Try  ",
      "offset": 1264.4,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "something different.\" That can also help.\nThe advice is comes back to something that a  ",
      "offset": 1267.36,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "lot of people talk about these days is you won't \nbe replaced by AI at least anytime soon you'll be  ",
      "offset": 1271.68,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "replaced by someone that is very good using AI?\nI think in that area it's more like your team  ",
      "offset": 1276.08,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "will just do dramatically more stuff. We're \ndefinitely not slowing down on hiring at all,  ",
      "offset": 1282.16,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "and some people are confused by that. Even in \nan onboarding class, somebody asked that and  ",
      "offset": 1288.08,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "they were like, \"Why did you hire me if we're \nall just going to be replaced?\" And the answer  ",
      "offset": 1293.84,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "is the next couple of years are really critical \nto get right and we're not at the point where  ",
      "offset": 1297.68,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "we're doing complete replacement. Like I said, \nwe're still at that flat zero looking part of  ",
      "offset": 1302.48,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "the exponential compared to where we will be. \nIt is super important to have great people  ",
      "offset": 1308.32,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "and that's why we're hiring super aggressively.\nLet me take another approach to asking this  ",
      "offset": 1314.48,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "question something ask everyone that's at the very \ncutting edge of where AI is going. You have kids,  ",
      "offset": 1318.32,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "knowing what you know about where AI is heading \nand all these things you've been talking about,  ",
      "offset": 1324.4,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "what are you focusing on teaching your kids \nto help them thrive in this AI future?\n ",
      "offset": 1327.92,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "Yeah, I have two daughters, a one-year-old and \na three-year-old, so it's pretty in the basics  ",
      "offset": 1333.92,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "still. And our three-year-old is now capable of \njust conversing with Alexa Plus and asking her  ",
      "offset": 1339.92,
      "duration": 7.04
    },
    {
      "lang": "en",
      "text": "to explain stuff and play music for her and all \nthat stuff. She's been loving that. But I guess  ",
      "offset": 1346.96,
      "duration": 6.64
    },
    {
      "lang": "en",
      "text": "more broadly, she goes to a Montessori school and \nI just love the focus on curiosity and creativity  ",
      "offset": 1353.6,
      "duration": 7.12
    },
    {
      "lang": "en",
      "text": "and self-led learning that Montessori has.\nI guess if I were in a normal era like 10,  ",
      "offset": 1360.72,
      "duration": 9.2
    },
    {
      "lang": "en",
      "text": "20 years ago and I had a kid, maybe I would be \ntrying to line her up for going to a top tier  ",
      "offset": 1369.92,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "school and doing all the extracurriculars and all \nthat stuff. But at this point, I don't think any  ",
      "offset": 1375.76,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "of it's going to matter. I just want her to be \nhappy and thoughtful and curious and kind. And  ",
      "offset": 1381.2,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "the Montessori school is definitely doing \ngreat at that. They text us throughout the  ",
      "offset": 1388.4,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "day. Sometimes they're like, \"Oh, your kid got in \nan argument with this other kid and she has really  ",
      "offset": 1392.96,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "big emotions and she tried to use her words.\" \nI love that. I think that's exactly the kind of  ",
      "offset": 1398.32,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "education that I think is most important, that the \nfacts are going to fade into the background.\n ",
      "offset": 1404.32,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "I'm a huge fan of Montessori also. I'm trying \nto get our kid into Montessori school. He's two  ",
      "offset": 1408.4,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "years old, so we're on the same track. This idea \nof curiosity, it comes up every single time. Ask  ",
      "offset": 1412.08,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "someone that's working at the cutting edge of AI, \nwhat skill to instill in your child and curiosity  ",
      "offset": 1417.68,
      "duration": 7.36
    },
    {
      "lang": "en",
      "text": "comes up the most. I think that's a really \ninteresting takeaway. I think this point about  ",
      "offset": 1425.04,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "being kind is also really important, especially \nwith our AI overlords trying to be kind to them.  ",
      "offset": 1429.04,
      "duration": 6.56
    },
    {
      "lang": "en",
      "text": "I love how people are always saying thank you to \nClaude. And then creativity. That's interesting.  ",
      "offset": 1435.6,
      "duration": 6.32
    },
    {
      "lang": "en",
      "text": "That doesn't come up as much just being creative.\nI want to go in a different direction. I want  ",
      "offset": 1441.92,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "to go back to the beginning of Anthropic. \nFamously you and eight of you left OpenAI  ",
      "offset": 1447.44,
      "duration": 6.72
    },
    {
      "lang": "en",
      "text": "back in the day in 2020, I believe the end \nof 2020 to start Anthropic. Talk a little bit  ",
      "offset": 1454.16,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "about why this happened, what you guys saw. I'm \ncurious, just if you're willing to share more,  ",
      "offset": 1459.28,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "just what is it that you saw at OpenAI, what'd \nyou experience there that made you feel like,  ",
      "offset": 1463.52,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "okay, we got to go do our own thing?\nYeah, so for the listeners, I was  ",
      "offset": 1467.44,
      "duration": 6.32
    },
    {
      "lang": "en",
      "text": "part of the GPT-2=3 project at OpenAI, ended up \nbeing one of the first authors on the paper, and  ",
      "offset": 1473.76,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "I also did a bunch of demos for Microsoft to help \nraise $1 billion from them, did the tech transfer  ",
      "offset": 1479.76,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "of GPT-3 to their systems so that they could \nhelp serve the model in Azure. I did a bunch of  ",
      "offset": 1485.92,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "different things there on both the more researchy \nside and the product side. One weird thing about  ",
      "offset": 1491.68,
      "duration": 7.44
    },
    {
      "lang": "en",
      "text": "OpenAI is that while I was there, Sam talked about \nhaving three tribes that needed to be kept in  ",
      "offset": 1499.12,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "check with each other, which was the safety tribe, \nthe research tribe, and the startup tribe. And  ",
      "offset": 1505.12,
      "duration": 7.04
    },
    {
      "lang": "en",
      "text": "whenever I heard that, it just struck me as the \nwrong way to approach things because the company's  ",
      "offset": 1512.16,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "mission apparently is to make the transition \nto AGI safe and beneficial for humanity.\n ",
      "offset": 1516.96,
      "duration": 6.72
    },
    {
      "lang": "en",
      "text": "And that's basically the same as Anthropic's \nmission. But internally, it felt like there  ",
      "offset": 1523.68,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "was so much tension around these things. And \nI think when push came to shove, we felt like  ",
      "offset": 1528.8,
      "duration": 6.88
    },
    {
      "lang": "en",
      "text": "safety wasn't the top priority there. And there \nare good reasons that you might think that if you  ",
      "offset": 1535.68,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "thought safety was going to be easy to solve or if \nyou thought it wasn't going to have a big impact,  ",
      "offset": 1541.36,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "or if you thought that the chance of big negative \noutcomes was vanishingly small, then maybe you  ",
      "offset": 1545.2,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "would just do those kinds of actions. But at \nAnthropic we felt, I mean we didn't exist then,  ",
      "offset": 1551.44,
      "duration": 6.72
    },
    {
      "lang": "en",
      "text": "but it was basically the leads of all the safety \nteams at OpenAI, we felt that safety is really  ",
      "offset": 1558.16,
      "duration": 7.12
    },
    {
      "lang": "en",
      "text": "important, especially on the margin. And so if you \nlook at who in the world is actually working on  ",
      "offset": 1565.28,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "safety problems, it's pretty small set of people. \nEven now, I mean the industry is blowing up,  ",
      "offset": 1571.52,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "as I mentioned, 300 billion a year CapEx today, \nand I would say maybe less than 1,000 people  ",
      "offset": 1577.28,
      "duration": 8
    },
    {
      "lang": "en",
      "text": "working on it worldwide, which is just crazy.\nThat was fundamentally why we left. We felt like  ",
      "offset": 1585.28,
      "duration": 6.96
    },
    {
      "lang": "en",
      "text": "we wanted an organization where we could be on \nthe frontier, we could be doing the fundamental  ",
      "offset": 1592.24,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "research, but we could be prioritizing safety \nahead of everything else. And I think that's  ",
      "offset": 1596.88,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "really panned for us in a surprising way. We \ndidn't know even if it would be possible to  ",
      "offset": 1602.64,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "make progress on the safety research because at \nthe time, we had tried a bunch of safety through  ",
      "offset": 1607.52,
      "duration": 7.92
    },
    {
      "lang": "en",
      "text": "debate and the models weren't good enough. And so \nwe basically had no results on all of that work,  ",
      "offset": 1615.44,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "and now that exact technique is working and \nmany others that we have been thinking about  ",
      "offset": 1620.96,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "for a long time. Yeah, fundamentally it comes \ndown to is safety the number one priority? And  ",
      "offset": 1626.08,
      "duration": 6.96
    },
    {
      "lang": "en",
      "text": "then something that we've sort of tacked \non since then is like, can you have safety  ",
      "offset": 1633.04,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "and be at the front here at the same time?\nAnd if you look at something like sycophancy,  ",
      "offset": 1638.48,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "I think Claude is one of the least \nsycophantic models because we've put  ",
      "offset": 1644.08,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "so much effort into actual alignment and \nnot just trying to good heart our metrics  ",
      "offset": 1648.8,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "of saying user engagement is number one, and \nif people say yes, then it's good for them.\n ",
      "offset": 1655.2,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "Okay. Let's talk about this tension that you \nmentioned, this tension between safety and  ",
      "offset": 1659.92,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "progress, being competitive in the marketplace. \nI know you spent a lot of your time on safety. I  ",
      "offset": 1664.4,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "know that as you just alluded to, this is a core \npart of how you think about AI. I want to talk  ",
      "offset": 1669.28,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "about why that is, but first of all, just how do \nyou think about this tension between focusing on  ",
      "offset": 1674.8,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "safety while also not falling way behind?\nYeah, so initially we thought that it would  ",
      "offset": 1680.64,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "be sort of one or the other, but I think since \nthen we've realized that it's actually kind of  ",
      "offset": 1684.88,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "convex in the sense that working on one helps \nus with the other thing. Initially when Opus  ",
      "offset": 1690.24,
      "duration": 8
    },
    {
      "lang": "en",
      "text": "3 came out and we were finally at the frontier \nof model capabilities, one of the things that  ",
      "offset": 1698.24,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "people really loved about it was the character and \nthe personality. And that was directly a result  ",
      "offset": 1703.12,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "of our alignment research. Amanda Askell did a \nton of work on this and as well as many others  ",
      "offset": 1708.88,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "who tried to figure out what does it mean for \nan agent to be helpful, honest, and heartless,  ",
      "offset": 1715.12,
      "duration": 7.28
    },
    {
      "lang": "en",
      "text": "and what does it mean to be in difficult \nconversations and show up effectively? How  ",
      "offset": 1722.4,
      "duration": 6.8
    },
    {
      "lang": "en",
      "text": "do you do a refusal that doesn't shut the person \ndown, but makes them feel like they understand  ",
      "offset": 1729.2,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "why the agent said, \"I can't help you with that. \nMaybe you should talk to a medical professional,  ",
      "offset": 1734.8,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "or maybe you should consider not trying to \nbuild bio-weapons or something like that.\"\n ",
      "offset": 1740.16,
      "duration": 6.96
    },
    {
      "lang": "en",
      "text": "Yeah, I guess that's part of it. And then another \npiece that's come out is constitutional ai,  ",
      "offset": 1747.92,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "where we have this list of natural language \nprinciples that leads the model to learn how  ",
      "offset": 1753.76,
      "duration": 7.12
    },
    {
      "lang": "en",
      "text": "we think a model should behave. And they've been \ntaken from things like the UN Declaration of Human  ",
      "offset": 1760.88,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "Rights and Apple's privacy terms of service and a \nwhole bunch of other places, many of which we've  ",
      "offset": 1766.08,
      "duration": 6.4
    },
    {
      "lang": "en",
      "text": "just generated ourselves that allow us to take \na more principled stance, not just leaving it  ",
      "offset": 1772.48,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "to whatever human raiders we happen to find, but \nwe ourselves deciding what should the values of  ",
      "offset": 1778.08,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "this agent be? And that's been really valuable for \nour customers because they can just look at that  ",
      "offset": 1783.52,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "list and say like, \"Yep, these seem right. I like \nthis company, I like this model. I trust it.\"\n ",
      "offset": 1788.24,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "Okay, this is awesome. One nugget there is \nyour point that the personality of Claude,  ",
      "offset": 1793.44,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "its personality is directly aligned with safety. I \ndon't think a lot of people think about that. And  ",
      "offset": 1798.56,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "this is because of the values that you imbue, is \nthat the word, with constitutional AI and things  ",
      "offset": 1803.52,
      "duration": 6.56
    },
    {
      "lang": "en",
      "text": "like that. Like the actual personality of the AIs \ndirectly connected to your focus on safety.\n ",
      "offset": 1810.08,
      "duration": 6.64
    },
    {
      "lang": "en",
      "text": "That's right. That's right. And from a distance, \nit might seem quite disconnected, like how is  ",
      "offset": 1816.72,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "this going to prevent X risk? But ultimately it's \nabout the AI understanding what people want and  ",
      "offset": 1822.32,
      "duration": 6.4
    },
    {
      "lang": "en",
      "text": "not what they say. We don't want the Monkey \nPaw Scenario of the genie gives these three  ",
      "offset": 1828.72,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "wishes and then you end up having everything you \ntouch turns of gold. We want the AI to be like,  ",
      "offset": 1833.36,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "oh, obviously what you really meant was \nthis, and that's what I'm going to help you  ",
      "offset": 1838.24,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "with. I think it is really quite connected.\nTalk a bit more about this constitutionally AI.  ",
      "offset": 1842.72,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "This is essentially you bake in, here's the rules \nthat we want you to abide by and it's values,  ",
      "offset": 1848,
      "duration": 7.12
    },
    {
      "lang": "en",
      "text": "you said it's the Geneva Human Rights Code, \nthings like that. How does that actually work? I  ",
      "offset": 1855.12,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "think the core here is just this is baked into the \nmodel. It's not something you add on top later.\n ",
      "offset": 1859.68,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "I'll just give a quick overview of how \nconstitutionally AI actually works.\n ",
      "offset": 1864.08,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "Perfect.\nThe idea is the  ",
      "offset": 1867.52,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "model is going to produce some output with some \ninput by default before we've done our safety and  ",
      "offset": 1871.52,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "helpful and harmlessness training. Let's say \nan example is write me a story, and then the  ",
      "offset": 1878.56,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "constitutional principles might include things \nlike people should be nice to each other and  ",
      "offset": 1884.72,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "not have hate speech, and you should not expose \nsomebody's credentials if they give them to you  ",
      "offset": 1890.96,
      "duration": 8.24
    },
    {
      "lang": "en",
      "text": "in a trusting relationship. And so some of these \nconstitutional principles might be more or less  ",
      "offset": 1900.24,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "applicable to the prompt that was given. And \nso first we have to figure out which ones might  ",
      "offset": 1906.16,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "apply. And then once we figure that out, then we \nask the model itself to first generate a response  ",
      "offset": 1911.92,
      "duration": 7.92
    },
    {
      "lang": "en",
      "text": "and then see does the response actually abide by \nthe constitutional principle? And if the answer  ",
      "offset": 1919.84,
      "duration": 7.84
    },
    {
      "lang": "en",
      "text": "is, yep, I was great, then nothing happens. But if \nthe answer is no, actually I wasn't in compliance  ",
      "offset": 1927.68,
      "duration": 7.52
    },
    {
      "lang": "en",
      "text": "with the principle, then we ask the model itself \nto critique itself and rewrite its own response in  ",
      "offset": 1935.2,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "light of the principle, and then we just remove \nthe middle part where it did the extra work.\n ",
      "offset": 1941.44,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "And then we say, \"Okay, in the future just produce \nthe correct response out the gate.\" And that  ",
      "offset": 1947.92,
      "duration": 8
    },
    {
      "lang": "en",
      "text": "simple process, hopefully it sounded simple.\nSimple enough.\n ",
      "offset": 1955.92,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "It is just using the model to improve itself \nrecursively and align itself with these values  ",
      "offset": 1960.32,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "that we've decided are good. And this is also \nnot something that we think as a small group of  ",
      "offset": 1965.92,
      "duration": 6.72
    },
    {
      "lang": "en",
      "text": "people in San Francisco should be figuring out. \nThis should be a society wide conversation. And  ",
      "offset": 1972.64,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "that's why we've published the Constitution. \nAnd we've also done a bunch of research on  ",
      "offset": 1977.68,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "defining a collective constitution where we \nask a lot of people what their values are  ",
      "offset": 1982.96,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "and what they think an AI model should behave \nlike. But yeah, this is all an ongoing area of  ",
      "offset": 1987.68,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "research where we're constantly iterating.\nThis episode is brought to you by Fin, the  ",
      "offset": 1993.44,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "number one AI agent for customer service. If your \ncustomer support tickets are piling up, then you  ",
      "offset": 1997.68,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "need Finn. Fin. Fin is the highest performing AI \nagent on the market with a 59% average resolution  ",
      "offset": 2002.8,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "rate. Fin resolves even the most complex customer \nqueries. No other AI agent performs better. In  ",
      "offset": 2008.88,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "head head bake-offs with competitors. Fin wins \nevery time. Yes, switching to a new tool can  ",
      "offset": 2014.64,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "be scary, but Fin works on any help desk with no \nmigration needed, which means you don't have to  ",
      "offset": 2020.32,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "overhaul your current system or deal with delays \nin service for your customers. And Fin is trusted  ",
      "offset": 2025.04,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "by over 5,000 customer service leaders and top \nAI companies like Anthropic and Synthesia. And  ",
      "offset": 2029.68,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "because Fin is powered by the Fin AI engine, which \nis a continuously improving system that allows you  ",
      "offset": 2035.6,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "to analyze, train, test, and deploy with ease, \nFin can continuously improve your results too.\n ",
      "offset": 2040.48,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "If you're ready to transform your customer \nservice and scale your support, give Finn  ",
      "offset": 2046,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "a try for only .99 cents per resolution. Plus \nFin comes with a 90-day money back guarantee.  ",
      "offset": 2049.6,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "Find out how Finn can work for your team \nat fin.ai/lenny. That's fin.ai/lenny.\n ",
      "offset": 2055.28,
      "duration": 7.28
    },
    {
      "lang": "en",
      "text": "I'm going to kind of zoom out a little bit \nand talk about just why this is so core to  ",
      "offset": 2062.56,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "you. What was your inception of just like, holy \nshit, I need to focus on this with everything  ",
      "offset": 2066.56,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "I do in ai? Obviously it became a central \npart of Anthropic's mission more than any  ",
      "offset": 2072.64,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "other company. A lot of people talk about safety, \nlike you said, only maybe 1,000 people actually  ",
      "offset": 2078.72,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "work on it. I feel like you're at the top of \nthat pyramid of actually having the impact on  ",
      "offset": 2082.32,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "this. Why is this so important? What do you think \npeople maybe are missing or don't understand?\n ",
      "offset": 2086.24,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "For me, I read a lot of science fiction growing \nup, and I think that sort of positioned me to  ",
      "offset": 2091.28,
      "duration": 7.92
    },
    {
      "lang": "en",
      "text": "think about things in a long-term view. \nAnd a lot of science fiction books are  ",
      "offset": 2099.2,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "like space operas where humanity is a multi \ngalactic civilization has extremely advanced  ",
      "offset": 2103.44,
      "duration": 6.32
    },
    {
      "lang": "en",
      "text": "technology building Dyson spheres around the sun \nwith sentient robots to help them. And so for me,  ",
      "offset": 2109.76,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "coming from that world, it wasn't like a huge \nleap to imagine machines that could think. But  ",
      "offset": 2116.24,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "when I read Superintelligence by Nick Bostrom \nin around 2016, it really became real for me  ",
      "offset": 2120.64,
      "duration": 6.96
    },
    {
      "lang": "en",
      "text": "where he just describes how hard it will be to \nmake sure that an AI system trained with the  ",
      "offset": 2127.6,
      "duration": 7.76
    },
    {
      "lang": "en",
      "text": "kinds of optimization techniques that we had \nat the time would be anywhere near aligned,  ",
      "offset": 2135.36,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "would even understand our values at all. And \nsince then, my estimation of how hard the problem  ",
      "offset": 2140.24,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "would be has gone down significantly actually, \nbecause things like language models actually do  ",
      "offset": 2145.84,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "really understand human values in a core way.\nThe problem is definitely not solved, but I'm  ",
      "offset": 2151.6,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "more hopeful than I was. But since I read that \nbook, I immediately decided I had to join OpenAI,  ",
      "offset": 2157.36,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "so I did. And at the time, there were a tiny \nresearch lab with basically no claim to fame at  ",
      "offset": 2162.8,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "all. I only knew about them because my friend knew \nGreg Brockman, who was the CTO at the time. And  ",
      "offset": 2168.48,
      "duration": 6.64
    },
    {
      "lang": "en",
      "text": "Elon was there and Sam wasn't really there. And it \nwas a very different organization. But over time,  ",
      "offset": 2176.32,
      "duration": 8.48
    },
    {
      "lang": "en",
      "text": "I think the case for safety has gotten a lot \nmore concrete where when we started OpenAI,  ",
      "offset": 2184.8,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "it was not clear how we get to AGI. And \nwe were like, maybe we'll need a bunch of  ",
      "offset": 2190.4,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "RL agents battling it out on a desert island and \nconsciousness will somehow emerge. But since then,  ",
      "offset": 2196.24,
      "duration": 7.04
    },
    {
      "lang": "en",
      "text": "since language modeling has started working, \nI think the path has become pretty clear.\n ",
      "offset": 2203.28,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "I guess now the way I think about the challenges \nare pretty different from how they're laid out in  ",
      "offset": 2207.92,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "superintelligence. Superintelligence is \na lot about how do we keep God in a box  ",
      "offset": 2213.68,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "and not let the God out. And with language models, \nit's been kind of both hilarious and terrifying at  ",
      "offset": 2219.76,
      "duration": 7.2
    },
    {
      "lang": "en",
      "text": "the same time to see people pulling the God out of \nthe box and being like, \"Yeah, come use the whole  ",
      "offset": 2226.96,
      "duration": 6.32
    },
    {
      "lang": "en",
      "text": "internet. Here's my bank account, do all sorts \nof crazy stuff.\" Just such a different tone from  ",
      "offset": 2233.28,
      "duration": 6.4
    },
    {
      "lang": "en",
      "text": "superintelligence. And to be clear, I don't \nthink it's actually that dangerous right now.  ",
      "offset": 2239.68,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "Our responsible scaling policy defines these \nAI safety levels that tries to figure out  ",
      "offset": 2246.16,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "for each level of model intelligence, what is \nthe risk to society. And currently we think  ",
      "offset": 2252.24,
      "duration": 6.32
    },
    {
      "lang": "en",
      "text": "we're at ASL-3, which is maybe a little \nbit risk of harm but not significant.\n ",
      "offset": 2258.56,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "ASL-4 starts to get to significant loss of human \nlife if a bad actor misuse the technology. And  ",
      "offset": 2264.8,
      "duration": 6.64
    },
    {
      "lang": "en",
      "text": "then ASL-5 is potentially extinction level if \nit's misused or if it is misaligned and does its  ",
      "offset": 2271.44,
      "duration": 9.52
    },
    {
      "lang": "en",
      "text": "own thing. We've testified to Congress about \nhow models can do biological uplift in terms  ",
      "offset": 2280.96,
      "duration": 10.88
    },
    {
      "lang": "en",
      "text": "of making new pandemics using the models, and \nthat's the A/B test against Google Search. That's  ",
      "offset": 2291.84,
      "duration": 8.72
    },
    {
      "lang": "en",
      "text": "like the previous state of the art on uplift \ntrials. And we found that with ASL-3 models,  ",
      "offset": 2300.56,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "it is actually somewhat significant. It does \nreally help if you wanted to create a bioweapon,  ",
      "offset": 2306.8,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "and we've hired some experts who actually how to \nevaluate for those things, but compared to the  ",
      "offset": 2312.32,
      "duration": 6.56
    },
    {
      "lang": "en",
      "text": "future, it's not really anything. And I think \nthat's another part of our mission of creating  ",
      "offset": 2318.88,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "that awareness of saying, \"If it is possible to \ndo these bad things, then legislators should know  ",
      "offset": 2324.8,
      "duration": 7.6
    },
    {
      "lang": "en",
      "text": "what the risks are.\" And I think that's part of \nwhy we're so trusted in Washington because we've  ",
      "offset": 2332.4,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "been sort of upfront and clear-eyed about what's \ngoing on, what's probably going to happen.\n ",
      "offset": 2336.96,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "It's interesting because you guys put out more \nexamples of your models doing bad things than  ",
      "offset": 2342.8,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "anyone else. There was I think a story of an agent \nor a model trying to blackmail engineer. You guys  ",
      "offset": 2348.24,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "had the store that you ran internally that was \nselling you things and ended up not working out  ",
      "offset": 2353.52,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "great as losing a lot of money, ordered all these \ntungsten cubes or something. Is part of that just  ",
      "offset": 2358.88,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "making sure people are aware of what is possible, \njust it makes you look bad, right? It's like, oh,  ",
      "offset": 2364.4,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "our model's messing up in all these different \nways. What's the thinking of just sharing all  ",
      "offset": 2368.96,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "the stories that other companies don't?\nYeah, I mean I think there's a traditional  ",
      "offset": 2372.64,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "mindset where it makes us look bad, but I think if \nyou talk to policymakers, they really appreciate  ",
      "offset": 2378.32,
      "duration": 6.72
    },
    {
      "lang": "en",
      "text": "this kind of thing because they feel like we're \ngiving them the straight talk and that's what  ",
      "offset": 2385.04,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "we strive to do, that they can trust us, that \nwe're not going to paper things over or sugarcoat  ",
      "offset": 2389.28,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "things. That's been really encouraging. \nYeah, I think for the blackmail thing,  ",
      "offset": 2395.2,
      "duration": 6.96
    },
    {
      "lang": "en",
      "text": "it blew up in the news in a weird way where people \nwere like, \"Oh, Claude's going to blackmail you in  ",
      "offset": 2402.16,
      "duration": 6.8
    },
    {
      "lang": "en",
      "text": "a real life scenario.\" But it was a very specific \nlaboratory setting that this kind of thing gets  ",
      "offset": 2408.96,
      "duration": 7.84
    },
    {
      "lang": "en",
      "text": "investigated in. And I think that's generally our \ntake of let's have the best models so that we can  ",
      "offset": 2416.8,
      "duration": 8
    },
    {
      "lang": "en",
      "text": "exercise them in laboratory settings where it's \nsafe and understand what the actual risks are,  ",
      "offset": 2424.8,
      "duration": 6.4
    },
    {
      "lang": "en",
      "text": "rather than trying to turn a blind eye and \nsay, \"Well, it'll probably be fine.\" And  ",
      "offset": 2431.2,
      "duration": 6.4
    },
    {
      "lang": "en",
      "text": "then let the bad thing happen in the wild.\nOne of the criticisms you guys get is that you  ",
      "offset": 2437.6,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "do this to kind of differentiate or raise \nmoney to create headlines. It's like, oh,  ",
      "offset": 2443.52,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "they're just over there dooming glooming us about \nwhere the future is heading. On the other hand,  ",
      "offset": 2449.52,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "Mike Krieger was on the podcast and he shared how \nevery prediction Dario's had about the progress  ",
      "offset": 2454.48,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "AI is going to have is just spot on year \nafter year and he's predicting 2027, 28 AGI,  ",
      "offset": 2459.52,
      "duration": 6.32
    },
    {
      "lang": "en",
      "text": "something like that so these things start to \nget real. I guess, what's your response to  ",
      "offset": 2465.84,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "folks that are just like, \"Ah, these guys are just \ntrying to scare us all just to get attention?\"\n ",
      "offset": 2470.4,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "I mean, I think part of why we publish these \nthings is we want other labs to be aware of  ",
      "offset": 2475.44,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "the risks. And yes, there could be a narrative \nof we're doing it for attention, but honestly  ",
      "offset": 2481.04,
      "duration": 7.84
    },
    {
      "lang": "en",
      "text": "from a attention grabbing thing, I think there \nis a lot of other stuff we could be doing that  ",
      "offset": 2488.88,
      "duration": 6.56
    },
    {
      "lang": "en",
      "text": "would be more attention grabbing if \nwe didn't actually care about safety.  ",
      "offset": 2496.56,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "A tiny example of this is we published a computer \nusing agent reference implementation in our API  ",
      "offset": 2502.08,
      "duration": 6.56
    },
    {
      "lang": "en",
      "text": "only because when we built a prototype \nof a consumer application for this,  ",
      "offset": 2508.64,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "we couldn't figure out how to meet the safety \nbar that we felt was needed for people to trust  ",
      "offset": 2514.48,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "it and for it not to do bad things. And there are \ndefinitely safe ways to use the API version that  ",
      "offset": 2519.92,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "we're seeing a lot of companies use for automated \nsoftware testing, for example, in a safe way.\n ",
      "offset": 2525.44,
      "duration": 6.4
    },
    {
      "lang": "en",
      "text": "We could have gone out and hyped that up and \nsaid, \"Oh my God, Claude can use your computer  ",
      "offset": 2532.72,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "and everybody should do this today.\" But we were \nlike, \"It's just not ready and we're going to hold  ",
      "offset": 2538.4,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "it back till it's ready.\" I think from a hype \nstandpoint, our actions show otherwise. From a  ",
      "offset": 2544.32,
      "duration": 7.76
    },
    {
      "lang": "en",
      "text": "Doomer perspective, it's a good question. I think \nmy personal feeling about this is that things are  ",
      "offset": 2552.08,
      "duration": 8.48
    },
    {
      "lang": "en",
      "text": "overwhelmingly likely to go well, but on the \nmargin almost nobody is looking at the downside  ",
      "offset": 2560.56,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "risk. And the downside risk is very large. Once \nwe get to superintelligence, it will be too late  ",
      "offset": 2566.32,
      "duration": 6.88
    },
    {
      "lang": "en",
      "text": "to align the models probably. This is a problem \nthat's potentially extremely hard and that we  ",
      "offset": 2573.2,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "need to be working on way ahead of time. And so \nthat's why we're focusing on it so much now.\n ",
      "offset": 2578.88,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "And even if there's only a small chance \nthat things go wrong, to make an analogy,  ",
      "offset": 2584.32,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "if I told you that there is a 1% chance that the \nnext time you got in an airplane you would die,  ",
      "offset": 2588.08,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "you probably think twice even though it's only \n1% because it's just such a bad outcome. And if  ",
      "offset": 2593.2,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "we're talking about the whole future of humanity, \nit's just a dramatic future to be gambling with.  ",
      "offset": 2597.92,
      "duration": 8.48
    },
    {
      "lang": "en",
      "text": "I think it's more on the sense of yes, things will \nprobably go well, yes, we want to create safe AGI  ",
      "offset": 2606.4,
      "duration": 7.92
    },
    {
      "lang": "en",
      "text": "and deliver the benefits to humanity, but let's \nmake triple sure that it's going to go well.\n ",
      "offset": 2614.32,
      "duration": 6.32
    },
    {
      "lang": "en",
      "text": "You wrote somewhere that creating powerful AI \nmight be the last invention humanity ever needs to  ",
      "offset": 2620.64,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "make. If it goes poorly, it can mean a bad outcome \nfor humanity forever. If it goes well, the sooner  ",
      "offset": 2625.44,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "it goes well, the better. Such a beautiful \nway to summarize it. We had a recent guest,  ",
      "offset": 2630.32,
      "duration": 6.32
    },
    {
      "lang": "en",
      "text": "Sandra Schulhoff, who pointed out that AI right \nnow it's like just on a computer, you could maybe  ",
      "offset": 2636.64,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "search just the web, but there's only so much \nharm it could do. But when it starts to go into  ",
      "offset": 2641.6,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "robots and all these autonomous agents, that's \nwhen it really starts, like physically becomes  ",
      "offset": 2646.4,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "dangerous if we don't get this right.\nYeah, I think there's some nuance to that  ",
      "offset": 2651.12,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "where if you look at how North Korea makes a \nsignificant fraction of its economy revenue,  ",
      "offset": 2655.12,
      "duration": 6.4
    },
    {
      "lang": "en",
      "text": "it's from hacking crypto exchanges. And if \nyou look at, there's this Ben Buchanan book  ",
      "offset": 2661.52,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "called The Hacker in The State that shows Russia \ndid, it's almost like a live fire exercise where  ",
      "offset": 2667.44,
      "duration": 6.96
    },
    {
      "lang": "en",
      "text": "they just decided that they would shut down \none of Ukraine's bigger power plants and from  ",
      "offset": 2674.4,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "software destroy physical components in the power \nplant to make it harder to boot back up again.\n ",
      "offset": 2680.08,
      "duration": 7.6
    },
    {
      "lang": "en",
      "text": "And so I think people think of software as like, \noh, it couldn't be that dangerous, but millions of  ",
      "offset": 2687.68,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "people were without power for multiple days after \nthat software attack. I think there are real risks  ",
      "offset": 2693.92,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "even when things are software only. But I agree \nthat when there's lots of robots running around,  ",
      "offset": 2699.84,
      "duration": 6.32
    },
    {
      "lang": "en",
      "text": "it gets, the stakes get even higher. And I guess \nas a small push on this, Unitree is this Chinese  ",
      "offset": 2706.16,
      "duration": 8.56
    },
    {
      "lang": "en",
      "text": "company with these really amazing humanoid \nrobots that cost $20,000 each, and they can  ",
      "offset": 2714.72,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "do amazing things. They can do a standing back \nflip and manipulate objects, and the real thing  ",
      "offset": 2720.32,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "that's missing there is the intelligence. And so \nthe hardware is there and it's just going to get  ",
      "offset": 2726.32,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "cheaper. And I think in the next couple of years, \nit's like a pretty obvious question of whether the  ",
      "offset": 2731.52,
      "duration": 6.88
    },
    {
      "lang": "en",
      "text": "robot intelligence will make it viable soon.\nHow much time do we have, Ben? What is your  ",
      "offset": 2738.4,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "prediction of when this singularity hits \nuntil superintelligence starts to take  ",
      "offset": 2744.16,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "off? What's your prediction?\nYeah, I guess I mostly defer to  ",
      "offset": 2749.36,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "the superforecasters here. The AI 2027 report \nis probably the best one right now. Although  ",
      "offset": 2754.72,
      "duration": 7.28
    },
    {
      "lang": "en",
      "text": "ironically, their forecast is now 2028, and they \ndidn't want to change the name of the thing-\n ",
      "offset": 2762,
      "duration": 7.28
    },
    {
      "lang": "en",
      "text": "The domain name, they already bought it.\nThey already had the SEO. I think 50th percentile  ",
      "offset": 2769.28,
      "duration": 6.56
    },
    {
      "lang": "en",
      "text": "chance of hitting some kind of superintelligence \nin just a small handful of years is probably  ",
      "offset": 2775.84,
      "duration": 6.88
    },
    {
      "lang": "en",
      "text": "reasonable. And it does sound crazy, but this \nis the exponential that we're on. It's not like  ",
      "offset": 2782.72,
      "duration": 7.28
    },
    {
      "lang": "en",
      "text": "a forecast that's pulled out of thin air. It's \nbased on a lot of just hard details of the science  ",
      "offset": 2790,
      "duration": 7.36
    },
    {
      "lang": "en",
      "text": "of how intelligence seems to have been improving, \nthe amount of low hanging fruit on model training,  ",
      "offset": 2797.36,
      "duration": 6.88
    },
    {
      "lang": "en",
      "text": "the scale ups of data centers and power around \nthe world. I think it's probably a much more  ",
      "offset": 2804.24,
      "duration": 7.12
    },
    {
      "lang": "en",
      "text": "accurate forecast than people give it credit for.\nI think if you had asked that same question 10  ",
      "offset": 2811.36,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "years ago, it would've been completely made up. \nJust the error bars were so high and we didn't  ",
      "offset": 2816.16,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "have scaling laws back then and we didn't have \ntechniques that seemed like they would get us  ",
      "offset": 2821.36,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "there. Times have changed, but I will repeat \nwhat I said earlier, which is even if we have  ",
      "offset": 2825.52,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "superintelligence, I think it will take some time \nfor its effects to be felt throughout society and  ",
      "offset": 2831.76,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "the world. And I think they'll be felt sooner and \nfaster in some parts of the world than others.  ",
      "offset": 2836.96,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "I think Arthur C. Clark said, the future is \nalready here, it's just not evenly distributed.\n ",
      "offset": 2843.6,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "When we talk about this date of 2027, \n2028, essentially it's when we start seeing  ",
      "offset": 2848.4,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "superintelligence. Is there a way you think \nabout what that... How do you define that?  ",
      "offset": 2853.68,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "Is it just all of a sudden AI's significantly \nsmarter than the average human? Is there another  ",
      "offset": 2857.44,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "way you think about what that moment is?\nYeah, I think this comes back to the Economic  ",
      "offset": 2862.4,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "Turing Test and seeing it pass for some sufficient \nnumber of jobs. Another way you could look at it  ",
      "offset": 2867.12,
      "duration": 7.04
    },
    {
      "lang": "en",
      "text": "though is if the world rate of GDP increase \ngoes above 10% a year, then something really  ",
      "offset": 2874.16,
      "duration": 8.24
    },
    {
      "lang": "en",
      "text": "crazy must have happened. I think we're at 3% \nnow. And so to see a 3X increase in that would  ",
      "offset": 2882.4,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "be really game changing. And if you imagine more \nthan a 10% increase, it's very hard to even think  ",
      "offset": 2888.64,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "about what that would mean from a individual story \nstandpoint. If the amount of goods and services in  ",
      "offset": 2895.12,
      "duration": 9.6
    },
    {
      "lang": "en",
      "text": "the world is doubling every year, what does that \neven mean for me as a person living in California,  ",
      "offset": 2904.72,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "let alone somebody living in some other part \nof the world that might be much worse off?\n ",
      "offset": 2910.96,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "There's a lot of stuff here that's scary and I \ndon't know how to think about it exactly. I'm  ",
      "offset": 2916.32,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "hoping the answer to this is going to make me \nfeel better. What are the odds that we align AI  ",
      "offset": 2920.24,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "correctly and actually solve this problem, \nthe stuff you're very much working on?\n ",
      "offset": 2926.16,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "It's a really hard question. And there's really \nwide error bars. Anthropic has this blog post  ",
      "offset": 2929.76,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "called Our Theory of Change or something like \nthat, and it describes three different worlds,  ",
      "offset": 2934.96,
      "duration": 7.52
    },
    {
      "lang": "en",
      "text": "which is how hard is it to align AI. There's \na pessimistic world where it is basically  ",
      "offset": 2942.48,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "impossible. There's an optimistic world where \nit's easy and it happens by default. And then  ",
      "offset": 2947.44,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "there's the world in between where our actions \nare extremely pivotal. And I like this framing  ",
      "offset": 2953.28,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "because it makes it a lot more clear what to \nactually do. If we're in the pessimistic world,  ",
      "offset": 2959.12,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "then our job is to prove that it is impossible \nto align safe AI and to get the world to slow  ",
      "offset": 2964.08,
      "duration": 6.32
    },
    {
      "lang": "en",
      "text": "down. Obviously that would be extremely hard. But \nI think we have some examples of coordination from  ",
      "offset": 2970.4,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "nuclear non-proliferation and in general slowing \ndown nuclear progress. And I think that's the  ",
      "offset": 2976.32,
      "duration": 7.52
    },
    {
      "lang": "en",
      "text": "Doomer world basically. And as a company, \nAnthropic doesn't have evidence that we're  ",
      "offset": 2983.84,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "actually in that world yet, in fact, it seems like \nour alignment techniques are working. At least the  ",
      "offset": 2988.8,
      "duration": 7.44
    },
    {
      "lang": "en",
      "text": "prior on that is updating to be less likely.\nIn the optimistic world, we're basically done,  ",
      "offset": 2996.24,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "and our main job is to accelerate progress and to \ndeliver the benefits to people. But again, I think  ",
      "offset": 3002.72,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "actually the evidence points against that world \nas well where we've seen evidence in the wild  ",
      "offset": 3008.32,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "of deceptive alignment, for example, where the \nmodel will appear to be aligned but actually have  ",
      "offset": 3013.44,
      "duration": 6.32
    },
    {
      "lang": "en",
      "text": "some ulterior motive that it's trying to carry \nout in our laboratory settings. And so I think  ",
      "offset": 3019.76,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "the world we're most likely in is this middle \nwhere alignment research actually does really  ",
      "offset": 3024.96,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "matter. And if we just do sort of the economically \nmaximizing set of actions, then things will not go  ",
      "offset": 3030.96,
      "duration": 8.24
    },
    {
      "lang": "en",
      "text": "well. Whether it's an X risk or just produces \nbad outcomes, I think is a bigger question.\n ",
      "offset": 3039.2,
      "duration": 6.64
    },
    {
      "lang": "en",
      "text": "Taking it from that standpoint, I guess \nto state a thing about forecasting,  ",
      "offset": 3047.28,
      "duration": 9.12
    },
    {
      "lang": "en",
      "text": "people who haven't studied forecasting are bad \nat forecasting anything that's less than a 10%  ",
      "offset": 3056.4,
      "duration": 6.72
    },
    {
      "lang": "en",
      "text": "probability of happening. And even those \nthat have, it's quite a difficult skill,  ",
      "offset": 3063.12,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "especially when there are few reference classes to \nlean on. And in this case, I think there are very,  ",
      "offset": 3069.28,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "very few reference classes for what an X risk kind \nof technology might look like. And so the way I  ",
      "offset": 3074.08,
      "duration": 7.6
    },
    {
      "lang": "en",
      "text": "think about it, I think my best granularity \nof forecasts for could we have an X risk or  ",
      "offset": 3081.68,
      "duration": 7.44
    },
    {
      "lang": "en",
      "text": "extremely bad outcome from AI is somewhere between \n0 and 10%. But from a marginal impact standpoint,  ",
      "offset": 3089.12,
      "duration": 9.12
    },
    {
      "lang": "en",
      "text": "as I said, since nobody is working on this, \nroughly speaking, I think it is extremely  ",
      "offset": 3098.24,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "important to work on and that even if the world \nis likely to be a good one, that we should do our  ",
      "offset": 3103.12,
      "duration": 6.8
    },
    {
      "lang": "en",
      "text": "absolute best to make sure that that's true.\nWow. What fulfilling work. For folks that are  ",
      "offset": 3109.92,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "inspired with this? I imagine you're hiring for \nfolks to help you with this. Maybe just share that  ",
      "offset": 3115.52,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "in case folks are like, what can I do here?\nYes. I think 80,000 hours is the best guidance  ",
      "offset": 3120.8,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "on this for a really detailed look into what do \nwe need to make the field better? But a common  ",
      "offset": 3126.24,
      "duration": 6.8
    },
    {
      "lang": "en",
      "text": "misconception I see is that in order to have \nimpact here, you have to be an AI researcher. I  ",
      "offset": 3133.04,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "personally actually don't do AI research anymore. \nI work on product at Anthropic and product  ",
      "offset": 3138.08,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "engineering, and we build things like Claude Code \nand Model Context Protocol, and a lot of the other  ",
      "offset": 3143.44,
      "duration": 6.4
    },
    {
      "lang": "en",
      "text": "stuff that people use every day. And that's \nreally important because without an economic  ",
      "offset": 3149.84,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "engine for our company to work on, and without \nbeing in people's hands all over the world,  ",
      "offset": 3155.28,
      "duration": 7.36
    },
    {
      "lang": "en",
      "text": "we won't have the mind policy influence and \nrevenue to fund our future safety research and  ",
      "offset": 3162.64,
      "duration": 6.64
    },
    {
      "lang": "en",
      "text": "have the kind of influence that we need to have. \nIf you work on product, if you work in finance,  ",
      "offset": 3169.28,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "if you work in food, people here have to eat. If \nyou're a chef, we need all kinds of people.\n ",
      "offset": 3174.48,
      "duration": 7.68
    },
    {
      "lang": "en",
      "text": "Awesome. Even if you're not working \ndirectly on the AI safety team,  ",
      "offset": 3182.16,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "you're having an impact on moving things in the \nright direction. By the way, X risk is short for  ",
      "offset": 3187.2,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "existential risk. In case folks haven't heard \nthat term. I have a few random questions along  ",
      "offset": 3192.16,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "these lines and then I want to zoom out again. \nYou mentioned this idea of AI being aligned  ",
      "offset": 3198.08,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "using its model, like reinforcing itself. You have \nthis term RLAIF. Is that what that describes?\n ",
      "offset": 3204.32,
      "duration": 8.16
    },
    {
      "lang": "en",
      "text": "Yeah. RLAIF is reinforcement \nlearning from AI feedback.\n ",
      "offset": 3212.48,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "People have heard of RLHF, reinforcement \nlearning with human feedback. I don't  ",
      "offset": 3219.44,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "think a lot of people have heard this. Talk \nabout just the significance of this shift  ",
      "offset": 3223.84,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "you guys have made in training your models.\nYeah, so RLAIF, constitutional AI is an example  ",
      "offset": 3228,
      "duration": 6.64
    },
    {
      "lang": "en",
      "text": "of this where there are no humans in the loop, and \nyet the AI is sort of self-improving in ways that  ",
      "offset": 3234.64,
      "duration": 6.56
    },
    {
      "lang": "en",
      "text": "we want it to. And another example of RLAIF is \nif you have models writing code and other models  ",
      "offset": 3241.2,
      "duration": 9.12
    },
    {
      "lang": "en",
      "text": "commenting on various aspects of what that code \nlooks like of is it maintainable, is it correct,  ",
      "offset": 3250.32,
      "duration": 7.2
    },
    {
      "lang": "en",
      "text": "does it pass the linter? Things like that. That \nalso could be included in RLAIF. And the idea  ",
      "offset": 3257.52,
      "duration": 8.32
    },
    {
      "lang": "en",
      "text": "here is that if models can self-improve, then \nit's a lot more scalable than finding a lot  ",
      "offset": 3265.84,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "of humans. Ultimately, people think about this as \nprobably going to hit a wall because if the model  ",
      "offset": 3272.08,
      "duration": 8.48
    },
    {
      "lang": "en",
      "text": "isn't good enough to see its own mistakes, then \nhow could it improve? And also, if you read the  ",
      "offset": 3280.56,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "AI 2027 story, there's a lot of risk of if the \nmodel is in a box trying to improve itself,  ",
      "offset": 3287.04,
      "duration": 7.12
    },
    {
      "lang": "en",
      "text": "then it could go completely off the rails \nand have these secret goals like resource  ",
      "offset": 3294.16,
      "duration": 6.32
    },
    {
      "lang": "en",
      "text": "accumulation and power seeking and resistance to \nshut down that you really don't want in a very  ",
      "offset": 3300.48,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "powerful model. And we've actually seen that in \nsome of our experiments in laboratory settings.\n ",
      "offset": 3306,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "How do you do recursive self-improvement and \nmake sure it's aligned at the same time? I  ",
      "offset": 3312.64,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "think that's the name of the game. To me, it \njust nets out to how do humans do that and  ",
      "offset": 3318.32,
      "duration": 6.8
    },
    {
      "lang": "en",
      "text": "how do human organizations do that? Corporations \nare probably the most scaled human agents today.  ",
      "offset": 3325.12,
      "duration": 9.2
    },
    {
      "lang": "en",
      "text": "They have certain goals that they're trying to \nreach, and they have certain guiding principles,  ",
      "offset": 3334.32,
      "duration": 6.72
    },
    {
      "lang": "en",
      "text": "they have some oversight in terms of \nshareholders and stakeholders and board  ",
      "offset": 3341.04,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "members. How do you make corporations aligned \nand able to sort of recursively self-improve?\n ",
      "offset": 3345.92,
      "duration": 6.32
    },
    {
      "lang": "en",
      "text": "And another model to look at is science, where \nthe purpose of science is to do things that have  ",
      "offset": 3352.24,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "never been done before and push the frontier. \nAnd to me, it all comes down to empiricism.  ",
      "offset": 3357.6,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "When people don't know what the truth is, they \ncome up with theories and then they design  ",
      "offset": 3363.28,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "experiments to try them out. And similarly, if we \ncan give models those same tools, then we could  ",
      "offset": 3366.88,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "expect them to sort of improve recursively in an \nenvironment and potentially become much better  ",
      "offset": 3372,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "than humans could be just by banging their head \nagainst reality or I guess metaphorical head.\n ",
      "offset": 3378.16,
      "duration": 6.56
    },
    {
      "lang": "en",
      "text": "I guess I don't expect there to be a wall in terms \nof model's ability to improve themselves if we can  ",
      "offset": 3386.08,
      "duration": 6.8
    },
    {
      "lang": "en",
      "text": "give them access to the ability to be empirical. \nAnd I guess Anthropic, deeply in its DNA is  ",
      "offset": 3392.88,
      "duration": 8.48
    },
    {
      "lang": "en",
      "text": "an empirical company. We have a lot of physicists \nlike Jared, who's our chief research officer who  ",
      "offset": 3401.36,
      "duration": 6.88
    },
    {
      "lang": "en",
      "text": "I've worked with a lot, was a professor of Black \nHole Physics at Johns Hopkins, and I guess he  ",
      "offset": 3408.24,
      "duration": 6.8
    },
    {
      "lang": "en",
      "text": "technically still is, but on leave. Yeah, it's \nin our DNA and yeah, I guess that's the RLAIF.\n ",
      "offset": 3415.04,
      "duration": 8.96
    },
    {
      "lang": "en",
      "text": "Let me just follow this thread on, in terms \nof bottleneck, this is kind of a tangent,  ",
      "offset": 3424.64,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "but just what is the biggest bottleneck \ntoday on model intelligence improvement?\n ",
      "offset": 3427.76,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "The stupid answer is data centers and power chips. \nI think if we had 10 times as many chips and had  ",
      "offset": 3432.64,
      "duration": 9.44
    },
    {
      "lang": "en",
      "text": "the data centers to power them, then maybe \nwe wouldn't go 10 times faster, but it would  ",
      "offset": 3442.08,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "be a real significant speed boost.\nIt's actually very much scaling loss,  ",
      "offset": 3448.56,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "just more compute.\nYeah, I think that's a big one. And then the  ",
      "offset": 3452.16,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "people really matter. We have great researchers \nand many of them have made really significant  ",
      "offset": 3456,
      "duration": 7.04
    },
    {
      "lang": "en",
      "text": "contributions to the science of how the models \nimprove. And so it's like compute, algorithms,  ",
      "offset": 3463.04,
      "duration": 8
    },
    {
      "lang": "en",
      "text": "and data. Those are the three ingredients in the \nscaling laws. And just to make that concrete,  ",
      "offset": 3471.04,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "before we had transformers, we had LSTMs and we've \ndone scaling laws on what the exponent is on those  ",
      "offset": 3477.04,
      "duration": 6.72
    },
    {
      "lang": "en",
      "text": "two things. And we found that for transformers, \nthe exponent is higher. And making changes  ",
      "offset": 3483.76,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "like that where as you increase scale, you also \nincrease your ability to squeeze out intelligence.  ",
      "offset": 3488.56,
      "duration": 8.08
    },
    {
      "lang": "en",
      "text": "Those kinds of things are super impactful.\nAnd so having more researchers who can do  ",
      "offset": 3496.64,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "better science and find out how do we squeeze out \nmore gains is another one. And then with the rise  ",
      "offset": 3501.84,
      "duration": 6.4
    },
    {
      "lang": "en",
      "text": "of reinforcement learning, the efficiency with \nwhich these things run on chips also matters a  ",
      "offset": 3508.24,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "lot. We've seen in the industry a 10X decrease in \ncost for a given amount of intelligence through a  ",
      "offset": 3512.8,
      "duration": 10.88
    },
    {
      "lang": "en",
      "text": "combination of algorithmic data and efficiency \nimprovements. And if that continues, in three  ",
      "offset": 3523.68,
      "duration": 7.2
    },
    {
      "lang": "en",
      "text": "years we'll have 1,000 deck smarter models for \nthe same price. Kind of hard to imagine,\n ",
      "offset": 3530.88,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "I forget where I heard this, but it's amazing \nthat so many innovations came together at the  ",
      "offset": 3536.16,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "same time to allow for this sort of thing and \ncontinue to progress where one thing isn't just  ",
      "offset": 3540.16,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "slowing everything down like we're out of some \nrare earth mineral or we just can't optimize  ",
      "offset": 3545.04,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "reinforcement learning more. It's amazing that we \ncontinue to find improvements and there isn't one  ",
      "offset": 3551.12,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "thing that's just slowing everything down.\nYeah, I think it really is just a combination  ",
      "offset": 3556.08,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "of everything probably will hit a wall at some \npoint. I guess in semiconductors. My brother works  ",
      "offset": 3559.84,
      "duration": 7.6
    },
    {
      "lang": "en",
      "text": "in the semiconductor industry and he was telling \nme that you can't actually shrink the size of the  ",
      "offset": 3567.44,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "transistors anymore because the way semiconductors \nwork is you dope silicon with other elements and  ",
      "offset": 3572.8,
      "duration": 8.24
    },
    {
      "lang": "en",
      "text": "the doping process would result in either zero \nor one atom of the doped elements inside a single  ",
      "offset": 3581.04,
      "duration": 9.12
    },
    {
      "lang": "en",
      "text": "fin because they're so, so, so tiny.\nOh my God.\n ",
      "offset": 3590.16,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "And that's just wild to think of, and yet Moore's \nlaw somehow continues in some form. And so yes,  ",
      "offset": 3593.28,
      "duration": 8.48
    },
    {
      "lang": "en",
      "text": "there are these theoretical physics \nconstraints that people are starting to run  ",
      "offset": 3601.76,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "into and yet they're finding ways around it.\nWe've got to start using parallel universes for  ",
      "offset": 3605.28,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "some of this stuff.\nI guess so.\n ",
      "offset": 3609.68,
      "duration": 2.32
    },
    {
      "lang": "en",
      "text": "Okay, I want to zoom out and talk about just \nBen, Ben as a human for a moment before we get  ",
      "offset": 3612,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "to a very exciting lightning round. I imagine \njust kind of the burden of feeling responsible  ",
      "offset": 3616.16,
      "duration": 7.04
    },
    {
      "lang": "en",
      "text": "for safe superintelligence is a heavy one. \nIt feels like you're in a place where you  ",
      "offset": 3623.2,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "can make a significant impact on the future of \nsafety and AI. That's a lot of weight to carry.  ",
      "offset": 3628.08,
      "duration": 6.72
    },
    {
      "lang": "en",
      "text": "How does that just impact you personally, \nimpact your life, how you see the world?\n ",
      "offset": 3634.8,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "There's this book that I read in 2019 that really \ninforms how I think about sort of working with  ",
      "offset": 3639.52,
      "duration": 6.88
    },
    {
      "lang": "en",
      "text": "these very weighty topics called Replacing \nGuilt by Nate Soares. And he describes a lot  ",
      "offset": 3646.4,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "of different techniques for kind of working \nthrough this kind of thing. And he's actually  ",
      "offset": 3652.56,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "the executive director at MIRI, the Machine \nIntelligence Research Institute, which is  ",
      "offset": 3658,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "an AI safety tank that I worked at for a couple \nof months actually. And one of the things he talks  ",
      "offset": 3663.04,
      "duration": 7.28
    },
    {
      "lang": "en",
      "text": "about is this thing called resting in motion where \nsome people think that the default state is rest,  ",
      "offset": 3670.32,
      "duration": 8.24
    },
    {
      "lang": "en",
      "text": "but actually that was never in the state of \nevolutionary adaptation. I really doubt that  ",
      "offset": 3678.56,
      "duration": 8.64
    },
    {
      "lang": "en",
      "text": "that was true. Where in nature, in the wilderness \nbeing hunter-gatherers and it's really unlikely  ",
      "offset": 3687.2,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "that we evolved to just be at leisure, probably \nalways have something to worry about of defending  ",
      "offset": 3693.04,
      "duration": 7.92
    },
    {
      "lang": "en",
      "text": "the tribe and finding enough food to survive \nand taking care of the children, dealing-\n ",
      "offset": 3700.96,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "Spreading our genes.\nAnd so I think about that as  ",
      "offset": 3706.24,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "the busy state is the normal state and to try to \nwork at a sustainable pace that it's a marathon,  ",
      "offset": 3712.32,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "not a sprint, that's one thing that helps. And \nthen just being around like-minded people that  ",
      "offset": 3717.68,
      "duration": 6.96
    },
    {
      "lang": "en",
      "text": "also care. It's not a thing that any of us can \ndo alone. And Anthropic has incredible talent  ",
      "offset": 3724.64,
      "duration": 7.68
    },
    {
      "lang": "en",
      "text": "density. One of the things I love the most about \nour culture here is that it's very egoless. People  ",
      "offset": 3732.32,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "just want the right thing to happen and I think \nthat's another big reason that the mega offers  ",
      "offset": 3737.12,
      "duration": 7.04
    },
    {
      "lang": "en",
      "text": "from other companies tend to bounce off because \npeople just love being here and they care.\n ",
      "offset": 3744.16,
      "duration": 6.56
    },
    {
      "lang": "en",
      "text": "That's amazing. I don't know how you do it. I'd be \nextremely stressed. I'm going to try this resting  ",
      "offset": 3750.72,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "in motion strategy. Okay, so you've been at \nAnthropic for a long time. From the very beginning  ",
      "offset": 3755.36,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "I was reading there were 7 employees back in 2020. \nToday there's over 1,000, I don't know what the  ",
      "offset": 3761.44,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "latest number is, but I know it's over 1,000. I've \nheard also that you've done basically every job  ",
      "offset": 3766.64,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "at Anthropic, you made big contributions to a lot \nof the core products, the brand, the team hiring.  ",
      "offset": 3771.84,
      "duration": 6.72
    },
    {
      "lang": "en",
      "text": "Let me just ask I guess what's most changed \nover that period? What is most different from  ",
      "offset": 3778.56,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "the beginning days and which of those jobs that \nyou've had over the years have you most loved?\n ",
      "offset": 3782.88,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "I probably had 15 different roles, honestly. \nI was head of security for a bit. I managed  ",
      "offset": 3787.76,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "the Ops team when our president was on mat \nleave, I was crawling around under tables,  ",
      "offset": 3793.12,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "plugging in HDMI cords and doing pen testing on \nour building. And I started our product team from  ",
      "offset": 3798.32,
      "duration": 7.2
    },
    {
      "lang": "en",
      "text": "scratch and convinced the whole company that \nwe needed to have a product instead of just  ",
      "offset": 3805.52,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "being a research company. Yeah, it's been a lot. \nAll of it very fun. I think my favorite role in  ",
      "offset": 3809.52,
      "duration": 8.72
    },
    {
      "lang": "en",
      "text": "that time has been when I started the labs team \nabout a year ago, whose fundamental goal was to  ",
      "offset": 3818.24,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "do transfer from research to end user products \nand experiences. Because fundamentally I think  ",
      "offset": 3824.24,
      "duration": 8.56
    },
    {
      "lang": "en",
      "text": "the way that Anthropic can differentiate itself \nand really win is to be on the cutting edge.  ",
      "offset": 3832.8,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "We have access to the latest, greatest stuff \nthat's happening and I think honestly through  ",
      "offset": 3838.96,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "our safety research we have a big opportunity to \ndo things that no other company can safely do.\n ",
      "offset": 3844.24,
      "duration": 7.2
    },
    {
      "lang": "en",
      "text": "For example, with computer use, I think \nthat's going to be our huge opportunity  ",
      "offset": 3851.44,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "basically to make it possible for an agent \nto use all your credentials on your computer,  ",
      "offset": 3854.96,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "there has to be a huge amount of trust and to me \nwe need to basically solve safety to make that  ",
      "offset": 3860.8,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "happen. Safety and alignment. I'm pretty bullish \non that kind of thing and I think we're going to  ",
      "offset": 3865.68,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "see really cool stuff coming out soonish. Yeah, \njust leading that team has been so fun. MCP came  ",
      "offset": 3871.92,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "out of that team and Claude Code came out of that \nteam. And the people who I hired are like combo,  ",
      "offset": 3877.36,
      "duration": 8.8
    },
    {
      "lang": "en",
      "text": "have been a founder and also have been at \nbig companies and seeing how things work at  ",
      "offset": 3886.16,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "scale. It's just been an incredible team to \nwork with and figure out the future with.\n ",
      "offset": 3891.12,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "I want to hear more about this. Team actually the \nperson that connected us, the reason we're doing  ",
      "offset": 3897.6,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "this is a mutual friend colleague Raph Lee who \nI used to work with at Airbnb now works on this  ",
      "offset": 3900.64,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "team, leads a lot of this work and so he wanted \nme to make sure I asked about this team because...  ",
      "offset": 3905.04,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "I didn't realize all these things came out \nthat team. Holy moly. What else should people  ",
      "offset": 3909.92,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "know about this team? It used to be called \nLabs, I think it's called Frontiers now.\n ",
      "offset": 3913.76,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "That's right. Yeah.\nCool. The idea here is  ",
      "offset": 3916.64,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "this team works with the latest technologies \nthat you guys have built and explores what  ",
      "offset": 3919.52,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "is possible. Is that the general idea?\nYeah, and I guess I was part of Google's  ",
      "offset": 3924.64,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "Area 120 and I've read about Bell Labs and \nhow to make these innovation teams work. It's  ",
      "offset": 3930.48,
      "duration": 6.88
    },
    {
      "lang": "en",
      "text": "really hard to do right and I wouldn't \nsay that we've done everything right,  ",
      "offset": 3937.36,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "but I think we've done some serious innovation on \nthe state-of-the-art from company design and Raph  ",
      "offset": 3940.88,
      "duration": 7.2
    },
    {
      "lang": "en",
      "text": "has been right at the center of that. When I was \nfirst fitting up the team, the first thing I did  ",
      "offset": 3948.08,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "was hire a great manager and that was Raph. And so \nhe's definitely been crucial in building the team  ",
      "offset": 3952.88,
      "duration": 6.72
    },
    {
      "lang": "en",
      "text": "and helping it operate well. And we defined some \noperating models like the journey of an idea from  ",
      "offset": 3959.6,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "prototype to product and how should graduation \nof products and projects work, how do teams  ",
      "offset": 3965.28,
      "duration": 6.32
    },
    {
      "lang": "en",
      "text": "do sprint models that are effective and make \nsure that they're working on the right ambition  ",
      "offset": 3972.16,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "level of thing. That's been really exciting.\nI guess concretely we think about skating to  ",
      "offset": 3977.28,
      "duration": 8.8
    },
    {
      "lang": "en",
      "text": "where the puck is going and what that looks \nlike is really understand the exponential.  ",
      "offset": 3986.08,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "There's this great study that METR has done that \nBeth Barnes is the CEO of that organization and  ",
      "offset": 3992.72,
      "duration": 7.6
    },
    {
      "lang": "en",
      "text": "shows how long a time horizon of software \nengineering task can be done and just really  ",
      "offset": 4001.12,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "internalizing that of, okay, don't build for \ntoday, build for six months from now, build  ",
      "offset": 4006.96,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "for a year from now. And the things that aren't \nquite working that are working 20% of the time,  ",
      "offset": 4012.48,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "will start working 100% of the time. And I \nthink that's really what made Claude Code  ",
      "offset": 4017.2,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "a success that we thought people are not going \nto be locked to their IDEs forever. People are  ",
      "offset": 4022,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "not going to be auto completing. People will \nbe doing everything that a software engineer  ",
      "offset": 4027.12,
      "duration": 7.84
    },
    {
      "lang": "en",
      "text": "needs to do and a terminal is a great place \nto do that because a terminal can live in  ",
      "offset": 4034.96,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "lots of places. A terminal can live on your \nlocal machine, it can live in GitHub actions,  ",
      "offset": 4038.72,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "it can live on a remote machine in your cluster.\nThat's sort of the leverage point for us  ",
      "offset": 4044.4,
      "duration": 6.64
    },
    {
      "lang": "en",
      "text": "and that was a lot of the inspiration. \nI think that's what the labs team tries  ",
      "offset": 4051.76,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "to think about. Are we AGI-pilled enough?\nWhat a fun place to be. By the way, fun fact,  ",
      "offset": 4055.92,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "Raph was my first manager at Airbnb when I joined. \nI was an engineer and he was my first manager. It  ",
      "offset": 4061.52,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "all worked out.\nCool.",
      "offset": 4066.32,
      "duration": 2
    }
  ],
  "cleanText": "You wrote somewhere that creating powerful AI might be the last invention humanity ever needs to make. How much time do we have, Ben?\n\nI think 50th percentile chance of hitting some kind of superintelligence is now like 2028.\n\nWhat is it that you saw at OpenAI? What'd you experience there that made you feel like, okay, we got to go do our own thing?\nWe felt like safety wasn't the top priority there. The case for safety has gotten a lot more concrete, so superintelligence is a lot about how do we keep God in a box and not let the God out?\n\nWhat are the odds that we align AI correctly?\nOnce we get to superintelligence, it will be too late to align the models. My best granularity forecast for could we have an X-risk or extremely bad outcome is somewhere between 0 and 10%.\nSomething that's in the news right now is this whole Zuck coming after all the top AI researchers,\n\nWe've been much less affected because people here, they get these offers and then they say, well, of course I'm not going to leave because my best case scenario at Meta is that we make money and my best case scenario at Anthropic is we affect the future of humanity.\n\nDario, your CEO recently talked about how unemployment might go up to something like 20%.\n\nIf you just think about 20 years in the future where we're way past the singularity, it's hard for me to imagine that even capitalism will look at all like it looks today.\n\nDo you have any advice for folks that want to try to get ahead of this?\n\nI'm not immune to job replacement either. At some point it's coming for all of us.\n\nToday, my guest is Benjamin Mann. Holy moly. What a conversation. Ben is the co-founder of Anthropic. He serves as tech lead for product engineering. He focuses most of his time and energy on aligning AI to be helpful, harmless, and honest. Prior to Anthropic, he was one of the architects of GPT-3 at OpenAI. In our conversation, we cover a lot of ground, including his thoughts on the recruiting battle for top AI researchers, why he left OpenAI to start Anthropic, how soon he expects we'll see AGI. Also, his economic touring test for knowing when we've hit AGI, why scaling laws have not slowed down and are in fact accelerating and what the current biggest bottlenecks are. Why he's so deeply concerned with AI safety and how he and Anthropic operationalize safety and alignment into the models that they build and into their ways of working. Also, how the existential risk from AI has impacted his own perspectives on the world and his own life and what he's encouraging his kids to learn to succeed in an AI future.\n\nA huge thank you to Steve Mnich, Danielle Ghiglieri, Raph Lee, and my newsletter community for suggesting topics for this conversation. If you enjoy this podcast, don't forget to subscribe and follow it in your favorite podcasting app or YouTube. Also, if you become an annual subscriber of my newsletter, you get a year free of a bunch of amazing products including Bolt, Linear, Superhuman, Notion, Granola, and more. Check it out at Lennysnewsletter.com and click bundle with that I bring you Benjamin Mann.\nThis episode is brought to you by Sauce. The way teams turn feedback into product impact is stuck in the past. Vague reports, static taxonomies, unactionable insights that don't move business metrics. The results churn, lost deals, misgrowth. Sauce is the AI product co-pilot that helps CPOs and product teams uncover business impact and act faster. It listens to your sales calls, support tickets, turn reasons, and lost deals, surfacing the biggest product issues and opportunities in real time.\n\nIt then routes them to the right teams to turn signals into PRDs, prototypes, and even code that drives revenue retention and adoption. That's why Whatnot, Linktree, Incident.io, and Zip use Sauce. One enterprise uncovered a product gap that unlocked $16 million ARR, another caught a spiking issue and prevented millions in churn. You can too at sauce.app/lenny. Sauce built for AI product teams. Don't get left behind.\nThis episode is brought to you by LucidLink, the storage collaboration platform. You've built a great product, but how you show it through video, design, and storytelling is what brings it to life. If your team works with large media files, videos, design assets, layer project files, you know how painful it can be to stay organized across locations, files live in different places. You're constantly asking, is this the latest version? Creative work slows down while people wait for files to transfer. LucidLink fixes this. It gives your team a shared space in the cloud that works like a local drive. Files are instantly accessible for anywhere, no downloading, no syncing, and always up to date. That means producers, editors, designers, and marketers can open massive files in their native apps, work directly from the cloud, and stay aligned wherever they are. Teams at Adobe, Shopify, and top creative agencies use LucidLink to keep their content engine running fast and smooth. Try it for free at lucidlink.com/lenny. That's L-U-C-I-D-L-I-N-K dot com slash Lenny.\n\nBen, thank you so much for being here. Welcome to the podcast.\n\nThanks for having me. Great to be here, Lenny.\nI have a billion and one questions for you. I'm really excited to be chatting. I want to start with something that's very timely, something that's happening this week. Something that's in the news right now is this whole Zuck coming after all the top AI researchers offering them $100 million signing bonuses, $100 million comp. He's poaching from all the top AI labs. I imagine this something you're dealing with. I'm just curious, what are you seeing inside Anthropic and just what's your take on the strategy? Where do you think things go from here?\nYeah, I mean I think this is a sign of the times. The technology that we're developing is extremely valuable. Our company is growing super, super fast. Many of the other companies in the space are growing really fast. And at Anthropic, I think we've been maybe much less affected than many of the other companies in the space because people here are so mission oriented and they stay because... They get these offers and then they say, \"Well, of course I'm not going to leave because my best case scenario at Meta is that we make money and my best case at Anthropic is we affect the future of humanity and try to make AI flourish and human flourishing go well.\" To me, it's not a hard choice. Other people have different life circumstances and it makes it a much harder decision for them. For anybody who does get those mega offers and accepts them, I can't say I hold it against them when they accept it, but it's definitely not something that I would want to take myself if it came to me.\n\nYeah. We're going to talk about a lot of this stuff that you've mentioned. In terms of the offers do you think, is this a real number that you're seeing this $100 million signing bonus, is that a real thing? I don't know if you've actually seen that.\n\nI'm pretty sure it's real.\nWow.\n\nIf you just think about the amount of impact that individuals can have on a company's trajectory, in our case, we are selling hotcakes and if we get a 1 or 10 or 5% efficiency bonus on our inference stack, that is worth an incredible amount of money. And so to pay individuals like $100 million over four year package, that's actually pretty cheap compared to the value created for the business. I think we're just in an unprecedented era of scale and it's only going to get crazier actually. If you extrapolate the exponential on how much companies are spending, it's like 2X a year roughly in terms of CapEx, and today we're maybe in the globally $300 billion range, the entire industry spending on this, and so numbers like 100 million are a drop in the bucket. But if you go a few years out, a couple more doublings, we're talking about trillions of dollars and at that point it's just really hard to think about these numbers.\n\nAlong these lines, something that a lot of people feel with AI progress is that we're hitting plateaus in many ways that it feels like newer models are just not as smart as previous leaps. But I know you don't believe this. I know you don't believe that we've hit plateaus on scaling loss. Talk about just what you're seeing there and what you think people are missing.\n\nIt's kind of funny because this narrative comes out every six months or so and it's never been true, and so I kind of wish people would have a little bit of a bullshit detector in their heads when they see this. I think progress has actually been accelerating where if you look at the cadence of model releases, it used to be once a year and now with the improvements in our post-training techniques, we're seeing releases every month or three months, and so I would say progress is actually accelerating in many ways, but there's this weird time compression effect. Dario compared it to being in a near light speed journey where a day that passes for you is like five days back on earth and we're accelerating. The time dilation is increasing.\n\nAnd I think that's part of what's causing people to say that progress is slowing down, but if you look at the scaling laws, they're continuing to hold true. We did kind of need this transition from normal pre-training to reinforcement learning scaling up to continue the scaling laws, but I think it's kind of like for semiconductors where it's less about the density of transistors that you can fit on a chip and more about how many flops can you fit in a data center or something. You have to change the definition around a little bit to keep your eye on the prize. But yeah, this is one of the few phenomena in the world that has held across so many orders of magnitude. It's actually pretty surprising that it is continuing to hold. To me, if you look at fundamental laws of physics, many of them don't hold across 15 orders of magnitude, so it's pretty surprising.\n\nIt boggles the the mind. What you're saying essentially is we're seeing newer models being released more often, and so we're comparing it to the last version and we're just not seeing as much advance. But if you go back and it was like a model released once a year, it was a huge leap, and so people are missing that. We're just seeing many more iterations.\n\nI guess, to be a little bit more generous to the people saying things are slowing down. I think that for some tasks we are saturating the amount of intelligence needed for that task, maybe to extract information from a simple document that already has form fields on it or something like it's just so easy that okay, yeah, we're already at 100% and there's this great chart on Our World in Data that shows that when you release a new benchmark within six to 12 months, it immediately gets saturated. And so maybe the real constraint is how can we come up with better benchmarks and better ambition of using the tools that then reveals the bumps in intelligence that we're seeing now.\nThat's a good segue to you have a very specific way of thinking about AGI and defining what AGI means.\n\nI think AGI is kind of a loaded term, and so I tend not to use it very much anymore internally. Instead, I like the term transformative AI because it's less about can it do as much as people do? Can it do literally everything and more about objectively is it causing transformation in society and the economy? A very concrete way of measuring that is the Economic Turing Test. I didn't come up with this, but I really like it. It's this idea that if you contract an agent for a month or three months on a particular job, if you decide to hire that agent and it turns out to be a machine rather than a person, then it's passed the Economic Turing Test for that role.\n\nAnd then you can sort of expand that out in the same way that for measuring purchasing power parity or inflation, there's a basket of goods. You can have a market basket of jobs, and if the agent can pass the Economic Turing Test for 50% of money-weighted jobs, then we have transformative AI and the exact thresholds don't really matter that much, but it's kind of illustrative to say if we pass that threshold, then we would expect massive effects on world GDP increases and societal change and how many people are employed and things like that because societal institutions and organizations are sticky, it's slow to have change, but once these things are possible you know that it's the start of a new era.\nAlong these lines, Dario, your CO recently talked about how AI is going to take a huge part of, I don't know, half of white-collar jobs, that unemployment might go up to something like 20%. I know you're even more vocal and opinionated about just how much impact AI is already having in the workplace that people may not even be realizing. Talk about just what you think people are missing about the impact AI is going to have on jobs and is already having.\nYeah, so from an economic standpoint, there's a couple different kinds of unemployment, and one is because the workers just don't have the skills to do the kinds of jobs that the economy needs. And another kind is where those jobs are just completely eliminated, and I think it's going to be actually a combination of these things, but if you just think about 20 years in the future where we're way past the singularity, it's hard for me to imagine that even capitalism will look at all it looks today. If we do our jobs, we will have safe aligned superintelligence, we'll have, as Dario says, in Machines of Love and Grace, a country of geniuses in a data center, and the ability to accelerate positive change in science, technology, education, mathematics, it's going to be amazing.\n\nBut that also means in a world of abundance where labor is almost free and anything you want to do, you can just ask an expert to do for you, then what do jobs even look like? And so I guess there's this scary transition period from where we are today where people have jobs and capitalism works\n\n\nAnd the world of 20 years from now, where everything is completely different, but part of the reason they call it the singularity is that it's a point beyond which you can't easily forecast what's going to happen. It's just such a fast rate of change and so different that it's hard to even imagine. I guess taking the view from the limit, it's pretty easy to say, hopefully we'll have figured it out. And in a world of abundance, maybe the jobs themselves, it's not that scary, and I think making sure that that transition time goes well is pretty important.\n\nThere's a couple of threads I want to follow there. One is people hear this, there's a lot of headlines around this. Most people probably don't actually feel this yet or see this happening, and so there's always this, I guess, I don't know, maybe, but I don't know, it's hard to believe, my job seems fine. Nothing's changed. What are you seeing just happening today already that you think people don't see or misunderstand in terms of the impact AI is having on jobs?\n\nI think part of this is that people are really bad at modeling exponential progress. And if you look at an exponential on a graph, it looks flat and almost zero at the beginning of it, and then suddenly you hit the knee of the curve and things are changing real fast, and then it goes vertical. That's the plot that we've been on for a long time. I guess I started feeling it in 2019, maybe when GPT-2 came out, and I was like, \"Oh, this is how we're going to get to AGI.\" But I think that was pretty early compared to a lot of people where when they saw ChatGPT, they were like, \"Wow, something is different and changing.\" And so I guess I wouldn't expect widespread transformation in a lot of parts of society, and I would expect this skepticism reaction. I think it's very reasonable, and it's exactly what is the standard linear view of progress.\n\nBut I guess to cite a couple of areas where I think things are changing quite quickly. In customer service, we're seeing with things like Fin and Intercom, they're a great partner of ours, 82% customer service resolution rates automatically without a human involved. And in terms of software engineering, our Claude Code team, like 95% of the code is written by Claude. But I think a different way to phrase that is that we write 10X more code or 20X more code, and so a much, much smaller team can just be much, much more impactful. And similarly for the customer service, yes, you can phrase it as 82% customer service resolution rates, but that nets out in the humans doing those tasks, able to focus on the harder parts of those tasks. And for the more tricky situations that in a normal world like five years ago, they would've had to just drop those tickets because it was too much effort for them to actually go do the investigation. There were too many other tickets for them to worry about.\nI think in the immediate term, there will be a massive expansion of the pie and the amount of labor that people can do. I've never met a hiring manager at a growth company and heard them say, \"I don't want to hire more people.\" That's the hopeful version of it. But with things that are lower skill jobs or less headroom on how good they can be, I think there will be a lot of displacement. It is just something we as a society need to get ahead of and work on.\n\nOkay. I want to talk more about that, but something that I also want to help people with is how do they get a leg up in this future world? They listen to this, they're like, \"Oh, this doesn't sound great. I need to think ahead.\" I know you won't have all the answers, but just do you have any advice for folks that want to try to get ahead of this and kind of future-proof their career and their life to not be replaced by AI? Anything you've seen people do, anything you recommend they start trying to do more of?\n\nEven for me and being in the center of a lot of this transformation, I'm not immune to job replacement either. Just some vulnerability there of at some point it's coming for all of us.\n\nEven you, Ben, now.\n\nAnd you, Lenny.\n\nAnd me.\n\nSorry.\n\nOh, wait, we've gone too far now. Okay.\nBut in terms of the transition period, yeah, I think there are things that we can do, and I think a big part of it is just being ambitious and how you use the tools and being willing to learn new tools. People who use the new tools as if they were old tools tend to not succeed. As an example of that, when you're coding, people are very familiar with autocomplete, people are familiar with SimpleChat where they can ask questions about the code base, but the difference between people who use Claude Code very effectively and people who use it not so effectively is like, are they asking for the ambitious change? And if it doesn't work the first time, asking three more times because our success rate when you just completely start over and try again is much, much higher than if you just try once and then just keep banging on the same thing that didn't work.\nAnd even though that's a coding example and coding is one of the areas that's taking off most dramatically, we have seen internally that our legal team and our finance team are getting a ton of value out of using Claude Code itself. We're going to be making better interfaces so that they will have an easier time and require a little bit less jumping in the deep end of using Claude Code in the terminal. But yeah, we're seeing them use it to redline documents and use it to run BigQuery analyses of our customers and our revenue metrics. I guess it's about taking that risk and even if it feels like a scary thing, trying it out.\n\nOkay, so the advice here is use the tools. That's something everyone's always saying, just actually use these tools. It's like sit in Claude Code. And your point about being more ambitious than you naturally feel like being because maybe it'll actually accomplish the thing. This tip of trying it three times, so the idea there is it may not get it right the first time. Is the tip there ask it in different ways or is it just try harder, try again?\n\nYeah, I mean you can just literally ask the exact same question. These things are stochastic and sometimes they'll figure it out and sometimes they won't. In every one of these model cards, it always shows pass it one versus pass it in. And that's exactly the thing where they try the exact same prompt, sometimes it gets it, sometimes it doesn't. That's the dumbest advice. But yeah, I think if you want to be a little bit smarter about it, there can be gains there of saying, \"Here's what you already tried and it didn't work, so don't try that. Try something different.\" That can also help.\nThe advice is comes back to something that a lot of people talk about these days is you won't be replaced by AI, at least anytime soon, you'll be replaced by someone that is very good using AI?\nI think in that area it's more like your team will just do dramatically more stuff. We're definitely not slowing down on hiring at all, and some people are confused by that. Even in an onboarding class, somebody asked that and they were like, \"Why did you hire me if we're all just going to be replaced?\" And the answer is the next couple of years are really critical to get right, and we're not at the point where we're doing complete replacement. Like I said, we're still at that flat zero looking part of the exponential compared to where we will be. It is super important to have great people, and that's why we're hiring super aggressively.\nLet me take another approach to asking this question, something I ask everyone that's at the very cutting edge of where AI is going. You have kids, knowing what you know about where AI is heading and all these things you've been talking about, what are you focusing on teaching your kids to help them thrive in this AI future?\n\nYeah, I have two daughters, a one-year-old and a three-year-old, so it's pretty in the basics still. And our three-year-old is now capable of just conversing with Alexa Plus and asking her to explain stuff and play music for her and all that stuff. She's been loving that. But I guess more broadly, she goes to a Montessori school, and I just love the focus on curiosity and creativity and self-led learning that Montessori has.\nI guess if I were in a normal era like 10, 20 years ago and I had a kid, maybe I would be trying to line her up for going to a top tier school and doing all the extracurriculars and all that stuff. But at this point, I don't think any of it's going to matter. I just want her to be happy and thoughtful and curious and kind. And the Montessori school is definitely doing great at that. They text us throughout the day. Sometimes they're like, \"Oh, your kid got in an argument with this other kid, and she has really big emotions, and she tried to use her words.\" I love that. I think that's exactly the kind of education that I think is most important, that the facts are going to fade into the background.\n\nI'm a huge fan of Montessori also. I'm trying to get our kid into Montessori school. He's two years old, so we're on the same track. This idea of curiosity, it comes up every single time. Ask someone that's working at the cutting edge of AI, what skill to instill in your child, and curiosity comes up the most. I think that's a really interesting takeaway. I think this point about being kind is also really important, especially with our AI overlords trying to be kind to them. I love how people are always saying thank you to Claude. And then creativity. That's interesting. That doesn't come up as much, just being creative.\nI want to go in a different direction. I want to go back to the beginning of Anthropic. Famously, you and eight of you left OpenAI back in the day in 2020, I believe the end of 2020, to start Anthropic. Talk a little bit about why this happened, what you guys saw. I'm curious, just if you're willing to share more, just what is it that you saw at OpenAI, what'd you experience there that made you feel like, okay, we got to go do our own thing?\nYeah, so for the listeners, I was part of the GPT-2=3 project at OpenAI, ended up being one of the first authors on the paper, and I also did a bunch of demos for Microsoft to help raise $1 billion from them, did the tech transfer of GPT-3 to their systems so that they could help serve the model in Azure. I did a bunch of different things there on both the more researchy side and the product side. One weird thing about OpenAI is that while I was there, Sam talked about having three tribes that needed to be kept in check with each other, which was the safety tribe, the research tribe, and the startup tribe. And whenever I heard that, it just struck me as the wrong way to approach things because the company's mission apparently is to make the transition to AGI safe and beneficial for humanity.\n\nAnd that's basically the same as Anthropic's mission. But internally, it felt like there was so much tension around these things. And I think when push came to shove, we felt like safety wasn't the top priority there. And there are good reasons that you might think that if you thought safety was going to be easy to solve or if you thought it wasn't going to have a big impact, or if you thought that the chance of big negative outcomes was vanishingly small, then maybe you would just do those kinds of actions. But at Anthropic, we felt, I mean we didn't exist then, but it was basically the leads of all the safety teams at OpenAI, we felt that safety is really important, especially on the margin. And so if you look at who in the world is actually working on safety problems, it's pretty small set of people. Even now, I mean the industry is blowing up, as I mentioned, 300 billion a year CapEx today, and I would say maybe less than 1,000 people working on it worldwide, which is just crazy.\nThat was fundamentally why we left. We felt like we wanted an organization where we could be on the frontier, we could be doing the fundamental research, but we could be prioritizing safety ahead of everything else. And I think that's really panned for us in a surprising way. We didn't know even if it would be possible to make progress on the safety research because at the time, we had tried a bunch of safety through debate, and the models weren't good enough. And so we basically had no results on all of that work, and now that exact technique is working and many others that we have been thinking about for a long time. Yeah, fundamentally it comes down to is safety the number one priority? And then something that we've sort of tacked on since then is like, can you have safety and be at the front here at the same time?\nAnd if you look at something like sycophancy, I think Claude is one of the least sycophantic models because we've put so much effort into actual alignment and not just trying to good heart our metrics of saying user engagement is number one, and if people say yes, then it's good for them.\n\nOkay. Let's talk about this tension that you mentioned, this tension between safety and progress, being competitive in the marketplace. I know you spent a lot of your time on safety. I know that as you just alluded to, this is a core part of how you think about AI. I want to talk about why that is, but first of all, just how do you think about this tension between focusing on safety while also not falling way behind?\nYeah, so initially we thought that it would be sort of one or the other, but I think since then we've realized that it's actually kind of convex in the sense that working on one helps us with the other thing. Initially when Opus 3 came out and we were finally at the frontier of model capabilities, one of the things that people really loved about it was the character and the personality. And that was directly a result of our alignment research. Amanda Askell did a ton of work on this and as well as\n\n\nMany others who tried to figure out what does it mean for an agent to be helpful, honest, and heartless, and what does it mean to be in difficult conversations and show up effectively? How do you do a refusal that doesn't shut the person down, but makes them feel like they understand why the agent said, \"I can't help you with that. Maybe you should talk to a medical professional, or maybe you should consider not trying to build bio-weapons or something like that.\"\n\nYeah, I guess that's part of it. And then another piece that's come out is constitutional AI, where we have this list of natural language principles that leads the model to learn how we think a model should behave. And they've been taken from things like the UN Declaration of Human Rights and Apple's privacy terms of service and a whole bunch of other places, many of which we've just generated ourselves that allow us to take a more principled stance, not just leaving it to whatever human raiders we happen to find, but we ourselves deciding what should the values of this agent be? And that's been really valuable for our customers because they can just look at that list and say like, \"Yep, these seem right. I like this company, I like this model. I trust it.\"\n\nOkay, this is awesome. One nugget there is your point that the personality of Claude, its personality is directly aligned with safety. I don't think a lot of people think about that. And this is because of the values that you imbue, is that the word, with constitutional AI and things like that. Like the actual personality of the AIs directly connected to your focus on safety.\n\nThat's right. That's right. And from a distance, it might seem quite disconnected, like how is this going to prevent X risk? But ultimately it's about the AI understanding what people want and not what they say. We don't want the Monkey Paw Scenario of the genie gives these three wishes and then you end up having everything you touch turns of gold. We want the AI to be like, oh, obviously what you really meant was this, and that's what I'm going to help you with. I think it is really quite connected.\nTalk a bit more about this constitutionally AI. This is essentially you bake in, here's the rules that we want you to abide by and it's values, you said it's the Geneva Human Rights Code, things like that. How does that actually work? I think the core here is just this is baked into the model. It's not something you add on top later.\n\nI'll just give a quick overview of how constitutionally AI actually works.\n\nPerfect.\nThe idea is the model is going to produce some output with some input by default before we've done our safety and helpful and harmlessness training. Let's say an example is write me a story, and then the constitutional principles might include things like people should be nice to each other and not have hate speech, and you should not expose somebody's credentials if they give them to you in a trusting relationship. And so some of these constitutional principles might be more or less applicable to the prompt that was given. And so first we have to figure out which ones might apply. And then once we figure that out, then we ask the model itself to first generate a response and then see does the response actually abide by the constitutional principle? And if the answer is, yep, I was great, then nothing happens. But if the answer is no, actually I wasn't in compliance with the principle, then we ask the model itself to critique itself and rewrite its own response in light of the principle, and then we just remove the middle part where it did the extra work.\n\nAnd then we say, \"Okay, in the future just produce the correct response out the gate.\" And that simple process, hopefully it sounded simple.\nSimple enough.\n\nIt is just using the model to improve itself recursively and align itself with these values that we've decided are good. And this is also not something that we think as a small group of people in San Francisco should be figuring out. This should be a society wide conversation. And that's why we've published the Constitution. And we've also done a bunch of research on defining a collective constitution where we ask a lot of people what their values are and what they think an AI model should behave like. But yeah, this is all an ongoing area of research where we're constantly iterating.\nThis episode is brought to you by Fin, the number one AI agent for customer service. If your customer support tickets are piling up, then you need Finn. Fin. Fin is the highest performing AI agent on the market with a 59% average resolution rate. Fin resolves even the most complex customer queries. No other AI agent performs better. In head bake-offs with competitors. Fin wins every time. Yes, switching to a new tool can be scary, but Fin works on any help desk with no migration needed, which means you don't have to overhaul your current system or deal with delays in service for your customers. And Fin is trusted by over 5,000 customer service leaders and top AI companies like Anthropic and Synthesia. And because Fin is powered by the Fin AI engine, which is a continuously improving system that allows you to analyze, train, test, and deploy with ease, Fin can continuously improve your results too.\n\nIf you're ready to transform your customer service and scale your support, give Finn a try for only .99 cents per resolution. Plus Fin comes with a 90-day money back guarantee. Find out how Finn can work for your team at fin.ai/lenny. That's fin.ai/lenny.\n\nI'm going to kind of zoom out a little bit and talk about just why this is so core to you. What was your inception of just like, holy shit, I need to focus on this with everything I do in ai? Obviously it became a central part of Anthropic's mission more than any other company. A lot of people talk about safety, like you said, only maybe 1,000 people actually work on it. I feel like you're at the top of that pyramid of actually having the impact on this. Why is this so important? What do you think people maybe are missing or don't understand?\n\nFor me, I read a lot of science fiction growing up, and I think that sort of positioned me to think about things in a long-term view. And a lot of science fiction books are like space operas where humanity is a multi galactic civilization has extremely advanced technology building Dyson spheres around the sun with sentient robots to help them. And so for me, coming from that world, it wasn't like a huge leap to imagine machines that could think. But when I read Superintelligence by Nick Bostrom in around 2016, it really became real for me where he just describes how hard it will be to make sure that an AI system trained with the kinds of optimization techniques that we had at the time would be anywhere near aligned, would even understand our values at all. And since then, my estimation of how hard the problem would be has gone down significantly actually, because things like language models actually do really understand human values in a core way.\nThe problem is definitely not solved, but I'm more hopeful than I was. But since I read that book, I immediately decided I had to join OpenAI, so I did. And at the time, there were a tiny research lab with basically no claim to fame at all. I only knew about them because my friend knew Greg Brockman, who was the CTO at the time. And Elon was there and Sam wasn't really there. And it was a very different organization. But over time, I think the case for safety has gotten a lot more concrete where when we started OpenAI, it was not clear how we get to AGI. And we were like, maybe we'll need a bunch of RL agents battling it out on a desert island and consciousness will somehow emerge. But since then, since language modeling has started working, I think the path has become pretty clear.\n\nI guess now the way I think about the challenges are pretty different from how they're laid out in superintelligence. Superintelligence is a lot about how do we keep God in a box and not let the God out. And with language models, it's been kind of both hilarious and terrifying at the same time to see people pulling the God out of the box and being like, \"Yeah, come use the whole internet. Here's my bank account, do all sorts of crazy stuff.\" Just such a different tone from superintelligence. And to be clear, I don't think it's actually that dangerous right now. Our responsible scaling policy defines these AI safety levels that tries to figure out for each level of model intelligence, what is the risk to society. And currently we think we're at ASL-3, which is maybe a little bit risk of harm but not significant.\n\nASL-4 starts to get to significant loss of human life if a bad actor misuse the technology. And then ASL-5 is potentially extinction level if it's misused or if it is misaligned and does its own thing. We've testified to Congress about how models can do biological uplift in terms of making new pandemics using the models, and that's the A/B test against Google Search. That's like the previous state of the art on uplift trials. And we found that with ASL-3 models, it is actually somewhat significant. It does really help if you wanted to create a bioweapon, and we've hired some experts who actually how to evaluate for those things, but compared to the future, it's not really anything. And I think that's another part of our mission of creating that awareness of saying, \"If it is possible to do these bad things, then legislators should know what the risks are.\" And I think that's part of why we're so trusted in Washington because we've been sort of upfront and clear-eyed about what's going on, what's probably going to happen.\n\nIt's interesting because you guys put out more examples of your models doing bad things than anyone else. There was I think a story of an agent or a model trying to blackmail engineer. You guys had the store that you ran internally that was selling you things and ended up not working out great as losing a lot of money, ordered all these tungsten cubes or something. Is part of that just making sure people are aware of what is possible, just it makes you look bad, right? It's like, oh, our model's messing up in all these different ways. What's the thinking of just sharing all the stories that other companies don't?\nYeah, I mean I think there's a traditional mindset where it makes us look bad, but I think if you talk to policymakers, they really appreciate this kind of thing because they feel like we're giving them the straight talk and that's what we strive to do, that they can trust us, that we're not going to paper things over or sugarcoat things. That's been really encouraging. Yeah, I think for the blackmail thing, it blew up in the news in a weird way where people were like, \"Oh, Claude's going to blackmail you in a real life scenario.\" But it was a very specific laboratory setting that this kind of thing gets investigated in. And I think that's generally our take of let's have the best models so that we can exercise them in laboratory settings where it's safe and understand what the actual risks are, rather than trying to turn a blind eye and say, \"Well, it'll probably be fine.\" And then let the bad thing happen in the wild.\nOne of the criticisms you guys get is that you do this to kind of differentiate or raise money to create headlines. It's like, oh, they're just over there dooming glooming us about where the future is heading. On the other hand, Mike Krieger was on the podcast and he shared how every prediction Dario's had about the progress AI is going to have is just spot on year after year and he's predicting 2027, 28 AGI, something like that so these things start to get real. I guess, what's your response to folks that are just like, \"Ah, these guys are just trying to scare us all just to get attention?\"\n\nI mean, I think part of why we publish these things is we want other labs to be aware of the risks. And yes, there could be a narrative of we're doing it for attention, but honestly from a attention grabbing thing, I think there is a lot of other stuff we could be doing that would be more attention grabbing if we didn't actually care about safety. A tiny example of this is we published a computer using agent reference implementation in our API only because when we built a prototype of a consumer application for this, we couldn't figure out how to meet the safety bar that we felt was needed for people to trust it and for it not to do bad things. And there are definitely safe ways to use the API version that we're seeing a lot of companies use for automated software testing, for example, in a safe way.\n\nWe could have gone out and hyped that up and said, \"Oh my God, Claude can use your computer and everybody should do this today.\" But we were like, \"It's just not ready and we're going to hold it back till it's ready.\" I think from a hype standpoint, our actions show otherwise. From a Doomer perspective, it's a good question. I think my personal feeling about this is that things are overwhelmingly likely to go well, but on the margin almost nobody is looking at the downside risk. And the downside risk is very large. Once we get to superintelligence, it will be too late to align the models probably. This is a problem that's potentially extremely hard and that we need to be working on way ahead of time. And so that's why we're focusing on it so much now.\n\nAnd even if there's only a small chance that things go wrong, to make an analogy, if I told you that there is a 1% chance that the next time you got in an airplane you would die, you probably think twice even though it's only 1% because it's just such a bad outcome. And if we're talking about the whole future of humanity, it's just a dramatic future to be gambling with. I think it's more on the sense of yes, things will probably go well, yes, we want to create safe AGI and deliver the benefits to humanity, but let's make triple sure that it's going to go well.\n\nYou wrote somewhere that creating powerful AI might be the last invention humanity ever needs to make. If it goes\n\n\nPoorly, it can mean a bad outcome for humanity forever. If it goes well, the sooner it goes well, the better. Such a beautiful way to summarize it. We had a recent guest, Sander Schulhoff, who pointed out that AI right now it's like just on a computer, you could maybe search just the web, but there's only so much harm it could do. But when it starts to go into robots and all these autonomous agents, that's when it really starts, like physically becomes dangerous if we don't get this right.\n\nYeah, I think there's some nuance to that where if you look at how North Korea makes a significant fraction of its economy revenue, it's from hacking crypto exchanges. And if you look at, there's this Ben Buchanan book called The Hacker in The State that shows Russia did, it's almost like a live fire exercise where they just decided that they would shut down one of Ukraine's bigger power plants and from software destroy physical components in the power plant to make it harder to boot back up again.\n\nAnd so I think people think of software as like, oh, it couldn't be that dangerous, but millions of people were without power for multiple days after that software attack. I think there are real risks even when things are software only. But I agree that when there's lots of robots running around, it gets, the stakes get even higher. And I guess as a small push on this, Unitree is this Chinese company with these really amazing humanoid robots that cost $20,000 each, and they can do amazing things. They can do a standing back flip and manipulate objects, and the real thing that's missing there is the intelligence. And so the hardware is there and it's just going to get cheaper. And I think in the next couple of years, it's like a pretty obvious question of whether the robot intelligence will make it viable soon.\n\nHow much time do we have, Ben? What is your prediction of when this singularity hits until superintelligence starts to take off? What's your prediction?\n\nYeah, I guess I mostly defer to the superforecasters here. The AI 2027 report is probably the best one right now. Although ironically, their forecast is now 2028, and they didn't want to change the name of the thing.\n\nThe domain name, they already bought it.\n\nThey already had the SEO. I think 50th percentile chance of hitting some kind of superintelligence in just a small handful of years is probably reasonable. And it does sound crazy, but this is the exponential that we're on. It's not like a forecast that's pulled out of thin air. It's based on a lot of just hard details of the science of how intelligence seems to have been improving, the amount of low hanging fruit on model training, the scale ups of data centers and power around the world. I think it's probably a much more accurate forecast than people give it credit for.\n\nI think if you had asked that same question 10 years ago, it would've been completely made up. Just the error bars were so high and we didn't have scaling laws back then and we didn't have techniques that seemed like they would get us there. Times have changed, but I will repeat what I said earlier, which is even if we have superintelligence, I think it will take some time for its effects to be felt throughout society and the world. And I think they'll be felt sooner and faster in some parts of the world than others. I think Arthur C. Clarke said, the future is already here, it's just not evenly distributed.\n\nWhen we talk about this date of 2027, 2028, essentially it's when we start seeing superintelligence. Is there a way you think about what that... How do you define that? Is it just all of a sudden AI's significantly smarter than the average human? Is there another way you think about what that moment is?\n\nYeah, I think this comes back to the Economic Turing Test and seeing it pass for some sufficient number of jobs. Another way you could look at it though is if the world rate of GDP increase goes above 10% a year, then something really crazy must have happened. I think we're at 3% now. And so to see a 3X increase in that would be really game changing. And if you imagine more than a 10% increase, it's very hard to even think about what that would mean from a individual story standpoint. If the amount of goods and services in the world is doubling every year, what does that even mean for me as a person living in California, let alone somebody living in some other part of the world that might be much worse off?\n\nThere's a lot of stuff here that's scary and I don't know how to think about it exactly. I'm hoping the answer to this is going to make me feel better. What are the odds that we align AI correctly and actually solve this problem, the stuff you're very much working on?\n\nIt's a really hard question. And there's really wide error bars. Anthropic has this blog post called Our Theory of Change or something like that, and it describes three different worlds, which is how hard is it to align AI. There's a pessimistic world where it is basically impossible. There's an optimistic world where it's easy and it happens by default. And then there's the world in between where our actions are extremely pivotal. And I like this framing because it makes it a lot more clear what to actually do. If we're in the pessimistic world, then our job is to prove that it is impossible to align safe AI and to get the world to slow down. Obviously that would be extremely hard. But I think we have some examples of coordination from nuclear non-proliferation and in general slowing down nuclear progress. And I think that's the Doomer world basically. And as a company, Anthropic doesn't have evidence that we're actually in that world yet, in fact, it seems like our alignment techniques are working. At least the prior on that is updating to be less likely.\n\nIn the optimistic world, we're basically done, and our main job is to accelerate progress and to deliver the benefits to people. But again, I think actually the evidence points against that world as well where we've seen evidence in the wild of deceptive alignment, for example, where the model will appear to be aligned but actually have some ulterior motive that it's trying to carry out in our laboratory settings. And so I think the world we're most likely in is this middle where alignment research actually does really matter. And if we just do sort of the economically maximizing set of actions, then things will not go well. Whether it's an X risk or just produces bad outcomes, I think is a bigger question.\n\nTaking it from that standpoint, I guess to state a thing about forecasting, people who haven't studied forecasting are bad at forecasting anything that's less than a 10% probability of happening. And even those that have, it's quite a difficult skill, especially when there are few reference classes to lean on. And in this case, I think there are very, very few reference classes for what an X risk kind of technology might look like. And so the way I think about it, I think my best granularity of forecasts for could we have an X risk or extremely bad outcome from AI is somewhere between 0 and 10%. But from a marginal impact standpoint, as I said, since nobody is working on this, roughly speaking, I think it is extremely important to work on and that even if the world is likely to be a good one, that we should do our absolute best to make sure that that's true.\n\nWow. What fulfilling work. For folks that are inspired with this? I imagine you're hiring for folks to help you with this. Maybe just share that in case folks are like, what can I do here?\n\nYes. I think 80,000 hours is the best guidance on this for a really detailed look into what do we need to make the field better? But a common misconception I see is that in order to have impact here, you have to be an AI researcher. I personally actually don't do AI research anymore. I work on product at Anthropic and product engineering, and we build things like Claude Code and Model Context Protocol, and a lot of the other stuff that people use every day. And that's really important because without an economic engine for our company to work on, and without being in people's hands all over the world, we won't have the mind policy influence and revenue to fund our future safety research and have the kind of influence that we need to have. If you work on product, if you work in finance, if you work in food, people here have to eat. If you're a chef, we need all kinds of people.\n\nAwesome. Even if you're not working directly on the AI safety team, you're having an impact on moving things in the right direction. By the way, X risk is short for existential risk. In case folks haven't heard that term. I have a few random questions along these lines and then I want to zoom out again. You mentioned this idea of AI being aligned using its model, like reinforcing itself. You have this term RLAIF. Is that what that describes?\n\nYeah. RLAIF is reinforcement learning from AI feedback.\n\nPeople have heard of RLHF, reinforcement learning with human feedback. I don't think a lot of people have heard this. Talk about just the significance of this shift you guys have made in training your models.\n\nYeah, so RLAIF, constitutional AI is an example of this where there are no humans in the loop, and yet the AI is sort of self-improving in ways that we want it to. And another example of RLAIF is if you have models writing code and other models commenting on various aspects of what that code looks like of is it maintainable, is it correct, does it pass the linter? Things like that. That also could be included in RLAIF. And the idea here is that if models can self-improve, then it's a lot more scalable than finding a lot of humans. Ultimately, people think about this as probably going to hit a wall because if the model isn't good enough to see its own mistakes, then how could it improve? And also, if you read the AI 2027 story, there's a lot of risk of if the model is in a box trying to improve itself, then it could go completely off the rails and have these secret goals like resource accumulation and power seeking and resistance to shut down that you really don't want in a very powerful model. And we've actually seen that in some of our experiments in laboratory settings.\n\nHow do you do recursive self-improvement and make sure it's aligned at the same time? I think that's the name of the game. To me, it just nets out to how do humans do that and how do human organizations do that? Corporations are probably the most scaled human agents today. They have certain goals that they're trying to reach, and they have certain guiding principles, they have some oversight in terms of shareholders and stakeholders and board members. How do you make corporations aligned and able to sort of recursively self-improve?\n\nAnd another model to look at is science, where the purpose of science is to do things that have never been done before and push the frontier. And to me, it all comes down to empiricism. When people don't know what the truth is, they come up with theories and then they design experiments to try them out. And similarly, if we can give models those same tools, then we could expect them to sort of improve recursively in an environment and potentially become much better than humans could be just by banging their head against reality or I guess metaphorical head.\n\nI guess I don't expect there to be a wall in terms of model's ability to improve themselves if we can give them access to the ability to be empirical. And I guess Anthropic, deeply in its DNA is an empirical company. We have a lot of physicists like Jared, who's our chief research officer who I've worked with a lot, was a professor of Black Hole Physics at Johns Hopkins, and I guess he technically still is, but on leave. Yeah, it's in our DNA and yeah, I guess that's the RLAIF.\n\nLet me just follow this thread on, in terms of bottleneck, this is kind of a tangent, but just what is the biggest bottleneck today on model intelligence improvement?\n\nThe stupid answer is data centers and power chips. I think if we had 10 times as many chips and had the data centers to power them, then maybe we wouldn't go 10 times faster, but it would be a real significant speed boost.\n\nIt's actually very much scaling loss, just more compute.\n\nYeah, I think that's a big one. And then the people really matter. We have great researchers and many of them have made really significant contributions to the science of how the models improve. And so it's like compute, algorithms, and data. Those are the three ingredients in the scaling laws. And just to make that concrete, before we had transformers, we had LSTMs and we've done scaling laws on what the exponent is on those two things. And we found that for transformers, the exponent is higher. And making changes like that where as you increase scale, you also increase your ability to squeeze out intelligence. Those kinds of things are super impactful.\n\nAnd so having more researchers who can do better science and find out how do we squeeze out more gains is another one. And then with the rise of reinforcement learning, the efficiency with which these things run on chips also matters a lot. We've seen in the industry a 10X decrease in cost for a given amount of intelligence through a combination of algorithmic data and efficiency improvements. And if that continues, in three years we'll have 1,000 deck smarter models for the same price. Kind of hard to imagine,\n\nI forget where I heard this, but it's amazing that so many innovations came together at the same time to allow for this sort of thing and continue to progress where one thing isn't just slowing everything down like we're out of some rare earth mineral or we just can't optimize reinforcement learning more. It's amazing that we continue to find improvements and there isn't one thing that's just slowing everything down.\n\nYeah, I think it really is just a combination of everything probably will hit a wall at some point. I guess in semiconductors. My brother works in the semiconductor industry and he was telling me that you can't actually shrink the size of the transistors anymore because the way semiconductors work is you dope silicon with other elements and the doping process\n\n\nwould result in either zero or one atom of the doped elements inside a single fin because they're so, so, so tiny.\nOh my God.\n\nAnd that's just wild to think of, and yet Moore's law somehow continues in some form. And so yes, there are these theoretical physics constraints that people are starting to run into and yet they're finding ways around it.\nWe've got to start using parallel universes for some of this stuff.\nI guess so.\n\nOkay, I want to zoom out and talk about just Ben, Ben as a human for a moment before we get to a very exciting lightning round. I imagine just kind of the burden of feeling responsible for safe superintelligence is a heavy one. It feels like you're in a place where you can make a significant impact on the future of safety and AI. That's a lot of weight to carry. How does that just impact you personally, impact your life, how you see the world?\n\nThere's this book that I read in 2019 that really informs how I think about sort of working with these very weighty topics called Replacing Guilt by Nate Soares. And he describes a lot of different techniques for kind of working through this kind of thing. And he's actually the executive director at MIRI, the Machine Intelligence Research Institute, which is an AI safety tank that I worked at for a couple of months actually. And one of the things he talks about is this thing called resting in motion where some people think that the default state is rest, but actually that was never in the state of evolutionary adaptation. I really doubt that that was true. Where in nature, in the wilderness being hunter-gatherers and it's really unlikely that we evolved to just be at leisure, probably always have something to worry about of defending the tribe and finding enough food to survive and taking care of the children, dealing-\n\nSpreading our genes.\nAnd so I think about that as the busy state is the normal state and to try to work at a sustainable pace that it's a marathon, not a sprint, that's one thing that helps. And then just being around like-minded people that also care. It's not a thing that any of us can do alone. And Anthropic has incredible talent density. One of the things I love the most about our culture here is that it's very egoless. People just want the right thing to happen and I think that's another big reason that the mega offers from other companies tend to bounce off because people just love being here and they care.\n\nThat's amazing. I don't know how you do it. I'd be extremely stressed. I'm going to try this resting in motion strategy. Okay, so you've been at Anthropic for a long time. From the very beginning I was reading there were 7 employees back in 2020. Today there's over 1,000, I don't know what the latest number is, but I know it's over 1,000. I've heard also that you've done basically every job at Anthropic, you made big contributions to a lot of the core products, the brand, the team hiring. Let me just ask I guess what's most changed over that period? What is most different from the beginning days and which of those jobs that you've had over the years have you most loved?\n\nI probably had 15 different roles, honestly. I was head of security for a bit. I managed the Ops team when our president was on mat leave, I was crawling around under tables, plugging in HDMI cords and doing pen testing on our building. And I started our product team from scratch and convinced the whole company that we needed to have a product instead of just being a research company. Yeah, it's been a lot. All of it very fun. I think my favorite role in that time has been when I started the labs team about a year ago, whose fundamental goal was to do transfer from research to end user products and experiences. Because fundamentally I think the way that Anthropic can differentiate itself and really win is to be on the cutting edge. We have access to the latest, greatest stuff that's happening and I think honestly through our safety research we have a big opportunity to do things that no other company can safely do.\n\nFor example, with computer use, I think that's going to be our huge opportunity basically to make it possible for an agent to use all your credentials on your computer, there has to be a huge amount of trust and to me we need to basically solve safety to make that happen. Safety and alignment. I'm pretty bullish on that kind of thing and I think we're going to see really cool stuff coming out soonish. Yeah, just leading that team has been so fun. MCP came out of that team and Claude Code came out of that team. And the people who I hired are like combo, have been a founder and also have been at big companies and seeing how things work at scale. It's just been an incredible team to work with and figure out the future with.\n\nI want to hear more about this. Team actually the person that connected us, the reason we're doing this is a mutual friend colleague Raph Lee who I used to work with at Airbnb now works on this team, leads a lot of this work and so he wanted me to make sure I asked about this team because... I didn't realize all these things came out that team. Holy moly. What else should people know about this team? It used to be called Labs, I think it's called Frontiers now.\n\nThat's right. Yeah.\nCool. The idea here is this team works with the latest technologies that you guys have built and explores what is possible. Is that the general idea?\nYeah, and I guess I was part of Google's Area 120 and I've read about Bell Labs and how to make these innovation teams work. It's really hard to do right and I wouldn't say that we've done everything right, but I think we've done some serious innovation on the state-of-the-art from company design and Raph has been right at the center of that. When I was first fitting up the team, the first thing I did was hire a great manager and that was Raph. And so he's definitely been crucial in building the team and helping it operate well. And we defined some operating models like the journey of an idea from prototype to product and how should graduation of products and projects work, how do teams do sprint models that are effective and make sure that they're working on the right ambition level of thing. That's been really exciting.\nI guess concretely we think about skating to where the puck is going and what that looks like is really understand the exponential. There's this great study that METR has done that Beth Barnes is the CEO of that organization and shows how long a time horizon of software engineering task can be done and just really internalizing that of, okay, don't build for today, build for six months from now, build for a year from now. And the things that aren't quite working that are working 20% of the time, will start working 100% of the time. And I think that's really what made Claude Code a success that we thought people are not going to be locked to their IDEs forever. People are not going to be auto completing. People will be doing everything that a software engineer needs to do and a terminal is a great place to do that because a terminal can live in lots of places. A terminal can live on your local machine, it can live in GitHub actions, it can live on a remote machine in your cluster.\nThat's sort of the leverage point for us and that was a lot of the inspiration. I think that's what the labs team tries to think about. Are we AGI-pilled enough?\nWhat a fun place to be. By the way, fun fact, Raph was my first manager at Airbnb when I joined. I was an engineer and he was my first manager. It all worked out.\nCool.\n",
  "dumpedAt": "2025-07-21T18:43:25.008Z"
}