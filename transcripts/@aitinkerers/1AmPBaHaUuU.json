{
  "episodeId": "1AmPBaHaUuU",
  "channelSlug": "@aitinkerers",
  "title": "Dynamic LLM Inference: Tomasz Kolinko's Effort Engine",
  "publishedAt": "2025-06-30T15:00:06.000Z",
  "rawLines": [
    {
      "lang": "en",
      "text": "Usually what I noticed is around 50 you",
      "offset": 0.08,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "don't see really a change in the output.",
      "offset": 2.8,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "So we can remove and that's interesting",
      "offset": 4.64,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "from the research point of view. You can",
      "offset": 6.08,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "remove like 50% of calculations.\n When I",
      "offset": 7.44,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "was in Warsaw for the OpenAI AI",
      "offset": 9.76,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "tinkerers hackathon, I asked several",
      "offset": 11.679,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "people who I should meet. Many said",
      "offset": 13.599,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "Thomas Kolinko. He's a true OG tinkerer.",
      "offset": 15.44,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "So I was not surprised when I went to",
      "offset": 18.24,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "his apartment and saw the same unitry",
      "offset": 19.84,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "robot that one of the teams was hacking",
      "offset": 22,
      "duration": 3.519
    },
    {
      "lang": "en",
      "text": "on earlier that weekend. In this video,",
      "offset": 23.68,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "we talk about a tinkerer that is not",
      "offset": 25.519,
      "duration": 5.121
    },
    {
      "lang": "en",
      "text": "afraid to dive into the guts of models",
      "offset": 27.68,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "and pull out the math to play with",
      "offset": 30.64,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "things in search of optimizations and",
      "offset": 32.559,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "abilities that are not being handed to",
      "offset": 35.28,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "you by the model providers themselves.",
      "offset": 37.04,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "It was incredibly scientifically honest",
      "offset": 38.879,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "to just see someone who could go so",
      "offset": 41.36,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "deeply into technical questioning of",
      "offset": 43.6,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "their own curiosity and playing with the",
      "offset": 45.76,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "core models. Like, it's okay. You can do",
      "offset": 48.16,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "that. So, I think this video is really",
      "offset": 50.239,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "inspiring for that reason. I think",
      "offset": 51.76,
      "duration": 5.319
    },
    {
      "lang": "en",
      "text": "you're going to love it. Let's go.",
      "offset": 53.039,
      "duration": 4.04
    },
    {
      "lang": "en",
      "text": "[Music]",
      "offset": 58.38,
      "duration": 7.119
    },
    {
      "lang": "en",
      "text": "Welcome to AI Tinkerers Oneshot. This is",
      "offset": 67.92,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "our global stage where we invite people",
      "offset": 70.159,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "who are doing something innovative in",
      "offset": 71.84,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "the in the community that deserves an",
      "offset": 73.84,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "extra look or a deeper dive under the",
      "offset": 75.439,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "hood. And today I am here with Thomas",
      "offset": 77.04,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "Kolinko. Um the story of how we met.",
      "offset": 80.4,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "we're here in Warsaw is we just did a",
      "offset": 82.88,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "big hackathon with OpenAI in Warsaw",
      "offset": 84.96,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "where almost a thousand people",
      "offset": 86.96,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "registered and we selected like 200 and",
      "offset": 88.159,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "after it was all said and done I asked",
      "offset": 90.64,
      "duration": 3.519
    },
    {
      "lang": "en",
      "text": "Arur like hey when I'm when I have a",
      "offset": 92.079,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "couple extra days here and I want to",
      "offset": 94.159,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "record somebody on on you know who",
      "offset": 95.439,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "should it be and he immediately said",
      "offset": 97.36,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "you. So welcome to the show.\n Thank you.",
      "offset": 99.36,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "Thanks for having me.\n Um so tell us a",
      "offset": 101.6,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "little bit about yourself.\n Uh I'm an",
      "offset": 104.079,
      "duration": 5.441
    },
    {
      "lang": "en",
      "text": "entrepreneur. I've been doing various",
      "offset": 106.72,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "projects since the last 20 years and I'm",
      "offset": 109.52,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "a member of the local communities. I",
      "offset": 111.439,
      "duration": 3.521
    },
    {
      "lang": "en",
      "text": "like to think so and and I like to",
      "offset": 113.28,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "support other projects and other younger",
      "offset": 114.96,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "people than me trying to set up their",
      "offset": 117.68,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "own things.\n Uh when it comes to AI, I",
      "offset": 119.84,
      "duration": 4.959
    },
    {
      "lang": "en",
      "text": "was thinking, you know, what can I do to",
      "offset": 122.399,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "really understand how the technology is",
      "offset": 124.799,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "working, right? And I like to dig really",
      "offset": 126.32,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "really deep into into AI\n and I said, hm,",
      "offset": 129.039,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "can I optimize somehow all the",
      "offset": 132.56,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "computations, all the all the things",
      "offset": 134.319,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "that are being done there? And uh and",
      "offset": 135.68,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "that's where effort engine came came",
      "offset": 138.08,
      "duration": 5.519
    },
    {
      "lang": "en",
      "text": "effort engine. So this is your your",
      "offset": 141.92,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "project. Tell us more. It's it's an",
      "offset": 143.599,
      "duration": 2.881
    },
    {
      "lang": "en",
      "text": "algorithm. This is not you know we see a",
      "offset": 144.8,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "lot of people who tinkering on like the",
      "offset": 146.48,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "application side or doing generating",
      "offset": 148.16,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "models and this is neither really. This",
      "offset": 150.48,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "is something else.\n Yeah. Well it started",
      "offset": 152,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "as an actual application. And I wanted",
      "offset": 153.92,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "to do something that rel that was",
      "offset": 155.92,
      "duration": 6.319
    },
    {
      "lang": "en",
      "text": "related to uh ra and to search I wanted",
      "offset": 157.519,
      "duration": 6.161
    },
    {
      "lang": "en",
      "text": "to have something that would allow me to",
      "offset": 162.239,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "search through the data using lms more",
      "offset": 163.68,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "efficiently\n but then I noticed that some",
      "offset": 166.08,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "of the algorithms are really maybe",
      "offset": 169.36,
      "duration": 5.519
    },
    {
      "lang": "en",
      "text": "perhaps maybe done better in a better",
      "offset": 171.92,
      "duration": 6.319
    },
    {
      "lang": "en",
      "text": "way\n right and I and I dug into that and",
      "offset": 174.879,
      "duration": 5.601
    },
    {
      "lang": "en",
      "text": "what else can I tell you here sorry and",
      "offset": 178.239,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "is there something you could show us",
      "offset": 180.48,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "like does",
      "offset": 181.68,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "uh so I will show it to in just just a",
      "offset": 183.68,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "moment and\n uh just a few words about",
      "offset": 186.72,
      "duration": 5.599
    },
    {
      "lang": "en",
      "text": "what it is. Uh so regularly when you do",
      "offset": 189.04,
      "duration": 7.839
    },
    {
      "lang": "en",
      "text": "an inference of AI using AI model uh",
      "offset": 192.319,
      "duration": 6.801
    },
    {
      "lang": "en",
      "text": "it's all very static right so so you can",
      "offset": 196.879,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "do you know you have a model that is I",
      "offset": 199.12,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "don't know 70 billion parameters or 7",
      "offset": 200.959,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "billion parameters or whatever and",
      "offset": 202.879,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "that's that's it right you have to do",
      "offset": 204.879,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "this 7 billion multiplications or",
      "offset": 206.8,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "whatever and there is no way to to",
      "offset": 208.879,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "optimize that really right you can cut",
      "offset": 210.48,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "down statically the model right so you",
      "offset": 212.239,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "can just cut it down and and uh distill",
      "offset": 214.08,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "it and make it smaller right you can",
      "offset": 216.56,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "quantize it. So, so you really remove",
      "offset": 218.64,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "some of the accuracy of of the model,",
      "offset": 221.36,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "but uh the computations are",
      "offset": 224,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "computations. It's very static, right?",
      "offset": 225.92,
      "duration": 2.64
    },
    {
      "lang": "en",
      "text": "So, you have the model, you have to do",
      "offset": 227.28,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "everything like that.\n So, what I did was",
      "offset": 228.56,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "I developed a new algorithm that allows",
      "offset": 230.56,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "you to dynamically choose which",
      "offset": 232.4,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "calculations have been done actually.",
      "offset": 234.08,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "Okay?\n And I managed to do that in a way",
      "offset": 235.84,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "that is efficient. So, just as efficient",
      "offset": 237.84,
      "duration": 5.679
    },
    {
      "lang": "en",
      "text": "on GPUs as as a regular thing. So, so,",
      "offset": 239.76,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "so you know it's not a it's easy to",
      "offset": 243.519,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "create a new algorithm uh that that",
      "offset": 245.76,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "would do the multiplications on a CPU,",
      "offset": 247.519,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "right? But if you if it goes into into",
      "offset": 250.239,
      "duration": 5.761
    },
    {
      "lang": "en",
      "text": "the distributed computing distributed",
      "offset": 252.4,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "distributed computing, it it's getting",
      "offset": 256,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "hard, right?\n So anyway, uh the algorithm",
      "offset": 257.68,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "what it allows you to do is\n it's",
      "offset": 261.12,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "inference side algorithm.\n It's exactly",
      "offset": 262.8,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "it's inference side. Okay.\n So what what",
      "offset": 264.72,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "it allows you to do is uh to have this",
      "offset": 266.72,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "uh choice of how precise do you want to",
      "offset": 269.36,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "be the inference to be from 100% which",
      "offset": 272.72,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "is essentially you do all the",
      "offset": 275.12,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "computations until I don't know 10% or",
      "offset": 276.16,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "5%. Right? So you can do that",
      "offset": 278.479,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "dynamically unlike I don't know uh",
      "offset": 280.32,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "quantization where you have to do it you",
      "offset": 282.56,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "know do the pre-process processing and",
      "offset": 284.32,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "and then you know you have a new model",
      "offset": 286.639,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "here you can do it even token by token",
      "offset": 288.479,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "if you if you wanted to right and that",
      "offset": 290.08,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "is something new that that people",
      "offset": 291.919,
      "duration": 2.881
    },
    {
      "lang": "en",
      "text": "haven't seen before. Sure. And the cool",
      "offset": 293.12,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "thing is uh you know it's uh I will get",
      "offset": 294.8,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "a little bit technical here.\n Uh inside",
      "offset": 298.88,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "of the when you're doing inference you",
      "offset": 301.04,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "have matrix multiplication right or",
      "offset": 302.96,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "vector by matrix multiplication. And the",
      "offset": 304.72,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "common wisdom was that you have to do",
      "offset": 307.039,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "the whole thing right? So you have to",
      "offset": 309.44,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "multiply the whole matrix if you want to",
      "offset": 310.8,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "perhaps you can multiply like every",
      "offset": 313.44,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "other row or something like that like",
      "offset": 315.199,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "but but it needs to be very in a",
      "offset": 317.6,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "specific uh order all all the",
      "offset": 320.4,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "computations right otherwise you will",
      "offset": 322.56,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "suffer a lot of penalties you will",
      "offset": 324.96,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "suffer penalty in speed\n and what I",
      "offset": 326.96,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "managed to do is create this algorithm",
      "offset": 330.4,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "where if it's 100% it is slower than",
      "offset": 332.4,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "just regular algorithm slightly right so",
      "offset": 334.96,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "100% it's slower\n 50% computations That's",
      "offset": 336.8,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "roughly break even.\n All right. So, you",
      "offset": 340.24,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "have this an optimization that's also",
      "offset": 342.08,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "very flexible. You can you can dial it",
      "offset": 344.24,
      "duration": 3.519
    },
    {
      "lang": "en",
      "text": "where you how much you want to do. It's",
      "offset": 346,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "lossy, I'm assuming.\n Yeah. Yeah. Of",
      "offset": 347.759,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "course,\n obviously. And then it's on the",
      "offset": 349.68,
      "duration": 3.519
    },
    {
      "lang": "en",
      "text": "inference side. How how does it work? I",
      "offset": 351.199,
      "duration": 3.521
    },
    {
      "lang": "en",
      "text": "mean, I might not have the math, but",
      "offset": 353.199,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "some of our listeners probably do. But",
      "offset": 354.72,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "what is the math?\n I will I will Okay.",
      "offset": 356.639,
      "duration": 5.041
    },
    {
      "lang": "en",
      "text": "When it comes to math, uh um before I",
      "offset": 358.72,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "get to that, I will tell you, yeah, it's",
      "offset": 361.68,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "lossy. Uh the cool thing that I",
      "offset": 363.44,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "discovered is that it's it's",
      "offset": 365.039,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "surprisingly it holds up surprisingly",
      "offset": 367.6,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "well. So I can cut down to I don't know",
      "offset": 369.52,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "20% of all the multiplications. I can",
      "offset": 371.68,
      "duration": 6.239
    },
    {
      "lang": "en",
      "text": "remove them\n and it's uh and it still LLM",
      "offset": 374.16,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "produces still a decent output and that",
      "offset": 377.919,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "is that is amazing if you think about it",
      "offset": 380,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "right\n now how it works uh it there is a",
      "offset": 381.84,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "paper that I published so so so readers",
      "offset": 384.96,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "can go into the details\n but uh long",
      "offset": 387.36,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "story short what what I do is what",
      "offset": 390.24,
      "duration": 4.959
    },
    {
      "lang": "en",
      "text": "algorithm does is you know if you think",
      "offset": 392.4,
      "duration": 5.519
    },
    {
      "lang": "en",
      "text": "about uh the matrix right and the vector",
      "offset": 395.199,
      "duration": 5.521
    },
    {
      "lang": "en",
      "text": "that you want to multiply uh right now",
      "offset": 397.919,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "all the regular algorithms you just have",
      "offset": 400.72,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "to multiply you know one by one",
      "offset": 402.479,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "everything right\n uh what I do is and",
      "offset": 404,
      "duration": 6.4
    },
    {
      "lang": "en",
      "text": "okay the way to explain it is a little",
      "offset": 407.68,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "bit uh reversed\n if you think about all",
      "offset": 410.4,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "the multiplications that you need to do",
      "offset": 413.52,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "I don't know all the 16 million",
      "offset": 415.199,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "multiplications if you have 4,000 by",
      "offset": 416.88,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "4,000 right\n if you imagine all the",
      "offset": 419.039,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "multiplications and if you sorted them",
      "offset": 421.84,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "uh and you only selected the ones that",
      "offset": 424.24,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "have the biggest output the the",
      "offset": 426.319,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "multiplications and only use those",
      "offset": 428.319,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "that's that's that's That's that's that",
      "offset": 430.88,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "would be that would be more or less how",
      "offset": 433.919,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "how the algorithm would work. So so",
      "offset": 435.84,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "essentially choosing the the",
      "offset": 438.16,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "multiplications that are that will have",
      "offset": 440.639,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "the biggest output and the biggest",
      "offset": 443.039,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "influence on the resulting vector that",
      "offset": 444.8,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "that goes into it. I don't know if I",
      "offset": 446.96,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "explained it that maybe perhaps\n so uh",
      "offset": 448.56,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "how much of an efficiency gain are we",
      "offset": 452.4,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "talking about like and how do you",
      "offset": 454.08,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "measure that? Is it tokens per second is",
      "offset": 455.44,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "that\n yeah so there is tokens per second",
      "offset": 457.12,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "and I can show you the demo uh in a",
      "offset": 458.8,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "moment. Okay. So the cool thing again",
      "offset": 460.319,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "about the algorithm is that it's",
      "offset": 462.72,
      "duration": 2.879
    },
    {
      "lang": "en",
      "text": "dynamic, right? It is something new",
      "offset": 463.919,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "because you can you can never before you",
      "offset": 465.599,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "could have really like benefited this",
      "offset": 468.319,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "this way uh when it comes to",
      "offset": 470.319,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "multiplications.\n Uh but uh\n how much",
      "offset": 472,
      "duration": 6.319
    },
    {
      "lang": "en",
      "text": "faster are we talking?\n Yeah. So so it",
      "offset": 475.919,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "can get the definition of speed is a",
      "offset": 478.319,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "little bit tricky, right? And uh because",
      "offset": 480.879,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "uh the question is not how much faster",
      "offset": 483.759,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "because I can do it 5% multiplications,",
      "offset": 485.199,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "right? But there is like you said a loss",
      "offset": 487.52,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "in quality, right? So, so the question",
      "offset": 489.28,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "is how much faster per the loss of",
      "offset": 490.72,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "quality and how do you compare it to the",
      "offset": 493.28,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "frontier of how do you compare it to",
      "offset": 494.879,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "right and the sad news here is that it's",
      "offset": 497.28,
      "duration": 5.359
    },
    {
      "lang": "en",
      "text": "slightly worse than the quantization in",
      "offset": 499.919,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "the end right so if you use quantization",
      "offset": 502.639,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "from 16 bits to I don't know 8 bits or",
      "offset": 505.039,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "four bits you will get twice or four",
      "offset": 507.039,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "times the speed right uh this algorithm",
      "offset": 509.199,
      "duration": 6.161
    },
    {
      "lang": "en",
      "text": "can get uh twice as fast perhaps three",
      "offset": 511.919,
      "duration": 6.161
    },
    {
      "lang": "en",
      "text": "times as fast but the loss in quality",
      "offset": 515.36,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "will be slightly more Okay. So what are",
      "offset": 518.08,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "the cases where this is better then?\n So",
      "offset": 520.08,
      "duration": 3.519
    },
    {
      "lang": "en",
      "text": "so the cases where where it's better",
      "offset": 521.839,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "right now it's uh you know this came up",
      "offset": 523.599,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "after after the premiere. So so that",
      "offset": 525.6,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "that was that was the thing.\n The cases",
      "offset": 527.279,
      "duration": 5.041
    },
    {
      "lang": "en",
      "text": "where it's better\n I don't yet know",
      "offset": 529.44,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "sadly. So so it's you know I would love",
      "offset": 532.32,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "to tell you oh this is the amazing thing",
      "offset": 534.32,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "but but you know the algorithm was",
      "offset": 535.76,
      "duration": 3.519
    },
    {
      "lang": "en",
      "text": "invented a year ago. If it was if it had",
      "offset": 537.04,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "like very solid cases\n then then it",
      "offset": 539.279,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "already would be implemented by",
      "offset": 542.32,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "everyone. So it's not the case we didn't",
      "offset": 543.44,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "find",
      "offset": 547.12,
      "duration": 3.279
    },
    {
      "lang": "en",
      "text": "it but I can show you some of the cases",
      "offset": 548.64,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "where it might be better but also it's",
      "offset": 550.399,
      "duration": 3.521
    },
    {
      "lang": "en",
      "text": "super interesting for the point of view",
      "offset": 552.32,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "of research\n because uh because uh",
      "offset": 553.92,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "because we get a different ch different",
      "offset": 556.72,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "way of approximating the same LLM and we",
      "offset": 559.2,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "can see how you know how various",
      "offset": 561.92,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "implementations differ\n also it's uh this",
      "offset": 563.519,
      "duration": 6.401
    },
    {
      "lang": "en",
      "text": "this ability to manage uh the quality of",
      "offset": 567.12,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "uh of output we can do it again",
      "offset": 569.92,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "dynamically but also layer by layer",
      "offset": 571.839,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "Yeah,\n we can change it very fast and we",
      "offset": 573.519,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "can",
      "offset": 575.2,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "have time to analyze in depth,",
      "offset": 577.2,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "you know, we can do like a thousand",
      "offset": 580.64,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "experiments, you know, managing various",
      "offset": 582.16,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "layers, you know, how how we manage the",
      "offset": 584.48,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "quality on each layer and see really",
      "offset": 586.48,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "which layers matter in which context,",
      "offset": 588.959,
      "duration": 3.521
    },
    {
      "lang": "en",
      "text": "right? So, there is a lot of potential",
      "offset": 590.959,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "there for for for the upcoming for the",
      "offset": 592.48,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "upcoming research. M that's where my",
      "offset": 595.04,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "mind was going was something like",
      "offset": 597.04,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "because you're tunable and I was going",
      "offset": 598.32,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "to ask because it's not quantized a",
      "offset": 600.24,
      "duration": 3.279
    },
    {
      "lang": "en",
      "text": "model that you're just you're running",
      "offset": 602.24,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "that or the other original it's one",
      "offset": 603.519,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "model still so that you could sort of",
      "offset": 605.76,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "intuitively think about doing something",
      "offset": 608,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "like\n I don't know running in parallel to",
      "offset": 609.76,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "get a quick answer to these or now is",
      "offset": 612.399,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "the\n if I run the model using your",
      "offset": 614.24,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "algorithm at full 100% and I ran it",
      "offset": 617.279,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "again at like 20% or something do I end",
      "offset": 619.76,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "up in a similar destination in the",
      "offset": 622.079,
      "duration": 5.681
    },
    {
      "lang": "en",
      "text": "latent face.\n Um I think\n do you know what",
      "offset": 623.92,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "I'm saying?\n Well, you end up because the",
      "offset": 627.76,
      "duration": 3.519
    },
    {
      "lang": "en",
      "text": "the output is very similar, right? So so",
      "offset": 629.68,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "you end up Yeah. Yeah. So so essentially",
      "offset": 631.279,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "you can do all the tricks that you would",
      "offset": 633.519,
      "duration": 3.521
    },
    {
      "lang": "en",
      "text": "do be able to do with with\n so there",
      "offset": 634.72,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "might be an application where you know",
      "offset": 637.04,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "you're able to like in real time",
      "offset": 638.32,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "depending on how fast it is get get the",
      "offset": 640.48,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "user an approximate answer that sort of",
      "offset": 642.079,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "satisfies them while coming back.\n So",
      "offset": 644.56,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "that's one thing. Another thing there",
      "offset": 646.399,
      "duration": 2.961
    },
    {
      "lang": "en",
      "text": "think about voice applications where you",
      "offset": 647.92,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "need to respond quickly.\n That is that is",
      "offset": 649.36,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "nice. Another thing is uh if we were",
      "offset": 651.2,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "able to predict which tokens are really",
      "offset": 653.76,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "the significant ones like where can we",
      "offset": 656.16,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "get down with with uh with the",
      "offset": 658.16,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "approximation when where can we use the",
      "offset": 660.24,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "lower approximation where where you know",
      "offset": 662.399,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "if you think about how the words are",
      "offset": 665.12,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "constructed or how the whole sentences",
      "offset": 666.48,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "usually it's just one or two key words",
      "offset": 668.24,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "that really matter where you really have",
      "offset": 670.399,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "to focus and say oh this is you know",
      "offset": 672.32,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "here we want to get inference right and",
      "offset": 675.12,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "we with the others it's like oh you know",
      "offset": 677.2,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "we can skip over it right Right?\n And",
      "offset": 680,
      "duration": 3.519
    },
    {
      "lang": "en",
      "text": "with the regular quantization, you had",
      "offset": 681.92,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "to load the whole like this version of",
      "offset": 683.519,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "the model, this version of the model,",
      "offset": 685.36,
      "duration": 2.64
    },
    {
      "lang": "en",
      "text": "like all the versions of the model.",
      "offset": 686.72,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "Right? Here you you can load it just",
      "offset": 688,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "just one and and manage it actively.",
      "offset": 690.32,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "Right? So that's one thing.\n Oh, the cool",
      "offset": 693.04,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "thing and the side effect of the whole",
      "offset": 695.04,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "thing. So one thing is the algorithm of",
      "offset": 696.56,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "course the data structure behind it is",
      "offset": 698.88,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "is totally different when it comes to",
      "offset": 700.399,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "how how I store uh the the the weights",
      "offset": 702.16,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "in",
      "offset": 704.56,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "the data structure is different.\n Yeah.",
      "offset": 706.72,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "Turns out that this data structure is is",
      "offset": 708.64,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "super interesting because you can",
      "offset": 712,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "dynamically also during during the load",
      "offset": 713.6,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "time you can choose to skip some of the",
      "offset": 716.079,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "weights that are the least important.",
      "offset": 718.16,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "Interesting.\n So, you know, usually you",
      "offset": 720.16,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "have you have like I don't know 70",
      "offset": 721.839,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "billion model parameter model, right?",
      "offset": 723.6,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "So, so you have to load all or nothing,",
      "offset": 725.76,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "right?\n Y\n either it fits in the VRAM or",
      "offset": 727.68,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "it doesn't fit. Here you can say you",
      "offset": 730,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "know like I almost have it just load top",
      "offset": 732.32,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "90% and skip all the all the all the all",
      "offset": 736.16,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "the smallest weights\n interesting\n and it",
      "offset": 738.56,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "works smoothly and it's\n can you pick and",
      "offset": 740.24,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "choose which ways experimentally\n because",
      "offset": 742.399,
      "duration": 5.601
    },
    {
      "lang": "en",
      "text": "the uh the way uh the algorithm works",
      "offset": 745.36,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "and the data structure works\n uh the",
      "offset": 748,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "weights are sorted in the in the in the",
      "offset": 750.399,
      "duration": 6.56
    },
    {
      "lang": "en",
      "text": "array right in the matrix. Uh so uh so",
      "offset": 752.72,
      "duration": 7.04
    },
    {
      "lang": "en",
      "text": "they're sorted from the the the biggest",
      "offset": 756.959,
      "duration": 5.041
    },
    {
      "lang": "en",
      "text": "ones to the smallest ones, right?\n So it",
      "offset": 759.76,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "it makes you know you can just float a",
      "offset": 762,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "part of it and that's and you're done.",
      "offset": 764.079,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "You don't need the whole of it. But then",
      "offset": 766,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "you cannot really pick and choose",
      "offset": 768.24,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "actually perhaps you could I I didn't",
      "offset": 770.399,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "think through that but the easiest thing",
      "offset": 772.399,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "is you just cut off the the lower",
      "offset": 774.399,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "weights. And now speaking of lowering",
      "offset": 776,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "the the computations what",
      "offset": 778.32,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "uh you know people say oh like we can if",
      "offset": 781.36,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "we have weights you know anyone that has",
      "offset": 784.8,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "played with with with optimizing the",
      "offset": 786.8,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "models says like the first thing you try",
      "offset": 788.56,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "is you just cut off the lowest weight",
      "offset": 790.639,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "right and then like you know cut off 5%",
      "offset": 792.8,
      "duration": 7.12
    },
    {
      "lang": "en",
      "text": "those 0.0 Z yeah\n sure\n but the model the",
      "offset": 795.279,
      "duration": 6.8
    },
    {
      "lang": "en",
      "text": "quality of model drops down very very",
      "offset": 799.92,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "fast after I don't know removing 20% or",
      "offset": 802.079,
      "duration": 5.041
    },
    {
      "lang": "en",
      "text": "30% of weights right\n here that's uh",
      "offset": 804.24,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "worth making clear this algorithm",
      "offset": 807.12,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "doesn't it uses all the weights like all",
      "offset": 809.12,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "the loaded weights so so again you can",
      "offset": 811.68,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "skip 5% or 10% but but all the things",
      "offset": 813.44,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "that are loaded are being used just not",
      "offset": 816.48,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "on every inference step so let's say",
      "offset": 819.04,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "that you have a token and then uh you",
      "offset": 821.2,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "know in this certain token the the input",
      "offset": 823.68,
      "duration": 2.56
    },
    {
      "lang": "en",
      "text": "vector",
      "offset": 825.36,
      "duration": 2.64
    },
    {
      "lang": "en",
      "text": "you will want to use this this this and",
      "offset": 826.24,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "this weights to multiply right because",
      "offset": 828,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "because of the when you multiply it",
      "offset": 830.24,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "those give you the highest result for",
      "offset": 832.8,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "for some other token the the input",
      "offset": 834.72,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "vector is different right and you might",
      "offset": 836.399,
      "duration": 3.521
    },
    {
      "lang": "en",
      "text": "want to use different different weights",
      "offset": 838.24,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "okay right so it's totally dynamic and",
      "offset": 839.92,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "it uses the whole of the network\n so I",
      "offset": 842.079,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "would say yeah that's that's another",
      "offset": 844.24,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "interesting thing about the algorithm",
      "offset": 846.56,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "and the appro uh the approach right",
      "offset": 848.079,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "because if you think about quantization",
      "offset": 850.24,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "like you remove some of the data from",
      "offset": 852,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "the from the network to quant quantize",
      "offset": 853.92,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "it. Here we actually use all the data.",
      "offset": 855.92,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "What is the memory impact of using all",
      "offset": 858.72,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "the data though? Are you always not",
      "offset": 860.399,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "saving there?\n So actually that's that's",
      "offset": 862.16,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "a cool thing. It uses just as much of",
      "offset": 864.24,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "memory as the regular model essentially",
      "offset": 866.16,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "like plus or minus I don't know 1% or",
      "offset": 868.24,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "something like that. The downside second",
      "offset": 870.16,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "downside that that is there and again I",
      "offset": 872.079,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "was working on that algorithm some time",
      "offset": 874.48,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "ago and I had to choose which which",
      "offset": 876.639,
      "duration": 5.841
    },
    {
      "lang": "en",
      "text": "directions I I go\n it is uh right now the",
      "offset": 878.639,
      "duration": 6.481
    },
    {
      "lang": "en",
      "text": "first implementations is only in 16 bit",
      "offset": 882.48,
      "duration": 5.919
    },
    {
      "lang": "en",
      "text": "so so that's that's the weights",
      "offset": 885.12,
      "duration": 6.399
    },
    {
      "lang": "en",
      "text": "that's how they are loaded in it can get",
      "offset": 888.399,
      "duration": 6.081
    },
    {
      "lang": "en",
      "text": "quantized and I actually prepared a Q4 I",
      "offset": 891.519,
      "duration": 5.841
    },
    {
      "lang": "en",
      "text": "think version or Q Q8 I don't remember",
      "offset": 894.48,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "uh but it's not as straightforward as",
      "offset": 897.36,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "with the regular algorithm Because with",
      "offset": 899.12,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "regular inference what you do is you",
      "offset": 900.88,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "just you know instead of 16 bits you",
      "offset": 902.959,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "have eight bits or whatever bits right",
      "offset": 905.68,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "and and and you have just a smaller",
      "offset": 907.68,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "array here you have to tweak the",
      "offset": 909.68,
      "duration": 3.519
    },
    {
      "lang": "en",
      "text": "algorithm a little bit to to to support",
      "offset": 911.44,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "that I don't you know it's all in the in",
      "offset": 913.199,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "the paper so I don't want to get too too",
      "offset": 915.279,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "deep into that but\n I'd love to I'd love",
      "offset": 916.88,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "to just you pull the paper up and show",
      "offset": 918.959,
      "duration": 3.281
    },
    {
      "lang": "en",
      "text": "people so they know they go\n I want I",
      "offset": 920.72,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "want to show you also the demo of the",
      "offset": 922.24,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "whole thing for let's let's go for it.",
      "offset": 923.92,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "Yeah. And it' be cool. I don't know if",
      "offset": 925.92,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "you have time to show or if you're able",
      "offset": 927.6,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "to show how you approached benchmarking",
      "offset": 929.279,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "it, you know, because that's a big part",
      "offset": 931.44,
      "duration": 2.48
    },
    {
      "lang": "en",
      "text": "of the research.\n Yeah, I will show you.",
      "offset": 932.48,
      "duration": 5.919
    },
    {
      "lang": "en",
      "text": "Yeah, exactly that. So, yeah,",
      "offset": 933.92,
      "duration": 6.719
    },
    {
      "lang": "en",
      "text": "right now it's loading. Uh just the",
      "offset": 938.399,
      "duration": 6.961
    },
    {
      "lang": "en",
      "text": "software,\n right?\n Mhm.\n Initially, the by",
      "offset": 940.639,
      "duration": 6.961
    },
    {
      "lang": "en",
      "text": "the way, the first implementation uh I",
      "offset": 945.36,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "built was on a on a MacBook. So, so it",
      "offset": 947.6,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "runs on on M1 processors.\n Cool. I should",
      "offset": 950.16,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "really finally find time to make it run",
      "offset": 952.88,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "on Nvidia. It's not 100% certain that",
      "offset": 955.04,
      "duration": 5.919
    },
    {
      "lang": "en",
      "text": "that it will actually\n work as well on",
      "offset": 958.16,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "Nvidia. So, so I cannot promise promise",
      "offset": 960.959,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "it here,\n but it works very nicely on",
      "offset": 963.04,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "MacBooks.\n So, anyway, the first thing",
      "offset": 964.959,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "that that that happens is it just",
      "offset": 966.8,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "forwards some text, right? So, so",
      "offset": 969.199,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "there's an initial text saying how are",
      "offset": 970.639,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "you feeling today? By the way, you don't",
      "offset": 972.399,
      "duration": 2.641
    },
    {
      "lang": "en",
      "text": "see my video.\n I don't see your screen.",
      "offset": 973.839,
      "duration": 2.56
    },
    {
      "lang": "en",
      "text": "Yeah.\n Okay. You don't see my screen. So,",
      "offset": 975.04,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "I will just\n Okay, thanks.\n Show it to",
      "offset": 976.399,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "you.\n I appreciate that.\n I'll show it to",
      "offset": 978.24,
      "duration": 2.959
    },
    {
      "lang": "en",
      "text": "you. Right. So the thing first thing",
      "offset": 979.759,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "that you see is you see here like",
      "offset": 981.199,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "initial text and how are you feeling",
      "offset": 983.68,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "today and it's it's just completing the",
      "offset": 985.6,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "text like any model does. The reason I",
      "offset": 988.16,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "have it here and that's a funny story is",
      "offset": 990.24,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "that it's uh I noticed that when I lower",
      "offset": 991.92,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "the the quality usually the text gets",
      "offset": 994.959,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "sadder and sadder for for this LLM. I",
      "offset": 997.44,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "don't know. Yeah. But but it's you know",
      "offset": 999.759,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "I need to do some research. Those are",
      "offset": 1002.079,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "the kind of quirks that that come up and",
      "offset": 1003.92,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "that was you know I don't know if it's",
      "offset": 1006.959,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "scientifically proven but but yeah like",
      "offset": 1008.88,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "that's exactly like me I don't get my",
      "offset": 1010.72,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "coffee like I will get groggy right so",
      "offset": 1012.639,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "this\n that's funny\n yeah I remember the",
      "offset": 1014.48,
      "duration": 3.279
    },
    {
      "lang": "en",
      "text": "first one of the first things that I",
      "offset": 1016.48,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "tried was what is the meaning of life",
      "offset": 1017.759,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "right and the 100% model said like oh",
      "offset": 1019.6,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "it's like to give to the people whatever",
      "offset": 1022,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "right and the and the and the and the",
      "offset": 1024.079,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "dumbest like when I cut down the quality",
      "offset": 1026.48,
      "duration": 4.959
    },
    {
      "lang": "en",
      "text": "yeah the lowest quality said oh the",
      "offset": 1029.439,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "meaning of life is to live and to",
      "offset": 1031.439,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "They're all going to be the AI is really",
      "offset": 1033.039,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "upset with us if we don't feed it",
      "offset": 1035.12,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "nuclear energy.\n Exactly.\n So, uh so now",
      "offset": 1036.64,
      "duration": 6.559
    },
    {
      "lang": "en",
      "text": "uh what's uh that's the loading screen,",
      "offset": 1040.319,
      "duration": 6.801
    },
    {
      "lang": "en",
      "text": "right? And it starts with 100%",
      "offset": 1043.199,
      "duration": 7.441
    },
    {
      "lang": "en",
      "text": "accuracy or or effort.\n Um\n the reason by",
      "offset": 1047.12,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "the way I use the name effort is that I",
      "offset": 1050.64,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "didn't find a name for this metric,",
      "offset": 1053.36,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "right? Because with quantization there",
      "offset": 1055.2,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "is like present quantization, right?",
      "offset": 1056.72,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "Sure. uh we have uh how was it called in",
      "offset": 1058.4,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "with uh with uh when there is a matrix",
      "offset": 1060.96,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "and not all the uh weights are",
      "offset": 1064.4,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "populated. I forgot the the English word",
      "offset": 1066,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "but there is there is this other thing",
      "offset": 1067.84,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "that is that is there right so there's\n I",
      "offset": 1069.84,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "think it's it's important to to",
      "offset": 1071.84,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "recognize that you're you're innovating",
      "offset": 1073.039,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "in the benchmark arena\n yeah so so it was",
      "offset": 1074.96,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "interesting because uh the question is",
      "offset": 1077.28,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "how do you you know call this new metric",
      "offset": 1079.44,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "and actually came up with the name",
      "offset": 1081.039,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "effort right which is which is like this",
      "offset": 1082.799,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "metric of how much effort does you want",
      "offset": 1084.799,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "to for a given result yeah exactly so",
      "offset": 1087.2,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "let me type in something example and I",
      "offset": 1089.44,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "don't remember which model actually this",
      "offset": 1091.52,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "was either mistro or llama All right,",
      "offset": 1093.2,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "let's\n ask it what it thinks about",
      "offset": 1096.08,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "McDonald's for breakfast.",
      "offset": 1097.28,
      "duration": 6.36
    },
    {
      "lang": "en",
      "text": "Okay. Uh,",
      "offset": 1099.52,
      "duration": 4.12
    },
    {
      "lang": "en",
      "text": "right. So, so the regular model\n and this",
      "offset": 1111.36,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "is running at 100% then.\n Yeah, it's",
      "offset": 1113.44,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "100%. Right. So, like I told you at 100%",
      "offset": 1115.12,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "I would say it's",
      "offset": 1117.679,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "50% or like twice as slow as\n And what",
      "offset": 1119.6,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "actual model you using for this the",
      "offset": 1122.48,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "base? I would love to remember because I",
      "offset": 1123.84,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "programmed it a year ago. So, so I think",
      "offset": 1126,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "it's either Mistral or Lama. I tested it",
      "offset": 1127.6,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "in both and it also works with how many",
      "offset": 1129.84,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "parameters?\n Um, this one is uh if I",
      "offset": 1132.16,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "remember correctly is 7TB.",
      "offset": 1135.12,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "That's a pretty big one. That's a nice",
      "offset": 1138.16,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "Yeah. Yeah. I think that's that's the",
      "offset": 1139.84,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "the the big model that's running here.",
      "offset": 1141.2,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "But also here it's, you know, I have 96",
      "offset": 1143.679,
      "duration": 6.721
    },
    {
      "lang": "en",
      "text": "GB of data. So, no, it wouldn't be 70B,",
      "offset": 1146.64,
      "duration": 5.279
    },
    {
      "lang": "en",
      "text": "but it's one of the bigger models,",
      "offset": 1150.4,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "right? And it's loading actually 80% or",
      "offset": 1151.919,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "90% of the of the model because it's\n I",
      "offset": 1153.76,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "mean I talk I don't know how many tokens",
      "offset": 1156.64,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "per second that was probably",
      "offset": 1158.16,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "it actually around seven or eight",
      "offset": 1160.64,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "tokens. So that's not that's not super",
      "offset": 1162.4,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "fast.\n But again inference on MacBooks in",
      "offset": 1164.4,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "general isn't isn't super fast, right?",
      "offset": 1167.2,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "It's good to know that the thing is uh",
      "offset": 1169.039,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "McDonald's offers a variety of breakfast",
      "offset": 1172.64,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "items that can be convenient. Uh okay. I",
      "offset": 1174.799,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "never tried this query. So it's hard",
      "offset": 1177.2,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "it's hard to say. But now you know if I",
      "offset": 1179.2,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "type in 50 it will just redo the whole",
      "offset": 1180.96,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "thing\n with the same query\n uh with the",
      "offset": 1182.88,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "same query just just you know lower",
      "offset": 1184.559,
      "duration": 5.681
    },
    {
      "lang": "en",
      "text": "lower effort.\n It takes\n so you can see",
      "offset": 1186.88,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "it's a lot faster. Yeah.\n Yeah. So yeah",
      "offset": 1190.24,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "and usually what I noticed is around 50",
      "offset": 1192.72,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "you don't see really a change in uh in",
      "offset": 1195.44,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "the output. So we can remove and that's",
      "offset": 1197.6,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "interesting from the research point of",
      "offset": 1199.28,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "view. remove% of calculations and",
      "offset": 1200.48,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "essentially the output is almost the",
      "offset": 1204.24,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "same benchmarks in a second. Uh the",
      "offset": 1205.76,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "downside is that this algorithm around",
      "offset": 1209.44,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "50 I would say it break even with the",
      "offset": 1211.12,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "regular matrix multiplication right so",
      "offset": 1212.88,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "so this is like we're still it's faster",
      "offset": 1214.799,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "than 100% of this algorithm but it's",
      "offset": 1216.88,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "more or less break even if we just use",
      "offset": 1219.679,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "regular multiplication but now\n here it",
      "offset": 1221.679,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "brought out a whole different angle",
      "offset": 1223.52,
      "duration": 2.48
    },
    {
      "lang": "en",
      "text": "though because the first one just talked",
      "offset": 1224.799,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "about convenience and taste and the menu",
      "offset": 1226,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "and this adds in the like analysis of",
      "offset": 1228.24,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "the nutrition.\n Oh yeah that's right. So",
      "offset": 1230.32,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "is that the example of getting more",
      "offset": 1232.559,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "negative? I will we'll see like this is",
      "offset": 1233.84,
      "duration": 4.959
    },
    {
      "lang": "en",
      "text": "this is I didn't try this example so",
      "offset": 1236.799,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "we'll see how how it goes right uh and",
      "offset": 1238.799,
      "duration": 7.521
    },
    {
      "lang": "en",
      "text": "what not I can go down to 30% and and",
      "offset": 1241.679,
      "duration": 8.88
    },
    {
      "lang": "en",
      "text": "you know still much faster right\n and",
      "offset": 1246.32,
      "duration": 7.04
    },
    {
      "lang": "en",
      "text": "you can see that it's almost super super",
      "offset": 1250.559,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "close right so one to the other it's",
      "offset": 1253.36,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "it's very close in the output 50\n yeah",
      "offset": 1255.039,
      "duration": 5.041
    },
    {
      "lang": "en",
      "text": "and if you think about it this is really",
      "offset": 1257.6,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "aside from the speed like we're only",
      "offset": 1260.08,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "doing 30% of multiplications Right. So,",
      "offset": 1262.159,
      "duration": 6.321
    },
    {
      "lang": "en",
      "text": "so just know it's interesting by itself.",
      "offset": 1264.799,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "Yeah. It's two and a half times faster",
      "offset": 1268.48,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "than the 100.\n Yeah. Exactly. Right. So,",
      "offset": 1269.679,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "so that's the thing. And around here, I",
      "offset": 1272,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "would say it's twice or or 30% faster",
      "offset": 1274,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "than the regular vanilla algorithm that",
      "offset": 1276.799,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "I would be running on this on this",
      "offset": 1278.96,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "laptop. Right now, with 20%, it still",
      "offset": 1280.559,
      "duration": 5.441
    },
    {
      "lang": "en",
      "text": "provides uh quality although you start",
      "offset": 1283.44,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "to see a little bit breakdowns here and",
      "offset": 1286,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "there. I don't know about here.\n Well,",
      "offset": 1287.84,
      "duration": 2.959
    },
    {
      "lang": "en",
      "text": "now it's really focused on egg muffin",
      "offset": 1289.2,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "and see some reason.\n So, it's different.",
      "offset": 1290.799,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "I I will show\n fruit and yogurt.\n Yeah, I",
      "offset": 1292.88,
      "duration": 3.279
    },
    {
      "lang": "en",
      "text": "will I will show you some some",
      "offset": 1295.12,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "interesting cases in a second, but you",
      "offset": 1296.159,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "know, it still produces decent text.\n I'm",
      "offset": 1298,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "just dying to hit one.\n Yeah. Well, so I",
      "offset": 1300.32,
      "duration": 5.599
    },
    {
      "lang": "en",
      "text": "will go like slower. So, let's say I",
      "offset": 1303.2,
      "duration": 6.479
    },
    {
      "lang": "en",
      "text": "think around 16% usually uh I it starts",
      "offset": 1305.919,
      "duration": 5.681
    },
    {
      "lang": "en",
      "text": "to break down really in quality, right?",
      "offset": 1309.679,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "So, there is a spot and\n you kind of are",
      "offset": 1311.6,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "you reaching back to like GPT2 insane",
      "offset": 1313.76,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "human talking kind of.\n Sorry. Sorry. Can",
      "offset": 1316,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "you\n remember in GPT2 it was like hard to",
      "offset": 1318.08,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "get it to be anything coherent. Oh yeah,",
      "offset": 1320.72,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "yeah, yeah. So, so I would say you you",
      "offset": 1322.72,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "can kind of see that. Yeah. Also,",
      "offset": 1324.72,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "another interesting thing I noticed, I",
      "offset": 1326.64,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "don't remember with which model or",
      "offset": 1329.12,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "perhaps all of them is that the lower I",
      "offset": 1331.28,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "get, the more technical they try to get.",
      "offset": 1333.44,
      "duration": 6.479
    },
    {
      "lang": "en",
      "text": "So, if you say uh how are for example,",
      "offset": 1336.96,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "and you just want to do simple text",
      "offset": 1339.919,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "completion, right? If you say how are",
      "offset": 1342.96,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "and you allow the model to finish,\n um",
      "offset": 1344.72,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "the higher quality models will say how",
      "offset": 1347.76,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "are you feeling today? feeling fine and",
      "offset": 1349.84,
      "duration": 6.079
    },
    {
      "lang": "en",
      "text": "what the lower I got usually it was like",
      "offset": 1352.08,
      "duration": 7.04
    },
    {
      "lang": "en",
      "text": "how are we supposed to do C++ whatever",
      "offset": 1355.919,
      "duration": 5.361
    },
    {
      "lang": "en",
      "text": "and then it goes to into HTML and whatn",
      "offset": 1359.12,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "not like like you seeing a model that",
      "offset": 1361.28,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "was trained only on technical forums I",
      "offset": 1363.28,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "don't know why why it happens like that",
      "offset": 1364.88,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "and just just you know and that's one of",
      "offset": 1367.12,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "the effects that I notic in a few cases",
      "offset": 1369.36,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "right\n are like that to some extent too",
      "offset": 1370.96,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "the ability to see the big picture while",
      "offset": 1372.559,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "also details\n exactly that so so that's",
      "offset": 1374.159,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "you know it tells you you know this is a",
      "offset": 1376.559,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "new method of of also looking at the",
      "offset": 1378.48,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "model Yeah.\n Again. So now 16%\n you know",
      "offset": 1380.159,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "instead of um what you called it what uh",
      "offset": 1382.72,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "what you call it effort engine you could",
      "offset": 1385.039,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "have called it labbotomy.",
      "offset": 1386.96,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "I that's funny because that was actually",
      "offset": 1389.039,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "one of the first names I wanted to",
      "offset": 1390.88,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "originally played with distillation of",
      "offset": 1392.88,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "of models and I was thinking of creating",
      "offset": 1394.64,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "a lobizer.",
      "offset": 1396.96,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "Yeah. Something that would just you know",
      "offset": 1398.72,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "pick things that I don't need from the",
      "offset": 1400.88,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "model. So so that's\n but you see it's",
      "offset": 1402.559,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "clearly this is three times faster now",
      "offset": 1404.32,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "legitimately. So that's that's uh yeah",
      "offset": 1406,
      "duration": 7.039
    },
    {
      "lang": "en",
      "text": "uh three times faster than the\n at 20 and",
      "offset": 1408.799,
      "duration": 8.081
    },
    {
      "lang": "en",
      "text": "now uh see\n oh here it's opinionated is",
      "offset": 1413.039,
      "duration": 5.361
    },
    {
      "lang": "en",
      "text": "not a typical choice for bre what are",
      "offset": 1416.88,
      "duration": 3.279
    },
    {
      "lang": "en",
      "text": "you doing here\n I I feel that you know",
      "offset": 1418.4,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "some of the layers of of conditioning",
      "offset": 1420.159,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "get get lost with the me time and I",
      "offset": 1422.88,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "think around 14 and 13% you will start",
      "offset": 1424.799,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "to finally see the breakdown in quality",
      "offset": 1427.28,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "although it's a simple question right I",
      "offset": 1429.6,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "will show you more\n here it's it's like",
      "offset": 1431.76,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "it's answering the question but not",
      "offset": 1434.32,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "directly it's sort talking McDonald's.",
      "offset": 1435.44,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "So, so we can see that, right? And then",
      "offset": 1437.919,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "if we go to 10%, it's we should start to",
      "offset": 1439.76,
      "duration": 5.279
    },
    {
      "lang": "en",
      "text": "see the breakdown, right?\n This is funny.",
      "offset": 1442.559,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "Yeah. And now it's, you know, it's just",
      "offset": 1445.039,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "got stuck on on\n because it's repeating.",
      "offset": 1447.2,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "Yeah. It's just repeating, right?\n It's",
      "offset": 1449.919,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "doing that.\n So, so speaking of GPT2,",
      "offset": 1451.039,
      "duration": 5.441
    },
    {
      "lang": "en",
      "text": "like that's kind of what what we can see",
      "offset": 1454.08,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "here, right? And\n well, even without with",
      "offset": 1456.48,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "without um you know, human reinforcement",
      "offset": 1458.159,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "learning or fine instruct fine tuning,",
      "offset": 1460.32,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "you do this, right?\n Yeah, exactly that.",
      "offset": 1462.24,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "So, so and then if I do 5% that's like",
      "offset": 1464.159,
      "duration": 6.481
    },
    {
      "lang": "en",
      "text": "totally broken, right? Uh it's nice that",
      "offset": 1467.2,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "like like you can see like we can play",
      "offset": 1470.64,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "with that and again this is the first",
      "offset": 1472.4,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "iteration so really it goes broadly",
      "offset": 1474.32,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "right so if I say 10% it's 10% across",
      "offset": 1476.159,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "all the layers but uh but what we could",
      "offset": 1478.4,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "do is we could play more so we could say",
      "offset": 1480.88,
      "duration": 5.679
    },
    {
      "lang": "en",
      "text": "oh let's give\n middle layers 10% and the",
      "offset": 1483.039,
      "duration": 5.441
    },
    {
      "lang": "en",
      "text": "side layers we can you know beginning",
      "offset": 1486.559,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "and end layers let's go 80% right so I",
      "offset": 1488.48,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "didn't even touch upon in my exploration",
      "offset": 1490.88,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "of this subject like how actually how",
      "offset": 1493.2,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "much can we optimize and I know some of",
      "offset": 1495.279,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "the some of uh weight some of the some",
      "offset": 1496.88,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "some of the mattresses I'm quite sure we",
      "offset": 1499.679,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "can get very low here without a loss of",
      "offset": 1502,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "quality any significant loss of quality",
      "offset": 1504.32,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "right\n have you have you played with the",
      "offset": 1506.32,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "combination of this and just sort of",
      "offset": 1508.24,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "messing with the weights\n uh just messing",
      "offset": 1510.08,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "with the weights\n yeah changing the",
      "offset": 1512.24,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "weights\n well not not intentionally",
      "offset": 1513.44,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "yeah but uh it is uh you know if you",
      "offset": 1516.24,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "change the weights and again this comes",
      "offset": 1518.88,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "down to um to this fact that if you",
      "offset": 1520.559,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "change the weights or you don't load",
      "offset": 1523.44,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "some of the weights statically you",
      "offset": 1525.12,
      "duration": 6.72
    },
    {
      "lang": "en",
      "text": "the model that gets gets like dumber,",
      "offset": 1528.72,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "right? Like it loses this quality very",
      "offset": 1531.84,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "fast. But here it's dynamically it's it",
      "offset": 1534.08,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "still uses like this 5% it still uses",
      "offset": 1536.48,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "all the weights. Okay,\n probably right or",
      "offset": 1539.36,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "most of the weights all the time\n just",
      "offset": 1542,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "different weights for different token",
      "offset": 1544.24,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "right so I would say the quality is",
      "offset": 1546.32,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "still there like the loss of quality is",
      "offset": 1549.12,
      "duration": 5.439
    },
    {
      "lang": "en",
      "text": "uh is uh is you know it's proceeding way",
      "offset": 1551.76,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "way slower\n so that's one of the things",
      "offset": 1554.559,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "and by the way like another cool thing",
      "offset": 1556.4,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "that I noticed was uh the way it broke",
      "offset": 1557.919,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "down and initially when I was",
      "offset": 1560.72,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "programming that also that was fun",
      "offset": 1562.64,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "because it was in Python that I wrote",
      "offset": 1564.72,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "the first version so it took me a whole",
      "offset": 1566.4,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "night to to to to to get the inference",
      "offset": 1568,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "of one sentence.\n Uh my theory was that",
      "offset": 1570.72,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "if I lower the quality from I don't know",
      "offset": 1574.48,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "100% to 50 or whatever.\n Uh what I would",
      "offset": 1576.48,
      "duration": 7.28
    },
    {
      "lang": "en",
      "text": "see is across all the layers it would be",
      "offset": 1580,
      "duration": 6.4
    },
    {
      "lang": "en",
      "text": "like start falling apart, right? And and",
      "offset": 1583.76,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "it would just drop down. So this is the",
      "offset": 1586.4,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "original output, right? So so this",
      "offset": 1587.919,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "vectors vector original vector and it",
      "offset": 1589.52,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "would just start to to fall out, right?",
      "offset": 1591.919,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "And go and go go down. But what was",
      "offset": 1593.919,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "interesting was that actually at least",
      "offset": 1597.039,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "in the llama that I checked but I guess",
      "offset": 1599.12,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "in other models too\n what happened was so",
      "offset": 1600.48,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "this is the first layer right usually it",
      "offset": 1603.039,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "was like okay it's falling down but then",
      "offset": 1605.12,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "it's getting back to the original uh",
      "offset": 1607.12,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "vector so there is in the model you know",
      "offset": 1608.88,
      "duration": 5.279
    },
    {
      "lang": "en",
      "text": "the middle can be can be somehow more",
      "offset": 1611.279,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "you know changing and then in the end it",
      "offset": 1614.159,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "still converges to the same output. So",
      "offset": 1616.799,
      "duration": 3.281
    },
    {
      "lang": "en",
      "text": "you could think, oh yeah, like\n can you",
      "offset": 1618.72,
      "duration": 2.64
    },
    {
      "lang": "en",
      "text": "can you walk us through the architecture",
      "offset": 1620.08,
      "duration": 2.719
    },
    {
      "lang": "en",
      "text": "a little bit? Like so we've walked",
      "offset": 1621.36,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "through the math, we've seen it in",
      "offset": 1622.799,
      "duration": 2.801
    },
    {
      "lang": "en",
      "text": "action, you've discussed the research",
      "offset": 1624.08,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "aspects of this, but I want to pop out",
      "offset": 1625.6,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "and just talk about like the tooling",
      "offset": 1627.6,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "like for example, if somebody wanted you",
      "offset": 1629.039,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "can download weights, right?\n That's\n if",
      "offset": 1630.64,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "you wanted to go mess with them like",
      "offset": 1632.48,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "this and what are the tools you using?",
      "offset": 1633.84,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "Uh sure. Uh just one one more thing",
      "offset": 1635.679,
      "duration": 3.521
    },
    {
      "lang": "en",
      "text": "before we get to architecture because",
      "offset": 1637.76,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "this example I I will show you one more",
      "offset": 1639.2,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "very quick example that I use for for",
      "offset": 1641.6,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "testing, right? Because this one, you",
      "offset": 1643.2,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "know, it's like a generic question",
      "offset": 1644.96,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "that's, you know, anyone has opinion",
      "offset": 1647.039,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "about McDonald's, I would say, right?",
      "offset": 1648.799,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "But but the question that I really like",
      "offset": 1650.96,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "that shows the quality is um is what was",
      "offset": 1652.72,
      "duration": 7.12
    },
    {
      "lang": "en",
      "text": "it? How far\n is Radom, which is a city in",
      "offset": 1656.64,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "in Poland, but you know, again, I would",
      "offset": 1659.84,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "not specify that it's that it's\n uh",
      "offset": 1662,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "that's in in Poland. It's a small city.",
      "offset": 1664.559,
      "duration": 3.281
    },
    {
      "lang": "en",
      "text": "So again, it shows how precise the model",
      "offset": 1666.159,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "is. How is Faris Random from\n Sydney?",
      "offset": 1667.84,
      "duration": 4.959
    },
    {
      "lang": "en",
      "text": "Yeah.\n Right. And this is a way more",
      "offset": 1671.039,
      "duration": 2.961
    },
    {
      "lang": "en",
      "text": "difficult question. Sure.\n Right. Because",
      "offset": 1672.799,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "it and it shows us how how the model",
      "offset": 1674,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "proceeds.\n Sure.\n Uh and again 5% it's",
      "offset": 1676.24,
      "duration": 6.159
    },
    {
      "lang": "en",
      "text": "like it's garbage\n but then",
      "offset": 1678.96,
      "duration": 7.36
    },
    {
      "lang": "en",
      "text": "just 15 I will go very fast.\n And in your",
      "offset": 1682.399,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "harness you set up an eval harness with",
      "offset": 1686.32,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "lots of these kinds of questions. Did",
      "offset": 1688.159,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "you make up those questions or did you",
      "offset": 1689.6,
      "duration": 2.64
    },
    {
      "lang": "en",
      "text": "find a set that you could use for this?",
      "offset": 1691.039,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "So I I did evolves with my own",
      "offset": 1692.24,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "questions. A lot of some of them I tried",
      "offset": 1694.399,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "to use the regular evals. So okay let me",
      "offset": 1696.799,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "let me finish this and I will go to to",
      "offset": 1699.6,
      "duration": 4.959
    },
    {
      "lang": "en",
      "text": "evolves. So right now it's 15 with 50%",
      "offset": 1701.44,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "you can see consistent answer but then",
      "offset": 1704.559,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "you can see that it says 85 miles",
      "offset": 1706.48,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "definitely it's not 85 miles right\n but",
      "offset": 1709.12,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "then you can see how the quality gets",
      "offset": 1711.279,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "improved\n of the of the whole of the",
      "offset": 1713.2,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "whole\n it's nice that like by nature of",
      "offset": 1715.52,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "the question it's clearer eval you know",
      "offset": 1717.44,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "obviously\n yeah yeah exactly right so so",
      "offset": 1719.679,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "this is it's that was my base base",
      "offset": 1721.52,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "testing because it's way faster than",
      "offset": 1724.48,
      "duration": 2.559
    },
    {
      "lang": "en",
      "text": "just running\n and at what point is it",
      "offset": 1725.76,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "converging to like reality or a good",
      "offset": 1727.039,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "answer\n I think I think around 25%",
      "offset": 1728.48,
      "duration": 6.96
    },
    {
      "lang": "en",
      "text": "Usually it's 25%. Right now it's it's",
      "offset": 1731.679,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "that's amazing because that's quite a",
      "offset": 1735.44,
      "duration": 5.119
    },
    {
      "lang": "en",
      "text": "lot less.\n Uh I would say usually it's",
      "offset": 1737.279,
      "duration": 6.241
    },
    {
      "lang": "en",
      "text": "around 25%. Here it's 30% but I think I",
      "offset": 1740.559,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "didn't load the all the weight so it's",
      "offset": 1743.52,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "slightly more.\n I have a question that's",
      "offset": 1745.279,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "a similar genre. Who was the president",
      "offset": 1746.64,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "before George Washington?\n Oh uh I",
      "offset": 1748.72,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "remember that we played with that thing",
      "offset": 1752.48,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "with GP3, right? Who was the president?",
      "offset": 1754,
      "duration": 5.88
    },
    {
      "lang": "en",
      "text": "Uhhuh.",
      "offset": 1756.88,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "And",
      "offset": 1764.88,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "so position doesn't didn't exist, right?",
      "offset": 1766.799,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "At 30% it says that I wonder if at at",
      "offset": 1769.36,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "where is it going to start saying it was",
      "offset": 1771.36,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "the king\n uh president before George",
      "offset": 1772.88,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "Washington was not a president. The",
      "offset": 1776,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "first president was no. So it's kind of",
      "offset": 1777.279,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "it's confused",
      "offset": 1779.44,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "but yeah around that that amount it",
      "offset": 1781.76,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "starts to break down. Now getting back",
      "offset": 1784.24,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "to architecture and how you can play",
      "offset": 1786.159,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "with it and also the benchmarks right.",
      "offset": 1787.679,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "Uh so how you can play with it you just",
      "offset": 1789.679,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "go to and uh I need to open the website",
      "offset": 1791.6,
      "duration": 8.4
    },
    {
      "lang": "en",
      "text": "uh give me one second. Uh",
      "offset": 1795.279,
      "duration": 7.721
    },
    {
      "lang": "en",
      "text": "uh",
      "offset": 1800,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "I guess you will link I remember there",
      "offset": 1804.08,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "was a better domain name here but but",
      "offset": 1806,
      "duration": 8.399
    },
    {
      "lang": "en",
      "text": "you just go to the to to the page uh uh",
      "offset": 1808.48,
      "duration": 9.199
    },
    {
      "lang": "en",
      "text": "oh kolinko.github.io Well,\n right. So, it",
      "offset": 1814.399,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "it explains all the the whole algorithm",
      "offset": 1817.679,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "and and provides some of the benchmarks.",
      "offset": 1819.6,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "It is difficult and I think we don't",
      "offset": 1822,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "appreciate how rare it is to to have a",
      "offset": 1824.88,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "really new algorithm that is not",
      "offset": 1827.6,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "explainable in two words essentially.",
      "offset": 1829.36,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "So, it is a complicated algorithm but",
      "offset": 1832,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "it's it explains how how the algorithm",
      "offset": 1833.84,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "works and what not.\n Mhm. just going to",
      "offset": 1836,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "the website and and reading about the",
      "offset": 1838.08,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "algorithm. Uh it it will be an",
      "offset": 1839.84,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "interesting read for you if you're into",
      "offset": 1842.32,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "algorithmics that will be something new",
      "offset": 1844.08,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "right um especially since it again works",
      "offset": 1846.08,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "on GPUs.\n Second thing is of course it's",
      "offset": 1848.559,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "there is a GitHub repo and it took a lot",
      "offset": 1851.12,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "of effort to really make it run on any",
      "offset": 1853.36,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "on any MacBook right so hopefully out of",
      "offset": 1856.159,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "the box uh speaking of GitHub repo I",
      "offset": 1858.24,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "really took a lot of effort to make sure",
      "offset": 1861.279,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "that it works on on a laptop from the",
      "offset": 1863.279,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "get- go. So you should do g clone you",
      "offset": 1865.12,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "should there will be weights to be",
      "offset": 1867.6,
      "duration": 4.959
    },
    {
      "lang": "en",
      "text": "downloaded from uh from hugging phase",
      "offset": 1869.84,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "converted weights into into this this",
      "offset": 1872.559,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "format\n and you should have it working I",
      "offset": 1874.559,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "I hope so you know you know how it is",
      "offset": 1876.96,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "with the projects right so so hopefully",
      "offset": 1879.279,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "that that works and I would say just",
      "offset": 1880.88,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "play with it now since it's a Mac uh Mac",
      "offset": 1883.2,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "app again I the reason I decided to go",
      "offset": 1885.84,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "with a Mac platform was so that any",
      "offset": 1888.64,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "developer just with a laptop could play",
      "offset": 1890.399,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "with it and of course Nvidia might be",
      "offset": 1892.24,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "better when it comes to people that are",
      "offset": 1895.039,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "working with with the actual science.",
      "offset": 1897.2,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "But but\n yeah, I I decided to go with the",
      "offset": 1898.72,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "with the MacBook and then you just, you",
      "offset": 1901.039,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "know, open it and\n completely open",
      "offset": 1903.039,
      "duration": 2.961
    },
    {
      "lang": "en",
      "text": "source. People could could take off your",
      "offset": 1904.559,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "work, take take it onto the next level.",
      "offset": 1906,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "Exactly. Use it as Yeah. And also one",
      "offset": 1908.64,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "thing is just preparing something and",
      "offset": 1911.36,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "from the theoretical perspective, but",
      "offset": 1913.039,
      "duration": 3.281
    },
    {
      "lang": "en",
      "text": "the second thing is you want to really",
      "offset": 1914.72,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "implement it and and to show how it",
      "offset": 1916.32,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "works and you want this reproductibility",
      "offset": 1918.48,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "that I care about. Uh the final thing",
      "offset": 1921.12,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "that you were asking was the benchmarks",
      "offset": 1922.96,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "themselves, right? And um and here\n I",
      "offset": 1924.48,
      "duration": 6.72
    },
    {
      "lang": "en",
      "text": "just need to",
      "offset": 1928.24,
      "duration": 9.52
    },
    {
      "lang": "en",
      "text": "show you.\n So uh speaking of uh uh",
      "offset": 1931.2,
      "duration": 8.56
    },
    {
      "lang": "en",
      "text": "speaking of metrics, the coolest metric",
      "offset": 1937.76,
      "duration": 3.519
    },
    {
      "lang": "en",
      "text": "that that I found and that was",
      "offset": 1939.76,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "recommended to me by a person whose name",
      "offset": 1941.279,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "right now I forgot sadly, but the guy",
      "offset": 1944.159,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "that that that is one one of the main",
      "offset": 1946,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "developers of llama.cpp.",
      "offset": 1947.76,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "uh he's thrown me the metric called KBAK",
      "offset": 1950.24,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "libler. I'm probably mispronouncing this",
      "offset": 1952.72,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "whole thing. And it's essentially what",
      "offset": 1955.2,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "it does is you have a text, you run it",
      "offset": 1957.2,
      "duration": 6.959
    },
    {
      "lang": "en",
      "text": "with the original model, the the the",
      "offset": 1960.48,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "well you see what the model produces,",
      "offset": 1964.159,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "right? Then you have it an approximated",
      "offset": 1966.64,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "model or quantiz model or in my case",
      "offset": 1968.799,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "model with a different effort. You run",
      "offset": 1971.6,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "it again and then you see how this text",
      "offset": 1973.519,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "more or less how similar are those texts",
      "offset": 1976.159,
      "duration": 5.041
    },
    {
      "lang": "en",
      "text": "like simplifying it a lot right and from",
      "offset": 1978.799,
      "duration": 5.921
    },
    {
      "lang": "en",
      "text": "this you can see\n how how var is the",
      "offset": 1981.2,
      "duration": 5.839
    },
    {
      "lang": "en",
      "text": "model in theory it could still produce",
      "offset": 1984.72,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "correct answers if you care about the",
      "offset": 1987.039,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "correctness right but but it would be",
      "offset": 1988.72,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "just different text but in practice it's",
      "offset": 1990.88,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "a good good enough metric and it's very",
      "offset": 1992.399,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "nice\n how's the met metric assessed is",
      "offset": 1994.08,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "just like a simple like Levvenstein or",
      "offset": 1995.919,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "is another inference\n no it's totally",
      "offset": 1997.919,
      "duration": 5.521
    },
    {
      "lang": "en",
      "text": "different so uh again like it's it's",
      "offset": 2000,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "best to to to Google and read about",
      "offset": 2003.44,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "that. But if you if you have a",
      "offset": 2004.96,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "prediction of each token, right, usually",
      "offset": 2006.799,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "it's a list of tokens that you predict,",
      "offset": 2008.88,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "right? Like you have this one vector and",
      "offset": 2010.64,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "you compare you compare uh you compare",
      "offset": 2012.72,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "you measure the tokens against each",
      "offset": 2015.36,
      "duration": 3.039
    },
    {
      "lang": "en",
      "text": "other in some way.\n Yeah. Exactly. So you",
      "offset": 2016.559,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "the whole the whole thing. Yeah. Got it.",
      "offset": 2018.399,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "So So it doesn't just look at this first",
      "offset": 2020.159,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "token but all the tokens that are might",
      "offset": 2022.799,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "be predicted their probabilities.\n So",
      "offset": 2024.64,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "it's a very mathematical approach to",
      "offset": 2027.2,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "Yes. Exactly. And I like that metric",
      "offset": 2028.799,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "because it's super\n super super fair,",
      "offset": 2030.64,
      "duration": 5.519
    },
    {
      "lang": "en",
      "text": "super easy to calculate, right? And and",
      "offset": 2033.44,
      "duration": 5.839
    },
    {
      "lang": "en",
      "text": "uh and it's nice.\n Uh so this is uh this",
      "offset": 2036.159,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "is the thing that I opened here. That's",
      "offset": 2039.279,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "the chart that the developer sent me.",
      "offset": 2041.279,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "That's the metric for oh sorry that's",
      "offset": 2043.36,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "the metric for for for lama 7B and it",
      "offset": 2045.519,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "gets quantized with various",
      "offset": 2048.8,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "quantizations. Right? So 8 six and and",
      "offset": 2050.24,
      "duration": 5.359
    },
    {
      "lang": "en",
      "text": "you can see it uh gets lost,\n right? So",
      "offset": 2052.56,
      "duration": 5.119
    },
    {
      "lang": "en",
      "text": "what I did was I did the same thing with",
      "offset": 2055.599,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "with effort\n and slightly different scale",
      "offset": 2057.679,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "and I know it's hard to compare but I'm",
      "offset": 2060.639,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "sure someone can make a post. The thing",
      "offset": 2062.879,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "is uh what you can see here the most",
      "offset": 2064.879,
      "duration": 5.681
    },
    {
      "lang": "en",
      "text": "important parts are\n well if you looked",
      "offset": 2068.159,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "at the numbers sadly it's not as good as",
      "offset": 2070.56,
      "duration": 5.359
    },
    {
      "lang": "en",
      "text": "as the quantization itself right so",
      "offset": 2073.679,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "hopefully again we can find some some",
      "offset": 2075.919,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "nice use for it also it's a new vector",
      "offset": 2078.159,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "matrix multiplication algorithm by",
      "offset": 2080.48,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "itself so so the uses may be in totally",
      "offset": 2081.839,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "different fields as well\n right uh but uh",
      "offset": 2083.919,
      "duration": 5.121
    },
    {
      "lang": "en",
      "text": "but what we can see here is that the",
      "offset": 2086.8,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "distance is you know 50% is quite close",
      "offset": 2089.04,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "and then it collapses around 20% or",
      "offset": 2091.679,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "something like that and and that's",
      "offset": 2093.919,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "that's one metric. Although for me",
      "offset": 2095.359,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "really like this metric shows nice it's",
      "offset": 2097.68,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "it's it's easy to to see that metric",
      "offset": 2100.24,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "right but then when you care about the",
      "offset": 2102.16,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "quality of the answer that's not the",
      "offset": 2105.28,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "best metric necessarily because what you",
      "offset": 2106.72,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "care about is the whether the answers",
      "offset": 2108.56,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "are are true so so we have a lot of",
      "offset": 2110.24,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "benchmarks right around that\n sadly again",
      "offset": 2112.24,
      "duration": 5.359
    },
    {
      "lang": "en",
      "text": "this this model runs on a MacBook so I",
      "offset": 2115.44,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "didn't when I was first of all I was at",
      "offset": 2117.599,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "the end of the very long marathon when I",
      "offset": 2120.32,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "was doing these benchmarks so I just",
      "offset": 2121.92,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "didn't have the strength to again",
      "offset": 2123.28,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "implement all the benchmarks that are",
      "offset": 2124.8,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "common. Um, and also regular benchmarks",
      "offset": 2127.119,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "take a lot of time when you try to run",
      "offset": 2130.16,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "them on a MacBook. So, I prepared my",
      "offset": 2131.92,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "simp simpler benchmark where it's just,",
      "offset": 2133.76,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "you know, list of questions and answers",
      "offset": 2136,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "that are that the model can get can can",
      "offset": 2137.599,
      "duration": 5.681
    },
    {
      "lang": "en",
      "text": "break. And\n I'm quite sure it will be",
      "offset": 2140.56,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "similar if we run it with the regular",
      "offset": 2143.28,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "benchmarks. Just give me one second. Let",
      "offset": 2145.28,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "me see\n where it is. So, the cool thing",
      "offset": 2147.119,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "is",
      "offset": 2149.599,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "uh the main thing is this is the",
      "offset": 2151.599,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "similarity. Cool. What you can see here",
      "offset": 2154.16,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "and again this is basic basic questions",
      "offset": 2156.32,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "and answers question benchmark right and",
      "offset": 2159.04,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "of course anyone should be skeptical if",
      "offset": 2161.68,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "anyone says oh I built my own benchmark",
      "offset": 2163.28,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "to train my model but sorry like that",
      "offset": 2165.119,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "was the simplest thing to do\n right but",
      "offset": 2167.359,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "what you can see here is really like",
      "offset": 2170.079,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "that it's starts to break around this",
      "offset": 2172.32,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "25% and it loses quality very fast after",
      "offset": 2174.72,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "20%. And that's for 7B model.\n I was",
      "offset": 2177.44,
      "duration": 7.04
    },
    {
      "lang": "en",
      "text": "testing it also on mix that was 70B or",
      "offset": 2181.2,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "something like that.\n And surprisingly,",
      "offset": 2184.48,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "it also worked. And if I remember",
      "offset": 2186.64,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "correctly, it worked even better uh than",
      "offset": 2189.119,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "than this one. Sadly, it's just, you",
      "offset": 2191.52,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "know, I have to manage my own time and",
      "offset": 2193.92,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "resources. So, I cannot I didn't make",
      "offset": 2195.359,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "the chart back then. Later on, it wasn't",
      "offset": 2198,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "working something right. So, so I cannot",
      "offset": 2200.079,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "show you the exact details. The other",
      "offset": 2201.599,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "cool thing that is uh that is there one",
      "offset": 2203.599,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "more benchmark here because this is the",
      "offset": 2206.16,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "output right the main thing that was",
      "offset": 2207.92,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "that is tested and that is quite",
      "offset": 2210.72,
      "duration": 3.119
    },
    {
      "lang": "en",
      "text": "interesting this this chart which is",
      "offset": 2212.24,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "very similar and this is the pure matrix",
      "offset": 2213.839,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "multiplication right so you have matrix",
      "offset": 2216.56,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "of weights you have a vector right and",
      "offset": 2218.48,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "you want to see how the quality how far",
      "offset": 2220.48,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "it is in cosine similarity score from",
      "offset": 2223.04,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "from the original vector just looking at",
      "offset": 2226.16,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "a single weight times times times vector",
      "offset": 2227.92,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "right and what you can see here is that",
      "offset": 2230.56,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "you can really cut down this 20% and the",
      "offset": 2232.64,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "output vector is very close in the in",
      "offset": 2235.119,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "the space to the to the original one and",
      "offset": 2237.2,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "that's where all that other things come",
      "offset": 2239.119,
      "duration": 6.641
    },
    {
      "lang": "en",
      "text": "come from right so so so you can",
      "offset": 2241.04,
      "duration": 7.36
    },
    {
      "lang": "en",
      "text": "and again if you I remember I was doing",
      "offset": 2245.76,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "the test where I just removed some of",
      "offset": 2248.4,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "the weights and if you remove weights",
      "offset": 2250.16,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "and try to get the same result it will",
      "offset": 2252.32,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "collapse very very fast right so it will",
      "offset": 2254.48,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "be closer to here\n right so those are are",
      "offset": 2256.56,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "the benchmarks one more thing that I",
      "offset": 2258.96,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "will show you in terms of benchmarks and",
      "offset": 2261.04,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "you know it's like",
      "offset": 2263.04,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "that was that was one of the interesting",
      "offset": 2265.92,
      "duration": 5.919
    },
    {
      "lang": "en",
      "text": "things uh here and I'm sure someone can",
      "offset": 2268.56,
      "duration": 6.64
    },
    {
      "lang": "en",
      "text": "zoom in and pause when they watch it.",
      "offset": 2271.839,
      "duration": 7.28
    },
    {
      "lang": "en",
      "text": "The question was uh how does it if it",
      "offset": 2275.2,
      "duration": 7.36
    },
    {
      "lang": "en",
      "text": "fails does it fail with time? So is it",
      "offset": 2279.119,
      "duration": 5.521
    },
    {
      "lang": "en",
      "text": "worse the longer the text gets? Because",
      "offset": 2282.56,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "my fear was okay if I start doing this",
      "offset": 2284.64,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "this kind of simplification optimization",
      "offset": 2286.96,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "you know after a thousand tokens it will",
      "offset": 2289.68,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "just the quality will drop down.\n Uh what",
      "offset": 2291.44,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "it shows is more or less that the",
      "offset": 2294.16,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "quality and this shows how far the token",
      "offset": 2296,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "is from the original one.\n It doesn't",
      "offset": 2297.839,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "depend on the distance right? So it",
      "offset": 2299.68,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "doesn't cut down on the it seems it",
      "offset": 2301.28,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "doesn't cut down on the on the context",
      "offset": 2303.04,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "length right and you have here by the",
      "offset": 2304.8,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "way this chart is also interesting",
      "offset": 2308.24,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "because it compares uh one of the",
      "offset": 2309.839,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "iterations of the algorithm with with",
      "offset": 2312,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "quantization right so it shows that okay",
      "offset": 2313.68,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "quantiz algorithm in this token it's way",
      "offset": 2316.48,
      "duration": 5.359
    },
    {
      "lang": "en",
      "text": "different right with my algorithm it's",
      "offset": 2319.92,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "different in this token so it's",
      "offset": 2321.839,
      "duration": 2.961
    },
    {
      "lang": "en",
      "text": "interesting",
      "offset": 2323.2,
      "duration": 6.639
    },
    {
      "lang": "en",
      "text": "view of how this various ways of of",
      "offset": 2324.8,
      "duration": 7.84
    },
    {
      "lang": "en",
      "text": "speeding up the whole process affect uh",
      "offset": 2329.839,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "the tokens that are that are there. So",
      "offset": 2332.64,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "you can see that some of the tokens with",
      "offset": 2334.32,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "quantization you know there quantization",
      "offset": 2336.24,
      "duration": 5.119
    },
    {
      "lang": "en",
      "text": "has problems some of the tokens my",
      "offset": 2338.8,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "algorithm has problems and it's not one",
      "offset": 2341.359,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "to one right so so it's interesting",
      "offset": 2343.119,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "again point of view for the research and",
      "offset": 2344.72,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "the one last thing and I'm surprised",
      "offset": 2347.119,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "that no one actually did that for this",
      "offset": 2348.88,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "view for for for regular quantization",
      "offset": 2351.2,
      "duration": 6.879
    },
    {
      "lang": "en",
      "text": "let me zoom in slightly so so",
      "offset": 2354.4,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "what I did is I don't think anyone",
      "offset": 2358.079,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "created this heat map so when you have a",
      "offset": 2360.88,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "metric of how",
      "offset": 2362.96,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "How far away is the model? What you can",
      "offset": 2364.32,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "do is you can make a heat map.\n So we",
      "offset": 2366.4,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "have an article here. So like some",
      "offset": 2368.72,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "random article about birds, right? And",
      "offset": 2370.64,
      "duration": 8.719
    },
    {
      "lang": "en",
      "text": "on top of it is where my effort my well",
      "offset": 2373.599,
      "duration": 8.961
    },
    {
      "lang": "en",
      "text": "with my with my al algorithm when when",
      "offset": 2379.359,
      "duration": 6.321
    },
    {
      "lang": "en",
      "text": "you get lower in effort where the words",
      "offset": 2382.56,
      "duration": 5.279
    },
    {
      "lang": "en",
      "text": "start to diverge, right? So you can see",
      "offset": 2385.68,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "here if it's white then it's like the",
      "offset": 2387.839,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "same, right? The would make the same",
      "offset": 2389.92,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "prediction. If it's red, there's a huge",
      "offset": 2392.32,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "divergence. It's an interesting way of",
      "offset": 2394.88,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "looking at at things. And I don't think",
      "offset": 2396.72,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "people even do that with quantization.",
      "offset": 2398.56,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "So, so I think it might be interesting",
      "offset": 2400.4,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "if if we start to look at models from",
      "offset": 2402.72,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "this perspective and then say, oh, like",
      "offset": 2404.72,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "you know what's what's here, right? Like",
      "offset": 2407.04,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "it goes back to your your your measure",
      "offset": 2409.28,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "of of difference on the being very",
      "offset": 2411.2,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "mathematical in the first visualizing it",
      "offset": 2413.119,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "this way is useful.\n Yeah. Yeah. So, so",
      "offset": 2415.76,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "yeah. So there are a few things that are",
      "offset": 2417.599,
      "duration": 5.041
    },
    {
      "lang": "en",
      "text": "interesting from the perspective of uh",
      "offset": 2420.4,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "of research, right?\n There might be one",
      "offset": 2422.64,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "takeaway for a lot of people who are",
      "offset": 2424.48,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "building apps is who are doing evals for",
      "offset": 2426.16,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "just kind of regular inference",
      "offset": 2428.32,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "applications, right? Is to build a heat",
      "offset": 2430.56,
      "duration": 3.279
    },
    {
      "lang": "en",
      "text": "map for your evals like you could do",
      "offset": 2431.92,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "that for another domain doesn't have to",
      "offset": 2433.839,
      "duration": 2.881
    },
    {
      "lang": "en",
      "text": "be for this problem.\n That's a great",
      "offset": 2434.96,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "idea.\n So more or less that's it. Like",
      "offset": 2436.72,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "like I said, sadly, you know,",
      "offset": 2439.2,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "quantization and it's one of the things",
      "offset": 2440.8,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "about scientific honesty, you have to",
      "offset": 2442.8,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "say, you know, if something is better,",
      "offset": 2444.4,
      "duration": 2.56
    },
    {
      "lang": "en",
      "text": "then you have to say that it's better,",
      "offset": 2445.68,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "right? So, so quantization is better or",
      "offset": 2446.96,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "at least it's, you know, perhaps I could",
      "offset": 2448.88,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "get this algorithm to be as good,\n but it",
      "offset": 2450.96,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "opens up interesting fields of research",
      "offset": 2453.28,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "that I think aren't that much explored",
      "offset": 2455.119,
      "duration": 6.401
    },
    {
      "lang": "en",
      "text": "and again what models do\n all in all is",
      "offset": 2457.44,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "still an open question in many cases,",
      "offset": 2461.52,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "right? And any new lens that we have is",
      "offset": 2463.2,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "maybe interesting.\n Well, okay, before we",
      "offset": 2466.64,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "wrap, I want to touch on you, you and",
      "offset": 2468.48,
      "duration": 3.119
    },
    {
      "lang": "en",
      "text": "the community a little bit more. You",
      "offset": 2470.16,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "know, again, Arur said like the OG",
      "offset": 2471.599,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "tinkerer is Thomas and you know, you've",
      "offset": 2473.76,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "clearly had a long career here um and",
      "offset": 2476.319,
      "duration": 3.121
    },
    {
      "lang": "en",
      "text": "done lots of interesting things. We",
      "offset": 2478.319,
      "duration": 2.961
    },
    {
      "lang": "en",
      "text": "didn't touch on many of them actually,",
      "offset": 2479.44,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "but I do want to touch on a couple of",
      "offset": 2481.28,
      "duration": 2.799
    },
    {
      "lang": "en",
      "text": "them and and also just how small the",
      "offset": 2482.56,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "community is, too. I think when I got",
      "offset": 2484.079,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "here, you said, you know, I said um I",
      "offset": 2485.44,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "was just with the 11 Labs founders in",
      "offset": 2487.44,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "London and they're from Poland, of",
      "offset": 2488.96,
      "duration": 3.119
    },
    {
      "lang": "en",
      "text": "course, and you pointed out the windows.",
      "offset": 2490.48,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "Their office is right there.\n I think so.",
      "offset": 2492.079,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "I think so. Yeah. They told me that that",
      "offset": 2493.52,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "it's in there. Yeah. Yeah. You know, so",
      "offset": 2495.28,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "that's a small a small world. I love it",
      "offset": 2498.24,
      "duration": 2.879
    },
    {
      "lang": "en",
      "text": "when you when you come to a place and",
      "offset": 2499.92,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "meet people and it feels small all of a",
      "offset": 2501.119,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "sudden and that's great. Um speaking of",
      "offset": 2503.839,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "that like meeting people uh getting",
      "offset": 2505.839,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "together you started a kind of incubator",
      "offset": 2508,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "garage or hacker house kind of a",
      "offset": 2510.72,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "concept.\n Oh yeah that was uh that was",
      "offset": 2512.48,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "fun. Uh we rented a you know after so",
      "offset": 2514.4,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "many years in software.\n Yeah.\n One day",
      "offset": 2517.359,
      "duration": 3.521
    },
    {
      "lang": "en",
      "text": "you wake up and you're tempted to do",
      "offset": 2519.28,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "something big and something loud and",
      "offset": 2520.88,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "weld something or or do do things like",
      "offset": 2522.72,
      "duration": 6.879
    },
    {
      "lang": "en",
      "text": "this\n and uh really Yeah. Free space",
      "offset": 2525.2,
      "duration": 6.639
    },
    {
      "lang": "en",
      "text": "work.\n Free space. Exactly. So, so me and",
      "offset": 2529.599,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "my friend were looking for a year for a",
      "offset": 2531.839,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "place that would be close to the city",
      "offset": 2533.599,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "center where we could just uh do cool",
      "offset": 2535.119,
      "duration": 6.881
    },
    {
      "lang": "en",
      "text": "stuff, build cool stuff. And an idea for",
      "offset": 2538,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "a while was to to have an accelerator.",
      "offset": 2542,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "You have photos of it. Yeah, actually I",
      "offset": 2543.92,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "have it.\n Yeah. Cool.\n Yeah. I don't have",
      "offset": 2545.359,
      "duration": 2.881
    },
    {
      "lang": "en",
      "text": "too many photos and of course photos",
      "offset": 2546.64,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "don't really do it justice, but people",
      "offset": 2548.24,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "that's that's my",
      "offset": 2551.28,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "So, it used to be actually a car chop",
      "offset": 2553.44,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "shop. Okay. So we found a lot of license",
      "offset": 2555.359,
      "duration": 6.321
    },
    {
      "lang": "en",
      "text": "plates of cars that were that ended",
      "offset": 2559.2,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "their life here,\n right? So so we we",
      "offset": 2561.68,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "clean it up and and we have like this is",
      "offset": 2564.96,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "a small part of this whole place and we",
      "offset": 2567.44,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "have a place where we can do various",
      "offset": 2569.2,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "cool things. One of the ideas was that I",
      "offset": 2570.96,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "wanted to venture builder related to",
      "offset": 2573.76,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "climate tech um here in Poland. But one",
      "offset": 2575.28,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "thing is climate tech is not very big in",
      "offset": 2577.76,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "Poland sadly.\n Second thing is AI is way",
      "offset": 2579.359,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "just way more interesting right now. So,",
      "offset": 2581.839,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "so I I decided to focus full time on AI.",
      "offset": 2583.68,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "And you were at the very first AI",
      "offset": 2586.64,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "tinkerers meet up.\n Oh, yeah.\n Just one",
      "offset": 2588.4,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "more thing. Uh because of my background",
      "offset": 2590.4,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "in uh in in startups, I love having",
      "offset": 2592.319,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "startups there and right now the place",
      "offset": 2594.88,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "serves as a as a hub as a meeting place",
      "offset": 2596.8,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "for a lot of startups and we're doing",
      "offset": 2599.76,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "we're doing meetups there and it's a",
      "offset": 2601.76,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "nice atmosphere and what people say is",
      "offset": 2603.68,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "oh yeah,\n Google campus is nice like it's",
      "offset": 2605.44,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "nice to have these offices and",
      "offset": 2608.16,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "everything, right? But when it comes to",
      "offset": 2609.599,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "this garage atmosphere that at least",
      "offset": 2611.359,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "people hear from, you know,\n it's more",
      "offset": 2613.839,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "appropriate for So this is this is the",
      "offset": 2615.599,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "place where, you know, you can drill a",
      "offset": 2617.52,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "hole in the wall,\n make stuff, build",
      "offset": 2619.44,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "stuff.\n Exactly. So So I'm not objective,",
      "offset": 2621.68,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "but people tend to say that it's super",
      "offset": 2624.319,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "inspirational to to be there. Yeah,",
      "offset": 2625.92,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "that's really cool.\n That's nice.\n And you",
      "offset": 2628.16,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "have the robot. Um\n Oh, yeah. The robot",
      "offset": 2630.319,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "the robot is there. But that's that's a",
      "offset": 2632.079,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "thing for another story.\n So what was the",
      "offset": 2634.88,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "story there? You just decided like you",
      "offset": 2636.56,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "need to go buy a uni tree robot and",
      "offset": 2638,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "start hacking.\n The thing is my story I",
      "offset": 2639.44,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "think what I'm good at is trying to hit",
      "offset": 2642.079,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "technologies trends way before they",
      "offset": 2644.48,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "happen. With AI I feel very weird",
      "offset": 2646.8,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "because I'm kind of not late to the game",
      "offset": 2648.8,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "but just I arrived with everyone else.",
      "offset": 2651.76,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "Of course I was interested in AI during",
      "offset": 2653.599,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "my computer science and and\n Well, we",
      "offset": 2655.92,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "didn't talk about it but you did the",
      "offset": 2658.079,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "Bitcoin uh yeah like but I left the the",
      "offset": 2659.04,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "crypto community.\n You weren't a crypto",
      "offset": 2662.56,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "bro. That's\n I wasn't a crypto bro. I was",
      "offset": 2663.76,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "the tech crypto person and the",
      "offset": 2666.16,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "technology there is quite nice. Uh I",
      "offset": 2668.4,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "built some cool stuff and and some",
      "offset": 2670.079,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "amazing people that I've met there.\n Um",
      "offset": 2672,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "but uh but yeah with AI I built chatbot",
      "offset": 2674.48,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "I tried to set up a chatbot uh\n startup",
      "offset": 2677.52,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "in 2007 actually. So so that's been way",
      "offset": 2680.96,
      "duration": 5.119
    },
    {
      "lang": "en",
      "text": "way before before its time and again",
      "offset": 2683.44,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "when it comes when there is a new",
      "offset": 2686.079,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "technology I try to figure out what's it",
      "offset": 2687.599,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "about. And the robot that you see in the",
      "offset": 2689.839,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "background, yeah, it's a Uni3 robot and",
      "offset": 2691.599,
      "duration": 5.041
    },
    {
      "lang": "en",
      "text": "they're super cheap right now. €2,000",
      "offset": 2694.079,
      "duration": 5.121
    },
    {
      "lang": "en",
      "text": "that is that's extremely cheap\n and and",
      "offset": 2696.64,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "it's nice to see what they are about,",
      "offset": 2699.2,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "how far it is.\n How hackable is it? Like",
      "offset": 2700.8,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "what can you do? What's\n Well, that's",
      "offset": 2702.64,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "that's the guys that were on hackathon",
      "offset": 2704,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "that I that I borrowed them, but I think",
      "offset": 2705.76,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "it is it is hackable. Uh I would love to",
      "offset": 2707.839,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "for them to they have this other version",
      "offset": 2711.04,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "that is educational that is like that",
      "offset": 2713.04,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "has an open SDK and has some accessories",
      "offset": 2715.359,
      "duration": 5.601
    },
    {
      "lang": "en",
      "text": "but that that version is is way better.",
      "offset": 2718.24,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "I would love them to give access to that",
      "offset": 2720.96,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "version. But yeah by itself this robot",
      "offset": 2722.96,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "it's a nice piece of technology. So I if",
      "offset": 2725.04,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "if you have if you can afford it I would",
      "offset": 2727.52,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "say go for it especially before the",
      "offset": 2729.359,
      "duration": 5.441
    },
    {
      "lang": "en",
      "text": "tariffs fit\n very much. But speaking of a",
      "offset": 2731.28,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "tinkerers, if I can add one more thing",
      "offset": 2734.8,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "and a word of p praise towards a",
      "offset": 2736.72,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "tinkerers,\n you know, um we have a lot of",
      "offset": 2739.119,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "communities here, AI communities, what",
      "offset": 2741.599,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "not startup communities\n and also",
      "offset": 2743.599,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "speaking of this algorithm, I spoke",
      "offset": 2745.839,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "about it with many people super",
      "offset": 2747.839,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "technical\n AI tinkerers was the only",
      "offset": 2749.44,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "place where I gave a fiveminut",
      "offset": 2752.56,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "presentation and the questions I got",
      "offset": 2754.56,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "from the room were questions that showed",
      "offset": 2756.48,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "such a deep understanding that I didn't",
      "offset": 2758.8,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "have these questions after an hour of",
      "offset": 2760.48,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "talk with a fellow computer scientist",
      "offset": 2762.24,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "and I was so amazed by the quality of",
      "offset": 2764.64,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "the the people and the discussion",
      "offset": 2766.319,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "afterwards that was that was something",
      "offset": 2767.92,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "amazing and that was uh great.\n Yeah, we",
      "offset": 2770,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "have a lot of technical cool technical",
      "offset": 2772.319,
      "duration": 3.121
    },
    {
      "lang": "en",
      "text": "people here in Warso and you managed to",
      "offset": 2773.76,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "to grab all of them and and invite them",
      "offset": 2775.44,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "to a tinkerers. So I respect that.",
      "offset": 2777.44,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "Thanks. Uh be sure if you're on the road",
      "offset": 2779.68,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "and you're in another city stop by one",
      "offset": 2781.359,
      "duration": 2.881
    },
    {
      "lang": "en",
      "text": "of the chapters we had, you know, it's",
      "offset": 2782.96,
      "duration": 2.639
    },
    {
      "lang": "en",
      "text": "funny at the at the hackathon we just",
      "offset": 2784.24,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "had here in Warsaw.\n Um I went through",
      "offset": 2785.599,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "the data and there were people from\n AI",
      "offset": 2787.839,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "tinkers chapters around the world. Yeah,",
      "offset": 2790.079,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "we had somebody from Koala Lur, Medí,",
      "offset": 2791.68,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "Colombia, of course, all over Europe.",
      "offset": 2794.24,
      "duration": 5.119
    },
    {
      "lang": "en",
      "text": "That was exciting to see.\n That is also",
      "offset": 2796.88,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "one thing that I was really missing this",
      "offset": 2799.359,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "this community aspect of of AI. And",
      "offset": 2801.119,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "again, getting the cool thing about the",
      "offset": 2804.079,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "blockchain community is that it's so",
      "offset": 2806.079,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "well connected and it has hackathons",
      "offset": 2807.52,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "every single week happening somewhere",
      "offset": 2809.52,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "else,\n right? And people meet in person.",
      "offset": 2811.119,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "And of course, it's easier because the",
      "offset": 2813.359,
      "duration": 3.281
    },
    {
      "lang": "en",
      "text": "budgets are are better and and whatnot",
      "offset": 2814.56,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "over there than with some AI projects.",
      "offset": 2816.64,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "They have the money, but now they have",
      "offset": 2818.88,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "the applications. Yeah. Yeah. So the",
      "offset": 2819.92,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "applications real world applications are",
      "offset": 2821.68,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "missing a little bit.\n The applications.",
      "offset": 2823.04,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "Yeah. But uh but now the community, you",
      "offset": 2824.72,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "know, when I left the community, I was",
      "offset": 2827.359,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "really missing that that human aspect.",
      "offset": 2828.56,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "And of course there are slacks, there",
      "offset": 2830.56,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "are discords,\n but uh I think Tinker is",
      "offset": 2831.92,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "this one missing community that that",
      "offset": 2835.28,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "thing that I was really missing from the",
      "offset": 2837.44,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "from the AI community. So I'm very",
      "offset": 2838.8,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "grateful for for doing the whole thing.",
      "offset": 2840.72,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "Yeah. Art Andreas and everyone here has",
      "offset": 2843.599,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "done a great job and it's it's a very",
      "offset": 2845.52,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "fun fun thing.\n Yeah, definitely.\n Thank",
      "offset": 2846.96,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "you. Well, where can people find out",
      "offset": 2849.2,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "about more of your work? I'm sure the",
      "offset": 2851.2,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "GitHub URLs or any other thing you'd",
      "offset": 2852.88,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "like.\n Yeah, GitHub GitHub URL is is one",
      "offset": 2854.24,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "thing. Right now, I'm on a sabbatical.",
      "offset": 2856.24,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "So, so I do totally different.\n What is",
      "offset": 2858,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "next?\n Uh, right now for me, dancing,",
      "offset": 2860.16,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "right? I'm focusing full time, but I",
      "offset": 2862.24,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "think I will, you know, it's hard to be",
      "offset": 2864.16,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "on a sabbatical during such an exciting",
      "offset": 2865.68,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "times, right? So, I'm already",
      "offset": 2868,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "brainstorming some of the ideas.\n I don't",
      "offset": 2869.2,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "know personally, perhaps I will go",
      "offset": 2871.68,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "deeper into research or I will try to do",
      "offset": 2873.52,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "a new project, startup. We'll see. We'll",
      "offset": 2875.44,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "see. But uh I would say if you want to",
      "offset": 2878.24,
      "duration": 5.359
    },
    {
      "lang": "en",
      "text": "follow me, go on to Twitter. Kolinko is",
      "offset": 2880.4,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "is my is my handle there.\n I don't tweet",
      "offset": 2883.599,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "that much, but when I start a new thing,",
      "offset": 2886.48,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "I will. Yeah, there will be over there.",
      "offset": 2888.319,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "We'll follow along and we'll check in on",
      "offset": 2890.96,
      "duration": 2.48
    },
    {
      "lang": "en",
      "text": "you. This is great to meet you. Thank",
      "offset": 2892.16,
      "duration": 2.32
    },
    {
      "lang": "en",
      "text": "you so much for your time.\n Thanks.",
      "offset": 2893.44,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "Thanks for the space. Take it.",
      "offset": 2894.48,
      "duration": 3.359
    }
  ],
  "cleanText": "Usually what I noticed is around 50, you\ndon't see really a change in the output.\nSo we can remove, and that's interesting\nfrom the research point of view. You can\nremove like 50% of calculations.\nWhen I\nwas in Warsaw for the OpenAI AI\nTinkerers hackathon, I asked several\npeople who I should meet. Many said\nTomasz Kolinko. He's a true OG tinkerer.\nSo I was not surprised when I went to\nhis apartment and saw the same unitry\nrobot that one of the teams was hacking\non earlier that weekend. In this video,\nwe talk about a tinkerer that is not\nafraid to dive into the guts of models\nand pull out the math to play with\nthings in search of optimizations and\nabilities that are not being handed to\nyou by the model providers themselves.\nIt was incredibly scientifically honest\nto just see someone who could go so\ndeeply into technical questioning of\ntheir own curiosity and playing with the\ncore models. Like, it's okay. You can do\nthat. So, I think this video is really\ninspiring for that reason. I think\nyou're going to love it. Let's go.\n[Music]\nWelcome to AI Tinkerers One-Shot. This is\nour global stage where we invite people\nwho are doing something innovative in\nthe community that deserves an\nextra look or a deeper dive under the\nhood. And today I am here with Tomasz\nKolinko. Um, the story of how we met.\nWe're here in Warsaw is we just did a\nbig hackathon with OpenAI in Warsaw\nwhere almost a thousand people\nregistered and we selected like 200 and\nafter it was all said and done I asked\nArur like, hey, when I'm when I have a\ncouple extra days here and I want to\nrecord somebody on, on you know, who\nshould it be, and he immediately said\nyou. So welcome to the show.\nThank you.\nThanks for having me.\nUm, so tell us a\nlittle bit about yourself.\nUh, I'm an\nentrepreneur. I've been doing various\nprojects since the last 20 years and I'm\na member of the local communities. I\nlike to think so and and I like to\nsupport other projects and other younger\npeople than me trying to set up their\nown things.\nUh, when it comes to AI, I\nwas thinking, you know, what can I do to\nreally understand how the technology is\nworking, right? And I like to dig really\nreally deep into AI\nand I said, hm,\ncan I optimize somehow all the\ncomputations, all the all the things\nthat are being done there? And uh and\nthat's where Effort Engine came. Came\nEffort Engine. So this is your your\nproject. Tell us more. It's it's an\nalgorithm. This is not, you know, we see a\nlot of people who tinkering on like the\napplication side or doing generating\nmodels, and this is neither really. This\nis something else.\nYeah. Well, it started\nas an actual application. And I wanted\nto do something that rel that was\nrelated to uh RA and to search. I wanted\nto have something that would allow me to\nsearch through the data using Large Language Models more\nefficiently\nbut then I noticed that some\nof the algorithms are really maybe\nperhaps maybe done better in a better\nway\nright? And I and I dug into that and\nwhat else can I tell you here, sorry, and\nis there something you could show us\nlike does\nuh, so I will show it to in just just a\nmoment and\nuh, just a few words about\nwhat it is. Uh, so regularly when you do\nan inference of AI using AI model, uh,\nit's all very static, right? So, so you can\ndo, you know, you have a model that is, I\ndon't know, 70 billion parameters or 7\nbillion parameters or whatever, and\nthat's that's it, right? You have to do\nthis 7 billion multiplications or\nwhatever, and there is no way to to\noptimize that really, right? You can cut\ndown statically the model, right? So you\ncan just cut it down and and uh, distill\nit and make it smaller, right? You can\nquantize it. So, so you really remove\nsome of the accuracy of of the model,\nbut uh, the computations are\ncomputations. It's very static, right?\nSo, you have the model, you have to do\neverything like that.\nSo, what I did was\nI developed a new algorithm that allows\nyou to dynamically choose which\ncalculations have been done actually.\nOkay?\nAnd I managed to do that in a way\nthat is efficient. So, just as efficient\non GPUs as as a regular thing. So, so,\nso, you know, it's not a, it's easy to\ncreate a new algorithm uh, that that\nwould do the multiplications on a CPU,\nright? But if you if it goes into into\nthe distributed computing, distributed\ndistributed computing, it it's getting\nhard, right?\nSo anyway, uh, the algorithm\nwhat it allows you to do is\nit's\ninference side algorithm.\nIt's exactly\nit's inference side. Okay.\nSo what what\nit allows you to do is uh, to have this\nuh, choice of how precise do you want to\nbe the inference to be from 100%, which\nis essentially you do all the\ncomputations, until I don't know, 10% or\n5%. Right? So you can do that\ndynamically, unlike, I don't know, uh,\nquantization where you have to do it, you\nknow, do the pre-process processing and\nand then, you know, you have a new model\nhere, you can do it even token by token\nif you if you wanted to, right? And that\nis something new that that people\nhaven't seen before. Sure. And the cool\nthing is, uh, you know, it's uh, I will get\na little bit technical here.\nUh, inside\nof the when you're doing inference, you\nhave matrix multiplication, right? Or\nvector by matrix multiplication. And the\ncommon wisdom was that you have to do\nthe whole thing, right? So you have to\nmultiply the whole matrix. If you want to\nperhaps you can multiply like every\nother row or something like that, like\nbut but it needs to be very in a\nspecific uh, order all all the\ncomputations, right? Otherwise you will\nsuffer a lot of penalties, you will\nsuffer penalty in speed\nand what I\nmanaged to do is create this algorithm\nwhere if it's 100%, it is slower than\njust regular algorithm slightly, right? So\n100%, it's slower.\n50% computations. That's\nroughly break even.\nAll right. So, you\nhave this an optimization that's also\nvery flexible. You can you can dial it\nwhere you how much you want to do. It's\nlossy, I'm assuming.\nYeah. Yeah. Of\ncourse,\nobviously. And then it's on the\ninference side. How how does it work? I\nmean, I might not have the math, but\nsome of our listeners probably do. But\nwhat is the math?\nI will I will Okay.\nWhen it comes to math, uh, um, before I\nget to that, I will tell you, yeah, it's\nlossy. Uh, the cool thing that I\ndiscovered is that it's it's\nsurprisingly, it holds up surprisingly\nwell. So I can cut down to, I don't know,\n20% of all the multiplications. I can\nremove them\nand it's uh, and it still Large Language Model\nproduces still a decent output, and that\nis that is amazing if you think about it,\nright?\nNow how it works, uh, it there is a\npaper that I published, so, so, so readers\ncan go into the details,\nbut uh, long\nstory short, what what I do is what\nalgorithm does is, you know, if you think\nabout uh, the matrix, right? And the vector\nthat you want to multiply, uh, right now\nall the regular algorithms, you just have\nto multiply, you know, one by one\neverything, right?\nUh, what I do is, and\nokay, the way to explain it is a little\nbit uh, reversed.\nIf you think about all\nthe multiplications that you need to do,\nI don't know, all the 16 million\nmultiplications if you have 4,000 by\n4,000, right?\nIf you imagine all the\nmultiplications and if you sorted them\nuh, and you only selected the ones that\nhave the biggest output, the the\nmultiplications and only use those,\nthat's that's that's that would be that\nwould be more or less how how the\nalgorithm would work. So, so\nessentially choosing the the\nmultiplications that are that will have\nthe biggest output and the biggest\ninfluence on the resulting vector that\nthat goes into it. I don't know if I\nexplained it that maybe perhaps\nso uh,\nhow much of an efficiency gain are we\ntalking about, like, and how do you\nmeasure that? Is it tokens per second? Is\nthat\nyeah, so there is tokens per second\nand I can show you the demo uh, in a\nmoment. Okay. So the cool thing again\nabout the algorithm is that it's\ndynamic, right? It is something new\nbecause you can you can never before you\ncould have really like benefited this\nthis way uh, when it comes to\nmultiplications.\nUh, but uh,\nhow much\nfaster are we talking?\nYeah. So, so it\ncan get the definition of speed is a\nlittle bit tricky, right? And uh, because\nuh, the question is not how much faster\nbecause I can do it 5% multiplications,\nright? But there is like you said a loss\nin quality, right? So, so the question\nis how much faster per the loss of\nquality and how do you compare it to the\nfrontier of how do you compare it to\nright? And the sad news here is that it's\nslightly worse than the quantization in\nthe end, right? So if you use quantization\nfrom 16 bits to, I don't know, 8 bits or\nfour bits, you will get twice or four\ntimes the speed, right? Uh, this algorithm\ncan get uh, twice as fast, perhaps three\ntimes as fast, but the loss in quality\nwill be slightly more. Okay. So what are\nthe cases where this is better then?\nSo\nso the cases where where it's better\nright now, it's uh, you know, this came up\nafter after the premiere. So, so that\nthat was that was the thing.\nThe cases\nwhere it's better\nI don't yet know\nsadly. So, so it's, you know, I would love\nto tell you, oh, this is the amazing thing,\nbut but you know, the algorithm was\ninvented a year ago. If it was if it had\nlike very solid cases\nthen then it\nalready would be implemented by\neveryone. So it's not the case. We didn't\nfind\nit, but I can show you some of the cases\nwhere it might be better, but also it's\nsuper interesting for the point of view\nof research\nbecause uh, because uh,\nbecause we get a different ch, different\nway of approximating the same Large Language Model and we\ncan see how, you know, how various\nimplementations differ.\nAlso, it's uh, this\nthis ability to manage uh, the quality of\nuh, of output, we can do it again\ndynamically, but also layer by layer.\nYeah,\nwe can change it very fast and we\ncan\nhave time to analyze in depth,\nyou know, we can do like a thousand\nexperiments, you know, managing various\nlayers, you know, how how we manage the\nquality on each layer and see really\nwhich layers matter in which context,\nright? So, there is a lot of potential\nthere for for for the upcoming for the\nupcoming research. M, that's where my\nmind was going was something like\nbecause you're tunable and I was going\nto ask because it's not quantized a\nmodel that you're just you're running\nthat or the other original, it's one\nmodel still, so that you could sort of\nintuitively think about doing something\nlike\nI don't know, running in parallel to\nget a quick answer to these or now is\nthe\nif I run the model using your\nalgorithm at full 100% and I ran it\nagain at like 20% or something, do I end\nup in a similar destination in the\nlatent face.\nUm, I think\ndo you know what\nI'm saying?\nWell, you end up because the\nthe output is very similar, right? So, so\nyou end up. Yeah. Yeah. So, so essentially\nyou can do all the tricks that you would\ndo be able to do with with\nso there\nmight be an application where, you know,\nyou're able to like in real time\ndepending on how fast it is, get get the\nuser an approximate answer that sort of\nsatisfies them while coming back.\nSo\nthat's one thing. Another thing there\nthink about voice applications where you\nneed to respond quickly.\nThat is that is\nnice. Another thing is uh, if we were\nable to predict which tokens are really\nthe significant ones, like where can we\nget down with with uh, with the\napproximation, when where can we use the\nlower approximation, where, you know, if\nyou think about how the words are\nconstructed or how the whole sentences,\nusually it's just one or two key words\nthat really matter, where you really have\nto focus and say, oh, this is, you know,\nhere we want to get inference right, and\nwe with the others, it's like, oh, you know,\nwe can skip over it, right? Right?\nAnd\nwith the regular quantization, you had\nto load the whole like this version of\nthe model, this version of the model,\nlike all the versions of the model.\nRight? Here you, you can load it just\njust one and and manage it actively.\nRight? So that's one thing.\nOh, the cool\nthing and the side effect of the whole\nthing. So one thing is the algorithm, of\ncourse, the data structure behind it is\nis totally different when it comes to\nhow how I store uh, the the the weights\nin\nthe data structure is different.\nYeah.\nTurns out that this data structure is is\nsuper interesting because you can\ndynamically also during during the load\ntime, you can choose to skip some of the\nweights that are the least important.\nInteresting.\nSo, you know, usually you\nhave you have like, I don't know, 70\nbillion model parameter model, right?\nSo, so you have to load all or nothing,\nright?\nY\neither it fits in the VRAM or\nit doesn't fit. Here you can say, you\nknow, like I almost have it, just load top\n90% and skip all the all the all the all\nthe smallest weights,\ninteresting\nand it\nworks smoothly and it's\ncan you pick and\nchoose which ways experimentally\nbecause\nthe uh, the way uh, the algorithm works\nand the data structure works,\nuh, the\nweights are sorted in the in the in the\narray, right? In the matrix. Uh, so uh, so\nthey're sorted from the the the biggest\nones to the smallest ones, right?\nSo it\nit makes, you know, you can just float a\npart of it and that's and you're done.\nYou don't need the whole of it. But then\nyou cannot really pick and choose\nactually, perhaps you could, I, I didn't\nthink through that, but the easiest thing\nis you just cut off the the lower\nweights. And now speaking of lowering\nthe the computations, what\nuh, you know, people say, oh, like we can, if\nwe have weights, you know, anyone that has\nplayed with with with optimizing the\nmodels says, like, the first thing you try\nis you just cut off the lowest weight,\nright? And then like, you know, cut off 5%\nthose 0.0 Z, yeah,\nsure,\nbut the model, the\nquality of model drops down very, very\nfast after, I don't know, removing 20% or\n30% of weights, right?\nHere, that's uh,\nworth making clear, this algorithm\ndoesn't, it uses all the weights, like all\nthe loaded weights, so, so again, you can\nskip 5% or 10%, but but all the things\nthat are loaded are being used, just not\non every inference step. So let's say\nthat you have a token and then uh, you\nknow, in this certain token, the the input\nvector\nyou will want to use this, this, this, and\nthis weights to multiply, right? Because\nbecause of the when you multiply it,\nthose give you the highest result for\nfor some other token, the the input\nvector is different, right? And you might\nwant to use different different weights,\nokay, right? So it's totally dynamic and\nit uses the whole of the network.\nSo I\nwould say, yeah, that's that's another\ninteresting thing about the algorithm\nand the appro uh, the approach, right?\nBecause if you think about quantization,\nlike you remove some\n\n\n of the data from the network to quantize it. Here we actually use all the data. What is the memory impact of using all the data though? Are you always not saving there?\n So actually that's a cool thing. It uses just as much of memory as the regular model essentially, like plus or minus, I don't know, 1% or something like that. The downside, second downside that is there, and again, I was working on that algorithm some time ago, and I had to choose which directions I go.\n It is, uh, right now the first implementations is only in 16 bit, so, so that's the weights, that's how they are loaded in. It can get quantized, and I actually prepared a Q4, I think, version or Q8, I don't remember, uh, but it's not as straightforward as with the regular algorithm. Because with regular inference, what you do is you just, you know, instead of 16 bits, you have eight bits or whatever bits, right? And and and you have just a smaller array. Here you have to tweak the algorithm a little bit to to to support that. I don't, you know, it's all in the paper, so I don't want to get too too deep into that, but\n I'd love to, I'd love to just you pull the paper up and show people so they know they go.\n I want, I want to show you also the demo of the whole thing. Let's, let's go for it.\n Yeah. And it'd be cool. I don't know if you have time to show or if you're able to show how you approached benchmarking it, you know, because that's a big part of the research.\n Yeah, I will show you.\n Yeah, exactly that. So, yeah, right now it's loading. Uh, just the software, right?\n Mhm.\n Initially, the, by the way, the first implementation, uh, I built was on a on a MacBook. So, so it runs on M1 processors.\n Cool. I should really finally find time to make it run on Nvidia. It's not 100% certain that that it will actually work as well on Nvidia. So, so I cannot promise it here, but it works very nicely on MacBooks.\n So, anyway, the first thing that that that happens is it just forwards some text, right? So, so there's an initial text saying, how are you feeling today? By the way, you don't see my video.\n I don't see your screen.\n Yeah.\n Okay. You don't see my screen. So, I will just\n Okay, thanks.\n Show it to you.\n I appreciate that.\n I'll show it to you. Right. So the thing, first thing that you see is you see here, like initial text and how are you feeling today, and it's, it's just completing the text like any model does. The reason I have it here, and that's a funny story, is that it's, uh, I noticed that when I lower the quality, usually the text gets sadder and sadder for this LLM. I don't know. Yeah. But, but it's, you know, I need to do some research. Those are the kind of quirks that that come up, and that was, you know, I don't know if it's scientifically proven, but, but yeah, like that's exactly like me. I don't get my coffee, like I will get groggy, right? So this,\n that's funny.\n Yeah, I remember the first one of the first things that I tried was what is the meaning of life, right? And the 100% model said, like, oh, it's like to give to the people, whatever, right? And the and the and the dumbest, like when I cut down the quality, yeah, the lowest quality said, oh, the meaning of life is to live and to\n They're all going to be the AI is really upset with us if we don't feed it nuclear energy.\n Exactly.\n So, uh, so now, uh, what's, uh, that's the loading screen, right? And it starts with 100% accuracy or or effort.\n Um, the reason, by the way, I use the name effort is that I didn't find a name for this metric, right? Because with quantization, there is like present quantization, right?\nSure. Uh, we have, uh, how was it called in with, uh, with, uh, when there is a matrix and not all the, uh, weights are populated. I forgot the the English word, but there is, there is this other thing that is that is there, right? So there's\n I think it's, it's important to to recognize that you're, you're innovating in the benchmark arena.\n Yeah, so, so it was interesting because, uh, the question is, how do you, you know, call this new metric? And actually came up with the name effort, right? Which is, which is like this metric of how much effort does you want to for a given result.\n Yeah, exactly. So let me type in something, example, and I don't remember which model actually this was, either Mistral or Llama. All right, let's ask it what it thinks about McDonald's for breakfast.\nOkay. Uh, right. So, so the regular model, and this is running at 100% then.\n Yeah, it's 100%. Right. So, like I told you, at 100%, I would say it's 50% or like twice as slow as.\n And what actual model you using for this, the base? I would love to remember because I programmed it a year ago. So, so I think it's either Mistral or Llama. I tested it in both, and it also works with how many parameters?\n Um, this one is, uh, if I remember correctly, is 7TB.\nThat's a pretty big one. That's a nice.\n Yeah. Yeah. I think that's the, the, the big model that's running here. But also here it's, you know, I have 96 GB of data. So, no, it wouldn't be 70B, but it's one of the bigger models, right? And it's loading actually 80% or 90% of the of the model because it's,\n I mean, I talk, I don't know how many tokens per second that was, probably, it actually around seven or eight tokens. So that's not, that's not super fast. But again, inference on MacBooks in general isn't, isn't super fast, right?\nIt's good to know that the thing is, uh, McDonald's offers a variety of breakfast items that can be convenient. Uh, okay. I never tried this query. So it's hard, it's hard to say. But now, you know, if I type in 50, it will just redo the whole thing with the same query, uh, with the same query, just, just, you know, lower, lower effort.\n It takes.\n So you can see it's a lot faster. Yeah.\n Yeah. So, yeah, and usually what I noticed is around 50, you don't see really a change in, uh, in the output. So we can remove, and that's interesting from the research point of view, remove % of calculations, and essentially the output is almost the same. Benchmarks in a second. Uh, the downside is that this algorithm around 50, I would say it break even with the regular matrix multiplication, right? So, so this is like we're still, it's faster than 100% of this algorithm, but it's more or less break even if we just use regular multiplication. But now,\n here it brought out a whole different angle though, because the first one just talked about convenience and taste and the menu, and this adds in the like analysis of the nutrition.\n Oh yeah, that's right. So is that the example of getting more negative? I will, we'll see, like this is, this is, I didn't try this example, so we'll see how, how it goes, right? Uh, and what not. I can go down to 30% and and, you know, still much faster, right?\n And you can see that it's almost super, super close, right? So one to the other, it's, it's very close in the output.\n 50.\n Yeah.\n And if you think about it, this is really, aside from the speed, like we're only doing 30% of multiplications. Right. So, so just know it's interesting by itself.\n Yeah. It's two and a half times faster than the 100.\n Yeah. Exactly. Right. So, so that's the thing. And around here, I would say it's twice or or 30% faster than the regular vanilla algorithm that I would be running on this on this laptop. Right now, with 20%, it still provides, uh, quality, although you start to see a little bit breakdowns here and there. I don't know about here.\n Well, now it's really focused on egg muffin and see some reason.\n So, it's different.\n I, I will show fruit and yogurt.\n Yeah, I will, I will show you some, some interesting cases in a second, but you know, it still produces decent text.\n I'm just dying to hit one.\n Yeah. Well, so I will go like slower. So, let's say I think around 16% usually, uh, I, it starts to break down really in quality, right? So, there is a spot, and\n you kind of are, you reaching back to like GPT2 insane human talking kind of.\n Sorry. Sorry. Can you\n remember in GPT2, it was like hard to get it to be anything coherent. Oh yeah, yeah, yeah. So, so I would say you, you can kind of see that. Yeah. Also, another interesting thing I noticed, I don't remember with which model or perhaps all of them, is that the lower I get, the more technical they try to get. So, if you say, uh, how are, for example, and you just want to do simple text completion, right? If you say how are, and you allow the model to finish, um, the higher quality models will say, how are you feeling today? Feeling fine, and what the lower I got, usually it was like, how are we supposed to do C++ whatever, and then it goes to into HTML and what not, like, like you seeing a model that was trained only on technical forums. I don't know why, why it happens like that, and just, just, you know, and that's one of the effects that I notice in a few cases, right?\n are like that to some extent too, the ability to see the big picture while also details.\n Exactly that. So, so that's, you know, it tells you, you know, this is a new method of of also looking at the model.\n Yeah.\n Again. So now 16%, you know, instead of, um, what you called it, what, uh, what you call it, Effort Engine, you could have called it lobotomy.\n I, that's funny, because that was actually one of the first names I wanted to originally played with distillation of of models, and I was thinking of creating a lobizer.\n Yeah. Something that would just, you know, pick things that I don't need from the model. So, so that's,\n but you see, it's clearly this is three times faster now, legitimately. So that's, that's, uh, yeah, uh, three times faster than the, at 20, and now, uh, see,\n oh, here it's opinionated is not a typical choice for bre, what are you doing here?\n I, I feel that, you know, some of the layers of of conditioning get, get lost with the me time, and I think around 14 and 13%, you will start to finally see the breakdown in quality, although it's a simple question, right? I will show you more.\n Here it's, it's like, it's answering the question, but not directly, it's sort of talking McDonald's. So, so we can see that, right? And then if we go to 10%, it's we should start to see the breakdown, right?\n This is funny.\n Yeah. And now it's, you know, it's just got stuck on on,\n because it's repeating.\n Yeah. It's just repeating, right?\n It's doing that.\n So, so speaking of GPT2, like that's kind of what, what we can see here, right? And\n well, even without, with, without, um, you know, human reinforcement learning or fine instruct fine tuning, you do this, right?\n Yeah, exactly that. So, so, and then if I do 5%, that's like totally broken, right? Uh, it's nice that, like, like you can see, like we can play with that, and again, this is the first iteration, so really it goes broadly, right? So if I say 10%, it's 10% across all the layers, but, uh, but what we could do is we could play more, so we could say, oh, let's give middle layers 10%, and the side layers, we can, you know, beginning and end layers, let's go 80%, right? So I didn't even touch upon in my exploration of this subject, like how actually how much can we optimize, and I know some of the, some of, uh, weight, some of the, some, some of the matrices, I'm quite sure we can get very low here without a loss of quality, any significant loss of quality, right?\n Have you, have you played with the combination of this and just sort of messing with the weights?\n Uh, just messing with the weights?\n Yeah, changing the weights?\n Well, not, not intentionally.\n Yeah, but, uh, it is, uh, you know, if you change the weights, and again, this comes down to, um, to this fact that if you change the weights or you don't load some of the weights statically, you, the model that gets, gets like dumber, right? Like it loses this quality very fast. But here it's dynamically, it's, it still uses like this 5%, it still uses all the weights. Okay,\n probably right, or most of the weights all the time, just different weights for different token, right? So I would say the quality is still there, like the loss of quality is, uh, is, uh, is, you know, it's proceeding way, way slower.\n So that's one of the things. And by the way, like another cool thing that I noticed was, uh, the way it broke down, and initially when I was programming that, also that was fun, because it was in Python that I wrote the first version, so it took me a whole night to to to to get the inference of one sentence.\n Uh, my theory was that if I lower the quality from, I don't know, 100% to 50 or whatever, uh, what I would see is across all the layers, it would be like start falling apart, right? And and it would just drop down. So this is the original output, right? So, so this vectors, vector, original vector, and it would just start to to fall out, right? And go and go, go down. But what was interesting was that actually, at least in the Llama that I checked, but I guess in other models too, what happened was, so this is the first layer, right? Usually it was like, okay, it's falling down, but then it's getting back to the original, uh, vector. So there is in the model, you know, the middle can be, can be somehow more, you know, changing, and then in the end, it still converges to the same output. So you could think, oh yeah, like,\n can you, can you walk us through the architecture a little bit? Like, so we've walked through the math, we've seen it in action, you've discussed the research aspects of this, but I want to pop out and just talk about like the tooling, like for example, if somebody wanted, you can download weights, right?\n That's, if you wanted to go mess with them like this, and what are the tools you using?\n Uh, sure. Uh, just one, one more thing before we get to architecture, because this example, I, I will show you one more very quick example that I use for for testing, right? Because this one, you know, it's like a generic question that's, you know, anyone has opinion about McDonald's, I would say, right? But, but the question that I really like that shows the quality is, um, is what was it? How far is Radom, which is a city in in Poland, but, you know, again, I would not specify that it's that it's, uh, that's in in Poland.\n\n\nIt's a small city.\nSo again, it shows how precise the model is.\nHow is Faris Random from Sydney?\nYeah.\nRight.\nAnd this is a way more difficult question.\nSure.\nRight.\nBecause it and it shows us how how the model proceeds.\nSure.\nUh, and again, 5%, it's like it's garbage, but then just 15, I will go very fast.\nAnd in your harness, you set up an eval harness with lots of these kinds of questions.\nDid you make up those questions, or did you find a set that you could use for this?\nSo, I did evals with my own questions.\nA lot of some of them I tried to use the regular evals.\nSo, okay, let me let me finish this, and I will go to to evals.\nSo, right now, it's 15 with 50%, you can see consistent answer, but then you can see that it says 85 miles.\nDefinitely, it's not 85 miles, right?\nBut then you can see how the quality gets improved of the of the whole of the whole.\nIt's nice that like by nature of the question, it's clearer eval, you know, obviously.\nYeah, yeah, exactly, right?\nSo, so this is it's that was my base base testing because it's way faster than just running.\nAnd at what point is it converging to like reality or a good answer?\nI think I think around 25%.\nUsually, it's 25%.\nRight now, it's it's that's amazing because that's quite a lot less.\nUh, I would say usually it's around 25%.\nHere, it's 30%, but I think I didn't load the all the weight, so it's slightly more.\nI have a question that's a similar genre.\nWho was the president before George Washington?\nOh, uh, I remember that we played with that thing with GP3, right?\nWho was the president?\nUhhuh.\nAnd so, position doesn't didn't exist, right?\nAt 30%, it says that I wonder if at at where is it going to start saying it was the king.\nUh, president before George Washington was not a president.\nThe first president was no.\nSo, it's kind of it's confused, but yeah, around that that amount, it starts to break down.\nNow, getting back to architecture and how you can play with it and also the benchmarks, right?\nUh, so how you can play with it, you just go to and uh, I need to open the website.\nUh, give me one second.\nUh, uh, I guess you will link.\nI remember there was a better domain name here, but but you just go to the to to the page, uh, uh, oh, kolinko.github.io.\nWell, right.\nSo, it it explains all the the whole algorithm and and provides some of the benchmarks.\nIt is difficult, and I think we don't appreciate how rare it is to to have a really new algorithm that is not explainable in two words essentially.\nSo, it is a complicated algorithm, but it's it explains how how the algorithm works and what not.\nMhm.\nJust going to the website and and reading about the algorithm.\nUh, it it will be an interesting read for you if you're into algorithmics.\nThat will be something new, right?\nUm, especially since it again works on GPUs.\nSecond thing is, of course, it's there is a GitHub repo, and it took a lot of effort to really make it run on any on any MacBook, right?\nSo, hopefully, out of the box, uh, speaking of GitHub repo, I really took a lot of effort to make sure that it works on on a laptop from the get-go.\nSo, you should do g clone, you should there will be weights to be downloaded from uh from hugging phase, converted weights into into this this format, and you should have it working.\nI I hope so, you know, you know how it is with the projects, right?\nSo, so hopefully that that works, and I would say just play with it.\nNow, since it's a Mac uh Mac app again, I the reason I decided to go with a Mac platform was so that any developer just with a laptop could play with it, and of course, Nvidia might be better when it comes to people that are working with with the actual science.\nBut but yeah, I I decided to go with the with the MacBook, and then you just, you know, open it and completely open source.\nPeople could could take off your work, take take it onto the next level.\nExactly.\nUse it as Yeah.\nAnd also one thing is just preparing something and from the theoretical perspective, but the second thing is you want to really implement it and and to show how it works, and you want this reproductibility that I care about.\nUh, the final thing that you were asking was the benchmarks themselves, right?\nAnd um and here I just need to show you.\nSo, uh, speaking of uh uh speaking of metrics, the coolest metric that that I found and that was recommended to me by a person whose name right now I forgot sadly, but the guy that that that is one one of the main developers of llama.cpp.\nUh, he's thrown me the metric called Kullback-Leibler divergence.\nI'm probably mispronouncing this whole thing.\nAnd it's essentially what it does is you have a text, you run it with the original model, the the the well, you see what the model produces, right?\nThen you have it an approximated model or quantiz model or in my case model with a different effort.\nYou run it again, and then you see how this text more or less how similar are those texts, like simplifying it a lot, right?\nAnd from this, you can see how how var is the model.\nIn theory, it could still produce correct answers if you care about the correctness, right?\nBut but it would be just different text, but in practice, it's a good good enough metric, and it's very nice.\nHow's the met metric assessed?\nIs just like a simple like Levvenstein or is another inference?\nNo, it's totally different.\nSo, uh, again, like it's it's best to to to Google and read about that.\nBut if you if you have a prediction of each token, right, usually it's a list of tokens that you predict, right?\nLike you have this one vector, and you compare you compare uh you compare you measure the tokens against each other in some way.\nYeah.\nExactly.\nSo, you the whole the whole thing.\nYeah.\nGot it.\nSo, So, it doesn't just look at this first token, but all the tokens that are might be predicted, their probabilities.\nSo, it's a very mathematical approach to Yes.\nExactly.\nAnd I like that metric because it's super super super fair, super easy to calculate, right?\nAnd and uh and it's nice.\nUh, so this is uh this is the thing that I opened here.\nThat's the chart that the developer sent me.\nThat's the metric for oh sorry, that's the metric for for for lama 7B, and it gets quantized with various quantizations, right?\nSo, 8, 6, and and you can see it uh gets lost, right?\nSo, what I did was I did the same thing with with effort and slightly different scale, and I know it's hard to compare, but I'm sure someone can make a post.\nThe thing is, uh, what you can see here, the most important parts are well, if you looked at the numbers, sadly, it's not as good as as the quantization itself, right?\nSo, hopefully, again, we can find some some nice use for it.\nAlso, it's a new vector matrix multiplication algorithm by itself, so so the uses may be in totally different fields as well, right?\nUh, but uh, but what we can see here is that the distance is, you know, 50% is quite close, and then it collapses around 20% or something like that, and and that's that's one metric.\nAlthough for me, really like this metric shows nice, it's it's easy to to see that metric, right?\nBut then when you care about the quality of the answer, that's not the best metric necessarily, because what you care about is the whether the answers are are true.\nSo, so we have a lot of benchmarks, right?\nAround that, sadly, again, this this model runs on a MacBook, so I didn't when I was first of all, I was at the end of the very long marathon when I was doing these benchmarks, so I just didn't have the strength to again implement all the benchmarks that are common.\nUm, and also regular benchmarks take a lot of time when you try to run them on a MacBook.\nSo, I prepared my simp simpler benchmark where it's just, you know, list of questions and answers that are that the model can get can can break.\nAnd I'm quite sure it will be similar if we run it with the regular benchmarks.\nJust give me one second.\nLet me see where it is.\nSo, the cool thing is uh the main thing is this is the similarity.\nCool.\nWhat you can see here, and again, this is basic basic questions and answers question benchmark, right?\nAnd of course, anyone should be skeptical if anyone says, oh, I built my own benchmark to train my model, but sorry, like that was the simplest thing to do, right?\nBut what you can see here is really like that it's starts to break around this 25%, and it loses quality very fast after 20%.\nAnd that's for 7B model.\nI was testing it also on mix that was 70B or something like that.\nAnd surprisingly, it also worked.\nAnd if I remember correctly, it worked even better uh than than this one.\nSadly, it's just, you know, I have to manage my own time and resources.\nSo, I cannot I didn't make the chart back then.\nLater on, it wasn't working something right.\nSo, so I cannot show you the exact details.\nThe other cool thing that is uh that is there one more benchmark here because this is the output, right?\nThe main thing that was that is tested and that is quite interesting, this this chart, which is very similar, and this is the pure matrix multiplication, right?\nSo, you have matrix of weights, you have a vector, right?\nAnd you want to see how the quality, how far it is in cosine similarity score from from the original vector, just looking at a single weight times times times vector, right?\nAnd what you can see here is that you can really cut down this 20%, and the output vector is very close in the in the space to the to the original one, and that's where all that other things come come from, right?\nSo, so so you can and again, if you I remember I was doing the test where I just removed some of the weights, and if you remove weights and try to get the same result, it will collapse very, very fast, right?\nSo, it will be closer to here, right?\nSo, those are are the benchmarks.\nOne more thing that I will show you in terms of benchmarks, and you know, it's like that was that was one of the interesting things uh here, and I'm sure someone can zoom in and pause when they watch it.\nThe question was uh how does it if it fails, does it fail with time?\nSo, is it worse the longer the text gets?\nBecause my fear was, okay, if I start doing this this kind of simplification optimization, you know, after a thousand tokens, it will just the quality will drop down.\nUh, what it shows is more or less that the quality, and this shows how far the token is from the original one.\nIt doesn't depend on the distance, right?\nSo, it doesn't cut down on the it seems it doesn't cut down on the on the context length, right?\nAnd you have here, by the way, this chart is also interesting because it compares uh one of the iterations of the algorithm with with quantization, right?\nSo, it shows that, okay, quantiz algorithm in this token, it's way different, right?\nWith my algorithm, it's different in this token, so it's interesting view of how this various ways of of speeding up the whole process affect uh the tokens that are that are there.\nSo, you can see that some of the tokens with quantization, you know, there quantization has problems, some of the tokens my algorithm has problems, and it's not one to one, right?\nSo, so it's interesting again point of view for the research.\nAnd the one last thing, and I'm surprised that no one actually did that for this view for for for regular quantization, let me zoom in slightly.\nSo, so what I did is I don't think anyone created this heat map, so when you have a metric of how how far away is the model?\nWhat you can do is you can make a heat map.\nSo, we have an article here.\nSo, like some random article about birds, right?\nAnd on top of it is where my effort my well with my with my al algorithm when when you get lower in effort, where the words start to diverge, right?\nSo, you can see here if it's white, then it's like the same, right?\nThe would make the same prediction.\nIf it's red, there's a huge divergence.\nIt's an interesting way of looking at at things, and I don't think people even do that with quantization.\nSo, so I think it might be interesting if if we start to look at models from this perspective and then say, oh, like, you know, what's what's here, right?\nLike it goes back to your your your measure of of difference on the being very mathematical in the first visualizing it this way is useful.\nYeah.\nYeah.\nSo, so yeah.\nSo, there are a few things that are interesting from the perspective of uh of research, right?\nThere might be one takeaway for a lot of people who are building apps is who are doing evals for just kind of regular inference applications, right?\nIs to build a heat map for your evals, like you could do that for another domain, doesn't have to be for this problem.\nThat's a great idea.\nSo, more or less that's it.\nLike like I said, sadly, you know, quantization and it's one of the things about scientific honesty, you have to say, you know, if something is better, then you have to say that it's better, right?\nSo, so quantization is better, or at least it's, you know, perhaps I could get this algorithm to be as good, but it opens up interesting fields of research that I think aren't that much explored, and again, what models do all in all is still an open question in many cases, right?\nAnd any new lens that we have is maybe interesting.\nWell, okay, before we wrap, I want to touch on you, you and the community a little bit more.\nYou know, again, Arur said like the OG tinkerer is Tomasz Kolinko, and you know, you've clearly had a long career here um and done lots of interesting things.\nWe didn't touch on many of them actually, but I do want to touch on a couple of them and and also just how small the community is, too.\nI think when I got here, you said, you know, I said um I was just with the 11 Labs founders in London, and they're from Poland, of course, and you pointed out the windows.\nTheir office is right there.\nI think so.\nI think so.\nYeah.\nThey told me that that it's in there.\nYeah.\nYeah.\nYou know, so that's a small a small world.\nI love it when you when you come to a place and meet people and it feels small all of a sudden, and that's great.\nUm, speaking of that, like meeting people uh getting together, you started a kind of incubator garage or hacker house kind of a concept.\nOh yeah, that was uh that was fun.\nUh, we rented a you know, after so many years in software.\nYeah.\nOne day, you wake up and you're tempted to do something big and something loud and weld something or or do do things like this and uh really Yeah.\nFree space work.\nFree space.\nExactly.\nSo, so me and my friend were looking for a year for a place that would be\n\n\nClose to the city center where we could just uh do cool stuff, build cool stuff.\nAnd an idea for a while was to to have an accelerator.\nYou have photos of it.\nYeah, actually I have it.\nYeah.\nCool.\nYeah.\nI don't have too many photos and of course photos don't really do it justice, but people that's that's my\n\nSo, it used to be actually a car chop shop.\nOkay.\nSo we found a lot of license plates of cars that were that ended their life here, right?\nSo so we we clean it up and and we have like this is a small part of this whole place and we have a place where we can do various cool things.\nOne of the ideas was that I wanted to venture builder related to climate tech um here in Poland.\nBut one thing is climate tech is not very big in Poland sadly.\nSecond thing is AI is way just way more interesting right now.\nSo, so I I decided to focus full time on AI.\nAnd you were at the very first AI Tinkerers meet up.\nOh, yeah.\nJust one more thing.\nUh because of my background in uh in in startups, I love having startups there and right now the place serves as a as a hub as a meeting place for a lot of startups and we're doing we're doing meetups there and it's a nice atmosphere and what people say is oh yeah, Google campus is nice like it's nice to have these offices and everything, right?\nBut when it comes to this garage atmosphere that at least people hear from, you know, it's more appropriate for So this is this is the place where, you know, you can drill a hole in the wall, make stuff, build stuff.\nExactly.\nSo So I'm not objective, but people tend to say that it's super inspirational to to be there.\nYeah, that's really cool.\nThat's nice.\nAnd you have the robot.\nUm Oh, yeah.\nThe robot the robot is there.\nBut that's that's a thing for another story.\nSo what was the story there?\nYou just decided like you need to go buy a uni tree robot and start hacking.\nThe thing is my story I think what I'm good at is trying to hit technologies trends way before they happen.\nWith AI I feel very weird because I'm kind of not late to the game but just I arrived with everyone else.\nOf course I was interested in AI during my computer science and and Well, we didn't talk about it but you did the Bitcoin uh yeah like but I left the the crypto community.\nYou weren't a crypto bro.\nThat's I wasn't a crypto bro.\nI was the tech crypto person and the technology there is quite nice.\nUh I built some cool stuff and and some amazing people that I've met there.\nUm but uh but yeah with AI I built chatbot I tried to set up a chatbot uh startup in 2007 actually.\nSo so that's been way way before before its time and again when it comes when there is a new technology I try to figure out what's it about.\nAnd the robot that you see in the background, yeah, it's a Uni3 robot and they're super cheap right now.\n€2,000 that is that's extremely cheap and and it's nice to see what they are about, how far it is.\nHow hackable is it?\nLike what can you do?\nWhat's Well, that's that's the guys that were on hackathon that I that I borrowed them, but I think it is it is hackable.\nUh I would love to for them to they have this other version that is educational that is like that has an open SDK and has some accessories but that that version is is way better.\nI would love them to give access to that version.\nBut yeah by itself this robot it's a nice piece of technology.\nSo I if if you have if you can afford it I would say go for it especially before the tariffs fit very much.\nBut speaking of AI Tinkerers, if I can add one more thing and a word of p praise towards AI Tinkerers, you know, um we have a lot of communities here, AI communities, what not startup communities and also speaking of this algorithm, I spoke about it with many people super technical AI Tinkerers was the only place where I gave a fiveminut presentation and the questions I got from the room were questions that showed such a deep understanding that I didn't have these questions after an hour of talk with a fellow computer scientist and I was so amazed by the quality of the the people and the discussion afterwards that was that was something amazing and that was uh great.\nYeah, we have a lot of technical cool technical people here in Warso and you managed to to grab all of them and and invite them to AI Tinkerers.\nSo I respect that.\nThanks.\nUh be sure if you're on the road and you're in another city stop by one of the chapters we had, you know, it's funny at the at the hackathon we just had here in Warsaw.\nUm I went through the data and there were people from AI Tinkerers chapters around the world.\nYeah, we had somebody from Koala Lur, Medí, Colombia, of course, all over Europe.\nThat was exciting to see.\nThat is also one thing that I was really missing this this community aspect of of AI.\nAnd again, getting the cool thing about the blockchain community is that it's so well connected and it has hackathons every single week happening somewhere else, right?\nAnd people meet in person.\nAnd of course, it's easier because the budgets are are better and and whatnot over there than with some AI projects.\nThey have the money, but now they have the applications.\nYeah.\nYeah.\nSo the applications real world applications are missing a little bit.\nThe applications.\nYeah.\nBut uh but now the community, you know, when I left the community, I was really missing that that human aspect.\nAnd of course there are slacks, there are discords, but uh I think AI Tinkerers is this one missing community that that thing that I was really missing from the from the AI community.\nSo I'm very grateful for for doing the whole thing.\nYeah.\nArt Andreas and everyone here has done a great job and it's it's a very fun fun thing.\nYeah, definitely.\nThank you.\nWell, where can people find out about more of your work?\nI'm sure the GitHub URLs or any other thing you'd like.\nYeah, GitHub GitHub URL is is one thing.\nRight now, I'm on a sabbatical.\nSo, so I do totally different.\nWhat is next?\nUh, right now for me, dancing, right?\nI'm focusing full time, but I think I will, you know, it's hard to be on a sabbatical during such an exciting times, right?\nSo, I'm already brainstorming some of the ideas.\nI don't know personally, perhaps I will go deeper into research or I will try to do a new project, startup.\nWe'll see.\nWe'll see.\nBut uh I would say if you want to follow me, go on to Twitter.\nKolinko is is my is my handle there.\nI don't tweet that much, but when I start a new thing, I will.\nYeah, there will be over there.\nWe'll follow along and we'll check in on you.\nThis is great to meet you.\nThank you so much for your time.\nThanks.\nThanks for the space.\nTake it.\n",
  "dumpedAt": "2025-07-21T18:43:25.951Z"
}