{
  "episodeId": "fZanKXHUeCQ",
  "channelSlug": "@aitinkerers",
  "title": "From SOP to API in Seconds: Steve Krenzel on Automating Business Logic with AI",
  "publishedAt": "2025-05-13T17:23:08.000Z",
  "rawLines": [
    {
      "lang": "en",
      "text": "I find the abstractions to be",
      "offset": 0.24,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "unnecessary. I I find that the APIs that",
      "offset": 2.36,
      "duration": 4.04
    },
    {
      "lang": "en",
      "text": "the foundation models have exposed",
      "offset": 4.56,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "around like tool use and structured",
      "offset": 6.4,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "output and everything are kind of",
      "offset": 7.919,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "sufficiently high level that not getting",
      "offset": 10.32,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "a lot of value from the genic",
      "offset": 12.719,
      "duration": 5.241
    },
    {
      "lang": "en",
      "text": "frameworks.",
      "offset": 14.96,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "[Music]",
      "offset": 18.46,
      "duration": 2.21
    },
    {
      "lang": "en",
      "text": "[Applause]",
      "offset": 18.94,
      "duration": 4.859
    },
    {
      "lang": "en",
      "text": "[Music]",
      "offset": 20.67,
      "duration": 3.129
    },
    {
      "lang": "en",
      "text": "Welcome AI tinkerers. This is our global",
      "offset": 28.4,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "stage where we take people who are doing",
      "offset": 30.96,
      "duration": 3.119
    },
    {
      "lang": "en",
      "text": "interesting stuff in Gen AI and we look",
      "offset": 32.559,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "under the hood of their side projects",
      "offset": 34.079,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "and their main projects. We try to get",
      "offset": 36.16,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "the snippets and the code snippets and",
      "offset": 37.52,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "things that will enable you in your",
      "offset": 39.68,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "projects. And today I am excited because",
      "offset": 41.2,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "we're here with Steve Krenzel of Logic",
      "offset": 43.2,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "Inc. Um and Steve is someone I've known",
      "offset": 45.68,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "for over 10 years who's done incredible",
      "offset": 47.76,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "things. And early in my generative AI",
      "offset": 49.52,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "journey, this is pre-Chat GPT. catching",
      "offset": 51.36,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "up with Steve and realizing that he was",
      "offset": 54.079,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "so deep in he was doing stuff that",
      "offset": 56.48,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "didn't have names like Rag yet or",
      "offset": 58.96,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "Agentic Workflows and he was doing it in",
      "offset": 61.44,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "the context of bigger companies. He's",
      "offset": 63.44,
      "duration": 3.039
    },
    {
      "lang": "en",
      "text": "worked at Salesforce, he's worked at",
      "offset": 64.72,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "Brex, he's worked at Twitter and more",
      "offset": 66.479,
      "duration": 5.361
    },
    {
      "lang": "en",
      "text": "and uh I'm so happy to have you Steve.",
      "offset": 69.119,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "Welcome. Thank you, Joe. It's it's",
      "offset": 71.84,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "awesome to be here. Tell us about Logic",
      "offset": 73.76,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "first and what it does and then we'll",
      "offset": 76.72,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "dive under the hood a little bit. Yeah.",
      "offset": 78.799,
      "duration": 3.281
    },
    {
      "lang": "en",
      "text": "Uh so Logic is all about putting",
      "offset": 80.64,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "intelligence to work. It's about uh",
      "offset": 82.08,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "taking this like new superpower that",
      "offset": 83.84,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "humanity has which is intelligence on",
      "offset": 85.68,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "demand and making easy to apply that",
      "offset": 87.68,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "either to like internal operational",
      "offset": 90.08,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "workflows or to product features you",
      "offset": 91.84,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "might be building and that's a super",
      "offset": 93.68,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "high level that could apply to anything",
      "offset": 95.36,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "but like where do you what is I know",
      "offset": 96.96,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "you're a ninemonth old company so",
      "offset": 99.36,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "ninemonth old company four people funded",
      "offset": 100.56,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "uh soon to be six people we have two",
      "offset": 102.88,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "engineers joining oh awesome um",
      "offset": 104.159,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "specifically you give us a document like",
      "offset": 106.88,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "an SOP or PRD some kind of description",
      "offset": 108.64,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "of a business process give us that",
      "offset": 110.799,
      "duration": 2.801
    },
    {
      "lang": "en",
      "text": "document and a couple seconds later,",
      "offset": 112.399,
      "duration": 2.481
    },
    {
      "lang": "en",
      "text": "we'll give you an API that implements",
      "offset": 113.6,
      "duration": 3.519
    },
    {
      "lang": "en",
      "text": "that document. So from that's it. That's",
      "offset": 114.88,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "very high level vision. So it's like SOP",
      "offset": 117.119,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "in and API we handle the rest. Here's an",
      "offset": 119.36,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "API. Well, it's a welltyped API with",
      "offset": 122,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "back testing, documentation, everything",
      "offset": 124.24,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "you need to start integrating with it.",
      "offset": 126.479,
      "duration": 2.961
    },
    {
      "lang": "en",
      "text": "Okay. And so today you would I mean",
      "offset": 127.84,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "that's really that could be anything.",
      "offset": 129.44,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "Yes. Can it be anything? So there are",
      "offset": 131.28,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "some constraints like it's very um we're",
      "offset": 133.12,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "not kind of doing any like multi-step",
      "offset": 135.44,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "agenic workflows. We're not interacting",
      "offset": 136.959,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "with external yet. Uh so it's it's it's",
      "offset": 138.879,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "very transactional in that you give us",
      "offset": 141.76,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "data that you want us to reason about or",
      "offset": 143.28,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "somehow interpret, we give you data",
      "offset": 145.28,
      "duration": 3.039
    },
    {
      "lang": "en",
      "text": "back. What is a killer use case today?",
      "offset": 146.64,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "Like something someone's using like are",
      "offset": 148.319,
      "duration": 3.121
    },
    {
      "lang": "en",
      "text": "people using it now? You have you have",
      "offset": 150.239,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "early. Yeah. Yeah. So so prod we have a",
      "offset": 151.44,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "number of customers we're working with.",
      "offset": 153.599,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "In fact, I think we ended December uh",
      "offset": 154.8,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "completing about 145,000 tasks for our",
      "offset": 156.879,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "customers. What is a task? Like how big",
      "offset": 159.04,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "is a task? Uh a task might be like",
      "offset": 160.56,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "moderating an item for like an",
      "offset": 162.4,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "e-commerce platform. So like some some",
      "offset": 164.4,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "inventory item comes in and they need",
      "offset": 166.239,
      "duration": 3.241
    },
    {
      "lang": "en",
      "text": "like standardized",
      "offset": 167.76,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "titles, colors. Oh, so some e-commerce",
      "offset": 169.48,
      "duration": 4.52
    },
    {
      "lang": "en",
      "text": "store has like a data feed of like from",
      "offset": 172.56,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "their supplier just inventory from third",
      "offset": 174,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "parties. A couple of them come with all",
      "offset": 176.16,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "caps and it's like Yeah. Or there might",
      "offset": 177.84,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "be like rogue HTML tags in them or",
      "offset": 179.2,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "random phone numbers in them. And so",
      "offset": 181.12,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "like and so today today if I wanted to",
      "offset": 183.76,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "automate that Yeah. I would I would have",
      "offset": 186.319,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "to I have humans doing it and I'm like",
      "offset": 188.48,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "no no no. So today I would I would have",
      "offset": 190,
      "duration": 6.159
    },
    {
      "lang": "en",
      "text": "to build an entire stack for my Genai",
      "offset": 192.48,
      "duration": 5.839
    },
    {
      "lang": "en",
      "text": "data pipeline and then I'd have to use",
      "offset": 196.159,
      "duration": 3.281
    },
    {
      "lang": "en",
      "text": "it and I have to test it. I have to",
      "offset": 198.319,
      "duration": 2.481
    },
    {
      "lang": "en",
      "text": "create evals. I have to do all this",
      "offset": 199.44,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "stuff and then I could solve that like",
      "offset": 200.8,
      "duration": 3.519
    },
    {
      "lang": "en",
      "text": "yeah we can moderate our content now",
      "offset": 202.8,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "that one task. Exactly. And then I could",
      "offset": 204.319,
      "duration": 2.801
    },
    {
      "lang": "en",
      "text": "do that over and over again for my top",
      "offset": 205.92,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "10 tasks. I can rank them probably.",
      "offset": 207.12,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "There's I mean Accenture is probably",
      "offset": 208.8,
      "duration": 2.48
    },
    {
      "lang": "en",
      "text": "collecting so much money doing this",
      "offset": 210.08,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "right now. And then you're saying you're",
      "offset": 211.28,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "saying like no no no you've already got",
      "offset": 213.519,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "a well-developed SOP. Yeah. Paste it in.",
      "offset": 215.12,
      "duration": 5.759
    },
    {
      "lang": "en",
      "text": "Is that exactly it? And and can you show",
      "offset": 218.4,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "us like Yeah. Okay. Yeah. To really make",
      "offset": 220.879,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "that concrete, the the that specific",
      "offset": 222.879,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "customer that I'm thinking of right now,",
      "offset": 224.48,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "they gave us an SOP that they've been",
      "offset": 226.08,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "using for about 9 years. They've had a",
      "offset": 227.68,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "team of four humans kind of working off",
      "offset": 229.36,
      "duration": 3.519
    },
    {
      "lang": "en",
      "text": "that SOP. We were able to put that in",
      "offset": 231.12,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "our system and immediately uh start uh",
      "offset": 232.879,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "automating portions of that workflow.",
      "offset": 235.68,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "And so like this is this kind of battle",
      "offset": 237.36,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "tested on really old SOPs that existed",
      "offset": 238.64,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "way before LLM's. Does your system also",
      "offset": 240.4,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "figure out like, hey, here's what I'm",
      "offset": 242.4,
      "duration": 3.039
    },
    {
      "lang": "en",
      "text": "good at, what I I still need the human",
      "offset": 243.599,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "to help me on. Yeah. So depending on how",
      "offset": 245.439,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "you phrase the SOP, you can you can uh",
      "offset": 247.599,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "kind of ask for uh confidence scores on",
      "offset": 249.519,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "things and like if you're if you're",
      "offset": 252.08,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "moderating say messages between humans,",
      "offset": 253.599,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "which is like um user generated content",
      "offset": 256.32,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "moderation is another use case. You can",
      "offset": 258.479,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "say like should I escalate to a human",
      "offset": 260.079,
      "duration": 2.961
    },
    {
      "lang": "en",
      "text": "because this is too ambiguous for me to",
      "offset": 261.68,
      "duration": 2.959
    },
    {
      "lang": "en",
      "text": "handle. Yeah, absolutely. Can you show",
      "offset": 263.04,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "us a real like use of it? Let's go.",
      "offset": 264.639,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "Yeah, absolutely. So I thought it'd be",
      "offset": 266.32,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "fun to actually just generate an SOP on",
      "offset": 268.4,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "demand. Um, and so I'm I'm just going to",
      "offset": 270.16,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "have Claude write an SOP for kind of",
      "offset": 272.56,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "taking in biographical information about",
      "offset": 274.88,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "a podcast guest and then outputting",
      "offset": 276.639,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "interesting questions for that for that",
      "offset": 278.56,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "guest. Okay. And so this is, you know,",
      "offset": 280,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "this is an SOP that that our system",
      "offset": 282.8,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "obviously has never seen before uh being",
      "offset": 284.16,
      "duration": 5.599
    },
    {
      "lang": "en",
      "text": "generated on demand by cloud. Um, and",
      "offset": 286.56,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "there will be nothing LLM specific in",
      "offset": 289.759,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "this. So there will be no chain of",
      "offset": 291.6,
      "duration": 2.879
    },
    {
      "lang": "en",
      "text": "thought. There will be no few shot",
      "offset": 293.199,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "examples. It's just going to be pretty",
      "offset": 294.479,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "The SOP doesn't have that. Right. Right.",
      "offset": 297.12,
      "duration": 3.56
    },
    {
      "lang": "en",
      "text": "Yeah. Okay.",
      "offset": 299.28,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "And when you give that to our system",
      "offset": 300.68,
      "duration": 4.44
    },
    {
      "lang": "en",
      "text": "behind the scenes, we'll we'll provide",
      "offset": 302.96,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "all of that. Like we do a bunch of",
      "offset": 305.12,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "prompt rewriting. Got it. Um but let me",
      "offset": 306.32,
      "duration": 6.4
    },
    {
      "lang": "en",
      "text": "just get this into logic. Yeah. Cool.",
      "offset": 308.72,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "And so let's",
      "offset": 315,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "see. Jiren podcast",
      "offset": 317.479,
      "duration": 6.56
    },
    {
      "lang": "en",
      "text": "interview. Let's call it uh for",
      "offset": 319.88,
      "duration": 7.56
    },
    {
      "lang": "en",
      "text": "guest. We'll publish this. And so that",
      "offset": 324.039,
      "duration": 4.841
    },
    {
      "lang": "en",
      "text": "as an end user that's kind of all you",
      "offset": 327.44,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "need to give us from here we we just we",
      "offset": 328.88,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "take it from here. So we'll um we'll do",
      "offset": 331.28,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "the prompt rewriting to make this more",
      "offset": 333.68,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "friendly for LM. We as part of this",
      "offset": 335.12,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "process we infer a schema for this",
      "offset": 337.12,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "process. So we'll figure out like oh",
      "offset": 339.28,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "what are the inputs needed to run this?",
      "offset": 340.56,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "What outputs are you expecting for it?",
      "offset": 342,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "We'll generate test cases for it so that",
      "offset": 343.68,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "you can have confidence like our system",
      "offset": 345.919,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "is doing what you what you expect it to",
      "offset": 347.28,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "do. We'll generate integration docs and",
      "offset": 349.039,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "a generate podcast interview is going to",
      "offset": 351.36,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "generate like stuff to prep the host",
      "offset": 352.88,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "like bio information and like questions",
      "offset": 354.8,
      "duration": 3.119
    },
    {
      "lang": "en",
      "text": "that we could ask and things like that.",
      "offset": 356.72,
      "duration": 2.4
    },
    {
      "lang": "en",
      "text": "Right. Right. So, so imagine you were",
      "offset": 357.919,
      "duration": 2.401
    },
    {
      "lang": "en",
      "text": "making like a little a little like",
      "offset": 359.12,
      "duration": 2.16
    },
    {
      "lang": "en",
      "text": "mobile app where you just wanted to",
      "offset": 360.32,
      "duration": 2.719
    },
    {
      "lang": "en",
      "text": "quickly enter some like some details",
      "offset": 361.28,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "about your guest and get back a set of",
      "offset": 363.039,
      "duration": 2.561
    },
    {
      "lang": "en",
      "text": "questions. Okay. And so the inputs would",
      "offset": 364.4,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "be like name or something and and Yeah.",
      "offset": 365.6,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "So the schema that that our system",
      "offset": 368.24,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "inferred was like a quick quick",
      "offset": 370,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "biography on on the guest um any kind of",
      "offset": 371.84,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "public things they published profiles",
      "offset": 375.6,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "and and maybe some previous interview",
      "offset": 377.6,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "appearances and then we'll our system",
      "offset": 379.36,
      "duration": 6.64
    },
    {
      "lang": "en",
      "text": "will return uh a list that that follows",
      "offset": 381.919,
      "duration": 8.161
    },
    {
      "lang": "en",
      "text": "this output schema. Cool. And uh I can",
      "offset": 386,
      "duration": 5.919
    },
    {
      "lang": "en",
      "text": "show so it looks like you're generating",
      "offset": 390.08,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "schema you're generating tests. Yeah,",
      "offset": 391.919,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "generating tests and the the number of",
      "offset": 394.08,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "tests that we generate will we kind of",
      "offset": 396.08,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "leave it up to the LM. Sometimes we'll",
      "offset": 398.56,
      "duration": 2.32
    },
    {
      "lang": "en",
      "text": "generate 30 tests, sometimes we'll",
      "offset": 399.759,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "generate in this case one test. Um and",
      "offset": 400.88,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "uh",
      "offset": 404.4,
      "duration": 3.519
    },
    {
      "lang": "en",
      "text": "it gives you we also give you",
      "offset": 405.919,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "integration docs for your for your",
      "offset": 407.919,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "engineering team. Um and so in this",
      "offset": 409.52,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "we'll show you the actual schema that",
      "offset": 412.08,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "we're using behind the scenes. Um all",
      "offset": 413.919,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "the examples in here are dynamically",
      "offset": 416.4,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "generated for your process and we give",
      "offset": 417.84,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "you developer docs and with libraries.",
      "offset": 420,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "That's so cool. Yeah. code example. So",
      "offset": 422.24,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "if you um you can just take this",
      "offset": 423.84,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "example. We can hop over to our",
      "offset": 426.479,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "terminal, paste this in. It's like that",
      "offset": 428.479,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "whole meme of service as a software. Not",
      "offset": 431.12,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "really a meme. It's a real thing. But",
      "offset": 433.039,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "like you could just like paste it and it",
      "offset": 434.319,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "start a company. Put this on a URL.",
      "offset": 436.319,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "That's the dream. Yeah. Okay. No, that's",
      "offset": 439.12,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "really interesting. Um, and so but",
      "offset": 441.199,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "practically speaking as a developer then",
      "offset": 443.28,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "now I have an endpoint for moderating",
      "offset": 444.8,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "and I'm going to have to look at the the",
      "offset": 446.72,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "schema but it's like I have inputs like",
      "offset": 448.4,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "name or whatever and then I have outputs",
      "offset": 450.08,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "like the bio and questions. So I call",
      "offset": 452.16,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "that API that's what I get. I get that",
      "offset": 454.319,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "in out but it's following my SOP and",
      "offset": 455.759,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "then it's got this platform behind it to",
      "offset": 458.4,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "like that I can go inspect and can I see",
      "offset": 459.919,
      "duration": 6.56
    },
    {
      "lang": "en",
      "text": "like what like are eval built in? How do",
      "offset": 462.96,
      "duration": 7.2
    },
    {
      "lang": "en",
      "text": "I how do I understand as a user of this?",
      "offset": 466.479,
      "duration": 7.681
    },
    {
      "lang": "en",
      "text": "Yes. So, let me show you. Let's do um",
      "offset": 470.16,
      "duration": 6.4
    },
    {
      "lang": "en",
      "text": "I'll just hop over to our We have a a",
      "offset": 474.16,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "welcome doc that is like a a goofy like",
      "offset": 476.56,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "you give us a a movie title and we we",
      "offset": 479.44,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "give you back emoji. Let me make this",
      "offset": 481.199,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "bigger. Yeah. And um and so like these",
      "offset": 482.879,
      "duration": 6.32
    },
    {
      "lang": "en",
      "text": "these test cases uh like so every time",
      "offset": 486.4,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "you every time you change your document,",
      "offset": 489.199,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "we will rederive the the we call them",
      "offset": 491.199,
      "duration": 5.041
    },
    {
      "lang": "en",
      "text": "the artifacts. So the schemas, the test",
      "offset": 494.16,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "cases, the integration docs. Sure. And",
      "offset": 496.24,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "uh we'll rerun these test cases. And so",
      "offset": 498.56,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "you can see we'll from like an eval",
      "offset": 500.56,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "perspective, you know, the the input for",
      "offset": 503.039,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "this test case is frozen. The the",
      "offset": 504.639,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "expected output is this. The actual",
      "offset": 507.36,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "output is different, but the test still",
      "offset": 508.96,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "passes. And we have the LM explain why",
      "offset": 511.36,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "it passes because both both sequences",
      "offset": 513.12,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "validly represent the movie frozen. The",
      "offset": 515.039,
      "duration": 3.281
    },
    {
      "lang": "en",
      "text": "actual result includes an additional but",
      "offset": 516.56,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "but it's fine. And so the we actually as",
      "offset": 518.32,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "part of our eval framework like if this",
      "offset": 520.719,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "were to fail, the LM would say why it",
      "offset": 522.24,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "believes it should fail. So it's not",
      "offset": 524.32,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "like exact matching, right? Which you",
      "offset": 526.08,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "have that's all that's the definition of",
      "offset": 528.72,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "geni evals right there is you have to",
      "offset": 530,
      "duration": 4.92
    },
    {
      "lang": "en",
      "text": "fuzzy match. And so there",
      "offset": 532.16,
      "duration": 5.679
    },
    {
      "lang": "en",
      "text": "there's this kind of like uh fuzzy",
      "offset": 534.92,
      "duration": 7.24
    },
    {
      "lang": "en",
      "text": "matching. We also do uh like",
      "offset": 537.839,
      "duration": 7.601
    },
    {
      "lang": "en",
      "text": "uh history. Okay. This uh let me if I",
      "offset": 542.16,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "was changing my SOP I mean the test",
      "offset": 545.44,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "could change radically depends on the",
      "offset": 547.44,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "change on the SOP. Uh, so does that mean",
      "offset": 548.8,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "just sort of like the history could be",
      "offset": 551.04,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "more or less relevant? I guess that's",
      "offset": 553.68,
      "duration": 5.839
    },
    {
      "lang": "en",
      "text": "right. Um, and so when we get into like",
      "offset": 556.16,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "part of the things we'll do is like",
      "offset": 559.519,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "we'll pull few shot examples from",
      "offset": 560.56,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "history that we've seen, but we kind of",
      "offset": 562.72,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "uh scope that down to a specific version",
      "offset": 565.12,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "of your document because because those",
      "offset": 567.12,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "inputs or the semantics might change",
      "offset": 569.44,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "between versions. Okay.",
      "offset": 571.04,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "Um, okay. Interesting. Can you take us",
      "offset": 574.32,
      "duration": 6.079
    },
    {
      "lang": "en",
      "text": "under the hood? Like I know you've done",
      "offset": 576.32,
      "duration": 6.72
    },
    {
      "lang": "en",
      "text": "uh AI in production at scale and some",
      "offset": 580.399,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "pretty innovative stuff. Like is there",
      "offset": 583.04,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "something in this system that you would",
      "offset": 584.959,
      "duration": 3.121
    },
    {
      "lang": "en",
      "text": "be willing to share with the audience of",
      "offset": 586.32,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "tinkerers that you think is kind of a",
      "offset": 588.08,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "hardearned less or trick or something?",
      "offset": 590.16,
      "duration": 5.119
    },
    {
      "lang": "en",
      "text": "Could be even an impromptu technique or",
      "offset": 593.04,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "something. I know you're full of these",
      "offset": 595.279,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "every time we talk. I get them. The",
      "offset": 596.64,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "nuggets. Yeah. Give us the nuggets. See",
      "offset": 598.48,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "if um that's that's a a tricky thing to",
      "offset": 600.24,
      "duration": 4.279
    },
    {
      "lang": "en",
      "text": "do on demand.",
      "offset": 603.279,
      "duration": 3.641
    },
    {
      "lang": "en",
      "text": "Uh",
      "offset": 604.519,
      "duration": 5.401
    },
    {
      "lang": "en",
      "text": "the let's see I I'll say I don't know if",
      "offset": 606.92,
      "duration": 4.84
    },
    {
      "lang": "en",
      "text": "I I might have mentioned this earlier",
      "offset": 609.92,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "but when one thing that we've found",
      "offset": 611.76,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "we've gotten a lot of value from is when",
      "offset": 613.2,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "we generate our schemas we we do a",
      "offset": 614.64,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "number of things like we always ensure",
      "offset": 617.44,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "that there's a a description for every",
      "offset": 618.8,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "field we find that this meaningfully",
      "offset": 620.56,
      "duration": 4.959
    },
    {
      "lang": "en",
      "text": "helps guide the LM's output. Sure. Um we",
      "offset": 622.56,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "also always provide both the input",
      "offset": 625.519,
      "duration": 4.121
    },
    {
      "lang": "en",
      "text": "schema and the output schema to the LLM.",
      "offset": 627.04,
      "duration": 6.239
    },
    {
      "lang": "en",
      "text": "Um that's good for for general guidance.",
      "offset": 629.64,
      "duration": 5.16
    },
    {
      "lang": "en",
      "text": "When are you running the LM after this",
      "offset": 633.279,
      "duration": 3.041
    },
    {
      "lang": "en",
      "text": "initial run to generate all this stuff?",
      "offset": 634.8,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "Like when you're when are you feeding it",
      "offset": 636.32,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "back in? Every time the SOP runs. Every",
      "offset": 637.68,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "single time we run the SOP, you use the",
      "offset": 639.12,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "full input and output with all the",
      "offset": 640.56,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "descriptions of Yeah. So the you know",
      "offset": 642,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "the the prompts that we generate behind",
      "offset": 644,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "the scenes are actually fairly large by",
      "offset": 645.839,
      "duration": 2.881
    },
    {
      "lang": "en",
      "text": "it sounds like it. I was going to ask",
      "offset": 647.76,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "like what model do you need to use? Like",
      "offset": 648.72,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "do you fit in do you need claude for",
      "offset": 650.72,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "this minimum or Yeah. I guess OpenAI's",
      "offset": 652.24,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "giving you 128K now, right? Yeah. Yeah.",
      "offset": 654.48,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "And so so and generally we're we're much",
      "offset": 656.72,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "smaller than that but like tens low tens",
      "offset": 659.68,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "of thousands of tokens or like 5 to 10",
      "offset": 661.76,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "thousand tokens we have some do you have",
      "offset": 663.44,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "a feel for speed implication of of so",
      "offset": 665.519,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "first of all I'm really curious about",
      "offset": 667.92,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "speed how it relates to prompt size but",
      "offset": 669.12,
      "duration": 6.32
    },
    {
      "lang": "en",
      "text": "also complexity like if you're asking it",
      "offset": 672.8,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "to think step by step and do more chain",
      "offset": 675.44,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "of thought advanced stuff in there does",
      "offset": 677.12,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "it get slower I know for the reasoning",
      "offset": 679.2,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "models certainly does but for base",
      "offset": 680.959,
      "duration": 6.721
    },
    {
      "lang": "en",
      "text": "models does it no so so the the size of",
      "offset": 683.12,
      "duration": 6.32
    },
    {
      "lang": "en",
      "text": "your input won't have too much of an",
      "offset": 687.68,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "impact on the latency like time to first",
      "offset": 689.44,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "token. Uh mostly most of your latency is",
      "offset": 692.079,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "going to come from how many tokens",
      "offset": 694.56,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "you're you're generating. And that just",
      "offset": 696.32,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "has to do with the like encoder decoder",
      "offset": 698.56,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "architecture of modern LLMs. The the uh",
      "offset": 700.399,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "yeah all all the efforts kind of in the",
      "offset": 704,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "in the decoding side. Okay. And then",
      "offset": 705.839,
      "duration": 2.721
    },
    {
      "lang": "en",
      "text": "what do you see runtime performance",
      "offset": 707.36,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "versus uh across the different models",
      "offset": 708.56,
      "duration": 3.519
    },
    {
      "lang": "en",
      "text": "right now? Do you use different models",
      "offset": 710.32,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "like llama 3? Do you use any fine tune",
      "offset": 712.079,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "for speed? Are you get are you into that",
      "offset": 714.88,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "at all yet? Yeah. So, right now we",
      "offset": 716.56,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "exclusively call out to OpenAI. Um, so",
      "offset": 719.36,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "we're kind of in the in the OpenAI",
      "offset": 722.32,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "family of models. Okay. Um, most of",
      "offset": 723.92,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "that's just because we we really",
      "offset": 726.32,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "optimize for quality over over latency",
      "offset": 728.639,
      "duration": 5.601
    },
    {
      "lang": "en",
      "text": "or even over cost. Got it. Um, it's",
      "offset": 731.68,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "really important to us that you can run",
      "offset": 734.24,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "a process a million times and without a",
      "offset": 735.36,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "human in the loop and have it work. So,",
      "offset": 737.68,
      "duration": 2.56
    },
    {
      "lang": "en",
      "text": "you just want one really smart model",
      "offset": 738.88,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "that you can rely on and get to know",
      "offset": 740.24,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "really well and you're not going to",
      "offset": 741.76,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "optimize. And we we have just started",
      "offset": 743.6,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "dabbling with multimodel support, not to",
      "offset": 745.6,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "be confused with multimodal support. Um,",
      "offset": 747.839,
      "duration": 6.8
    },
    {
      "lang": "en",
      "text": "and uh both within the the the OpenAI",
      "offset": 751.12,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "family of language models like GPT40",
      "offset": 754.639,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "mini versus GPT40 where we'll",
      "offset": 757.2,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "dynamically choose between between the",
      "offset": 758.48,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "two. Um, but we're also starting to look",
      "offset": 760,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "at uh like calling out the Gemini for",
      "offset": 762.079,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "some use cases. What draws you to",
      "offset": 764.24,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "Gemini?",
      "offset": 766.16,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "The the first thing that really drew us",
      "offset": 768.24,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "a a couple months ago were their",
      "offset": 770.079,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "embeddings. they just have really great",
      "offset": 772,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "embeddings um at least for from our from",
      "offset": 773.76,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "our testing and we we're we're starting",
      "offset": 776.079,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "to do more and more with semantic",
      "offset": 778.48,
      "duration": 4.919
    },
    {
      "lang": "en",
      "text": "retrieval and and uh with semantic",
      "offset": 780.16,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "clustering and uh and so they had a",
      "offset": 783.399,
      "duration": 4.041
    },
    {
      "lang": "en",
      "text": "really good embedding showing but then",
      "offset": 786.16,
      "duration": 2.64
    },
    {
      "lang": "en",
      "text": "most recently what's good about it is",
      "offset": 787.44,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "the quality. Yeah, it's it it's you know",
      "offset": 788.8,
      "duration": 5.159
    },
    {
      "lang": "en",
      "text": "embeddings are are kind of this",
      "offset": 791.92,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "fuzzy nebulous uh array of vectors but",
      "offset": 793.959,
      "duration": 6.361
    },
    {
      "lang": "en",
      "text": "um but but for our use cases we just",
      "offset": 798.16,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "found them to be uh more relevant",
      "offset": 800.32,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "retrieval. Yeah. Interesting. And you",
      "offset": 803.76,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "tested that pretty rigorously.",
      "offset": 805.519,
      "duration": 6.401
    },
    {
      "lang": "en",
      "text": "And um uh and so that that kind of first",
      "offset": 808.72,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "got Gemini or the like the the whole",
      "offset": 811.92,
      "duration": 5.359
    },
    {
      "lang": "en",
      "text": "Google AI offering on our radar, but",
      "offset": 814.56,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "then recently all their Gemini",
      "offset": 817.279,
      "duration": 3.521
    },
    {
      "lang": "en",
      "text": "announcements around their their models",
      "offset": 819.2,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "have just been really really impressive.",
      "offset": 820.8,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "Have you played with the prompt caching",
      "offset": 822.399,
      "duration": 5.761
    },
    {
      "lang": "en",
      "text": "or uh exec code execution? They have",
      "offset": 824.48,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "code execution. People don't understand",
      "offset": 828.16,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "this, but like in one shot they'll",
      "offset": 829.6,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "generate and run the code, not just give",
      "offset": 831.76,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "you the tool. So, we don't do any code",
      "offset": 833.519,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "execution, but we do we use prompt",
      "offset": 835.04,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "caching extensively. And in fact, I",
      "offset": 837.199,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "guess this is another trick like um or",
      "offset": 839.68,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "like a tip for people building on LMS.",
      "offset": 842.16,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "Um structure your prompts so that",
      "offset": 844.8,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "they're append only. So, a lot of people",
      "offset": 847.36,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "will use like a template library to like",
      "offset": 849.279,
      "duration": 2.641
    },
    {
      "lang": "en",
      "text": "have this prompt and then they'll kind",
      "offset": 850.8,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "of templatize names or data within the",
      "offset": 851.92,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "middle of the prompt. Don't do that. Uh",
      "offset": 853.76,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "describe the process that you want to",
      "offset": 855.68,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "happen and then uh and as part of this",
      "offset": 856.959,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "describe the input data and then append",
      "offset": 860,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "your data to the end. So the attention,",
      "offset": 862.399,
      "duration": 3.281
    },
    {
      "lang": "en",
      "text": "you know, the beginning and the end of a",
      "offset": 864.399,
      "duration": 3.281
    },
    {
      "lang": "en",
      "text": "prompt is classically more, you know, a",
      "offset": 865.68,
      "duration": 4.959
    },
    {
      "lang": "en",
      "text": "paid attention to. And so my worry is",
      "offset": 867.68,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "like by taking the really important",
      "offset": 870.639,
      "duration": 3.041
    },
    {
      "lang": "en",
      "text": "parts, the variables and putting them at",
      "offset": 872.32,
      "duration": 2.639
    },
    {
      "lang": "en",
      "text": "the very end, are you losing anything",
      "offset": 873.68,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "there? And have you tested that? Yeah.",
      "offset": 874.959,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "Um, I mean, I get the implication for",
      "offset": 877.76,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "caching is super. Yeah. But that's why",
      "offset": 879.279,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "you're doing it, right? It's it's",
      "offset": 881.76,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "motivated by caching. We we we calls to",
      "offset": 883.6,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "the LM, we try to maximize the prefix",
      "offset": 886.56,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "that we send to the LLM just so that you",
      "offset": 888.32,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "can benefit from caching. Yeah. maximize",
      "offset": 890.24,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "the shared prefix and yes and and as a",
      "offset": 892.16,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "consequence maximize caching. Okay. Um",
      "offset": 894.079,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "and but you're not financial quality",
      "offset": 896.8,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "trade-off by doing this technique. No.",
      "offset": 898.399,
      "duration": 5.321
    },
    {
      "lang": "en",
      "text": "And and and uh no and I I I especially",
      "offset": 900,
      "duration": 7.12
    },
    {
      "lang": "en",
      "text": "by appending the inputs to the end. I",
      "offset": 903.72,
      "duration": 5.32
    },
    {
      "lang": "en",
      "text": "think I think that's fine in terms of",
      "offset": 907.12,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "like not getting lost kind of in the",
      "offset": 909.04,
      "duration": 2.56
    },
    {
      "lang": "en",
      "text": "middle of things. Yeah. And it won't",
      "offset": 910.24,
      "duration": 2.32
    },
    {
      "lang": "en",
      "text": "have any effect on chain of thought",
      "offset": 911.6,
      "duration": 2.56
    },
    {
      "lang": "en",
      "text": "because you can still ask it to sequence",
      "offset": 912.56,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "its output in a chain of thought way.",
      "offset": 914.16,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "Yeah. Although the way we do chain of",
      "offset": 916.24,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "thought is also interesting. We like we",
      "offset": 917.839,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "rely heavily on structured output for",
      "offset": 920.72,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "for chain of thought like structured",
      "offset": 922.639,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "output a lot structured output is is",
      "offset": 924.959,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "probably one of the most underutilized",
      "offset": 927.839,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "uh tools that that when people think",
      "offset": 930.16,
      "duration": 5.119
    },
    {
      "lang": "en",
      "text": "about how to improve the output of of an",
      "offset": 932.639,
      "duration": 5.681
    },
    {
      "lang": "en",
      "text": "LLM in that like a lot of people think",
      "offset": 935.279,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "of structured output as just like a nice",
      "offset": 938.32,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "way to like JSON parse uh you know a",
      "offset": 939.6,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "response and just get like fields that",
      "offset": 941.76,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "they were expecting. But the really neat",
      "offset": 943.6,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "thing with structured output is you",
      "offset": 945.6,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "actually get to guide and force the LM",
      "offset": 947.519,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "to go in certain directions. And so like",
      "offset": 950.32,
      "duration": 5.519
    },
    {
      "lang": "en",
      "text": "for a chain of thought, you can your",
      "offset": 953.199,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "output schema if you're using structured",
      "offset": 955.839,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "output, you can say like analysis and",
      "offset": 958.079,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "then conclusion. And like in your",
      "offset": 960.24,
      "duration": 5.279
    },
    {
      "lang": "en",
      "text": "analysis, you can instruct it to do its",
      "offset": 962.24,
      "duration": 5.599
    },
    {
      "lang": "en",
      "text": "thinking and the structured output will",
      "offset": 965.519,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "guarantee that the the fields that get",
      "offset": 967.839,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "generated will be in the same order that",
      "offset": 970.639,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "you define them in the schema. So by",
      "offset": 971.839,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "forcing the analysis to be generated",
      "offset": 973.519,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "before the conclusion, you force the LM",
      "offset": 974.959,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "to do things in a certain order. One of",
      "offset": 977.279,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "the really cool things you can you can",
      "offset": 979.519,
      "duration": 2.32
    },
    {
      "lang": "en",
      "text": "do with that is if you really want the",
      "offset": 980.639,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "LM to go deep, your chain of thought can",
      "offset": 981.839,
      "duration": 5.521
    },
    {
      "lang": "en",
      "text": "start with like an initial thought and",
      "offset": 984.56,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "then you can say like okay revised",
      "offset": 987.36,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "thought. Okay, revise thought two,",
      "offset": 989.36,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "revise thought three. You can you can if",
      "offset": 991.04,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "if you want if you want the LM to have",
      "offset": 992.8,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "to like think about it answer for say 20",
      "offset": 994.8,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "steps you can force that just by",
      "offset": 996.959,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "specifying a schema that forces it to to",
      "offset": 999.199,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "think about that for you can't you",
      "offset": 1002.079,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "literally can't skip it because it's",
      "offset": 1003.44,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "forced in the that's part and one one",
      "offset": 1004.959,
      "duration": 5.601
    },
    {
      "lang": "en",
      "text": "interesting example um we can you show",
      "offset": 1007.6,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "us any of these prompts like an example",
      "offset": 1010.56,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "of either the backloading your variables",
      "offset": 1012.399,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "prompt like from your code. Yeah. Yeah.",
      "offset": 1014.56,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "Um well first let me show you like so in",
      "offset": 1016.399,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "in Logic we have like one of our",
      "offset": 1018.8,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "documents uh that that we use is like we",
      "offset": 1020.399,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "just send it a raw diff from git and",
      "offset": 1022.56,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "then we have it comment on all these",
      "offset": 1025.199,
      "duration": 3.281
    },
    {
      "lang": "en",
      "text": "different fields so like code complexity",
      "offset": 1027.12,
      "duration": 2.959
    },
    {
      "lang": "en",
      "text": "naming conventions code organization",
      "offset": 1028.48,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "testing so forth and so wait this is",
      "offset": 1030.079,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "this is a logic flow that you've built",
      "offset": 1033.52,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "for for analyzing code y and then you're",
      "offset": 1035.039,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "using it okay that's cool yeah so we we",
      "offset": 1037.6,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "build logic with logic yeah that makes",
      "offset": 1039.679,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "sense that's really cool and uh and if",
      "offset": 1041.36,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "we hop into some of our test cas you",
      "offset": 1043.839,
      "duration": 3.281
    },
    {
      "lang": "en",
      "text": "expose these recipes into a marketplace",
      "offset": 1045.28,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "or something eventually. Yes. Yeah,",
      "offset": 1047.12,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "we're like a template library is is on",
      "offset": 1048.48,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "our radar for sure because you could",
      "offset": 1050.24,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "create just a project for that. Yes.",
      "offset": 1051.44,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "Just related to coding. Yeah, that's",
      "offset": 1053.919,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "really cool. And um and so because we",
      "offset": 1055.36,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "rely on structured output even if the LM",
      "offset": 1057.84,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "has awesome and find security",
      "offset": 1059.44,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "vulnerabilities to just constantly",
      "offset": 1061.2,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "monitor the stuff that would be too",
      "offset": 1062.72,
      "duration": 6.4
    },
    {
      "lang": "en",
      "text": "arduous to do. Yeah. and and and so even",
      "offset": 1066.64,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "if the LM has nothing of importance to",
      "offset": 1069.12,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "say on a topic, we still force it to say",
      "offset": 1071.039,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "like nothing nothing significant here.",
      "offset": 1073.039,
      "duration": 6.241
    },
    {
      "lang": "en",
      "text": "Sure. Um but uh if so I'll show you",
      "offset": 1075.679,
      "duration": 5.601
    },
    {
      "lang": "en",
      "text": "there's like a Japanese train platform",
      "offset": 1079.28,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "operator. They have they have to do the",
      "offset": 1081.28,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "thing. Yes. Yeah. It's checklists. Yeah.",
      "offset": 1083.28,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "Um, one of the other fun one of the cool",
      "offset": 1086.32,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "things with with Logic is like it if we",
      "offset": 1087.919,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "have an idea of like if we if we just",
      "offset": 1089.84,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "want something powered by an LLM, it",
      "offset": 1091.36,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "it's lightweight enough now that we we",
      "offset": 1093.36,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "just kind of do it like if um like our",
      "offset": 1094.96,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "team doesn't write get commit messages",
      "offset": 1097.52,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "anymore. So I think right yeah uh so",
      "offset": 1099.6,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "like if I just run actually let me do no",
      "offset": 1101.679,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "verify because I don't I don't want to",
      "offset": 1104.64,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "run like our llinter and everything. Um",
      "offset": 1106.24,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "you'll see logic is writing your get",
      "offset": 1108.24,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "commit message. So we just took this",
      "offset": 1109.84,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "diff setting up the logic right now and",
      "offset": 1111.52,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "in a few seconds uh we'll get back uh",
      "offset": 1113.52,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "yeah so so I'm working on our handling",
      "offset": 1116.48,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "middleware and if you look one of the",
      "offset": 1118.16,
      "duration": 3.519
    },
    {
      "lang": "en",
      "text": "really cool things with this is like if",
      "offset": 1120,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "you look at our actual commits",
      "offset": 1121.679,
      "duration": 5.841
    },
    {
      "lang": "en",
      "text": "um everybody has standardized commits.",
      "offset": 1124.799,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "So like if I look at the description of",
      "offset": 1127.52,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "this you'll see we always have like a",
      "offset": 1128.96,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "top level summary bullets of of what",
      "offset": 1130.96,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "happened and all of our all of our",
      "offset": 1133.12,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "commits are standardized. That's so",
      "offset": 1135.12,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "cool. Um and so we don't have any just",
      "offset": 1136.4,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "like work in progress commits or any low",
      "offset": 1138.64,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "value commits. At the end of the quarter",
      "offset": 1140.559,
      "duration": 2.641
    },
    {
      "lang": "en",
      "text": "you can just you can have it do your uh",
      "offset": 1141.84,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "your quarterly reviews too. Yeah. So so",
      "offset": 1143.2,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "I do at the end of the month I actually",
      "offset": 1146.4,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "give it our git lo I I have a different",
      "offset": 1147.76,
      "duration": 3.279
    },
    {
      "lang": "en",
      "text": "workflow that gives us our git log and",
      "offset": 1149.44,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "summarize like everything that that we",
      "offset": 1151.039,
      "duration": 3.121
    },
    {
      "lang": "en",
      "text": "did. Yeah. But but you asked for an",
      "offset": 1152.32,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "actual example of let me show you one of",
      "offset": 1154.16,
      "duration": 7.6
    },
    {
      "lang": "en",
      "text": "one of my fun um uh bots that I actually",
      "offset": 1157.44,
      "duration": 6.72
    },
    {
      "lang": "en",
      "text": "published two years ago is my Pickhams",
      "offset": 1161.76,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "bot that uh picks teams and in the",
      "offset": 1164.16,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "ESPN's Pickhams game and actually need",
      "offset": 1167.52,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "to up I I modified the bot for this year",
      "offset": 1170.4,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "but I didn't publish it yet. Um did you",
      "offset": 1172.24,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "present this at AI Tinkers once? I did.",
      "offset": 1174.96,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "Yeah. An early like I think the second",
      "offset": 1177.12,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "AI tankers event Seattle I presented",
      "offset": 1179.44,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "this. So a couple years ago and then and",
      "offset": 1181.039,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "then you uh Okay. And so you've you've",
      "offset": 1182.72,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "kept it going. You collect a lot of data",
      "offset": 1184.559,
      "duration": 2.48
    },
    {
      "lang": "en",
      "text": "and now you've you've made improvements.",
      "offset": 1185.84,
      "duration": 2.719
    },
    {
      "lang": "en",
      "text": "Yeah. And so the first year that I ran",
      "offset": 1187.039,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "this um it was in the top 15% of all",
      "offset": 1188.559,
      "duration": 5.681
    },
    {
      "lang": "en",
      "text": "people on ESPN. Uh this year it was in",
      "offset": 1191.36,
      "duration": 8.319
    },
    {
      "lang": "en",
      "text": "the top 5%. And let me show you",
      "offset": 1194.24,
      "duration": 5.439
    },
    {
      "lang": "en",
      "text": "our chain of thought prompt. So you can",
      "offset": 1199.72,
      "duration": 4.52
    },
    {
      "lang": "en",
      "text": "actually see we have like this this",
      "offset": 1202.32,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "actually let me make this much bigger.",
      "offset": 1204.24,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "Um, so we have this this thought schema",
      "offset": 1206.72,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "that is like reflect on your current",
      "offset": 1209.2,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "approach. Yeah. Give it give it an award",
      "offset": 1211.12,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "and then determine a next step like do",
      "offset": 1213.6,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "you want to continue down this line of",
      "offset": 1215.28,
      "duration": 2.16
    },
    {
      "lang": "en",
      "text": "thought? Do you want to adjust it or do",
      "offset": 1216.32,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "you want to backtrack it? And then in",
      "offset": 1217.44,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "our actual schema so our our final",
      "offset": 1219.28,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "schema has um an analysis and a",
      "offset": 1222.16,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "conclusion like I mentioned before. But",
      "offset": 1225.2,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "if you look at our analysis schema,",
      "offset": 1226.88,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "you'll see initial thought, revised",
      "offset": 1228.96,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "thought, revised thought, revised",
      "offset": 1231.28,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "thought, and then final thought and then",
      "offset": 1232.4,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "final reflection. So we actually force",
      "offset": 1234,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "the LM to keep thinking and keep",
      "offset": 1236.159,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "revising. And if you if you look at the",
      "offset": 1238.4,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "output, you'll see you'll see be like,",
      "offset": 1240.799,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "&quot;Oh yeah, like this line of thinking is",
      "offset": 1242.88,
      "duration": 2.24
    },
    {
      "lang": "en",
      "text": "good, so I'm going to continue with it.&quot;",
      "offset": 1244,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "Oh, yeah. And and you and you you can",
      "offset": 1245.12,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "actually get a sense of its certainty in",
      "offset": 1247.36,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "its decision based on how many times it",
      "offset": 1249.52,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "continues versus adjust versus",
      "offset": 1251.039,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "backtrack. Yeah. Very cool. And this is",
      "offset": 1252.48,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "open source. Yeah. This would be a great",
      "offset": 1255.2,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "repo for anyone to study just on prompts",
      "offset": 1257.28,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "engineering obviously to look at the",
      "offset": 1259.12,
      "duration": 3.88
    },
    {
      "lang": "en",
      "text": "code and and Yeah, for sure. And",
      "offset": 1260.32,
      "duration": 6.88
    },
    {
      "lang": "en",
      "text": "um it yeah it this is uh I'll just kick",
      "offset": 1263,
      "duration": 5.4
    },
    {
      "lang": "en",
      "text": "this off. So like it's it's fully",
      "offset": 1267.2,
      "duration": 2.959
    },
    {
      "lang": "en",
      "text": "autonomous. It just like ops up on ESPN.",
      "offset": 1268.4,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "It'll start crawling news articles.",
      "offset": 1270.159,
      "duration": 3.041
    },
    {
      "lang": "en",
      "text": "It'll figure out Oh yeah what what",
      "offset": 1271.44,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "matches are in play. We're just past the",
      "offset": 1273.2,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "regular season right now. Uh so they",
      "offset": 1275.28,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "won't it won't do anything too",
      "offset": 1277.2,
      "duration": 2.08
    },
    {
      "lang": "en",
      "text": "interesting. I never told you this",
      "offset": 1278.4,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "before. My first job computer science",
      "offset": 1279.28,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "grad straight into espn.com fantasy",
      "offset": 1281.12,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "games team built the live draft Java",
      "offset": 1284.48,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "outlet. But I I know some people are",
      "offset": 1287.2,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "still working there who can actually I",
      "offset": 1289.36,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "can put this on their radar. Um funny. I",
      "offset": 1291.52,
      "duration": 5.279
    },
    {
      "lang": "en",
      "text": "didn't know that. Oh yeah. Yeah. And so",
      "offset": 1294,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "now it'll it'll kind of go through a",
      "offset": 1296.799,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "bunch of different articles. The the the",
      "offset": 1298.4,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "prompt is long enough now that that it",
      "offset": 1300.48,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "it takes a couple minutes to to run",
      "offset": 1302.799,
      "duration": 2.961
    },
    {
      "lang": "en",
      "text": "through everything, but you can see",
      "offset": 1304.48,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "initial thought, you know, at least for",
      "offset": 1305.76,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "for this uh an analyzing the article.",
      "offset": 1307.52,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "Can I can I read a couple of these? So",
      "offset": 1309.919,
      "duration": 2.401
    },
    {
      "lang": "en",
      "text": "like article focus on Kansas City Chiefs",
      "offset": 1310.88,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "primarily Travisky's comments. And then",
      "offset": 1312.32,
      "duration": 2.719
    },
    {
      "lang": "en",
      "text": "here it's like it looks like it's",
      "offset": 1313.84,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "repeating. No, it looks like it's didn't",
      "offset": 1315.039,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "really It ran out of it ran out of",
      "offset": 1316.88,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "revision thoughts. It just sort of",
      "offset": 1318.88,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "you've tapped out the intelligence of",
      "offset": 1320.32,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "this guy, right? So, yeah. So, and for",
      "offset": 1322,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "the for the news analysis, it tends to",
      "offset": 1325.039,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "be pretty straightforward because all",
      "offset": 1326.799,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "we're asking it to do for the news",
      "offset": 1329.12,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "analysis is determine like is this for",
      "offset": 1330.159,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "fantasy, is this for college, or is this",
      "offset": 1332.08,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "for NFL? So, it tends to uh get to the",
      "offset": 1333.36,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "conclusion pretty quickly. Yeah. Okay.",
      "offset": 1336.559,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "Um, but when it gets Are you doing a",
      "offset": 1338.48,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "oneshot of like grab all the news and",
      "offset": 1340,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "then just do this chain of thought or do",
      "offset": 1341.44,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "you cycle back and go like because you",
      "offset": 1343.12,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "could ideate questions and then go back",
      "offset": 1345.36,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "to the web and get more answers, right?",
      "offset": 1346.799,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "Yeah. No, we so it's it's a it's a fixed",
      "offset": 1349.12,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "workflow. Okay. Where we we hop on uh",
      "offset": 1352.159,
      "duration": 5.841
    },
    {
      "lang": "en",
      "text": "like ESPN's news article uh like the",
      "offset": 1355.6,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "main news site, we grab the like top 20",
      "offset": 1358,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "headlines or something and then we kind",
      "offset": 1360.96,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "of fan that out into a bunch of parallel",
      "offset": 1362.48,
      "duration": 6.88
    },
    {
      "lang": "en",
      "text": "analysis. And we we have um let's see I",
      "offset": 1364.4,
      "duration": 7.36
    },
    {
      "lang": "en",
      "text": "I call them in this repo I call them",
      "offset": 1369.36,
      "duration": 4.439
    },
    {
      "lang": "en",
      "text": "tools",
      "offset": 1371.76,
      "duration": 5.919
    },
    {
      "lang": "en",
      "text": "um but uh let me make this bigger. We",
      "offset": 1373.799,
      "duration": 6.521
    },
    {
      "lang": "en",
      "text": "have a a news analyst tool. Mhm. Which",
      "offset": 1377.679,
      "duration": 5.681
    },
    {
      "lang": "en",
      "text": "is just you know basically figure out is",
      "offset": 1380.32,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "this related to the NFL or not and and",
      "offset": 1383.36,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "and we filter out anything that's not",
      "offset": 1384.88,
      "duration": 3.279
    },
    {
      "lang": "en",
      "text": "related to the NFL. Okay. And then we",
      "offset": 1386.4,
      "duration": 5.759
    },
    {
      "lang": "en",
      "text": "have the uh the predict the winner tool",
      "offset": 1388.159,
      "duration": 6.561
    },
    {
      "lang": "en",
      "text": "um where we we actually give it the",
      "offset": 1392.159,
      "duration": 6.801
    },
    {
      "lang": "en",
      "text": "matchups. We uh inject the article like",
      "offset": 1394.72,
      "duration": 6.64
    },
    {
      "lang": "en",
      "text": "article summaries and then we have it",
      "offset": 1398.96,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "choose the the winning team and the way",
      "offset": 1401.36,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "we structure our prompts like the for",
      "offset": 1404.159,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "the for the winner tool we actually like",
      "offset": 1406,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "this is the schema that we define for",
      "offset": 1408.96,
      "duration": 4.959
    },
    {
      "lang": "en",
      "text": "that tool. We unconditionally wrap these",
      "offset": 1410.4,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "schemas in that chain of thought schema",
      "offset": 1413.919,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "that I showed you earlier. So like the",
      "offset": 1416.08,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "complexity of that whole chain of",
      "offset": 1418.4,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "thought process is just kind of confined",
      "offset": 1419.919,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "to one file and then anywhere else we",
      "offset": 1421.84,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "call the LLM we just unconditionally",
      "offset": 1423.919,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "wrap this new schema in that super",
      "offset": 1425.84,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "complex. Got it. Okay, that makes sense.",
      "offset": 1428.08,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "So you don't have to like rele it over.",
      "offset": 1430.08,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "Yeah. Are you using any frameworks for",
      "offset": 1431.919,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "like the work the execution of these",
      "offset": 1434,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "workflows uh or is it just homegrown for",
      "offset": 1435.6,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "this repo? Uh homegrown for this repo.",
      "offset": 1438.159,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "Have you tried a gentic tool like crew",
      "offset": 1440.159,
      "duration": 5.601
    },
    {
      "lang": "en",
      "text": "AI, Lang Graph, etc.? Yeah, especially",
      "offset": 1442.96,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "at Brex, I spent a bunch of time with",
      "offset": 1445.76,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "Langraph. Um, and this was, you know,",
      "offset": 1447.28,
      "duration": 7.12
    },
    {
      "lang": "en",
      "text": "the summer of 23, so everything was was",
      "offset": 1451.2,
      "duration": 6.479
    },
    {
      "lang": "en",
      "text": "super nent, but um y uh some time with",
      "offset": 1454.4,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "Llama Index. Yeah.",
      "offset": 1457.679,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "Dabbled briefly with Crew. Um I was just",
      "offset": 1460.24,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "looking at the like the small agents",
      "offset": 1463.279,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "thing that Hugging Face released.",
      "offset": 1464.799,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "What did you what were your conclusions",
      "offset": 1466.72,
      "duration": 2.64
    },
    {
      "lang": "en",
      "text": "after looking at those others on that",
      "offset": 1468.159,
      "duration": 3.561
    },
    {
      "lang": "en",
      "text": "one? I don't know if if",
      "offset": 1469.36,
      "duration": 6.559
    },
    {
      "lang": "en",
      "text": "uh if I I find the abstractions to be",
      "offset": 1471.72,
      "duration": 6.52
    },
    {
      "lang": "en",
      "text": "unnecessary. I I find that the APIs that",
      "offset": 1475.919,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "the foundation models have exposed",
      "offset": 1478.24,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "around like tool use and structured",
      "offset": 1480,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "output and everything are kind of",
      "offset": 1481.52,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "sufficiently high level that not getting",
      "offset": 1483.919,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "a lot of value from the genic",
      "offset": 1486.32,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "frameworks. Sure. Um do you ever worry",
      "offset": 1488.559,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "that as the base models get better that",
      "offset": 1490.48,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "using a framework might hold you back?",
      "offset": 1492,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "Yeah. Yeah. because you're kind of",
      "offset": 1494.24,
      "duration": 4.52
    },
    {
      "lang": "en",
      "text": "forced to uh",
      "offset": 1495.919,
      "duration": 7.281
    },
    {
      "lang": "en",
      "text": "uh use what whatever uh abstractions",
      "offset": 1498.76,
      "duration": 7
    },
    {
      "lang": "en",
      "text": "that that framework gives you. And I I",
      "offset": 1503.2,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "guess from from a certain lens you could",
      "offset": 1505.76,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "argue that framework will abstract away",
      "offset": 1507.039,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "the nuances between different models but",
      "offset": 1508.64,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "at least for the things that I'm doing I",
      "offset": 1511.6,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "uh I want to be a little bit closer to",
      "offset": 1514.88,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "where the foundation models are. I all",
      "offset": 1516.72,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "of the last two years certainly everyone",
      "offset": 1518.48,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "I talked to said yeah of course I've",
      "offset": 1520.64,
      "duration": 3.039
    },
    {
      "lang": "en",
      "text": "experimented with different models",
      "offset": 1522.559,
      "duration": 2.161
    },
    {
      "lang": "en",
      "text": "coming out because it's fun and",
      "offset": 1523.679,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "interesting but yeah I'm using GPT4 for",
      "offset": 1524.72,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "everything because I want the best",
      "offset": 1527.279,
      "duration": 2.321
    },
    {
      "lang": "en",
      "text": "quality I don't care about cost right",
      "offset": 1528.32,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "now whatever do you think in 2025 I",
      "offset": 1529.6,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "think the two forces are going to be",
      "offset": 1532.159,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "actual alternatives that are good",
      "offset": 1534.08,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "quality on on different aspects or speed",
      "offset": 1536.159,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "performance needs as we get these",
      "offset": 1538.799,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "systems scaled or it could be like the",
      "offset": 1540.08,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "finally differentiated features that are",
      "offset": 1542.4,
      "duration": 3.279
    },
    {
      "lang": "en",
      "text": "being baked in you know like I mentioned",
      "offset": 1544.24,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "Gemini has code execution ution baked in",
      "offset": 1545.679,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "or grounding. I mean, Google's pretty",
      "offset": 1548.64,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "good at search. I think the grounding is",
      "offset": 1550.48,
      "duration": 2.64
    },
    {
      "lang": "en",
      "text": "probably pretty good. I've played with",
      "offset": 1551.84,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "it. Pretty gamechanging. Yeah. You know,",
      "offset": 1553.12,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "if you need to bring in data, you can",
      "offset": 1555.039,
      "duration": 3.281
    },
    {
      "lang": "en",
      "text": "probably rip out a lot of code, just use",
      "offset": 1556.799,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "grounding. But that's going to tie you",
      "offset": 1558.32,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "to Gemini to some extent, right? Do you",
      "offset": 1559.679,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "think this year we're going to see we're",
      "offset": 1561.52,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "going to see the similarly the agentic",
      "offset": 1564.08,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "frameworks actually provide enough value",
      "offset": 1565.52,
      "duration": 2.84
    },
    {
      "lang": "en",
      "text": "where you're like, yeah, I'm not going",
      "offset": 1567.12,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "to there's so much value like your",
      "offset": 1568.36,
      "duration": 4.04
    },
    {
      "lang": "en",
      "text": "alternative answer could be, yeah,",
      "offset": 1570.64,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "there's so much value in these aentric",
      "offset": 1572.4,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "platforms that you'd be dumb not to use",
      "offset": 1574,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "them. I haven't heard a single person",
      "offset": 1575.84,
      "duration": 5.24
    },
    {
      "lang": "en",
      "text": "say that. When will we hear that?",
      "offset": 1577.12,
      "duration": 7.52
    },
    {
      "lang": "en",
      "text": "Oh, that's a good question. Um, I don't",
      "offset": 1581.08,
      "duration": 5.24
    },
    {
      "lang": "en",
      "text": "know that we'll we may never hear that.",
      "offset": 1584.64,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "Uh but",
      "offset": 1586.32,
      "duration": 5.959
    },
    {
      "lang": "en",
      "text": "it seems",
      "offset": 1589.76,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "like everybody quickly reaches feature",
      "offset": 1592.279,
      "duration": 7.241
    },
    {
      "lang": "en",
      "text": "parody as soon as one like Gemini,",
      "offset": 1595.44,
      "duration": 6.479
    },
    {
      "lang": "en",
      "text": "OpenAI, Anthropic,",
      "offset": 1599.52,
      "duration": 4.039
    },
    {
      "lang": "en",
      "text": "um",
      "offset": 1601.919,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "uh I guess AWS is trying to throw in",
      "offset": 1603.559,
      "duration": 5.48
    },
    {
      "lang": "en",
      "text": "with Nova now. U that force that might",
      "offset": 1606,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "bring you is not going to be a problem",
      "offset": 1609.039,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "because they'll just yeah parody it. And",
      "offset": 1610.32,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "it's it's very similar to well adjacent",
      "offset": 1612.96,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "but like we have a and this was true at",
      "offset": 1615.6,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "Brex as well but like we have a policy",
      "offset": 1617.84,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "of no fine-tuning uh which is which is",
      "offset": 1619.36,
      "duration": 6.4
    },
    {
      "lang": "en",
      "text": "kind of controversial. Yeah. Or it can",
      "offset": 1623.039,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "be controversial in in some circles.",
      "offset": 1625.76,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "There's a my next guest are only in on",
      "offset": 1627.84,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "fine-tuning. Yes. Yeah. So just just as",
      "offset": 1630.799,
      "duration": 6.281
    },
    {
      "lang": "en",
      "text": "a matter of policy uh there's",
      "offset": 1634.08,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "there's tons of reasons but the big ones",
      "offset": 1637.08,
      "duration": 6.36
    },
    {
      "lang": "en",
      "text": "are for well for logic one the user",
      "offset": 1639.52,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "experience we're trying to give to a",
      "offset": 1643.44,
      "duration": 2.479
    },
    {
      "lang": "en",
      "text": "user where they give us a document and",
      "offset": 1644.64,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "we give them back an API seconds later",
      "offset": 1645.919,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "that works we just don't have any data",
      "offset": 1647.52,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "to fine tune on like there's nothing to",
      "offset": 1649.6,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "fine tune on we we just need that to",
      "offset": 1650.96,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "work out of the gate um wait you could",
      "offset": 1652.4,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "find tune on the interim steps of like",
      "offset": 1655.12,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "how do you go from this to a an API spec",
      "offset": 1656.799,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "in whatever right or schema right you",
      "offset": 1659.52,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "could you could fine on that stuff. The",
      "offset": 1661.76,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "internal workflows that we do to derive",
      "offset": 1663.84,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "artifacts certainly we could we could",
      "offset": 1665.84,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "fine-tune. Uh but but you you're saying",
      "offset": 1667.279,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "to do that because it's an optimization.",
      "offset": 1670.64,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "We're too young to optimize. Is that the",
      "offset": 1672,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "driver?",
      "offset": 1674.32,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "Partially. Um there you could fine-tune",
      "offset": 1676.399,
      "duration": 6.4
    },
    {
      "lang": "en",
      "text": "on very narrow specific Well, what is",
      "offset": 1680.32,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "your reason to not fine-tune when you're",
      "offset": 1682.799,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "Yeah. So like you risk comingling data.",
      "offset": 1684.48,
      "duration": 5.919
    },
    {
      "lang": "en",
      "text": "So like you you if you're if you're",
      "offset": 1688.399,
      "duration": 3.121
    },
    {
      "lang": "en",
      "text": "going to fine-tune, you really want to",
      "offset": 1690.399,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "fine-tune on high quality synthetic data",
      "offset": 1691.52,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "because if you especially if you're",
      "offset": 1694.24,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "dealing with customer data, um you don't",
      "offset": 1695.6,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "want customer you don't want it to",
      "offset": 1697.84,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "accidentally be regurgitated. Uh okay,",
      "offset": 1699.36,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "you have concerns about incorporating",
      "offset": 1702,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "data that can be leaked, right? So then",
      "offset": 1703.6,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "you need so then you need like good",
      "offset": 1705.36,
      "duration": 2.64
    },
    {
      "lang": "en",
      "text": "synthetic data and you lose visibility",
      "offset": 1706.48,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "once you fine-tuned, right? Once you",
      "offset": 1708,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "give once you fine-tune on a piece of",
      "offset": 1709.919,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "data, you never know how that will be",
      "offset": 1711.52,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "used or surfaced again, right? Uh but",
      "offset": 1713.279,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "the the bigger thing or maybe the",
      "offset": 1715.679,
      "duration": 5.441
    },
    {
      "lang": "en",
      "text": "biggest thing for for us is typically",
      "offset": 1717.919,
      "duration": 5.841
    },
    {
      "lang": "en",
      "text": "models that you can fine-tune tend to be",
      "offset": 1721.12,
      "duration": 5.279
    },
    {
      "lang": "en",
      "text": "a couple months behind uh the kind of",
      "offset": 1723.76,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "the state-of-the-art. Makes sense. Um,",
      "offset": 1726.399,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "so like when uh you know when when 35",
      "offset": 1728.24,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "Turbo came out, you couldn't fine-tune",
      "offset": 1731.6,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "it for like until like 6 months after it",
      "offset": 1733.44,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "came out and then every So if you want",
      "offset": 1735.52,
      "duration": 5.519
    },
    {
      "lang": "en",
      "text": "to be at the the bleeding edge, but also",
      "offset": 1737.44,
      "duration": 4.959
    },
    {
      "lang": "en",
      "text": "if you want to do like any kind of AB",
      "offset": 1741.039,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "testing between prompts, um, uh, you",
      "offset": 1742.399,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "need to be able to just like dynamically",
      "offset": 1745.84,
      "duration": 4.68
    },
    {
      "lang": "en",
      "text": "call out to different models. And then",
      "offset": 1747.039,
      "duration": 5.921
    },
    {
      "lang": "en",
      "text": "the maybe this is actually the the real",
      "offset": 1750.52,
      "duration": 5.32
    },
    {
      "lang": "en",
      "text": "high order bit. The inner loop uh like",
      "offset": 1752.96,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "your your dev inner loop just becomes so",
      "offset": 1755.84,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "much slower because every time you",
      "offset": 1758,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "change your prompt you now need to you",
      "offset": 1759.36,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "know re rerun the fine tuning process",
      "offset": 1761.919,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "and now you're kind of in the the",
      "offset": 1764.159,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "previous era of like classical data",
      "offset": 1766.32,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "science models where like you make some",
      "offset": 1768.159,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "tweaks and then you go it's like oh it's",
      "offset": 1769.76,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "compiling throwing wait for those yeah",
      "offset": 1771.52,
      "duration": 5.759
    },
    {
      "lang": "en",
      "text": "team to like finish or flow to finish.",
      "offset": 1775.12,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "So there's so I don't know if there's",
      "offset": 1777.279,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "any individual uh like killer reason",
      "offset": 1778.64,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "why. Yeah. Okay. It sounds like yeah the",
      "offset": 1781.6,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "strategy reason for now would be like",
      "offset": 1784.08,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "that you're then latching on to",
      "offset": 1785.6,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "foundation models that are behind the",
      "offset": 1788,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "curve and you're picking up this",
      "offset": 1789.279,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "operational dev maintenance task that",
      "offset": 1791.679,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "slows you down. Yeah. And you need",
      "offset": 1794.559,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "velocity matters. Okay. Okay. That's",
      "offset": 1796.159,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "cool. Those are strong arguments. Um",
      "offset": 1798.559,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "okay. Interesting. So all right. Thanks",
      "offset": 1800.799,
      "duration": 3.281
    },
    {
      "lang": "en",
      "text": "for showing us this this and and so the",
      "offset": 1802.159,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "performance went this year to what? Oh,",
      "offset": 1804.08,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "top top 5%. And uh yeah, and so let's",
      "offset": 1806.24,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "see if we can um Well, maybe it's not",
      "offset": 1810.32,
      "duration": 2.959
    },
    {
      "lang": "en",
      "text": "worth doesn't that I don't know about",
      "offset": 1812.08,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "the I'm not I'm not up on it, but like",
      "offset": 1813.279,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "aren't there marketplaces where if",
      "offset": 1815.52,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "you're top 5% you can make some money",
      "offset": 1816.88,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "like maybe I'm not a I'm not a sports",
      "offset": 1818.88,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "gambler. I'm not even So here's the",
      "offset": 1820.88,
      "duration": 2.24
    },
    {
      "lang": "en",
      "text": "funny thing. I'm not even really a",
      "offset": 1822.08,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "sports person. Like like data is amazing",
      "offset": 1823.12,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "in sports. One of the things I I",
      "offset": 1825.279,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "literally just learned is that they play",
      "offset": 1826.64,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "football on Christmas and so typically I",
      "offset": 1828.399,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "run this bot on Thursdays because I",
      "offset": 1830.88,
      "duration": 3.519
    },
    {
      "lang": "en",
      "text": "wanted to have like the latest news and",
      "offset": 1832.64,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "Christmas was on a Wednesday this year.",
      "offset": 1834.399,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "And so there were just three games that",
      "offset": 1836.24,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "I didn't put any picks in for cuz I I",
      "offset": 1837.84,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "didn't know football was played on",
      "offset": 1840.32,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "Christmas cuz that's that but that kind",
      "offset": 1841.6,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "of conveys how little I know about",
      "offset": 1843.84,
      "duration": 3.88
    },
    {
      "lang": "en",
      "text": "football but I was still top 5% of",
      "offset": 1845.12,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "uh players. So that's amazing. Oh,",
      "offset": 1847.72,
      "duration": 6.199
    },
    {
      "lang": "en",
      "text": "that's really cool. Okay. Um really",
      "offset": 1850.72,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "cool. Well, maybe we could turn to our",
      "offset": 1853.919,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "So, we talked about logic. Um, you know,",
      "offset": 1855.36,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "I just want to go to the high level. So,",
      "offset": 1857.12,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "SOP to an API in the interim, you do",
      "offset": 1858.64,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "schemas, you generate the test, you run",
      "offset": 1861.6,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "it. There's some visibility you may give",
      "offset": 1863.2,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "people who are using it. Now, they're",
      "offset": 1864.72,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "using it. They can modify and tweak it",
      "offset": 1866.24,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "and you're keeping things up to date.",
      "offset": 1867.84,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "The next logical thing would be like,",
      "offset": 1869.679,
      "duration": 3.041
    },
    {
      "lang": "en",
      "text": "not only do I give you my SAP and my",
      "offset": 1871.2,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "data, but it gives you tools and I give",
      "offset": 1872.72,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "you my APIs and now maybe you can do",
      "offset": 1874.399,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "some work for me. And that's got to be",
      "offset": 1876.08,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "coming, right? That's that's exciting.",
      "offset": 1877.76,
      "duration": 3.279
    },
    {
      "lang": "en",
      "text": "Yeah. Absolutely. And and then you can",
      "offset": 1879.6,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "provide tools, too. Yeah. Right. because",
      "offset": 1881.039,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "there's lots of things that I might want",
      "offset": 1883.44,
      "duration": 2.959
    },
    {
      "lang": "en",
      "text": "to do that are like all this browser use",
      "offset": 1884.399,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "stuff is interesting, but at the end of",
      "offset": 1886.399,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "the day, like there's a lot of work",
      "offset": 1888.559,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "there to get over captions and to make",
      "offset": 1889.6,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "it scale and and and like if you",
      "offset": 1892,
      "duration": 5.279
    },
    {
      "lang": "en",
      "text": "incorporated that into here, so my SOP",
      "offset": 1894.48,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "needs a human to use a browser, that's",
      "offset": 1897.279,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "like all of my SOPs. Yes. Right. Yeah.",
      "offset": 1899.039,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "And you could actually use that tool",
      "offset": 1901.36,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "internally, that's pretty powerful.",
      "offset": 1902.559,
      "duration": 3.281
    },
    {
      "lang": "en",
      "text": "Yeah. And so so we we're we're starting",
      "offset": 1904.08,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "to get into some of that. um like third",
      "offset": 1905.84,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "the broader umbrella is like third party",
      "offset": 1908.799,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "integrations but we we we we call it",
      "offset": 1910.399,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "tools and triggers. So like external",
      "offset": 1912.96,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "systems that can trigger a document and",
      "offset": 1914.64,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "then external systems that a document",
      "offset": 1916,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "can interact with and uh and so that",
      "offset": 1917.76,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "that could be anything from Slack to",
      "offset": 1921.12,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "browsing the web. We we just started",
      "offset": 1923.36,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "doing some like if you give us URLs that",
      "offset": 1925.919,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "match certain media types like PDFs or",
      "offset": 1927.84,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "images, we'll crawl those and we'll",
      "offset": 1929.76,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "incorporate those. Is that just on the",
      "offset": 1931.2,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "SAP creation side or is that also on the",
      "offset": 1932.88,
      "duration": 3.519
    },
    {
      "lang": "en",
      "text": "API usage side? On the input side. Yeah.",
      "offset": 1934.64,
      "duration": 3.279
    },
    {
      "lang": "en",
      "text": "Okay. If you send us a link to a PDF,",
      "offset": 1936.399,
      "duration": 3.041
    },
    {
      "lang": "en",
      "text": "we'll we'll crawl it. Okay. Cuz I got I",
      "offset": 1937.919,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "got a stream of like invoices, PDFs that",
      "offset": 1939.44,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "are coming in. I This is Okay. Yeah. So",
      "offset": 1941.6,
      "duration": 4.439
    },
    {
      "lang": "en",
      "text": "like email email thread uh specifically",
      "offset": 1943.519,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "invoice like invoice email threads is",
      "offset": 1946.039,
      "duration": 3.961
    },
    {
      "lang": "en",
      "text": "something where that we work with",
      "offset": 1948.559,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "somebody on on doing. Okay. Yeah. I",
      "offset": 1950,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "guess email threads is a good invoices",
      "offset": 1952.48,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "and PDFs is a good What are some of",
      "offset": 1954.399,
      "duration": 3.721
    },
    {
      "lang": "en",
      "text": "those other buckets? video, uh, social",
      "offset": 1955.84,
      "duration": 6.319
    },
    {
      "lang": "en",
      "text": "media data and and, um, uh, like one of",
      "offset": 1958.12,
      "duration": 6.039
    },
    {
      "lang": "en",
      "text": "the interesting use cases that somebody",
      "offset": 1962.159,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "is using this for is forensics analysis",
      "offset": 1964.159,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "on on, uh, fraudulent invoices. So,",
      "offset": 1966,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "giving us like a new invoice and then",
      "offset": 1968.559,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "historical invoices and and they have",
      "offset": 1970.08,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "like a list of tells that they look for.",
      "offset": 1971.76,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "Oh, that's so cool. And um, yeah, that",
      "offset": 1973.2,
      "duration": 5.599
    },
    {
      "lang": "en",
      "text": "was a Oh, I've I've run of that SOP in a",
      "offset": 1975.919,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "e-commerce business and and it's",
      "offset": 1978.799,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "specifically with chargebacks and",
      "offset": 1980.24,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "fighting the chargeback. You have to",
      "offset": 1981.919,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "gather data, do some forensics, and then",
      "offset": 1983.679,
      "duration": 2.561
    },
    {
      "lang": "en",
      "text": "if you present it, you can get the",
      "offset": 1985.12,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "money. Yeah. And as a vendor, if you",
      "offset": 1986.24,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "lose that, you're going to lose 50 bucks",
      "offset": 1988.32,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "as a fee. So, they're doubly expensive.",
      "offset": 1990.08,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "You've already lost the product. Yeah.",
      "offset": 1992.72,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "You know, you're never going to see the",
      "offset": 1994.559,
      "duration": 2.24
    },
    {
      "lang": "en",
      "text": "money, but then you have to pay this",
      "offset": 1995.6,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "this chargeback fee on top of that. So,",
      "offset": 1996.799,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "it's it's a So, you know, it's to me",
      "offset": 1999.2,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "it's just a data problem because if you",
      "offset": 2001.679,
      "duration": 3.041
    },
    {
      "lang": "en",
      "text": "can gather enough of this other",
      "offset": 2003.12,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "corrupting data that this person scammed",
      "offset": 2004.72,
      "duration": 2.959
    },
    {
      "lang": "en",
      "text": "you, then the credit card company might",
      "offset": 2006.48,
      "duration": 3.039
    },
    {
      "lang": "en",
      "text": "side with you. But, it's a lot of it's",
      "offset": 2007.679,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "very specialized work and it's kind of",
      "offset": 2009.519,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "irregular, right? and and and the",
      "offset": 2011.36,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "specialized nature of the work also like",
      "offset": 2012.88,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "the the threat landscape can change",
      "offset": 2014.24,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "fairly rapidly and the the you know the",
      "offset": 2015.84,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "partner that we're working with this on",
      "offset": 2018,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "they they had their engineering team",
      "offset": 2019.679,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "build something inhouse that worked",
      "offset": 2021.76,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "quite well but the their frustration",
      "offset": 2024.08,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "with it was that their fraud ops team",
      "offset": 2026.48,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "who are the actual experts in how money",
      "offset": 2028.32,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "moves around the world would have to",
      "offset": 2030.08,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "schedule with engineering every time",
      "offset": 2031.919,
      "duration": 2.401
    },
    {
      "lang": "en",
      "text": "they wanted to update that process",
      "offset": 2033.12,
      "duration": 2.399
    },
    {
      "lang": "en",
      "text": "because like all this was checked in the",
      "offset": 2034.32,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "code and managed in code and so we",
      "offset": 2035.519,
      "duration": 2.961
    },
    {
      "lang": "en",
      "text": "allowed them to do like an aversion of",
      "offset": 2037.12,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "control where the fraud ops team can now",
      "offset": 2038.48,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "just update the document we'll run the",
      "offset": 2040.32,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "back test for them uh and and when",
      "offset": 2042,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "they're confident that it won't break",
      "offset": 2044.08,
      "duration": 2.56
    },
    {
      "lang": "en",
      "text": "anything, just publish it and and it",
      "offset": 2045.12,
      "duration": 5.279
    },
    {
      "lang": "en",
      "text": "goes live. Um we have talked a lot about",
      "offset": 2046.64,
      "duration": 5.519
    },
    {
      "lang": "en",
      "text": "uh prompt engineering and within the",
      "offset": 2050.399,
      "duration": 3.121
    },
    {
      "lang": "en",
      "text": "context of the product and the side",
      "offset": 2052.159,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "project. You actually wrote the the",
      "offset": 2053.52,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "prompt engineering guide while you were",
      "offset": 2056.079,
      "duration": 2.56
    },
    {
      "lang": "en",
      "text": "at Brett. It became quite popular,",
      "offset": 2057.359,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "right? Could you show people that? Yeah.",
      "offset": 2058.639,
      "duration": 2.48
    },
    {
      "lang": "en",
      "text": "And just quickly give them a quick",
      "offset": 2060.079,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "overview just I want people to see this",
      "offset": 2061.119,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "resource and have it in case they uh",
      "offset": 2063.28,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "haven't seen it before. Yeah. So tell us",
      "offset": 2065.04,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "the backstory. So the the backstory is",
      "offset": 2066.72,
      "duration": 7.04
    },
    {
      "lang": "en",
      "text": "it was early 2023. Uh LLMs were were",
      "offset": 2070,
      "duration": 5.919
    },
    {
      "lang": "en",
      "text": "just kind of becoming hot on the scene",
      "offset": 2073.76,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "and uh we basically it fell on me. I I I",
      "offset": 2075.919,
      "duration": 5.361
    },
    {
      "lang": "en",
      "text": "ran a team at Brex called the Office of",
      "offset": 2080,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "the CTO and it fell on me to kind of get",
      "offset": 2081.28,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "our 600 engineers to be thinking about",
      "offset": 2083.919,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "LMS and how to work with them reliably",
      "offset": 2086.72,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "and why what do they do? What can't they",
      "offset": 2088.399,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "do? and um and I have a bit of a history",
      "offset": 2090.56,
      "duration": 6.079
    },
    {
      "lang": "en",
      "text": "in NLP and language modeling and so I",
      "offset": 2093.919,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "just kind of did a massive brain dump of",
      "offset": 2096.639,
      "duration": 2.881
    },
    {
      "lang": "en",
      "text": "everything I knew about language",
      "offset": 2098.4,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "modeling at that at that point in time.",
      "offset": 2099.52,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "Yeah, that's really cool. I wonder like",
      "offset": 2101.68,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "how has this evolved like versus two",
      "offset": 2104.16,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "years ago, what in here is still",
      "offset": 2106.16,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "relevant, what isn't. Yeah. So, it's",
      "offset": 2107.359,
      "duration": 3.121
    },
    {
      "lang": "en",
      "text": "it's I haven't looked at this for a",
      "offset": 2109.04,
      "duration": 2.64
    },
    {
      "lang": "en",
      "text": "minute and it's it's interesting seeing",
      "offset": 2110.48,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "that the two-year uh time stamp. Um the",
      "offset": 2111.68,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "I mean if if you if you want the first",
      "offset": 2115.359,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "section I think is it should be like",
      "offset": 2118.16,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "required reading for anybody working",
      "offset": 2119.68,
      "duration": 2.56
    },
    {
      "lang": "en",
      "text": "with LLM. Like knowing knowing the",
      "offset": 2120.88,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "history of how how we got to where we",
      "offset": 2122.24,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "are is is important. It will actually be",
      "offset": 2124.32,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "really good for your mental model of why",
      "offset": 2127.44,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "certain things work well and why not. Um",
      "offset": 2129.92,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "I'm just going to skim through this.",
      "offset": 2133.04,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "Hidden prompts are still very much a big",
      "offset": 2135.2,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "well now now we we would call them a",
      "offset": 2137.52,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "system prompt. Um, but like not one of",
      "offset": 2139.2,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "the things I warn a lot in here is like",
      "offset": 2142,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "don't embed secrets in your system",
      "offset": 2143.76,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "prompt. Anything that the LLM sees, you",
      "offset": 2145.28,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "should assume the user will see. That",
      "offset": 2147.44,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "still holds true. Jbreaks are still very",
      "offset": 2149.599,
      "duration": 6.321
    },
    {
      "lang": "en",
      "text": "much a problem. Y um",
      "offset": 2152,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "do you do you find yourself um I mean",
      "offset": 2155.92,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "the the tricky thing with secrets is",
      "offset": 2157.92,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "like sometimes that secret context is",
      "offset": 2159.68,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "necessary to get a good answer, but I",
      "offset": 2162.16,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "don't want to share the secret context",
      "offset": 2164.56,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "in my answer. So what are the how do you",
      "offset": 2165.76,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "get around that? Is it I mean beyond",
      "offset": 2168.32,
      "duration": 2.64
    },
    {
      "lang": "en",
      "text": "prompt engineering I've I've done",
      "offset": 2169.76,
      "duration": 2.4
    },
    {
      "lang": "en",
      "text": "personally I've done things with like",
      "offset": 2170.96,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "for example emails I go great I'm just",
      "offset": 2172.16,
      "duration": 4.439
    },
    {
      "lang": "en",
      "text": "going to filter my output this is post",
      "offset": 2174.4,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "LLM to just obsuscate emails just in",
      "offset": 2176.599,
      "duration": 5.801
    },
    {
      "lang": "en",
      "text": "case. Yeah. Are there things like that",
      "offset": 2180.16,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "that you've learned in your toolkit",
      "offset": 2182.4,
      "duration": 3.719
    },
    {
      "lang": "en",
      "text": "because right that you can't just say no",
      "offset": 2184,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "secrets in a rag application right it's",
      "offset": 2186.119,
      "duration": 4.921
    },
    {
      "lang": "en",
      "text": "well you you shouldn't you shouldn't",
      "offset": 2189.2,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "give the LLM anything you don't want the",
      "offset": 2191.04,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "user to see. And for example, like I",
      "offset": 2192.64,
      "duration": 6.8
    },
    {
      "lang": "en",
      "text": "might be matching job candidates with",
      "offset": 2195.68,
      "duration": 6.56
    },
    {
      "lang": "en",
      "text": "jobs and I got the full bio to make a",
      "offset": 2199.44,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "good match, but like when I give it to",
      "offset": 2202.24,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "my user, I only want to give them the",
      "offset": 2203.68,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "teaser. Like how do I do my application",
      "offset": 2205.44,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "and follow your role? That's fair. Okay.",
      "offset": 2208.88,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "Yeah. Yeah. And that's tricky because it",
      "offset": 2210.8,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "and it when we when we refer to like",
      "offset": 2213.52,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "secrets in in your prompt, there's like",
      "offset": 2215.359,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "concerns about foundation models",
      "offset": 2218.16,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "training on those secrets which is is",
      "offset": 2219.92,
      "duration": 2.64
    },
    {
      "lang": "en",
      "text": "one concern and then there's concerns",
      "offset": 2221.28,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "about leaking the secrets to the user.",
      "offset": 2222.56,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "Yes. And and so so solely concerned with",
      "offset": 2224,
      "duration": 7.44
    },
    {
      "lang": "en",
      "text": "the the second camp um in your in in",
      "offset": 2227.599,
      "duration": 6.321
    },
    {
      "lang": "en",
      "text": "that specific case I so the strategy",
      "offset": 2231.44,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "will vary from case to case. In your",
      "offset": 2233.92,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "specific scenario, I would probably rely",
      "offset": 2235.68,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "on structured output to kind of force",
      "offset": 2238.72,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "DLM to maybe only output the title of a",
      "offset": 2240.48,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "job and then make sure that that title",
      "offset": 2242.88,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "is kind of in your in your set of titles",
      "offset": 2245.359,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "or or you know, whatever information",
      "offset": 2248.4,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "you're comfortable giving the user,",
      "offset": 2250,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "force the LLM to only output that and",
      "offset": 2251.599,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "that can't be prompt engineered by the",
      "offset": 2253.839,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "end user like prompt injected to like",
      "offset": 2255.44,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "work around because the structured",
      "offset": 2258,
      "duration": 2.48
    },
    {
      "lang": "en",
      "text": "output does a good job. Well, the",
      "offset": 2259.2,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "structured out will will mandate that",
      "offset": 2260.48,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "that it outputs a a title of or in in",
      "offset": 2262.64,
      "duration": 5.439
    },
    {
      "lang": "en",
      "text": "this scenario a title of some sort and",
      "offset": 2266.32,
      "duration": 3.039
    },
    {
      "lang": "en",
      "text": "then it would fall on you kind of",
      "offset": 2268.079,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "post-processing and making sure that job",
      "offset": 2269.359,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "title actually exists in your in your",
      "offset": 2271.119,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "set of job titles. Um, okay. That way a",
      "offset": 2273.359,
      "duration": 5.121
    },
    {
      "lang": "en",
      "text": "user is not like, hey, it's April 1st",
      "offset": 2276.079,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "and all the job titles are now people's",
      "offset": 2278.48,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "email address, right? I would never",
      "offset": 2280.24,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "expose like free form text to like",
      "offset": 2282.16,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "generate free form text to the user. And",
      "offset": 2284.56,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "I know that that's a hard constraint. If",
      "offset": 2286.56,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "you need to do that, you need to start",
      "offset": 2288.48,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "looking at like guardrail models. Uh,",
      "offset": 2289.52,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "and you think that's still true in 2025?",
      "offset": 2291.68,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "Absolutely. Interesting. Why? Where have",
      "offset": 2294.32,
      "duration": 6.32
    },
    {
      "lang": "en",
      "text": "you been burned? Uh, for I mean I tread",
      "offset": 2296.56,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "carefully enough that I haven't been",
      "offset": 2300.64,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "burned recently, but but I would but in",
      "offset": 2301.76,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "your testing you in testing you've seen",
      "offset": 2303.52,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "the the danger. Yeah. Interesting. Where",
      "offset": 2305.44,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "where in this in this prompt engineering",
      "offset": 2308.56,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "guide, what do you think is beyond that",
      "offset": 2310.48,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "one? What else is relevant still? Yeah.",
      "offset": 2312.16,
      "duration": 6.88
    },
    {
      "lang": "en",
      "text": "So um uh giving the bot a a fish which",
      "offset": 2315.04,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "is like today in context learning. Give",
      "offset": 2319.04,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "it give it as much context as you as you",
      "offset": 2320.96,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "possibly can. The more you you tell it",
      "offset": 2322.72,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "the better it'll do. Semantic search has",
      "offset": 2325.52,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "come so far since this post. Uh which is",
      "offset": 2327.76,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "kind of neat. Command grammarss. Um this",
      "offset": 2331.04,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "was today we have like code execution",
      "offset": 2334,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "built into Gemini. So this might be less",
      "offset": 2335.76,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "relevant but if you if you want it to",
      "offset": 2337.68,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "and we have we have like function",
      "offset": 2341.76,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "calling now too. This was written before",
      "offset": 2342.96,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "we had tool use. Um, and so this was",
      "offset": 2344.72,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "just a pattern that we found to give us",
      "offset": 2346.96,
      "duration": 6.72
    },
    {
      "lang": "en",
      "text": "the ability to to use tools. Um, let's",
      "offset": 2349.52,
      "duration": 6.079
    },
    {
      "lang": "en",
      "text": "see. Various ways of embedding data. I",
      "offset": 2353.68,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "think a lot of this still applies like",
      "offset": 2355.599,
      "duration": 5.681
    },
    {
      "lang": "en",
      "text": "you always structure your prompts in we",
      "offset": 2356.88,
      "duration": 6.479
    },
    {
      "lang": "en",
      "text": "find markdown works really really well.",
      "offset": 2361.28,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "Um, different foundation models will be",
      "offset": 2363.359,
      "duration": 3.041
    },
    {
      "lang": "en",
      "text": "a little different like anthropic you",
      "offset": 2365.04,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "might want to use more XML tags. OpenI,",
      "offset": 2366.4,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "you might want to be a little more JSON",
      "offset": 2368.8,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "heavy, but generally if you're writing",
      "offset": 2370.16,
      "duration": 4.959
    },
    {
      "lang": "en",
      "text": "your the meat of your prompt in",
      "offset": 2373.44,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "Markdown, it'll be really happy with",
      "offset": 2375.119,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "that. Um, there's just a ton of training",
      "offset": 2377.52,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "data, especially from GitHub written in",
      "offset": 2379.599,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "Markdown. Uh, and that that includes",
      "offset": 2380.96,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "like tabular information. You want to",
      "offset": 2383.68,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "use tables. Uh, like LMS will do really",
      "offset": 2385.68,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "really well with tables. If you have a",
      "offset": 2388.72,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "really large table, um, I need to I need",
      "offset": 2390.68,
      "duration": 4.2
    },
    {
      "lang": "en",
      "text": "to validate this. So, I haven't",
      "offset": 2394.24,
      "duration": 2.32
    },
    {
      "lang": "en",
      "text": "validated this in in a in a in a minute,",
      "offset": 2394.88,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "but um you used to have a problem where",
      "offset": 2396.56,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "the headers of the table, it might lose",
      "offset": 2398.72,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "context if you had a very long table.",
      "offset": 2401.2,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "And so, in those scenarios, you might",
      "offset": 2402.72,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "want to be more robust and and have like",
      "offset": 2404,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "JSON blobs where the field is repeated",
      "offset": 2405.68,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "over and over and over again. So, you",
      "offset": 2407.44,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "you get that locality. Um but the the",
      "offset": 2408.64,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "way you embed data in your prompts, the",
      "offset": 2411.92,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "higher take away from this is like the",
      "offset": 2414.32,
      "duration": 2.799
    },
    {
      "lang": "en",
      "text": "way think about the way you embed your",
      "offset": 2415.599,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "data in your prompts that it makes a big",
      "offset": 2417.119,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "difference. You've reminded me of one of",
      "offset": 2419.04,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "the biggest constraints right now that I",
      "offset": 2420.8,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "think all the model providers give us,",
      "offset": 2422.24,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "which is even if the context input could",
      "offset": 2424.4,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "be fairly large, the the output's not",
      "offset": 2427.119,
      "duration": 2.801
    },
    {
      "lang": "en",
      "text": "right. You're pretty you're pretty",
      "offset": 2428.8,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "limited there. And in an SOP where we",
      "offset": 2429.92,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "can do anything, you've got to have",
      "offset": 2432.32,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "cases where the end report is going to",
      "offset": 2434.079,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "exceed 7,000 tokens or whatever. Yes.",
      "offset": 2436.24,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "Like what do you do in that case?",
      "offset": 2438.72,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "Because doesn't that does that imply a",
      "offset": 2440.16,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "complete refactoring of your prompt?",
      "offset": 2443.359,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "because you're now you're talking by",
      "offset": 2446,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "definition a workflow to actually",
      "offset": 2447.04,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "assemble something that's longer than",
      "offset": 2448.72,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "any one run, right? Yeah. So, so in that",
      "offset": 2450.48,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "scenario, you would break up the process",
      "offset": 2453.04,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "into smaller uh like subprocesses. Um",
      "offset": 2454.72,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "how do you keep it coherent? Because you",
      "offset": 2458.16,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "know, writing a report and asking it to",
      "offset": 2459.52,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "do the intro and later the conclusion",
      "offset": 2461.52,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "doesn't always result in a coherent.",
      "offset": 2464.24,
      "duration": 5.24
    },
    {
      "lang": "en",
      "text": "Yeah, fortunately I mean the output",
      "offset": 2466.16,
      "duration": 6.159
    },
    {
      "lang": "en",
      "text": "context I believe for GPT4 right now is",
      "offset": 2469.48,
      "duration": 5.56
    },
    {
      "lang": "en",
      "text": "uh around 16,000 tokens 16 384. Okay.",
      "offset": 2472.319,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "It's it's a it's a good chunk of space.",
      "offset": 2475.04,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "Um and we haven't ran into any",
      "offset": 2477.359,
      "duration": 3.281
    },
    {
      "lang": "en",
      "text": "production issues where it's been a",
      "offset": 2479.44,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "problem. Okay. In in our internal use",
      "offset": 2480.64,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "cases, you mean? Yeah. Okay. And um in",
      "offset": 2482.64,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "our derived artifacts, we've run into",
      "offset": 2485.76,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "like when we do test case generation.",
      "offset": 2487.44,
      "duration": 7.2
    },
    {
      "lang": "en",
      "text": "Sure. Um uh we we now have to",
      "offset": 2489.44,
      "duration": 6.639
    },
    {
      "lang": "en",
      "text": "this this might be an interesting",
      "offset": 2494.64,
      "duration": 2.959
    },
    {
      "lang": "en",
      "text": "sidebar. our our test case generation.",
      "offset": 2496.079,
      "duration": 3.561
    },
    {
      "lang": "en",
      "text": "We we actually have one agent that",
      "offset": 2497.599,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "enumerates kind of interesting cases to",
      "offset": 2499.64,
      "duration": 7.16
    },
    {
      "lang": "en",
      "text": "test uh anywhere from one to 30 and then",
      "offset": 2502.56,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "we fan out to individual agents to",
      "offset": 2506.8,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "actually write the inputs and the",
      "offset": 2508.64,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "outputs. Um and we landed on that. Our",
      "offset": 2509.92,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "initial take on this was to have a",
      "offset": 2513.52,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "single agent just like write you know",
      "offset": 2515.44,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "write 10 interesting test cases and very",
      "offset": 2517.119,
      "duration": 5.361
    },
    {
      "lang": "en",
      "text": "rapidly especially for larger inputs",
      "offset": 2520.4,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "outputs we exhausted our our our",
      "offset": 2522.48,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "context. Makes sense. Um and uh so yeah",
      "offset": 2524.8,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "so so now we we have like one one person",
      "offset": 2528.16,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "enumerate or one agent enumerate",
      "offset": 2530.56,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "interesting test cases then individual",
      "offset": 2532.64,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "agents uh write the detailed test case",
      "offset": 2534.16,
      "duration": 5.439
    },
    {
      "lang": "en",
      "text": "and then we have a firewall uh between",
      "offset": 2537.359,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "the agent that actually runs those test",
      "offset": 2539.599,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "cases and and so that's why like we may",
      "offset": 2541.119,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "generate test cases and the first time",
      "offset": 2543.28,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "you run them they fail and and and they",
      "offset": 2544.8,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "fail because we're we're doing like real",
      "offset": 2547.28,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "we're doing it properly. Right. Okay.",
      "offset": 2549.119,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "And you don't want the you don't want",
      "offset": 2550.88,
      "duration": 2.959
    },
    {
      "lang": "en",
      "text": "the person running the test to know.",
      "offset": 2551.839,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "Yeah. Of course that makes sense. And on",
      "offset": 2553.839,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "the first part about feeding it to",
      "offset": 2555.839,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "multiple just to implement each test you",
      "offset": 2557.28,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "are you feeding like the essentially the",
      "offset": 2559.28,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "entire outline it's you do number three.",
      "offset": 2560.56,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "Yeah. Basically. Okay. So it's simple.",
      "offset": 2562.48,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "Um interesting. Let's see let's talk",
      "offset": 2564.56,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "about so we've talked about that. Um",
      "offset": 2567.359,
      "duration": 5.441
    },
    {
      "lang": "en",
      "text": "let's talk about um we've talked about",
      "offset": 2569.119,
      "duration": 6.081
    },
    {
      "lang": "en",
      "text": "speed and quality extensively. Can we",
      "offset": 2572.8,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "just talk about what's going on in the",
      "offset": 2575.2,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "technology world right now in AI? We're",
      "offset": 2576.64,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "on an exponential path of just releases.",
      "offset": 2579.119,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "December was a huge month. I can't",
      "offset": 2581.2,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "believe even Christmas day. Yes. Yeah.",
      "offset": 2582.8,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "like it was non-stop. Do not go on",
      "offset": 2584.8,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "vacation. You're gonna miss fundamental",
      "offset": 2586.8,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "stuff. Speaking of like deep seat out of",
      "offset": 2588.72,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "nowhere, right, which is intellectually",
      "offset": 2591.76,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "very interesting for a lot of reasons.",
      "offset": 2593.599,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "Yes. Um and it can't even answer all the",
      "offset": 2595.119,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "questions because of censorship and you",
      "offset": 2597.76,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "know, I mean it's all sorts of and the",
      "offset": 2599.359,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "training cost rumors. It's like a whole",
      "offset": 2601.119,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "interesting thing like like the source",
      "offset": 2603.119,
      "duration": 2.881
    },
    {
      "lang": "en",
      "text": "of your foundation model may start",
      "offset": 2604.48,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "influencing whether or not you use it",
      "offset": 2606,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "hugely. Hugely. Uh then you've got um",
      "offset": 2608.079,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "really good modes to Ford open source on",
      "offset": 2611.2,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "coding standards which are going to be",
      "offset": 2613.359,
      "duration": 3.121
    },
    {
      "lang": "en",
      "text": "relevant for what you're doing. V2 I",
      "offset": 2614.4,
      "duration": 2.959
    },
    {
      "lang": "en",
      "text": "don't know if you've played I have",
      "offset": 2616.48,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "access to V2 the trusted tester Google",
      "offset": 2617.359,
      "duration": 5.841
    },
    {
      "lang": "en",
      "text": "video it's mindblowingly good.",
      "offset": 2620.96,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "the quality would fool any reasonable",
      "offset": 2623.2,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "person. This is real video footage.",
      "offset": 2625.76,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "There's no question about it. Unlike",
      "offset": 2628.079,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "previous generations of like I was",
      "offset": 2630,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "pretty good like 80% of the time I could",
      "offset": 2631.92,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "tell something was AI generated image",
      "offset": 2634.079,
      "duration": 5.361
    },
    {
      "lang": "en",
      "text": "with with the V2. I mean and so that",
      "offset": 2636.319,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "just came out that's not even out yet.",
      "offset": 2639.44,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "It just was announced. Um you've what",
      "offset": 2640.96,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "are some of the things that have most",
      "offset": 2643.52,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "impressed you or or and do you ever",
      "offset": 2644.8,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "worry doing a startup like you're 9",
      "offset": 2646.8,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "months in 9 months from now you could be",
      "offset": 2648.88,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "like oh whoops like everything we built",
      "offset": 2650.88,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "last year is one prompt away in this new",
      "offset": 2653.359,
      "duration": 6.801
    },
    {
      "lang": "en",
      "text": "thing. Yeah. 03. I mean, fortunately so",
      "offset": 2656.56,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "far, we we we've built to be kind of",
      "offset": 2660.16,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "foundation model agnostic. And so",
      "offset": 2662.8,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "whenever there's these improvements,",
      "offset": 2664.64,
      "duration": 2.64
    },
    {
      "lang": "en",
      "text": "we've just kind of gotten to take",
      "offset": 2666,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "advantage of them. Like, uh, but does",
      "offset": 2667.28,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "your whole company get done in one shot",
      "offset": 2670,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "and I pay OpenAI $2,000 to run for 15",
      "offset": 2672.079,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "minutes? Do you understand what I'm",
      "offset": 2674.72,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "saying? Yeah. I don't think so. I ask",
      "offset": 2676.16,
      "duration": 6.439
    },
    {
      "lang": "en",
      "text": "the hard questions. Yeah. Yeah. Yeah. in",
      "offset": 2679.44,
      "duration": 7.2
    },
    {
      "lang": "en",
      "text": "that the if you're trying to",
      "offset": 2682.599,
      "duration": 7.161
    },
    {
      "lang": "en",
      "text": "productionize any uh any kind of",
      "offset": 2686.64,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "business workflow, you're going to want",
      "offset": 2689.76,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "rigor around like like you're going to",
      "offset": 2691.92,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "want back testing, you're going to want",
      "offset": 2693.839,
      "duration": 2.801
    },
    {
      "lang": "en",
      "text": "like versioning, you're going to want to",
      "offset": 2695.04,
      "duration": 2.799
    },
    {
      "lang": "en",
      "text": "be able to roll back. You're going to",
      "offset": 2696.64,
      "duration": 2.959
    },
    {
      "lang": "en",
      "text": "want like access control over who can",
      "offset": 2697.839,
      "duration": 3.041
    },
    {
      "lang": "en",
      "text": "approve schema changes versus who can",
      "offset": 2699.599,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "approve content changes. Okay. Uh all",
      "offset": 2700.88,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "like kind of all the rigor around that",
      "offset": 2703.76,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "process changing. That's really where",
      "offset": 2706.079,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "our our value is. Yeah, that makes",
      "offset": 2707.68,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "sense. And certainly companies could",
      "offset": 2709.52,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "move into that and and I think we're",
      "offset": 2711.68,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "starting to see some startups get into",
      "offset": 2714.079,
      "duration": 5.361
    },
    {
      "lang": "en",
      "text": "that. Um is there an analogy between",
      "offset": 2716.48,
      "duration": 5.359
    },
    {
      "lang": "en",
      "text": "like what happened with like I don't",
      "offset": 2719.44,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "know going from like horses to cars and",
      "offset": 2721.839,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "and building highways and stuff. Is",
      "offset": 2723.599,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "there are we just so early that it's",
      "offset": 2725.2,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "hard for us to see because that worry of",
      "offset": 2726.96,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "like I'm just going to you know the next",
      "offset": 2728.72,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "reason he gets to a tipping point where",
      "offset": 2731.119,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "it's like I can you know self-improve",
      "offset": 2732.64,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "and I can run like all the stuff that",
      "offset": 2735.44,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "you're doing that you're a smart guy and",
      "offset": 2737.839,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "you've got a great team. Yeah. Or doing",
      "offset": 2739.28,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "it will just do out of the box and it'll",
      "offset": 2741.2,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "run its own code and like so therefore",
      "offset": 2743.119,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "your customer can access that. Like do",
      "offset": 2744.88,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "you think about that stuff? Do you care",
      "offset": 2747.119,
      "duration": 2.881
    },
    {
      "lang": "en",
      "text": "about it or do you have a strategy that",
      "offset": 2748.48,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "says no we're going to be okay? And I",
      "offset": 2750,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "think a lot of people don't necessarily",
      "offset": 2751.76,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "have a great answer here yet. Yeah. I",
      "offset": 2753.28,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "mean, it's tough to, you know, you can't",
      "offset": 2755.52,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "predict the future. I I'm feeling pretty",
      "offset": 2759.2,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "good about where we're at. Um, but is",
      "offset": 2760.88,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "there an analogy for for like other",
      "offset": 2763.04,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "technology disruptive technology",
      "offset": 2764.96,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "revolutions where it was like in the end",
      "offset": 2766.96,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "it was okay like we didn't like I",
      "offset": 2769.119,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "remember when the internet came it was",
      "offset": 2771.359,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "like oh I don't know like maybe the",
      "offset": 2773.119,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "wallet will be baked into the browser.",
      "offset": 2775.359,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "Why do we need to have PayPal? Like why",
      "offset": 2776.96,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "do we Yeah. When it's early enough, you",
      "offset": 2778.8,
      "duration": 5.08
    },
    {
      "lang": "en",
      "text": "don't know.",
      "offset": 2780.72,
      "duration": 3.16
    },
    {
      "lang": "en",
      "text": "Yeah. I don't know if if an analogy",
      "offset": 2784.04,
      "duration": 4.84
    },
    {
      "lang": "en",
      "text": "comes Yeah. comes to mind. Yeah. Okay.",
      "offset": 2786.72,
      "duration": 5.119
    },
    {
      "lang": "en",
      "text": "Interesting. Um so back to like things",
      "offset": 2788.88,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "that have been released that have been",
      "offset": 2791.839,
      "duration": 2.161
    },
    {
      "lang": "en",
      "text": "impressive that you're like, &quot;Oh, we",
      "offset": 2792.88,
      "duration": 2.32
    },
    {
      "lang": "en",
      "text": "need to go and like take advantage of",
      "offset": 2794,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "that or look into it.&quot; Yeah. And",
      "offset": 2795.2,
      "duration": 2.639
    },
    {
      "lang": "en",
      "text": "obviously a lot has happened. I think",
      "offset": 2796.72,
      "duration": 2.24
    },
    {
      "lang": "en",
      "text": "maybe if you go back six months in",
      "offset": 2797.839,
      "duration": 2.641
    },
    {
      "lang": "en",
      "text": "multimodal, for example, you're talking",
      "offset": 2798.96,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "about ingesting things and that's got to",
      "offset": 2800.48,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "be on your radar. Are you using",
      "offset": 2802.319,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "multimodal models to do your processing",
      "offset": 2803.92,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "now? Yeah. Yeah. So we um you mentioned",
      "offset": 2805.839,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "PDF ingestion. So take me down to like",
      "offset": 2809.52,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "detail level there like how do you",
      "offset": 2811.599,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "approach understanding a PDF? Yeah. So",
      "offset": 2813.76,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "what charts and graphs and things that",
      "offset": 2816.8,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "might be in there? Yeah. So so",
      "offset": 2818.48,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "especially business documents they tend",
      "offset": 2820.4,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "to have like tables, figures, charts, um",
      "offset": 2821.68,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "visual components that might have logos.",
      "offset": 2824.64,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "Uh do you get any like scanned in and",
      "offset": 2826.56,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "filled out by hand? Yeah, absolutely.",
      "offset": 2828.319,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "Yeah. And um and so the the the kind of",
      "offset": 2830.56,
      "duration": 5.519
    },
    {
      "lang": "en",
      "text": "naive approach of which will get you",
      "offset": 2833.839,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "pretty good mileage of just giving the",
      "offset": 2836.079,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "raw image to uh to the foundation model",
      "offset": 2837.599,
      "duration": 6.321
    },
    {
      "lang": "en",
      "text": "will will do pretty good. Um we've",
      "offset": 2841.04,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "gotten a lot of uh mileage from doing",
      "offset": 2843.92,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "some pre-processing",
      "offset": 2846.72,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "um and and various like OCR techniques",
      "offset": 2848.56,
      "duration": 6.4
    },
    {
      "lang": "en",
      "text": "and and uh there's there's tools out",
      "offset": 2852,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "there like with with special models for",
      "offset": 2854.96,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "extracting tables from documents, table",
      "offset": 2856.56,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "oriented models. Um, there's actually a",
      "offset": 2858.319,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "really awesome project that I think was",
      "offset": 2860.48,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "trending on GitHub just a week or two",
      "offset": 2862.64,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "ago, uh, called Dockling. D O C L I N G.",
      "offset": 2864.079,
      "duration": 6.161
    },
    {
      "lang": "en",
      "text": "And, uh, let me see if I can pull that",
      "offset": 2868.56,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "up real quick. Cool. Uh, yeah, Dockling.",
      "offset": 2870.24,
      "duration": 6.32
    },
    {
      "lang": "en",
      "text": "And this like if you're sending any",
      "offset": 2873.599,
      "duration": 6.161
    },
    {
      "lang": "en",
      "text": "documents or images to an LLM, I would",
      "offset": 2876.56,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "highly recommend you run it through this",
      "offset": 2879.76,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "first. Um, what are the competing tools",
      "offset": 2880.96,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "that might have been around before",
      "offset": 2883.119,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "Dockling that that you'd tried? And I'm",
      "offset": 2884.48,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "just trying to understand I'm going to",
      "offset": 2887.04,
      "duration": 3.279
    },
    {
      "lang": "en",
      "text": "establish where is Dockling in this set",
      "offset": 2888,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "of because there's a hodgepodge of",
      "offset": 2890.319,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "techniques. Yeah. And tools and open",
      "offset": 2891.52,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "source, right? Yeah. Where does where",
      "offset": 2893.359,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "does this sit? What made it what made it",
      "offset": 2895.52,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "stand out for you? Just the ergonom like",
      "offset": 2897.04,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "this this is actually just using",
      "offset": 2899.68,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "leveraging a bunch of other tools",
      "offset": 2901.44,
      "duration": 2.32
    },
    {
      "lang": "en",
      "text": "underneath the scenes like a bunch of",
      "offset": 2902.64,
      "duration": 2.719
    },
    {
      "lang": "en",
      "text": "different hugging face models. It's just",
      "offset": 2903.76,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "a wonderful it just brings together a",
      "offset": 2905.359,
      "duration": 3.121
    },
    {
      "lang": "en",
      "text": "bunch of existing technologies in a",
      "offset": 2907.2,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "really nice ergonomic package. Okay.",
      "offset": 2908.48,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "Wow. PDF, DocX, P, PowerPoint, Excel.",
      "offset": 2910.4,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "They're doing a lot more than just PDF.",
      "offset": 2913.44,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "Yes. They're a document. Yeah. Uh and",
      "offset": 2915.119,
      "duration": 5.361
    },
    {
      "lang": "en",
      "text": "and they're wrapping a lot of other",
      "offset": 2918.8,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "libraries and best practices. They",
      "offset": 2920.48,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "probably have a router or some other",
      "offset": 2922,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "stuff. Okay. Yeah. Interesting. Cool.",
      "offset": 2923.52,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "And then you got this word of the",
      "offset": 2925.44,
      "duration": 2.08
    },
    {
      "lang": "en",
      "text": "community to sort of like make it",
      "offset": 2926.4,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "better, right? And let me see if I Yeah.",
      "offset": 2927.52,
      "duration": 8.319
    },
    {
      "lang": "en",
      "text": "So here like here's an example of let me",
      "offset": 2931.52,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "Does it handle chunking and and vector",
      "offset": 2935.839,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "indexing of long documents here? No. No.",
      "offset": 2937.76,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "The the it's it's purely just um uh so",
      "offset": 2939.839,
      "duration": 5.041
    },
    {
      "lang": "en",
      "text": "like here's here's a sample invoice.",
      "offset": 2942.96,
      "duration": 5.119
    },
    {
      "lang": "en",
      "text": "Mhm. You you send this over to Dockling.",
      "offset": 2944.88,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "I already have the output up here. And",
      "offset": 2948.079,
      "duration": 4.201
    },
    {
      "lang": "en",
      "text": "it'll convert it to like a nice uh just",
      "offset": 2949.359,
      "duration": 6.401
    },
    {
      "lang": "en",
      "text": "markdown formatted. Wow. Uh chunk of",
      "offset": 2952.28,
      "duration": 4.839
    },
    {
      "lang": "en",
      "text": "text that you can embed in your in your",
      "offset": 2955.76,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "LM and LMS's I think I mentioned earlier",
      "offset": 2957.119,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "like love markdown. And so so we'll",
      "offset": 2959.599,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "whenever we give an LM an image, we'll",
      "offset": 2961.92,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "also kind of pre-process it and give it",
      "offset": 2964.24,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "kind of a textual. Okay. So you're not",
      "offset": 2965.92,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "outsourcing to Dockling. could that",
      "offset": 2968.16,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "would be a naive approach but you're",
      "offset": 2969.839,
      "duration": 3.121
    },
    {
      "lang": "en",
      "text": "saying no use doc to create struct",
      "offset": 2971.04,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "structured text extracted from lots of",
      "offset": 2972.96,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "document formats but also pass in the",
      "offset": 2975.92,
      "duration": 2.48
    },
    {
      "lang": "en",
      "text": "original this is yeah this is",
      "offset": 2977.359,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "complimentary to providing the image",
      "offset": 2978.4,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "it's not it when you say providing the",
      "offset": 2980.559,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "image are you like just the graphs and",
      "offset": 2982.24,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "and image data within a PDF or",
      "offset": 2984.319,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "screenshot so if you give us a PDF uh",
      "offset": 2986.24,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "with 20 pages we convert it to 20 images",
      "offset": 2987.92,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "and send send all the images yeah I like",
      "offset": 2989.839,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "such an elegant approach it's like just",
      "offset": 2992.64,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "works you know screenshot every page run",
      "offset": 2994.64,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "it through dockling put those two",
      "offset": 2996.8,
      "duration": 2.799
    },
    {
      "lang": "en",
      "text": "together and that works. Yeah. Yeah. And",
      "offset": 2998.319,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "and it'll reconcile. Um when I when I",
      "offset": 2999.599,
      "duration": 5.121
    },
    {
      "lang": "en",
      "text": "say and that works, have you like sort",
      "offset": 3002.4,
      "duration": 5.679
    },
    {
      "lang": "en",
      "text": "of scientifically measured that quality",
      "offset": 3004.72,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "output of that approach versus bespoke",
      "offset": 3008.079,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "approaches or things that people were",
      "offset": 3010.8,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "tinkering with, you know, a year ago?",
      "offset": 3012.079,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "Yeah. I mean, we we measured the out",
      "offset": 3014.24,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "like the output quality. Um are there",
      "offset": 3016.72,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "benchmarks for PDF extraction at this",
      "offset": 3018.96,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "point? Like the public benchmarks? Oh,",
      "offset": 3020.64,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "in in that sense like in terms of like",
      "offset": 3022.24,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "uh like publishing a paper on quality, I",
      "offset": 3024.24,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "don't think we've done benchmarks of of",
      "offset": 3026.4,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "of that rigor. Mostly it's just like our",
      "offset": 3027.839,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "customer has a problem, here's one",
      "offset": 3030.079,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "solution that solves it. Oh, like oh, we",
      "offset": 3031.92,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "changed that solution. Is this better?",
      "offset": 3033.52,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "And um because ultimately you're trying",
      "offset": 3035.2,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "to like get to human quality. Like if a",
      "offset": 3037.68,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "human was tasked with doing this SOP",
      "offset": 3039.68,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "with the PDF all day long, how many",
      "offset": 3041.76,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "mistakes would they make and can your",
      "offset": 3044.16,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "approach be better than that? Yeah. And",
      "offset": 3045.839,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "and one of the one of the um like really",
      "offset": 3047.599,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "interesting data points that we just got",
      "offset": 3050.64,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "uh a week or so ago is one of the use",
      "offset": 3052.24,
      "duration": 6.4
    },
    {
      "lang": "en",
      "text": "cases that we're in production for uh",
      "offset": 3055.92,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "has to do with um it's e-commerce",
      "offset": 3058.64,
      "duration": 6.28
    },
    {
      "lang": "en",
      "text": "related inventory moderation and um and",
      "offset": 3061.119,
      "duration": 7.121
    },
    {
      "lang": "en",
      "text": "the humans working on the task would get",
      "offset": 3064.92,
      "duration": 6.919
    },
    {
      "lang": "en",
      "text": "would make kind of gross mistakes uh",
      "offset": 3068.24,
      "duration": 7.52
    },
    {
      "lang": "en",
      "text": "large mistakes in about 24% of use cases",
      "offset": 3071.839,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "and we found that the LM would make",
      "offset": 3075.76,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "mistakes in about 2% of use cases. So",
      "offset": 3077.599,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "it's huge huge reduction in rate of",
      "offset": 3079.599,
      "duration": 5.601
    },
    {
      "lang": "en",
      "text": "error and uh that was based on a human",
      "offset": 3081.839,
      "duration": 5.361
    },
    {
      "lang": "en",
      "text": "expert eval. So just looking at hundreds",
      "offset": 3085.2,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "of examples uh having that analyzed by",
      "offset": 3087.2,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "the human and the uh the CEO was kind of",
      "offset": 3089.44,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "in such disbelief at that number that we",
      "offset": 3093.44,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "did a second human eval with the CEO as",
      "offset": 3095.52,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "the evaluator and the the result was",
      "offset": 3097.52,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "replicated. So, so when when we do uh",
      "offset": 3099.04,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "kind of analysis that's typically",
      "offset": 3102.079,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "sometimes we'll use GPT4 as a judge um",
      "offset": 3103.599,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "or we'll use a human as a judge. Very",
      "offset": 3106.319,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "cool. Have you tried um just going",
      "offset": 3108.4,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "through other things that have come out",
      "offset": 3110.24,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "um anthropic computer use MCP? Have you",
      "offset": 3111.92,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "used the new real time voice APIs or",
      "offset": 3115.2,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "just enhanced voice open? Dabble with a",
      "offset": 3117.76,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "few of them real real briefly. MCP.",
      "offset": 3120.4,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "Yeah. In fact, we we did a series of uh",
      "offset": 3122.48,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "blog posts over the holidays and one of",
      "offset": 3124.72,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "them is on MCP and kind of how to set it",
      "offset": 3126.079,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "up and um we've been thinking we just",
      "offset": 3128,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "started kind of as as part of our like",
      "offset": 3130.72,
      "duration": 3.119
    },
    {
      "lang": "en",
      "text": "third party integration strategy getting",
      "offset": 3132.319,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "into MCP would be really nice because",
      "offset": 3133.839,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "imagine you're just talking with Claude",
      "offset": 3135.68,
      "duration": 2.879
    },
    {
      "lang": "en",
      "text": "and like Claude can just like call out",
      "offset": 3137.2,
      "duration": 2.56
    },
    {
      "lang": "en",
      "text": "to different documents like business",
      "offset": 3138.559,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "logic documents. It might be a a place",
      "offset": 3139.76,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "that business customers get to is just",
      "offset": 3141.76,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "using cloud as an interface. Yeah. And",
      "offset": 3143.52,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "and and then you could off them into",
      "offset": 3145.2,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "your system or whatever it is. MCP is",
      "offset": 3146.72,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "quite nice. I I'm bullish on MCP. Yeah.",
      "offset": 3148.72,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "It's a really nice design. Do you have",
      "offset": 3151.52,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "you seen any other products implement it",
      "offset": 3152.96,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "yet? Uh or other competing entities use",
      "offset": 3154.8,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "it as a standard? Will it become a",
      "offset": 3158.16,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "universal standard? Yeah, right now the",
      "offset": 3159.52,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "number of integrations is pretty tiny.",
      "offset": 3161.04,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "Um I will give a shout out to Everart",
      "offset": 3162.8,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "which is uh founded by a former",
      "offset": 3165.04,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "colleague of mine from Brex. They just",
      "offset": 3167.76,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "launched an MCP integration. Um I",
      "offset": 3169.76,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "suspect over the next three to six",
      "offset": 3173.2,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "months we'll see an explosion of MCP",
      "offset": 3175.76,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "integrations, but we're not quite there",
      "offset": 3177.76,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "yet. Yeah. Very cool. Um, is there",
      "offset": 3179.119,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "anything else you want to show? Anything",
      "offset": 3181.359,
      "duration": 2.161
    },
    {
      "lang": "en",
      "text": "else you're tinkering on? What other",
      "offset": 3182.559,
      "duration": 2.961
    },
    {
      "lang": "en",
      "text": "side project you got going on, Steve?",
      "offset": 3183.52,
      "duration": 5.88
    },
    {
      "lang": "en",
      "text": "Yeah, I think um, uh, one",
      "offset": 3185.52,
      "duration": 6.559
    },
    {
      "lang": "en",
      "text": "other, so like we we we talked about",
      "offset": 3189.4,
      "duration": 4.679
    },
    {
      "lang": "en",
      "text": "like transactional workflows. Yeah. And",
      "offset": 3192.079,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "then we talked about or the",
      "offset": 3194.079,
      "duration": 2.801
    },
    {
      "lang": "en",
      "text": "transactional use case, then we talked",
      "offset": 3195.52,
      "duration": 3.039
    },
    {
      "lang": "en",
      "text": "about the pickins bot, which is kind of",
      "offset": 3196.88,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "a predefined workflow. Um, but we",
      "offset": 3198.559,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "haven't really discussed like fully",
      "offset": 3201.68,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "autonomous agents yet. And that's",
      "offset": 3203.599,
      "duration": 5.041
    },
    {
      "lang": "en",
      "text": "something behind the scenes at Logic,",
      "offset": 3206.72,
      "duration": 5.359
    },
    {
      "lang": "en",
      "text": "we're always kind of tinkering uh with",
      "offset": 3208.64,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "with more complex workflows and more",
      "offset": 3212.079,
      "duration": 5.441
    },
    {
      "lang": "en",
      "text": "complex agents. And over time, we plan",
      "offset": 3213.92,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "to make those more accessible to our",
      "offset": 3217.52,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "customers. What's an example of one that",
      "offset": 3219.28,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "you've played with? Yeah. So, I'll I'll",
      "offset": 3220.72,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "I'll show you. Um so, we we have um we",
      "offset": 3223.68,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "we this is an in-house agent that we",
      "offset": 3226.88,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "built. We call it Party. uh programmatic",
      "offset": 3228.72,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "artificial developer and um and this is",
      "offset": 3231.28,
      "duration": 5.839
    },
    {
      "lang": "en",
      "text": "kind of in in response to just like we",
      "offset": 3234.16,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "have we use AI to like write our our",
      "offset": 3237.119,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "commit messages and we have AI to review",
      "offset": 3239.68,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "our code we also want AI to help us",
      "offset": 3241.44,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "write our code and I think just like",
      "offset": 3243.119,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "many developers today you know we use",
      "offset": 3245.119,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "cursor",
      "offset": 3247.04,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "uh um Ader uh if you're in Vim code",
      "offset": 3248.559,
      "duration": 5.881
    },
    {
      "lang": "en",
      "text": "companion",
      "offset": 3252.4,
      "duration": 4.52
    },
    {
      "lang": "en",
      "text": "um but I",
      "offset": 3254.44,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "found especially on the open source side",
      "offset": 3256.92,
      "duration": 5.8
    },
    {
      "lang": "en",
      "text": "the quality of coding agents is way",
      "offset": 3259.44,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "behind like the latest of cursor and",
      "offset": 3262.72,
      "duration": 4.28
    },
    {
      "lang": "en",
      "text": "devon and",
      "offset": 3265.04,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "um uh and so we we've just been",
      "offset": 3267,
      "duration": 4.76
    },
    {
      "lang": "en",
      "text": "experimenting with like building an open",
      "offset": 3269.44,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "source uh coding agent uh and it works",
      "offset": 3271.76,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "at the terminal sounds. Yeah. Yeah.",
      "offset": 3274.96,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "Yeah. So so it's all it's it's like 100%",
      "offset": 3276.96,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "terminal. Show us. Yeah. Yeah. So let",
      "offset": 3278.8,
      "duration": 4.84
    },
    {
      "lang": "en",
      "text": "let's see.",
      "offset": 3281.76,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "Um let's just make a a make like a is",
      "offset": 3283.64,
      "duration": 4.919
    },
    {
      "lang": "en",
      "text": "the idea though that I would fire this",
      "offset": 3287.44,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "up in an existing repo or uh yeah so it",
      "offset": 3288.559,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "can work so so this is designed to work",
      "offset": 3291.28,
      "duration": 2.799
    },
    {
      "lang": "en",
      "text": "against the logic codebase so we can",
      "offset": 3292.8,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "give it we use linear for task",
      "offset": 3294.079,
      "duration": 2.881
    },
    {
      "lang": "en",
      "text": "management we can give it a linear task",
      "offset": 3295.76,
      "duration": 2.319
    },
    {
      "lang": "en",
      "text": "and it'll go and implement it but we'll",
      "offset": 3296.96,
      "duration": 2.08
    },
    {
      "lang": "en",
      "text": "we'll just do something from scratch",
      "offset": 3298.079,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "here. Okay. Okay. Okay. So I'll create a",
      "offset": 3299.04,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "directory called and this do you plan to",
      "offset": 3301.44,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "open source this just getting to the I",
      "offset": 3303.119,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "want to play with it too. Okay cool. Um,",
      "offset": 3304.96,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "will it use logic on the back end to",
      "offset": 3307.44,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "implement itself or is this too? So,",
      "offset": 3309.76,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "right now it's fully self-contained. Um,",
      "offset": 3311.2,
      "duration": 5.359
    },
    {
      "lang": "en",
      "text": "if we if we GA logic before we open",
      "offset": 3313.44,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "source this uh there will probably be",
      "offset": 3316.559,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "some kind of logic integration. Yeah,",
      "offset": 3318.48,
      "duration": 3.119
    },
    {
      "lang": "en",
      "text": "but but I don't think there's no",
      "offset": 3320.48,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "universe where logic would be mandated.",
      "offset": 3321.599,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "Um, so we'll give it a a project route",
      "offset": 3324.8,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "of",
      "offset": 3327.599,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "uh test tic tactoe. We'll give it a",
      "offset": 3329.599,
      "duration": 8.161
    },
    {
      "lang": "en",
      "text": "description of um implement a",
      "offset": 3332.24,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "simple tic",
      "offset": 3338.92,
      "duration": 7.28
    },
    {
      "lang": "en",
      "text": "tac tic tac toe game from",
      "offset": 3340.839,
      "duration": 9.201
    },
    {
      "lang": "en",
      "text": "scratch. Uh setup get and mpm and",
      "offset": 3346.2,
      "duration": 8.159
    },
    {
      "lang": "en",
      "text": "everything. Uh give it a simple though",
      "offset": 3350.04,
      "duration": 9.559
    },
    {
      "lang": "en",
      "text": "static tend only. Okay. Um, and so, so",
      "offset": 3354.359,
      "duration": 6.921
    },
    {
      "lang": "en",
      "text": "one of the things this agent can do is",
      "offset": 3359.599,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "execute comm execute commands on your",
      "offset": 3361.28,
      "duration": 3.039
    },
    {
      "lang": "en",
      "text": "behalf. And by default, we have it ask",
      "offset": 3362.799,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "you if you if it can run the command.",
      "offset": 3364.319,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "But I'm going to run with uh no fear",
      "offset": 3366.24,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "mode. You're brave. And uh just let it",
      "offset": 3368.64,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "let it loose in my terminal. Okay. Um",
      "offset": 3370.88,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "and it's telling you what it's doing",
      "offset": 3372.96,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "here. It's basically installing it all",
      "offset": 3374,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "the things it's going to need to create",
      "offset": 3375.76,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "this this project. Yeah. And so this is",
      "offset": 3376.96,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "actually broken down into three sub",
      "offset": 3378.72,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "agents. The the first agent is a",
      "offset": 3380.559,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "planner. Um and it can only list",
      "offset": 3382.079,
      "duration": 5.681
    },
    {
      "lang": "en",
      "text": "directories. Mhm. and read files. That's",
      "offset": 3385.28,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "all it can do. So, this is going to",
      "offset": 3387.76,
      "duration": 2.64
    },
    {
      "lang": "en",
      "text": "crawl the code base. This is a simple",
      "offset": 3389.28,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "one. It's an empty directory. So, it",
      "offset": 3390.4,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "listed the directory. And just based on",
      "offset": 3392,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "the amount of time it's taking for this",
      "offset": 3394.079,
      "duration": 2.641
    },
    {
      "lang": "en",
      "text": "response, it's coming it's now coming up",
      "offset": 3395.359,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "with a plan. Y uh to turn this empty",
      "offset": 3396.72,
      "duration": 7.359
    },
    {
      "lang": "en",
      "text": "directory into a a tic-tac-toe game. And",
      "offset": 3399.839,
      "duration": 6.161
    },
    {
      "lang": "en",
      "text": "so, this is the detailed step-by-step",
      "offset": 3404.079,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "plan. Then, we have another agent that",
      "offset": 3406,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "will review and revise this plan. Um,",
      "offset": 3408.72,
      "duration": 8.079
    },
    {
      "lang": "en",
      "text": "wow. And then once it's reviewed, yeah.",
      "offset": 3412.16,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "So the plan provides a comprehensive",
      "offset": 3416.799,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "guide. So the the reviewer signed off on",
      "offset": 3418,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "it. Now it gets passed to a third agent",
      "offset": 3420.48,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "that is an engineer that actually",
      "offset": 3422.319,
      "duration": 2.48
    },
    {
      "lang": "en",
      "text": "implements it. And the engineer has",
      "offset": 3423.44,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "additional tools. This is the engineer",
      "offset": 3424.799,
      "duration": 2.881
    },
    {
      "lang": "en",
      "text": "that can actually run commands. Are they",
      "offset": 3426.4,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "going to write tests also? Uh it might",
      "offset": 3427.68,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "if we if we ask it to write test, it",
      "offset": 3430.24,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "would. So you can see, you know, here",
      "offset": 3432,
      "duration": 5.119
    },
    {
      "lang": "en",
      "text": "it's like initializing a git uh repo. Uh",
      "offset": 3433.68,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "adding node modules to our git ignore,",
      "offset": 3437.119,
      "duration": 5.121
    },
    {
      "lang": "en",
      "text": "initializing npm. Nice. Uh how does this",
      "offset": 3438.96,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "compare to like open devon or something?",
      "offset": 3442.24,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "Do you know? I haven't done like a head",
      "offset": 3443.76,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "comparison or like I haven't done a",
      "offset": 3446.559,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "regular comparison the approach of the",
      "offset": 3448.319,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "how it manifests to you as a user or a",
      "offset": 3450.48,
      "duration": 3.96
    },
    {
      "lang": "en",
      "text": "developer",
      "offset": 3452.72,
      "duration": 3.56
    },
    {
      "lang": "en",
      "text": "the one of",
      "offset": 3454.44,
      "duration": 4.919
    },
    {
      "lang": "en",
      "text": "the I'll say the the open source agent",
      "offset": 3456.28,
      "duration": 4.519
    },
    {
      "lang": "en",
      "text": "or coding agent that I spent the most",
      "offset": 3459.359,
      "duration": 3.121
    },
    {
      "lang": "en",
      "text": "Oh, and well, here we We have we have",
      "offset": 3460.799,
      "duration": 5.121
    },
    {
      "lang": "en",
      "text": "tic tacto and uh",
      "offset": 3462.48,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "Yeah. So the game board's a little a",
      "offset": 3465.92,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "little but we you know we we could we",
      "offset": 3468.4,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "could have it revise that. Yeah. Yeah.",
      "offset": 3470.72,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "Yeah. Can is it still in a loop where",
      "offset": 3472.319,
      "duration": 4.04
    },
    {
      "lang": "en",
      "text": "you could actually give it feedback?",
      "offset": 3474.079,
      "duration": 6.321
    },
    {
      "lang": "en",
      "text": "Uh there's no yet interactivity where",
      "offset": 3476.359,
      "duration": 6.121
    },
    {
      "lang": "en",
      "text": "this is at right now. But we could I",
      "offset": 3480.4,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "could um you know change the description",
      "offset": 3482.48,
      "duration": 6.4
    },
    {
      "lang": "en",
      "text": "to say like uh Oh. So if you run a new",
      "offset": 3485.2,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "description here, it's going to be okay.",
      "offset": 3488.88,
      "duration": 2.719
    },
    {
      "lang": "en",
      "text": "Yeah, but I need to look at the code",
      "offset": 3490.4,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "base I'm in already. Yeah. So, let's",
      "offset": 3491.599,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "make it a lot. Oh, that's really cool.",
      "offset": 3493.44,
      "duration": 4.04
    },
    {
      "lang": "en",
      "text": "Beautiful.",
      "offset": 3495.92,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "Uh how how in today's world of this",
      "offset": 3497.48,
      "duration": 6.2
    },
    {
      "lang": "en",
      "text": "implementation, how with a very large",
      "offset": 3501.2,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "codebase, what would it do? Yeah. So,",
      "offset": 3503.68,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "that that's kind of the neat thing. So,",
      "offset": 3506,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "the the agent right now it when it runs",
      "offset": 3507.359,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "it has no pre-existing context on the",
      "offset": 3510.319,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "codebase. And so, everything it gathers",
      "offset": 3512.24,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "it it just gathers by crawling the",
      "offset": 3513.92,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "codebase and listing directories and",
      "offset": 3515.28,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "reading files. Yeah. Um, you have a step",
      "offset": 3516.72,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "where you're gonna vector index that and",
      "offset": 3519.04,
      "duration": 2.559
    },
    {
      "lang": "en",
      "text": "do all that stuff. There's a counterpart",
      "offset": 3520.4,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "where we Yeah. So, there's a there's a",
      "offset": 3521.599,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "counter. No. So, so there's a a",
      "offset": 3524.079,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "secondary project that we're working on",
      "offset": 3526.16,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "that actually indexes your codebase for",
      "offset": 3527.92,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "you and actually comes up with it not",
      "offset": 3529.76,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "only will index the content of your",
      "offset": 3532.559,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "code, um, but it'll actually look at",
      "offset": 3534.4,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "your git log and find like, oh, these",
      "offset": 3536,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "these files tend to be modified together",
      "offset": 3538,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "and like, oh, you use these patterns and",
      "offset": 3539.52,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "like every time you check in code, you",
      "offset": 3541.2,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "add tests. So, if you do that and then",
      "offset": 3542.48,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "the last thing you just need is a is a",
      "offset": 3544.24,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "VS Code plugin of some kind. Yep. Then",
      "offset": 3545.68,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "we're done. We've got a open source like",
      "offset": 3547.52,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "that's Yeah, that's really cool. And so",
      "offset": 3550,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "just while this is wrapping up, we can",
      "offset": 3551.92,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "kind of we can just hop in here and we",
      "offset": 3553.359,
      "duration": 5.041
    },
    {
      "lang": "en",
      "text": "can, you know, uh that's super I'll I'll",
      "offset": 3555.04,
      "duration": 7.319
    },
    {
      "lang": "en",
      "text": "just cat this instead of it up. So, you",
      "offset": 3558.4,
      "duration": 6.159
    },
    {
      "lang": "en",
      "text": "know, simple, you know, this is a very",
      "offset": 3562.359,
      "duration": 4.681
    },
    {
      "lang": "en",
      "text": "simple kind of toy example, but um but",
      "offset": 3564.559,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "it looks like it completed and it's it",
      "offset": 3567.04,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "just added everything to git. If we hop",
      "offset": 3569.2,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "over to git, we look at our log. Yeah,",
      "offset": 3570.799,
      "duration": 7.681
    },
    {
      "lang": "en",
      "text": "we should see two commits now. And if we",
      "offset": 3573.44,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "refresh a little better, try harder.",
      "offset": 3578.68,
      "duration": 5.24
    },
    {
      "lang": "en",
      "text": "Yeah. Yeah. Put the crypted lines. Um,",
      "offset": 3581.52,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "that's really cool. Hey, you you were",
      "offset": 3583.92,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "Were you Correct me if I'm wrong, when",
      "offset": 3585.92,
      "duration": 2.879
    },
    {
      "lang": "en",
      "text": "we met, you were like a Ruby on Rails",
      "offset": 3587.52,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "guy, right? Yeah. Yeah. Okay. And then",
      "offset": 3588.799,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "now, are you doing Ruby on Rails at all",
      "offset": 3591.04,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "anymore? No. I mean, I there's a I have",
      "offset": 3592.64,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "nostalgia for it. Fine. But what do you",
      "offset": 3595.119,
      "duration": 3.041
    },
    {
      "lang": "en",
      "text": "use for like your production right now",
      "offset": 3596.64,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "for Everything Everything's TypeScript.",
      "offset": 3598.16,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "I'm all in on on TypeScript. Um, we just",
      "offset": 3599.839,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "because of the ecosystem sometime.",
      "offset": 3602.72,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "Sorry. back end everything's",
      "offset": 3604.319,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "everything's typed end to end typescript",
      "offset": 3605.76,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "um we because of the tooling ecosystem",
      "offset": 3607.839,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "like dockling we have started",
      "offset": 3609.839,
      "duration": 2.881
    },
    {
      "lang": "en",
      "text": "incorporating python into some places",
      "offset": 3611.04,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "but we really keep python kind of",
      "offset": 3612.72,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "isolated you run it as a process somehow",
      "offset": 3614.799,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "you ever run servers that are running",
      "offset": 3617.359,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "stuff in the python because you're not",
      "offset": 3619.04,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "using any third party uh you know the",
      "offset": 3620.559,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "python runs like part of a work it's",
      "offset": 3622.48,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "just a binary that we we call out to",
      "offset": 3624.48,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "okay cool nice yeah um and that's just",
      "offset": 3626.48,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "so you can get things like dockling or",
      "offset": 3629.2,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "whatever that are out there yeah yeah",
      "offset": 3630.64,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "because the for better or worse so much",
      "offset": 3632.24,
      "duration": 4.599
    },
    {
      "lang": "en",
      "text": "of the ecosystem uh the data science and",
      "offset": 3634.16,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "like uh machine learning ecosystem is in",
      "offset": 3636.839,
      "duration": 5.801
    },
    {
      "lang": "en",
      "text": "Python. Yeah, I'm I'm doing everything",
      "offset": 3639.44,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "that I do is Ruby on Rail still and I",
      "offset": 3642.64,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "bridge over via APIs. Yeah. So I haven't",
      "offset": 3644.4,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "had a need to do I mean I do lots of",
      "offset": 3647.04,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "like library incorporation that is based",
      "offset": 3648.799,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "on Python somewhere but like you I keep",
      "offset": 3650.4,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "it contained. Yeah. I haven't uh Ruby",
      "offset": 3652.64,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "back in like this would have been like",
      "offset": 3656.16,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "2010 2012 uh with like HAML and coffees",
      "offset": 3657.52,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "script and all that good stuff. I I",
      "offset": 3660.88,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "occasionally miss it. There are things",
      "offset": 3663.68,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "that Rails still does better than than",
      "offset": 3664.96,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "anybody else. And I think the thing for",
      "offset": 3666.72,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "me I think I think one of the most",
      "offset": 3668.72,
      "duration": 3.119
    },
    {
      "lang": "en",
      "text": "compelling reasons probably although the",
      "offset": 3670.16,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "languages models don't seem to care a",
      "offset": 3671.839,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "lot of the serverless tooling especially",
      "offset": 3674.559,
      "duration": 3.121
    },
    {
      "lang": "en",
      "text": "but even the models themselves default",
      "offset": 3676.16,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "to like Python world and and and even",
      "offset": 3677.68,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "even they're even opinionated on stuff",
      "offset": 3680.96,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "like which uh CSS framework to use.",
      "offset": 3682.319,
      "duration": 5.121
    },
    {
      "lang": "en",
      "text": "Yeah. React is obviously like a default",
      "offset": 3685.2,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "when output. In fact, I've fed when I",
      "offset": 3687.44,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "feed my codebase a lot of times it's",
      "offset": 3689.839,
      "duration": 2.401
    },
    {
      "lang": "en",
      "text": "like, &quot;Oh, here's, you know, it's",
      "offset": 3691.04,
      "duration": 2.319
    },
    {
      "lang": "en",
      "text": "basically telling me you should rewrite",
      "offset": 3692.24,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "it in in React.&quot; Yeah. I'm like, &quot;No,",
      "offset": 3693.359,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "no, please don't. Not yet.&quot; So, I",
      "offset": 3695.52,
      "duration": 5.839
    },
    {
      "lang": "en",
      "text": "actually have a a draft post uh uh",
      "offset": 3697.359,
      "duration": 7.041
    },
    {
      "lang": "en",
      "text": "called New Year, New LAMP stack where uh",
      "offset": 3701.359,
      "duration": 5.601
    },
    {
      "lang": "en",
      "text": "LAMP is LLM aware methods and practices.",
      "offset": 3704.4,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "And so, it's all about how LLMs are",
      "offset": 3706.96,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "starting to shape our technical",
      "offset": 3709.2,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "decisions. And like you get we get so",
      "offset": 3710.88,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "much leverage from the LLM that rather",
      "offset": 3713.2,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "than fight it and try to make it",
      "offset": 3715.04,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "accommodate our decisions, we embrace",
      "offset": 3716.799,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "the decisions that it is most like most",
      "offset": 3718.72,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "comfortable with. I'm I'm excited about",
      "offset": 3721.359,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "that philosophy. A lot of lot of like",
      "offset": 3723.2,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "piecing together I think the evolution",
      "offset": 3724.72,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "just in two years where LM are so",
      "offset": 3726.16,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "unreliable that you have to really",
      "offset": 3729.2,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "constrain them and then you know if I'm",
      "offset": 3730.96,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "creating an email I'm going to like do",
      "offset": 3732.72,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "it the old way and then this one piece",
      "offset": 3734.72,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "of dynamic content I'm going test the",
      "offset": 3736.24,
      "duration": 2.24
    },
    {
      "lang": "en",
      "text": "heck out of it. I'm going to insert it",
      "offset": 3737.52,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "into that paragraph. These days I am",
      "offset": 3738.48,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "quickly becoming",
      "offset": 3741.92,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "like throw all that away. I'm just gonna",
      "offset": 3743.48,
      "duration": 5.48
    },
    {
      "lang": "en",
      "text": "ask the really smart model. I'm going to",
      "offset": 3746.48,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "tell it everything about my user and my",
      "offset": 3748.96,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "goal and I'm going to write the email.",
      "offset": 3750.559,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "Yeah. And then decide if you send it or",
      "offset": 3752.88,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "not. Is it appropriate? And I'm going to",
      "offset": 3754.799,
      "duration": 2.881
    },
    {
      "lang": "en",
      "text": "have a better experience for my user",
      "offset": 3756.4,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "with more conversion or whatever it is.",
      "offset": 3757.68,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "And I'm increasingly doing everything",
      "offset": 3759.2,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "that way. And it makes me believe that",
      "offset": 3760.64,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "like well all the CRUD literally that I",
      "offset": 3762.16,
      "duration": 7.679
    },
    {
      "lang": "en",
      "text": "create will be replaced with disposable",
      "offset": 3765.92,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "software that's created and executed on",
      "offset": 3769.839,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "the fly including the UI components. Why",
      "offset": 3771.52,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "wouldn't it be? It's some maybe not this",
      "offset": 3773.68,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "year but next year. Also this notion",
      "offset": 3776.24,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "it's more like kind of spec driven",
      "offset": 3778.559,
      "duration": 2.56
    },
    {
      "lang": "en",
      "text": "development where you just kind of",
      "offset": 3780.16,
      "duration": 2.399
    },
    {
      "lang": "en",
      "text": "describe what you want to happen and you",
      "offset": 3781.119,
      "duration": 3.041
    },
    {
      "lang": "en",
      "text": "don't really care about the specifics.",
      "offset": 3782.559,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "And it and I I think one of the",
      "offset": 3784.16,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "interesting divides that we'll we'll",
      "offset": 3786.799,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "start to see uh kind of bifrocate more",
      "offset": 3788.16,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "and more as time progresses is like",
      "offset": 3790.559,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "companies that had code bases that",
      "offset": 3792.799,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "existed before 2023 and companies that",
      "offset": 3795.359,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "created code bases after 2023 where like",
      "offset": 3797.359,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "the way you structure your codebase the",
      "offset": 3799.92,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "technical decisions you make any anybody",
      "offset": 3801.44,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "creating a new codebase today is is",
      "offset": 3803.599,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "probably going to structure it if you're",
      "offset": 3805.76,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "making it friendly for LLM is probably",
      "offset": 3807.119,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "going to structure it very differently",
      "offset": 3808.48,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "and you kind of get into this what would",
      "offset": 3810.079,
      "duration": 2.321
    },
    {
      "lang": "en",
      "text": "some of those components you think would",
      "offset": 3811.28,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "be everything from uh the size of your",
      "offset": 3812.4,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "files. Like you no longer want single",
      "offset": 3816.24,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "files that are 10,000 lines long. Not",
      "offset": 3817.92,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "that you ever wanted this, but like you",
      "offset": 3819.839,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "you certainly don't want a single source",
      "offset": 3821.2,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "file that's like 10,000 lines long. Uh",
      "offset": 3822.559,
      "duration": 5.121
    },
    {
      "lang": "en",
      "text": "the the libraries that you're using um",
      "offset": 3824.88,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "the the naming conventions like if you",
      "offset": 3827.68,
      "duration": 3.119
    },
    {
      "lang": "en",
      "text": "have the LLM working in your codebase",
      "offset": 3829.28,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "and it starts hallucinating methods or",
      "offset": 3830.799,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "variables, that's probably a sign that",
      "offset": 3832.64,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "your code isn't readable. So like make",
      "offset": 3834.16,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "implement those methods, implement those",
      "offset": 3836.559,
      "duration": 2.641
    },
    {
      "lang": "en",
      "text": "variables because DLM's assuming they",
      "offset": 3837.839,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "should exist more more times than not",
      "offset": 3839.2,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "they they should exist and your codebase",
      "offset": 3841.76,
      "duration": 2.64
    },
    {
      "lang": "en",
      "text": "will be better for it. Any kind of",
      "offset": 3843.039,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "libraries that it's using um training",
      "offset": 3844.4,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "data so like a lot of the bench a lot of",
      "offset": 3846.96,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "the benchmarks are Python but a lot of",
      "offset": 3849.2,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "the the a lot of the models work really",
      "offset": 3851.119,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "well with Python or JavaScript that",
      "offset": 3852.88,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "could have have to do with the",
      "offset": 3855.2,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "underlying training data. Um, but it",
      "offset": 3856.799,
      "duration": 6.881
    },
    {
      "lang": "en",
      "text": "kind of hearkens back to like like the",
      "offset": 3860.079,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "way we build factories. Like if you if",
      "offset": 3863.68,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "you have an existing factory and you",
      "offset": 3865.119,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "want to add automation to it, you kind",
      "offset": 3867.2,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "of need to adapt the machines to the",
      "offset": 3868.88,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "factory, right? Whereas like if you're",
      "offset": 3870.64,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "building a new factory from scratch, you",
      "offset": 3872.16,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "adapt you can change the factory to work",
      "offset": 3874,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "with with the machines. Yep. Um, and I",
      "offset": 3876,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "think uh you you see that same divide",
      "offset": 3878.64,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "with new new versus old code bases where",
      "offset": 3880.72,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "you can just make your new codebase work",
      "offset": 3882.72,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "with the machines. Yeah, it's a great",
      "offset": 3884.96,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "great opportunity for uh new companies",
      "offset": 3887.599,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "to come in and move be able to move",
      "offset": 3889.44,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "faster because they architect them in a",
      "offset": 3891.2,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "way that will allow them to naturally",
      "offset": 3893.44,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "move faster. Yeah, I mean it's it's like",
      "offset": 3894.559,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "our four-person team. It feels like",
      "offset": 3897.039,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "we're we're doing the work of like a 20",
      "offset": 3898.96,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "to 30 person team. We we just like I I",
      "offset": 3900.799,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "three years ago I could not have",
      "offset": 3903.839,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "imagined the level of productivity that",
      "offset": 3904.88,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "we'd have. Yeah. Very cool. It's",
      "offset": 3906.559,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "awesome. I love what you're doing. Um",
      "offset": 3908.319,
      "duration": 3.121
    },
    {
      "lang": "en",
      "text": "thanks for joining us today. Is there",
      "offset": 3910.319,
      "duration": 2.28
    },
    {
      "lang": "en",
      "text": "anything else you'd like to share before",
      "offset": 3911.44,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "we wrap it up? I think that's it. All",
      "offset": 3912.599,
      "duration": 4.041
    },
    {
      "lang": "en",
      "text": "right. Thanks a lot, Steve. This is",
      "offset": 3915.44,
      "duration": 5.08
    },
    {
      "lang": "en",
      "text": "awesome. Appreciate it.",
      "offset": 3916.64,
      "duration": 3.88
    },
    {
      "lang": "en",
      "text": "[Applause]",
      "offset": 3920.7,
      "duration": 4.81
    },
    {
      "lang": "en",
      "text": "[Music]",
      "offset": 3922.45,
      "duration": 3.06
    }
  ],
  "cleanText": "I find the abstractions to be unnecessary. I find that the APIs that the foundation models have exposed around like tool use and structured output and everything are kind of sufficiently high level that not getting a lot of value from the genic frameworks.\n\n[Music]\n\n[Applause]\n\n[Music]\n\nWelcome AI Tinkerers. This is our global stage where we take people who are doing interesting stuff in Gen AI and we look under the hood of their side projects and their main projects. We try to get the snippets and the code snippets and things that will enable you in your projects. And today I am excited because we're here with Steve Krenzel of Logic Inc. Um and Steve is someone I've known for over 10 years who's done incredible things. And early in my generative AI journey, this is pre-Chat GPT, catching up with Steve and realizing that he was so deep in he was doing stuff that didn't have names like Rag yet or Agentic Workflows and he was doing it in the context of bigger companies. He's worked at Salesforce, he's worked at Brex, he's worked at Twitter and more and uh I'm so happy to have you Steve. Welcome.\n\nThank you, Joe. It's it's awesome to be here.\n\nTell us about Logic first and what it does and then we'll dive under the hood a little bit.\n\nYeah. Uh so Logic is all about putting intelligence to work. It's about uh taking this like new superpower that humanity has which is intelligence on demand and making easy to apply that either to like internal operational workflows or to product features you might be building and that's a super high level that could apply to anything but like where do you what is I know you're a ninemonth old company so ninemonth old company four people funded uh soon to be six people we have two engineers joining oh awesome um specifically you give us a document like an SOP or PRD some kind of description of a business process give us that document and a couple seconds later, we'll give you an API that implements that document. So from that's it. That's very high level vision. So it's like SOP in and API we handle the rest. Here's an API. Well, it's a welltyped API with back testing, documentation, everything you need to start integrating with it.\n\nOkay. And so today you would I mean that's really that could be anything.\n\nYes.\n\nCan it be anything?\n\nSo there are some constraints like it's very um we're not kind of doing any like multi-step agentic workflows. We're not interacting with external yet. Uh so it's it's it's very transactional in that you give us data that you want us to reason about or somehow interpret, we give you data back.\n\nWhat is a killer use case today? Like something someone's using like are people using it now? You have you have early.\n\nYeah. Yeah. So so prod we have a number of customers we're working with. In fact, I think we ended December uh completing about 145,000 tasks for our customers.\n\nWhat is a task? Like how big is a task?\n\nUh a task might be like moderating an item for like an e-commerce platform. So like some some inventory item comes in and they need like standardized titles, colors.\n\nOh, so some e-commerce store has like a data feed of like from their supplier just inventory from third parties. A couple of them come with all caps and it's like Yeah. Or there might be like rogue HTML tags in them or random phone numbers in them. And so like and so today today if I wanted to automate that Yeah. I would I would have to I have humans doing it and I'm like no no no. So today I would I would have to build an entire stack for my Genai data pipeline and then I'd have to use it and I have to test it. I have to create evals. I have to do all this stuff and then I could solve that like yeah we can moderate our content now that one task.\n\nExactly. And then I could do that over and over again for my top 10 tasks. I can rank them probably. There's I mean Accenture is probably collecting so much money doing this right now. And then you're saying you're saying like no no no you've already got a well-developed SOP. Yeah. Paste it in. Is that exactly it? And and can you show us like Yeah. Okay. Yeah. To really make that concrete, the the that specific customer that I'm thinking of right now, they gave us an SOP that they've been using for about 9 years. They've had a team of four humans kind of working off that SOP. We were able to put that in our system and immediately uh start uh automating portions of that workflow. And so like this is this kind of battle tested on really old SOPs that existed way before LLMs.\n\nDoes your system also figure out like, hey, here's what I'm good at, what I I still need the human to help me on.\n\nYeah. So depending on how you phrase the SOP, you can you can uh kind of ask for uh confidence scores on things and like if you're if you're moderating say messages between humans, which is like um user generated content moderation is another use case. You can say like should I escalate to a human because this is too ambiguous for me to handle.\n\nYeah, absolutely. Can you show us a real like use of it? Let's go.\n\nYeah, absolutely. So I thought it'd be fun to actually just generate an SOP on demand. Um, and so I'm I'm just going to have Claude write an SOP for kind of taking in biographical information about a podcast guest and then outputting interesting questions for that for that guest. Okay. And so this is, you know, this is an SOP that that our system obviously has never seen before uh being generated on demand by cloud. Um, and there will be nothing LLM specific in this. So there will be no chain of thought. There will be no few shot examples. It's just going to be pretty The SOP doesn't have that. Right. Right. Yeah. Okay.\n\nAnd when you give that to our system behind the scenes, we'll we'll provide all of that. Like we do a bunch of prompt rewriting. Got it. Um but let me just get this into Logic. Yeah. Cool. And so let's see. Jiren podcast interview. Let's call it uh for guest. We'll publish this. And so that as an end user that's kind of all you need to give us from here we we just we take it from here. So we'll um we'll do the prompt rewriting to make this more friendly for LM. We as part of this process we infer a schema for this process. So we'll figure out like oh what are the inputs needed to run this? What outputs are you expecting for it? We'll generate test cases for it so that you can have confidence like our system is doing what you what you expect it to do. We'll generate integration docs and a generate podcast interview is going to generate like stuff to prep the host like bio information and like questions that we could ask and things like that. Right. Right. So, so imagine you were making like a little a little like mobile app where you just wanted to quickly enter some like some details about your guest and get back a set of questions. Okay. And so the inputs would be like name or something and and Yeah. So the schema that that our system inferred was like a quick quick biography on on the guest um any kind of public things they published profiles and and maybe some previous interview appearances and then we'll our system will return uh a list that that follows this output schema.\n\nCool. And uh I can show so it looks like you're generating schema you're generating tests.\n\nYeah, generating tests and the the number of tests that we generate will we kind of leave it up to the LM. Sometimes we'll generate 30 tests, sometimes we'll generate in this case one test. Um and uh it gives you we also give you integration docs for your for your engineering team. Um and so in this we'll show you the actual schema that we're using behind the scenes. Um all the examples in here are dynamically generated for your process and we give you developer docs and with libraries.\n\nThat's so cool.\n\nYeah. code example. So if you um you can just take this example. We can hop over to our terminal, paste this in. It's like that whole meme of service as a software. Not really a meme. It's a real thing. But like you could just like paste it and it start a company. Put this on a URL.\n\nThat's the dream.\n\nYeah. Okay. No, that's really interesting. Um, and so but practically speaking as a developer then now I have an endpoint for moderating and I'm going to have to look at the the schema but it's like I have inputs like name or whatever and then I have outputs like the bio and questions. So I call that API that's what I get. I get that in out but it's following my SOP and then it's got this platform behind it to like that I can go inspect and can I see like what like are eval built in? How do I how do I understand as a user of this?\n\nYes. So, let me show you. Let's do um I'll just hop over to our We have a a welcome doc that is like a a goofy like you give us a a movie title and we we give you back emoji. Let me make this bigger. Yeah. And um and so like these these test cases uh like so every time you every time you change your document, we will rederive the the we call them the artifacts. So the schemas, the test cases, the integration docs. Sure. And uh we'll rerun these test cases. And so you can see we'll from like an eval perspective, you know, the the input for this test case is frozen. The the expected output is this. The actual output is different, but the test still passes. And we have the LM explain why it passes because both both sequences validly represent the movie frozen. The actual result includes an additional but but it's fine. And so the we actually as part of our eval framework like if this were to fail, the LM would say why it believes it should fail. So it's not like exact matching, right? Which you have that's all that's the definition of geni evals right there is you have to fuzzy match. And so there there's this kind of like uh fuzzy matching. We also do uh like uh history. Okay. This uh let me if I was changing my SOP I mean the test could change radically depends on the change on the SOP. Uh, so does that mean just sort of like the history could be more or less relevant? I guess that's right. Um, and so when we get into like part of the things we'll do is like we'll pull few shot examples from history that we've seen, but we kind of uh scope that down to a specific version of your document because because those inputs or the semantics might change between versions.\n\nOkay.\n\nUm, okay. Interesting. Can you take us under the hood? Like I know you've done uh AI in production at scale and some pretty innovative stuff. Like is there something in this system that you would be willing to share with the audience of tinkerers that you think is kind of a hardearned less or trick or something? Could be even an impromptu technique or something. I know you're full of these every time we talk. I get them. The nuggets. Yeah. Give us the nuggets. See if um that's that's a a tricky thing to do on demand. Uh the let's see I I'll say I don't know if I I might have mentioned this earlier but when one thing that we've found we've gotten a lot of value from is when we generate our schemas we we do a number of things like we always ensure that there's a a description for every field we find that this meaningfully helps guide the LM's output. Sure. Um we also always provide both the input schema and the output schema to the LLM. Um that's good for for general guidance.\n\nWhen are you running the LM after this initial run to generate all this stuff? Like when you're when are you feeding it back in?\n\nEvery time the SOP runs. Every single time we run the SOP, you use the full input and output with all the descriptions of Yeah. So the you know the the prompts that we generate behind the scenes are actually fairly large by it sounds like it. I was going to ask like what model do you need to use? Like do you fit in do you need claude for this minimum or Yeah. I guess OpenAI's giving you 128K now, right?\n\nYeah. Yeah. And so so and generally we're we're much smaller than that but like tens low tens of thousands of tokens or like 5 to 10 thousand tokens we have some do you have a feel for speed implication of of so first of all I'm really curious about speed how it relates to prompt size but also complexity like if you're asking it to think step by step and do more chain of thought advanced stuff in there does it get slower I know for the reasoning models certainly does but for base models does it no so so the the size of your input won't have too much of an impact on the latency like time to first token. Uh mostly most of your latency is going to come from how many tokens you're you're generating. And that just has to do with the like encoder decoder architecture of modern LLMs. The the uh yeah all all the efforts kind of in the in the decoding side.\n\nOkay. And then what do you see runtime performance versus uh across the different models right now? Do you use different models like llama 3? Do you use any fine tune for speed? Are you get are you into that at all yet?\n\nYeah. So, right now we exclusively call out to OpenAI. Um, so we're kind of in the in the OpenAI family of models. Okay. Um, most of that's just because we we really optimize for quality over over latency or even over cost. Got it. Um, it's really important to us that you can run a process a million times and without a human in the loop and have it work. So, you just want one really smart model that you can rely on and get to know really well and you're not going to optimize. And we we have just started dabbling with multimodel support, not to be confused with multimodal support. Um, and uh both within the the the OpenAI family of language models like GPT40 mini versus GPT40 where we'll dynamically choose between between the two. Um, but we're also starting to look at uh like calling out the Gemini for some use cases.\n\nWhat draws you to Gemini?\n\nThe the first thing that really drew us a a couple months ago were their embeddings. they just have really great embeddings um at least for from our from our testing and we we're we're starting to do more and more with semantic retrieval and and uh with semantic clustering and uh and so they had a really good embedding showing but then most recently what's good about it is the quality. Yeah, it's it it's you know embeddings are are kind of this fuzzy neb\n\n\nUlous uh array of vectors, but um, but for our use cases, we just found them to be uh, more relevant retrieval.\nYeah, interesting.\nAnd you tested that pretty rigorously.\nAnd um, uh, and so that that kind of first got Gemini or the like the the whole Google AI offering on our radar, but then recently all their Gemini announcements around their their models have just been really, really impressive.\nHave you played with the prompt caching or uh, exec code execution?\nThey have code execution.\nPeople don't understand this, but like in one shot, they'll generate and run the code, not just give you the tool.\nSo, we don't do any code execution, but we do, we use prompt caching extensively.\nAnd in fact, I guess this is another trick, like um, or like a tip for people building on LLMs.\nUm, structure your prompts so that they're append only.\nSo, a lot of people will use like a template library to like have this prompt, and then they'll kind of templatize names or data within the middle of the prompt.\nDon't do that.\nUh, describe the process that you want to happen, and then uh, and as part of this, describe the input data, and then append your data to the end.\nSo the attention, you know, the beginning and the end of a prompt is classically more, you know, a paid attention to.\nAnd so my worry is like, by taking the really important parts, the variables, and putting them at the very end, are you losing anything there?\nAnd have you tested that?\nYeah.\nUm, I mean, I get the implication for caching is super.\nYeah.\nBut that's why you're doing it, right?\nIt's it's motivated by caching.\nWe, we, we calls to the LLM, we try to maximize the prefix that we send to the LLM just so that you can benefit from caching.\nYeah, maximize the shared prefix, and yes, and and as a consequence, maximize caching.\nOkay.\nUm, and but you're not financial quality trade-off by doing this technique.\nNo.\nAnd and and uh, no, and I, I, I especially by appending the inputs to the end, I think, I think that's fine in terms of like not getting lost kind of in the middle of things.\nYeah.\nAnd it won't have any effect on chain of thought because you can still ask it to sequence its output in a chain of thought way.\nYeah.\nAlthough the way we do chain of thought is also interesting.\nWe like, we rely heavily on structured output for for chain of thought, like structured output a lot.\nStructured output is is probably one of the most underutilized uh, tools that that when people think about how to improve the output of of an LLM, in that like a lot of people think of structured output as just like a nice way to like JSON parse uh, you know, a response and just get like fields that they were expecting.\nBut the really neat thing with structured output is you actually get to guide and force the LLM to go in certain directions.\nAnd so like for a chain of thought, you can your output schema, if you're using structured output, you can say like analysis and then conclusion.\nAnd like in your analysis, you can instruct it to do its thinking, and the structured output will guarantee that the the fields that get generated will be in the same order that you define them in the schema.\nSo by forcing the analysis to be generated before the conclusion, you force the LLM to do things in a certain order.\nOne of the really cool things you can you can do with that is if you really want the LLM to go deep, your chain of thought can start with like an initial thought, and then you can say like, okay, revised thought.\nOkay, revise thought two, revise thought three.\nYou can, you can, if if you want, if you want the LLM to have to like think about it answer for say 20 steps, you can force that just by specifying a schema that forces it to to think about that for you.\nYou can't, you literally can't skip it because it's forced in the that's part.\nAnd one, one interesting example, um, we can, you show us any of these prompts, like an example of either the backloading your variables prompt, like from your code?\nYeah.\nYeah.\nUm, well, first let me show you, like, so in Logic, we have like one of our documents, uh, that that we use is like, we just send it a raw diff from Git, and then we have it comment on all these different fields, so like code complexity, naming conventions, code organization, testing, so forth.\nAnd so, wait, this is this is a Logic flow that you've built for for analyzing code?\nYeah.\nAnd then you're using it?\nOkay, that's cool.\nYeah, so we we build Logic with Logic.\nYeah, that makes sense.\nThat's really cool.\nAnd uh, and if we hop into some of our test cas, you expose these recipes into a marketplace or something eventually.\nYes.\nYeah, we're like a template library is is on our radar for sure because you could create just a project for that.\nYes.\nJust related to coding.\nYeah, that's really cool.\nAnd um, and so because we rely on structured output, even if the LLM has awesome and find security vulnerabilities to just constantly monitor the stuff that would be too arduous to do.\nYeah.\nAnd and and so even if the LLM has nothing of importance to say on a topic, we still force it to say like nothing, nothing significant here.\nSure.\nUm, but uh, if so, I'll show you, there's like a Japanese train platform operator.\nThey have, they have to do the thing.\nYes.\nYeah.\nIt's checklists.\nYeah.\nUm, one of the other fun, one of the cool things with with Logic is like it, if we have an idea of like, if we, if we just want something powered by an LLM, it, it's lightweight enough now that we, we just kind of do it, like if um, like our team doesn't write get commit messages anymore.\nSo I think, right?\nYeah, uh, so like if I just run, actually, let me do no verify because I don't, I don't want to run like our linter and everything.\nUm, you'll see Logic is writing your get commit message.\nSo we just took this diff, setting up the Logic right now, and in a few seconds, uh, we'll get back, uh, yeah, so, so I'm working on our handling middleware, and if you look, one of the really cool things with this is like, if you look at our actual commits, um, everybody has standardized commits.\nSo like if I look at the description of this, you'll see we always have like a top level summary, bullets of of what happened, and all of our, all of our commits are standardized.\nThat's so cool.\nUm, and so we don't have any just like work in progress commits or any low value commits.\nAt the end of the quarter, you can just, you can have it do your uh, your quarterly reviews too.\nYeah.\nSo, so I do at the end of the month, I actually give it our Git log, I, I have a different workflow that gives us our Git log and summarize like everything that that we did.\nYeah.\nBut, but you asked for an actual example of, let me show you one of, one of my fun, um, uh, bots that I actually published two years ago is my Pickhams bot that uh, picks teams and in the ESPN's Pickhams game, and actually need to up, I, I modified the bot for this year, but I didn't publish it yet.\nUm, did you present this at AI Tinkerers once?\nI did.\nYeah.\nAn early, like, I think the second AI Tinkerers event, Seattle, I presented this.\nSo a couple years ago, and then and then you, uh, Okay.\nAnd so you've, you've kept it going.\nYou collect a lot of data, and now you've, you've made improvements.\nYeah.\nAnd so the first year that I ran this, um, it was in the top 15% of all people on ESPN.\nUh, this year it was in the top 5%.\nAnd let me show you our chain of thought prompt.\nSo you can actually see, we have like this, this, actually let me make this much bigger.\nUm, so we have this this thought schema that is like reflect on your current approach.\nYeah.\nGive it, give it an award, and then determine a next step, like, do you want to continue down this line of thought?\nDo you want to adjust it, or do you want to backtrack it?\nAnd then in our actual schema, so our our final schema has um, an analysis and a conclusion, like I mentioned before.\nBut if you look at our analysis schema, you'll see initial thought, revised thought, revised thought, revised thought, and then final thought, and then final reflection.\nSo we actually force the LLM to keep thinking and keep revising.\nAnd if you, if you look at the output, you'll see, you'll see be like, &quot;Oh yeah, like this line of thinking is good, so I'm going to continue with it.&quot;\nOh, yeah.\nAnd and you and you, you can actually get a sense of its certainty in its decision based on how many times it continues versus adjust versus backtrack.\nYeah.\nVery cool.\nAnd this is open source.\nYeah.\nThis would be a great repo for anyone to study just on prompts engineering, obviously, to look at the code and and Yeah, for sure.\nAnd um, it, yeah, it this is uh, I'll just kick this off.\nSo like it's, it's fully autonomous.\nIt just like ops up on ESPN.\nIt'll start crawling news articles.\nIt'll figure out, Oh yeah, what what matches are in play.\nWe're just past the regular season right now.\nUh, so they won't, it won't do anything too interesting.\nI never told you this before.\nMy first job, computer science grad, straight into espn.com fantasy games team, built the live draft Java outlet.\nBut I, I know some people are still working there who can actually, I can put this on their radar.\nUm, funny.\nI didn't know that.\nOh yeah.\nYeah.\nAnd so now it'll, it'll kind of go through a bunch of different articles.\nThe the the prompt is long enough now that that it, it takes a couple minutes to to run through everything, but you can see initial thought, you know, at least for for this uh, an analyzing the article.\nCan I, can I read a couple of these?\nSo like article focus on Kansas City Chiefs primarily, Travisky's comments.\nAnd then here it's like it looks like it's repeating.\nNo, it looks like it's didn't really, it ran out of, it ran out of revision thoughts.\nIt just sort of, you've tapped out the intelligence of this guy, right?\nSo, yeah.\nSo, and for the for the news analysis, it tends to be pretty straightforward because all we're asking it to do for the news analysis is determine like, is this for fantasy, is this for college, or is this for NFL?\nSo, it tends to uh, get to the conclusion pretty quickly.\nYeah.\nOkay.\nUm, but when it gets, are you doing a One-Shot of like grab all the news and then just do this chain of thought, or do you cycle back and go like, because you could ideate questions and then go back to the web and get more answers, right?\nYeah.\nNo, we, so it's, it's a, it's a fixed workflow.\nOkay.\nWhere we, we hop on uh, like ESPN's news article uh, like the main news site, we grab the like top 20 headlines or something, and then we kind of fan that out into a bunch of parallel analysis.\nAnd we, we have um, let's see, I, I call them, in this repo, I call them tools, um, but uh, let me make this bigger.\nWe have a a news analyst tool.\nMhm.\nWhich is just, you know, basically figure out is this related to the NFL or not, and and and we filter out anything that's not related to the NFL.\nOkay.\nAnd then we have the uh, the predict the winner tool, um, where we, we actually give it the matchups.\nWe uh, inject the article like article summaries, and then we have it choose the the winning team, and the way we structure our prompts, like the for the for the winner tool, we actually like this is the schema that we define for that tool.\nWe unconditionally wrap these schemas in that chain of thought schema that I showed you earlier.\nSo like the complexity of that whole chain of thought process is just kind of confined to one file, and then anywhere else we call the LLM, we just unconditionally wrap this new schema in that super complex.\nGot it.\nOkay, that makes sense.\nSo you don't have to like rele it over.\nYeah.\nAre you using any frameworks for like the work, the execution of these workflows, uh, or is it just homegrown for this repo?\nUh, homegrown for this repo.\nHave you tried a gentic tool like Crew AI, Lang Graph, etc.?\nYeah, especially at Brex, I spent a bunch of time with Langraph.\nUm, and this was, you know, the summer of 23, so everything was was super nent, but um, y uh, some time with Llama Index.\nYeah.\nDabbled briefly with Crew.\nUm, I was just looking at the like the small agents thing that Hugging Face released.\nWhat did you, what were your conclusions after looking at those others on that one?\nI don't know if, if uh, if I, I find the abstractions to be unnecessary.\nI, I find that the APIs that the foundation models have exposed around like tool use and structured output and everything are kind of sufficiently high level that not getting a lot of value from the genic frameworks.\nSure.\nUm, do you ever worry that as the base models get better that using a framework might hold you back?\nYeah.\nYeah, because you're kind of forced to uh, uh, use what whatever uh, abstractions that that framework gives you.\nAnd I, I guess from from a certain lens, you could argue that framework will abstract away the nuances between different models, but at least for the things that I'm doing, I uh, I want to be a little bit closer to where the foundation models are.\nI all of the last two years, certainly everyone I talked to said, yeah, of course, I've experimented with different models coming out because it's fun and interesting, but yeah, I'm using GPT4 for everything because I want the best quality, I don't care about cost right now, whatever.\nDo you think in 2025, I think the two forces are going to be actual alternatives that are good quality on on different aspects or speed performance needs as we get these systems scaled, or it could be like the finally differentiated features that are being baked in, you know, like I mentioned Gemini has code execution ution baked in or grounding.\nI mean, Google's pretty good at search.\nI think the grounding is probably pretty good.\nI've played with it.\nPretty gamechanging.\nYeah.\nYou know, if you need to bring in data, you can probably rip out a lot of code, just use grounding.\nBut that's going to tie you to Gemini to some extent, right?\nDo you think this year we're going to see, we're going to see the similarly the agentic frameworks actually provide enough value where you're like, yeah, I'm not going to, there's so much value, like your alternative answer could be, yeah, there's so much value in these aentric platforms that you'd be dumb not to use them.\nI haven't heard a single person say that.\nWhen will we hear that?\nOh, that's a good question.\nUm, I don't know that we'll, we may never hear that.\nUh, but it seems like everybody quickly reaches feature parody as soon as one, like Gemini, OpenAI, Anthropic, um, uh, I guess AWS is trying to throw in with Nova now.\nU, that force that might bring you is not going to be a problem because they'll just yeah, parody it.\nAnd it's it's very similar to, well, adjacent, but like we have a and this\n\n\nIt was true at Brex as well, but like, we have a policy of no fine-tuning, uh, which is, which is kind of controversial.\nYeah, or it can be controversial in some circles.\nThere's a... my next guest are only in on fine-tuning.\nYes.\nYeah.\nSo, just, just as a matter of policy, uh, there's, there's tons of reasons, but the big ones are for, well, for logic.\nOne, the user experience we're trying to give to a user where they give us a document and we give them back an API seconds later that works.\nWe just don't have any data to fine-tune on.\nLike, there's nothing to fine-tune on.\nWe, we just need that to work out of the gate.\nUm, wait, you could fine-tune on the interim steps of like, how do you go from this to an API spec in whatever, right?\nOr schema, right?\nYou could, you could fine-tune on that stuff.\nThe internal workflows that we do to derive artifacts, certainly we could, we could fine-tune.\nUh, but, but you, you're saying to do that because it's an optimization.\nWe're too young to optimize.\nIs that the driver?\nPartially.\nUm, there, you could fine-tune on very narrow, specific...\nWell, what is your reason to not fine-tune when you're...\nYeah.\nSo, like, you risk comingling data.\nSo, like, you, you, if you're, if you're going to fine-tune, you really want to fine-tune on high-quality synthetic data, because if you, especially if you're dealing with customer data, um, you don't want customer... you don't want it to accidentally be regurgitated.\nUh, okay, you have concerns about incorporating data that can be leaked, right?\nSo then you need... so then you need like good synthetic data, and you lose visibility once you fine-tuned, right?\nOnce you give, once you fine-tune on a piece of data, you never know how that will be used or surfaced again, right?\nUh, but the, the bigger thing, or maybe the biggest thing for, for us, is typically models that you can fine-tune tend to be a couple months behind, uh, the kind of the state-of-the-art.\nMakes sense.\nUm, so like when, uh, you know, when 3.5 Turbo came out, you couldn't fine-tune it for like until like six months after it came out, and then every...\nSo, if you want to be at the bleeding edge, but also if you want to do like any kind of A/B testing between prompts, um, uh, you need to be able to just like dynamically call out to different models.\nAnd then the, maybe this is actually the, the real high-order bit.\nThe inner loop, uh, like your, your dev inner loop just becomes so much slower because every time you change your prompt, you now need to, you know, re-run the fine-tuning process, and now you're kind of in the previous era of like classical data science models where like you make some tweaks and then you go, it's like, oh, it's compiling, throwing, wait for those, yeah, team to like finish or flow to finish.\nSo, there's... so, I don't know if there's any individual, uh, like killer reason why.\nYeah.\nOkay.\nIt sounds like, yeah, the strategy reason for now would be like that you're then latching on to foundation models that are behind the curve, and you're picking up this operational dev maintenance task that slows you down.\nYeah.\nAnd you need velocity matters.\nOkay.\nOkay.\nThat's cool.\nThose are strong arguments.\nUm, okay.\nInteresting.\nSo, all right.\nThanks for showing us this, this, and, and so the performance went this year to what?\nOh, top, top 5%.\nAnd, uh, yeah, and so let's see if we can...\nUm, well, maybe it's not worth... doesn't that... I don't know about the... I'm not, I'm not up on it, but like, aren't there marketplaces where if you're top 5%, you can make some money?\nLike, maybe I'm not a... I'm not a sports gambler.\nI'm not even...\nSo, here's the funny thing.\nI'm not even really a sports person.\nLike, like data is amazing in sports.\nOne of the things I, I literally just learned is that they play football on Christmas, and so typically I run this bot on Thursdays because I wanted to have like the latest news, and Christmas was on a Wednesday this year.\nAnd so there were just three games that I didn't put any picks in for, cuz I, I didn't know football was played on Christmas, cuz that's that, but that kind of conveys how little I know about football, but I was still top 5% of, uh, players.\nSo, that's amazing.\nOh, that's really cool.\nOkay.\nUm, really cool.\nWell, maybe we could turn to our...\nSo, we talked about logic.\nUm, you know, I just want to go to the high level.\nSo, SOP to an API in the interim, you do schemas, you generate the test, you run it.\nThere's some visibility you may give people who are using it.\nNow, they're using it.\nThey can modify and tweak it, and you're keeping things up to date.\nThe next logical thing would be like, not only do I give you my SOP and my data, but it gives you tools, and I give you my APIs, and now maybe you can do some work for me.\nAnd that's got to be coming, right?\nThat's, that's exciting.\nYeah.\nAbsolutely.\nAnd, and then you can provide tools, too.\nYeah, right, because there's lots of things that I might want to do that are like, all this browser use stuff is interesting, but at the end of the day, like, there's a lot of work there to get over captions and to make it scale, and, and, and like, if you incorporated that into here, so my SOP needs a human to use a browser, that's like all of my SOPs.\nYes.\nRight.\nYeah.\nAnd you could actually use that tool internally, that's pretty powerful.\nYeah.\nAnd so, so we, we're, we're starting to get into some of that.\nUm, like third... the broader umbrella is like third-party integrations, but we, we, we call it tools and triggers.\nSo, like external systems that can trigger a document, and then external systems that a document can interact with, and, uh, and so that, that could be anything from Slack to browsing the web.\nWe, we just started doing some like, if you give us URLs that match certain media types, like PDFs or images, we'll crawl those and we'll incorporate those.\nIs that just on the SOP creation side, or is that also on the API usage side?\nOn the input side.\nYeah.\nOkay.\nIf you send us a link to a PDF, we'll, we'll crawl it.\nOkay, cuz I got, I got a stream of like invoices, PDFs that are coming in.\nI... This is...\nOkay.\nYeah.\nSo, like email, email thread, uh, specifically invoice, like invoice email threads is something where that we work with somebody on on doing.\nOkay.\nYeah.\nI guess email threads is a good... invoices and PDFs is a good...\nWhat are some of those other buckets?\nVideo, uh, social media data, and, um, uh, like one of the interesting use cases that somebody is using this for is forensics analysis on, on, uh, fraudulent invoices.\nSo, giving us like a new invoice and then historical invoices, and, and they have like a list of tells that they look for.\nOh, that's so cool.\nAnd, um, yeah, that was a...\nOh, I've, I've run of that SOP in an e-commerce business, and, and it's specifically with chargebacks and fighting the chargeback.\nYou have to gather data, do some forensics, and then if you present it, you can get the money.\nYeah.\nAnd as a vendor, if you lose that, you're going to lose 50 bucks as a fee.\nSo, they're doubly expensive.\nYou've already lost the product.\nYeah.\nYou know, you're never going to see the money, but then you have to pay this, this chargeback fee on top of that.\nSo, it's, it's a...\nSo, you know, it's to me, it's just a data problem, because if you can gather enough of this other corrupting data that this person scammed you, then the credit card company might side with you.\nBut, it's a lot of, it's very specialized work, and it's kind of irregular, right?\nAnd, and, and the specialized nature of the work also, like the, the threat landscape can change fairly rapidly, and the, the, you know, the partner that we're working with this on, they, they had their engineering team build something in-house that worked quite well, but the, their frustration with it was that their fraud ops team, who are the actual experts in how money moves around the world, would have to schedule with engineering every time they wanted to update that process, because like all this was checked in the code and managed in code, and so we allowed them to do like a version of control where the fraud ops team can now just update the document, we'll run the back test for them, uh, and, and when they're confident that it won't break anything, just publish it, and, and it goes live.\nUm, we have talked a lot about, uh, prompt engineering and within the context of the product and the side project.\nYou actually wrote the Prompt Engineering Guide while you were at Brex.\nIt became quite popular, right?\nCould you show people that?\nYeah.\nAnd just quickly give them a quick overview, just I want people to see this resource and have it in case they, uh, haven't seen it before.\nYeah.\nSo, tell us the backstory.\nSo, the, the backstory is, it was early 2023.\nUh, LLMs were, were just kind of becoming hot on the scene, and, uh, we basically, it fell on me.\nI, I, I ran a team at Brex called the Office of the CTO, and it fell on me to kind of get our 600 engineers to be thinking about LLMs and how to work with them reliably and why, what do they do?\nWhat can't they do?\nAnd, um, and I have a bit of a history in NLP and language modeling, and so I just kind of did a massive brain dump of everything I knew about language modeling at that, at that point in time.\nYeah, that's really cool.\nI wonder like, how has this evolved, like, versus two years ago, what in here is still relevant, what isn't?\nYeah.\nSo, it's, it's... I haven't looked at this for a minute, and it's, it's interesting seeing that the two-year, uh, time stamp.\nUm, the... I mean, if, if you, if you want the first section, I think is... it should be like required reading for anybody working with LLMs.\nLike, knowing, knowing the history of how, how we got to where we are is, is important.\nIt will actually be really good for your mental model of why certain things work well and why not.\nUm, I'm just going to skim through this.\nHidden prompts are still very much a big... well, now, now we would call them a system prompt.\nUm, but like, not one of the things I warn a lot in here is like, don't embed secrets in your system prompt.\nAnything that the LLM sees, you should assume the user will see.\nThat still holds true.\nJ-breaks are still very much a problem.\nY...\nUm...\nDo you, do you find yourself... um, I mean, the, the tricky thing with secrets is like, sometimes that secret context is necessary to get a good answer, but I don't want to share the secret context in my answer.\nSo, what are the... how do you get around that?\nIs it... I mean, beyond prompt engineering, I've, I've done personally, I've done things with like, for example, emails, I go, great, I'm just going to filter my output, this is post-LLM, to just obsuscate emails just in case.\nYeah.\nAre there things like that that you've learned in your toolkit, because, right, that you can't just say no secrets in a rag application, right?\nIt's... well, you, you shouldn't... you shouldn't give the LLM anything you don't want the user to see.\nAnd for example, like, I might be matching job candidates with jobs, and I got the full bio to make a good match, but like, when I give it to my user, I only want to give them the teaser.\nLike, how do I do my application and follow your role?\nThat's fair.\nOkay.\nYeah.\nYeah.\nAnd that's tricky because it, and it, when we, when we refer to like secrets in, in your prompt, there's like concerns about foundation models training on those secrets, which is, is one concern, and then there's concerns about leaking the secrets to the user.\nYes.\nAnd, and so, so solely concerned with the, the second camp, um, in your, in, in that specific case, I... so, the strategy will vary from case to case.\nIn your specific scenario, I would probably rely on structured output to kind of force LLM to maybe only output the title of a job, and then make sure that that title is kind of in your, in your set of titles, or, or, you know, whatever information you're comfortable giving the user, force the LLM to only output that, and that can't be prompt engineered by the end user, like prompt injected to like work around, because the structured output does a good job.\nWell, the structured out will, will mandate that that it outputs a, a title of, or in, in this scenario, a title of some sort, and then it would fall on you kind of post-processing and making sure that job title actually exists in your, in your set of job titles.\nUm, okay.\nThat way a user is not like, hey, it's April 1st, and all the job titles are now people's email address, right?\nI would never expose like free-form text to like generate free-form text to the user.\nAnd I know that that's a hard constraint.\nIf you need to do that, you need to start looking at like guardrail models.\nUh, and you think that's still true in 2025?\nAbsolutely.\nInteresting.\nWhy?\nWhere have you been burned?\nUh, for... I mean, I tread carefully enough that I haven't been burned recently, but, but in your testing, you... in testing, you've seen the, the danger.\nYeah.\nInteresting.\nWhere, where in this, in this Prompt Engineering Guide, what do you think is beyond that one?\nWhat else is relevant still?\nYeah.\nSo, um, uh, giving the bot a fish, which is like today in context learning.\nGive it, give it as much context as you, as you possibly can.\nThe more you, you tell it, the better it'll do.\nSemantic search has come so far since this post, uh, which is kind of neat.\nCommand grammars.\nUm, this was today, we have like code execution built into Gemini.\nSo, this might be less relevant, but if you, if you want it to... and we have, we have like function calling now, too.\nThis was written before we had tool use.\nUm, and so this was just a pattern that we found to give us the ability to, to use tools.\nUm, let's see.\nVarious ways of embedding data.\nI think a lot of this still applies, like you always structure your prompts in... we find markdown works really, really well.\nUm, different foundation models will be a little different, like Anthropic, you might want to use more XML tags.\nOpenAI, you might want to be a little more JSON heavy, but generally, if you're writing your, the meat of your prompt in Markdown, it'll be really happy with that.\nUm, there's just a ton of training data, especially from GitHub, written in Markdown.\nUh, and that, that includes like tabular information.\nYou want to use tables.\nUh, like LLMs will do really, really well with tables.\nIf you have a really large table, um, I need to... I need to validate this.\nSo, I...\n\n\nValidated this in a minute, but um, you used to have a problem where the headers of the table, it might lose context if you had a very long table. And so, in those scenarios, you might want to be more robust and have like JSON blobs where the field is repeated over and over again, so you get that locality. Um, but the way you embed data in your prompts, the higher takeaway from this is like the way you think about the way you embed your data in your prompts that it makes a big difference. You've reminded me of one of the biggest constraints right now that I think all the model providers give us, which is even if the context input could be fairly large, the output's not right. You're pretty, you're pretty limited there. And in an SOP where we can do anything, you've got to have cases where the end report is going to exceed 7,000 tokens or whatever. Yes. Like, what do you do in that case? Because doesn't that imply a complete refactoring of your prompt? Because you're now, you're talking by definition a workflow to actually assemble something that's longer than any one run, right? Yeah. So, so in that scenario, you would break up the process into smaller uh, like subprocesses. Um, how do you keep it coherent? Because you know, writing a report and asking it to do the intro and later the conclusion doesn't always result in a coherent. Yeah, fortunately, I mean, the output context, I believe for GPT4 right now is uh, around 16,000 tokens, 16,384. Okay. It's, it's a, it's a good chunk of space. Um, and we haven't run into any production issues where it's been a problem. Okay. In our internal use cases, you mean? Yeah. Okay. And um, in our derived artifacts, we've run into like when we do test case generation. Sure. Um, uh, we, we now have to, this, this might be an interesting sidebar. Our test case generation. We, we actually have one agent that enumerates kind of interesting cases to test uh, anywhere from one to 30, and then we fan out to individual agents to actually write the inputs and the outputs. Um, and we landed on that. Our initial take on this was to have a single agent just like write, you know, write 10 interesting test cases, and very rapidly, especially for larger inputs outputs, we exhausted our context. Makes sense. Um, and uh, so yeah, so, so now we, we have like one person enumerate or one agent enumerate interesting test cases, then individual agents uh, write the detailed test case, and then we have a firewall uh, between the agent that actually runs those test cases and, and so that's why like we may generate test cases, and the first time you run them, they fail, and, and, and they fail because we're, we're doing like real, we're doing it properly. Right. Okay. And you don't want the, you don't want the person running the test to know. Yeah. Of course, that makes sense. And on the first part about feeding it to multiple, just to implement each test, you are, you feeding like the essentially the entire outline, it's you do number three. Yeah. Basically. Okay. So it's simple. Um, interesting. Let's see, let's talk about, so we've talked about that. Um, let's talk about, um, we've talked about speed and quality extensively. Can we just talk about what's going on in the technology world right now in AI? We're on an exponential path of just releases. December was a huge month. I can't believe even Christmas day. Yes. Yeah. Like it was non-stop. Do not go on vacation. You're gonna miss fundamental stuff. Speaking of like deep seat out of nowhere, right, which is intellectually very interesting for a lot of reasons. Yes. Um, and it can't even answer all the questions because of censorship, and you know, I mean, it's all sorts of, and the training cost rumors. It's like a whole interesting thing, like, like the source of your foundation model may start influencing whether or not you use it hugely. Hugely. Uh, then you've got um, really good modes to Ford open source on coding standards, which are going to be relevant for what you're doing. V2, I don't know if you've played, I have access to V2, the trusted tester Google video, it's mindblowingly good. The quality would fool any reasonable person. This is real video footage. There's no question about it. Unlike previous generations of like, I was pretty good, like 80% of the time I could tell something was AI generated image with, with the V2. I mean, and so that just came out, that's not even out yet. It just was announced. Um, you've, what are some of the things that have most impressed you or, or, and do you ever worry doing a startup like you're nine months in, nine months from now you could be like, oh, whoops, like everything we built last year is one prompt away in this new thing. Yeah. 03. I mean, fortunately, so far, we, we, we've built to be kind of foundation model agnostic. And so whenever there's these improvements, we've just kind of gotten to take advantage of them. Like, uh, but does your whole company get done in one shot and I pay OpenAI $2,000 to run for 15 minutes? Do you understand what I'm saying? Yeah. I don't think so. I ask the hard questions. Yeah. Yeah. Yeah. In that the, if you're trying to productionize any uh, any kind of business workflow, you're going to want rigor around like, like you're going to want back testing, you're going to want like versioning, you're going to want to be able to roll back. You're going to want like access control over who can approve schema changes versus who can approve content changes. Okay. Uh, all like kind of all the rigor around that process changing. That's really where our value is. Yeah, that makes sense. And certainly companies could move into that, and and I think we're starting to see some startups get into that. Um, is there an analogy between like what happened with like, I don't know, going from like horses to cars and and building highways and stuff? Is there, are we just so early that it's hard for us to see because that worry of like, I'm just going to, you know, the next reason he gets to a tipping point where it's like I can, you know, self-improve and I can run like all the stuff that you're doing that you're a smart guy and you've got a great team. Yeah. Or doing it will just do out of the box and it'll run its own code and like, so therefore your customer can access that. Like, do you think about that stuff? Do you care about it, or do you have a strategy that says, no, we're going to be okay? And I think a lot of people don't necessarily have a great answer here yet. Yeah. I mean, it's tough to, you know, you can't predict the future. I, I'm feeling pretty good about where we're at. Um, but is there an analogy for, for like other technology disruptive technology revolutions where it was like in the end, it was okay, like we didn't, like I remember when the internet came, it was like, oh, I don't know, like maybe the wallet will be baked into the browser. Why do we need to have PayPal? Like, why do we, Yeah. When it's early enough, you don't know.\nYeah. I don't know if an analogy comes, Yeah. comes to mind. Yeah. Okay. Interesting. Um, so back to like things that have been released that have been impressive that you're like, \"Oh, we need to go and like take advantage of that or look into it.\" Yeah. And obviously a lot has happened. I think maybe if you go back six months in multimodal, for example, you're talking about ingesting things, and that's got to be on your radar. Are you using multimodal models to do your processing now? Yeah. Yeah. So we, um, you mentioned PDF ingestion. So take me down to like detail level there, like how do you approach understanding a PDF? Yeah. So what charts and graphs and things that might be in there? Yeah. So, so especially business documents, they tend to have like tables, figures, charts, um, visual components that might have logos. Uh, do you get any like scanned in and filled out by hand? Yeah, absolutely. Yeah. And um, and so the, the, the kind of naive approach of which will get you pretty good mileage of just giving the raw image to uh, to the foundation model will, will do pretty good. Um, we've gotten a lot of uh, mileage from doing some pre-processing, um, and and various like OCR techniques, and and uh, there's, there's tools out there like with, with special models for extracting tables from documents, table oriented models. Um, there's actually a really awesome project that I think was trending on GitHub just a week or two ago, uh, called Dockling. D O C L I N G. And, uh, let me see if I can pull that up real quick. Cool. Uh, yeah, Dockling. And this, like, if you're sending any documents or images to an LLM, I would highly recommend you run it through this first. Um, what are the competing tools that might have been around before Dockling that that you'd tried? And I'm just trying to understand, I'm going to establish where is Dockling in this set of, because there's a hodgepodge of techniques. Yeah. And tools and open source, right? Yeah. Where does, where does this sit? What made it, what made it stand out for you? Just the ergonom, like, this, this is actually just using, leveraging a bunch of other tools underneath the scenes, like a bunch of different hugging face models. It's just a wonderful, it just brings together a bunch of existing technologies in a really nice ergonomic package. Okay. Wow. PDF, DocX, P, PowerPoint, Excel. They're doing a lot more than just PDF. Yes. They're a document. Yeah. Uh, and, and they're wrapping a lot of other libraries and best practices. They probably have a router or some other stuff. Okay. Yeah. Interesting. Cool. And then you got this word of the community to sort of like make it better, right? And let me see if I, Yeah. So here, like, here's an example of, let me, Does it handle chunking and and vector indexing of long documents here? No. No. The, the, it's, it's purely just, um, uh, so like, here's, here's a sample invoice.\nMhm. You, you send this over to Dockling. I already have the output up here. And it'll convert it to like a nice uh, just markdown formatted. Wow. Uh, chunk of text that you can embed in your in your LM and LMS's, I think I mentioned earlier, like love markdown. And so, so we'll, whenever we give an LM an image, we'll also kind of pre-process it and give it kind of a textual. Okay. So you're not outsourcing to Dockling. Could that would be a naive approach, but you're saying, no, use doc to create struct structured text extracted from lots of document formats, but also pass in the original. This is, yeah, this is complimentary to providing the image. It's not, it, when you say providing the image, are you like just the graphs and and image data within a PDF or screenshot? So if you give us a PDF uh, with 20 pages, we convert it to 20 images and send, send all the images. Yeah, I like such an elegant approach, it's like just works, you know, screenshot every page, run it through Dockling, put those two together, and that works. Yeah. Yeah. And, and it'll reconcile. Um, when I, when I say, and that works, have you like sort of scientifically measured that quality output of that approach versus bespoke approaches or things that people were tinkering with, you know, a year ago? Yeah. I mean, we, we measured the out, like the output quality. Um, are there benchmarks for PDF extraction at this point? Like the public benchmarks? Oh, in, in that sense, like in terms of like uh, like publishing a paper on quality, I don't think we've done benchmarks of of that rigor. Mostly it's just like our customer has a problem, here's one solution that solves it. Oh, like, oh, we changed that solution. Is this better? And um, because ultimately you're trying to like get to human quality. Like if a human was tasked with doing this SOP with the PDF all day long, how many mistakes would they make, and can your approach be better than that? Yeah. And, and one of the, one of the um, like really interesting data points that we just got uh, a week or so ago is one of the use cases that we're in production for uh, has to do with um, it's e-commerce related inventory moderation, and um, and the humans working on the task would get, would make kind of gross mistakes, uh, large mistakes in about 24% of use cases, and we found that the LM would make mistakes in about 2% of use cases. So it's huge, huge reduction in rate of error, and uh, that was based on a human expert eval. So just looking at hundreds of examples, uh, having that analyzed by the human, and the uh, the CEO was kind of in such disbelief at that number that we did a second human eval with the CEO as the evaluator, and the the result was replicated. So, so when, when we do uh, kind of analysis, that's typically sometimes we'll use GPT4 as a judge, um, or we'll use a human as a judge. Very cool. Have you tried um, just going through other things that have come out, um, Anthropic computer use MCP? Have you used the new real time voice APIs or just enhanced voice open? Dabble with a few of them real, real briefly. MCP. Yeah. In fact, we, we did a series of uh, blog posts over the holidays, and one of them is on MCP and kind of how to set it up, and um, we've been thinking, we just started kind of as, as part of our like third party integration strategy, getting into MCP would be really nice because imagine you're just talking with Claude, and like Claude can just like call out to different documents, like business logic documents. It might be a place that business customers get to is just using cloud as an interface. Yeah. And, and, and then you could off them into your system or whatever it is. MCP is quite nice. I, I'm bullish on MCP. Yeah. It's a really nice design. Do you have, you seen any other products implement it yet? Uh, or other competing entities use it as a standard? Will it become a universal standard? Yeah, right now the number of integrations is pretty tiny.\nUm, I will give a shout out to Everart, which is uh, founded by a former colleague of mine from Brex. They just launched an MCP integration. Um, I suspect over the next three to six months we'll see an explosion of MCP integrations, but we're not quite there yet. Yeah. Very cool. Um, is there anything else you want to show? Anything else you're tinkering on? What other side project you got going on, Steve?\nYeah, I think, um, uh, one other, so like, we, we, we talked about like transactional workflows. Yeah. And then we talked about or the transactional use case, then we talked about the pickins\n\n\nBot, which is kind of a predefined workflow.\nUm, but we haven't really discussed like fully autonomous agents yet.\nAnd that's something behind the scenes at Logic, we're always kind of tinkering uh with more complex workflows and more complex agents.\nAnd over time, we plan to make those more accessible to our customers.\nWhat's an example of one that you've played with?\nYeah.\nSo, I'll I'll I'll show you.\nUm so, we we have um we we this is an in-house agent that we built.\nWe call it Party, uh programmatic artificial developer, and um and this is kind of in in response to just like we have we use AI to like write our our commit messages and we have AI to review our code.\nWe also want AI to help us write our code, and I think just like many developers today, you know, we use cursor, uh um Ader, uh if you're in Vim, code companion, um but I found especially on the open source side, the quality of coding agents is way behind like the latest of cursor and Devon and um uh and so we we've just been experimenting with like building an open source uh coding agent uh and it works at the terminal sounds.\nYeah.\nYeah.\nYeah.\nSo so it's all it's it's like 100% terminal.\nShow us.\nYeah.\nYeah.\nSo let let's see.\nUm let's just make a a make like a is the idea though that I would fire this up in an existing repo or uh yeah so it can work so so this is designed to work against the Logic codebase so we can give it we use linear for task management.\nWe can give it a linear task and it'll go and implement it, but we'll we'll just do something from scratch here.\nOkay.\nOkay.\nOkay.\nSo I'll create a directory called and this do you plan to open source this just getting to the I want to play with it too.\nOkay cool.\nUm, will it use Logic on the back end to implement itself or is this too?\nSo, right now it's fully self-contained.\nUm, if we if we GA Logic before we open source this uh there will probably be some kind of Logic integration.\nYeah, but but I don't think there's no universe where Logic would be mandated.\nUm, so we'll give it a a project route of uh test tic tac toe.\nWe'll give it a description of um implement a simple tic tac toe game from scratch.\nUh setup get and mpm and everything.\nUh give it a simple though static tend only.\nOkay.\nUm, and so, so one of the things this agent can do is execute comm execute commands on your behalf.\nAnd by default, we have it ask you if you if it can run the command.\nBut I'm going to run with uh no fear mode.\nYou're brave.\nAnd uh just let it let it loose in my terminal.\nOkay.\nUm and it's telling you what it's doing here.\nIt's basically installing it all the things it's going to need to create this this project.\nYeah.\nAnd so this is actually broken down into three sub agents.\nThe the first agent is a planner.\nUm and it can only list directories.\nMhm.\nAnd read files.\nThat's all it can do.\nSo, this is going to crawl the code base.\nThis is a simple one.\nIt's an empty directory.\nSo, it listed the directory.\nAnd just based on the amount of time it's taking for this response, it's coming it's now coming up with a plan.\nY uh to turn this empty directory into a a tic-tac-toe game.\nAnd so, this is the detailed step-by-step plan.\nThen, we have another agent that will review and revise this plan.\nUm, wow.\nAnd then once it's reviewed, yeah.\nSo the plan provides a comprehensive guide.\nSo the the reviewer signed off on it.\nNow it gets passed to a third agent that is an engineer that actually implements it.\nAnd the engineer has additional tools.\nThis is the engineer that can actually run commands.\nAre they going to write tests also?\nUh it might if we if we ask it to write test, it would.\nSo you can see, you know, here it's like initializing a git uh repo.\nUh adding node modules to our git ignore, initializing npm.\nNice.\nUh how does this compare to like open devon or something?\nDo you know?\nI haven't done like a head comparison or like I haven't done a regular comparison the approach of the how it manifests to you as a user or a developer the one of the I'll say the the open source agent or coding agent that I spent the most Oh, and well, here we We have we have tic tacto and uh Yeah.\nSo the game board's a little a little but we you know we we could we could have it revise that.\nYeah.\nYeah.\nYeah.\nCan is it still in a loop where you could actually give it feedback?\nUh there's no yet interactivity where this is at right now.\nBut we could I could um you know change the description to say like uh Oh.\nSo if you run a new description here, it's going to be okay.\nYeah, but I need to look at the code base I'm in already.\nYeah.\nSo, let's make it a lot.\nOh, that's really cool.\nBeautiful.\nUh how how in today's world of this implementation, how with a very large codebase, what would it do?\nYeah.\nSo, that that's kind of the neat thing.\nSo, the the agent right now it when it runs it has no pre-existing context on the codebase.\nAnd so, everything it gathers it it just gathers by crawling the codebase and listing directories and reading files.\nYeah.\nUm, you have a step where you're gonna vector index that and do all that stuff.\nThere's a counterpart where we Yeah.\nSo, there's a there's a counter.\nNo.\nSo, so there's a a secondary project that we're working on that actually indexes your codebase for you and actually comes up with it not only will index the content of your code, um, but it'll actually look at your git log and find like, oh, these these files tend to be modified together and like, oh, you use these patterns and like every time you check in code, you add tests.\nSo, if you do that and then the last thing you just need is a is a VS Code plugin of some kind.\nYep.\nThen we're done.\nWe've got a open source like that's Yeah, that's really cool.\nAnd so just while this is wrapping up, we can kind of we can just hop in here and we can, you know, uh that's super I'll I'll just cat this instead of it up.\nSo, you know, simple, you know, this is a very simple kind of toy example, but um but it looks like it completed and it's it just added everything to git.\nIf we hop over to git, we look at our log.\nYeah, we should see two commits now.\nAnd if we refresh a little better, try harder.\nYeah.\nYeah.\nPut the crypted lines.\nUm, that's really cool.\nHey, you you were Were you Correct me if I'm wrong, when we met, you were like a Ruby on Rails guy, right?\nYeah.\nYeah.\nOkay.\nAnd then now, are you doing Ruby on Rails at all anymore?\nNo.\nI mean, I there's a I have nostalgia for it.\nFine.\nBut what do you use for like your production right now for Everything Everything's TypeScript.\nI'm all in on on TypeScript.\nUm, we just because of the ecosystem sometime.\nSorry.\nback end everything's everything's typed end to end typescript um we because of the tooling ecosystem like dockling we have started incorporating python into some places but we really keep python kind of isolated you run it as a process somehow you ever run servers that are running stuff in the python because you're not using any third party uh you know the python runs like part of a work it's just a binary that we we call out to okay cool nice yeah um and that's just so you can get things like dockling or whatever that are out there yeah yeah because the for better or worse so much of the ecosystem uh the data science and like uh machine learning ecosystem is in Python.\nYeah, I'm I'm doing everything that I do is Ruby on Rail still and I bridge over via APIs.\nYeah.\nSo I haven't had a need to do I mean I do lots of like library incorporation that is based on Python somewhere but like you I keep it contained.\nYeah.\nI haven't uh Ruby back in like this would have been like 2010 2012 uh with like HAML and coffees script and all that good stuff.\nI I occasionally miss it.\nThere are things that Rails still does better than than anybody else.\nAnd I think the thing for me I think I think one of the most compelling reasons probably although the languages models don't seem to care a lot of the serverless tooling especially but even the models themselves default to like Python world and and and even even they're even opinionated on stuff like which uh CSS framework to use.\nYeah.\nReact is obviously like a default when output.\nIn fact, I've fed when I feed my codebase a lot of times it's like, &quot;Oh, here's, you know, it's basically telling me you should rewrite it in in React.&quot;\nYeah.\nI'm like, &quot;No, no, please don't.\nNot yet.&quot;\nSo, I actually have a a draft post uh uh called New Year, New LAMP stack where uh LAMP is LLM aware methods and practices.\nAnd so, it's all about how LLMs are starting to shape our technical decisions.\nAnd like you get we get so much leverage from the LLM that rather than fight it and try to make it accommodate our decisions, we embrace the decisions that it is most like most comfortable with.\nI'm I'm excited about that philosophy.\nA lot of lot of like piecing together I think the evolution just in two years where LM are so unreliable that you have to really constrain them and then you know if I'm creating an email I'm going to like do it the old way and then this one piece of dynamic content I'm going test the heck out of it.\nI'm going to insert it into that paragraph.\nThese days I am quickly becoming like throw all that away.\nI'm just gonna ask the really smart model.\nI'm going to tell it everything about my user and my goal and I'm going to write the email.\nYeah.\nAnd then decide if you send it or not.\nIs it appropriate?\nAnd I'm going to have a better experience for my user with more conversion or whatever it is.\nAnd I'm increasingly doing everything that way.\nAnd it makes me believe that like well all the CRUD literally that I create will be replaced with disposable software that's created and executed on the fly including the UI components.\nWhy wouldn't it be?\nIt's some maybe not this year but next year.\nAlso this notion it's more like kind of spec driven development where you just kind of describe what you want to happen and you don't really care about the specifics.\nAnd it and I I think one of the interesting divides that we'll we'll start to see uh kind of bifrocate more and more as time progresses is like companies that had code bases that existed before 2023 and companies that created code bases after 2023 where like the way you structure your codebase the technical decisions you make any anybody creating a new codebase today is is probably going to structure it if you're making it friendly for LLM is probably going to structure it very differently and you kind of get into this what would some of those components you think would be everything from uh the size of your files.\nLike you no longer want single files that are 10,000 lines long.\nNot that you ever wanted this, but like you you certainly don't want a single source file that's like 10,000 lines long.\nUh the the libraries that you're using um the the naming conventions like if you have the LLM working in your codebase and it starts hallucinating methods or variables, that's probably a sign that your code isn't readable.\nSo like make implement those methods, implement those variables because DLM's assuming they should exist more more times than not they they should exist and your codebase will be better for it.\nAny kind of libraries that it's using um training data so like a lot of the bench a lot of the benchmarks are Python but a lot of the the a lot of the models work really well with Python or JavaScript that could have have to do with the underlying training data.\nUm, but it kind of hearkens back to like like the way we build factories.\nLike if you if you have an existing factory and you want to add automation to it, you kind of need to adapt the machines to the factory, right?\nWhereas like if you're building a new factory from scratch, you adapt you can change the factory to work with with the machines.\nYep.\nUm, and I think uh you you see that same divide with new new versus old code bases where you can just make your new codebase work with the machines.\nYeah, it's a great great opportunity for uh new companies to come in and move be able to move faster because they architect them in a way that will allow them to naturally move faster.\nYeah, I mean it's it's like our four-person team.\nIt feels like we're we're doing the work of like a 20 to 30 person team.\nWe we just like I I three years ago I could not have imagined the level of productivity that we'd have.\nYeah.\nVery cool.\nIt's awesome.\nI love what you're doing.\nUm thanks for joining us today.\nIs there anything else you'd like to share before we wrap it up?\nI think that's it.\nAll right.\nThanks a lot, Steve.\nThis is awesome.\nAppreciate it.\n[Applause]\n[Music]\n",
  "dumpedAt": "2025-07-21T18:43:24.191Z"
}