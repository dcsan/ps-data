{
  "episodeId": "PU2h0V-pANQ",
  "channelSlug": "@boundaryml",
  "title": "Cracking the prompting interview: ðŸ¦„ Ep #9",
  "publishedAt": "2025-06-13T14:45:03.000Z",
  "rawLines": [
    {
      "lang": "en",
      "text": "you we've seen this in like SQL",
      "offset": 0.32,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "generation or maybe this is a tactic we",
      "offset": 2.96,
      "duration": 2.879
    },
    {
      "lang": "en",
      "text": "can talk about today like we've seen it",
      "offset": 4.4,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "like SQL generation okay have the model",
      "offset": 5.839,
      "duration": 4.001
    },
    {
      "lang": "en",
      "text": "generate a JSON object that can be",
      "offset": 7.839,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "determined turned into a SQL query for",
      "offset": 9.84,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "SVGs the TL draw guy was talking about",
      "offset": 12.32,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "this at AI engineer last week have the",
      "offset": 14.4,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "model generate a structured object that",
      "offset": 16.48,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "it's good at writing that then",
      "offset": 18.32,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "deterministic code can turn into an SVG",
      "offset": 19.84,
      "duration": 5.599
    },
    {
      "lang": "en",
      "text": "and I think have the model generate code",
      "offset": 22.32,
      "duration": 4.959
    },
    {
      "lang": "en",
      "text": "that then you can like bake it's like",
      "offset": 25.439,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "creating different views of the same",
      "offset": 27.279,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "thing. Um, and then once that's baked,",
      "offset": 29.359,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "then you can deterministically execute",
      "offset": 31.92,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "that code with a programming runtime.",
      "offset": 33.76,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "Yeah. All right. Well, with that, let's",
      "offset": 36.559,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "get started. Um, my name is Byov. This",
      "offset": 39.36,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "is Dexter. We've been doing this every",
      "offset": 42.399,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "week for the last few weeks now. Um,",
      "offset": 44.559,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "it's been months. We started in March,",
      "offset": 47.6,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "dude. Oh, wow. Yes. But we took a break,",
      "offset": 49.2,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "so I don't know if that counts. The",
      "offset": 51.6,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "break is where I define the line. Um,",
      "offset": 52.879,
      "duration": 5.041
    },
    {
      "lang": "en",
      "text": "okay. But regardless, uh, the whole",
      "offset": 55.44,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "point of this these episodes with AI",
      "offset": 57.92,
      "duration": 2.959
    },
    {
      "lang": "en",
      "text": "that works is to talk about real",
      "offset": 59.44,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "practical AI applications where we don't",
      "offset": 60.879,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "just talk about highle stuff, but really",
      "offset": 63.359,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "try and show the code behind how things",
      "offset": 64.879,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "work. Uh, we've talked about a bunch of",
      "offset": 67.28,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "things in the past from MCP servers with",
      "offset": 69.52,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "10,000 plus tools to 12 factor agents by",
      "offset": 71.119,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "Dexter all the way to human learn how to",
      "offset": 73.76,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "use humans as tools and then just really",
      "offset": 76,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "how to think about prompts. But today, I",
      "offset": 78.32,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "think we want to do something that was",
      "offset": 80.88,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "different. It's going to be a lot more",
      "offset": 82.24,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "varied in conversation than our previous",
      "offset": 84,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "conversations which are all about",
      "offset": 86.4,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "focusing on one depth thing. Today we",
      "offset": 88,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "want to talk about just prompting as a",
      "offset": 90.32,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "whole. Nothing fancy, just plain old",
      "offset": 91.92,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "prompting. And many of you",
      "offset": 94.96,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "uh and actually Dexter, do you want to",
      "offset": 98.4,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "give a little precursor while I get the",
      "offset": 100,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "screen recording up? Well, I think like",
      "offset": 102.159,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "many of the things that we end up",
      "offset": 104.4,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "talking about, you can take like what is",
      "offset": 105.759,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "a really simple problem that folks kind",
      "offset": 107.6,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "of can look at and just say, &quot;Oh, that's",
      "offset": 109.92,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "solved.&quot; like like classification. It's",
      "offset": 111.439,
      "duration": 3.281
    },
    {
      "lang": "en",
      "text": "like, okay, I know how to pass the LM a",
      "offset": 113.04,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "list of labels and get it to output one",
      "offset": 114.72,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "of those labels with structured outputs",
      "offset": 116.88,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "or something like that. And then you go",
      "offset": 118.32,
      "duration": 2.64
    },
    {
      "lang": "en",
      "text": "and you look under the hood and it's",
      "offset": 119.92,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "like, oh, like actually there's a lot of",
      "offset": 120.96,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "room where I thought the ceiling was of",
      "offset": 123.6,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "like, okay, here's the techniques.",
      "offset": 125.92,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "Here's how you do it. There's so much",
      "offset": 127.04,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "more room to basically open up the box",
      "offset": 129.28,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "and rip out all the wires and redo",
      "offset": 133.04,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "everything and like engineer it to get",
      "offset": 135.12,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "much better results. And I think like",
      "offset": 138.16,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "the core of that is always prompting.",
      "offset": 139.92,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "Um, and so I'm really excited today to",
      "offset": 141.76,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "learn about both like just some basic",
      "offset": 144.56,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "techniques framed in terms of certain",
      "offset": 146.8,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "types of problems. Um, and and I think",
      "offset": 148.959,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "today one of the things that it will be",
      "offset": 151.92,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "cool is um, we're not going to talk as",
      "offset": 153.519,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "much about like one big overarching",
      "offset": 155.36,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "problem like we usually do. We're just",
      "offset": 157.2,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "going to give you a grab bag of um small",
      "offset": 159.04,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "tips and tricks that are reusable across",
      "offset": 162.16,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "problem spaces and like lower level",
      "offset": 164.08,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "advice that you can apply to lots of",
      "offset": 166.08,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "problems. And I think hopefully uh if",
      "offset": 168,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "folks are down, I think we put a thread",
      "offset": 170.4,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "in in the Boundary Discord. Um if anyone",
      "offset": 171.76,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "wants to share their prompts, uh the",
      "offset": 174.879,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "most I've ever learned about prompt",
      "offset": 176.8,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "engineering is showing Fib AI",
      "offset": 178.239,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "applications that I've written and",
      "offset": 180.08,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "having him roast my prompt and tell me",
      "offset": 182.239,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "what we're doing wrong.",
      "offset": 184.4,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "Um actually with that what I'll do is in",
      "offset": 186.72,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "the thing in here I will actually just",
      "offset": 189.44,
      "duration": 6.879
    },
    {
      "lang": "en",
      "text": "post a link to this thread copy thread",
      "offset": 191.28,
      "duration": 8.8
    },
    {
      "lang": "en",
      "text": "and I'll post this in chat if uh anyone",
      "offset": 196.319,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "wants they're welcome to post their",
      "offset": 200.08,
      "duration": 2.879
    },
    {
      "lang": "en",
      "text": "prompts that they want to share. This",
      "offset": 201.599,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "will be recorded",
      "offset": 202.959,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "um and like just post it on here. We'll",
      "offset": 204.8,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "fix your prompts at the end and we'll",
      "offset": 207.76,
      "duration": 2.479
    },
    {
      "lang": "en",
      "text": "just show you how we would think about",
      "offset": 209.2,
      "duration": 1.84
    },
    {
      "lang": "en",
      "text": "them. Doesn't mean that they'll",
      "offset": 210.239,
      "duration": 1.92
    },
    {
      "lang": "en",
      "text": "necessarily get better. It might just",
      "offset": 211.04,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "give you another technique or two. But",
      "offset": 212.159,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "with that, let's go into the topic.",
      "offset": 214.56,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "Cracking the prompting interview. I",
      "offset": 216.159,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "think prompting is literally like",
      "offset": 218.08,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "software engineering. And we're just",
      "offset": 219.599,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "going to use the same techniques to do a",
      "offset": 221.2,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "couple of things off the bat. So, let's",
      "offset": 222.799,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "start off with a very common problem",
      "offset": 225.12,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "that I always see, which is always the",
      "offset": 226.4,
      "duration": 5.119
    },
    {
      "lang": "en",
      "text": "first one that I'm going to talk about,",
      "offset": 230.319,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "which is like labels.",
      "offset": 231.519,
      "duration": 5.601
    },
    {
      "lang": "en",
      "text": "And this I think the most common example",
      "offset": 234.64,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "of this problem that I see is citations.",
      "offset": 237.12,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "So, imagine that I have a prompt. My",
      "offset": 239.439,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "prompt will have a bunch of text that I",
      "offset": 241.68,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "refer to it in for the context of rag.",
      "offset": 243.519,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "With the rag, I will have it give me",
      "offset": 245.92,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "like the URL or something attached to it",
      "offset": 247.68,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "and I will have a bunch of these",
      "offset": 251.2,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "along the way. So I have like a URL with",
      "offset": 253.92,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "some data and then I want to go get that",
      "offset": 255.76,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "and somehow in my answer I want the LM",
      "offset": 258.079,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "to give me out the URL.",
      "offset": 260.799,
      "duration": 5.761
    },
    {
      "lang": "en",
      "text": "Uh this is this a problem that I",
      "offset": 263.28,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "resonates with this couple people. Does",
      "offset": 266.56,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "anyone have ideas for how we could make",
      "offset": 267.919,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "this better?",
      "offset": 269.36,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "If not, we'll just go right into it. Uh,",
      "offset": 274.88,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "if today's session is going to be Are",
      "offset": 277.04,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "you gonna are you going to replace the",
      "offset": 278.8,
      "duration": 5.119
    },
    {
      "lang": "en",
      "text": "URL with a sentinel token?",
      "offset": 280.4,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "Kind of. Yeah, exactly. Because what I",
      "offset": 283.919,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "want is I want the answer that over here",
      "offset": 285.68,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "to be an answer, but I want to include",
      "offset": 288.4,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "the citations that that remap to that",
      "offset": 290.479,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "specific thing. Now, the problem is, as",
      "offset": 292.8,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "we all know, URLs can be really really",
      "offset": 295.199,
      "duration": 2.881
    },
    {
      "lang": "en",
      "text": "funky. like just the URL for this",
      "offset": 296.72,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "Excalad draw is I don't know let me see",
      "offset": 298.08,
      "duration": 5.839
    },
    {
      "lang": "en",
      "text": "if I can share one um like if I go to",
      "offset": 300.88,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "like I don't know the random browser",
      "offset": 303.919,
      "duration": 5.481
    },
    {
      "lang": "en",
      "text": "page I probably have something open",
      "offset": 305.44,
      "duration": 3.96
    },
    {
      "lang": "en",
      "text": "where' it go",
      "offset": 310.08,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "sorry",
      "offset": 312.08,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "if I just go to like for example our",
      "offset": 314.96,
      "duration": 2.64
    },
    {
      "lang": "en",
      "text": "YouTube channel let me just show some of",
      "offset": 316.32,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "these videos the these URLs are",
      "offset": 317.6,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "basically you I could have this as the",
      "offset": 320.56,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "citation URL for my model and let's just",
      "offset": 322.16,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "take a look at what it would mean for",
      "offset": 324.56,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "the model to generate this.",
      "offset": 325.68,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "Let's just go look at the tokenizer",
      "offset": 328.56,
      "duration": 2.56
    },
    {
      "lang": "en",
      "text": "because I think this is the most",
      "offset": 330.08,
      "duration": 2.399
    },
    {
      "lang": "en",
      "text": "important thing to think about if a",
      "offset": 331.12,
      "duration": 2.639
    },
    {
      "lang": "en",
      "text": "model can generate something accurately",
      "offset": 332.479,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "or not. This is what the model has to",
      "offset": 333.759,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "generate. There's a bunch of tokens. So",
      "offset": 336.24,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "these tokens make sense. It can probably",
      "offset": 338.56,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "do this. YouTube is a single token. A",
      "offset": 340.479,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "YouTube is a single token. That's kind",
      "offset": 342.56,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "of interesting actually. Um I learned",
      "offset": 343.759,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "that today. Uh watch is a single token.",
      "offset": 345.759,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "We're good. Question mark V is a single",
      "offset": 348,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "token which also probably makes sense",
      "offset": 349.52,
      "duration": 2.959
    },
    {
      "lang": "en",
      "text": "because YouTube probably is a",
      "offset": 351.28,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "predominant force in the tokenizer for",
      "offset": 352.479,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "some reason. But everything else here",
      "offset": 354.24,
      "duration": 5.679
    },
    {
      "lang": "en",
      "text": "breaks down. This ends up and this is",
      "offset": 356.16,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "there's like models can generate a",
      "offset": 359.919,
      "duration": 2.881
    },
    {
      "lang": "en",
      "text": "string. If you type in that string, you",
      "offset": 361.44,
      "duration": 3.039
    },
    {
      "lang": "en",
      "text": "say, &quot;Hey, model make this string for",
      "offset": 362.8,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "me.&quot; It's going to make it. But your",
      "offset": 364.479,
      "duration": 5.041
    },
    {
      "lang": "en",
      "text": "point is basically that like the more",
      "offset": 366.639,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "tokens that you're asking the model to",
      "offset": 369.52,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "generate accurately, the more kind of",
      "offset": 371.44,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "effort it has to put on that and the the",
      "offset": 373.68,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "less likely it's going to get it right.",
      "offset": 375.919,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "Exactly. So in order for the model to",
      "offset": 378.16,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "get this part of the URL correct",
      "offset": 379.6,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "specifically, it has to generate 10",
      "offset": 382,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "tokens perfectly. If we remove this",
      "offset": 383.68,
      "duration": 3.519
    },
    {
      "lang": "en",
      "text": "part, let's assume it'll get question",
      "offset": 385.919,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "mark V correct. It has to get eight",
      "offset": 387.199,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "tokens perfectly correct. If it messes",
      "offset": 389.199,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "up in any of these, it becomes a useless",
      "offset": 391.36,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "link.",
      "offset": 393.36,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "So how can we change that? Well, we can",
      "offset": 394.88,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "do something really, really simple. And",
      "offset": 396.56,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "I will just use YouTube along the way.",
      "offset": 398.8,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "And I'll write a basic prompt that does",
      "offset": 402,
      "duration": 6.88
    },
    {
      "lang": "en",
      "text": "this and tries to go about this.",
      "offset": 403.84,
      "duration": 6.88
    },
    {
      "lang": "en",
      "text": "Oops.",
      "offset": 408.88,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "So, we're going to write a question new",
      "offset": 410.72,
      "duration": 6.64
    },
    {
      "lang": "en",
      "text": "file like uh labels.baml.",
      "offset": 413.12,
      "duration": 5.359
    },
    {
      "lang": "en",
      "text": "I'm going to have a function that's",
      "offset": 417.36,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "going to say given uh like answer",
      "offset": 418.479,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "question. I'm going to say here's a",
      "offset": 421.44,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "question. I'm going to give it a list of",
      "offset": 423.919,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "links",
      "offset": 425.599,
      "duration": 5.081
    },
    {
      "lang": "en",
      "text": "or content.",
      "offset": 427.36,
      "duration": 3.32
    },
    {
      "lang": "en",
      "text": "I'll say like this will have like a URL",
      "offset": 435.039,
      "duration": 5.681
    },
    {
      "lang": "en",
      "text": "which will be a string and then content",
      "offset": 438.08,
      "duration": 6.32
    },
    {
      "lang": "en",
      "text": "which will be a string and then",
      "offset": 440.72,
      "duration": 8.12
    },
    {
      "lang": "en",
      "text": "what we'll return here is some answer",
      "offset": 444.4,
      "duration": 4.44
    },
    {
      "lang": "en",
      "text": "and then citations",
      "offset": 449.199,
      "duration": 7.041
    },
    {
      "lang": "en",
      "text": "string array at definition",
      "offset": 452.72,
      "duration": 6.879
    },
    {
      "lang": "en",
      "text": "list of URLs",
      "offset": 456.24,
      "duration": 8.76
    },
    {
      "lang": "en",
      "text": "that are relevant. Okay.",
      "offset": 459.599,
      "duration": 5.401
    },
    {
      "lang": "en",
      "text": "Open AI GPT40.",
      "offset": 465.68,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "Great.",
      "offset": 468.8,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "And um ctx",
      "offset": 470.479,
      "duration": 6.4
    },
    {
      "lang": "en",
      "text": "output format.",
      "offset": 474,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "Sorry, I'm on a live prompt, so I'm",
      "offset": 476.879,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "going to try and be as fast as possible.",
      "offset": 478.4,
      "duration": 6.56
    },
    {
      "lang": "en",
      "text": "Full user question.",
      "offset": 482.08,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "Okay. So output format is you're telling",
      "offset": 484.96,
      "duration": 7.679
    },
    {
      "lang": "en",
      "text": "it how to output the uh answer.",
      "offset": 486.879,
      "duration": 8.32
    },
    {
      "lang": "en",
      "text": "Exactly. And you're and you're putting",
      "offset": 492.639,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "the output format and the relevant",
      "offset": 495.199,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "content into the system prompt and then",
      "offset": 496.879,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "we're putting the user the question of",
      "offset": 499.68,
      "duration": 3.519
    },
    {
      "lang": "en",
      "text": "the user prompt.",
      "offset": 501.199,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "Exactly. So I'm going to do this. So now",
      "offset": 503.199,
      "duration": 6.72
    },
    {
      "lang": "en",
      "text": "this my prompt um and I will literally",
      "offset": 506.319,
      "duration": 6.001
    },
    {
      "lang": "en",
      "text": "just ask cursor to",
      "offset": 509.919,
      "duration": 6.721
    },
    {
      "lang": "en",
      "text": "generate me a test case for this rag use",
      "offset": 512.32,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "case",
      "offset": 516.64,
      "duration": 4.6
    },
    {
      "lang": "en",
      "text": "use",
      "offset": 518.24,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "resume.",
      "offset": 521.76,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "They're all the same file. They're all",
      "offset": 526.16,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "going to have a test case in them. I'm",
      "offset": 528,
      "duration": 7.279
    },
    {
      "lang": "en",
      "text": "going to move this use.l as uh",
      "offset": 530.08,
      "duration": 9.759
    },
    {
      "lang": "en",
      "text": "as a reference for how works. So, I'll",
      "offset": 535.279,
      "duration": 5.921
    },
    {
      "lang": "en",
      "text": "just have it generate a test case really",
      "offset": 539.839,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "fast and then it'll just go do something",
      "offset": 541.2,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "for me. But we can see how like um and",
      "offset": 543.839,
      "duration": 5.601
    },
    {
      "lang": "en",
      "text": "then this takes a little bit, but we can",
      "offset": 547.04,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "see how like the model might struggle to",
      "offset": 549.44,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "go do something. Great. Accept. Cool.",
      "offset": 551.44,
      "duration": 5.839
    },
    {
      "lang": "en",
      "text": "Let's go do this. Um, and oh man, are",
      "offset": 555.12,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "you going to make these URLs really",
      "offset": 557.279,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "freaking crazy and then uh see if we can",
      "offset": 558.8,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "actually get the model to screw it up.",
      "offset": 561.76,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "We're just going to use this.",
      "offset": 563.36,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "So, this is one YouTube URL",
      "offset": 566.32,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "and I will copy another YouTube URL from",
      "offset": 569.2,
      "duration": 5.639
    },
    {
      "lang": "en",
      "text": "a different video",
      "offset": 571.44,
      "duration": 3.399
    },
    {
      "lang": "en",
      "text": "and I will point this out. It's not even",
      "offset": 576.88,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "a matter of like the model will screw",
      "offset": 578.72,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "this up. It the the point here is it",
      "offset": 579.92,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "doesn't matter if the model does this",
      "offset": 582.24,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "perfectly or not. The point that matters",
      "offset": 583.76,
      "duration": 6.8
    },
    {
      "lang": "en",
      "text": "is the model might screw it up.",
      "offset": 586.56,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "And if it screws it up, I have no",
      "offset": 590.56,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "guarantee on this end. So there's small",
      "offset": 592.64,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "things that I can do. So now that I have",
      "offset": 595.36,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "some citation thing in here, I can do",
      "offset": 597.519,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "something nice in my Python code to help",
      "offset": 599.76,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "reduce some of these errors.",
      "offset": 601.36,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "Oh, you can put like a guard. This is",
      "offset": 605.04,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "from the eval thing. You can put a",
      "offset": 606.56,
      "duration": 2.56
    },
    {
      "lang": "en",
      "text": "runtime guard of like, hey, if it",
      "offset": 607.76,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "outputs a URL that wasn't in our input",
      "offset": 609.12,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "set, bounce it back and tell it to try",
      "offset": 610.959,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "again. actually open just this one",
      "offset": 613.12,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "folder really fast. Uh",
      "offset": 615.2,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "that way it's gonna be a little bit",
      "offset": 618.88,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "cleaner. There we go. Uh otherwise",
      "offset": 620.079,
      "duration": 5.281
    },
    {
      "lang": "en",
      "text": "Python versions don't work for monor",
      "offset": 623.2,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "repos which is the worst sin that Python",
      "offset": 625.36,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "have committed. We're getting there. I",
      "offset": 627.279,
      "duration": 5.361
    },
    {
      "lang": "en",
      "text": "think the uv.python stuff might actually",
      "offset": 630,
      "duration": 8.76
    },
    {
      "lang": "en",
      "text": "eventually fix it. I really hope so.",
      "offset": 632.64,
      "duration": 6.12
    },
    {
      "lang": "en",
      "text": "So one thing I can do is I can literally",
      "offset": 639.839,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "just get the answer equals this and then",
      "offset": 641.519,
      "duration": 8.56
    },
    {
      "lang": "en",
      "text": "I can say like for URL in answer uh",
      "offset": 644.399,
      "duration": 8.801
    },
    {
      "lang": "en",
      "text": "answer citations I somehow assert that",
      "offset": 650.079,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "the URL starts with this. I could like",
      "offset": 653.2,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "build some small search. I could I could",
      "offset": 655.04,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "assert that the URLs are actually in the",
      "offset": 657.12,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "actual content array that comes in",
      "offset": 658.64,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "there.",
      "offset": 660.24,
      "duration": 4.92
    },
    {
      "lang": "en",
      "text": "Um,",
      "offset": 662.16,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "oh,",
      "offset": 665.36,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "I got it. I'll I'll I'll get the link.",
      "offset": 667.76,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "Um, so we can actually go build this",
      "offset": 670.8,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "URL, right, for us. Now, we can actually",
      "offset": 672.56,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "go further. The problem is right over",
      "offset": 674.56,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "here, the URLs, as we saw, have a",
      "offset": 676.64,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "problem with how the model is going to",
      "offset": 678.88,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "generate them.",
      "offset": 680.399,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "So, let's go fix that actually. And",
      "offset": 682.48,
      "duration": 6.84
    },
    {
      "lang": "en",
      "text": "let's say this is our actual URLs.",
      "offset": 685.12,
      "duration": 4.2
    },
    {
      "lang": "en",
      "text": "uh",
      "offset": 691.279,
      "duration": 6.041
    },
    {
      "lang": "en",
      "text": "from BAML client.types",
      "offset": 693.44,
      "duration": 3.88
    },
    {
      "lang": "en",
      "text": "import content. Cool. Now what I can do",
      "offset": 698.399,
      "duration": 5.601
    },
    {
      "lang": "en",
      "text": "here is instead of actually putting this",
      "offset": 701.92,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "URL as is I could literally put a I",
      "offset": 704,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "could first change this completely and",
      "offset": 707.36,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "say what I actually want to do is I",
      "offset": 710.16,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "won't list a return of citations. I will",
      "offset": 712.24,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "actually list an index",
      "offset": 714.16,
      "duration": 7.679
    },
    {
      "lang": "en",
      "text": "index of the content.",
      "offset": 717.279,
      "duration": 6.161
    },
    {
      "lang": "en",
      "text": "And now that this returns an index of",
      "offset": 721.839,
      "duration": 3.041
    },
    {
      "lang": "en",
      "text": "the content, what I will do here is",
      "offset": 723.44,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "literally just print this out. Content",
      "offset": 724.88,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "loop.index0",
      "offset": 729.2,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "content idx. And now my prompt looks",
      "offset": 730.88,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "like this. Instead of actually dumping",
      "offset": 734.56,
      "duration": 4.959
    },
    {
      "lang": "en",
      "text": "the actual URL, I just say content idx00",
      "offset": 736.88,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "0. I can actually put like dashes here",
      "offset": 739.519,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "separators. I can put them beforehand",
      "offset": 741.2,
      "duration": 6.12
    },
    {
      "lang": "en",
      "text": "because that might actually better",
      "offset": 743.44,
      "duration": 3.88
    },
    {
      "lang": "en",
      "text": "content.",
      "offset": 747.839,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "I can do this. Um, and now it's actually",
      "offset": 749.839,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "called content content one content zero.",
      "offset": 752.48,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "And now I just remove the idea of the",
      "offset": 754.8,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "URL completely from the model. And the",
      "offset": 756.399,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "model will not do this. And when I go",
      "offset": 758.8,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "run this,",
      "offset": 760.959,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "what we'll find is great, we get zero",
      "offset": 763.519,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "and one because those are relevant",
      "offset": 765.279,
      "duration": 2.641
    },
    {
      "lang": "en",
      "text": "indexes. And like let's make up a third",
      "offset": 766.399,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "one that doesn't matter.",
      "offset": 767.92,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "Um,",
      "offset": 771.279,
      "duration": 3.521
    },
    {
      "lang": "en",
      "text": "Europe",
      "offset": 773.2,
      "duration": 8.439
    },
    {
      "lang": "en",
      "text": "is pretty cool and has great pasta",
      "offset": 774.8,
      "duration": 6.839
    },
    {
      "lang": "en",
      "text": "and ideally it shouldn't pick up the",
      "offset": 781.76,
      "duration": 2.639
    },
    {
      "lang": "en",
      "text": "right content. It should only pick up",
      "offset": 783.12,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "zero and one. And now what I can do in",
      "offset": 784.399,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "my code instead of doing it in the model",
      "offset": 786,
      "duration": 6.72
    },
    {
      "lang": "en",
      "text": "is I can convert the URL into the actual",
      "offset": 788.079,
      "duration": 7.601
    },
    {
      "lang": "en",
      "text": "citation. So now I can just say like",
      "offset": 792.72,
      "duration": 6.96
    },
    {
      "lang": "en",
      "text": "content of URL dot um what is it",
      "offset": 795.68,
      "duration": 6.32
    },
    {
      "lang": "en",
      "text": "contents of URL",
      "offset": 799.68,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "URL or the actual URL that I actually",
      "offset": 802,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "want. So it becomes an index based",
      "offset": 804.48,
      "duration": 3.039
    },
    {
      "lang": "en",
      "text": "lookup instead of a real one. So the",
      "offset": 805.76,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "idea is you really don't you really want",
      "offset": 807.519,
      "duration": 5.841
    },
    {
      "lang": "en",
      "text": "to do your best to not rely on models",
      "offset": 809.36,
      "duration": 6.88
    },
    {
      "lang": "en",
      "text": "generating long sequences of tokens that",
      "offset": 813.36,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "don't make sense for the model to",
      "offset": 816.24,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "actually intuitively think about sim.",
      "offset": 817.76,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "There's no meaning. There's no meaning",
      "offset": 820.24,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "baked into that random string of",
      "offset": 821.92,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "characters. It's just a pointer.",
      "offset": 823.36,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "Exactly. And if you can go further and",
      "offset": 825.839,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "if you go back to our content about",
      "offset": 829.12,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "dynamic enums, you could, for example,",
      "offset": 830.639,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "make this a dynamic enum that then has",
      "offset": 832.72,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "an alias that gets mapped back to the",
      "offset": 834.639,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "actual value. I was going to say we",
      "offset": 836.72,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "could go into all of the fancy BAML",
      "offset": 838.32,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "features that make this even easier. Um,",
      "offset": 840.16,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "I'm going to say we are 20 minutes in.",
      "offset": 842.16,
      "duration": 3.119
    },
    {
      "lang": "en",
      "text": "So, if you if you want to move on to the",
      "offset": 843.76,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "next tip or do you want to wrap this one",
      "offset": 845.279,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "up or or do you have more on the label",
      "offset": 846.72,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "stuff? Perfect.",
      "offset": 848.639,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "It's don't use sequence of tokens that",
      "offset": 850.88,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "don't make sense for the model. Go",
      "offset": 852.56,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "update it on your own. We got one",
      "offset": 854.24,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "question. Symbol simple tuning also",
      "offset": 856.48,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "applies here. Exactly. Symbol tuning is",
      "offset": 858.88,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "the exact same thing. Docs will cover",
      "offset": 861.76,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "that. Can't talk about that right now",
      "offset": 863.519,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "because of time constraints. We're going",
      "offset": 865.199,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "to do another one. Diorization.",
      "offset": 867.36,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "So, we've all seen diorization examples.",
      "offset": 869.6,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "We're like, um, do this. Make a make a",
      "offset": 871.68,
      "duration": 5.839
    },
    {
      "lang": "en",
      "text": "transcript.",
      "offset": 875.36,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "to diorization",
      "offset": 877.519,
      "duration": 5.721
    },
    {
      "lang": "en",
      "text": "diorization",
      "offset": 880.24,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "function",
      "offset": 883.839,
      "duration": 3.521
    },
    {
      "lang": "en",
      "text": "use",
      "offset": 885.36,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "labels of ammo as an example.",
      "offset": 887.36,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "Do you want to do a quick uh whiteboard",
      "offset": 890.48,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "on like what what do we mean by",
      "offset": 892.399,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "diorization while it's working? Fine. I",
      "offset": 893.839,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "will go do this. I'll describe some",
      "offset": 897.04,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "words over here. So let's talk about",
      "offset": 898.56,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "diorization.",
      "offset": 901.04,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "Diorization. Diorization. Diorization is",
      "offset": 902.72,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "this idea that we have audio coming in",
      "offset": 905.68,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "and we want to turn the audio snippets",
      "offset": 908.48,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "into like a",
      "offset": 910.8,
      "duration": 6.719
    },
    {
      "lang": "en",
      "text": "speaker plus transcript uh section. So",
      "offset": 914,
      "duration": 4.959
    },
    {
      "lang": "en",
      "text": "each of these will always have a speaker",
      "offset": 917.519,
      "duration": 2.721
    },
    {
      "lang": "en",
      "text": "and each of these will and then",
      "offset": 918.959,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "transform into like who said what. So",
      "offset": 920.24,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "the idea is most of these sequences come",
      "offset": 922.32,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "from",
      "offset": 924.399,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "and mo what most of these will do is",
      "offset": 926.56,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "they'll basically say l literally say",
      "offset": 928.8,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "speaker zero speaker one speaker zero",
      "offset": 930.88,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "speaker one",
      "offset": 932.8,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "um and you might actually want to go do",
      "offset": 934.959,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "something more than that because you",
      "offset": 937.12,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "might be having a conversation between a",
      "offset": 938.56,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "nurse and a patient. So you might",
      "offset": 940.16,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "actually want to say speaker one is a",
      "offset": 941.6,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "nurse, speaker two is a patient and",
      "offset": 943.04,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "transform your transcript to that. I'm",
      "offset": 945.199,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "going to show you a prompting trip that",
      "offset": 948.8,
      "duration": 6.479
    },
    {
      "lang": "en",
      "text": "is going to reduce the amount of um text",
      "offset": 950.399,
      "duration": 6.401
    },
    {
      "lang": "en",
      "text": "that we might have to generate by an",
      "offset": 955.279,
      "duration": 3.281
    },
    {
      "lang": "en",
      "text": "order of magnitude to solve this",
      "offset": 956.8,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "problem. Because if I want to go from",
      "offset": 958.56,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "person one to",
      "offset": 960.24,
      "duration": 7.12
    },
    {
      "lang": "en",
      "text": "uh speaker, I like nurse",
      "offset": 963.759,
      "duration": 7.08
    },
    {
      "lang": "en",
      "text": "versus patient",
      "offset": 967.36,
      "duration": 3.479
    },
    {
      "lang": "en",
      "text": "versus like other cuz maybe their",
      "offset": 972.639,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "husband or wife spoke up into it in the",
      "offset": 976.16,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "middle of it. I want to know exactly who",
      "offset": 978.399,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "these personas are. So, let's go do",
      "offset": 980.24,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "that.",
      "offset": 983.12,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "re real quick. Um, is there is does it",
      "offset": 984.24,
      "duration": 5.599
    },
    {
      "lang": "en",
      "text": "is I imagine this is probably equivalent",
      "offset": 987.12,
      "duration": 4.959
    },
    {
      "lang": "en",
      "text": "whether you're doing audio or raw just",
      "offset": 989.839,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "like a raw transcript of a conversation,",
      "offset": 992.079,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "right? Yes. So, I'm going to assume that",
      "offset": 994.399,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "the transcript is going to have a",
      "offset": 996.72,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "speaker. Uh, let's just say the",
      "offset": 998.639,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "transcript is on, let's simplify this a",
      "offset": 1000.959,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "little bit. Let's say the transcript is",
      "offset": 1002.88,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "literally just a string.",
      "offset": 1004.56,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "And what I want to do is I want to",
      "offset": 1007.44,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "identify the speakers that exist for",
      "offset": 1008.959,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "each of these, right? So the transcript",
      "offset": 1010.48,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "is literally just going to be a string",
      "offset": 1013.44,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "and I I have um no other information",
      "offset": 1015.519,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "about it.",
      "offset": 1018.24,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "Uh transcript will turn into that. And",
      "offset": 1020.48,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "then what I want is I want to return a",
      "offset": 1022.399,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "di transcript which is going to be a",
      "offset": 1023.839,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "bunch of speaker segments. Don't need",
      "offset": 1025.76,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "this. And this will just have",
      "offset": 1027.439,
      "duration": 6.561
    },
    {
      "lang": "en",
      "text": "speaker string uh text. And you might",
      "offset": 1031.52,
      "duration": 5.439
    },
    {
      "lang": "en",
      "text": "even say that this is like nurse",
      "offset": 1034,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "doctor patient or other. Uh so let's",
      "offset": 1036.959,
      "duration": 5.441
    },
    {
      "lang": "en",
      "text": "let's like write it like here.",
      "offset": 1040.24,
      "duration": 6.199
    },
    {
      "lang": "en",
      "text": "Cool. Um,",
      "offset": 1042.4,
      "duration": 4.039
    },
    {
      "lang": "en",
      "text": "identify identify the speakers",
      "offset": 1046.559,
      "duration": 4.681
    },
    {
      "lang": "en",
      "text": "output format",
      "offset": 1052.96,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "and then",
      "offset": 1056.48,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "roll user.",
      "offset": 1058.64,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "Okay, cool. That's probably good enough.",
      "offset": 1061.44,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "Oh, that's actually pretty cool.",
      "offset": 1063.52,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "Um,",
      "offset": 1066.16,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "let's change this. But you actually just",
      "offset": 1068.24,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "want the raw text, right? Yeah. So I",
      "offset": 1069.52,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "will Oh yeah, that's true. Thank you for",
      "offset": 1072.16,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "identifying that, Dexter.",
      "offset": 1073.76,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "Actually, I think our test case is",
      "offset": 1076.559,
      "duration": 6.201
    },
    {
      "lang": "en",
      "text": "converted correctly. Uh",
      "offset": 1077.919,
      "duration": 4.841
    },
    {
      "lang": "en",
      "text": "are you I'm hurt.",
      "offset": 1089.2,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "My knee",
      "offset": 1092.48,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "hurts.",
      "offset": 1094.4,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "I'm sorry.",
      "offset": 1096.24,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "Sorry. So, so this is",
      "offset": 1098.48,
      "duration": 5.439
    },
    {
      "lang": "en",
      "text": "already has the speakers identified",
      "offset": 1101.28,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "though, right? Like, but it doesn't tell",
      "offset": 1103.919,
      "duration": 5.681
    },
    {
      "lang": "en",
      "text": "me who's who.",
      "offset": 1106.16,
      "duration": 7.04
    },
    {
      "lang": "en",
      "text": "Okay. Is so would this technique work?",
      "offset": 1109.6,
      "duration": 7.439
    },
    {
      "lang": "en",
      "text": "Like is this applicable also to just a",
      "offset": 1113.2,
      "duration": 7.04
    },
    {
      "lang": "en",
      "text": "like non like if I just have a a stream",
      "offset": 1117.039,
      "duration": 5.281
    },
    {
      "lang": "en",
      "text": "of text and I don't it's not already",
      "offset": 1120.24,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "split up by speaker.",
      "offset": 1122.32,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "I guess this Okay, so this just assumes",
      "offset": 1125.039,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "you have turn detection but not",
      "offset": 1127.039,
      "duration": 5.041
    },
    {
      "lang": "en",
      "text": "necessarily um let's say we don't know",
      "offset": 1129.2,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "who the speaker is. We don't know",
      "offset": 1132.08,
      "duration": 2.4
    },
    {
      "lang": "en",
      "text": "anything about this. What we really want",
      "offset": 1133.2,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "to do is we want to go and convert this",
      "offset": 1134.48,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "in a really quick way. Um so I'm going",
      "offset": 1136.24,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "to go change it's been hurting for three",
      "offset": 1139.12,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "days now. Uh fix um he's been",
      "offset": 1140.48,
      "duration": 7.12
    },
    {
      "lang": "en",
      "text": "complaining about it for a while. So",
      "offset": 1144.32,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "this is interesting because there might",
      "offset": 1147.6,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "be a lot of other content here. So let's",
      "offset": 1148.96,
      "duration": 4.959
    },
    {
      "lang": "en",
      "text": "just see firstly what the what the what",
      "offset": 1151.28,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "the raw thing ends up being.",
      "offset": 1153.919,
      "duration": 7.361
    },
    {
      "lang": "en",
      "text": "Yeah. And cool. This this seems kind of",
      "offset": 1157.2,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "interesting. It's like cool. It has",
      "offset": 1161.28,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "other it has all these other things in",
      "offset": 1162.72,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "here. Let's try and make this better",
      "offset": 1164.16,
      "duration": 5.519
    },
    {
      "lang": "en",
      "text": "really fast. Um and I'm going to combine",
      "offset": 1166.72,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "like two or three different of the",
      "offset": 1169.679,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "prompting tips right in one as I go. So,",
      "offset": 1170.96,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "the first thing I want to notice is,",
      "offset": 1173.84,
      "duration": 8.32
    },
    {
      "lang": "en",
      "text": "hey, this is probably not very useful.",
      "offset": 1176,
      "duration": 8.24
    },
    {
      "lang": "en",
      "text": "So, let's try and just like fix this.",
      "offset": 1182.16,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "Well, part of it is not useful. Well,",
      "offset": 1184.24,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "one, I'm outputting the whole transcript",
      "offset": 1186.08,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "over and over again. That sounds bad. I",
      "offset": 1187.679,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "see. Let's see if we can do this in a",
      "offset": 1190.799,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "slightly better way. Um, so what I'm",
      "offset": 1192.64,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "going to do is I'm going to say",
      "offset": 1195.919,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "dialogue",
      "offset": 1198.48,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "index int. So, I'm going to give it give",
      "offset": 1200.24,
      "duration": 5.679
    },
    {
      "lang": "en",
      "text": "it the dialog index. And here, I'm just",
      "offset": 1203.679,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "going to like write this in my prompt",
      "offset": 1205.919,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "really fast so I don't have to think",
      "offset": 1207.44,
      "duration": 6.32
    },
    {
      "lang": "en",
      "text": "about this. But, uh, the right way to do",
      "offset": 1210,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "this is honestly to just make this thing",
      "offset": 1213.76,
      "duration": 5.72
    },
    {
      "lang": "en",
      "text": "an array.",
      "offset": 1216.24,
      "duration": 3.24
    },
    {
      "lang": "en",
      "text": "H, sorry,",
      "offset": 1220.08,
      "duration": 3.32
    },
    {
      "lang": "en",
      "text": "I love cursor. And we'll make this an",
      "offset": 1228.64,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "array. And now instead of dumping the",
      "offset": 1231.12,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "transcript out as we are, what we'll do",
      "offset": 1233.76,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "is we'll say for line and transcript",
      "offset": 1235.6,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "print out the line. And now what we'll",
      "offset": 1238,
      "duration": 5.679
    },
    {
      "lang": "en",
      "text": "also say is this loop.index0",
      "offset": 1240.32,
      "duration": 6.359
    },
    {
      "lang": "en",
      "text": "dialogue",
      "offset": 1243.679,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "this. We'll add an extra space in there",
      "offset": 1247.44,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "and then we'll add that in. So now this",
      "offset": 1249.6,
      "duration": 8.16
    },
    {
      "lang": "en",
      "text": "is an assumption that the the script is",
      "offset": 1253.2,
      "duration": 7.28
    },
    {
      "lang": "en",
      "text": "already an array or are we just",
      "offset": 1257.76,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "converting the script into an array like",
      "offset": 1260.48,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "well you can just split by you can just",
      "offset": 1262.88,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "split by I'm assuming if you have some",
      "offset": 1264.559,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "way have a speaker colon here you have a",
      "offset": 1266.24,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "way to convert this into an array of",
      "offset": 1268.24,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "some kind. Okay. Right. Yeah. I think I",
      "offset": 1269.679,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "think in yeah I think the questions that",
      "offset": 1272.4,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "a lot of people are asking is kind of",
      "offset": 1273.84,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "the like the real time actual speech to",
      "offset": 1275.44,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "text use case is you don't have those",
      "offset": 1278,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "like separators unless you're using like",
      "offset": 1281.12,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "a separate like turn detection model",
      "offset": 1283.2,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "basically. Yes, but most people should",
      "offset": 1285.12,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "be using a turn detection model. So I'm",
      "offset": 1287.919,
      "duration": 2.561
    },
    {
      "lang": "en",
      "text": "assuming that you have that right now.",
      "offset": 1289.36,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "You're analyzing a transcript in post.",
      "offset": 1290.48,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "We can remove the speaker labels as well",
      "offset": 1292.559,
      "duration": 3.041
    },
    {
      "lang": "en",
      "text": "so it's like a little bit more clear.",
      "offset": 1294.24,
      "duration": 2.4
    },
    {
      "lang": "en",
      "text": "It's like we just have all the",
      "offset": 1295.6,
      "duration": 2.559
    },
    {
      "lang": "en",
      "text": "statements that are literally speech to",
      "offset": 1296.64,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "text per line of some kind. I'm going to",
      "offset": 1298.159,
      "duration": 6.241
    },
    {
      "lang": "en",
      "text": "go run this now. Now, you'll notice the",
      "offset": 1301.12,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "model's actually really really good at",
      "offset": 1304.4,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "just spitting out the dialogue index and",
      "offset": 1306.08,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "who the who the speaker is in each of",
      "offset": 1307.76,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "these scenarios. Oh, so it doesn't have",
      "offset": 1309.52,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "to reoutput the actual text itself.",
      "offset": 1312,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "Exactly. Order of magnit you can imagine",
      "offset": 1314.24,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "for long transcripts, this is an order",
      "offset": 1317.039,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "of magnitude",
      "offset": 1318.96,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "cheaper in terms of how much text it has",
      "offset": 1320.799,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "to output. And we can reduce this even",
      "offset": 1323.76,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "further and just like alias this to like",
      "offset": 1325.52,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "alias idx.",
      "offset": 1328.159,
      "duration": 5.041
    },
    {
      "lang": "en",
      "text": "And then it'll be a lot shorter. And now",
      "offset": 1331.52,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "it's just now it's just outputting the",
      "offset": 1333.2,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "index in the speaker.",
      "offset": 1334.64,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "I'm a little curious what would happen",
      "offset": 1337.12,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "if you just put it all as one big",
      "offset": 1339.36,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "string. What do you mean? Oh, like like",
      "offset": 1340.96,
      "duration": 5.839
    },
    {
      "lang": "en",
      "text": "if you didn't split them out. I imagine",
      "offset": 1344.559,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "it's probably not going to work as well.",
      "offset": 1346.799,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "But it it the reason that this works a",
      "offset": 1348.24,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "lot better is twofold. one I'm actually",
      "offset": 1351.28,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "telling the model what the index is. So",
      "offset": 1353.6,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "the model has to go back and say I'm",
      "offset": 1355.76,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "let's look at what the model does turn",
      "offset": 1358.48,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "by turn. It's going to in first output",
      "offset": 1359.84,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "idx0.",
      "offset": 1361.919,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "Then all it has to do is in its token",
      "offset": 1363.44,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "during the attention mechanism the model",
      "offset": 1365.84,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "goes back into its tokenizer. So it",
      "offset": 1368.24,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "literally will go back through all the",
      "offset": 1370.24,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "tokens and just say okay what tokens do",
      "offset": 1371.36,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "I want to look at? I want to look at",
      "offset": 1373.52,
      "duration": 2.32
    },
    {
      "lang": "en",
      "text": "next zero. It's going to go and just say",
      "offset": 1374.559,
      "duration": 2.721
    },
    {
      "lang": "en",
      "text": "okay I need to understand this part of",
      "offset": 1375.84,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "this this part of the segment. It's",
      "offset": 1377.28,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "easier for it to focus. So even though",
      "offset": 1379.52,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "it's a little redundant, it helps the",
      "offset": 1382.08,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "model be a little bit more focused on",
      "offset": 1384,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "its part. Now it's like, okay, what who",
      "offset": 1386.48,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "likely said this?",
      "offset": 1388.4,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "And then it's like and then it goes out",
      "offset": 1390.799,
      "duration": 2.801
    },
    {
      "lang": "en",
      "text": "and starts spitting out the next token,",
      "offset": 1392.32,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "spits out idx. So at the point of idx,",
      "offset": 1393.6,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "now it says, oh, what's the next idx I",
      "offset": 1396.48,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "need? Oh, let me go back a couple tokens",
      "offset": 1398.4,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "here. It's like that was zero. I",
      "offset": 1400.32,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "probably need one next. We're reducing",
      "offset": 1402.32,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "the burden on the model. That's the main",
      "offset": 1404.72,
      "duration": 6.4
    },
    {
      "lang": "en",
      "text": "that's the main leverage here. the model",
      "offset": 1408.48,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "at any point is able to do way less work",
      "offset": 1411.12,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "and then therefore output more. Does",
      "offset": 1413.44,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "that make sense, Dexter? Yeah, I got",
      "offset": 1415.36,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "you. Cool. Cool.",
      "offset": 1417.84,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "Now, the thing is we may not actually",
      "offset": 1420.72,
      "duration": 4.959
    },
    {
      "lang": "en",
      "text": "know exactly who's talking here. Like",
      "offset": 1423.52,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "this other thing, we might have made a",
      "offset": 1425.679,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "bug and not actually introduced other.",
      "offset": 1427.12,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "And in this scenario, what we'll find is",
      "offset": 1430.32,
      "duration": 5.599
    },
    {
      "lang": "en",
      "text": "likely the model",
      "offset": 1432.72,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "will do something just output to nurse.",
      "offset": 1435.919,
      "duration": 5.441
    },
    {
      "lang": "en",
      "text": "it kind of hallucinated on its own. So",
      "offset": 1438.24,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "we can actually just add other as a",
      "offset": 1441.36,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "fallback. So we the model doesn't tend",
      "offset": 1444.32,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "to hallucinate. We want to prevent",
      "offset": 1446.32,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "hallucinations when possible. And we do",
      "offset": 1447.76,
      "duration": 3.519
    },
    {
      "lang": "en",
      "text": "that by giving the model an out. That's",
      "offset": 1449.2,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "another and this is the same with all",
      "offset": 1451.279,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "the all the classifier examples that",
      "offset": 1453.279,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "that we talk about, right? is like",
      "offset": 1455.279,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "classify the things you know you are",
      "offset": 1457.279,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "good at classifying in the fastest,",
      "offset": 1458.88,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "cheapest, most efficient way and then",
      "offset": 1460.799,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "allow the model to have an escape hatch",
      "offset": 1463.44,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "in which case you'll handle it in a",
      "offset": 1465.6,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "different way either by sending it to a",
      "offset": 1467.679,
      "duration": 3.281
    },
    {
      "lang": "en",
      "text": "human to classify or sending it to a",
      "offset": 1469.279,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "bigger smarter model or whatever it is.",
      "offset": 1470.96,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "Exactly. But now let's do another thing.",
      "offset": 1473.84,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "Let's do another thing. Clues. Let's add",
      "offset": 1476.24,
      "duration": 5.439
    },
    {
      "lang": "en",
      "text": "some clues here. So I'm going to help",
      "offset": 1478.96,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "model things that I'm Exactly. So I'm",
      "offset": 1481.679,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "going to help the model think about what",
      "offset": 1483.76,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "it is. And it's literally just like",
      "offset": 1485.279,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "it's literally just dumping the text",
      "offset": 1488,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "here.",
      "offset": 1489.52,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "Um, and like this is not very useful.",
      "offset": 1491.919,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "Add description things that help",
      "offset": 1493.679,
      "duration": 8.081
    },
    {
      "lang": "en",
      "text": "uh uh inference to",
      "offset": 1497.279,
      "duration": 5.601
    },
    {
      "lang": "en",
      "text": "let's just add a little bit more",
      "offset": 1501.76,
      "duration": 2.24
    },
    {
      "lang": "en",
      "text": "dialogue here and we'll see what it",
      "offset": 1502.88,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "does.",
      "offset": 1504,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "um",
      "offset": 1506.4,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "uh let's say uh",
      "offset": 1508.72,
      "duration": 6.319
    },
    {
      "lang": "en",
      "text": "what uh might",
      "offset": 1511.84,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "uh relevant. So let's so we're noticing",
      "offset": 1515.039,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "that what it's doing is just outputting",
      "offset": 1517.84,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "all the clues but a lot of the times",
      "offset": 1519.44,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "it's kind of obvious who the speaker is.",
      "offset": 1520.88,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "So let's just do this only if not",
      "offset": 1523.039,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "obvious",
      "offset": 1525.6,
      "duration": 9.199
    },
    {
      "lang": "en",
      "text": "uh list out facts that uh help us",
      "offset": 1528.559,
      "duration": 10.72
    },
    {
      "lang": "en",
      "text": "um identify help us analyze yeah John's",
      "offset": 1534.799,
      "duration": 6.88
    },
    {
      "lang": "en",
      "text": "suggesting deductive reasoning steps",
      "offset": 1539.279,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "which I think is gets a little towards",
      "offset": 1541.679,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "some of the stuff we've done in the past",
      "offset": 1544.559,
      "duration": 5.321
    },
    {
      "lang": "en",
      "text": "around like structured reasoning stuff.",
      "offset": 1545.6,
      "duration": 4.28
    },
    {
      "lang": "en",
      "text": "Oh, a speaker. Maybe I had a much better",
      "offset": 1550.48,
      "duration": 6.079
    },
    {
      "lang": "en",
      "text": "test case pulled up earlier.",
      "offset": 1553.919,
      "duration": 6.161
    },
    {
      "lang": "en",
      "text": "So, and now you're noticing over here",
      "offset": 1556.559,
      "duration": 6.641
    },
    {
      "lang": "en",
      "text": "now something a lot more interesting.",
      "offset": 1560.08,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "It says speaker zero other because they",
      "offset": 1563.2,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "don't know yet. Speaker one uses",
      "offset": 1565.2,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "personal pronouns indicating injury.",
      "offset": 1566.96,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "That means that they're probably a",
      "offset": 1568.88,
      "duration": 2.799
    },
    {
      "lang": "en",
      "text": "patient.",
      "offset": 1570.159,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "Speaking about the patient, so probably",
      "offset": 1571.679,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "other",
      "offset": 1573.84,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "along the way.",
      "offset": 1575.44,
      "duration": 5.839
    },
    {
      "lang": "en",
      "text": "So, it's actually a lot more useful to",
      "offset": 1578.64,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "actually go do this. And now we can have",
      "offset": 1581.279,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "a lot more com confidence behind what's",
      "offset": 1582.48,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "happening. But it's also it's it's",
      "offset": 1584.559,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "gotten it's it's gotten worse at picking",
      "offset": 1587.12,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "the ones where it was the doctor. The",
      "offset": 1589.44,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "doctor the doctor and nurse are worse.",
      "offset": 1591.12,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "Yes. But that might be because when you",
      "offset": 1593.84,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "really think about it, doctor and nurse",
      "offset": 1597.6,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "are actually confusing because how does",
      "offset": 1599.84,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "it actually identify correctly between",
      "offset": 1602.799,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "the doctor and the nurse?",
      "offset": 1604.24,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "And we can go about this one more time.",
      "offset": 1606.88,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "And if we actually go look at this, if I",
      "offset": 1609.039,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "were to read this transcript, there is",
      "offset": 1611.44,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "no freaking way I as the human would",
      "offset": 1613.2,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "actually be able to know if it's",
      "offset": 1615.6,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "actually a doctor or pat doctor or not",
      "offset": 1616.64,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "without knowing how many people are in",
      "offset": 1620.4,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "the room.",
      "offset": 1622,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "Very true. I could be talking to my",
      "offset": 1624.08,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "brother. Exactly. Exactly. And that's",
      "offset": 1626.48,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "the point. This could be my uncle",
      "offset": 1629.44,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "talking [Â __Â ]",
      "offset": 1630.72,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "Uh so whenever some when you said doctor",
      "offset": 1632.48,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "and patient got nurse you're right we",
      "offset": 1634.799,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "intuitively felt that way but remember",
      "offset": 1636.799,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "the model has no context around this so",
      "offset": 1639.039,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "let's add some more cont sorry could you",
      "offset": 1641.44,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "go to so before you clear this out could",
      "offset": 1643.36,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "you go to the third index index number",
      "offset": 1644.72,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "two",
      "offset": 1646.48,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "yeah this this time it seems to have",
      "offset": 1648.08,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "gotten it because it's making",
      "offset": 1650.24,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "assumptions yeah yeah about it right",
      "offset": 1652.4,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "it's make but now it's taking more from",
      "offset": 1655.52,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "the prompt itself like the actual output",
      "offset": 1657.84,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "format Right. Exactly. It's literally",
      "offset": 1660.48,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "just like ah you're probably either a",
      "offset": 1662.559,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "doctor or a patient. Like there's no",
      "offset": 1664,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "there's no way around this. But now that",
      "offset": 1665.679,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "we forced the model to be like who uh if",
      "offset": 1667.12,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "not only if not obvious go list out",
      "offset": 1670.64,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "facts.",
      "offset": 1672.64,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "And in fact the obvious answer for",
      "offset": 1674.32,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "identifying speakers maybe other in all",
      "offset": 1676.559,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "scenarios.",
      "offset": 1679.039,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "And that's what I would do if I had I",
      "offset": 1681.2,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "would unlabel everything. But then I",
      "offset": 1683.039,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "would say, &quot;Oh,",
      "offset": 1685.36,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "but now we know for sure that this one",
      "offset": 1687.52,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "is a patient because it has been",
      "offset": 1689.36,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "nonobviously stated.&quot;",
      "offset": 1691.52,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "But we can go further. We can make this",
      "offset": 1694.08,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "a little bit better.",
      "offset": 1695.679,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "There",
      "offset": 1698.96,
      "duration": 7.68
    },
    {
      "lang": "en",
      "text": "there were four people in the room.",
      "offset": 1700.48,
      "duration": 10.52
    },
    {
      "lang": "en",
      "text": "Doctor Josh,",
      "offset": 1706.64,
      "duration": 4.36
    },
    {
      "lang": "en",
      "text": "uh, nurse",
      "offset": 1711.76,
      "duration": 6.519
    },
    {
      "lang": "en",
      "text": "off",
      "offset": 1715.279,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "text",
      "offset": 1718.48,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "uh uh friend",
      "offset": 1722,
      "duration": 6.559
    },
    {
      "lang": "en",
      "text": "unidentified.",
      "offset": 1725.84,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "So we can go do this because maybe for",
      "offset": 1728.559,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "my EMR I know exactly who visited",
      "offset": 1730.48,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "but I don't know I don't have any",
      "offset": 1733.44,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "information on the other person at all.",
      "offset": 1734.799,
      "duration": 7.721
    },
    {
      "lang": "en",
      "text": "So now let's add this in here",
      "offset": 1738,
      "duration": 4.52
    },
    {
      "lang": "en",
      "text": "and say for context.",
      "offset": 1743.039,
      "duration": 4.041
    },
    {
      "lang": "en",
      "text": "And now let's let's run this.",
      "offset": 1752.48,
      "duration": 4.199
    },
    {
      "lang": "en",
      "text": "And now what we find is that the model",
      "offset": 1757.12,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "gets a lot better.",
      "offset": 1759.2,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "Right? So you could you could look at",
      "offset": 1761.84,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "like if you want to do this for a random",
      "offset": 1764.08,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "event, you could go get the people off",
      "offset": 1765.679,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "the Google calendar event and just",
      "offset": 1767.919,
      "duration": 2.961
    },
    {
      "lang": "en",
      "text": "inject that at the top be like here's",
      "offset": 1769.44,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "the people and here's their domains and",
      "offset": 1770.88,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "here's you know two sentences of deep",
      "offset": 1772.48,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "research about who this person is.",
      "offset": 1774.96,
      "duration": 5.839
    },
    {
      "lang": "en",
      "text": "Exactly. And this this mechanism of how",
      "offset": 1777.279,
      "duration": 6.4
    },
    {
      "lang": "en",
      "text": "we felt like it got more inaccurate and",
      "offset": 1780.799,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "might have diverted us from actually",
      "offset": 1783.679,
      "duration": 3.281
    },
    {
      "lang": "en",
      "text": "exploring this prompt further is",
      "offset": 1785.2,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "actually important to understand why the",
      "offset": 1786.96,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "model did this. step back, rethink, and",
      "offset": 1788.399,
      "duration": 5.041
    },
    {
      "lang": "en",
      "text": "remember that the model did this because",
      "offset": 1791.12,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "if I were to be completely objective,",
      "offset": 1793.44,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "show this to a random person and tell",
      "offset": 1795.2,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "them to identify speakers, they also",
      "offset": 1796.72,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "would likely pick other. If they had to",
      "offset": 1798.559,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "be like if the choice would be wrong or",
      "offset": 1801.279,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "be correct, I too would prefer to be not",
      "offset": 1803.44,
      "duration": 5.839
    },
    {
      "lang": "en",
      "text": "wrong and just pick other because other",
      "offset": 1807.279,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "is never wrong.",
      "offset": 1809.279,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "Cool. Um, are we going to do triple back",
      "offset": 1811.679,
      "duration": 5.441
    },
    {
      "lang": "en",
      "text": "takes today? I'll do that in a second.",
      "offset": 1815.039,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "That's tip number two where we use",
      "offset": 1817.12,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "diorization.",
      "offset": 1819.279,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "And I want to show one last ver variant",
      "offset": 1820.88,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "of this trick which is these clues.",
      "offset": 1823.76,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "So instead of outputting clues, we can",
      "offset": 1827.36,
      "duration": 6.4
    },
    {
      "lang": "en",
      "text": "just do this description",
      "offset": 1829.52,
      "duration": 8.159
    },
    {
      "lang": "en",
      "text": "uh as a precursor",
      "offset": 1833.76,
      "duration": 6.56
    },
    {
      "lang": "en",
      "text": "to the comment.",
      "offset": 1837.679,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "As a precursor",
      "offset": 1840.32,
      "duration": 6.64
    },
    {
      "lang": "en",
      "text": "comment to this field.",
      "offset": 1843.279,
      "duration": 6.88
    },
    {
      "lang": "en",
      "text": "So sometimes we want",
      "offset": 1846.96,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "but we don't want it to do reasoning as",
      "offset": 1850.159,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "a data field. I don't want to deal with",
      "offset": 1852.48,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "that. I just want to like output",
      "offset": 1854.08,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "something and I want to show you what",
      "offset": 1855.44,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "happens here.",
      "offset": 1857.919,
      "duration": 5.88
    },
    {
      "lang": "en",
      "text": "Um if this works",
      "offset": 1859.84,
      "duration": 3.959
    },
    {
      "lang": "en",
      "text": "uh example. Okay. So this is getting",
      "offset": 1865.44,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "into like how do we how do we this is a",
      "offset": 1867.919,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "great leeway. This is like how do we get",
      "offset": 1869.76,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "the model to output busted JSON in a way",
      "offset": 1871.279,
      "duration": 6.64
    },
    {
      "lang": "en",
      "text": "that like actually helps it get better",
      "offset": 1875.84,
      "duration": 5.079
    },
    {
      "lang": "en",
      "text": "answers.",
      "offset": 1877.919,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "Like comments in JSON are technically",
      "offset": 1883.76,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "not valid.",
      "offset": 1885.52,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "Yeah. Let's see if I can force it to do",
      "offset": 1888.24,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "this. I have to actually read the prompt",
      "offset": 1889.6,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "and see what it's doing.",
      "offset": 1890.96,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "Uh",
      "offset": 1893.919,
      "duration": 5.401
    },
    {
      "lang": "en",
      "text": "use",
      "offset": 1896.32,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "As",
      "offset": 1900.64,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "if",
      "offset": 1902.72,
      "duration": 7.6
    },
    {
      "lang": "en",
      "text": "if not if speaker is ambiguous.",
      "offset": 1904.48,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "List relevant comments",
      "offset": 1913.36,
      "duration": 4.36
    },
    {
      "lang": "en",
      "text": "to help narrow",
      "offset": 1918.64,
      "duration": 6.2
    },
    {
      "lang": "en",
      "text": "help",
      "offset": 1921.84,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "narrow",
      "offset": 1924.96,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "down",
      "offset": 1928.08,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "uh",
      "offset": 1930.08,
      "duration": 8.079
    },
    {
      "lang": "en",
      "text": "to help narrow down um the",
      "offset": 1932.88,
      "duration": 5.279
    },
    {
      "lang": "en",
      "text": "use first.",
      "offset": 1946.24,
      "duration": 3.48
    },
    {
      "lang": "en",
      "text": "Well,",
      "offset": 1951.519,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "I'm going to go run this and see what",
      "offset": 1955.039,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "the model does.",
      "offset": 1956.32,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "Okay, I can't get it to do it. Let me",
      "offset": 1958.399,
      "duration": 5.4
    },
    {
      "lang": "en",
      "text": "try and put this out.",
      "offset": 1959.84,
      "duration": 3.959
    },
    {
      "lang": "en",
      "text": "This is like the weirdest trick that",
      "offset": 1965.039,
      "duration": 5.081
    },
    {
      "lang": "en",
      "text": "I've learned and",
      "offset": 1966.559,
      "duration": 3.561
    },
    {
      "lang": "en",
      "text": "uh so not directly in the generated",
      "offset": 1976.64,
      "duration": 5.919
    },
    {
      "lang": "en",
      "text": "output format but just in the prompt",
      "offset": 1978.32,
      "duration": 6.079
    },
    {
      "lang": "en",
      "text": "and",
      "offset": 1982.559,
      "duration": 6.281
    },
    {
      "lang": "en",
      "text": "use for and had",
      "offset": 1984.399,
      "duration": 4.441
    },
    {
      "lang": "en",
      "text": "and then",
      "offset": 1989.279,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "accident.",
      "offset": 1991.76,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "Okay. So, you always tell me not to use",
      "offset": 1994.24,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "a few shot prompting.",
      "offset": 1996.48,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "I do",
      "offset": 1998.799,
      "duration": 5.681
    },
    {
      "lang": "en",
      "text": "because this is more about the structure",
      "offset": 2001.44,
      "duration": 5.359
    },
    {
      "lang": "en",
      "text": "of the response, not about the actual",
      "offset": 2004.48,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "like learning from examples basically.",
      "offset": 2006.799,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "Exactly. So, let's see if I can get the",
      "offset": 2009.36,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "model to output this. And sometimes I",
      "offset": 2011.76,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "can't. Sometimes the model doesn't",
      "offset": 2013.519,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "really listen. Um, and just dump that",
      "offset": 2014.72,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "info as another field. So, let's do",
      "offset": 2017.2,
      "duration": 6.4
    },
    {
      "lang": "en",
      "text": "another last thing. prefix equals answer",
      "offset": 2019.039,
      "duration": 8.801
    },
    {
      "lang": "en",
      "text": "with this. I noticed OpenAI has been",
      "offset": 2023.6,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "doing this.",
      "offset": 2027.84,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "Oh, where like I think for whatever",
      "offset": 2029.44,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "reason whenever you use the word JSON,",
      "offset": 2032.399,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "they trigger something special in the",
      "offset": 2034,
      "duration": 2.799
    },
    {
      "lang": "en",
      "text": "prompt that goes to like some other",
      "offset": 2035.6,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "model or something",
      "offset": 2036.799,
      "duration": 5.041
    },
    {
      "lang": "en",
      "text": "or like secretly turns on. Oh, there you",
      "offset": 2039.12,
      "duration": 6.84
    },
    {
      "lang": "en",
      "text": "go. Yes, exactly.",
      "offset": 2041.84,
      "duration": 4.12
    },
    {
      "lang": "en",
      "text": "And now the model's actually um uh",
      "offset": 2046.24,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "writing some more comments but it's",
      "offset": 2049.919,
      "duration": 5.521
    },
    {
      "lang": "en",
      "text": "writing the comments be after um",
      "offset": 2050.96,
      "duration": 8.399
    },
    {
      "lang": "en",
      "text": "if list relevant facts on speaker before",
      "offset": 2055.44,
      "duration": 6.399
    },
    {
      "lang": "en",
      "text": "the speaker field",
      "offset": 2059.359,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "the reasoning before the output. Yeah.",
      "offset": 2061.839,
      "duration": 7.361
    },
    {
      "lang": "en",
      "text": "Uh question. So the reason to do this is",
      "offset": 2065.839,
      "duration": 7.121
    },
    {
      "lang": "en",
      "text": "to save the tokens on writing clue every",
      "offset": 2069.2,
      "duration": 7.04
    },
    {
      "lang": "en",
      "text": "single time or it is it's not always",
      "offset": 2072.96,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "about that. It's just like the model",
      "offset": 2076.24,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "might just it's just another tool in",
      "offset": 2078.32,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "your toolbox for how you can get the",
      "offset": 2080.56,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "model to output what you want. Clues is",
      "offset": 2082.24,
      "duration": 5.119
    },
    {
      "lang": "en",
      "text": "one way to do it.",
      "offset": 2085.04,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "But and you can also do the thing we do.",
      "offset": 2087.359,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "It's like put the reasoning at the top",
      "offset": 2088.8,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "and then dump the JSON. And it sounds",
      "offset": 2090.8,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "like this is just like, okay, if we want",
      "offset": 2092.96,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "really targeted reasoning on each field.",
      "offset": 2094.399,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "Exactly. And maybe like this is way more",
      "offset": 2097.04,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "token efficient than having it output a",
      "offset": 2099.599,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "bunch of extra JSON.",
      "offset": 2101.68,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "Exactly. And you'll notice you saw me",
      "offset": 2104.079,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "iterate a little bit on this prompt over",
      "offset": 2106.64,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "here. Like I did a couple of things to",
      "offset": 2108.24,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "go do this, but this goes into the very",
      "offset": 2110.079,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "next tip that I want to really talk",
      "offset": 2113.2,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "about, which is one,",
      "offset": 2114.72,
      "duration": 6.879
    },
    {
      "lang": "en",
      "text": "it's called RTFP. Um, for those of you",
      "offset": 2118.64,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "that don't know RTFM, it means read the",
      "offset": 2121.599,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "[Â __Â ] manual. RTFP means read the",
      "offset": 2123.68,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "[Â __Â ] prompt. Um, and I say that with",
      "offset": 2126.16,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "a lot of love because most people don't",
      "offset": 2128.56,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "actually read the prompt. And you saw",
      "offset": 2130.48,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "what I did when this didn't work over",
      "offset": 2131.92,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "here. I just read the prompt. I was",
      "offset": 2133.839,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "like, &quot;Oh, the if I go back to the add",
      "offset": 2135.839,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "description mechanism, let me give you a",
      "offset": 2138.32,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "little bit more of a description of why",
      "offset": 2140.079,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "I didn't like this.&quot;",
      "offset": 2142.8,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "When I go read this, I'm like, &quot;Oh, this",
      "offset": 2145.359,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "thing over here. Maybe it's getting",
      "offset": 2147.76,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "confused by the double comments.",
      "offset": 2149.28,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "And you can see how that might be",
      "offset": 2152.8,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "confusing to the model. So since I'm",
      "offset": 2154.32,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "using comments like nested comments in",
      "offset": 2156.4,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "comments, I'm like, okay, let me just",
      "offset": 2158.72,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "try and simplify this problem for the",
      "offset": 2160.56,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "model and give it that in a place where",
      "offset": 2162.32,
      "duration": 6.64
    },
    {
      "lang": "en",
      "text": "it can't be confused. And that was the",
      "offset": 2166.24,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "intuition that I had out here.",
      "offset": 2168.96,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "Um, so it really just boils down to",
      "offset": 2172.72,
      "duration": 2.879
    },
    {
      "lang": "en",
      "text": "reading the prompt because if we can",
      "offset": 2174.4,
      "duration": 2.32
    },
    {
      "lang": "en",
      "text": "read the prompt, then we can see what",
      "offset": 2175.599,
      "duration": 2.48
    },
    {
      "lang": "en",
      "text": "the model might be doing. And of course,",
      "offset": 2176.72,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "we can never actually know what's",
      "offset": 2178.079,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "actually happening,",
      "offset": 2179.68,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "but it allows us to actually know what",
      "offset": 2181.92,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "it allows us to iterate a little bit",
      "offset": 2184.4,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "faster and then we can say, &quot;Oh, that",
      "offset": 2185.92,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "isn't working. Let me go fix that.&quot;",
      "offset": 2187.28,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "There's a question about why not use fot",
      "offset": 2189.2,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "prompting. There's a couple reasons.",
      "offset": 2191.119,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "Typically, the way to have done fot",
      "offset": 2193.119,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "prompting in this example would have",
      "offset": 2194.72,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "been me to actually go and write an",
      "offset": 2196.72,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "example and then write out the answer.",
      "offset": 2198.96,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "But that's not what I wanted. I just",
      "offset": 2201.04,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "wanted the model to understand that it",
      "offset": 2202.88,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "has the ability to go do this. it has",
      "offset": 2204.48,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "the ability to list out facts before it",
      "offset": 2206.96,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "actually spits out the speaker field. So",
      "offset": 2209.76,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "I just wanted to give it the structure",
      "offset": 2212.48,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "so it understands the thing it has to",
      "offset": 2213.839,
      "duration": 5.361
    },
    {
      "lang": "en",
      "text": "mimic. I don't it's not the",
      "offset": 2215.839,
      "duration": 5.921
    },
    {
      "lang": "en",
      "text": "Go ahead, Dexter. Yeah. And and all this",
      "offset": 2219.2,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "is again is like okay cool like yeah",
      "offset": 2221.76,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "probably just outputting JSON is good",
      "offset": 2224.16,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "enough. Outputting reasoning first is a",
      "offset": 2225.92,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "little bit better. Having reasoning in",
      "offset": 2227.839,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "your JSON fields is probably a little",
      "offset": 2229.44,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "bit better. But if you're running this",
      "offset": 2231.119,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "kind of thing a h 100,000 times a day,",
      "offset": 2232.96,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "then a tiny half a percent improvement",
      "offset": 2236.079,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "either in efficiency or in speed or in",
      "offset": 2238.4,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "token efficiency or in accuracy is",
      "offset": 2240.88,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "massively valuable. And this is what we",
      "offset": 2244.32,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "talk about every week on this show like",
      "offset": 2246.8,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "how do you how do you unlock those like",
      "offset": 2248.16,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "near the top of the accuracy range? How",
      "offset": 2250.24,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "do you push things even further? Yeah.",
      "offset": 2252.4,
      "duration": 4.959
    },
    {
      "lang": "en",
      "text": "How do you get another half a percent?",
      "offset": 2255.04,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "And this isn't again remember this isn't",
      "offset": 2257.359,
      "duration": 5.121
    },
    {
      "lang": "en",
      "text": "say that this technique will work always",
      "offset": 2259.44,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "but it is another technique that you",
      "offset": 2262.48,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "have available to yourself just like we",
      "offset": 2264.24,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "use this other technique to not spit out",
      "offset": 2266.24,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "the entire dialogue but rather only spit",
      "offset": 2267.68,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "out the index",
      "offset": 2270.079,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "and we use this other technique to say",
      "offset": 2272.72,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "oh dialogue index is actually a lot more",
      "offset": 2274.64,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "tokens let's use purely the word index",
      "offset": 2276.48,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "instead so it spits out the output",
      "offset": 2279.68,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "tokens are way less",
      "offset": 2281.68,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "it's small things that can make a",
      "offset": 2284.48,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "difference and And if I actually were to",
      "offset": 2286.079,
      "duration": 5.361
    },
    {
      "lang": "en",
      "text": "look at this, my hunch actually says",
      "offset": 2287.28,
      "duration": 6.72
    },
    {
      "lang": "en",
      "text": "index itself. Where'd it go? Index is",
      "offset": 2291.44,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "probably wrong. I should actually",
      "offset": 2294,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "probably use like index",
      "offset": 2295.839,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "because this is just a more popular",
      "offset": 2298.16,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "token that the model will have",
      "offset": 2299.839,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "understandings of or rather than idx.",
      "offset": 2301.359,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "Even though idx is a single token, it's",
      "offset": 2303.359,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "just more commonly understood",
      "offset": 2305.44,
      "duration": 6.96
    },
    {
      "lang": "en",
      "text": "existing processes. Um, cool.",
      "offset": 2308.079,
      "duration": 6.241
    },
    {
      "lang": "en",
      "text": "Question. Quick question. So we do this",
      "offset": 2312.4,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "actually hundreds and thousands of times",
      "offset": 2314.32,
      "duration": 6.96
    },
    {
      "lang": "en",
      "text": "a day where we put out um um reasoning",
      "offset": 2316.48,
      "duration": 8.879
    },
    {
      "lang": "en",
      "text": "and we use the reasoning as um eval um",
      "offset": 2321.28,
      "duration": 7.36
    },
    {
      "lang": "en",
      "text": "for another model. So is there a way to",
      "offset": 2325.359,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "achieve or make it a bit more efficient.",
      "offset": 2328.64,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "Um so we literally spit out clues and",
      "offset": 2331.599,
      "duration": 6.321
    },
    {
      "lang": "en",
      "text": "these are at least a a long sentence.",
      "offset": 2334.24,
      "duration": 8.96
    },
    {
      "lang": "en",
      "text": "Um, so any any tips or tricks to make it",
      "offset": 2337.92,
      "duration": 6.8
    },
    {
      "lang": "en",
      "text": "more One way is like if you really",
      "offset": 2343.2,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "wanted if you really wanted like um uh",
      "offset": 2344.72,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "if you really wanted that I would",
      "offset": 2347.2,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "actually put your reasoning afterwards",
      "offset": 2348.4,
      "duration": 5.8
    },
    {
      "lang": "en",
      "text": "like assessment.",
      "offset": 2350.88,
      "duration": 3.32
    },
    {
      "lang": "en",
      "text": "So if you want to do an eval thing right",
      "offset": 2354.64,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "over here description",
      "offset": 2356.24,
      "duration": 7.44
    },
    {
      "lang": "en",
      "text": "final assessment",
      "offset": 2360.56,
      "duration": 6.32
    },
    {
      "lang": "en",
      "text": "of the speaker",
      "offset": 2363.68,
      "duration": 8.72
    },
    {
      "lang": "en",
      "text": "uh given any clues prior uh clues in",
      "offset": 2366.88,
      "duration": 7.12
    },
    {
      "lang": "en",
      "text": "comments",
      "offset": 2372.4,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "I just do this",
      "offset": 2374,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "um and just like let the model spit it",
      "offset": 2377.52,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "out. Now you can use assessment as a",
      "offset": 2380.24,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "thing. But now you'll see that",
      "offset": 2381.68,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "assessment is actually kind of big. So",
      "offset": 2382.96,
      "duration": 6.52
    },
    {
      "lang": "en",
      "text": "what I'll do is like use phrases",
      "offset": 2385.119,
      "duration": 4.361
    },
    {
      "lang": "en",
      "text": "uh not complete sentences. And then I",
      "offset": 2391.839,
      "duration": 8.601
    },
    {
      "lang": "en",
      "text": "would also add into here.",
      "offset": 2396.4,
      "duration": 4.04
    },
    {
      "lang": "en",
      "text": "So now I'll notice over here what it's",
      "offset": 2403.839,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "doing and it will just spit something",
      "offset": 2406,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "out. And I would probably have to tweak",
      "offset": 2407.44,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "this model. So sometimes GT4 is not very",
      "offset": 2408.88,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "good. So let me try anthropic.",
      "offset": 2410.56,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "Is that the right model? We'll find out.",
      "offset": 2413.599,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "Oh, that is not the right model. Uh",
      "offset": 2416,
      "duration": 6.28
    },
    {
      "lang": "en",
      "text": "dude, I think it's 1020.",
      "offset": 2418.48,
      "duration": 3.8
    },
    {
      "lang": "en",
      "text": "Uh 2024 1020.",
      "offset": 2422.88,
      "duration": 6.959
    },
    {
      "lang": "en",
      "text": "Custom Sonic. There you go.",
      "offset": 2425.839,
      "duration": 5.921
    },
    {
      "lang": "en",
      "text": "Oh, I don't have an API key. One second.",
      "offset": 2429.839,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "Um I will not be sharing my API key this",
      "offset": 2431.76,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "time around.",
      "offset": 2433.599,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "Oh, that's why I come here every week.",
      "offset": 2435.359,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "It's because you always you always leak",
      "offset": 2438.48,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "at least one key. I also forget to",
      "offset": 2440,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "deactivate it.",
      "offset": 2442.16,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "Um",
      "offset": 2445.52,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "okay,",
      "offset": 2447.359,
      "duration": 5.961
    },
    {
      "lang": "en",
      "text": "let me um",
      "offset": 2449.359,
      "duration": 3.961
    },
    {
      "lang": "en",
      "text": "yeah, and just uh answering while he's",
      "offset": 2453.52,
      "duration": 2.559
    },
    {
      "lang": "en",
      "text": "doing that, answering the question from",
      "offset": 2455.04,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "the thread. Um",
      "offset": 2456.079,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "uh why not use fuchsia prompting? Um, we",
      "offset": 2458.8,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "talked about this a little bit, but um,",
      "offset": 2461.28,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "it's basically uh, the content of the",
      "offset": 2463.2,
      "duration": 7.2
    },
    {
      "lang": "en",
      "text": "examples uh, tends to greatly steer the",
      "offset": 2466.48,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "model's response.",
      "offset": 2470.4,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "And",
      "offset": 2472.72,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "so like you can get you can get the",
      "offset": 2474.4,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "right structural results without",
      "offset": 2476.8,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "actually putting content in your",
      "offset": 2479.2,
      "duration": 3.119
    },
    {
      "lang": "en",
      "text": "examples.",
      "offset": 2480.64,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "Yes. Um, so there we go. Uh so now you",
      "offset": 2482.319,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "can see over here when I switch this",
      "offset": 2485.839,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "cloud I actually get really nice things",
      "offset": 2487.359,
      "duration": 3.281
    },
    {
      "lang": "en",
      "text": "where it's assessment comes with this",
      "offset": 2489.2,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "and now you could plug this into your",
      "offset": 2490.64,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "evals. We got a way less tokens out",
      "offset": 2492,
      "duration": 6.88
    },
    {
      "lang": "en",
      "text": "here. It's way it's way shorter",
      "offset": 2494.079,
      "duration": 6.081
    },
    {
      "lang": "en",
      "text": "u because we're not using complete",
      "offset": 2498.88,
      "duration": 2.56
    },
    {
      "lang": "en",
      "text": "sentences. So if you really care about",
      "offset": 2500.16,
      "duration": 2.959
    },
    {
      "lang": "en",
      "text": "evals and want to like you want to store",
      "offset": 2501.44,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "the data anyway go do that. But honestly",
      "offset": 2503.119,
      "duration": 3.521
    },
    {
      "lang": "en",
      "text": "if it were up to me I wouldn't do any of",
      "offset": 2505.04,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "this eval stuff online. I would have a",
      "offset": 2506.64,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "separate process that pulls all my data",
      "offset": 2509.52,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "down and runs a separate eval including",
      "offset": 2511.44,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "the assessment for each of these",
      "offset": 2513.119,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "segments off the raw data itself and",
      "offset": 2514.4,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "just run a completely separate process.",
      "offset": 2517.599,
      "duration": 3.121
    },
    {
      "lang": "en",
      "text": "It's going to be way cheaper, way faster",
      "offset": 2519.28,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "because don't add more latency to a",
      "offset": 2520.72,
      "duration": 2.879
    },
    {
      "lang": "en",
      "text": "pipeline that has this. Each of these",
      "offset": 2522.16,
      "duration": 2.64
    },
    {
      "lang": "en",
      "text": "things that you're generating here is",
      "offset": 2523.599,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "latency. So a very latency sensitive",
      "offset": 2524.8,
      "duration": 5.519
    },
    {
      "lang": "en",
      "text": "pipeline generally for speech to text.",
      "offset": 2526.8,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "Cool.",
      "offset": 2530.319,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "Um cool. Let's talk about uh so at this",
      "offset": 2531.839,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "point we've covered labels. Don't use",
      "offset": 2535.52,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "UIDs. Don't use URLs. Use like indexes",
      "offset": 2537.04,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "whenever possible and remap them",
      "offset": 2539.92,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "programmatically to the right thing.",
      "offset": 2541.359,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "We've talked about diorization. Don't",
      "offset": 2543.44,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "emit the full transcript. Have the again",
      "offset": 2545.119,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "have the index have the model represent",
      "offset": 2547.28,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "something that is way better than the",
      "offset": 2549.04,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "full transcript. In this case, an index",
      "offset": 2550.72,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "of transcript. We've talked about using",
      "offset": 2552.48,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "inline comments to guide reasoning of",
      "offset": 2554.96,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "sorts. We've talked about read RTF.",
      "offset": 2557.44,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "Reading the prompt. Read it always,",
      "offset": 2560.64,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "especially when you get stuck. Instead",
      "offset": 2562.64,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "of trying to keep prompting more, just",
      "offset": 2564.16,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "keep reading it. We've talked about",
      "offset": 2565.68,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "fuchsia prompting with structure, not",
      "offset": 2567.68,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "with actual content, and how we can",
      "offset": 2570.16,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "leverage that along the way. And I think",
      "offset": 2571.68,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "the next thing I want to talk about is",
      "offset": 2574.4,
      "duration": 2.48
    },
    {
      "lang": "en",
      "text": "something that we've mentioned a few",
      "offset": 2575.599,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "times. Um, but it's all about codegen.",
      "offset": 2576.88,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "So, I'm going to go ahead and pull up a",
      "offset": 2580.24,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "random uh",
      "offset": 2582.24,
      "duration": 7.359
    },
    {
      "lang": "en",
      "text": "file. Hey, Webhub Anup here. Before you",
      "offset": 2585.68,
      "duration": 6.639
    },
    {
      "lang": "en",
      "text": "move forward, I in my mind I'm still",
      "offset": 2589.599,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "confused about using this technique",
      "offset": 2592.319,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "where you somehow use ginger to get an",
      "offset": 2594.319,
      "duration": 6.081
    },
    {
      "lang": "en",
      "text": "index on that array.",
      "offset": 2597.599,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "I",
      "offset": 2600.4,
      "duration": 5.679
    },
    {
      "lang": "en",
      "text": "go ahead versus using symbol tuning",
      "offset": 2602.079,
      "duration": 6.581
    },
    {
      "lang": "en",
      "text": "thing. So when to use",
      "offset": 2606.079,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "[Music]",
      "offset": 2608.66,
      "duration": 3.74
    },
    {
      "lang": "en",
      "text": "Okay, so just for context, let me just",
      "offset": 2610.72,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "pull up a symbol example. So then I we",
      "offset": 2612.4,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "can just talk about it.",
      "offset": 2614.72,
      "duration": 5.879
    },
    {
      "lang": "en",
      "text": "Um",
      "offset": 2617.599,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "uh I have symbol tuning right here. So",
      "offset": 2624.4,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "the idea of symbol tuning is I want to",
      "offset": 2626.079,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "do a classification example. I guess",
      "offset": 2628.16,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "I'll do this",
      "offset": 2630.64,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "symbol",
      "offset": 2632.72,
      "duration": 5.32
    },
    {
      "lang": "en",
      "text": "tuning.",
      "offset": 2635.04,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "Uh, oops.",
      "offset": 2639.76,
      "duration": 4.28
    },
    {
      "lang": "en",
      "text": "Okay. Uh, I have a classification",
      "offset": 2647.359,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "prompt. Instead of actually classifying",
      "offset": 2649.44,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "the prompt, I want the model to spit out",
      "offset": 2651.04,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "one of these categories. And I have a",
      "offset": 2652.72,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "couple different ways I can go do this.",
      "offset": 2654.72,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "Oh, that's interesting. Um, I have a",
      "offset": 2656.16,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "couple different ways that I can go do",
      "offset": 2659.359,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "this. But one of the ways is like",
      "offset": 2660.64,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "instead of the model actually spitting",
      "offset": 2663.599,
      "duration": 3.121
    },
    {
      "lang": "en",
      "text": "out",
      "offset": 2665.119,
      "duration": 5.921
    },
    {
      "lang": "en",
      "text": "um all of my um classes I can and",
      "offset": 2666.72,
      "duration": 5.599
    },
    {
      "lang": "en",
      "text": "instead of actually writing like the",
      "offset": 2671.04,
      "duration": 3.039
    },
    {
      "lang": "en",
      "text": "word refund in the prompt I can write",
      "offset": 2672.319,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "just the symbol K1 and when the model",
      "offset": 2674.079,
      "duration": 6.561
    },
    {
      "lang": "en",
      "text": "runs this it will spit out K4 which then",
      "offset": 2676.96,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "gets remapped to account issue for me",
      "offset": 2680.64,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "automatically. The benefit of this",
      "offset": 2682.4,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "approach is the model again it's same",
      "offset": 2684.4,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "it's the exact same thing as a YouTube",
      "offset": 2686.16,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "URL thing where the model when it sees",
      "offset": 2687.599,
      "duration": 5.681
    },
    {
      "lang": "en",
      "text": "the word account issue it associates",
      "offset": 2690.56,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "these tokens with something semantically",
      "offset": 2693.28,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "meaningful and what I want to do is my",
      "offset": 2695.359,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "meaning of an account issue is actually",
      "offset": 2698.56,
      "duration": 3.279
    },
    {
      "lang": "en",
      "text": "encoded in my description way better",
      "offset": 2700,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "than natural you want to say zero",
      "offset": 2701.839,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "attention on the label name because",
      "offset": 2704.319,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "that's for the coders and the program",
      "offset": 2706.16,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "that's consuming this all attention on",
      "offset": 2708,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "the description so that I can control",
      "offset": 2710.319,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "exactly what the LM is going to output.",
      "offset": 2712.24,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "Exactly. Exactly. It's about reducing",
      "offset": 2715.359,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "the number of variability in the",
      "offset": 2718.24,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "problem. Dexter said it beautifully and",
      "offset": 2719.68,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "symbol tuning is a technique lets me do",
      "offset": 2722.64,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "this. The thing that we're talking about",
      "offset": 2724.48,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "with diorization where we output",
      "offset": 2726,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "um where we actually output like the",
      "offset": 2728.8,
      "duration": 3.519
    },
    {
      "lang": "en",
      "text": "actual index here that's basically the",
      "offset": 2730.48,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "same thing. Instead of the model",
      "offset": 2732.319,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "outputting the actual text of the line,",
      "offset": 2734.079,
      "duration": 5.121
    },
    {
      "lang": "en",
      "text": "it's outputting the index of the line in",
      "offset": 2736.16,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "the conversation.",
      "offset": 2739.2,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "And instead of letting the model infer",
      "offset": 2740.88,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "the index, because I could do that. I",
      "offset": 2742.88,
      "duration": 2.719
    },
    {
      "lang": "en",
      "text": "don't actually have to write this. I",
      "offset": 2744.64,
      "duration": 2.959
    },
    {
      "lang": "en",
      "text": "could just let the model infer the index",
      "offset": 2745.599,
      "duration": 5.681
    },
    {
      "lang": "en",
      "text": "by writing something like this instead.",
      "offset": 2747.599,
      "duration": 6.081
    },
    {
      "lang": "en",
      "text": "Just like a break. Yeah, model could",
      "offset": 2751.28,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "count. But why make the life harder for",
      "offset": 2753.68,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "the model? Like this is Yeah. Now you're",
      "offset": 2756.96,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "asking the model to count [Â __Â ] Are you",
      "offset": 2758.56,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "kidding me? That's terrifying. It's like",
      "offset": 2760.16,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "it's like uh you know when you do these",
      "offset": 2761.68,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "coding agents and you have like no line",
      "offset": 2763.28,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "numbers in the file versus every time",
      "offset": 2766,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "you give it to the model give it line",
      "offset": 2767.599,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "numbers and suddenly it can do these",
      "offset": 2768.96,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "edits way better. Right. Exactly. And it",
      "offset": 2770.319,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "this goes back to RTFP. If I if I read",
      "offset": 2773.2,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "this prompt even as a human I know",
      "offset": 2775.839,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "exactly what index this is without",
      "offset": 2778,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "having to spend any time about it.",
      "offset": 2779.44,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "But if I don't have these lines in there",
      "offset": 2782.079,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "that becomes a lot harder for me to go",
      "offset": 2784.079,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "do. And I think it's small things like",
      "offset": 2785.68,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "this that actually dramatically change",
      "offset": 2788,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "the quality uh of your outputs in a way",
      "offset": 2789.599,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "that I think um can make a huge",
      "offset": 2792.319,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "difference. So Anov, I hope I related uh",
      "offset": 2794.24,
      "duration": 6.079
    },
    {
      "lang": "en",
      "text": "the questions uh across across the board",
      "offset": 2796.96,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "for the one of how simulting relates to",
      "offset": 2800.319,
      "duration": 6.161
    },
    {
      "lang": "en",
      "text": "diorization and the examples. And I we",
      "offset": 2802.64,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "won't go into this today, I think, but",
      "offset": 2806.48,
      "duration": 2.639
    },
    {
      "lang": "en",
      "text": "like again, take all the advice from the",
      "offset": 2807.68,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "evals chapter and like don't go just",
      "offset": 2809.119,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "applying all this stuff willy-nilly.",
      "offset": 2811.119,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "Like get a real set, understand what how",
      "offset": 2812.88,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "your performance is today. Try changing",
      "offset": 2815.28,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "these small things, you know, whether",
      "offset": 2817.359,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "it's like, oh, I found a bug from",
      "offset": 2819.2,
      "duration": 3.119
    },
    {
      "lang": "en",
      "text": "production. Let me drop it in as a test",
      "offset": 2820.56,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "case and just change the prompt until I",
      "offset": 2822.319,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "fix this one without breaking all the",
      "offset": 2824.48,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "other ones. or even having a bigger eval",
      "offset": 2826.079,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "set which is like hey our accuracy is",
      "offset": 2828.56,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "84% and if I make this change and run",
      "offset": 2829.92,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "the exact same data through the pipeline",
      "offset": 2832.319,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "now it's 88%.",
      "offset": 2834.48,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "Exactly. Exactly.",
      "offset": 2836.64,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "Um",
      "offset": 2839.52,
      "duration": 2.64
    },
    {
      "lang": "en",
      "text": "let's start with the last part code",
      "offset": 2841.04,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "genen. Um this is something we showed a",
      "offset": 2842.16,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "couple times and this is kind of",
      "offset": 2843.76,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "related.",
      "offset": 2846.8,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "Well yeah this directly leads from the",
      "offset": 2848.319,
      "duration": 2.881
    },
    {
      "lang": "en",
      "text": "other one because it's again it's like",
      "offset": 2850,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "how do we get the model to create",
      "offset": 2851.2,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "invalid JSON for good? like how how can",
      "offset": 2853.359,
      "duration": 5.281
    },
    {
      "lang": "en",
      "text": "by getting the model to create broken",
      "offset": 2856.8,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "JSON you can actually get way better",
      "offset": 2858.64,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "performance and we'll talk about like",
      "offset": 2860.88,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "why that works by looking like under the",
      "offset": 2862.24,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "hood at like samplers and stuff right",
      "offset": 2864.079,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "yeah let's do that that's actually a",
      "offset": 2866.4,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "good idea um so in this case I want some",
      "offset": 2867.599,
      "duration": 7.441
    },
    {
      "lang": "en",
      "text": "code and I'll say uh a binary search",
      "offset": 2871.119,
      "duration": 7.761
    },
    {
      "lang": "en",
      "text": "tree with actually no let's do this a",
      "offset": 2875.04,
      "duration": 7.24
    },
    {
      "lang": "en",
      "text": "sorting algorithm",
      "offset": 2878.88,
      "duration": 3.4
    },
    {
      "lang": "en",
      "text": "with merge sort All right, cool. That's",
      "offset": 2882.56,
      "duration": 9.12
    },
    {
      "lang": "en",
      "text": "redundant. Um, so let's do this firstly.",
      "offset": 2887.52,
      "duration": 5.599
    },
    {
      "lang": "en",
      "text": "And it's going to output this. And",
      "offset": 2891.68,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "again, if I have a chat app, this is",
      "offset": 2893.119,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "excellent.",
      "offset": 2895.28,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "Um, this is really, really excellent. I",
      "offset": 2897.68,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "could show this to the user. They'll be",
      "offset": 2900.48,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "pretty happy. And we'll see the quality",
      "offset": 2902.24,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "of the code right here. It looks pretty",
      "offset": 2903.92,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "good. Uh, it has some comments and stuff",
      "offset": 2905.92,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "in it. It looks generally useful.",
      "offset": 2907.839,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "But the minute that this is the way",
      "offset": 2910.8,
      "duration": 3.039
    },
    {
      "lang": "en",
      "text": "models want to write code by the way.",
      "offset": 2912.24,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "Like this is if you if you just want to",
      "offset": 2913.839,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "get the very best code performance, let",
      "offset": 2915.76,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "it write it between markdown back ticks",
      "offset": 2918.079,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "because that is what is the majority",
      "offset": 2920,
      "duration": 5.599
    },
    {
      "lang": "en",
      "text": "present in the training set. Yeah. Now",
      "offset": 2922.64,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "I'm going to change this to actually",
      "offset": 2925.599,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "return a data model because hey, I want",
      "offset": 2926.72,
      "duration": 3.119
    },
    {
      "lang": "en",
      "text": "the code so I can go find it. I don't",
      "offset": 2928.319,
      "duration": 2.561
    },
    {
      "lang": "en",
      "text": "want to do some parsing. I want to",
      "offset": 2929.839,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "render it just the code part without all",
      "offset": 2930.88,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "those prefix or maybe I want to go run",
      "offset": 2932.559,
      "duration": 2.961
    },
    {
      "lang": "en",
      "text": "it and go do something, right? You don't",
      "offset": 2933.92,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "want to have to write code to strip out",
      "offset": 2935.52,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "that like Python back tick thing cuz",
      "offset": 2937.04,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "you're just going to turn around and run",
      "offset": 2938.88,
      "duration": 4.959
    },
    {
      "lang": "en",
      "text": "it. Maybe. And now we got this. And I",
      "offset": 2940,
      "duration": 5.119
    },
    {
      "lang": "en",
      "text": "don't actually know the quality of this",
      "offset": 2943.839,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "code, but we'll see. All I do know is it",
      "offset": 2945.119,
      "duration": 6.081
    },
    {
      "lang": "en",
      "text": "did output a lot of things. And I want",
      "offset": 2948.8,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "everyone to note something very, very",
      "offset": 2951.2,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "important here. This is actually what",
      "offset": 2952.559,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "the model output. This is raw. I just",
      "offset": 2954.559,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "copied directly the string the model",
      "offset": 2956.8,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "came out with. If I go back to the",
      "offset": 2958.319,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "tokenizer, I'll show you. I want to show",
      "offset": 2959.92,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "everyone what this means.",
      "offset": 2961.68,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "We can see what it did. Yo, slash and n",
      "offset": 2964.72,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "are two different tokens.",
      "offset": 2967.76,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "Yeah, exactly. So, it's actually",
      "offset": 2969.68,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "N. It's outputting a bunch of space",
      "offset": 2972.96,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "characters. It's It's not actually",
      "offset": 2975.04,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "outputting code. It's outputting",
      "offset": 2976.8,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "something slightly different. It's",
      "offset": 2978.64,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "something that looks like code. Will you",
      "offset": 2980,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "Sorry. Can I screenshot that and then",
      "offset": 2982.48,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "can you drop the other output into the",
      "offset": 2984.16,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "tokenizer as well? Sorry. Pop back and",
      "offset": 2986.079,
      "duration": 6.801
    },
    {
      "lang": "en",
      "text": "let me get a screenshot real quick.",
      "offset": 2989.44,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "Yeah, I'll put it side by side. How",
      "offset": 2992.88,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "about that? Okay. Yeah.",
      "offset": 2994.319,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "Because I think this is really",
      "offset": 2997.599,
      "duration": 4.121
    },
    {
      "lang": "en",
      "text": "important.",
      "offset": 2998.72,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "Okay.",
      "offset": 3001.839,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "So, if you get rid of the back ticks and",
      "offset": 3009.119,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "the actual like preamble and stuff, how",
      "offset": 3011.359,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "do the tokens compare? I'll I'll leave",
      "offset": 3013.839,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "that in there actually. Okay. Uh because",
      "offset": 3016.319,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "I think it's important. And this one has",
      "offset": 3018.079,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "like a Java example as well. So, why",
      "offset": 3020.079,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "don't I get rid of the Java example?",
      "offset": 3021.599,
      "duration": 7.681
    },
    {
      "lang": "en",
      "text": "Yeah. uh just to like keep it in. Um",
      "offset": 3024,
      "duration": 9
    },
    {
      "lang": "en",
      "text": "there's something in here.",
      "offset": 3029.28,
      "duration": 3.72
    },
    {
      "lang": "en",
      "text": "And this seems to have a print example",
      "offset": 3035.04,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "as well. So we leave that in there. What",
      "offset": 3036.64,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "we'll notice here is not it's not really",
      "offset": 3039.119,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "about the token counter in the O. What's",
      "offset": 3041.44,
      "duration": 2.879
    },
    {
      "lang": "en",
      "text": "really important here is like the",
      "offset": 3043.2,
      "duration": 2.159
    },
    {
      "lang": "en",
      "text": "quality of the code that's being",
      "offset": 3044.319,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "generated. First thing that we notice up",
      "offset": 3045.359,
      "duration": 5.121
    },
    {
      "lang": "en",
      "text": "front is recursively sort both halves.",
      "offset": 3047.52,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "So this comes out and then if we go look",
      "offset": 3050.48,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "at this all these backslash ends are",
      "offset": 3052.4,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "actually having to be forcefully",
      "offset": 3055.359,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "generated by the model to be correctly",
      "offset": 3057.119,
      "duration": 5.361
    },
    {
      "lang": "en",
      "text": "syntactical JSON out of here because you",
      "offset": 3058.8,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "can't have new lines in JSON. You have",
      "offset": 3062.48,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "to have escaped new lines. Exactly.",
      "offset": 3064.24,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "Instead of letting the model just do",
      "offset": 3066.72,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "escape new lines. So what if we just",
      "offset": 3067.92,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "told the model to go do that instead?",
      "offset": 3069.68,
      "duration": 8.159
    },
    {
      "lang": "en",
      "text": "What we'll find is code description use",
      "offset": 3071.92,
      "duration": 9.28
    },
    {
      "lang": "en",
      "text": "use triple use back tick use triple back",
      "offset": 3077.839,
      "duration": 7.801
    },
    {
      "lang": "en",
      "text": "ticks to format code",
      "offset": 3081.2,
      "duration": 4.44
    },
    {
      "lang": "en",
      "text": "Python.",
      "offset": 3087.2,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "Now let's go read the prompt. Let's see",
      "offset": 3090.88,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "what the prompt looks like. This is what",
      "offset": 3092.319,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "the prompt looks like. Use triple back",
      "offset": 3093.68,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "to read the prompt.",
      "offset": 3096.16,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "Um,",
      "offset": 3098.079,
      "duration": 5.121
    },
    {
      "lang": "en",
      "text": "and now when I go run this, what I get",
      "offset": 3099.76,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "is the model will output code exactly",
      "offset": 3103.2,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "how it was outputting before,",
      "offset": 3105.04,
      "duration": 5.279
    },
    {
      "lang": "en",
      "text": "but in a way that still allows me to do",
      "offset": 3108.64,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "structured prompting. So this is not",
      "offset": 3110.319,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "valid JSON. And like the subtle thing",
      "offset": 3112.64,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "here is like and this is kind of like I",
      "offset": 3114.72,
      "duration": 2.56
    },
    {
      "lang": "en",
      "text": "think we were having a conversation",
      "offset": 3116.48,
      "duration": 2.079
    },
    {
      "lang": "en",
      "text": "yesterday about like one of the cool",
      "offset": 3117.28,
      "duration": 2.559
    },
    {
      "lang": "en",
      "text": "things you can do with BAML and why",
      "offset": 3118.559,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "having a parser that is separate from",
      "offset": 3119.839,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "the that is outside of the model itself",
      "offset": 3121.76,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "is really powerful is because you can",
      "offset": 3124.079,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "let the model use regular new lines and",
      "offset": 3125.599,
      "duration": 5.041
    },
    {
      "lang": "en",
      "text": "it's output and then turn them back into",
      "offset": 3128.24,
      "duration": 6.319
    },
    {
      "lang": "en",
      "text": "J like regular like JSON that works.",
      "offset": 3130.64,
      "duration": 7.199
    },
    {
      "lang": "en",
      "text": "Yes. Um so now let's go do this. Now I",
      "offset": 3134.559,
      "duration": 5.921
    },
    {
      "lang": "en",
      "text": "want to make this as a lesson plan for",
      "offset": 3137.839,
      "duration": 5.921
    },
    {
      "lang": "en",
      "text": "the following input as a lesson with",
      "offset": 3140.48,
      "duration": 6.079
    },
    {
      "lang": "en",
      "text": "diffs.",
      "offset": 3143.76,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "So now what I'm going to do is I'm going",
      "offset": 3146.559,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "to output an array of code snippets,",
      "offset": 3148.319,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "not one",
      "offset": 3151.04,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "but multiple arrays. And then I'm going",
      "offset": 3153.2,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "to say make a plan um to for to go do",
      "offset": 3155.599,
      "duration": 6.281
    },
    {
      "lang": "en",
      "text": "this example",
      "offset": 3158.64,
      "duration": 3.24
    },
    {
      "lang": "en",
      "text": "section one blah blah blah section two",
      "offset": 3162.319,
      "duration": 7.161
    },
    {
      "lang": "en",
      "text": "blah blah blah blah blah. Cool.",
      "offset": 3164.88,
      "duration": 4.6
    },
    {
      "lang": "en",
      "text": "And again, we're using Fuse Shop the",
      "offset": 3169.92,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "example of using comments as guiding",
      "offset": 3171.92,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "principles. We're going to do the same",
      "offset": 3174.4,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "thing here. Uh, and then we'll add a",
      "offset": 3175.68,
      "duration": 6.159
    },
    {
      "lang": "en",
      "text": "little title here. String.",
      "offset": 3178.16,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "Um, this is funny. This is what I",
      "offset": 3181.839,
      "duration": 2.961
    },
    {
      "lang": "en",
      "text": "actually did for a workshop a couple",
      "offset": 3183.44,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "weeks ago was we had said, &quot;Hey, here's",
      "offset": 3184.8,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "the final product. Output it as sections",
      "offset": 3186.8,
      "duration": 5.519
    },
    {
      "lang": "en",
      "text": "in a lesson plan.&quot;",
      "offset": 3189.359,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "So, now we're going to do the same",
      "offset": 3192.319,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "thing.",
      "offset": 3193.44,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "And now what the model is I'm fixing",
      "offset": 3195.92,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "this bug. I mean this is cool but uh why",
      "offset": 3197.68,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "why would you want to do it this way?",
      "offset": 3200.559,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "Why would you want to do this? I guess",
      "offset": 3202.319,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "I'll show you the output because I think",
      "offset": 3204.4,
      "duration": 2.4
    },
    {
      "lang": "en",
      "text": "the output will make it more clear. So",
      "offset": 3205.599,
      "duration": 2.48
    },
    {
      "lang": "en",
      "text": "the first thing is I wanted to build a",
      "offset": 3206.8,
      "duration": 3.279
    },
    {
      "lang": "en",
      "text": "lesson plan of so I did reasoning for",
      "offset": 3208.079,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "like what lesson plan I wanted to go do.",
      "offset": 3210.079,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "So it said we're going to go do this.",
      "offset": 3212.16,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "Then it's going to actually output the",
      "offset": 3214.72,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "code and create a merge function that",
      "offset": 3216.079,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "combines two sorted arrays. Great.",
      "offset": 3218.559,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "Create a basic merge sort function with",
      "offset": 3220.4,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "recursion. So it's actually incrementing",
      "offset": 3222.16,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "it. Now you can imagine that I walk",
      "offset": 3224.079,
      "duration": 6.081
    },
    {
      "lang": "en",
      "text": "someone through the code one by one,",
      "offset": 3225.76,
      "duration": 6.4
    },
    {
      "lang": "en",
      "text": "right? And now it's intending create a",
      "offset": 3230.16,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "sort with array splitting recursive",
      "offset": 3232.16,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "calls. So now it's incrementally going",
      "offset": 3233.839,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "to do this. Now I can build a UI on top",
      "offset": 3235.599,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "of this that literally has step one,",
      "offset": 3237.359,
      "duration": 2.881
    },
    {
      "lang": "en",
      "text": "step two, step three and teach someone",
      "offset": 3238.8,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "merge sort with this benefit along the",
      "offset": 3240.24,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "way,",
      "offset": 3242.72,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "right? And along the whole time, if I",
      "offset": 3244.8,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "get rid of this section, I will I will",
      "offset": 3247.2,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "literally just comment this part out.",
      "offset": 3248.96,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "I'll show you how much harder it becomes",
      "offset": 3252,
      "duration": 5.88
    },
    {
      "lang": "en",
      "text": "for the model to actually generate this.",
      "offset": 3253.52,
      "duration": 4.36
    },
    {
      "lang": "en",
      "text": "like this is now like uh becoming",
      "offset": 3259.44,
      "duration": 6.159
    },
    {
      "lang": "en",
      "text": "significantly harder for the model to",
      "offset": 3262.96,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "actually keep track of its own code",
      "offset": 3265.599,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "because even as a developer this would",
      "offset": 3267.839,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "be very very hard for me to even unread",
      "offset": 3270.559,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "and understand this and most of the",
      "offset": 3273.839,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "training data in the model's codegen",
      "offset": 3275.76,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "doesn't actually have backslash ends as",
      "offset": 3277.92,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "this. It has it as the actual back",
      "offset": 3280.48,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "slashn. So the code quality that you're",
      "offset": 3282.16,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "getting is going to be way worse. So",
      "offset": 3284.72,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "when we go to like a harder problem,",
      "offset": 3286.319,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "let's go into a harder problem because",
      "offset": 3288,
      "duration": 2.559
    },
    {
      "lang": "en",
      "text": "merge sort is something that we all know",
      "offset": 3289.119,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "like even the basic models can go do.",
      "offset": 3290.559,
      "duration": 6.641
    },
    {
      "lang": "en",
      "text": "Um create a what is it? What's a harder",
      "offset": 3293.839,
      "duration": 7.601
    },
    {
      "lang": "en",
      "text": "problem next? Uh Kubernetes operator to",
      "offset": 3297.2,
      "duration": 9.159
    },
    {
      "lang": "en",
      "text": "spin up RDS instances in Golang.",
      "offset": 3301.44,
      "duration": 4.919
    },
    {
      "lang": "en",
      "text": "To spin up RDS spin up Yeah. instances",
      "offset": 3308.96,
      "duration": 6.159
    },
    {
      "lang": "en",
      "text": "in Golang.",
      "offset": 3312.96,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "I have no idea.",
      "offset": 3315.119,
      "duration": 5.121
    },
    {
      "lang": "en",
      "text": "I have no idea what half those words",
      "offset": 3318.8,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "mean because sadly I work in algorithms",
      "offset": 3320.24,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "land. Um, and we're seeing what the",
      "offset": 3321.92,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "model did. So, I want you to Oh, it made",
      "offset": 3324.64,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "a diff. Yeah, model made a diff. I also",
      "offset": 3325.92,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "want us to notice a couple other things.",
      "offset": 3330.24,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "The model actually intuitively just put",
      "offset": 3331.68,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "out back tick new lines anyway. It",
      "offset": 3333.599,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "actually was like, you know what? I am",
      "offset": 3335.52,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "not going to put out backslash ends. I'm",
      "offset": 3337.599,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "just going to spit out this. So, the",
      "offset": 3339.76,
      "duration": 5.359
    },
    {
      "lang": "en",
      "text": "model intuitively did this for us.",
      "offset": 3341.68,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "without us even having to prompt it at",
      "offset": 3345.119,
      "duration": 3.041
    },
    {
      "lang": "en",
      "text": "that. And that just goes to show that",
      "offset": 3346.72,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "the model's intuitive behavior",
      "offset": 3348.16,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "is not to spit out",
      "offset": 3350.88,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "uh escaped JSON. And the reason it",
      "offset": 3354.4,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "probably did this is because Go is just",
      "offset": 3356.4,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "a lot more technical than Python or",
      "offset": 3359.68,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "TypeScript and other things. So the",
      "offset": 3362.16,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "minute it got to like a hard mode",
      "offset": 3363.839,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "problem, it did the most basic things",
      "offset": 3365.28,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "for itself.",
      "offset": 3367.359,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "Yeah. Um, you want to pop back to the",
      "offset": 3369.44,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "whiteboard for really quick and just",
      "offset": 3371.52,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "highlight I I I want to highlight the",
      "offset": 3372.799,
      "duration": 5.921
    },
    {
      "lang": "en",
      "text": "sampling part of this. Um, you have it",
      "offset": 3374.72,
      "duration": 7.04
    },
    {
      "lang": "en",
      "text": "too. Yeah. Yeah.",
      "offset": 3378.72,
      "duration": 5.599
    },
    {
      "lang": "en",
      "text": "Um,",
      "offset": 3381.76,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "there you go. So, okay. So, you got that",
      "offset": 3384.319,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "up. Scroll down a little bit. Um so",
      "offset": 3386,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "basically like if if if you know how",
      "offset": 3388.559,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "samplers work essentially you have um at",
      "offset": 3390.72,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "any given point you have you know the",
      "offset": 3393.44,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "model is writing code and it's writing",
      "offset": 3395.599,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "like you know code import OS and then at",
      "offset": 3397.119,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "any given point it's it's we're at let's",
      "offset": 3401.359,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "say we're right here and we're",
      "offset": 3403.359,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "generating like the we're asking what's",
      "offset": 3405.359,
      "duration": 5.681
    },
    {
      "lang": "en",
      "text": "the next token at this moment there is",
      "offset": 3407.52,
      "duration": 6.4
    },
    {
      "lang": "en",
      "text": "you know an a distribution of what the",
      "offset": 3411.04,
      "duration": 5.279
    },
    {
      "lang": "en",
      "text": "next token's going to be right and in",
      "offset": 3413.92,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "this case It's almost always going to be",
      "offset": 3416.319,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "like new line, kind of classic new line,",
      "offset": 3417.92,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "and then there's going to be a long tail",
      "offset": 3421.2,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "of other characters that might be next,",
      "offset": 3423.68,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "right? You might have, you know,",
      "offset": 3426,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "semicolon here.",
      "offset": 3427.2,
      "duration": 5.359
    },
    {
      "lang": "en",
      "text": "Um, because maybe some code has like",
      "offset": 3430.24,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "import OS, semicolon, and then another",
      "offset": 3432.559,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "import. Maybe if it's red code",
      "offset": 3434.319,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "serialized in JSON, maybe there is a",
      "offset": 3436.96,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "backslash here which is going to lead it",
      "offset": 3438.799,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "to correctly type the slashn. And maybe",
      "offset": 3440.559,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "there's some other characters here",
      "offset": 3443.04,
      "duration": 3.279
    },
    {
      "lang": "en",
      "text": "defined by your temperature, right, of",
      "offset": 3444.64,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "like different probabilities of that",
      "offset": 3446.319,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "that's the next token. Does this make",
      "offset": 3448.319,
      "duration": 6.8
    },
    {
      "lang": "en",
      "text": "sense? Yep. So when you put on strict",
      "offset": 3450.799,
      "duration": 6.481
    },
    {
      "lang": "en",
      "text": "mode or strict JSON mode, and even in",
      "offset": 3455.119,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "some of the more like old school",
      "offset": 3457.28,
      "duration": 2.559
    },
    {
      "lang": "en",
      "text": "function calling modes, they're starting",
      "offset": 3458.48,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "to enforce this. Basically,",
      "offset": 3459.839,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "that is going to when the model gets to",
      "offset": 3462.96,
      "duration": 5.359
    },
    {
      "lang": "en",
      "text": "its like time to do the correct output,",
      "offset": 3465.359,
      "duration": 5.681
    },
    {
      "lang": "en",
      "text": "it's just going to X out anything that",
      "offset": 3468.319,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "would break the JSON schema. Which means",
      "offset": 3471.04,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "that a new line is not a valid character",
      "offset": 3473.52,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "because a new line is not valid JSON.",
      "offset": 3475.68,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "And this is why when people say like uh",
      "offset": 3478.16,
      "duration": 6.639
    },
    {
      "lang": "en",
      "text": "you know using strict mode reduces the",
      "offset": 3481.68,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "accuracy of your outputs, it's because",
      "offset": 3484.799,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "now you're removing the big one and you",
      "offset": 3486.72,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "have a very very like tight distribution",
      "offset": 3488.88,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "of the other things. Now these",
      "offset": 3492.88,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "probabilities get balanced out and you",
      "offset": 3494.64,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "have a bunch of things that are like",
      "offset": 3497.119,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "probably next but like not clear. And so",
      "offset": 3498.24,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "you're likely to get weird janky code",
      "offset": 3501.28,
      "duration": 3.519
    },
    {
      "lang": "en",
      "text": "with like semicolons in it instead of",
      "offset": 3503.04,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "backslashes or even like invalid syntax",
      "offset": 3504.799,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "because you're not letting the model",
      "offset": 3507.2,
      "duration": 2.399
    },
    {
      "lang": "en",
      "text": "write code in the way that it's been",
      "offset": 3508.48,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "trained to write code. Yeah. And this",
      "offset": 3509.599,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "applies not just for codegen, but it",
      "offset": 3512.4,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "applies to any domain where anytime",
      "offset": 3514.079,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "you're having the model not pick its",
      "offset": 3515.839,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "best token. You're basically telling the",
      "offset": 3517.599,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "model you know better than the model,",
      "offset": 3520,
      "duration": 3.119
    },
    {
      "lang": "en",
      "text": "which may be true in some scenarios. I",
      "offset": 3521.52,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "want to articulate that. But most of the",
      "offset": 3523.119,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "time in machine learning, what we've",
      "offset": 3526.16,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "learned is let the model do what it does",
      "offset": 3528.079,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "best. and just let it output the best",
      "offset": 3529.76,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "token. Uh and in computer vision we had",
      "offset": 3531.92,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "this problem all the time where we'd",
      "offset": 3533.68,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "always let the model like we trying to",
      "offset": 3534.96,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "be very clever about the model where we",
      "offset": 3536.88,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "do oh let's do this pre-processing let's",
      "offset": 3539.119,
      "duration": 3.281
    },
    {
      "lang": "en",
      "text": "do this post-processing it turned out",
      "offset": 3540.72,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "the best answer as all the VLM have",
      "offset": 3542.4,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "showed is literally just give it all to",
      "offset": 3544.799,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "the model let it decide and I think the",
      "offset": 3547.92,
      "duration": 2.879
    },
    {
      "lang": "en",
      "text": "same thing is true with token generation",
      "offset": 3549.52,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "or everything else too like don't try",
      "offset": 3550.799,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "and be clever with token generation just",
      "offset": 3552.4,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "let the model pick the best token",
      "offset": 3554,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "um I think that's all we have time for",
      "offset": 3557.2,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "today in terms of actual topics and",
      "offset": 3559.44,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "prompting techniques techniques. Um, I",
      "offset": 3561.28,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "hope that this was incredibly useful for",
      "offset": 3563.599,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "everyone else. Um, what we'll do for the",
      "offset": 3565.68,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "next 15 20 minutes is I'll go to the",
      "offset": 3568,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "Discord and I'll see what prompts that",
      "offset": 3569.92,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "we have submitted, if we have any at",
      "offset": 3572.72,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "all. And there's a couple in here. Oh,",
      "offset": 3574.4,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "there are. Oh, that's actually more than",
      "offset": 3577.68,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "I expected. All right. Uh, there's two.",
      "offset": 3579.28,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "Exact. That's more than I expected. Um,",
      "offset": 3582,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "what I will do here is I will go do",
      "offset": 3584.96,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "this. Uh, let's just bring this one up.",
      "offset": 3586.96,
      "duration": 6.639
    },
    {
      "lang": "en",
      "text": "I use this prompt to evaluate LMS on",
      "offset": 3591.44,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "their ability to make sense of LM",
      "offset": 3593.599,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "generated uh events. But before we go",
      "offset": 3595.119,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "into this, does anyone have questions",
      "offset": 3597.76,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "while I go read this prompt that people",
      "offset": 3599.76,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "want to go ask for? Feel free to come",
      "offset": 3601.44,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "off mute and just ask if you after you",
      "offset": 3603.52,
      "duration": 7.319
    },
    {
      "lang": "en",
      "text": "raise your hand and um come on in.",
      "offset": 3605.52,
      "duration": 5.319
    },
    {
      "lang": "en",
      "text": "So I do have a question about that code",
      "offset": 3611.68,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "gen",
      "offset": 3613.839,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "because like when we're talk I do agree",
      "offset": 3615.68,
      "duration": 6.399
    },
    {
      "lang": "en",
      "text": "that like letting the code gen do its",
      "offset": 3618.4,
      "duration": 6.56
    },
    {
      "lang": "en",
      "text": "thing is much better uh and produces a",
      "offset": 3622.079,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "lot better results. But on the other",
      "offset": 3624.96,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "hand um like when you're working in an",
      "offset": 3627.839,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "established codebase usually it has its",
      "offset": 3630.72,
      "duration": 7.359
    },
    {
      "lang": "en",
      "text": "own like style and things like that. Um,",
      "offset": 3633.839,
      "duration": 8.081
    },
    {
      "lang": "en",
      "text": "how do you resolve that problem?",
      "offset": 3638.079,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "Yeah, my uh Dex might have his own",
      "offset": 3641.92,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "opinions. My answer for all of that is",
      "offset": 3644.319,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "always the same thing, which is just add",
      "offset": 3646.96,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "more software on top of it. If you want",
      "offset": 3648.559,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "stuff to be formatted in a good way,",
      "offset": 3650.48,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "literally just run a llinter on the",
      "offset": 3652.24,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "generated code. It will be formatted",
      "offset": 3653.68,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "exactly how you want it to be formatted.",
      "offset": 3655.52,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "If you don't have a llinter with an",
      "offset": 3658.079,
      "duration": 2.801
    },
    {
      "lang": "en",
      "text": "opinionated formatting, it's probably",
      "offset": 3659.359,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "not mimicking that. If you if you feel",
      "offset": 3660.88,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "like you don't have the llinter rules,",
      "offset": 3663.04,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "go write a quick LM prompt to look at",
      "offset": 3665.04,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "your existing code, generate lint rules",
      "offset": 3666.96,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "off of that, and then go run the",
      "offset": 3668.72,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "formatter. Um, but oh, because what I've",
      "offset": 3670,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "seen in coding agents is a lot of like,",
      "offset": 3673.92,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "okay, cool, read a couple like the like",
      "offset": 3675.68,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "using cloud code or something, it reads",
      "offset": 3677.76,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "a couple files and then what it's read",
      "offset": 3679.04,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "in the codebase already kind of",
      "offset": 3681.04,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "propagates down to the next code it",
      "offset": 3682.4,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "generates. But it almost sounds like",
      "offset": 3683.92,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "what would be much more efficient would",
      "offset": 3685.52,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "be like take a couple of the files and",
      "offset": 3686.88,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "have the model generate either like",
      "offset": 3689.04,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "hardcore lint because not all style can",
      "offset": 3690.64,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "be enforced by a llinter, right? The",
      "offset": 3692.24,
      "duration": 2.48
    },
    {
      "lang": "en",
      "text": "llinters are getting better but not",
      "offset": 3693.52,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "everything. But like either create a",
      "offset": 3694.72,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "biome rule set or an eslint rule set or",
      "offset": 3697.04,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "whatever it is or even just create a",
      "offset": 3698.96,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "prompt that is like here's a bunch of",
      "offset": 3701.68,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "examples of how we write code that so",
      "offset": 3703.2,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "the model doesn't have to read entire",
      "offset": 3704.64,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "files but you capture it succinctly.",
      "offset": 3705.92,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "Exactly. Yeah. And like do a little bit",
      "offset": 3707.76,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "of extra leg work to find the models",
      "offset": 3710,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "that represent it. And I think this is",
      "offset": 3711.68,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "the same way if you think about like",
      "offset": 3713.28,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "just hiring a new developer. There's",
      "offset": 3714.4,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "ways to build your dev team where you're",
      "offset": 3716.64,
      "duration": 2.64
    },
    {
      "lang": "en",
      "text": "like ah people on my dev team will just",
      "offset": 3717.92,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "figure out some coding format and",
      "offset": 3719.28,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "alignment. But if you really care about",
      "offset": 3720.96,
      "duration": 3.119
    },
    {
      "lang": "en",
      "text": "code quality and want it to be",
      "offset": 3722.96,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "consistent then you add a llinter. You",
      "offset": 3724.079,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "add a formatter and then it becomes",
      "offset": 3727.28,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "uniform automatically. So like and the",
      "offset": 3728.88,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "most ultimate way to do this is to end",
      "offset": 3731.68,
      "duration": 3.119
    },
    {
      "lang": "en",
      "text": "up using some language like go which",
      "offset": 3733.359,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "like forces like if you want to export",
      "offset": 3734.799,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "things that has to be capital like",
      "offset": 3736.319,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "developers don't even get a choice or",
      "offset": 3738.16,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "use black which is like a very",
      "offset": 3739.68,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "opinionated python formatter which says",
      "offset": 3741.44,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "no configuration it's just the way it is",
      "offset": 3743.44,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "and I think the same things apply for",
      "offset": 3745.839,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "like stylistic guidelines.",
      "offset": 3747.28,
      "duration": 6.88
    },
    {
      "lang": "en",
      "text": "Um does that make sense?",
      "offset": 3750.64,
      "duration": 6.479
    },
    {
      "lang": "en",
      "text": "Uh yeah I think um there's also like in",
      "offset": 3754.16,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "cursor for example there are also cursor",
      "offset": 3757.119,
      "duration": 6.161
    },
    {
      "lang": "en",
      "text": "rules. Um, yeah. Uh, which I think also",
      "offset": 3759.2,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "help with this. Although I haven't",
      "offset": 3763.28,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "really explored a lot of cursor rules.",
      "offset": 3764.88,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "Yeah. Yeah. Cursor rules are a great way",
      "offset": 3768.64,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "to go do that as well. But I think like",
      "offset": 3770.24,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "if you're building an app that generates",
      "offset": 3772.16,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "code, then you can't use cursor rules.",
      "offset": 3773.68,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "So then you have to build your own",
      "offset": 3775.359,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "equivalent of cursor rules.",
      "offset": 3776.4,
      "duration": 5.919
    },
    {
      "lang": "en",
      "text": "Um, that's really if you're using",
      "offset": 3779.04,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "cursor, then cursor rules should",
      "offset": 3782.319,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "hopefully just fix that for you. Why",
      "offset": 3783.839,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "cursor does this since cursor has built",
      "offset": 3785.599,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "a system like this. They basically added",
      "offset": 3787.599,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "a lot of software on top of their",
      "offset": 3789.599,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "codegen to make their codegen more in",
      "offset": 3791.359,
      "duration": 5.841
    },
    {
      "lang": "en",
      "text": "line with your codebase.",
      "offset": 3794.16,
      "duration": 5.679
    },
    {
      "lang": "en",
      "text": "Cool. That makes sense.",
      "offset": 3797.2,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "All right. Thank you. All right. Thanks,",
      "offset": 3799.839,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "Jonathan. Uh, one last question and then",
      "offset": 3801.839,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "I'm going to go into this prompt now",
      "offset": 3803.68,
      "duration": 5.159
    },
    {
      "lang": "en",
      "text": "that I've actually read it.",
      "offset": 3805.039,
      "duration": 3.8
    },
    {
      "lang": "en",
      "text": "Cool. Going once, going twice. All",
      "offset": 3809.76,
      "duration": 7.279
    },
    {
      "lang": "en",
      "text": "right. Hack night at GitHub. Okay. So,",
      "offset": 3812.96,
      "duration": 5.599
    },
    {
      "lang": "en",
      "text": "this was a prompt where it seems to be",
      "offset": 3817.039,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "like someone wants to look at an LM and",
      "offset": 3818.559,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "come up with like some sort of like a",
      "offset": 3820.319,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "plan for the most of this event. I mean,",
      "offset": 3822.16,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "it looks like the the prompt is",
      "offset": 3824.96,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "basically come up with a plan and the",
      "offset": 3827.359,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "rest of it is just input context, right?",
      "offset": 3829.2,
      "duration": 5.119
    },
    {
      "lang": "en",
      "text": "Yeah, exactly. So, the first thing I'll",
      "offset": 3831.44,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "notice is like let's just go back and",
      "offset": 3834.319,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "write this prompt.",
      "offset": 3836.079,
      "duration": 7.04
    },
    {
      "lang": "en",
      "text": "Um, and actually, oh yeah, plan.baml",
      "offset": 3839.039,
      "duration": 7.08
    },
    {
      "lang": "en",
      "text": "family",
      "offset": 3843.119,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "function make event. Well, actually, I'm",
      "offset": 3847.119,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "not gonna actually do this. I don't want",
      "offset": 3850.72,
      "duration": 3.06
    },
    {
      "lang": "en",
      "text": "this,",
      "offset": 3852.559,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "[Laughter]",
      "offset": 3853.78,
      "duration": 5.059
    },
    {
      "lang": "en",
      "text": "right?",
      "offset": 3855.839,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "And this thing will uh make uh this a",
      "offset": 3861.44,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "better function.",
      "offset": 3864.4,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "Okay. So, the first thing I'll notice",
      "offset": 3867.2,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "about this is Oh, what the heck? Did I",
      "offset": 3868.96,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "not pay? Oh, that's so funny. We have a",
      "offset": 3872.16,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "bug. We have a",
      "offset": 3874.24,
      "duration": 5.839
    },
    {
      "lang": "en",
      "text": "That's so funny. We have a bug where uh",
      "offset": 3877.44,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "com is it come as like markdown front",
      "offset": 3880.079,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "matter or something. It's like d-",
      "offset": 3882.72,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "comments. I think we strip it out. Uh",
      "offset": 3885.039,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "that's so funny.",
      "offset": 3887.52,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "Um fascinating. So like the first thing",
      "offset": 3889.76,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "when it comes to So let's let's catch",
      "offset": 3892.319,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "everyone else on what this prompt is.",
      "offset": 3893.92,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "This prompt is pretty simple. does come",
      "offset": 3896.4,
      "duration": 2.959
    },
    {
      "lang": "en",
      "text": "up with a plan to make the most of this",
      "offset": 3897.92,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "event and then you dump the actual event",
      "offset": 3899.359,
      "duration": 3.041
    },
    {
      "lang": "en",
      "text": "from like Luma or something else out",
      "offset": 3900.88,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "there. Now, the most intuitive way is to",
      "offset": 3902.4,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "just send that to the prompt and like we",
      "offset": 3905.28,
      "duration": 3.519
    },
    {
      "lang": "en",
      "text": "send the chat GPT it will go do",
      "offset": 3907.28,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "something. So, like if I have this, by",
      "offset": 3908.799,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "the way, if whoever wrote that prompt is",
      "offset": 3911.68,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "is here, feel free to come off mute and",
      "offset": 3913.52,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "give a little more context around what",
      "offset": 3915.359,
      "duration": 3.041
    },
    {
      "lang": "en",
      "text": "this is and what you use it for. Yeah.",
      "offset": 3916.72,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "Um, so I'm the one who posted it. This",
      "offset": 3918.4,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "is how I, you know, Luma has like a",
      "offset": 3920.96,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "hundred events a month in San Francisco",
      "offset": 3922.96,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "and I don't read them all manually at",
      "offset": 3925.68,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "first. So I use something like this to",
      "offset": 3927.76,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "try to surface the ones I want to go to.",
      "offset": 3929.44,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "And this is how I found out about BAML.",
      "offset": 3931.599,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "So you know, a pretty crude version",
      "offset": 3933.52,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "works for me and I just want to make it",
      "offset": 3935.76,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "a little more comprehensive, systemic,",
      "offset": 3937.68,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "and all that. And you know, I just don't",
      "offset": 3940,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "have an actual process for it, but I",
      "offset": 3942.64,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "know it kind of it works for me to make",
      "offset": 3944.24,
      "duration": 5.119
    },
    {
      "lang": "en",
      "text": "the sense of San Francisco tech scene.",
      "offset": 3945.839,
      "duration": 5.841
    },
    {
      "lang": "en",
      "text": "And I think I could do more with it.",
      "offset": 3949.359,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "Yeah. So over here you can see what it",
      "offset": 3951.68,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "come up with. And this is typically what",
      "offset": 3953.28,
      "duration": 3.519
    },
    {
      "lang": "en",
      "text": "you'd expect out of this sort of thing.",
      "offset": 3954.64,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "That said, what I actually want is and",
      "offset": 3956.799,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "this is step number one. Literally just",
      "offset": 3959.68,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "stop asking the model to actually go do",
      "offset": 3961.359,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "like spit out the plan as a string. Have",
      "offset": 3964,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "the model actually spit out a",
      "offset": 3965.839,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "preparation sub for you of like what to",
      "offset": 3967.2,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "go do. And when you actually go do this,",
      "offset": 3970.16,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "let's actually paste. I'll just copy and",
      "offset": 3972.16,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "paste this in myself.",
      "offset": 3974.16,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "I think I copied and pasted this example",
      "offset": 3977.2,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "as well. So, I'll make this a test case.",
      "offset": 3978.96,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "Uh, I like the Discord only let you copy",
      "offset": 3982.88,
      "duration": 7.76
    },
    {
      "lang": "en",
      "text": "one time. I know. That's so funny.",
      "offset": 3985.2,
      "duration": 7.359
    },
    {
      "lang": "en",
      "text": "Cool.",
      "offset": 3990.64,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "Great. So, I have this test case now.",
      "offset": 3992.559,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "And when I go run this, instead of the",
      "offset": 3994.72,
      "duration": 3.119
    },
    {
      "lang": "en",
      "text": "model actually spitting this stuff up",
      "offset": 3996.079,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "here, it's actually giving me something",
      "offset": 3997.839,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "a little bit better of like what I can",
      "offset": 3999.039,
      "duration": 5.441
    },
    {
      "lang": "en",
      "text": "go talk to. And in this case, I have a",
      "offset": 4001.68,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "way better experience of like who I",
      "offset": 4004.48,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "actually should go meet. I can make this",
      "offset": 4006.079,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "more targeted by simply just changing my",
      "offset": 4007.76,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "schema class networking",
      "offset": 4009.599,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "uh",
      "offset": 4014,
      "duration": 5.599
    },
    {
      "lang": "en",
      "text": "uh class networking",
      "offset": 4015.599,
      "duration": 7
    },
    {
      "lang": "en",
      "text": "opportunity.",
      "offset": 4019.599,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "Okay. name",
      "offset": 4025.2,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "season",
      "offset": 4027.28,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "value",
      "offset": 4029.44,
      "duration": 7.28
    },
    {
      "lang": "en",
      "text": "uh value high medium low description how",
      "offset": 4031.119,
      "duration": 7.281
    },
    {
      "lang": "en",
      "text": "valuable",
      "offset": 4036.72,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "how yeah we we'll push all this code",
      "offset": 4038.4,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "John the person",
      "offset": 4040,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "is to myself",
      "offset": 4042.64,
      "duration": 5.919
    },
    {
      "lang": "en",
      "text": "and my career",
      "offset": 4045.28,
      "duration": 5.759
    },
    {
      "lang": "en",
      "text": "goals yeah the other thing I think would",
      "offset": 4048.559,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "benefit a lot here is like a lot more",
      "offset": 4051.039,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "context about me and who I am although I",
      "offset": 4053.44,
      "duration": 2.879
    },
    {
      "lang": "en",
      "text": "guess if you're probably pasting this",
      "offset": 4055.359,
      "duration": 3.041
    },
    {
      "lang": "en",
      "text": "into chat GPT, then you have your memory",
      "offset": 4056.319,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "and stuff at play to kind of like give",
      "offset": 4058.4,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "that grounding. So the main thing that",
      "offset": 4061.2,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "you'll notice here is I I'm actually",
      "offset": 4064.4,
      "duration": 2.56
    },
    {
      "lang": "en",
      "text": "going to change this. I'm going to make",
      "offset": 4065.68,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "this a lot better. I'm going to say that",
      "offset": 4066.96,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "this is I want to meet these people",
      "offset": 4068.48,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "value and then it's going to dump out",
      "offset": 4070.72,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "the reason for why. And you notice that",
      "offset": 4072.24,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "actually changed out a lot of the more",
      "offset": 4074.4,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "general generally specific ones. Like",
      "offset": 4076.24,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "this was very",
      "offset": 4078.319,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "like random, but this is a lot more",
      "offset": 4080.319,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "pointed oriented. I can go act on this.",
      "offset": 4082.4,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "What else I can do here is I can say",
      "offset": 4084.88,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "like I can actually change this and like",
      "offset": 4086.64,
      "duration": 5.479
    },
    {
      "lang": "en",
      "text": "entity",
      "offset": 4089.119,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "class",
      "offset": 4094.24,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "company type",
      "offset": 4095.76,
      "duration": 4.959
    },
    {
      "lang": "en",
      "text": "company",
      "offset": 4098.96,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "name",
      "offset": 4100.719,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "class person",
      "offset": 4103.12,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "type",
      "offset": 4105.92,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "empty",
      "offset": 4107.44,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "equals this. And now when I go run this,",
      "offset": 4109.52,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "it should actually spit out what I",
      "offset": 4112.96,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "actually want. So now I can actually go",
      "offset": 4114.48,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "like specifically look these up and I",
      "offset": 4116.48,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "can build a small little UI around this,",
      "offset": 4118.319,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "like a React component that actually",
      "offset": 4119.92,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "renders these in with like LinkedIn",
      "offset": 4121.199,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "searches and follow-up sequences on top",
      "offset": 4123.279,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "of that. So then I can just go ahead and",
      "offset": 4125.199,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "say, &quot;Oh, here's a link to the company's",
      "offset": 4127.92,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "URL. Here's who they are, and here's how",
      "offset": 4129.44,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "they are.&quot; And this is just like a IML",
      "offset": 4131.279,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "speakers. Cool. No one specific was",
      "offset": 4133.04,
      "duration": 2.4
    },
    {
      "lang": "en",
      "text": "highlighted on there. So I don't",
      "offset": 4134.48,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "actually have like anyone ambiguous.",
      "offset": 4135.44,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "People are ambiguous there. But if you",
      "offset": 4137.6,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "put first name, last name, you could",
      "offset": 4139.92,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "also probably force it to like it. It",
      "offset": 4141.359,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "wouldn't even output that, right? Like",
      "offset": 4143.04,
      "duration": 2.799
    },
    {
      "lang": "en",
      "text": "if you want to if you want to drive the",
      "offset": 4144.239,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "output to the point where it's like,",
      "offset": 4145.839,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "okay, I only want things that are",
      "offset": 4147.839,
      "duration": 2.721
    },
    {
      "lang": "en",
      "text": "actually useful. I don't want this kind",
      "offset": 4149.359,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "of like hallucinating sloppy like talk",
      "offset": 4150.56,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "to a IML speakers. Like, okay, that's",
      "offset": 4152.56,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "[Â __Â ] Like I I only want like you to",
      "offset": 4154.239,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "pull out people with actual names. So I",
      "offset": 4156.239,
      "duration": 3.281
    },
    {
      "lang": "en",
      "text": "was like, if there was a speaker name in",
      "offset": 4158.159,
      "duration": 2.881
    },
    {
      "lang": "en",
      "text": "the description of like this person will",
      "offset": 4159.52,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "be speaking, then it could go tell you",
      "offset": 4161.04,
      "duration": 5.159
    },
    {
      "lang": "en",
      "text": "some things about them.",
      "offset": 4162.64,
      "duration": 3.559
    },
    {
      "lang": "en",
      "text": "And we can guarantee that at least the",
      "offset": 4168.159,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "first name or the last name exist and",
      "offset": 4169.92,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "then all other entities will just get",
      "offset": 4172.719,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "dropped.",
      "offset": 4174.319,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "So we still get these but then we they",
      "offset": 4176.64,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "actually just get dropped from our final",
      "offset": 4179.759,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "parsing because like it doesn't meet the",
      "offset": 4181.92,
      "duration": 2.879
    },
    {
      "lang": "en",
      "text": "constraint that we need which is first",
      "offset": 4183.52,
      "duration": 3.279
    },
    {
      "lang": "en",
      "text": "and last name need to actually exist. So",
      "offset": 4184.799,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "all even if that all generates it you",
      "offset": 4186.799,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "can drop it. But the whole point of this",
      "offset": 4188.48,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "is instead of actually having the model",
      "offset": 4190.159,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "spit out a string, what I really did is",
      "offset": 4191.6,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "I focused on what I care about, what I",
      "offset": 4193.52,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "want to see, and what I want to",
      "offset": 4195.679,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "personally derive out of this prompt,",
      "offset": 4197.44,
      "duration": 2.799
    },
    {
      "lang": "en",
      "text": "which is I think what John, you're",
      "offset": 4198.719,
      "duration": 2.721
    },
    {
      "lang": "en",
      "text": "trying to do is like see if things are",
      "offset": 4200.239,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "going to help you like grow out of these",
      "offset": 4201.44,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "events. So then I would just focus the",
      "offset": 4203.92,
      "duration": 6.56
    },
    {
      "lang": "en",
      "text": "specific stuff on here to say like focus",
      "offset": 4207.52,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "on how it helps me and myself it is to",
      "offset": 4210.48,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "myself and my career goals. Yeah. Guide",
      "offset": 4212.88,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "the reasoning with as much context as",
      "offset": 4215.84,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "possible. And I bet if you took this",
      "offset": 4217.84,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "JSON object and dropped it into V 0, you",
      "offset": 4219.12,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "could make a nice UI for this in, you",
      "offset": 4221.04,
      "duration": 6.639
    },
    {
      "lang": "en",
      "text": "know, 60 seconds. Oh yeah, I bet. Uh,",
      "offset": 4222.8,
      "duration": 6.72
    },
    {
      "lang": "en",
      "text": "this is",
      "offset": 4227.679,
      "duration": 8.201
    },
    {
      "lang": "en",
      "text": "same in line with this. make a UI for",
      "offset": 4229.52,
      "duration": 6.36
    },
    {
      "lang": "en",
      "text": "I'll probably go do something",
      "offset": 4242.08,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "um and I'll go build some out some UI",
      "offset": 4245.28,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "for me and now we have a full app that",
      "offset": 4247.44,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "we can just go use directly without",
      "offset": 4249.6,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "thinking about it",
      "offset": 4251.76,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "with small little rendering stuff as",
      "offset": 4254.4,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "well. Come on, this takes a while. And",
      "offset": 4256.08,
      "duration": 7.36
    },
    {
      "lang": "en",
      "text": "then you can do one more.",
      "offset": 4259.84,
      "duration": 8.2
    },
    {
      "lang": "en",
      "text": "Uh, we got time for one more prompt.",
      "offset": 4263.44,
      "duration": 4.6
    },
    {
      "lang": "en",
      "text": "I saw someone else typing in",
      "offset": 4269.28,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "or sorry, go ahead. Can I just drop the",
      "offset": 4272.64,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "prompt in the chat or should I? Uh,",
      "offset": 4274.96,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "it'll probably be too long, but you will",
      "offset": 4277.12,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "have to do it in the Discord sadly. Oh,",
      "offset": 4279.52,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "yeah. Yeah. Yeah. Okay, cool. Brashant",
      "offset": 4280.96,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "had another one as well that was uh",
      "offset": 4282.56,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "answering questions with like verbosity",
      "offset": 4284.8,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "and things like that. Yeah. So, so",
      "offset": 4287.199,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "actually you kind of answered many of",
      "offset": 4289.12,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "these in the previous example.",
      "offset": 4290.64,
      "duration": 5.519
    },
    {
      "lang": "en",
      "text": "Yeah. Okay.",
      "offset": 4293.36,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "Um, and then we'll do the last one",
      "offset": 4296.159,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "really fast. Um, while we're out here",
      "offset": 4297.92,
      "duration": 5.759
    },
    {
      "lang": "en",
      "text": "and let's while while Vzero is loading,",
      "offset": 4299.679,
      "duration": 5.601
    },
    {
      "lang": "en",
      "text": "I hate this. I This is the part I hate",
      "offset": 4303.679,
      "duration": 5.761
    },
    {
      "lang": "en",
      "text": "the most about Vzero. It takes so long.",
      "offset": 4305.28,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "Okay. While a lot of uh deterministic",
      "offset": 4309.44,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "code,",
      "offset": 4311.44,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "you are tasked with a video editing",
      "offset": 4313.44,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "plan. Okay.",
      "offset": 4315.6,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "I'm gonna This is sick. Okay, I'm just",
      "offset": 4317.36,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "gonna",
      "offset": 4320.08,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "do this. All right, so right over here,",
      "offset": 4322.96,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "by the way, we can see this.",
      "offset": 4324.48,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "So now it has a fun little UI for me to",
      "offset": 4326.88,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "go do build this in. Uh not to not to",
      "offset": 4329.12,
      "duration": 8.68
    },
    {
      "lang": "en",
      "text": "edit, just to view the final outcome.",
      "offset": 4331.84,
      "duration": 5.96
    },
    {
      "lang": "en",
      "text": "Oh, do you find the frowny face makes",
      "offset": 4342.32,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "Verscell make better content? No, I was",
      "offset": 4344.4,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "just annoyed that it did the wrong",
      "offset": 4346.96,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "thing.",
      "offset": 4348.239,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "Video. Uh, well, maybe if you went and",
      "offset": 4350.4,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "read your prompt.",
      "offset": 4352.48,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "That's Well, I can't read the visor",
      "offset": 4355.36,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "prompt. Um, so it's a little bit harder.",
      "offset": 4357.12,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "Um, insert script expert here. What is",
      "offset": 4360.159,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "this trying to do? Do you have your Do",
      "offset": 4362.4,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "you have your data models and everything",
      "offset": 4364,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "else on here?",
      "offset": 4365.28,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "If you don't, then I I can try, but it's",
      "offset": 4368.48,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "harder to do without like actual",
      "offset": 4371.52,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "function types because this prompt is a",
      "offset": 4372.96,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "little bit more complex. But let me just",
      "offset": 4374.719,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "give you some general guidelines that I",
      "offset": 4376.48,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "see right off this right off my right",
      "offset": 4378.159,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "off the top of my head when I read this",
      "offset": 4380,
      "duration": 7.36
    },
    {
      "lang": "en",
      "text": "prompt. The first thing that I see is",
      "offset": 4382.719,
      "duration": 6.241
    },
    {
      "lang": "en",
      "text": "I don't actually think you need all this",
      "offset": 4387.36,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "data. Like this is a lot more redundant.",
      "offset": 4388.96,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "you're uh I'm not sure if this is all a",
      "offset": 4392.32,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "system prompt or a user prompt, but when",
      "offset": 4394.32,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "I go look at this, the first thing that",
      "offset": 4397.04,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "I see is that this is not uh it's like",
      "offset": 4398.8,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "mixing and matching both the content and",
      "offset": 4401.36,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "the instructions all over the place",
      "offset": 4404.48,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "because like you're listing out your you",
      "offset": 4406.8,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "have instructions, content,",
      "offset": 4408.8,
      "duration": 6.64
    },
    {
      "lang": "en",
      "text": "instructions, content, instructions,",
      "offset": 4411.04,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "instructions. It looks like more",
      "offset": 4415.44,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "content. Oh, that's uh this is the",
      "offset": 4417.52,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "output schema. Oh, this is the output",
      "offset": 4419.76,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "format. Yeah. So, it looks like you're",
      "offset": 4421.44,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "But then there's more instructions.",
      "offset": 4424,
      "duration": 2.719
    },
    {
      "lang": "en",
      "text": "Yeah. It just feels like you're we're",
      "offset": 4425.44,
      "duration": 2.799
    },
    {
      "lang": "en",
      "text": "mixing a lot of instructions and it",
      "offset": 4426.719,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "doesn't read um in the way that I would",
      "offset": 4428.239,
      "duration": 5.761
    },
    {
      "lang": "en",
      "text": "write this if I were a human. And we're",
      "offset": 4431.6,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "also writing a lot of things. It's like",
      "offset": 4434,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "you are a blah blah blah blah blah. Like",
      "offset": 4435.44,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "the model doesn't care who it is. It",
      "offset": 4437.52,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "just has to know the job it wants to do.",
      "offset": 4439.12,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "You don't need to tell it this is my",
      "offset": 4441.12,
      "duration": 2.16
    },
    {
      "lang": "en",
      "text": "role. If you notice in any of the",
      "offset": 4442.32,
      "duration": 2.879
    },
    {
      "lang": "en",
      "text": "prompts, I didn't I didn't like I wasn't",
      "offset": 4443.28,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "like you're a senior engineer that does",
      "offset": 4445.199,
      "duration": 2.801
    },
    {
      "lang": "en",
      "text": "blah blah blah blah blah. I just like",
      "offset": 4446.48,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "write the code um from this prompt.",
      "offset": 4448,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "That's like the first thing I would do.",
      "offset": 4451.36,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "So, let's just like there we go. And by",
      "offset": 4452.719,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "the way, for people generating this now,",
      "offset": 4455.199,
      "duration": 2.561
    },
    {
      "lang": "en",
      "text": "you can generate this kind of UI",
      "offset": 4456.56,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "automatically from here. And this would",
      "offset": 4457.76,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "be super super easy for me to go code",
      "offset": 4460.159,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "gen. And then I could put buttons on",
      "offset": 4461.92,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "here that I'll call like enrich which",
      "offset": 4463.679,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "calls another LM function that finds all",
      "offset": 4465.76,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "the data about that company using like a",
      "offset": 4467.52,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "research thing that I go built. Sorry, I",
      "offset": 4469.6,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "context switch really fast.",
      "offset": 4471.679,
      "duration": 5.361
    },
    {
      "lang": "en",
      "text": "Um, but let me go back really fast and",
      "offset": 4474.4,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "start a new chat thing.",
      "offset": 4477.04,
      "duration": 9.48
    },
    {
      "lang": "en",
      "text": "Uh, make this prompt better and no XML.",
      "offset": 4479.44,
      "duration": 7.08
    },
    {
      "lang": "en",
      "text": "Um, and the error markdown is the thing",
      "offset": 4486.719,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "that hopefully we'll fix in. Yeah,",
      "offset": 4488.96,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "Pashant the the you are we were just",
      "offset": 4491.52,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "talking about this before the episode",
      "offset": 4493.36,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "that like asking models to adopt a role",
      "offset": 4494.48,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "is uh I think I think the best prompt",
      "offset": 4497.76,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "engineers out there have been talking",
      "offset": 4500.48,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "for months about if not longer about how",
      "offset": 4501.92,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "that doesn't really work very well or",
      "offset": 4504.4,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "like it doesn't have that much effect on",
      "offset": 4505.84,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "the output. The funny thing is that um",
      "offset": 4508.239,
      "duration": 7.521
    },
    {
      "lang": "en",
      "text": "uh this comes right out of uh clawed ROM",
      "offset": 4512.08,
      "duration": 7.44
    },
    {
      "lang": "en",
      "text": "generation as well. Yeah,",
      "offset": 4515.76,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "I bet this just because there's a lot of",
      "offset": 4519.52,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "data in the training set doesn't mean",
      "offset": 4521.84,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "it's correct or good data. Yeah, just",
      "offset": 4523.36,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "like the most code out there is kind of",
      "offset": 4526,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "[Â __Â ] You probably shouldn't follow most",
      "offset": 4527.52,
      "duration": 7.04
    },
    {
      "lang": "en",
      "text": "code. Uh but um a lot of code is still",
      "offset": 4529.36,
      "duration": 6.4
    },
    {
      "lang": "en",
      "text": "very good and you should follow that,",
      "offset": 4534.56,
      "duration": 2.32
    },
    {
      "lang": "en",
      "text": "but it's all about finding the right",
      "offset": 4535.76,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "segments. So in this case, the first",
      "offset": 4536.88,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "thing I do is like get rid of this.",
      "offset": 4538.64,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "Create a segmentation plan for the",
      "offset": 4542.64,
      "duration": 3.039
    },
    {
      "lang": "en",
      "text": "following script. Break in logical for",
      "offset": 4543.92,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "each segment. Ensure it contains",
      "offset": 4545.679,
      "duration": 3.281
    },
    {
      "lang": "en",
      "text": "complete thought or idea. Estimate a",
      "offset": 4546.88,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "reasonable time. Consider the pacing.",
      "offset": 4548.96,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "Um, and it's important to kind of like",
      "offset": 4551.12,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "describe what these mean because it",
      "offset": 4553.52,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "probably doesn't actually know and I I",
      "offset": 4556.48,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "have no idea what it actually means for",
      "offset": 4557.92,
      "duration": 3.279
    },
    {
      "lang": "en",
      "text": "fast, slow, or medium. Like I'm just it",
      "offset": 4559.36,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "just made stuff up. You need to go and",
      "offset": 4561.199,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "actually understand your own uh thing",
      "offset": 4562.719,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "for that. Uh, and like if you you could",
      "offset": 4565.04,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "even force it in the schema, right? You",
      "offset": 4568.239,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "could be like, &quot;Okay, cool. I know how",
      "offset": 4569.92,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "long this is and I can say I know I want",
      "offset": 4571.199,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "exactly, you know, do it in code and say",
      "offset": 4573.36,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "I want exactly 40 cuts because I want 30",
      "offset": 4575.36,
      "duration": 6.4
    },
    {
      "lang": "en",
      "text": "to 40 cuts versus something else. I want",
      "offset": 4577.92,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "a",
      "offset": 4581.76,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "because then we're not making the model",
      "offset": 4583.28,
      "duration": 4.919
    },
    {
      "lang": "en",
      "text": "count.",
      "offset": 4585.199,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "There you go. And instead of actually",
      "offset": 4595.44,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "outputting all the stuff,",
      "offset": 4596.88,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "I will actually just literally tell the",
      "offset": 4599.44,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "model to go do this. I will literally",
      "offset": 4600.88,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "tell it exactly what I want the pacing",
      "offset": 4603.199,
      "duration": 3.281
    },
    {
      "lang": "en",
      "text": "to be. Instead of describing all the",
      "offset": 4604.96,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "pacings, I will specifically only emit",
      "offset": 4606.48,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "the pacing that's actually relevant to",
      "offset": 4608.719,
      "duration": 3.121
    },
    {
      "lang": "en",
      "text": "the model. And that's the same thing.",
      "offset": 4610,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "The user and the program see a single",
      "offset": 4611.84,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "word fast, but then you translate that",
      "offset": 4613.92,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "into more verbose instructions, but only",
      "offset": 4616.48,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "the LM sees that part. And the LM is not",
      "offset": 4618.96,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "seeing everything else. So if I change",
      "offset": 4622,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "this from slow to fast, it sees this",
      "offset": 4623.52,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "one. Whereas in this one, it sees slow,",
      "offset": 4625.04,
      "duration": 5.679
    },
    {
      "lang": "en",
      "text": "right? So now it's able to actually go",
      "offset": 4629.04,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "do this along the way. Um, and now when",
      "offset": 4630.719,
      "duration": 6.401
    },
    {
      "lang": "en",
      "text": "I go, you can run it. Why not? Yeah, why",
      "offset": 4634.56,
      "duration": 5.56
    },
    {
      "lang": "en",
      "text": "not?",
      "offset": 4637.12,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "And I don't even know what transition",
      "offset": 4641.199,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "is. Like if transitions have a separate",
      "offset": 4642.56,
      "duration": 6.4
    },
    {
      "lang": "en",
      "text": "cut, like sure, let's do this.",
      "offset": 4644.56,
      "duration": 8.76
    },
    {
      "lang": "en",
      "text": "Let's let's just run it this way.",
      "offset": 4648.96,
      "duration": 4.36
    },
    {
      "lang": "en",
      "text": "and it's able to go do this. Now,",
      "offset": 4653.52,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "duration is kind of uh is kind of",
      "offset": 4654.88,
      "duration": 3.279
    },
    {
      "lang": "en",
      "text": "misleading and the description is kind",
      "offset": 4656.56,
      "duration": 6.8
    },
    {
      "lang": "en",
      "text": "of uh uh seconds. I'm going to change",
      "offset": 4658.159,
      "duration": 8.08
    },
    {
      "lang": "en",
      "text": "this",
      "offset": 4663.36,
      "duration": 5.879
    },
    {
      "lang": "en",
      "text": "uh",
      "offset": 4666.239,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "I don't think we need duration because",
      "offset": 4673.52,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "the duration is essentially the content.",
      "offset": 4675.12,
      "duration": 7.28
    },
    {
      "lang": "en",
      "text": "So, we can skip it. Yes. But um you",
      "offset": 4677.679,
      "duration": 7.281
    },
    {
      "lang": "en",
      "text": "might benefit from actually having uh",
      "offset": 4682.4,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "duration in there just so that a model",
      "offset": 4684.96,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "can like plan for each segment. It's the",
      "offset": 4686.719,
      "duration": 6.161
    },
    {
      "lang": "en",
      "text": "same thing expected duration kind of",
      "offset": 4690.48,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "right cuz you have you have a thing in",
      "offset": 4692.88,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "there where you're thinking about uh",
      "offset": 4695.76,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "prompting but you want the model to also",
      "offset": 4697.84,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "be thinking about duration like the",
      "offset": 4699.679,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "amount of inference it has it's about",
      "offset": 4701.36,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "the amount caches why do we have a",
      "offset": 4703.12,
      "duration": 3.119
    },
    {
      "lang": "en",
      "text": "reddis cache not because we can't go to",
      "offset": 4704.8,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "the database because we don't want to go",
      "offset": 4706.239,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "to the database all the time why are we",
      "offset": 4707.76,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "putting duration here so the model can",
      "offset": 4709.679,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "just like kind of think about this now",
      "offset": 4711.44,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "we see that this content is like pretty",
      "offset": 4713.92,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "uh short form which is totally fine but",
      "offset": 4716.4,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "if we want this be full content. Then we",
      "offset": 4719.12,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "can just do this. We can we can guide",
      "offset": 4721.76,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "the model to generating more text. Use",
      "offset": 4724.48,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "Well, I think your input test case is",
      "offset": 4726.96,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "really is really um small. I think this",
      "offset": 4728.64,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "is actually the right the right text",
      "offset": 4731.44,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "straight from the input thing. So like",
      "offset": 4733.36,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "we need like a way longer script to",
      "offset": 4735.12,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "really test this. Anyways, so drop in a",
      "offset": 4736.719,
      "duration": 5.121
    },
    {
      "lang": "en",
      "text": "can I drop in a script? I have one.",
      "offset": 4739.84,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "Yeah, drop in a script. Yes, send a",
      "offset": 4741.84,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "script. Yeah.",
      "offset": 4743.199,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "[Â __Â ] yeah.",
      "offset": 4745.52,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "On the [Â __Â ] AI that works. There you",
      "offset": 4747.44,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "go. This proof of computing.",
      "offset": 4749.52,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "I like this. We should do this more. We",
      "offset": 4753.6,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "should uh we should take people's real",
      "offset": 4755.52,
      "duration": 7.88
    },
    {
      "lang": "en",
      "text": "problems and solve them. Let's run it.",
      "offset": 4757.84,
      "duration": 5.56
    },
    {
      "lang": "en",
      "text": "Right.",
      "offset": 4766.239,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "So, you can actually see what it did. It",
      "offset": 4768.32,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "actually spit out all the content as a",
      "offset": 4770.48,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "line,",
      "offset": 4772.32,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "but the duration seconds is 60 for",
      "offset": 4774.56,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "everything. No. Do you still want it to",
      "offset": 4776.719,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "be a list by Bob or do you want it just",
      "offset": 4778.8,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "be a single string? Uh, we can Oh,",
      "offset": 4780.239,
      "duration": 5.521
    },
    {
      "lang": "en",
      "text": "sorry. Yes.",
      "offset": 4782.8,
      "duration": 5.96
    },
    {
      "lang": "en",
      "text": "Estimated",
      "offset": 4785.76,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "seconds. Um, let's give it some",
      "offset": 4789.12,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "description like what how how do you",
      "offset": 4791.12,
      "duration": 5.16
    },
    {
      "lang": "en",
      "text": "estimate duration?",
      "offset": 4792.88,
      "duration": 3.4
    },
    {
      "lang": "en",
      "text": "Uh, let's say every 1,00 characters is",
      "offset": 4796.88,
      "duration": 8.799
    },
    {
      "lang": "en",
      "text": "um a minute or 60 seconds or",
      "offset": 4801.44,
      "duration": 5.759
    },
    {
      "lang": "en",
      "text": "something like that. Oh, are we going to",
      "offset": 4805.679,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "make the model count characters",
      "offset": 4807.199,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "every like let's try this a 1,00 every",
      "offset": 4810,
      "duration": 7.44
    },
    {
      "lang": "en",
      "text": "uh so typically every 120 words per",
      "offset": 4813.679,
      "duration": 7.601
    },
    {
      "lang": "en",
      "text": "minute so uh you can count words or",
      "offset": 4817.44,
      "duration": 7.12
    },
    {
      "lang": "en",
      "text": "characters I don't know words per minute",
      "offset": 4821.28,
      "duration": 7.72
    },
    {
      "lang": "en",
      "text": "uh what is average",
      "offset": 4824.56,
      "duration": 4.44
    },
    {
      "lang": "en",
      "text": "right and we might actually find that",
      "offset": 4829.04,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "like hey if we do this it's actually",
      "offset": 4830.48,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "when we do slower pacing it's going to",
      "offset": 4832.88,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "be a little bit it's about 100 words per",
      "offset": 4834.32,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "minute.",
      "offset": 4835.92,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "If we do this, it's going to be like 120",
      "offset": 4838.32,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "and we do fast, it's going to be like",
      "offset": 4840.88,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "150. So, you might actually like find",
      "offset": 4843.199,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "that it's useful to actually guide the",
      "offset": 4845.76,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "model appropriately for the different",
      "offset": 4847.199,
      "duration": 2.48
    },
    {
      "lang": "en",
      "text": "use cases because that's what I would",
      "offset": 4848.48,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "do. I would I would have a slightly talk",
      "offset": 4849.679,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "faster voice in general, not just like",
      "offset": 4851.52,
      "duration": 4.679
    },
    {
      "lang": "en",
      "text": "the pacing.",
      "offset": 4853.04,
      "duration": 3.159
    },
    {
      "lang": "en",
      "text": "It would be interesting to also have",
      "offset": 4857.6,
      "duration": 3.119
    },
    {
      "lang": "en",
      "text": "this like start suggesting like, hey,",
      "offset": 4858.88,
      "duration": 3.279
    },
    {
      "lang": "en",
      "text": "what do you want to show on the screen",
      "offset": 4860.719,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "during this cut? Right. Exactly. And",
      "offset": 4862.159,
      "duration": 5.281
    },
    {
      "lang": "en",
      "text": "then you do like a image search and pull",
      "offset": 4865.6,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "that in um background image. So let's do",
      "offset": 4867.44,
      "duration": 5.279
    },
    {
      "lang": "en",
      "text": "that.",
      "offset": 4870.64,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "This would be a fun building like an",
      "offset": 4872.719,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "example of this end to end of like how",
      "offset": 4874.4,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "to just like generate automated video",
      "offset": 4876.239,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "content from little scripts an end toend",
      "offset": 4878.159,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "content pipeline",
      "offset": 4880.64,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "to make you can come help me build my my",
      "offset": 4883.679,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "company. I was going to say yeah we have",
      "offset": 4886.48,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "to be careful not to build a open source",
      "offset": 4888.4,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "competitor to Sahill.",
      "offset": 4890.48,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "I would love for that.",
      "offset": 4892.88,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "Um uh a description",
      "offset": 4896.719,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "description",
      "offset": 4899.92,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "that is that is so I have a I have a",
      "offset": 4902.159,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "couple of questions over here. So",
      "offset": 4905.92,
      "duration": 3.279
    },
    {
      "lang": "en",
      "text": "earlier in the example you were you were",
      "offset": 4907.36,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "showing how we can create indexes and to",
      "offset": 4909.199,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "to make sure that we are not spitting",
      "offset": 4912.239,
      "duration": 5.281
    },
    {
      "lang": "en",
      "text": "out so much uh text and saving tokens. I",
      "offset": 4914,
      "duration": 7.36
    },
    {
      "lang": "en",
      "text": "know like obviously this is a slightly",
      "offset": 4917.52,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "different case where we have to spit out",
      "offset": 4921.36,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "the text. Are there any tips or tricks",
      "offset": 4923.12,
      "duration": 5.119
    },
    {
      "lang": "en",
      "text": "we could use to",
      "offset": 4925.36,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "do that index thing in here in any way,",
      "offset": 4928.239,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "shape or form? Well, I don't actually",
      "offset": 4931.36,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "know if you have to spit out the text in",
      "offset": 4933.679,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "form. Like honestly, you could just make",
      "offset": 4935.04,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "this a lookup table based on strings.",
      "offset": 4936.639,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "Like you just spit out every line, every",
      "offset": 4938.4,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "sentence into itself",
      "offset": 4940.239,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "as like a thing. And then you could have",
      "offset": 4942.56,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "the model spit out like a span",
      "offset": 4943.92,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "of like from dialog one to dialog 7 do",
      "offset": 4946.96,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "this dialog one to three and it'll",
      "offset": 4949.44,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "naturally find break points in the",
      "offset": 4951.84,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "dialogue and now you can go do that. You",
      "offset": 4954.639,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "can ask you can build a separate",
      "offset": 4957.36,
      "duration": 2.799
    },
    {
      "lang": "en",
      "text": "pipeline that says if you really care",
      "offset": 4958.719,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "about like cost and latency I would",
      "offset": 4960.159,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "build a separate pipeline that says",
      "offset": 4962.639,
      "duration": 2.961
    },
    {
      "lang": "en",
      "text": "given all these dialogues what is the",
      "offset": 4963.84,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "most intuitive break points to inject",
      "offset": 4965.6,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "into here and then you go generate the",
      "offset": 4968.08,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "background image and everything off of",
      "offset": 4970.96,
      "duration": 2.4
    },
    {
      "lang": "en",
      "text": "that.",
      "offset": 4972.08,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "So you can solve this problem in many",
      "offset": 4973.36,
      "duration": 2.48
    },
    {
      "lang": "en",
      "text": "different ways but it's more about",
      "offset": 4974.8,
      "duration": 2.32
    },
    {
      "lang": "en",
      "text": "identifying the indexes of where the",
      "offset": 4975.84,
      "duration": 2.399
    },
    {
      "lang": "en",
      "text": "break point should be for where",
      "offset": 4977.12,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "transition should happen.",
      "offset": 4978.239,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "Oh, so becomes similar to kind of almost",
      "offset": 4980.48,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "the diorization where maybe you just",
      "offset": 4982.96,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "wanted to output like the first like the",
      "offset": 4984.48,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "the biggest like the smallest unique",
      "offset": 4986.56,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "chunk that like offsets the text there.",
      "offset": 4988.08,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "Exactly. Exactly. Where would you go in",
      "offset": 4991.04,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "that?",
      "offset": 4993.52,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "Cool. Uh we're 90 minutes. We should",
      "offset": 4995.36,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "probably wrap it up. Uh this was super",
      "offset": 4997.04,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "fun y'all. Thank you so much Vib for",
      "offset": 4998.96,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "sharing your prompting wisdom. For those",
      "offset": 5000.56,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "of you who made it to the very end,",
      "offset": 5002.4,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "congrats. We'll uh there's no prize",
      "offset": 5003.76,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "except that you got to learn more. And",
      "offset": 5006.08,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "uh we will push all the code and the",
      "offset": 5008.56,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "video and we'll send out a blast and uh",
      "offset": 5010.48,
      "duration": 5.759
    },
    {
      "lang": "en",
      "text": "come catch us next week and um we should",
      "offset": 5013.36,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "figure out what we're going to do next",
      "offset": 5016.239,
      "duration": 2.801
    },
    {
      "lang": "en",
      "text": "week. We have a we have a we have a long",
      "offset": 5017.36,
      "duration": 2.799
    },
    {
      "lang": "en",
      "text": "backlog of things, but we're going to",
      "offset": 5019.04,
      "duration": 2.56
    },
    {
      "lang": "en",
      "text": "figure it out and we'll we'll we'll",
      "offset": 5020.159,
      "duration": 3.121
    },
    {
      "lang": "en",
      "text": "update y'all with uh what's coming next.",
      "offset": 5021.6,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "So, thanks everybody.",
      "offset": 5023.28,
      "duration": 7.12
    },
    {
      "lang": "en",
      "text": "Thanks. Awesome. Thanks y'all. See you.",
      "offset": 5025.36,
      "duration": 5.04
    }
  ],
  "cleanText": "You, we've seen this in like SQL generation, or maybe this is a tactic we can talk about today, like we've seen it like SQL generation. Okay, have the model generate a JSON object that can be determined, turned into a SQL query for SVGs. The TL draw guy was talking about this at AI engineer last week. Have the model generate a structured object that it's good at writing that then deterministic code can turn into an SVG. And I think have the model generate code that then you can like bake. It's like creating different views of the same thing. Um, and then once that's baked, then you can deterministically execute that code with a programming runtime.\n\nYeah. All right. Well, with that, let's get started. Um, my name is Byov. This is Dexter. We've been doing this every week for the last few weeks now. Um, it's been months. We started in March, dude. Oh, wow. Yes. But we took a break, so I don't know if that counts. The break is where I define the line. Um, okay. But regardless, uh, the whole point of these episodes with AI that works is to talk about real practical AI applications where we don't just talk about high-level stuff, but really try and show the code behind how things work. Uh, we've talked about a bunch of things in the past from MCP servers with 10,000 plus tools to 12 factor agents by Dexter all the way to human learn how to use humans as tools and then just really how to think about prompts. But today, I think we want to do something that was different. It's going to be a lot more varied in conversation than our previous conversations which are all about focusing on one depth thing. Today we want to talk about just prompting as a whole. Nothing fancy, just plain old prompting. And many of you, uh, and actually Dexter, do you want to give a little precursor while I get the screen recording up?\n\nWell, I think like many of the things that we end up talking about, you can take like what is a really simple problem that folks kind of can look at and just say, \"Oh, that's solved,\" like classification. It's like, okay, I know how to pass the LM a list of labels and get it to output one of those labels with structured outputs or something like that. And then you go and you look under the hood and it's like, oh, like actually there's a lot of room where I thought the ceiling was of like, okay, here's the techniques. Here's how you do it. There's so much more room to basically open up the box and rip out all the wires and redo everything and like engineer it to get much better results. And I think like the core of that is always prompting. Um, and so I'm really excited today to learn about both like just some basic techniques framed in terms of certain types of problems. Um, and and I think today one of the things that it will be cool is um, we're not going to talk as much about like one big overarching problem like we usually do. We're just going to give you a grab bag of um small tips and tricks that are reusable across problem spaces and like lower level advice that you can apply to lots of problems. And I think hopefully, uh, if folks are down, I think we put a thread in in the Boundary Discord. Um, if anyone wants to share their prompts, uh, the most I've ever learned about prompt engineering is showing Fib AI applications that I've written and having him roast my prompt and tell me what we're doing wrong.\n\nUm, actually with that what I'll do is in the thing in here I will actually just post a link to this thread copy thread and I'll post this in chat if uh anyone wants, they're welcome to post their prompts that they want to share. This will be recorded.\n\nUm, and like just post it on here. We'll fix your prompts at the end and we'll just show you how we would think about them. Doesn't mean that they'll necessarily get better. It might just give you another technique or two. But with that, let's go into the topic. Cracking the prompting interview. I think prompting is literally like software engineering. And we're just going to use the same techniques to do a couple of things off the bat. So, let's start off with a very common problem that I always see, which is always the first one that I'm going to talk about, which is like labels.\n\nAnd this I think the most common example of this problem that I see is citations. So, imagine that I have a prompt. My prompt will have a bunch of text that I refer to it in for the context of rag. With the rag, I will have it give me like the URL or something attached to it and I will have a bunch of these along the way. So I have like a URL with some data and then I want to go get that and somehow in my answer I want the LM to give me out the URL.\n\nUh, this is this a problem that I resonates with this couple people. Does anyone have ideas for how we could make this better?\n\nIf not, we'll just go right into it. Uh, if today's session is going to be Are you gonna are you going to replace the URL with a sentinel token?\n\nKind of. Yeah, exactly. Because what I want is I want the answer that over here to be an answer, but I want to include the citations that that remap to that specific thing. Now, the problem is, as we all know, URLs can be really, really funky. Like just the URL for this Excalad draw is I don't know, let me see if I can share one. Um, like if I go to like, I don't know, the random browser page, I probably have something open where' it go.\n\nSorry.\n\nIf I just go to like, for example, our YouTube channel, let me just show some of these videos. The these URLs are basically you, I could have this as the citation URL for my model and let's just take a look at what it would mean for the model to generate this.\n\nLet's just go look at the tokenizer because I think this is the most important thing to think about if a model can generate something accurately or not. This is what the model has to generate. There's a bunch of tokens. So these tokens make sense. It can probably do this. YouTube is a single token. A YouTube is a single token. That's kind of interesting actually. Um, I learned that today. Uh, watch is a single token. We're good. Question mark V is a single token, which also probably makes sense because YouTube probably is a predominant force in the tokenizer for some reason. But everything else here breaks down. This ends up and this is there's like models can generate a string. If you type in that string, you say, \"Hey, model, make this string for me.\" It's going to make it. But your point is basically that like the more tokens that you're asking the model to generate accurately, the more kind of effort it has to put on that and the the less likely it's going to get it right.\n\nExactly. So in order for the model to get this part of the URL correct specifically, it has to generate 10 tokens perfectly. If we remove this part, let's assume it'll get question mark V correct. It has to get eight tokens perfectly correct. If it messes up in any of these, it becomes a useless link.\n\nSo how can we change that? Well, we can do something really, really simple. And I will just use YouTube along the way. And I'll write a basic prompt that does this and tries to go about this.\n\nOops.\n\nSo, we're going to write a question new file like uh labels.baml.\n\nI'm going to have a function that's going to say given uh like answer question. I'm going to say here's a question. I'm going to give it a list of links or content.\n\nI'll say like this will have like a URL which will be a string and then content which will be a string and then what we'll return here is some answer and then citations string array at definition list of URLs that are relevant. Okay.\n\nOpen AI GPT40.\n\nGreat.\n\nAnd um ctx output format.\n\nSorry, I'm on a live prompt, so I'm going to try and be as fast as possible. Full user question.\n\nOkay. So output format is you're telling it how to output the uh answer.\n\nExactly. And you're and you're putting the output format and the relevant content into the system prompt and then we're putting the user the question of the user prompt.\n\nExactly. So I'm going to do this. So now this my prompt um and I will literally just ask cursor to generate me a test case for this rag use case use resume.\n\nThey're all the same file. They're all going to have a test case in them. I'm going to move this use.l as uh as a reference for how works. So, I'll just have it generate a test case really fast and then it'll just go do something for me. But we can see how like um and then this takes a little bit, but we can see how like the model might struggle to go do something. Great. Accept. Cool. Let's go do this. Um, and oh man, are you going to make these URLs really freaking crazy and then uh see if we can actually get the model to screw it up.\n\nWe're just going to use this.\n\nSo, this is one YouTube URL and I will copy another YouTube URL from a different video and I will point this out. It's not even a matter of like the model will screw this up. It the the point here is it doesn't matter if the model does this perfectly or not. The point that matters is the model might screw it up.\n\nAnd if it screws it up, I have no guarantee on this end. So there's small things that I can do. So now that I have some citation thing in here, I can do something nice in my Python code to help reduce some of these errors.\n\nOh, you can put like a guard. This is from the eval thing. You can put a runtime guard of like, hey, if it outputs a URL that wasn't in our input set, bounce it back and tell it to try again. actually open just this one folder really fast. Uh, that way it's gonna be a little bit cleaner. There we go. Uh, otherwise Python versions don't work for monor repos which is the worst sin that Python have committed. We're getting there. I think the uv.python stuff might actually eventually fix it. I really hope so.\n\nSo one thing I can do is I can literally just get the answer equals this and then I can say like for URL in answer uh answer citations I somehow assert that the URL starts with this. I could like build some small search. I could I could assert that the URLs are actually in the actual content array that comes in there.\n\nUm,\n\noh,\n\nI got it. I'll I'll I'll get the link.\n\nUm, so we can actually go build this URL, right, for us. Now, we can actually go further. The problem is right over here, the URLs, as we saw, have a problem with how the model is going to generate them.\n\nSo, let's go fix that actually. And let's say this is our actual URLs.\n\nuh\n\nfrom BAML client.types import content. Cool. Now what I can do here is instead of actually putting this URL as is I could literally put a I could first change this completely and say what I actually want to do is I won't list a return of citations. I will actually list an index index of the content.\n\nAnd now that this returns an index of the content, what I will do here is literally just print this out. Content loop.index0 content idx. And now my prompt looks like this. Instead of actually dumping the actual URL, I just say content idx00 0. I can actually put like dashes here separators. I can put them beforehand because that might actually better content.\n\nI can do this. Um, and now it's actually called content content one content zero. And now I just remove the idea of the URL completely from the model. And the model will not do this. And when I go run this, what we'll find is great, we get zero and one because those are relevant indexes. And like let's make up a third one that doesn't matter.\n\nUm,\n\nEurope is pretty cool and has great pasta and ideally it shouldn't pick up the right content. It should only pick up zero and one. And now what I can do in my code instead of doing it in the model is I can convert the URL into the actual citation. So now I can just say like content of URL dot um what is it contents of URL URL or the actual URL that I actually want. So it becomes an index based lookup instead of a real one. So the idea is you really don't you really want to do your best to not rely on models generating long sequences of tokens that don't make sense for the model to actually intuitively think about sim. There's no meaning. There's no meaning baked into that random string of characters. It's just a pointer.\n\nExactly. And if you can go further and if you go back to our content about dynamic enums, you could, for example, make this a dynamic enum that then has an alias that gets mapped back to the actual value. I was going to say we could go into all of the fancy BAML features that make this even easier. Um, I'm going to say we are 20 minutes in. So, if you if you want to move on to the next tip or do you want to wrap this one up or or do you have more on the label stuff? Perfect.\n\nIt's don't use sequence of tokens that don't make sense for the model. Go update it on your own. We got one question. Symbol simple tuning also applies here. Exactly. Symbol tuning is the exact same thing. Docs will cover that. Can't talk about that right now because of time constraints. We're going to do another one. Diorization.\n\nSo, we've all seen diorization examples. We're like, um, do this. Make a make a transcript.\n\nto diorization diorization function use labels of ammo as an example.\n\nDo you want to do a quick uh whiteboard on like what what do we mean by diorization while it's working? Fine. I will go do this. I'll describe some words over here. So let's talk about diorization.\n\nDiorization. Diorization. Diorization is this idea that we have audio coming in and we want to turn the audio snippets into like a speaker plus transcript uh section. So each of these will always have a speaker and each of these will and then transform into like who said what. So the idea is most of these sequences come from and mo what most of these will do is they'll basically say l literally say speaker zero speaker one speaker zero speaker one um and you might actually want to go do something more than that because you might be having a conversation between a nurse and a patient. So you might actually want to say speaker one is a nurse, speaker two is a patient and transform\n\n\nYour transcript to that.\nI'm going to show you a prompting trip that is going to reduce the amount of text that we might have to generate by an order of magnitude to solve this problem.\nBecause if I want to go from person one to uh speaker, I like nurse versus patient versus like other cuz maybe their husband or wife spoke up into it in the middle of it.\nI want to know exactly who these personas are.\nSo, let's go do that.\nre real quick.\nUm, is there is does it is I imagine this is probably equivalent whether you're doing audio or raw just like a raw transcript of a conversation, right?\nYes.\nSo, I'm going to assume that the transcript is going to have a speaker.\nUh, let's just say the transcript is on, let's simplify this a little bit.\nLet's say the transcript is literally just a string.\nAnd what I want to do is I want to identify the speakers that exist for each of these, right?\nSo the transcript is literally just going to be a string and I I have um no other information about it.\nUh transcript will turn into that.\nAnd then what I want is I want to return a di transcript which is going to be a bunch of speaker segments.\nDon't need this.\nAnd this will just have speaker string uh text.\nAnd you might even say that this is like nurse doctor patient or other.\nUh so let's let's like write it like here.\nCool.\nUm, identify identify the speakers output format and then roll user.\nOkay, cool.\nThat's probably good enough.\nOh, that's actually pretty cool.\nUm, let's change this.\nBut you actually just want the raw text, right?\nYeah.\nSo I will Oh yeah, that's true.\nThank you for identifying that, Dexter.\nActually, I think our test case is converted correctly.\nUh are you I'm hurt.\nMy knee hurts.\nI'm sorry.\nSorry.\nSo, so this is already has the speakers identified though, right?\nLike, but it doesn't tell me who's who.\nOkay.\nIs so would this technique work?\nLike is this applicable also to just a like non like if I just have a a stream of text and I don't it's not already split up by speaker.\nI guess this Okay, so this just assumes you have turn detection but not necessarily um let's say we don't know who the speaker is.\nWe don't know anything about this.\nWhat we really want to do is we want to go and convert this in a really quick way.\nUm so I'm going to go change it's been hurting for three days now.\nUh fix um he's been complaining about it for a while.\nSo this is interesting because there might be a lot of other content here.\nSo let's just see firstly what the what the what the raw thing ends up being.\nYeah.\nAnd cool.\nThis this seems kind of interesting.\nIt's like cool.\nIt has other it has all these other things in here.\nLet's try and make this better really fast.\nUm and I'm going to combine like two or three different of the prompting tips right in one as I go.\nSo, the first thing I want to notice is, hey, this is probably not very useful.\nSo, let's try and just like fix this.\nWell, part of it is not useful.\nWell, one, I'm outputting the whole transcript over and over again.\nThat sounds bad.\nI see.\nLet's see if we can do this in a slightly better way.\nUm, so what I'm going to do is I'm going to say dialogue index int.\nSo, I'm going to give it give it the dialog index.\nAnd here, I'm just going to like write this in my prompt really fast so I don't have to think about this.\nBut, uh, the right way to do this is honestly to just make this thing an array.\nH, sorry, I love cursor.\nAnd we'll make this an array.\nAnd now instead of dumping the transcript out as we are, what we'll do is we'll say for line and transcript print out the line.\nAnd now what we'll also say is this loop.index0 dialogue this.\nWe'll add an extra space in there and then we'll add that in.\nSo now this is an assumption that the the script is already an array or are we just converting the script into an array like well you can just split by you can just split by I'm assuming if you have some way have a speaker colon here you have a way to convert this into an array of some kind.\nOkay.\nRight.\nYeah.\nI think I think in yeah I think the questions that a lot of people are asking is kind of the like the real time actual speech to text use case is you don't have those like separators unless you're using like a separate like turn detection model basically.\nYes, but most people should be using a turn detection model.\nSo I'm assuming that you have that right now.\nYou're analyzing a transcript in post.\nWe can remove the speaker labels as well so it's like a little bit more clear.\nIt's like we just have all the statements that are literally speech to text per line of some kind.\nI'm going to go run this now.\nNow, you'll notice the model's actually really really good at just spitting out the dialogue index and who the who the speaker is in each of these scenarios.\nOh, so it doesn't have to reoutput the actual text itself.\nExactly.\nOrder of magnit you can imagine for long transcripts, this is an order of magnitude cheaper in terms of how much text it has to output.\nAnd we can reduce this even further and just like alias this to like alias idx.\nAnd then it'll be a lot shorter.\nAnd now it's just now it's just outputting the index in the speaker.\nI'm a little curious what would happen if you just put it all as one big string.\nWhat do you mean?\nOh, like like if you didn't split them out.\nI imagine it's probably not going to work as well.\nBut it it the reason that this works a lot better is twofold.\none I'm actually telling the model what the index is.\nSo the model has to go back and say I'm let's look at what the model does turn by turn.\nIt's going to in first output idx0.\nThen all it has to do is in its token during the attention mechanism the model goes back into its tokenizer.\nSo it literally will go back through all the tokens and just say okay what tokens do I want to look at?\nI want to look at next zero.\nIt's going to go and just say okay I need to understand this part of this this part of the segment.\nIt's easier for it to focus.\nSo even though it's a little redundant, it helps the model be a little bit more focused on its part.\nNow it's like, okay, what who likely said this?\nAnd then it's like and then it goes out and starts spitting out the next token, spits out idx.\nSo at the point of idx, now it says, oh, what's the next idx I need?\nOh, let me go back a couple tokens here.\nIt's like that was zero.\nI probably need one next.\nWe're reducing the burden on the model.\nThat's the main that's the main leverage here.\nthe model at any point is able to do way less work and then therefore output more.\nDoes that make sense, Dexter?\nYeah, I got you.\nCool.\nCool.\nNow, the thing is we may not actually know exactly who's talking here.\nLike this other thing, we might have made a bug and not actually introduced other.\nAnd in this scenario, what we'll find is likely the model will do something just output to nurse.\nit kind of hallucinated on its own.\nSo we can actually just add other as a fallback.\nSo we the model doesn't tend to hallucinate.\nWe want to prevent hallucinations when possible.\nAnd we do that by giving the model an out.\nThat's another and this is the same with all the all the classifier examples that that we talk about, right?\nis like classify the things you know you are good at classifying in the fastest, cheapest, most efficient way and then allow the model to have an escape hatch in which case you'll handle it in a different way either by sending it to a human to classify or sending it to a bigger smarter model or whatever it is.\nExactly.\nBut now let's do another thing.\nLet's do another thing.\nClues.\nLet's add some clues here.\nSo I'm going to help model things that I'm Exactly.\nSo I'm going to help the model think about what it is.\nAnd it's literally just like it's literally just dumping the text here.\nUm, and like this is not very useful.\nAdd description things that help uh uh inference to let's just add a little bit more dialogue here and we'll see what it does.\num uh let's say uh what uh might uh relevant.\nSo let's so we're noticing that what it's doing is just outputting all the clues but a lot of the times it's kind of obvious who the speaker is.\nSo let's just do this only if not obvious uh list out facts that uh help us um identify help us analyze yeah John's suggesting deductive reasoning steps which I think is gets a little towards some of the stuff we've done in the past around like structured reasoning stuff.\nOh, a speaker.\nMaybe I had a much better test case pulled up earlier.\nSo, and now you're noticing over here now something a lot more interesting.\nIt says speaker zero other because they don't know yet.\nSpeaker one uses personal pronouns indicating injury.\nThat means that they're probably a patient.\nSpeaking about the patient, so probably other along the way.\nSo, it's actually a lot more useful to actually go do this.\nAnd now we can have a lot more com confidence behind what's happening.\nBut it's also it's it's gotten it's it's gotten worse at picking the ones where it was the doctor.\nThe doctor the doctor and nurse are worse.\nYes.\nBut that might be because when you really think about it, doctor and nurse are actually confusing because how does it actually identify correctly between the doctor and the nurse?\nAnd we can go about this one more time.\nAnd if we actually go look at this, if I were to read this transcript, there is no freaking way I as the human would actually be able to know if it's actually a doctor or pat doctor or not without knowing how many people are in the room.\nVery true.\nI could be talking to my brother.\nExactly.\nExactly.\nAnd that's the point.\nThis could be my uncle talking [Â __Â ]\nUh so whenever some when you said doctor and patient got nurse you're right we intuitively felt that way but remember the model has no context around this so let's add some more cont sorry could you go to so before you clear this out could you go to the third index index number two yeah this this time it seems to have gotten it because it's making assumptions yeah yeah about it right it's make but now it's taking more from the prompt itself like the actual output format Right.\nExactly.\nIt's literally just like ah you're probably either a doctor or a patient.\nLike there's no there's no way around this.\nBut now that we forced the model to be like who uh if not only if not obvious go list out facts.\nAnd in fact the obvious answer for identifying speakers maybe other in all scenarios.\nAnd that's what I would do if I had I would unlabel everything.\nBut then I would say, &quot;Oh, but now we know for sure that this one is a patient because it has been nonobviously stated.&quot;\nBut we can go further.\nWe can make this a little bit better.\nThere there were four people in the room.\nDoctor Josh, uh, nurse off text uh uh friend unidentified.\nSo we can go do this because maybe for my EMR I know exactly who visited but I don't know I don't have any information on the other person at all.\nSo now let's add this in here and say for context.\nAnd now let's let's run this.\nAnd now what we find is that the model gets a lot better.\nRight?\nSo you could you could look at like if you want to do this for a random event, you could go get the people off the Google calendar event and just inject that at the top be like here's the people and here's their domains and here's you know two sentences of deep research about who this person is.\nExactly.\nAnd this this mechanism of how we felt like it got more inaccurate and might have diverted us from actually exploring this prompt further is actually important to understand why the model did this.\nstep back, rethink, and remember that the model did this because if I were to be completely objective, show this to a random person and tell them to identify speakers, they also would likely pick other.\nIf they had to be like if the choice would be wrong or be correct, I too would prefer to be not wrong and just pick other because other is never wrong.\nCool.\nUm, are we going to do triple back takes today?\nI'll do that in a second.\nThat's tip number two where we use diorization.\nAnd I want to show one last ver variant of this trick which is these clues.\nSo instead of outputting clues, we can just do this description uh as a precursor to the comment.\nAs a precursor comment to this field.\nSo sometimes we want but we don't want it to do reasoning as a data field.\nI don't want to deal with that.\nI just want to like output something and I want to show you what happens here.\nUm if this works uh example.\nOkay.\nSo this is getting into like how do we how do we this is a great leeway.\nThis is like how do we get the model to output busted JSON in a way that like actually helps it get better answers.\nLike comments in JSON are technically not valid.\nYeah.\nLet's see if I can force it to do this.\nI have to actually read the prompt and see what it's doing.\nUh use As if if not if speaker is ambiguous.\nList relevant comments to help narrow help narrow down uh to help narrow down um the use first.\nWell, I'm going to go run this and see what the model does.\nOkay, I can't get it to do it.\nLet me try and put this out.\nThis is like the weirdest trick that I've learned and uh so not directly in the generated output format but just in the prompt and use for and had and then accident.\nOkay.\nSo, you always tell me not to use a few shot prompting.\nI do because this is more about the structure of the response, not about the actual like learning from examples basically.\nExactly.\nSo, let's see if I can get the model to output this.\nAnd sometimes I can't.\nSometimes the model doesn't really listen.\nUm, and just dump that info as another field.\nSo, let's do another last thing.\nprefix equals answer with this.\nI noticed OpenAI has been doing this.\nOh, where like I think for whatever reason whenever you use the word JSON, they trigger something special in the prompt that goes to like some other model or something or like secretly turns on.\nOh, there you go.\nYes, exactly.\n\n\nAnd now the model's actually, um, uh, writing some more comments, but it's writing the comments be after, um, if list relevant facts on speaker before the speaker field, the reasoning before the output.\nYeah.\nUh, question.\nSo the reason to do this is to save the tokens on writing clue every single time, or it is, it's not always about that.\nIt's just like the model might just, it's just another tool in your toolbox for how you can get the model to output what you want.\nClues is one way to do it.\nBut, and you can also do the thing we do.\nIt's like put the reasoning at the top and then dump the JSON.\nAnd it sounds like this is just like, okay, if we want really targeted reasoning on each field.\nExactly.\nAnd maybe like this is way more token efficient than having it output a bunch of extra JSON.\nExactly.\nAnd you'll notice you saw me iterate a little bit on this prompt over here.\nLike I did a couple of things to go do this, but this goes into the very next tip that I want to really talk about, which is one, it's called RTFP.\nUm, for those of you that don't know RTFM, it means read the [Â __Â ] manual.\nRTFP means read the [Â __Â ] prompt.\nUm, and I say that with a lot of love because most people don't actually read the prompt.\nAnd you saw what I did when this didn't work over here.\nI just read the prompt.\nI was like, \"Oh, the if I go back to the add description mechanism, let me give you a little bit more of a description of why I didn't like this.\"\nWhen I go read this, I'm like, \"Oh, this thing over here.\nMaybe it's getting confused by the double comments.\nAnd you can see how that might be confusing to the model.\nSo since I'm using comments like nested comments in comments, I'm like, okay, let me just try and simplify this problem for the model and give it that in a place where it can't be confused.\nAnd that was the intuition that I had out here.\nUm, so it really just boils down to reading the prompt because if we can read the prompt, then we can see what the model might be doing.\nAnd of course, we can never actually know what's actually happening, but it allows us to actually know what it allows us to iterate a little bit faster and then we can say, \"Oh, that isn't working.\nLet me go fix that.\"\nThere's a question about why not use few-shot prompting.\nThere's a couple reasons.\nTypically, the way to have done few-shot prompting in this example would have been me to actually go and write an example and then write out the answer.\nBut that's not what I wanted.\nI just wanted the model to understand that it has the ability to go do this.\nIt has the ability to list out facts before it actually spits out the speaker field.\nSo I just wanted to give it the structure so it understands the thing it has to mimic.\nI don't, it's not the...\nGo ahead, Dexter.\nYeah.\nAnd and all this is again is like, okay, cool, like, yeah, probably just outputting JSON is good enough.\nOutputting reasoning first is a little bit better.\nHaving reasoning in your JSON fields is probably a little bit better.\nBut if you're running this kind of thing a hundred thousand times a day, then a tiny half a percent improvement either in efficiency or in speed or in token efficiency or in accuracy is massively valuable.\nAnd this is what we talk about every week on this show, like, how do you, how do you unlock those, like, near the top of the accuracy range?\nHow do you push things even further?\nYeah.\nHow do you get another half a percent?\nAnd this isn't, again, remember, this isn't say that this technique will work always, but it is another technique that you have available to yourself, just like we use this other technique to not spit out the entire dialogue, but rather only spit out the index, and we use this other technique to say, oh, dialogue index is actually a lot more tokens, let's use purely the word index instead, so it spits out the output tokens are way less.\nIt's small things that can make a difference.\nAnd if I actually were to look at this, my hunch actually says index itself.\nWhere'd it go?\nIndex is probably wrong.\nI should actually probably use like index because this is just a more popular token that the model will have understandings of or rather than idx.\nEven though idx is a single token, it's just more commonly understood existing processes.\nUm, cool.\nQuestion.\nQuick question.\nSo we do this actually hundreds and thousands of times a day where we put out, um, um, reasoning, and we use the reasoning as, um, eval, um, for another model.\nSo is there a way to achieve or make it a bit more efficient?\nUm, so we literally spit out clues, and these are at least a, a long sentence.\nUm, so any, any tips or tricks to make it more...\nOne way is like, if you really wanted, if you really wanted, like, um, uh, if you really wanted that, I would actually put your reasoning afterwards, like assessment.\nSo if you want to do an eval thing right over here, description, final assessment of the speaker, uh, given any clues prior, uh, clues in comments, I just do this, um, and just like let the model spit it out.\nNow you can use assessment as a thing.\nBut now you'll see that assessment is actually kind of big.\nSo what I'll do is like use phrases, uh, not complete sentences.\nAnd then I would also add into here.\nSo now I'll notice over here what it's doing, and it will just spit something out.\nAnd I would probably have to tweak this model.\nSo sometimes GPT-4 is not very good.\nSo let me try anthropic.\nIs that the right model?\nWe'll find out.\nOh, that is not the right model.\nUh, dude, I think it's 1020.\nUh, 2024 1020.\nCustom Sonic.\nThere you go.\nOh, I don't have an API key.\nOne second.\nUm, I will not be sharing my API key this time around.\nOh, that's why I come here every week.\nIt's because you always, you always leak at least one key.\nI also forget to deactivate it.\nUm, okay, let me, um...\nYeah, and just, uh, answering while he's doing that, answering the question from the thread.\nUm, uh, why not use few-shot prompting?\nUm, we talked about this a little bit, but, um, it's basically, uh, the content of the examples, uh, tends to greatly steer the model's response.\nAnd so, like, you can get, you can get the right structural results without actually putting content in your examples.\nYes.\nUm, so there we go.\nUh, so now you can see over here when I switch this cloud, I actually get really nice things where it's assessment comes with this, and now you could plug this into your evals.\nWe got a way less tokens out here.\nIt's way, it's way shorter, because we're not using complete sentences.\nSo if you really care about evals and want to, like, you want to store the data anyway, go do that.\nBut honestly, if it were up to me, I wouldn't do any of this eval stuff online.\nI would have a separate process that pulls all my data down and runs a separate eval, including the assessment for each of these segments off the raw data itself and just run a completely separate process.\nIt's going to be way cheaper, way faster because don't add more latency to a pipeline that has this.\nEach of these things that you're generating here is latency.\nSo a very latency-sensitive pipeline generally for speech to text.\nCool.\nUm, cool.\nLet's talk about, uh, so at this point we've covered labels.\nDon't use UIDs.\nDon't use URLs.\nUse like indexes whenever possible and remap them programmatically to the right thing.\nWe've talked about diorization.\nDon't emit the full transcript.\nHave the, again, have the index, have the model represent something that is way better than the full transcript.\nIn this case, an index of transcript.\nWe've talked about using inline comments to guide reasoning of sorts.\nWe've talked about read RTFP.\nReading the prompt.\nRead it always, especially when you get stuck.\nInstead of trying to keep prompting more, just keep reading it.\nWe've talked about few-shot prompting with structure, not with actual content, and how we can leverage that along the way.\nAnd I think the next thing I want to talk about is something that we've mentioned a few times, um, but it's all about codegen.\nSo, I'm going to go ahead and pull up a random, uh, file.\nHey, Webhub Anup here.\nBefore you move forward, I, in my mind, I'm still confused about using this technique where you somehow use ginger to get an index on that array.\nI...\ngo ahead versus using symbol tuning thing.\nSo when to use...\n[Music]\nOkay, so just for context, let me just pull up a symbol example.\nSo then I, we can just talk about it.\nUm, uh, I have symbol tuning right here.\nSo the idea of symbol tuning is I want to do a classification example.\nI guess I'll do this symbol tuning.\nUh, oops.\nOkay.\nUh, I have a classification prompt.\nInstead of actually classifying the prompt, I want the model to spit out one of these categories.\nAnd I have a couple different ways I can go do this.\nOh, that's interesting.\nUm, I have a couple different ways that I can go do this.\nBut one of the ways is like, instead of the model actually spitting out, um, all of my, um, classes, I can, and instead of actually writing like the word refund in the prompt, I can write just the symbol K1, and when the model runs this, it will spit out K4, which then gets remapped to account issue for me automatically.\nThe benefit of this approach is the model, again, it's same, it's the exact same thing as a YouTube URL thing where the model when it sees the word account issue, it associates these tokens with something semantically meaningful, and what I want to do is my meaning of an account issue is actually encoded in my description way better than natural.\nYou want to say zero attention on the label name because that's for the coders and the program that's consuming this, all attention on the description so that I can control exactly what the LM is going to output.\nExactly.\nExactly.\nIt's about reducing the number of variability in the problem.\nDexter said it beautifully, and symbol tuning is a technique lets me do this.\nThe thing that we're talking about with diorization where we output, um, where we actually output like the actual index here, that's basically the same thing.\nInstead of the model outputting the actual text of the line, it's outputting the index of the line in the conversation.\nAnd instead of letting the model infer the index, because I could do that, I don't actually have to write this.\nI could just let the model infer the index by writing something like this instead.\nJust like a break.\nYeah, model could count.\nBut why make the life harder for the model?\nLike this is...\nYeah.\nNow you're asking the model to count [Â __Â ].\nAre you kidding me?\nThat's terrifying.\nIt's like, it's like, uh, you know, when you do these coding agents and you have like no line numbers in the file versus every time you give it to the model, give it line numbers and suddenly it can do these edits way better.\nRight.\nExactly.\nAnd it, this goes back to RTFP.\nIf I, if I read this prompt, even as a human, I know exactly what index this is without having to spend any time about it.\nBut if I don't have these lines in there, that becomes a lot harder for me to go do.\nAnd I think it's small things like this that actually dramatically change the quality, uh, of your outputs in a way that I think, um, can make a huge difference.\nSo Anov, I hope I related, uh, the questions, uh, across, across the board for the one of how symbol tuning relates to diorization and the examples.\nAnd I, we won't go into this today, I think, but like, again, take all the advice from the evals chapter and like, don't go just applying all this stuff willy-nilly.\nLike get a real set, understand what how your performance is today.\nTry changing these small things, you know, whether it's like, oh, I found a bug from production.\nLet me drop it in as a test case and just change the prompt until I fix this one without breaking all the other ones.\nOr even having a bigger eval set, which is like, hey, our accuracy is 84%, and if I make this change and run the exact same data through the pipeline, now it's 88%.\nExactly.\nExactly.\nUm, let's start with the last part, code gen.\nUm, this is something we showed a couple times, and this is kind of related.\nWell, yeah, this directly leads from the other one because it's again, it's like, how do we get the model to create invalid JSON for good?\nLike, how, how can, by getting the model to create broken JSON, you can actually get way better performance, and we'll talk about like why that works by looking like under the hood at like samplers and stuff, right?\nYeah, let's do that.\nThat's actually a good idea.\nUm, so in this case, I want some code, and I'll say, uh, a binary search tree with, actually, no, let's do this, a sorting algorithm with merge sort.\nAll right, cool.\nThat's redundant.\nUm, so let's do this firstly.\nAnd it's going to output this.\nAnd again, if I have a chat app, this is excellent.\nUm, this is really, really excellent.\nI could show this to the user.\nThey'll be pretty happy.\nAnd we'll see the quality of the code right here.\nIt looks pretty good.\nUh, it has some comments and stuff in it.\nIt looks generally useful.\nBut the minute that this is the way models want to write code, by the way.\nLike this is, if you, if you just want to get the very best code performance, let it write it between markdown backticks because that is what is the majority present in the training set.\nYeah.\nNow I'm going to change this to actually return a data model because, hey, I want the code so I can go find it.\nI don't want to do some parsing.\nI want to render it just the code part without all those prefix, or maybe I want to go run it and go do something, right?\nYou don't want to have to write code to strip out that like Python backtick thing 'cause you're just going to turn around and run it.\nMaybe.\nAnd now we got this.\nAnd I don't actually know the quality of this code, but we'll see.\nAll I do know is it did output a lot of things.\nAnd I want everyone to note something very, very important here.\nThis is actually what the model output.\nThis is raw.\nI just copied directly the string the model came out with.\nIf I go back to the tokenizer, I'll show you.\nI want to show everyone what this means.\nWe can see what it did.\nYo, slash and n are two different tokens.\nYeah, exactly.\nSo, it's actually N.\nIt's outputting a bunch of space characters.\nIt's, it's not actually outputting code.\nIt's outputting something.\n\n\nslightly different. It's something that looks like code. Will you\nSorry. Can I screenshot that and then can you drop the other output into the tokenizer as well? Sorry. Pop back and let me get a screenshot real quick.\nYeah, I'll put it side by side. How about that? Okay. Yeah.\nBecause I think this is really important.\nOkay.\nSo, if you get rid of the back ticks and the actual like preamble and stuff, how do the tokens compare? I'll I'll leave that in there actually. Okay. Uh because I think it's important. And this one has like a Java example as well. So, why don't I get rid of the Java example?\nYeah. uh just to like keep it in. Um there's something in here.\nAnd this seems to have a print example as well. So we leave that in there. What we'll notice here is not it's not really about the token counter in the O. What's really important here is like the quality of the code that's being generated. First thing that we notice up front is recursively sort both halves.\nSo this comes out and then if we go look at this all these backslash ends are actually having to be forcefully generated by the model to be correctly syntactical JSON out of here because you can't have new lines in JSON. You have to have escaped new lines. Exactly.\nInstead of letting the model just do escape new lines. So what if we just told the model to go do that instead?\nWhat we'll find is code description use use triple use back tick use triple back ticks to format code\nPython.\nNow let's go read the prompt. Let's see what the prompt looks like. This is what the prompt looks like. Use triple back to read the prompt.\nUm,\nand now when I go run this, what I get is the model will output code exactly how it was outputting before, but in a way that still allows me to do structured prompting. So this is not valid JSON. And like the subtle thing here is like and this is kind of like I think we were having a conversation yesterday about like one of the cool things you can do with BAML and why having a parser that is separate from the that is outside of the model itself is really powerful is because you can let the model use regular new lines and it's output and then turn them back into J like regular like JSON that works.\nYes. Um so now let's go do this. Now I want to make this as a lesson plan for the following input as a lesson with diffs.\nSo now what I'm going to do is I'm going to output an array of code snippets, not one but multiple arrays. And then I'm going to say make a plan um to for to go do this example\nsection one blah blah blah section two blah blah blah blah blah. Cool.\nAnd again, we're using Fuse Shop the example of using comments as guiding principles. We're going to do the same thing here. Uh, and then we'll add a little title here. String.\nUm, this is funny. This is what I actually did for a workshop a couple weeks ago was we had said, &quot;Hey, here's the final product. Output it as sections in a lesson plan.&quot;\nSo, now we're going to do the same thing.\nAnd now what the model is I'm fixing this bug. I mean this is cool but uh why why would you want to do it this way?\nWhy would you want to do this? I guess I'll show you the output because I think the output will make it more clear. So the first thing is I wanted to build a lesson plan of so I did reasoning for like what lesson plan I wanted to go do.\nSo it said we're going to go do this. Then it's going to actually output the code and create a merge function that combines two sorted arrays. Great.\nCreate a basic merge sort function with recursion. So it's actually incrementing it. Now you can imagine that I walk someone through the code one by one, right? And now it's intending create a sort with array splitting recursive calls. So now it's incrementally going to do this. Now I can build a UI on top of this that literally has step one, step two, step three and teach someone merge sort with this benefit along the way, right? And along the whole time, if I get rid of this section, I will I will literally just comment this part out.\nI'll show you how much harder it becomes for the model to actually generate this.\nlike this is now like uh becoming significantly harder for the model to actually keep track of its own code because even as a developer this would be very very hard for me to even unread and understand this and most of the training data in the model's codegen doesn't actually have backslash ends as this. It has it as the actual back slashn. So the code quality that you're getting is going to be way worse. So when we go to like a harder problem, let's go into a harder problem because merge sort is something that we all know like even the basic models can go do.\nUm create a what is it? What's a harder problem next? Uh Kubernetes operator to spin up RDS instances in Golang.\nTo spin up RDS spin up Yeah. instances in Golang.\nI have no idea.\nI have no idea what half those words mean because sadly I work in algorithms land. Um, and we're seeing what the model did. So, I want you to Oh, it made a diff. Yeah, model made a diff. I also want us to notice a couple other things.\nThe model actually intuitively just put out back tick new lines anyway. It actually was like, you know what? I am not going to put out backslash ends. I'm just going to spit out this. So, the model intuitively did this for us.\nwithout us even having to prompt it at that. And that just goes to show that the model's intuitive behavior is not to spit out uh escaped JSON. And the reason it probably did this is because Go is just a lot more technical than Python or TypeScript and other things. So the minute it got to like a hard mode problem, it did the most basic things for itself.\nYeah. Um, you want to pop back to the whiteboard for really quick and just highlight I I I want to highlight the sampling part of this. Um, you have it too. Yeah. Yeah.\nUm,\nthere you go. So, okay. So, you got that up. Scroll down a little bit. Um so basically like if if if you know how samplers work essentially you have um at any given point you have you know the model is writing code and it's writing like you know code import OS and then at any given point it's it's we're at let's say we're right here and we're generating like the we're asking what's the next token at this moment there is you know an a distribution of what the next token's going to be right and in this case It's almost always going to be like new line, kind of classic new line, and then there's going to be a long tail of other characters that might be next, right? You might have, you know, semicolon here.\nUm, because maybe some code has like import OS, semicolon, and then another import. Maybe if it's red code serialized in JSON, maybe there is a backslash here which is going to lead it to correctly type the slashn. And maybe there's some other characters here defined by your temperature, right, of like different probabilities of that that's the next token. Does this make sense? Yep. So when you put on strict mode or strict JSON mode, and even in some of the more like old school function calling modes, they're starting to enforce this. Basically, that is going to when the model gets to its like time to do the correct output, it's just going to X out anything that would break the JSON schema. Which means that a new line is not a valid character because a new line is not valid JSON.\nAnd this is why when people say like uh you know using strict mode reduces the accuracy of your outputs, it's because now you're removing the big one and you have a very very like tight distribution of the other things. Now these probabilities get balanced out and you have a bunch of things that are like probably next but like not clear. And so you're likely to get weird janky code with like semicolons in it instead of backslashes or even like invalid syntax because you're not letting the model write code in the way that it's been trained to write code. Yeah. And this applies not just for codegen, but it applies to any domain where anytime you're having the model not pick its best token. You're basically telling the model you know better than the model, which may be true in some scenarios. I want to articulate that. But most of the time in machine learning, what we've learned is let the model do what it does best. and just let it output the best token. Uh and in computer vision we had this problem all the time where we'd always let the model like we trying to be very clever about the model where we do oh let's do this pre-processing let's do this post-processing it turned out the best answer as all the VLM have showed is literally just give it all to the model let it decide and I think the same thing is true with token generation or everything else too like don't try and be clever with token generation just let the model pick the best token\num I think that's all we have time for today in terms of actual topics and prompting techniques techniques. Um, I hope that this was incredibly useful for everyone else. Um, what we'll do for the next 15 20 minutes is I'll go to the Discord and I'll see what prompts that we have submitted, if we have any at all. And there's a couple in here. Oh, there are. Oh, that's actually more than I expected. All right. Uh, there's two. Exact. That's more than I expected. Um, what I will do here is I will go do this. Uh, let's just bring this one up.\nI use this prompt to evaluate LMS on their ability to make sense of LM generated uh events. But before we go into this, does anyone have questions while I go read this prompt that people want to go ask for? Feel free to come off mute and just ask if you after you raise your hand and um come on in.\nSo I do have a question about that codegen because like when we're talk I do agree that like letting the code gen do its thing is much better uh and produces a lot better results. But on the other hand um like when you're working in an established codebase usually it has its own like style and things like that. Um, how do you resolve that problem?\nYeah, my uh Dex might have his own opinions. My answer for all of that is always the same thing, which is just add more software on top of it. If you want stuff to be formatted in a good way, literally just run a llinter on the generated code. It will be formatted exactly how you want it to be formatted. If you don't have a llinter with an opinionated formatting, it's probably not mimicking that. If you if you feel like you don't have the llinter rules, go write a quick LM prompt to look at your existing code, generate lint rules off of that, and then go run the formatter. Um, but oh, because what I've seen in coding agents is a lot of like, okay, cool, read a couple like the like using cloud code or something, it reads a couple files and then what it's read in the codebase already kind of propagates down to the next code it generates. But it almost sounds like what would be much more efficient would be like take a couple of the files and have the model generate either like hardcore lint because not all style can be enforced by a llinter, right? The llinters are getting better but not everything. But like either create a biome rule set or an eslint rule set or whatever it is or even just create a prompt that is like here's a bunch of examples of how we write code that so the model doesn't have to read entire files but you capture it succinctly.\nExactly. Yeah. And like do a little bit of extra leg work to find the models that represent it. And I think this is the same way if you think about like just hiring a new developer. There's ways to build your dev team where you're like ah people on my dev team will just figure out some coding format and alignment. But if you really care about code quality and want it to be consistent then you add a llinter. You add a formatter and then it becomes uniform automatically. So like and the most ultimate way to do this is to end up using some language like go which like forces like if you want to export things that has to be capital like developers don't even get a choice or use black which is like a very opinionated python formatter which says no configuration it's just the way it is and I think the same things apply for like stylistic guidelines.\nUm does that make sense?\nUh yeah I think um there's also like in cursor for example there are also cursor rules. Um, yeah. Uh, which I think also help with this. Although I haven't really explored a lot of cursor rules.\nYeah. Yeah. Cursor rules are a great way to go do that as well. But I think like if you're building an app that generates code, then you can't use cursor rules. So then you have to build your own equivalent of cursor rules.\nUm, that's really if you're using cursor, then cursor rules should hopefully just fix that for you. Why cursor does this since cursor has built a system like this. They basically added a lot of software on top of their codegen to make their codegen more in line with your codebase.\nCool. That makes sense.\nAll right. Thank you. All right. Thanks, Jonathan. Uh, one last question and then I'm going to go into this prompt now that I've actually read it.\nCool. Going once, going twice. All right. Hack night at GitHub. Okay. So, this was a prompt where it seems to be like someone wants to look at an LM and come up with like some sort of like a plan for the most of this event. I mean, it looks like the the prompt is basically come up with a plan and the rest of it is just input context, right?\nYeah, exactly. So, the first thing I'll notice is like let's just go back and write this prompt.\nUm, and actually, oh yeah, plan.baml family\nfunction make event. Well, actually, I'm not gonna actually do this. I don't want this,\n[Laughter]\nright?\nAnd this thing will uh make uh this a better function.\nOkay. So, the first thing I'll notice about this is Oh, what the heck? Did I not pay? Oh, that's so funny. We have a bug. We have a\nThat's so funny. We have a bug where uh com is it come as like markdown front matter or something. It's like d- comments. I think we strip it out. Uh that's so funny.\nUm fascinating. So like the first thing when it\n\n\nComes to. So let's catch everyone else on what this prompt is. This prompt is pretty simple. Does come up with a plan to make the most of this event, and then you dump the actual event from like Luma or something else out there. Now, the most intuitive way is to just send that to the prompt, and like we send the chat GPT, it will go do something. So, like if I have this, by the way, if whoever wrote that prompt is here, feel free to come off mute and give a little more context around what this is and what you use it for. Yeah.\n\nUm, so I'm the one who posted it. This is how I, you know, Luma has like a hundred events a month in San Francisco, and I don't read them all manually at first. So I use something like this to try to surface the ones I want to go to. And this is how I found out about BAML. So you know, a pretty crude version works for me, and I just want to make it a little more comprehensive, systemic, and all that. And you know, I just don't have an actual process for it, but I know it kind of it works for me to make the sense of San Francisco tech scene. And I think I could do more with it. Yeah.\n\nSo over here you can see what it come up with. And this is typically what you'd expect out of this sort of thing. That said, what I actually want is, and this is step number one. Literally just stop asking the model to actually go do, like spit out the plan as a string. Have the model actually spit out a preparation sub for you of like what to go do. And when you actually go do this, let's actually paste. I'll just copy and paste this in myself.\n\nI think I copied and pasted this example as well. So, I'll make this a test case. Uh, I like the Discord only let you copy one time. I know. That's so funny. Cool. Great. So, I have this test case now. And when I go run this, instead of the model actually spitting this stuff up here, it's actually giving me something a little bit better of like what I can go talk to. And in this case, I have a way better experience of like who I actually should go meet. I can make this more targeted by simply just changing my schema class networking, uh, uh, class networking opportunity. Okay. Name, season, value, uh, value high, medium, low, description, how valuable, how, yeah, we, we'll push all this code.\n\nJohn, the person is to myself and my career goals. Yeah, the other thing I think would benefit a lot here is like a lot more context about me and who I am, although I guess if you're probably pasting this into chat GPT, then you have your memory and stuff at play to kind of like give that grounding. So the main thing that you'll notice here is I, I'm actually going to change this. I'm going to make this a lot better. I'm going to say that this is I want to meet these people value, and then it's going to dump out the reason for why. And you notice that actually changed out a lot of the more general, generally specific ones. Like this was very like random, but this is a lot more pointed oriented. I can go act on this.\n\nWhat else I can do here is I can say like I can actually change this and like entity class, company type, company name, class person, type, empty equals this. And now when I go run this, it should actually spit out what I actually want. So now I can actually go like specifically look these up, and I can build a small little UI around this, like a React component that actually renders these in with like LinkedIn searches and follow-up sequences on top of that. So then I can just go ahead and say, &quot;Oh, here's a link to the company's URL. Here's who they are, and here's how they are.&quot; And this is just like a IML speakers. Cool. No one specific was highlighted on there. So I don't actually have like anyone ambiguous. People are ambiguous there. But if you put first name, last name, you could also probably force it to like it. It wouldn't even output that, right? Like if you want to, if you want to drive the output to the point where it's like, okay, I only want things that are actually useful. I don't want this kind of like hallucinating sloppy like talk to a IML speakers. Like, okay, that's [Â __Â ] Like I, I only want like you to pull out people with actual names. So I was like, if there was a speaker name in the description of like this person will be speaking, then it could go tell you some things about them.\n\nAnd we can guarantee that at least the first name or the last name exist, and then all other entities will just get dropped. So we still get these, but then we they actually just get dropped from our final parsing because like it doesn't meet the constraint that we need, which is first and last name need to actually exist. So all even if that all generates it, you can drop it. But the whole point of this is instead of actually having the model spit out a string, what I really did is I focused on what I care about, what I want to see, and what I want to personally derive out of this prompt, which is I think what John, you're trying to do is like see if things are going to help you like grow out of these events. So then I would just focus the specific stuff on here to say like focus on how it helps me and myself it is to myself and my career goals. Yeah. Guide the reasoning with as much context as possible. And I bet if you took this JSON object and dropped it into V 0, you could make a nice UI for this in, you know, 60 seconds. Oh yeah, I bet. Uh, this is same in line with this. Make a UI for. I'll probably go do something. Um, and I'll go build some out some UI for me, and now we have a full app that we can just go use directly without thinking about it with small little rendering stuff as well. Come on, this takes a while. And then you can do one more.\n\nUh, we got time for one more prompt. I saw someone else typing in, or sorry, go ahead. Can I just drop the prompt in the chat or should I? Uh, it'll probably be too long, but you will have to do it in the Discord sadly. Oh, yeah. Yeah. Yeah. Okay, cool. Brashant had another one as well that was uh answering questions with like verbosity and things like that. Yeah. So, so actually you kind of answered many of these in the previous example. Yeah. Okay. Um, and then we'll do the last one really fast. Um, while we're out here, and let's while Vzero is loading, I hate this. I This is the part I hate the most about Vzero. It takes so long. Okay. While a lot of uh deterministic code, you are tasked with a video editing plan. Okay. I'm gonna This is sick. Okay, I'm just gonna do this. All right, so right over here, by the way, we can see this.\n\nSo now it has a fun little UI for me to go do build this in. Uh not to not to edit, just to view the final outcome. Oh, do you find the frowny face makes Verscell make better content? No, I was just annoyed that it did the wrong thing. Video. Uh, well, maybe if you went and read your prompt. That's Well, I can't read the visor prompt. Um, so it's a little bit harder. Um, insert script expert here. What is this trying to do? Do you have your Do you have your data models and everything else on here? If you don't, then I I can try, but it's harder to do without like actual function types because this prompt is a little bit more complex. But let me just give you some general guidelines that I see right off this right off my right off the top of my head when I read this prompt. The first thing that I see is I don't actually think you need all this data. Like this is a lot more redundant. you're uh I'm not sure if this is all a system prompt or a user prompt, but when I go look at this, the first thing that I see is that this is not uh it's like mixing and matching both the content and the instructions all over the place because like you're listing out your you have instructions, content, instructions, content, instructions, instructions. It looks like more content. Oh, that's uh this is the output schema. Oh, this is the output format. Yeah. So, it looks like you're But then there's more instructions. Yeah. It just feels like you're we're mixing a lot of instructions and it doesn't read um in the way that I would write this if I were a human. And we're also writing a lot of things. It's like you are a blah blah blah blah blah. Like the model doesn't care who it is. It just has to know the job it wants to do. You don't need to tell it this is my role. If you notice in any of the prompts, I didn't I didn't like I wasn't like you're a senior engineer that does blah blah blah blah blah. I just like write the code um from this prompt. That's like the first thing I would do. So, let's just like there we go. And by the way, for people generating this now, you can generate this kind of UI automatically from here. And this would be super super easy for me to go code gen. And then I could put buttons on here that I'll call like enrich which calls another LM function that finds all the data about that company using like a research thing that I go built. Sorry, I context switch really fast.\n\nUm, but let me go back really fast and start a new chat thing. Uh, make this prompt better and no XML. Um, and the error markdown is the thing that hopefully we'll fix in. Yeah, Pashant, the you are we were just talking about this before the episode that like asking models to adopt a role is uh I think I think the best prompt engineers out there have been talking for months about if not longer about how that doesn't really work very well or like it doesn't have that much effect on the output. The funny thing is that um uh this comes right out of uh clawed ROM generation as well. Yeah, I bet this just because there's a lot of data in the training set doesn't mean it's correct or good data. Yeah, just like the most code out there is kind of [Â __Â ] You probably shouldn't follow most code. Uh but um a lot of code is still very good and you should follow that, but it's all about finding the right segments. So in this case, the first thing I do is like get rid of this. Create a segmentation plan for the following script. Break in logical for each segment. Ensure it contains complete thought or idea. Estimate a reasonable time. Consider the pacing. Um, and it's important to kind of like describe what these mean because it probably doesn't actually know and I I have no idea what it actually means for fast, slow, or medium. Like I'm just it just made stuff up. You need to go and actually understand your own uh thing for that. Uh, and like if you you could even force it in the schema, right? You could be like, &quot;Okay, cool. I know how long this is and I can say I know I want exactly, you know, do it in code and say I want exactly 40 cuts because I want 30 to 40 cuts versus something else. I want a because then we're not making the model count.\n\nThere you go. And instead of actually outputting all the stuff, I will actually just literally tell the model to go do this. I will literally tell it exactly what I want the pacing to be. Instead of describing all the pacings, I will specifically only emit the pacing that's actually relevant to the model. And that's the same thing. The user and the program see a single word fast, but then you translate that into more verbose instructions, but only the LM sees that part. And the LM is not seeing everything else. So if I change this from slow to fast, it sees this one. Whereas in this one, it sees slow, right? So now it's able to actually go do this along the way. Um, and now when I go, you can run it. Why not? Yeah, why not? And I don't even know what transition is. Like if transitions have a separate cut, like sure, let's do this. Let's let's just run it this way. and it's able to go do this. Now, duration is kind of uh is kind of misleading and the description is kind of uh uh seconds. I'm going to change this uh I don't think we need duration because the duration is essentially the content. So, we can skip it. Yes. But um you might benefit from actually having uh duration in there just so that a model can like plan for each segment. It's the same thing expected duration kind of right cuz you have you have a thing in there where you're thinking about uh prompting but you want the model to also be thinking about duration like the amount of inference it has it's about the amount caches why do we have a reddis cache not because we can't go to the database because we don't want to go to the database all the time why are we putting duration here so the model can just like kind of think about this now we see that this content is like pretty uh short form which is totally fine but if we want this be full content. Then we can just do this. We can we can guide the model to generating more text. Use Well, I think your input test case is really is really um small. I think this is actually the right the right text straight from the input thing. So like we need like a way longer script to really test this. Anyways, so drop in a can I drop in a script? I have one. Yeah, drop in a script. Yes, send a script. Yeah. [Â __Â ] yeah. On the [Â __Â ] ai-that-works. There you go. This proof of computing. I like this. We should do this more. We should uh we should take people's real problems and solve them. Let's run it. Right. So, you can actually see what it did. It actually spit out all the content as a line, but the duration seconds is 60 for everything. No. Do you still want it to be a list by Bob or do you want it just be a single string? Uh, we can Oh, sorry. Yes. Estimated seconds. Um, let's give it some description like what how how do you estimate duration? Uh, let's say every 1,000 characters is um a minute or 60 seconds or something like that. Oh, are we going to make the model count characters every like let's try this a 1,000 every uh so typically every 120 words per minute so uh you can count words or characters I don't know words per minute\n\n\nUh, what is average?\nRight, and we might actually find that, like, hey, if we do this, it's actually, when we do slower pacing, it's going to be a little bit. It's about 100 words per minute.\nIf we do this, it's going to be like 120, and we do fast, it's going to be like 150.\nSo, you might actually like find that it's useful to actually guide the model appropriately for the different use cases because that's what I would do.\nI would I would have a slightly talk faster voice in general, not just like the pacing.\nIt would be interesting to also have this like start suggesting like, hey, what do you want to show on the screen during this cut?\nRight.\nExactly.\nAnd then you do like a image search and pull that in, um, background image.\nSo let's do that.\nThis would be a fun building like an example of this end to end of like how to just like generate automated video content from little scripts, an end to end content pipeline.\nTo make you can come help me build my my company.\nI was going to say, yeah, we have to be careful not to build a open source competitor to Sahill.\nI would love for that.\nUm, uh, a description.\nDescription.\nThat is, that is, so I have a I have a couple of questions over here.\nSo earlier in the example you were you were showing how we can create indexes and to to make sure that we are not spitting out so much uh text and saving tokens.\nI know like obviously this is a slightly different case where we have to spit out the text.\nAre there any tips or tricks we could use to do that index thing in here in any way, shape, or form?\nWell, I don't actually know if you have to spit out the text in form.\nLike honestly, you could just make this a lookup table based on strings.\nLike you just spit out every line, every sentence into itself as like a thing.\nAnd then you could have the model spit out like a span of like from dialog one to dialog 7 do this dialog one to three and it'll naturally find break points in the dialogue and now you can go do that.\nYou can ask you can build a separate pipeline that says if you really care about like cost and latency I would build a separate pipeline that says given all these dialogues what is the most intuitive break points to inject into here and then you go generate the background image and everything off of that.\nSo you can solve this problem in many different ways, but it's more about identifying the indexes of where the break point should be for where transition should happen.\nOh, so becomes similar to kind of almost the diorization where maybe you just wanted to output like the first like the the biggest like the smallest unique chunk that like offsets the text there.\nExactly.\nExactly.\nWhere would you go in that?\nCool.\nUh, we're 90 minutes.\nWe should probably wrap it up.\nUh, this was super fun y'all.\nThank you so much Vib for sharing your prompting wisdom.\nFor those of you who made it to the very end, congrats.\nWe'll uh there's no prize except that you got to learn more.\nAnd uh we will push all the code and the video and we'll send out a blast and uh come catch us next week and um we should figure out what we're going to do next week.\nWe have a we have a we have a long backlog of things, but we're going to figure it out and we'll we'll we'll update y'all with uh what's coming next.\nSo, thanks everybody.\nThanks.\nAwesome.\nThanks y'all.\nSee you.\n",
  "dumpedAt": "2025-07-21T18:43:26.239Z"
}