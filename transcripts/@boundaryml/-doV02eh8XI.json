{
  "episodeId": "-doV02eh8XI",
  "channelSlug": "@boundaryml",
  "title": "ðŸ¦„ ai that works: Context Engineering and memory deep dive #13",
  "publishedAt": "2025-07-09T14:45:00.000Z",
  "rawLines": [
    {
      "lang": "en",
      "text": "Um, do you want to",
      "offset": 0.16,
      "duration": 5.119
    },
    {
      "lang": "en",
      "text": "I know a couple folks have not takers in",
      "offset": 3.2,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "here. Do you want to make me host and I",
      "offset": 5.279,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "can I can We're going to drop the",
      "offset": 8.16,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "notetakers because we'll be pushing this",
      "offset": 9.599,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "to YouTube.",
      "offset": 11.12,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "Yes, I generally do that. But yes, we",
      "offset": 14.16,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "should do that.",
      "offset": 16.88,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "I hate the summaries that they put in",
      "offset": 19.68,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "the chat sometimes.",
      "offset": 21.279,
      "duration": 3.521
    },
    {
      "lang": "en",
      "text": "Um",
      "offset": 24.4,
      "duration": 3.119
    },
    {
      "lang": "en",
      "text": "Oh, yeah.",
      "offset": 24.8,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "I think today's episode is going to be",
      "offset": 27.519,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "really really fun. Uh we'll start at",
      "offset": 29.199,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "10:05 as per usual. But while we're um",
      "offset": 30.88,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "while we're catching up on this, a few",
      "offset": 34.079,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "of you probably got our email from last",
      "offset": 36.48,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "time uh that was coming through it. What",
      "offset": 38.079,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "did everyone think? Uh did it feel good?",
      "offset": 40.239,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "Did it feel too AI generated? Give us",
      "offset": 42.8,
      "duration": 5.64
    },
    {
      "lang": "en",
      "text": "feedback so we can make it better.",
      "offset": 44.48,
      "duration": 3.96
    },
    {
      "lang": "en",
      "text": "You can type in the chat or just hop off",
      "offset": 50.64,
      "duration": 5.32
    },
    {
      "lang": "en",
      "text": "mute and just ask.",
      "offset": 52.32,
      "duration": 3.64
    },
    {
      "lang": "en",
      "text": "You're outsourcing your 5e valves. I",
      "offset": 56.399,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "see.",
      "offset": 58.719,
      "duration": 5.401
    },
    {
      "lang": "en",
      "text": "I am outsourcing my 5e valves.",
      "offset": 59.6,
      "duration": 4.52
    },
    {
      "lang": "en",
      "text": "Yeah. I mean, my feedback, I guess, is",
      "offset": 65.28,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "that it was very human. I'm currently",
      "offset": 67.439,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "disappointed to hear that it was not",
      "offset": 70.64,
      "duration": 3.24
    },
    {
      "lang": "en",
      "text": "human written.",
      "offset": 72.32,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "[Laughter]",
      "offset": 73.88,
      "duration": 5.16
    },
    {
      "lang": "en",
      "text": "He got you. Vibbov got you.",
      "offset": 76.24,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "Yeah.",
      "offset": 79.04,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "That's a hard problem I think facing our",
      "offset": 81.28,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "industry which is",
      "offset": 83.84,
      "duration": 5.279
    },
    {
      "lang": "en",
      "text": "it does not feel good to be you know",
      "offset": 85.52,
      "duration": 6.639
    },
    {
      "lang": "en",
      "text": "reading AI content.",
      "offset": 89.119,
      "duration": 5.281
    },
    {
      "lang": "en",
      "text": "Well, one of the things that we do is I",
      "offset": 92.159,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "I agree but one of the things like we do",
      "offset": 94.4,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "is we do this content live. So for",
      "offset": 96.799,
      "duration": 3.521
    },
    {
      "lang": "en",
      "text": "people that want the human content, they",
      "offset": 98.96,
      "duration": 3.119
    },
    {
      "lang": "en",
      "text": "can come out here, but it does take like",
      "offset": 100.32,
      "duration": 3.119
    },
    {
      "lang": "en",
      "text": "10 or so hours to do a lot of the",
      "offset": 102.079,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "follow-up work that comes from the",
      "offset": 103.439,
      "duration": 2.801
    },
    {
      "lang": "en",
      "text": "content afterwards",
      "offset": 104.799,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "like send you everything",
      "offset": 106.24,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "and like just No, I'm not saying I don't",
      "offset": 108.96,
      "duration": 5.119
    },
    {
      "lang": "en",
      "text": "understand like just the pure",
      "offset": 111.6,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "human hours saved. Yeah.",
      "offset": 114.079,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "Right. That's why it's a problem. If if",
      "offset": 116,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "it was just like why are you doing this?",
      "offset": 118.399,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "Stop. Then it wouldn't be a problem, you",
      "offset": 120.96,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "know?",
      "offset": 123.2,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "Yeah. I also like I've definitely I I",
      "offset": 124.96,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "definitely feel the like exhaustion from",
      "offset": 127.759,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "AI content. I was having like a",
      "offset": 129.679,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "conversation with a guy on X last week",
      "offset": 131.2,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "and every question I had, he would",
      "offset": 133.84,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "respond to me with like a link to like a",
      "offset": 135.52,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "notion page that was clearly just like",
      "offset": 138.48,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "entirely written by 03 and it was like",
      "offset": 140.64,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "10 pages of slop and I was just like,",
      "offset": 142.64,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "&quot;All right, man. If you're not even",
      "offset": 145.2,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "going to try, then I'm out of here.&quot;",
      "offset": 146.319,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "Yeah.",
      "offset": 148.959,
      "duration": 2.401
    },
    {
      "lang": "en",
      "text": "Yeah.",
      "offset": 149.52,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "I think the I think the slop is what",
      "offset": 151.36,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "really bothers me. I we actually have a",
      "offset": 153.76,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "thing in our team where like",
      "offset": 155.44,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "we don't want to discourage people from",
      "offset": 157.04,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "using AI for writing docs",
      "offset": 158.72,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "but one of the things that we do is we",
      "offset": 160.56,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "put a little snippet in the notion that",
      "offset": 162.48,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "says this was written by Chad GBT and",
      "offset": 163.92,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "then it's like a collapsible section",
      "offset": 165.68,
      "duration": 3.279
    },
    {
      "lang": "en",
      "text": "that people can read. So it's very",
      "offset": 167.04,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "upfront, very and like no one has to",
      "offset": 168.959,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "guess whether you chat GPT your notes or",
      "offset": 170.72,
      "duration": 3.519
    },
    {
      "lang": "en",
      "text": "not because that's the worst when it",
      "offset": 172.56,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "feels like you ask them to write a doc",
      "offset": 174.239,
      "duration": 3.041
    },
    {
      "lang": "en",
      "text": "and like [Â __Â ] did you just chat GBT this",
      "offset": 175.44,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "whole thing and like not even read it",
      "offset": 177.28,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "where but if they",
      "offset": 179.2,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "you start with this was AI generated",
      "offset": 180.56,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "content and you let someone know that I",
      "offset": 182.64,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "think it's a lot better of a dynamic and",
      "offset": 185.44,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "a relationship between both people",
      "offset": 187.68,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "because both people can opt",
      "offset": 189.28,
      "duration": 2.879
    },
    {
      "lang": "en",
      "text": "well and you did that in the email right",
      "offset": 190.4,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "the email says at the end I mean we were",
      "offset": 192.159,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "talking about doing an episode on like",
      "offset": 193.92,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "tactics for authenticity but here's",
      "offset": 195.84,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "Here's your here's your mini episode",
      "offset": 197.68,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "which is uh yeah if you declare that",
      "offset": 199.68,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "something is AI generated it becomes a",
      "offset": 202.08,
      "duration": 5.519
    },
    {
      "lang": "en",
      "text": "lot easier to um",
      "offset": 204.56,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "I think if I can find it",
      "offset": 207.599,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "get away with uh I'm not saying get away",
      "offset": 209.12,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "with it but it's just like it it becomes",
      "offset": 211.2,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "a lot more authentic of like hey I made",
      "offset": 212.48,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "this with AI like let me know if it",
      "offset": 214,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "sucks like I actually put in time and we",
      "offset": 215.84,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "reviewed it but it was it was originally",
      "offset": 217.44,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "AI generated I think you polish these a",
      "offset": 219.599,
      "duration": 3.521
    },
    {
      "lang": "en",
      "text": "little bit before you paste them in",
      "offset": 221.84,
      "duration": 2.399
    },
    {
      "lang": "en",
      "text": "right",
      "offset": 223.12,
      "duration": 2.479
    },
    {
      "lang": "en",
      "text": "um no this one was actually directly",
      "offset": 224.239,
      "duration": 1.761
    },
    {
      "lang": "en",
      "text": "past",
      "offset": 225.599,
      "duration": 2
    },
    {
      "lang": "en",
      "text": "this one was just raw Huh?",
      "offset": 226,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "Oh, I did do some I did do some editing.",
      "offset": 227.599,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "Um, let's see if I can find the unicorn",
      "offset": 230.159,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "emoji.",
      "offset": 231.68,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "I I actually just went through in zero",
      "offset": 233.92,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "inbox this morning. Um,",
      "offset": 235.84,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "I can I can find the email from you.",
      "offset": 237.68,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "Uh, I can pull up on just to give people",
      "offset": 240.08,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "context like this is a problem that I",
      "offset": 242.72,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "think is really really hard to go solve.",
      "offset": 244.239,
      "duration": 3.121
    },
    {
      "lang": "en",
      "text": "So, we spent about two episodes talking",
      "offset": 245.92,
      "duration": 3.519
    },
    {
      "lang": "en",
      "text": "about this in the last two episodes",
      "offset": 247.36,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "which was specifically how can you go",
      "offset": 249.439,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "send",
      "offset": 251.28,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "a content that doesn't feel as bad.",
      "offset": 253.04,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "Let's see if I can see the preview. So,",
      "offset": 255.519,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "this is what the email was listed as",
      "offset": 257.759,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "like hello coder. We linked to the",
      "offset": 259.68,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "actual repo. We didn't get the exact",
      "offset": 261.519,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "full path in the repo. That's something",
      "offset": 263.44,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "we want to improve. We got the YouTube",
      "offset": 264.88,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "link in there. We got a little summary",
      "offset": 267.44,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "with things uh in there. The bolding",
      "offset": 270,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "didn't work. I did have to do that",
      "offset": 272.16,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "manually. I It gave me double asterisk,",
      "offset": 273.68,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "but",
      "offset": 276.88,
      "duration": 2.48
    },
    {
      "lang": "en",
      "text": "mark down.",
      "offset": 277.44,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "Uh we have the next session link that",
      "offset": 279.36,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "was AI generated uh thanks to the work",
      "offset": 282.16,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "that Dexter did that we talked about.",
      "offset": 284.08,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "And then this thing was uh manually",
      "offset": 286.32,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "added by me and that was done for that",
      "offset": 288.8,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "reason which was just to let people know",
      "offset": 290.88,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "that's an AI generated email using our",
      "offset": 292.479,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "content pipeline that we built over the",
      "offset": 294.4,
      "duration": 3.519
    },
    {
      "lang": "en",
      "text": "last two episodes and just shared that",
      "offset": 295.919,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "with folks.",
      "offset": 297.919,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "So if you if you haven't gotten this",
      "offset": 299.84,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "email, sign up. You'll get an email this",
      "offset": 301.28,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "week from this week's recording. Um, but",
      "offset": 302.96,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "with that, uh, let's get back into",
      "offset": 305.84,
      "duration": 2.799
    },
    {
      "lang": "en",
      "text": "today's",
      "offset": 308.32,
      "duration": 2
    },
    {
      "lang": "en",
      "text": "Let's talk memory.",
      "offset": 308.639,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "Let's talk context engineering. But",
      "offset": 310.32,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "yeah, it's gonna be really fun. Um, so",
      "offset": 312.96,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "for those of you that don't know, I'm",
      "offset": 314.88,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "VIBO. I'm one of the co-creators of",
      "offset": 316.32,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "BAML. And this is",
      "offset": 318,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "Dexter. Who is",
      "offset": 321.52,
      "duration": 6.399
    },
    {
      "lang": "en",
      "text": "I'm Dexter. I am uh I work on uh a",
      "offset": 323.6,
      "duration": 6.64
    },
    {
      "lang": "en",
      "text": "company called Human Layer. um build",
      "offset": 327.919,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "tools to help people build better AI",
      "offset": 330.24,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "agents and agents that feel more like",
      "offset": 332.24,
      "duration": 3.519
    },
    {
      "lang": "en",
      "text": "real people. And we have a very special",
      "offset": 334.08,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "guest with us today. Um we got Brian",
      "offset": 335.759,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "who's another very awesome YC founder",
      "offset": 338.479,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "who is actually building products that",
      "offset": 340.639,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "all the things we've been talking about",
      "offset": 342.72,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "for the last two months. Brian is",
      "offset": 344,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "building these into a real product that",
      "offset": 346.16,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "real people use and get value out of. So",
      "offset": 347.84,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "I Brian, you want to uh any further",
      "offset": 349.6,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "further intro or you want to talk about",
      "offset": 351.919,
      "duration": 2.56
    },
    {
      "lang": "en",
      "text": "kind of what you've been working on",
      "offset": 353.6,
      "duration": 2.56
    },
    {
      "lang": "en",
      "text": "briefly?",
      "offset": 354.479,
      "duration": 3.041
    },
    {
      "lang": "en",
      "text": "Sure. I mean, we're gonna go more into",
      "offset": 356.16,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "depth later on, but um I'm Brian. Um no,",
      "offset": 357.52,
      "duration": 6.88
    },
    {
      "lang": "en",
      "text": "Dexter, no viab. Um but yeah, we're I",
      "offset": 361.44,
      "duration": 5.599
    },
    {
      "lang": "en",
      "text": "work at a company called Orin. Um we're",
      "offset": 364.4,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "basically building all of these like AI",
      "offset": 367.039,
      "duration": 5.681
    },
    {
      "lang": "en",
      "text": "principles into like proactive um first",
      "offset": 368.8,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "AI tutors for mostly middle schoolers",
      "offset": 372.72,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "these days. So, it's a very very",
      "offset": 374.88,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "hands-on application of all of these",
      "offset": 377.039,
      "duration": 5.361
    },
    {
      "lang": "en",
      "text": "things into education space. So, excited",
      "offset": 378.639,
      "duration": 5.521
    },
    {
      "lang": "en",
      "text": "to jump into it.",
      "offset": 382.4,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "Cool. Cool. Thanks, Brian.",
      "offset": 384.16,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "With that, let's let's talk about the",
      "offset": 386.4,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "thing that we're going to talk about",
      "offset": 388.56,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "today, which is context engineering. The",
      "offset": 389.36,
      "duration": 3.279
    },
    {
      "lang": "en",
      "text": "word has been going around a lot.",
      "offset": 391.28,
      "duration": 3.039
    },
    {
      "lang": "en",
      "text": "Dexter, you gave a great talk about it",
      "offset": 392.639,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "at the AI engineer summit, but what the",
      "offset": 394.319,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "heck is context engineering? What are",
      "offset": 397.36,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "all these terms that people toss around?",
      "offset": 399.52,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "Is it a real thing? Is it more just like",
      "offset": 401.039,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "vibes? I Let's talk about it in detail",
      "offset": 402.96,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "today. And I think the thing I want to",
      "offset": 405.12,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "start with is actually the diagram that",
      "offset": 407.039,
      "duration": 2.961
    },
    {
      "lang": "en",
      "text": "Dexter has made because I think it",
      "offset": 408.479,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "really solidifies what this is.",
      "offset": 410,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "Yeah. You want to pull it up?",
      "offset": 412.72,
      "duration": 3.039
    },
    {
      "lang": "en",
      "text": "Pull it up. Go ahead and screen share.",
      "offset": 414.319,
      "duration": 2.961
    },
    {
      "lang": "en",
      "text": "I'll let you drive.",
      "offset": 415.759,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "Um, so this is a chapter from",
      "offset": 417.28,
      "duration": 2.4
    },
    {
      "lang": "en",
      "text": "one second.",
      "offset": 419.199,
      "duration": 0.961
    },
    {
      "lang": "en",
      "text": "Yeah, go ahead.",
      "offset": 419.68,
      "duration": 1.84
    },
    {
      "lang": "en",
      "text": "Uh, and just so everyone knows, the",
      "offset": 420.16,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "benefit of the Zoom call is you're",
      "offset": 421.52,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "welcome to chime in, ask questions as",
      "offset": 423.12,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "you go on. Otherwise, you can always",
      "offset": 424.8,
      "duration": 2.959
    },
    {
      "lang": "en",
      "text": "watch the recording on Fridays as it",
      "offset": 426.08,
      "duration": 2.959
    },
    {
      "lang": "en",
      "text": "goes live.",
      "offset": 427.759,
      "duration": 2.961
    },
    {
      "lang": "en",
      "text": "Um, but if you have questions, just hop",
      "offset": 429.039,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "off uh hop off mute or if it gets to be",
      "offset": 430.72,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "too wieldy, uh, just like raise your",
      "offset": 433.44,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "hand and then or type in the chat. We'll",
      "offset": 435.759,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "keep an eye on it.",
      "offset": 437.68,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "Yeah, this is not meant to be totally",
      "offset": 439.759,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "this is not meant to be totally one way.",
      "offset": 442,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "uh drop in your questions, come off",
      "offset": 443.68,
      "duration": 2.799
    },
    {
      "lang": "en",
      "text": "mute, ask tell us tell us what you think",
      "offset": 444.88,
      "duration": 2.879
    },
    {
      "lang": "en",
      "text": "is wrong, tell us what you've seen in",
      "offset": 446.479,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "the real world. Um we're all figuring",
      "offset": 447.759,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "this out together. Uh which is uh leads",
      "offset": 449.599,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "us into speaking of figuring this out",
      "offset": 452.479,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "together. So this is a chapter from",
      "offset": 454.08,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "12actor agents um which was published in",
      "offset": 455.68,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "April and one of the diagrams I think I",
      "offset": 457.84,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "think I remember actually when I vibar",
      "offset": 459.759,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "reviewed an early version of this in",
      "offset": 462.08,
      "duration": 2.959
    },
    {
      "lang": "en",
      "text": "like late March and he saw this diagram",
      "offset": 463.36,
      "duration": 3.119
    },
    {
      "lang": "en",
      "text": "said this is the best diagram in the",
      "offset": 465.039,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "entire paper. Um and it's basically this",
      "offset": 466.479,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "idea that like LMS are completely",
      "offset": 469.28,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "stateless functions. Um, and the only",
      "offset": 471.759,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "thing that affects the quality of your",
      "offset": 474.16,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "AI application is the quality of the",
      "offset": 475.599,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "tokens that are coming out of your",
      "offset": 477.68,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "model. And the only thing that affects",
      "offset": 478.96,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "the quality of the tokens that are",
      "offset": 481.039,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "coming out other than like tuning things",
      "offset": 482.319,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "like temperature and uh training your",
      "offset": 484.319,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "own model is the tokens you put in. And",
      "offset": 487.28,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "so while people will say that rag and",
      "offset": 489.599,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "memory and agentic history and prompt",
      "offset": 492.16,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "engineering are all kind of like",
      "offset": 494.479,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "different like disciplines and you need",
      "offset": 496.56,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "different tools and they're all like",
      "offset": 499.039,
      "duration": 2.961
    },
    {
      "lang": "en",
      "text": "have their own like blackboxy",
      "offset": 500.479,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "implementations. Um at the end of the",
      "offset": 502,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "day it's all just strings going into",
      "offset": 504.56,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "models and tokens in and tokens out. Um,",
      "offset": 506.319,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "there's a little bit of the structured",
      "offset": 508.639,
      "duration": 4.001
    },
    {
      "lang": "en",
      "text": "output thing, which I think uh is uh",
      "offset": 509.68,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "probably part of why Vibb really likes",
      "offset": 512.64,
      "duration": 3.519
    },
    {
      "lang": "en",
      "text": "this because structured outputs is not",
      "offset": 514.32,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "just what the model outputs, but what",
      "offset": 516.159,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "you do with it and how you parse it and",
      "offset": 518,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "how you turn it into data for a program",
      "offset": 519.599,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "to use. Um, but this really popped off.",
      "offset": 521.12,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "I think it was like June 12th that uh",
      "offset": 524.08,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "the cognition guy did a blog post. I'm",
      "offset": 527.12,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "actually going to put a I'm going to",
      "offset": 529.12,
      "duration": 2.719
    },
    {
      "lang": "en",
      "text": "post something soon of like the history",
      "offset": 530.24,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "of the term context engineering and",
      "offset": 531.839,
      "duration": 2.721
    },
    {
      "lang": "en",
      "text": "where it came from and all the different",
      "offset": 533.36,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "people who kind of like I think kind of",
      "offset": 534.56,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "came to the same conclusion around the",
      "offset": 536.24,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "same time. Um but there was a tweet from",
      "offset": 537.92,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "Toby Lutkkey in June um talking about he",
      "offset": 541.36,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "loves context engineering. Andre",
      "offset": 544,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "Karpathy jumped on the train. There's",
      "offset": 545.519,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "now a hundred blog posts about context",
      "offset": 547.12,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "engineering that you can go read about.",
      "offset": 549.6,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "Um but today we're going to go deep into",
      "offset": 551.04,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "code and we're going to talk about the",
      "offset": 552.88,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "anatomy of a prompt and the anatomy of a",
      "offset": 554.24,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "context and how you can build this stuff",
      "offset": 556.16,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "programmatically.",
      "offset": 557.839,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "Yeah. And I think um do you want to pull",
      "offset": 559.68,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "up the whiteboard really fast?",
      "offset": 562.08,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "Yeah, let's do it.",
      "offset": 564,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "And just to give everyone a summary of",
      "offset": 566.16,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "all the concepts that we'll be covering",
      "offset": 567.68,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "today uh just as a brief high level",
      "offset": 569.36,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "overview. The agenda for today is we're",
      "offset": 571.279,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "going to go share um how to build each",
      "offset": 573.279,
      "duration": 5.441
    },
    {
      "lang": "en",
      "text": "of these contexts into the prompt. We'll",
      "offset": 576.08,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "do a very high level overview of exactly",
      "offset": 578.72,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "how rag is context, how prompt and",
      "offset": 580.72,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "during is context, how state and history",
      "offset": 582.64,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "are context, and how memory is context.",
      "offset": 584.72,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "So we'll actually build out some of",
      "offset": 587.44,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "these prompts, write them by hand, talk",
      "offset": 588.64,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "about some of the analogies in there. Um",
      "offset": 590.24,
      "duration": 5.599
    },
    {
      "lang": "en",
      "text": "after that we're gonna deep dive into",
      "offset": 593.76,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "memory and focus a little bit more on",
      "offset": 595.839,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "memory because we've got this awesome",
      "offset": 598.64,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "guest Brian um on the show and he is",
      "offset": 600.56,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "going to he's generous enough to talk",
      "offset": 603.36,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "about some his blog post that both Dex",
      "offset": 606.08,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "and I were super impressed by uh that",
      "offset": 608.399,
      "duration": 5.521
    },
    {
      "lang": "en",
      "text": "really outlines some details around",
      "offset": 611.279,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "memory that I think a lot of people",
      "offset": 613.92,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "overlook. Um, and but I think that level",
      "offset": 615.279,
      "duration": 5.521
    },
    {
      "lang": "en",
      "text": "of context engineering of how you go do",
      "offset": 618.64,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "that is the key to making applications",
      "offset": 620.8,
      "duration": 6.719
    },
    {
      "lang": "en",
      "text": "feel like magic when using LMS. It's",
      "offset": 624.72,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "like the same thing like when you when",
      "offset": 627.519,
      "duration": 2.961
    },
    {
      "lang": "en",
      "text": "you first use an app and like you use a",
      "offset": 628.88,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "gesture on it, it just feels right.",
      "offset": 630.48,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "Like for those of you that don't like",
      "offset": 633.44,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "reverse scrolling on your Mac, you're",
      "offset": 634.8,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "wrong. But for the rest of us, it feels",
      "offset": 636.16,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "natural when you scrolling.",
      "offset": 638.8,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "And that's the whole point of it. um it",
      "offset": 640.959,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "just feels right when you try a UX",
      "offset": 643.76,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "pattern for the first time. And the same",
      "offset": 645.519,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "way when you use an L1 and to do that to",
      "offset": 646.88,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "make it feel right, you got to context",
      "offset": 648.959,
      "duration": 6.641
    },
    {
      "lang": "en",
      "text": "engineer, right? So with that, let's",
      "offset": 651.76,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "write some prompts and look at how all",
      "offset": 655.6,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "this fits into the actual context",
      "offset": 657.68,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "engineering piece.",
      "offset": 659.76,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "So in the whiteboard, we've got a few",
      "offset": 661.76,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "things.",
      "offset": 664.32,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "Yeah.",
      "offset": 664.64,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "The first one is then that like what is",
      "offset": 667.519,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "your prompt going to end up looking",
      "offset": 670.72,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "like? So I think the first thing that",
      "offset": 671.92,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "people write is a prompt that has a",
      "offset": 673.76,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "bunch of like a you are kind of role",
      "offset": 675.36,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "with a system instruction or something",
      "offset": 677.839,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "up here. Um I have opinions about that.",
      "offset": 679.279,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "I will refrain from sharing that right",
      "offset": 681.76,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "now. Um because I think that's",
      "offset": 683.76,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "irrelevant to the point that we're",
      "offset": 686.24,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "trying to make here. uh which is",
      "offset": 687.519,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "well it's just like prompt engineering",
      "offset": 689.76,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "is one one element of your context that",
      "offset": 691.2,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "you pass in whether it's the system",
      "offset": 695.04,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "prompt or the user message and I think",
      "offset": 696.399,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "it's kind of like we're going to stay a",
      "offset": 699.04,
      "duration": 3.039
    },
    {
      "lang": "en",
      "text": "little more high level than like advice",
      "offset": 700.399,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "on any particular one and we're going to",
      "offset": 702.079,
      "duration": 3.041
    },
    {
      "lang": "en",
      "text": "drill down memory because I think it's",
      "offset": 703.76,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "the most underserved the most hotly",
      "offset": 705.12,
      "duration": 5.279
    },
    {
      "lang": "en",
      "text": "debated and the most like valuable thing",
      "offset": 708.56,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "to get right",
      "offset": 710.399,
      "duration": 6.56
    },
    {
      "lang": "en",
      "text": "exactly so this UR concept is uh is like",
      "offset": 712.24,
      "duration": 6.96
    },
    {
      "lang": "en",
      "text": "a prompted juring trick that we're So,",
      "offset": 716.959,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "every single thing you've heard from",
      "offset": 719.2,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "like I'll tip you $10 to do things or",
      "offset": 720.56,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "you must be correct or do not",
      "offset": 724.079,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "hallucinate. All of those are prompt",
      "offset": 725.68,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "engineering tips. And whether you choose",
      "offset": 728.079,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "to put this message in a pro in a system",
      "offset": 730.32,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "message or whether you choose to put it",
      "offset": 732.56,
      "duration": 5.279
    },
    {
      "lang": "en",
      "text": "in a user message",
      "offset": 734.079,
      "duration": 7.361
    },
    {
      "lang": "en",
      "text": "um is really also prompt engineering.",
      "offset": 737.839,
      "duration": 5.921
    },
    {
      "lang": "en",
      "text": "The fact that Enthropic only allows you",
      "offset": 741.44,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "to send a system message if you send at",
      "offset": 743.76,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "least one user message. That's a prompt",
      "offset": 745.6,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "engineering constraint that they've",
      "offset": 749.04,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "applied on their system. The fact that",
      "offset": 751.12,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "images can't be passed into user",
      "offset": 753.519,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "messages or can't be passed into system",
      "offset": 755.44,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "messages for a lot of things. That's",
      "offset": 757.68,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "again another constraint applied by the",
      "offset": 759.68,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "model that you have to go do. So",
      "offset": 762.24,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "whenever people do prompt engineering, I",
      "offset": 764.399,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "think they stop at just the words that",
      "offset": 765.92,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "they're using. But I think one important",
      "offset": 767.279,
      "duration": 3.521
    },
    {
      "lang": "en",
      "text": "thing to think about when you do prompt",
      "offset": 769.44,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "engineering when you think about the",
      "offset": 770.8,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "context building around it is the kind",
      "offset": 772.24,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "of model that you're using and what",
      "offset": 774.8,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "affinities it has and what you should be",
      "offset": 776.399,
      "duration": 5.281
    },
    {
      "lang": "en",
      "text": "doing. So for example, OpenAI is clearly",
      "offset": 778.72,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "doing some work to capture prompt",
      "offset": 781.68,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "injections and stuff that disobey the",
      "offset": 783.2,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "system message and try and train the",
      "offset": 785.76,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "model for that benefit. So you might",
      "offset": 788,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "benefit towards trusting OpenAI more and",
      "offset": 791.04,
      "duration": 6.479
    },
    {
      "lang": "en",
      "text": "more and using system messages for them.",
      "offset": 794,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "On the other hand, Enthropic seems to",
      "offset": 797.519,
      "duration": 3.521
    },
    {
      "lang": "en",
      "text": "have guidance that you should just use",
      "offset": 799.76,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "the user message whenever possible",
      "offset": 801.04,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "instead of a system message. So you",
      "offset": 802.48,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "should just do that because the model",
      "offset": 804.16,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "people are they're biasing the way that",
      "offset": 805.6,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "they train the model to go do that. And",
      "offset": 807.839,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "part of context engineering isn't just",
      "offset": 809.92,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "assembling this. It's actually being",
      "offset": 811.44,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "aware of what affinities each model has",
      "offset": 812.88,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "and actually learning that along the",
      "offset": 815.279,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "way. And that that's just vocab that we",
      "offset": 817.279,
      "duration": 2.881
    },
    {
      "lang": "en",
      "text": "have to learn. It's kind of like when we",
      "offset": 818.959,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "use databases, we decide that Reddus is",
      "offset": 820.16,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "really good for a quick in-memory",
      "offset": 822.959,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "system, but you would never put your",
      "offset": 825.04,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "entire database purely on Reddus if you",
      "offset": 827.2,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "need cold storage that can never die.",
      "offset": 829.44,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "It's just the wrong database to use for",
      "offset": 832,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "that tool. And most people don't have to",
      "offset": 833.6,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "know that, but your database engineer at",
      "offset": 836.32,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "your company better know that. Similarly",
      "offset": 838.32,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "true for models. At some point, the best",
      "offset": 841.04,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "context engineers are going to know the",
      "offset": 843.76,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "nuances about the models, and that's",
      "offset": 845.279,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "just part of the job. Anything you want",
      "offset": 846.8,
      "duration": 5.279
    },
    {
      "lang": "en",
      "text": "to add on to that, Dex?",
      "offset": 849.519,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "Um, no. I think that makes a ton of",
      "offset": 852.079,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "sense. Sorry, I'm just going to pause",
      "offset": 853.92,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "for a sec while I go close a bunch of",
      "offset": 855.199,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "tabs.",
      "offset": 857.12,
      "duration": 5.48
    },
    {
      "lang": "en",
      "text": "Okay. Yeah, go for it.",
      "offset": 858.639,
      "duration": 3.961
    },
    {
      "lang": "en",
      "text": "Cool. With that, let's talk about the",
      "offset": 863.199,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "next thing. Rag.",
      "offset": 866.079,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "Cool.",
      "offset": 868.399,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "I think the way that most people do rag",
      "offset": 869.44,
      "duration": 6.8
    },
    {
      "lang": "en",
      "text": "is also very um very interesting because",
      "offset": 872.079,
      "duration": 8.161
    },
    {
      "lang": "en",
      "text": "rag is I think rag actually in my",
      "offset": 876.24,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "opinion rag lives in a hybrid both in",
      "offset": 880.24,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "and outside of the model. Uh it's a",
      "offset": 882.24,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "little bit it it kind of behaves like",
      "offset": 884.48,
      "duration": 5.599
    },
    {
      "lang": "en",
      "text": "structured outputs in that sense. Um and",
      "offset": 886.48,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "I think the reason for that actually",
      "offset": 890.079,
      "duration": 2.721
    },
    {
      "lang": "en",
      "text": "with that let's talk about structured",
      "offset": 891.6,
      "duration": 3.039
    },
    {
      "lang": "en",
      "text": "outputs first then go into rag. Um, I",
      "offset": 892.8,
      "duration": 3.279
    },
    {
      "lang": "en",
      "text": "wanted to go in the circle, but I think",
      "offset": 894.639,
      "duration": 2.961
    },
    {
      "lang": "en",
      "text": "because rag behaves slightly",
      "offset": 896.079,
      "duration": 2.401
    },
    {
      "lang": "en",
      "text": "differently, I want to talk about",
      "offset": 897.6,
      "duration": 2.56
    },
    {
      "lang": "en",
      "text": "structured outputs. It's like why is",
      "offset": 898.48,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "structured outputs in and out of the",
      "offset": 900.16,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "model? Well, we've probably all seen",
      "offset": 901.68,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "these things about like where you can",
      "offset": 904.56,
      "duration": 2.719
    },
    {
      "lang": "en",
      "text": "use like a whole bunch of different",
      "offset": 906,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "tools. You can use XML, you can use like",
      "offset": 907.279,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "we can use BAML's format, we can use",
      "offset": 910.16,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "like JSON, we can use like uh we can",
      "offset": 912,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "have it emit Python code that we then",
      "offset": 914.959,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "run into a Python data model directly.",
      "offset": 917.04,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "There's so many different ways of doing",
      "offset": 919.68,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "structured outputs and All of these",
      "offset": 921.279,
      "duration": 6.641
    },
    {
      "lang": "en",
      "text": "methods involve two parts. They involve",
      "offset": 924.88,
      "duration": 6.639
    },
    {
      "lang": "en",
      "text": "a method that involves both. Actually, I",
      "offset": 927.92,
      "duration": 6.32
    },
    {
      "lang": "en",
      "text": "have a great diagram for this.",
      "offset": 931.519,
      "duration": 3.521
    },
    {
      "lang": "en",
      "text": "Drop it in.",
      "offset": 934.24,
      "duration": 2.64
    },
    {
      "lang": "en",
      "text": "Let me Yeah, that's what I'm meant to",
      "offset": 935.04,
      "duration": 2.159
    },
    {
      "lang": "en",
      "text": "do.",
      "offset": 936.88,
      "duration": 1.36
    },
    {
      "lang": "en",
      "text": "Are you able to just paste the image",
      "offset": 937.199,
      "duration": 2.241
    },
    {
      "lang": "en",
      "text": "into the whiteboard?",
      "offset": 938.24,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "I can I can",
      "offset": 939.44,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "cool.",
      "offset": 942.8,
      "duration": 2.159
    },
    {
      "lang": "en",
      "text": "Um,",
      "offset": 943.36,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "I think when it comes to structured",
      "offset": 944.959,
      "duration": 2.721
    },
    {
      "lang": "en",
      "text": "outputs, there's like these three",
      "offset": 946.32,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "diagrams that a lot of people",
      "offset": 947.68,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "um Let's see. Oh, I cannot I I can",
      "offset": 950.24,
      "duration": 6.519
    },
    {
      "lang": "en",
      "text": "screenshot and then paste the image in.",
      "offset": 952.399,
      "duration": 4.36
    },
    {
      "lang": "en",
      "text": "When it comes to structured outputs,",
      "offset": 957.36,
      "duration": 2.399
    },
    {
      "lang": "en",
      "text": "there's like three parts that we",
      "offset": 958.639,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "basically have in the model. We have the",
      "offset": 959.759,
      "duration": 5.121
    },
    {
      "lang": "en",
      "text": "prompt that we actually have. We have",
      "offset": 963.12,
      "duration": 3.519
    },
    {
      "lang": "en",
      "text": "the actual model itself and then we have",
      "offset": 964.88,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "the code that we run outside of it. The",
      "offset": 966.639,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "context",
      "offset": 969.68,
      "duration": 2.56
    },
    {
      "lang": "en",
      "text": "so JSON schema.",
      "offset": 970.24,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "So JSON schema and this is part of it is",
      "offset": 972.24,
      "duration": 3.039
    },
    {
      "lang": "en",
      "text": "like when you when you use the OpenAI",
      "offset": 973.839,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "SDK and you put tools in like if you",
      "offset": 975.279,
      "duration": 5.041
    },
    {
      "lang": "en",
      "text": "read their docs essentially or any any",
      "offset": 978.639,
      "duration": 2.961
    },
    {
      "lang": "en",
      "text": "of these model SDKs, right? When you",
      "offset": 980.32,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "when you pass tools into a method or",
      "offset": 981.6,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "push them into an API endpoint, you're",
      "offset": 984,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "passing JSON schemas that then get",
      "offset": 985.519,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "encoded and injected into the system",
      "offset": 987.759,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "message in a way that like the model has",
      "offset": 989.519,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "been trained on basically.",
      "offset": 991.199,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "Yeah. And I'm going to I'm going to put",
      "offset": 993.839,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "like a couple different tools in here",
      "offset": 995.12,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "just so we have the screenshots",
      "offset": 996.639,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "um as I go talk really fast. and go",
      "offset": 999.519,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "capture these.",
      "offset": 1002.079,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "Yeah. No, these are great.",
      "offset": 1002.88,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "Um, and just so people know how context",
      "offset": 1005.92,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "engineering works with structured",
      "offset": 1009.36,
      "duration": 2.64
    },
    {
      "lang": "en",
      "text": "outputs because it is very nuanced and",
      "offset": 1010.399,
      "duration": 3.041
    },
    {
      "lang": "en",
      "text": "it's not to say that there's one right",
      "offset": 1012,
      "duration": 2.639
    },
    {
      "lang": "en",
      "text": "answer. I think that's the whole point",
      "offset": 1013.44,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "of these conversations. There is no",
      "offset": 1014.639,
      "duration": 5.041
    },
    {
      "lang": "en",
      "text": "right answer. Um, all of it is really",
      "offset": 1016.639,
      "duration": 5.841
    },
    {
      "lang": "en",
      "text": "really based around",
      "offset": 1019.68,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "uh just understanding the tools that you",
      "offset": 1022.48,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "have at your leisure to then go ahead",
      "offset": 1024.64,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "and go make these things actually better",
      "offset": 1027.28,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "along the way. Okay. So I think I have",
      "offset": 1030.48,
      "duration": 1.839
    },
    {
      "lang": "en",
      "text": "all the",
      "offset": 1031.919,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "this one is constrain generation, right?",
      "offset": 1032.319,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "Uh",
      "offset": 1035.28,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "yes, exactly.",
      "offset": 1037.199,
      "duration": 2.64
    },
    {
      "lang": "en",
      "text": "Yeah. Where it's like the output is",
      "offset": 1038.24,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "actually like the sampler gets struck",
      "offset": 1039.839,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "out for anything that's not valid JSON.",
      "offset": 1042,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "Exactly. So like how is structured",
      "offset": 1044.559,
      "duration": 5.521
    },
    {
      "lang": "en",
      "text": "output context engineering? Well, the",
      "offset": 1046.4,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "point is I think in the very beginning",
      "offset": 1050.08,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "at the thing at the very top if you want",
      "offset": 1052.08,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "to scroll up.",
      "offset": 1053.919,
      "duration": 1.441
    },
    {
      "lang": "en",
      "text": "Yeah.",
      "offset": 1055.12,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "Yeah. There you go. This is the most",
      "offset": 1055.36,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "trivial way to do structure generation.",
      "offset": 1058.16,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "You write the prompt, the model does its",
      "offset": 1060.08,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "thing and you run JSON.parse.",
      "offset": 1061.76,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "There's different tuning things that we",
      "offset": 1064,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "can do. So for example, we can do",
      "offset": 1065.6,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "context engineering by using some prompt",
      "offset": 1067.12,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "engineering to say your output should do",
      "offset": 1069.28,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "exactly that. The model does whatever.",
      "offset": 1072.16,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "Then we run JSON.parse.",
      "offset": 1074,
      "duration": 3.039
    },
    {
      "lang": "en",
      "text": "We can do better.",
      "offset": 1076.16,
      "duration": 4.36
    },
    {
      "lang": "en",
      "text": "Jason, you got a question.",
      "offset": 1077.039,
      "duration": 3.481
    },
    {
      "lang": "en",
      "text": "Yeah, I didn't want to interrupt by Bob",
      "offset": 1081.2,
      "duration": 2.32
    },
    {
      "lang": "en",
      "text": "there, but",
      "offset": 1082.799,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "go for it. Would you consider like MCP a",
      "offset": 1083.52,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "structured output like uh the the schema",
      "offset": 1087.52,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "that goes along with an MCP server and",
      "offset": 1090,
      "duration": 3.039
    },
    {
      "lang": "en",
      "text": "then the tools that go with it? Would",
      "offset": 1091.84,
      "duration": 2.48
    },
    {
      "lang": "en",
      "text": "you consider that like a structured",
      "offset": 1093.039,
      "duration": 2.961
    },
    {
      "lang": "en",
      "text": "output of sorts for the for the model",
      "offset": 1094.32,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "that then gives back a different output",
      "offset": 1096,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "of its own?",
      "offset": 1097.84,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "Um I think MCP falls into it's like a",
      "offset": 1099.28,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "higher level concept than just like",
      "offset": 1103.44,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "purely context engineering. And if",
      "offset": 1105.52,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "you're using an MCP server as an input",
      "offset": 1108.24,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "into a prompt, then yes, you're",
      "offset": 1110.32,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "basically doing structured outputs with",
      "offset": 1112.24,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "an MCP server.",
      "offset": 1113.52,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "That's all you're doing.",
      "offset": 1114.96,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "Go ahead.",
      "offset": 1117.52,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "Yeah, the canonical way to use MCP is to",
      "offset": 1118.64,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "use the list tools endpoint, get back",
      "offset": 1120.64,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "the JSON schema in a format that then",
      "offset": 1123.12,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "you can just send to the uh model",
      "offset": 1125.039,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "endpoint in the same way that like any",
      "offset": 1127.52,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "normal nonMCP client would request a",
      "offset": 1129.84,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "tool call and tell it about the schemas.",
      "offset": 1131.919,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "For what it's worth, it might work. It",
      "offset": 1134.08,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "might work a little differently with",
      "offset": 1136.559,
      "duration": 3.521
    },
    {
      "lang": "en",
      "text": "anthropics models because it's like the",
      "offset": 1137.84,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "protocol's opinionated about anthropics",
      "offset": 1140.08,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "models, but that's how everything else",
      "offset": 1141.919,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "implements it like Gemini or whatever",
      "offset": 1143.36,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "and cursor is it's yeah it's just",
      "offset": 1145.76,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "calling list tools and then passing them",
      "offset": 1147.6,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "in as like normal nonMCP tools like the",
      "offset": 1150,
      "duration": 5.919
    },
    {
      "lang": "en",
      "text": "model doesn't know that it's that it's",
      "offset": 1154.08,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "MCP only anthropics like models are",
      "offset": 1155.919,
      "duration": 5.601
    },
    {
      "lang": "en",
      "text": "aware of MCP as a thing which exists",
      "offset": 1158.4,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "separately from normal tools and prompt",
      "offset": 1161.52,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "context. Oh, they're actually aware of",
      "offset": 1164.24,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "that in the prompt API.",
      "offset": 1166.16,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "Yeah. So, this is like why um anthropics",
      "offset": 1168.08,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "models support other primitives like",
      "offset": 1170.72,
      "duration": 4.959
    },
    {
      "lang": "en",
      "text": "resources for example. um is",
      "offset": 1172.64,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "because and like why cursor and one surf",
      "offset": 1175.679,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "don't support resources is because it's",
      "offset": 1178.16,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "a separate MCP like",
      "offset": 1179.76,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "primitive or API or whatever you want to",
      "offset": 1181.84,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "call it that",
      "offset": 1183.919,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "yeah it's different from tools but it's",
      "offset": 1186.72,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "difficult to model as tools and so most",
      "offset": 1188.64,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "and so it's not portable to other models",
      "offset": 1191.84,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "because they don't have a predefined way",
      "offset": 1193.84,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "to understand it in their system prompt",
      "offset": 1195.2,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "and so you would kind of have to educate",
      "offset": 1197.039,
      "duration": 3.041
    },
    {
      "lang": "en",
      "text": "the model about what they are and what",
      "offset": 1198.559,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "they do or kind of like wrap them as",
      "offset": 1200.08,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "tools and So it's not portable. So",
      "offset": 1203.12,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "people just generally use tools.",
      "offset": 1205.2,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "Okay, cool. Thanks, Kyle.",
      "offset": 1207.52,
      "duration": 2
    },
    {
      "lang": "en",
      "text": "Cool.",
      "offset": 1209.12,
      "duration": 2.48
    },
    {
      "lang": "en",
      "text": "Um, all right. So go to go back into",
      "offset": 1209.52,
      "duration": 3.279
    },
    {
      "lang": "en",
      "text": "that. The next thing that we could do is",
      "offset": 1211.6,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "we could do better prompt engineering",
      "offset": 1212.799,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "where we say say I don't know if it",
      "offset": 1214.48,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "doesn't know the answer and then we'll",
      "offset": 1217.28,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "literally say if I don't know is in the",
      "offset": 1218.72,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "output, return something else otherwise",
      "offset": 1220.32,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "JSON.parse. And you can clearly see how",
      "offset": 1222.24,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "we're doing context engineering on the",
      "offset": 1224.64,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "left side and doing something else on",
      "offset": 1226.08,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "the right side. And that's the point of",
      "offset": 1228.72,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "this. This is not all context",
      "offset": 1230.48,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "engineering.",
      "offset": 1232.159,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "The other thing you can do is you can do",
      "offset": 1234.159,
      "duration": 2.161
    },
    {
      "lang": "en",
      "text": "constraints.",
      "offset": 1235.84,
      "duration": 1.76
    },
    {
      "lang": "en",
      "text": "So you're saying this stuff on the left",
      "offset": 1236.32,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "is like the tokens you send into the",
      "offset": 1237.6,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "model is context engineering but there's",
      "offset": 1239.2,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "a little bit part of this where like",
      "offset": 1240.88,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "your parsing afterwards needs to kind of",
      "offset": 1243.6,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "know about what you put into the context",
      "offset": 1246,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "and so it's kind of bridges those two",
      "offset": 1248.08,
      "duration": 2.56
    },
    {
      "lang": "en",
      "text": "sides.",
      "offset": 1249.679,
      "duration": 2.48
    },
    {
      "lang": "en",
      "text": "Exactly. You're kind of touching both",
      "offset": 1250.64,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "ends. The next part of the approach is",
      "offset": 1252.159,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "something like this where you actually",
      "offset": 1254.72,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "modify the model. you send you you do",
      "offset": 1255.679,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "send some tokens in like the JSON scheme",
      "offset": 1257.84,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "or something else and then the model",
      "offset": 1260.159,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "basically limits you to only follow JSON",
      "offset": 1261.76,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "grammar specifically.",
      "offset": 1263.919,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "So we're not we are doing context",
      "offset": 1265.84,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "engineering in the fact that we're",
      "offset": 1267.919,
      "duration": 3.041
    },
    {
      "lang": "en",
      "text": "giving it JSON schema or some spec. But",
      "offset": 1268.88,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "then instead of our parser having",
      "offset": 1270.96,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "knowledge the model has knowledge of",
      "offset": 1272.559,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "what to go do and this has trade-offs",
      "offset": 1274.48,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "that we've talked about many times.",
      "offset": 1276.48,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "Basically constraint generation probably",
      "offset": 1277.679,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "hurts that output quality but it is a",
      "offset": 1279.6,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "technique that you can leverage.",
      "offset": 1281.84,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "The next thing that",
      "offset": 1284.48,
      "duration": 2.24
    },
    {
      "lang": "en",
      "text": "yeah we'll link to the other episode",
      "offset": 1285.44,
      "duration": 2.64
    },
    {
      "lang": "en",
      "text": "where we talked about that a little bit",
      "offset": 1286.72,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "more in depth especially for codegen.",
      "offset": 1288.08,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "Yeah the other episode the other thing",
      "offset": 1290.96,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "you can do over here is you can use tool",
      "offset": 1292.88,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "calling. Tool calling is very similar.",
      "offset": 1294.4,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "You do you do a special kind of context",
      "offset": 1296,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "engineering where you send in the tools",
      "offset": 1298.08,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "in a special field into your API call as",
      "offset": 1299.84,
      "duration": 6.4
    },
    {
      "lang": "en",
      "text": "special tokens into the model. And now",
      "offset": 1303.12,
      "duration": 7.039
    },
    {
      "lang": "en",
      "text": "the model limits what tools you return",
      "offset": 1306.24,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "using the JSON struct using the JSON",
      "offset": 1310.159,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "approach that they have over there as",
      "offset": 1312.24,
      "duration": 2.4
    },
    {
      "lang": "en",
      "text": "well. the same approach as this",
      "offset": 1313.52,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "constraint generation and then you also",
      "offset": 1314.64,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "have to change your parser to read",
      "offset": 1318.24,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "output.tools instead of just the raw",
      "offset": 1319.679,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "string from now on. So now we have we've",
      "offset": 1321.76,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "modified both the context and the model",
      "offset": 1324.32,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "and also the parser.",
      "offset": 1327.2,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "And then you have the last technique",
      "offset": 1329.44,
      "duration": 3.119
    },
    {
      "lang": "en",
      "text": "which is one that we do which is you do",
      "offset": 1330.88,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "context engineering to inject the data",
      "offset": 1332.559,
      "duration": 3.521
    },
    {
      "lang": "en",
      "text": "type into the prompt but you don't",
      "offset": 1334.4,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "change the model and you basically just",
      "offset": 1336.08,
      "duration": 4.959
    },
    {
      "lang": "en",
      "text": "run a special parser that is able to go",
      "offset": 1338.799,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "ahead and take again take knowledge of",
      "offset": 1341.039,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "what you put into the prompt and parse",
      "offset": 1343.039,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "it out. XML is the same way. You have to",
      "offset": 1344.64,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "know that you gave the prompt XML format",
      "offset": 1347.6,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "to then run XML.parse instead. It",
      "offset": 1349.76,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "doesn't just magically work. And that's",
      "offset": 1352.24,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "exactly why the thing at the top that",
      "offset": 1354.159,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "Dexter is showing you, you go to the",
      "offset": 1356,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "top. That's why structured outputs lives",
      "offset": 1358,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "on this bridge. Your context has to know",
      "offset": 1360.24,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "about structured outputs, but so do many",
      "offset": 1362.799,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "other things, including possibly the",
      "offset": 1364.64,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "model and the supporting code around it.",
      "offset": 1366.64,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "Um, I saw that there's a question in the",
      "offset": 1369.76,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "middle. Sorry to have uh gone on for a",
      "offset": 1371.2,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "bit. Do you want to hop on and ask your",
      "offset": 1374.08,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "question?",
      "offset": 1375.679,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "Yeah, come on. Yes. So I I was just",
      "offset": 1377.12,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "thinking of context as different",
      "offset": 1380,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "sections or different types. One could",
      "offset": 1382.88,
      "duration": 6.32
    },
    {
      "lang": "en",
      "text": "be knowledge again or you have memory",
      "offset": 1386.08,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "and etc. that ven diagram actually",
      "offset": 1389.2,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "explains it better. So when you talk",
      "offset": 1391.2,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "about context I think giving some nuance",
      "offset": 1392.88,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "or basically where we uh give tags or",
      "offset": 1395.28,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "different identifiers for the context is",
      "offset": 1397.919,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "helping it break down. So that also sort",
      "offset": 1400.64,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "of helps us map out what the context is.",
      "offset": 1402.72,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "Right? For example, you might give",
      "offset": 1404.88,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "knowledge, you might give instructions.",
      "offset": 1406,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "Uh for example, even a structure for its",
      "offset": 1408.24,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "output or input is also again an",
      "offset": 1411.28,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "instruction for the LLM to follow. Do",
      "offset": 1413.44,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "you agree?",
      "offset": 1415.52,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "Kind of. I think the fact that you're",
      "offset": 1417.36,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "labeling a section as some section is",
      "offset": 1418.96,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "just a prompt engineering technique. So",
      "offset": 1421.52,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "like if you're labeling this as like if",
      "offset": 1423.76,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "you add an instruction here that says",
      "offset": 1425.76,
      "duration": 5.279
    },
    {
      "lang": "en",
      "text": "like structure uh is like schema and you",
      "offset": 1427.12,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "like let's say you did some tags like",
      "offset": 1431.039,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "this",
      "offset": 1432.64,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "where you you put some tags like this in",
      "offset": 1434.159,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "your prompt.",
      "offset": 1436,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "That's a prompt engineering trick you're",
      "offset": 1437.76,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "employing to go do this. The thing that",
      "offset": 1440,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "is context is the full prompt that you",
      "offset": 1442.159,
      "duration": 6.081
    },
    {
      "lang": "en",
      "text": "send in. The full prompt as is the",
      "offset": 1444.799,
      "duration": 6.161
    },
    {
      "lang": "en",
      "text": "actual API request you make fully is",
      "offset": 1448.24,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "your context.",
      "offset": 1450.96,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "The sections of it are an abstract",
      "offset": 1453.36,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "concept that we are making up right now.",
      "offset": 1455.36,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "But really the only context that matters",
      "offset": 1457.76,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "is the tokens in and the tokens out",
      "offset": 1459.6,
      "duration": 2.64
    },
    {
      "lang": "en",
      "text": "even.",
      "offset": 1461.76,
      "duration": 2
    },
    {
      "lang": "en",
      "text": "And you could put this stuff in any",
      "offset": 1462.24,
      "duration": 2.799
    },
    {
      "lang": "en",
      "text": "order. You could put your structured",
      "offset": 1463.76,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "output instructions in the system",
      "offset": 1465.039,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "message or the user message. They're",
      "offset": 1466.88,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "kind of orthogonal concepts.",
      "offset": 1468.64,
      "duration": 3.519
    },
    {
      "lang": "en",
      "text": "You could put it at the beginning, you",
      "offset": 1470.72,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "could put at the end. Like all of these",
      "offset": 1472.159,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "are totally different. You could scatter",
      "offset": 1474.08,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "it four times in your prompt, repeating",
      "offset": 1475.52,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "it different times. You could put it",
      "offset": 1477.52,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "without these tags, without these tags.",
      "offset": 1479.279,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "or or with these tags it doesn't really",
      "offset": 1482.159,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "matter but you have to try different",
      "offset": 1484.159,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "things to get the right model to",
      "offset": 1486.559,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "understand.",
      "offset": 1488.24,
      "duration": 5.559
    },
    {
      "lang": "en",
      "text": "Does that answer your question BJ?",
      "offset": 1489.76,
      "duration": 4.039
    },
    {
      "lang": "en",
      "text": "It does. Thank you.",
      "offset": 1494.24,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "And so we split this up into",
      "offset": 1497.039,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "Yeah, sorry. Go ahead.",
      "offset": 1499.2,
      "duration": 2.64
    },
    {
      "lang": "en",
      "text": "I was going to say we split this up into",
      "offset": 1500.48,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "kind of parsing and instructions",
      "offset": 1501.84,
      "duration": 2.959
    },
    {
      "lang": "en",
      "text": "basically. That's how kind of would",
      "offset": 1503.36,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "clarify this. And it's kind of like when",
      "offset": 1504.799,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "you're writing an essay, like there's a",
      "offset": 1507.6,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "general guiding for writing a good",
      "offset": 1509.52,
      "duration": 2.56
    },
    {
      "lang": "en",
      "text": "essay. You have an introduction",
      "offset": 1511.2,
      "duration": 2
    },
    {
      "lang": "en",
      "text": "paragraph, you have three body",
      "offset": 1512.08,
      "duration": 2.479
    },
    {
      "lang": "en",
      "text": "paragraphs, and you have a conclusion",
      "offset": 1513.2,
      "duration": 2.64
    },
    {
      "lang": "en",
      "text": "paragraph. That's a pretty good",
      "offset": 1514.559,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "structure in general, but not all essays",
      "offset": 1515.84,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "have to follow that format. It's the",
      "offset": 1518.24,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "same with prompts. There's probably good",
      "offset": 1520.4,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "guidelines for prompts, and you should",
      "offset": 1522.64,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "probably follow them, especially if the",
      "offset": 1524.08,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "model providers are giving you some",
      "offset": 1525.44,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "suggestions.",
      "offset": 1527.279,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "But they're not the end all beall. And",
      "offset": 1529.2,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "you have to remember that everyone is",
      "offset": 1531.279,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "discovering the capabilities of these",
      "offset": 1532.88,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "models at roughly the same time. Even",
      "offset": 1534.48,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "the model providers are also discovering",
      "offset": 1536.32,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "them at roughly the same time. Like all",
      "offset": 1538.48,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "this emergent behavior is new to them as",
      "offset": 1540.559,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "well. So whenever a company whenever",
      "offset": 1542.64,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "someone comes out and says this is the",
      "offset": 1545.279,
      "duration": 2.481
    },
    {
      "lang": "en",
      "text": "best prompting technique and they happen",
      "offset": 1546.4,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "to work at openthropic,",
      "offset": 1547.76,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "they just discovered that for the first",
      "offset": 1549.6,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "time. No one has all the context right",
      "offset": 1550.88,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "now because the space is moving so fast.",
      "offset": 1552.48,
      "duration": 2.48
    },
    {
      "lang": "en",
      "text": "So just remember",
      "offset": 1554.32,
      "duration": 2.56
    },
    {
      "lang": "en",
      "text": "like no one designed the models to be",
      "offset": 1554.96,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "good at XML or be good at parsing XML.",
      "offset": 1556.88,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "people just over time we're trying lots",
      "offset": 1559.44,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "of things and realize oh if you put XML",
      "offset": 1561.2,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "into the model it's really good at",
      "offset": 1563.039,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "focusing the types of attention on the",
      "offset": 1565.039,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "certain stanzas that you want",
      "offset": 1566.96,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "and the reason by the way that people",
      "offset": 1568.88,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "seem to perceive XML is better I don't",
      "offset": 1570.48,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "actually think it is is because the",
      "offset": 1572.48,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "parsing of XML is much more tolerant to",
      "offset": 1574.72,
      "duration": 6.8
    },
    {
      "lang": "en",
      "text": "escape tokens than the parsing of JSON",
      "offset": 1577.36,
      "duration": 7.36
    },
    {
      "lang": "en",
      "text": "so YAML is whitespace specific JSON",
      "offset": 1581.52,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "doesn't even allow most types of whites",
      "offset": 1584.72,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "space XML is a lot more forgiving and so",
      "offset": 1586.32,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "like if you have the model writing code",
      "offset": 1588.64,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "and the code has new lines, it's very",
      "offset": 1590.08,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "hard for the it's much harder for the",
      "offset": 1592.32,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "model to write slashn instead of an",
      "offset": 1593.76,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "actual new line character because most",
      "offset": 1595.84,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "of the code in the training set is",
      "offset": 1598.4,
      "duration": 3.279
    },
    {
      "lang": "en",
      "text": "written with real new lines, not with",
      "offset": 1599.919,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "the slash and the n.",
      "offset": 1601.679,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "Exactly. So when that happens, what ends",
      "offset": 1604.48,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "up happening is you're just biasing",
      "offset": 1606.32,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "towards certain directions. Anyway, we",
      "offset": 1607.679,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "got we're Yeah, we're almost 20 minutes",
      "offset": 1610.24,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "in. We got to we got to hit the other",
      "offset": 1612.88,
      "duration": 4.84
    },
    {
      "lang": "en",
      "text": "categories quickly.",
      "offset": 1614.159,
      "duration": 3.561
    },
    {
      "lang": "en",
      "text": "Yes. What's up, John?",
      "offset": 1617.76,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "Sorry, I couldn't figure out the how to",
      "offset": 1620,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "raise a hand, so I'm just dropping in.",
      "offset": 1621.52,
      "duration": 5.759
    },
    {
      "lang": "en",
      "text": "Um, so I love this diagram, but one of",
      "offset": 1624.32,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "the thing I'm kind of struggling with is",
      "offset": 1627.279,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "that the right now it's kind of like a",
      "offset": 1629.36,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "mixing up both um behavioral instruction",
      "offset": 1630.88,
      "duration": 6.56
    },
    {
      "lang": "en",
      "text": "like a how to with the factual",
      "offset": 1634,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "information like either rag or memory",
      "offset": 1637.44,
      "duration": 5.839
    },
    {
      "lang": "en",
      "text": "we're going to talk about. Is it is it",
      "offset": 1639.52,
      "duration": 6.159
    },
    {
      "lang": "en",
      "text": "helpful to like even split out that",
      "offset": 1643.279,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "Yeah. Yeah. there like a instructional",
      "offset": 1645.679,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "versus factual like a context.",
      "offset": 1647.36,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "Depends on the model. It honestly",
      "offset": 1650.159,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "depends on the model. Some models",
      "offset": 1652.159,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "benefit from having it in different",
      "offset": 1653.76,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "places. Um",
      "offset": 1655.279,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "I will we'll show some prompts in a",
      "offset": 1657.919,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "second that show how to go write this,",
      "offset": 1660.08,
      "duration": 4.959
    },
    {
      "lang": "en",
      "text": "but it really really really varies. Um I",
      "offset": 1662.08,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "wish I think about machine learning that",
      "offset": 1665.039,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "a lot of people I think have a hard",
      "offset": 1667.52,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "thing is like we all in software",
      "offset": 1668.96,
      "duration": 2.959
    },
    {
      "lang": "en",
      "text": "engineering we have llinters that make",
      "offset": 1670.48,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "us say like this is the right way to",
      "offset": 1671.919,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "name our variables. this is the right",
      "offset": 1673.76,
      "duration": 3.279
    },
    {
      "lang": "en",
      "text": "way to structure our code and there's",
      "offset": 1675.6,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "often a right answer for so many things.",
      "offset": 1677.039,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "But",
      "offset": 1680.64,
      "duration": 3.519
    },
    {
      "lang": "en",
      "text": "this whole context engineering piece is",
      "offset": 1682.24,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "much more like a system design interview",
      "offset": 1684.159,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "question where it's all about trade-offs",
      "offset": 1686,
      "duration": 2.64
    },
    {
      "lang": "en",
      "text": "in different dimensions and just",
      "offset": 1687.279,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "understanding the trade-offs",
      "offset": 1688.64,
      "duration": 2.56
    },
    {
      "lang": "en",
      "text": "and that's how you",
      "offset": 1690.399,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "and like observability as well. I'll use",
      "offset": 1691.2,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "that more generally obviously in in LM",
      "offset": 1694.08,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "world it's more like eval but like the",
      "offset": 1695.919,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "idea of like how will you know when you",
      "offset": 1697.52,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "try I I work with a lot of teams and",
      "offset": 1699.919,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "they end up like just trying every",
      "offset": 1701.279,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "technique and every paper without",
      "offset": 1703.039,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "knowing how they will know what's better",
      "offset": 1704.64,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "and what's not and like I don't know",
      "offset": 1706.159,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "what's better and every problem domain",
      "offset": 1708.32,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "and every model is different and so when",
      "offset": 1710.88,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "you think of this as all kind of like",
      "offset": 1712.96,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "overlapping parts of this challenge of",
      "offset": 1714.88,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "how do I get the right tokens into the",
      "offset": 1717.039,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "LLM so I get the best quality answer",
      "offset": 1718.48,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "it's it's a mindset shift and This",
      "offset": 1721.2,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "encourages you to kind of like open your",
      "offset": 1723.52,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "awareness and your your approaches. I",
      "offset": 1725.919,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "don't know what's best, but I know the",
      "offset": 1728.559,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "more things you can try and the more",
      "offset": 1730.32,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "flexible you are with how you mix this",
      "offset": 1732.24,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "and interle this information together,",
      "offset": 1733.84,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "the more likely you are to stumble on to",
      "offset": 1736.24,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "and discover some emergent property of",
      "offset": 1738.24,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "like, oh, when we do it this way, it's",
      "offset": 1740.08,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "actually like 20% better. And that makes",
      "offset": 1742.24,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "our agent go from like kind of just a",
      "offset": 1744.399,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "toy to like, oh, this actually solves",
      "offset": 1747.039,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "real problems reliably.",
      "offset": 1748.96,
      "duration": 2.56
    },
    {
      "lang": "en",
      "text": "Exactly.",
      "offset": 1751.039,
      "duration": 2.161
    },
    {
      "lang": "en",
      "text": "Yeah. That's kind of like John's comment",
      "offset": 1751.52,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "right now is that how to measure best is",
      "offset": 1753.2,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "like is it more factual? Is there more",
      "offset": 1754.88,
      "duration": 5.679
    },
    {
      "lang": "en",
      "text": "like a usable for next like software to",
      "offset": 1757.12,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "use. So anyway,",
      "offset": 1760.559,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "let's keep going. I'll probably follow",
      "offset": 1762.32,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "up with you guys.",
      "offset": 1764,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "Yeah, we did a chat on eval like a",
      "offset": 1765.36,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "couple episodes back. If you go on the",
      "offset": 1767.52,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "YouTube channel, you'll find it. Um",
      "offset": 1768.72,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "let's talk about rag and I'll talk about",
      "offset": 1770.32,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "why rag is very similar to structured",
      "offset": 1771.76,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "outputs in this sense and why we should",
      "offset": 1774,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "update this diagram accordingly. Um,",
      "offset": 1775.44,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "well, the way rag works is also very",
      "offset": 1778.32,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "similar. You have a bunch of context",
      "offset": 1780.64,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "that lives in some database somewhere,",
      "offset": 1783.039,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "and we're going to want to pull parts of",
      "offset": 1785.279,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "this. Oops, sorry. Pulled the wrong",
      "offset": 1787.279,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "diagram. I'm not as fast at Excel Draw",
      "offset": 1789.2,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "as Dexter is. Um, he's a beast at this",
      "offset": 1791.679,
      "duration": 6.161
    },
    {
      "lang": "en",
      "text": "stuff. Uh, we have a bunch of",
      "offset": 1795.2,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "I just love I played too many RTS games.",
      "offset": 1797.84,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "I I'm obsessed with hotkeys.",
      "offset": 1799.919,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "I can tell. Um, and what we will want to",
      "offset": 1802.399,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "do is we'll want to pull some of these",
      "offset": 1804.88,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "contexts and put them into our prompt.",
      "offset": 1806.48,
      "duration": 5.439
    },
    {
      "lang": "en",
      "text": "That is naive rag. That is the most",
      "offset": 1809.44,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "naive form of rag that you can do. It's",
      "offset": 1811.919,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "just like in structured outputs. If you",
      "offset": 1813.76,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "scroll down, Dexter,",
      "offset": 1815.36,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "the most naive form of structured",
      "offset": 1818.32,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "outputs that we can do is literally dump",
      "offset": 1820.399,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "the uh is literally dump the JSON schema",
      "offset": 1822.32,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "into the problem and just JSON pars.",
      "offset": 1825.12,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "You're right.",
      "offset": 1827.36,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "And hope it didn't put markdown back",
      "offset": 1827.84,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "ticks around the JSON. Hope. Pray.",
      "offset": 1830.24,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "Right. Exactly.",
      "offset": 1832.48,
      "duration": 2.4
    },
    {
      "lang": "en",
      "text": "That it didn't.",
      "offset": 1833.2,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "Yeah.",
      "offset": 1834.88,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "So, but on the other hand, what we",
      "offset": 1836.24,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "really want to do with rag is rag is a",
      "offset": 1838.24,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "problem. And the way that I think about",
      "offset": 1840.88,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "the problem is it's basically a problem",
      "offset": 1842.88,
      "duration": 6.039
    },
    {
      "lang": "en",
      "text": "that looks at this function signature.",
      "offset": 1844.64,
      "duration": 4.279
    },
    {
      "lang": "en",
      "text": "It takes in some query of some type T",
      "offset": 1849.039,
      "duration": 6.321
    },
    {
      "lang": "en",
      "text": "and returns a ve of another of a data",
      "offset": 1851.84,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "type. That's all a rag function is.",
      "offset": 1855.36,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "Whether I use a vector database or",
      "offset": 1857.84,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "anything else for that doesn't really",
      "offset": 1859.52,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "matter. It's really up to me as an",
      "offset": 1861.12,
      "duration": 5.439
    },
    {
      "lang": "en",
      "text": "engineer to decide",
      "offset": 1864.88,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "retrieve retrieval. This is just the",
      "offset": 1866.559,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "retrieval part, right? Like I would say",
      "offset": 1869.12,
      "duration": 2.559
    },
    {
      "lang": "en",
      "text": "like",
      "offset": 1870.88,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "this is the R, this is the a",
      "offset": 1871.679,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "augmentation augmenting your context",
      "offset": 1874,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "with more info and then the generation",
      "offset": 1876,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "happens when you send it all to the",
      "offset": 1877.6,
      "duration": 2.319
    },
    {
      "lang": "en",
      "text": "model.",
      "offset": 1878.88,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "Exactly. And a lot of people think rag",
      "offset": 1879.919,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "is about the model. Rag is not about the",
      "offset": 1882.32,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "model. Rag is purely about retrieving",
      "offset": 1884.08,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "the right data and then the way you",
      "offset": 1886.399,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "augment it into the prompt is the way",
      "offset": 1888.24,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "that you're rendering into the prompt.",
      "offset": 1891.12,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "So you could dump it in directly. You",
      "offset": 1893.279,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "could dump it in reformatting it in a",
      "offset": 1895.52,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "way that's different for the model. So",
      "offset": 1897.919,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "we talked about like rendering XML",
      "offset": 1899.279,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "schemas instead of JSON schema or",
      "offset": 1901.12,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "rendering BAML schema instead of JSON",
      "offset": 1902.799,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "schema or wrapping with these schema",
      "offset": 1904.96,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "tags around your schema part of it. It's",
      "offset": 1907.679,
      "duration": 5.281
    },
    {
      "lang": "en",
      "text": "the same thing. The way you render is up",
      "offset": 1910.96,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "to you. There's a thousand ways to",
      "offset": 1912.96,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "render the same context in your prompt",
      "offset": 1914.48,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "and you have to decide which way that",
      "offset": 1917.44,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "is. And that's why rag sits on the",
      "offset": 1919.039,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "border of context interning because you",
      "offset": 1921.84,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "have to choose how you augment the model",
      "offset": 1923.6,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "and you have to choose how you get the",
      "offset": 1925.519,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "context. And the parameters that you",
      "offset": 1927.12,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "choose to get the context may not even",
      "offset": 1929.6,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "be ones that are fed directly into the",
      "offset": 1931.2,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "model. It may be a totally different",
      "offset": 1932.799,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "query than the direct user message",
      "offset": 1934,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "you're putting in. It's just a function",
      "offset": 1936.24,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "that takes in a query, spits out some",
      "offset": 1938.48,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "data, and then you have a function that",
      "offset": 1940.24,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "has a",
      "offset": 1941.76,
      "duration": 6.399
    },
    {
      "lang": "en",
      "text": "I should write this this way. Dev render",
      "offset": 1944,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "to prompt.",
      "offset": 1948.159,
      "duration": 1.841
    },
    {
      "lang": "en",
      "text": "Yep.",
      "offset": 1949.76,
      "duration": 2.56
    },
    {
      "lang": "en",
      "text": "That takes in a data object of type data",
      "offset": 1950,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "and spits out really this should speak",
      "offset": 1952.32,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "spit out",
      "offset": 1954.159,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "a ve of tokens, but I will just say it's",
      "offset": 1955.44,
      "duration": 4.959
    },
    {
      "lang": "en",
      "text": "a string.",
      "offset": 1958.48,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "I'll say it's a ve of tokens. And the",
      "offset": 1960.399,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "reason I'll say that specifically is",
      "offset": 1962.08,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "because one rag context doesn't have to",
      "offset": 1963.919,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "be a string. It can easily meet it can",
      "offset": 1965.919,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "easily just be like for example each rag",
      "offset": 1969.039,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "context could be its own user message.",
      "offset": 1971.039,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "It could be a user and system message",
      "offset": 1973.12,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "pair. It could be a system message. It",
      "offset": 1974.559,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "could be a user assistant message. If a",
      "offset": 1976.48,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "new model comes out with a new message",
      "offset": 1978.48,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "type, it could be that.",
      "offset": 1980.08,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "And that's why there's overlap here.",
      "offset": 1982.64,
      "duration": 3.519
    },
    {
      "lang": "en",
      "text": "That's why the prompt engineering is",
      "offset": 1984.32,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "overlapped with rag because how you use",
      "offset": 1986.159,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "these system and user tokens might be",
      "offset": 1988,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "interled with how you pass context to",
      "offset": 1991.039,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "the model. And I've seen a lot of things",
      "offset": 1993.039,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "where it's like you go retrieve all the",
      "offset": 1994.399,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "documents, but actually the thing that",
      "offset": 1996.24,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "works best is user asks a question and",
      "offset": 1997.679,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "then the system returns I need this and",
      "offset": 2000.64,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "then you put it in as a new user message",
      "offset": 2002.48,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "for the answer. And that's a better way",
      "offset": 2004.48,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "to get the model to pay attention to the",
      "offset": 2006.08,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "data in the rag chunks.",
      "offset": 2007.84,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "Exactly. Um, and as you can probably",
      "offset": 2010.559,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "guess, state and memory are the exact",
      "offset": 2013.84,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "same thing. People call it rag, people",
      "offset": 2015.76,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "call it state, people call it history,",
      "offset": 2018.24,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "people call it memory. All of them are",
      "offset": 2019.919,
      "duration": 3.521
    },
    {
      "lang": "en",
      "text": "basically the same thing. You're using",
      "offset": 2021.919,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "context to pull some data in and then",
      "offset": 2023.44,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "you're rendering it into the model. The",
      "offset": 2026,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "rendering part is the context",
      "offset": 2028.159,
      "duration": 5.041
    },
    {
      "lang": "en",
      "text": "engineering. The pulling part is some",
      "offset": 2029.84,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "excess code that you will write that",
      "offset": 2033.2,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "looks like this. And that is",
      "offset": 2036,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "I think the naive",
      "offset": 2038.08,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "sorry I was going to say like the",
      "offset": 2039.679,
      "duration": 6.641
    },
    {
      "lang": "en",
      "text": "the naive uh the naive rag is basically",
      "offset": 2042.48,
      "duration": 6.399
    },
    {
      "lang": "en",
      "text": "uh just embed the query and send it in.",
      "offset": 2046.32,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "We did a long thing about embeddings and",
      "offset": 2048.879,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "rags and stuff like that. Memory can",
      "offset": 2050.399,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "just be thought of as like rag against",
      "offset": 2052.399,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "past conversations. And again, that's",
      "offset": 2054.24,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "the naive thing. But it doesn't have to",
      "offset": 2055.839,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "just be an embedding comparison in a",
      "offset": 2058.639,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "cosign distance, right? This function,",
      "offset": 2060.32,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "this query of t, this t object could be",
      "offset": 2062,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "a string that the user asked. It could",
      "offset": 2064.48,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "be a string that the user asked plus the",
      "offset": 2066.72,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "user's ID plus like the date of the",
      "offset": 2068.72,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "query and a time range to query memory",
      "offset": 2071.04,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "for because all of that could be stuff",
      "offset": 2072.96,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "in your UI where they say I only want to",
      "offset": 2074.72,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "consider memory. So like memory becomes",
      "offset": 2077.04,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "this more complex thing where like when",
      "offset": 2079.04,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "you get your memory context, your object",
      "offset": 2081.119,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "that you use to fetch it could be a",
      "offset": 2083.28,
      "duration": 3.119
    },
    {
      "lang": "en",
      "text": "combination of like okay we're going to",
      "offset": 2085.04,
      "duration": 3.119
    },
    {
      "lang": "en",
      "text": "filter only things from a certain time",
      "offset": 2086.399,
      "duration": 3.041
    },
    {
      "lang": "en",
      "text": "range. We're going to summarize them",
      "offset": 2088.159,
      "duration": 2.561
    },
    {
      "lang": "en",
      "text": "before but at the end of the day we're",
      "offset": 2089.44,
      "duration": 2.56
    },
    {
      "lang": "en",
      "text": "going to come back with data that's",
      "offset": 2090.72,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "going to get pushed into the prompt.",
      "offset": 2092,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "Yeah. And I think like conversationally",
      "offset": 2093.76,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "the way that we use rag is rag is",
      "offset": 2096.56,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "usually a backed against factual data",
      "offset": 2098.56,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "and that's what people nomenclaturely",
      "offset": 2100.96,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "call rag. Memory is usually the same",
      "offset": 2103.359,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "kind of conversation but against",
      "offset": 2106.32,
      "duration": 7.039
    },
    {
      "lang": "en",
      "text": "previous uh like information about the",
      "offset": 2108,
      "duration": 7.52
    },
    {
      "lang": "en",
      "text": "about the person that's interacting with",
      "offset": 2113.359,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "the system or the entity that's",
      "offset": 2115.52,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "interacting with the system. And then",
      "offset": 2117.119,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "state and history usually refers to like",
      "offset": 2118.88,
      "duration": 6.64
    },
    {
      "lang": "en",
      "text": "conversations more focused on the actual",
      "offset": 2120.8,
      "duration": 6.72
    },
    {
      "lang": "en",
      "text": "uh stuff that's going on currently in",
      "offset": 2125.52,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "the or more focused on the context that",
      "offset": 2127.52,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "like the past interactions that that",
      "offset": 2130.88,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "system has had. memory.",
      "offset": 2132.64,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "It's like your actual like chat history",
      "offset": 2134.48,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "basically.",
      "offset": 2136.8,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "Yeah. And it's very different than",
      "offset": 2138.079,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "memory because memory to me is more like",
      "offset": 2139.68,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "factualized information that are already",
      "offset": 2141.44,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "aggated in some summarized. So for",
      "offset": 2143.44,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "example, if I go to chat GBT",
      "offset": 2144.56,
      "duration": 5.519
    },
    {
      "lang": "en",
      "text": "let's just see what it says. Um uh am I",
      "offset": 2147.28,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "screen sharing? I don't know if you want",
      "offset": 2150.079,
      "duration": 2.481
    },
    {
      "lang": "en",
      "text": "to screen share.",
      "offset": 2151.76,
      "duration": 2.48
    },
    {
      "lang": "en",
      "text": "You are not. You want to screen share?",
      "offset": 2152.56,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "Uh did you put a link in?",
      "offset": 2154.24,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "Um I'm screen sharing.",
      "offset": 2156.64,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "No, I I I don't know if you want to",
      "offset": 2158.56,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "share. Yeah, I am now.",
      "offset": 2160,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "Stop share.",
      "offset": 2162.4,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "I did. Yeah, go for it. Uh, it is",
      "offset": 2164,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "sharing. Yeah, if you go to the top",
      "offset": 2165.52,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "tabs, if you don't see Vibe off screen,",
      "offset": 2167.04,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "you can go to the Zoom top tab and",
      "offset": 2168.64,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "select it.",
      "offset": 2170.56,
      "duration": 3.039
    },
    {
      "lang": "en",
      "text": "Okay. So, like for example, it's like,",
      "offset": 2171.76,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "well, I like what are five words that",
      "offset": 2173.599,
      "duration": 6.561
    },
    {
      "lang": "en",
      "text": "describe me.",
      "offset": 2177.28,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "If I put this on here,",
      "offset": 2180.16,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "this is going to be a memory task. And",
      "offset": 2183.2,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "this is slightly different. Uh, nice.",
      "offset": 2185.68,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "I'm playful. Uh, this is slightly",
      "offset": 2187.839,
      "duration": 6.721
    },
    {
      "lang": "en",
      "text": "different. uh than than a lot of other",
      "offset": 2190.56,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "uh uh things as opposed to state in",
      "offset": 2194.56,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "history. Stay in history is me asking it",
      "offset": 2196.72,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "about the same conversation two or three",
      "offset": 2198.64,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "times. That's stay in history. But",
      "offset": 2200.24,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "memory is more just like using",
      "offset": 2203.52,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "factualized information having being",
      "offset": 2205.44,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "able to have a conversation about this.",
      "offset": 2206.96,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "Um",
      "offset": 2210.079,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "yeah, Brian, that's why I did that and I",
      "offset": 2211.68,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "didn't ask Desk to do it because I",
      "offset": 2213.44,
      "duration": 2.24
    },
    {
      "lang": "en",
      "text": "wasn't sure. I didn't want to make him",
      "offset": 2214.64,
      "duration": 2.479
    },
    {
      "lang": "en",
      "text": "say no in case he didn't want to do it.",
      "offset": 2215.68,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "I was like whatever. We'll see what it",
      "offset": 2217.119,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "says. Impatient and irritable would",
      "offset": 2218.48,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "probably be mine. I like I like to yell",
      "offset": 2221.119,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "at GPT in all caps. It seems to work",
      "offset": 2223.2,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "well.",
      "offset": 2225.44,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "Um,",
      "offset": 2227.119,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "cool. Um, that's the whole point of",
      "offset": 2228.88,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "engineering.",
      "offset": 2232.32,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "Does this make sense to you in terms of",
      "offset": 2233.44,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "like assistant and user as part of state",
      "offset": 2235.28,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "and history? Like this is like your chat",
      "offset": 2237.359,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "history and how you tell the model. And",
      "offset": 2239.119,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "like again, you could use assistant and",
      "offset": 2241.04,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "user messages or you could just put,",
      "offset": 2243.04,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "hey, here's the past conversation and",
      "offset": 2244.96,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "stuff it all into one user message. And",
      "offset": 2246.64,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "this is the context engineering part.",
      "offset": 2248.96,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "Yes. So, what I want to do for the next",
      "offset": 2252,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "uh 10 minutes uh really fast is I want",
      "offset": 2254.8,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "to do some live prompting, but before we",
      "offset": 2257.68,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "do that, I want to check really fast",
      "offset": 2259.76,
      "duration": 5.359
    },
    {
      "lang": "en",
      "text": "with Brian and see if we want to get him",
      "offset": 2261.119,
      "duration": 5.761
    },
    {
      "lang": "en",
      "text": "on first to talk about some of the stuff",
      "offset": 2265.119,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "that he's been doing in memory and talk",
      "offset": 2266.88,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "about his blog post. And then after",
      "offset": 2268.079,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "that, we'll do some of the actual",
      "offset": 2270.16,
      "duration": 2.56
    },
    {
      "lang": "en",
      "text": "prompting in the second part of the",
      "offset": 2271.359,
      "duration": 2.881
    },
    {
      "lang": "en",
      "text": "show, which goes over. So, you can watch",
      "offset": 2272.72,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "the recording if you'd like or stay on",
      "offset": 2274.24,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "where we do live coding. But I think",
      "offset": 2276,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "what we should be doing right now is I'd",
      "offset": 2278.16,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "love to have Brian come on and actually",
      "offset": 2279.76,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "Deck, do you want to pull up his blog",
      "offset": 2281.68,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "post while he does it?",
      "offset": 2282.88,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "Yeah. So, Bri Brian sent me this a while",
      "offset": 2284.88,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "ago. I thought this was one of the best",
      "offset": 2287.599,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "uh like practical guides on how to build",
      "offset": 2289.68,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "agents. It goes through all this",
      "offset": 2291.76,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "proactive stuff and when to trigger",
      "offset": 2293.04,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "things and there's a really nice like",
      "offset": 2294.48,
      "duration": 5.359
    },
    {
      "lang": "en",
      "text": "concise zero slop section on like how",
      "offset": 2296.88,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "they do short-term versus long-term",
      "offset": 2299.839,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "memory and stuff like this. So, I saw",
      "offset": 2301.76,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "this and I was like, we're going to do a",
      "offset": 2303.2,
      "duration": 2.879
    },
    {
      "lang": "en",
      "text": "memory episode. I would love to have",
      "offset": 2304.72,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "Brian come in and talk about kind of how",
      "offset": 2306.079,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "he thinks about this and sort of the",
      "offset": 2308.4,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "lessons they learned along the way and",
      "offset": 2310.8,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "maybe maybe if we can convince him if",
      "offset": 2312.32,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "he's feeling generous to uh share some",
      "offset": 2314,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "code.",
      "offset": 2316.16,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "Do you want to blog post",
      "offset": 2318.88,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "and Brian can walk us through?",
      "offset": 2321.04,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "Yeah, I can I I can share screen with",
      "offset": 2322.24,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "the blog post and should be able to go",
      "offset": 2324.48,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "show some some code and some logs and",
      "offset": 2327.119,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "stuff just for local data.",
      "offset": 2329.04,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "Can you guys see the blog post on my",
      "offset": 2332,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "screen?",
      "offset": 2333.68,
      "duration": 2.159
    },
    {
      "lang": "en",
      "text": "Yes.",
      "offset": 2335.04,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "Are we good? Wonderful. Um,",
      "offset": 2335.839,
      "duration": 6.801
    },
    {
      "lang": "en",
      "text": "I think like all the stuff you're saying",
      "offset": 2339.44,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "about memory is correct where it's like",
      "offset": 2342.64,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "it's really just a form of context",
      "offset": 2344.64,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "engineering, but how you do it I think",
      "offset": 2346.16,
      "duration": 5.439
    },
    {
      "lang": "en",
      "text": "is very use case specific. Um, like for",
      "offset": 2348.48,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "example for for us for tutoring, it's",
      "offset": 2351.599,
      "duration": 5.361
    },
    {
      "lang": "en",
      "text": "okay like we want to have this agent",
      "offset": 2354.48,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "kind of exist and and kind of be a",
      "offset": 2356.96,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "partner to families throughout months",
      "offset": 2358.8,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "and hopefully years as well. And so it's",
      "offset": 2360.64,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "okay if like it can't remember the exact",
      "offset": 2363.28,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "thing that happened like 14 months ago,",
      "offset": 2365.04,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "right? Like if it can just get the",
      "offset": 2367.359,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "general like what were we doing roughly",
      "offset": 2368.64,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "in that season, you know, that's fine,",
      "offset": 2371.2,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "right? Um and but we need to be very",
      "offset": 2372.88,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "like very episodic where it understands",
      "offset": 2375.44,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "like how has the students progressed",
      "offset": 2378.16,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "through time, right? Um we care less",
      "offset": 2380.079,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "about like, you know, your dog's name is",
      "offset": 2383.52,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "Charlie versus uh what were you like bad",
      "offset": 2386.079,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "at a month ago, right? We want to know",
      "offset": 2389.44,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "more of the episodic stuff than the",
      "offset": 2391.359,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "factual things.",
      "offset": 2392.96,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "And so we basically had to had to figure",
      "offset": 2395.2,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "out a way to like",
      "offset": 2397.28,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "how did we Yeah. Go ahead.",
      "offset": 2398.48,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "To interrupt you. It sounds like the",
      "offset": 2401.04,
      "duration": 2.4
    },
    {
      "lang": "en",
      "text": "first thing that you guys did is you",
      "offset": 2402.32,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "defined really clearly what was the",
      "offset": 2403.44,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "success criteria for your thing. Like it",
      "offset": 2405.76,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "sounds like very clearly, you know, you",
      "offset": 2407.599,
      "duration": 2.321
    },
    {
      "lang": "en",
      "text": "don't care about knowing your dog's",
      "offset": 2408.88,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "name. You do care about knowing like",
      "offset": 2409.92,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "educational progress of the student over",
      "offset": 2411.68,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "time. And I know that seems obvious",
      "offset": 2413.52,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "obviously like hearing that that seems",
      "offset": 2416.4,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "super obvious to me about your problem",
      "offset": 2418.56,
      "duration": 2.16
    },
    {
      "lang": "en",
      "text": "statement. and it's like, &quot;Oh, you're",
      "offset": 2419.839,
      "duration": 1.841
    },
    {
      "lang": "en",
      "text": "brilliant tutoring app. Obviously, you",
      "offset": 2420.72,
      "duration": 2.879
    },
    {
      "lang": "en",
      "text": "don't care about your dog's name.&quot; Um,",
      "offset": 2421.68,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "but I can also see how someone did an",
      "offset": 2423.599,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "email or like showed their boss that",
      "offset": 2426.48,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "didn't do that. Someone could be upset",
      "offset": 2428.079,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "by it, but like you've clearly made a",
      "offset": 2429.52,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "very hard stance. So, like this is like",
      "offset": 2431.359,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "it seems like you communicate across",
      "offset": 2433.44,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "your team even what is a success",
      "offset": 2435.04,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "criteria with a pretty good like herring",
      "offset": 2436.88,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "test that someone else can run and be",
      "offset": 2439.68,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "like, &quot;Oh, does should this pass? Should",
      "offset": 2441.359,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "this not pass?&quot; Which I kind of really",
      "offset": 2443.28,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "like about what you just said there.",
      "offset": 2445.28,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "Yeah. It's less about like the dog's",
      "offset": 2448.16,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "name or like that specific fact, but",
      "offset": 2449.76,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "more like what type of information do we",
      "offset": 2451.76,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "care about having in the long run,",
      "offset": 2454.16,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "right? So, uh, a lot of times like the",
      "offset": 2455.92,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "core insight here is that when people",
      "offset": 2458.319,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "are buying or paying for tutoring, um,",
      "offset": 2459.76,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "they need to feel like they're able to",
      "offset": 2462.72,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "offload the cognitive workload of of",
      "offset": 2464.079,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "managing somebody's learning, right?",
      "offset": 2466.079,
      "duration": 2.801
    },
    {
      "lang": "en",
      "text": "Like that's kind of the value that",
      "offset": 2467.68,
      "duration": 2.32
    },
    {
      "lang": "en",
      "text": "you're you're you're actually",
      "offset": 2468.88,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "purchasing. And so if if you can't trust",
      "offset": 2470,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "that the solution that you're purchasing",
      "offset": 2472.96,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "is um like able to keep track of what",
      "offset": 2474.72,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "your kid is doing in the long term and",
      "offset": 2477.839,
      "duration": 2.801
    },
    {
      "lang": "en",
      "text": "like how they're progressing and all",
      "offset": 2479.359,
      "duration": 3.121
    },
    {
      "lang": "en",
      "text": "that stuff and like maybe like",
      "offset": 2480.64,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "remembers, oh you're on vacation this",
      "offset": 2482.48,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "week and so let's not do lessons, but",
      "offset": 2484.16,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "also like you know last year we had",
      "offset": 2486,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "trouble in math like let's let's hammer",
      "offset": 2488.4,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "in the algebra this year, right? Like",
      "offset": 2490.079,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "stuff like that is kind of what you're",
      "offset": 2491.599,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "paying for. And so if we'd obviously",
      "offset": 2493.839,
      "duration": 5.121
    },
    {
      "lang": "en",
      "text": "love to know what your dog's name is and",
      "offset": 2497.359,
      "duration": 3.281
    },
    {
      "lang": "en",
      "text": "and remember that, but like if we had to",
      "offset": 2498.96,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "pick one, we have to optimize for one",
      "offset": 2500.64,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "first.",
      "offset": 2502.96,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "Um,",
      "offset": 2504.24,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "and I'm not having the chat up, so if",
      "offset": 2506.48,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "there's questions in there, let me know.",
      "offset": 2508.16,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "Yeah, show off the diagrams that you I",
      "offset": 2510.16,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "think those are sick.",
      "offset": 2511.92,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "Well, I like also this like conversation",
      "offset": 2514.16,
      "duration": 5.439
    },
    {
      "lang": "en",
      "text": "about like how you decided to like",
      "offset": 2516.319,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "basically like what made you decide to",
      "offset": 2519.599,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "want to build this yourself versus like",
      "offset": 2521.52,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "outsource it to one of the frameworks",
      "offset": 2523.52,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "out there that are all quite popular and",
      "offset": 2525.359,
      "duration": 5.281
    },
    {
      "lang": "en",
      "text": "in broad use.",
      "offset": 2527.839,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "Yeah. Like honestly like after doing a",
      "offset": 2530.64,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "deep dive and this was a few months ago",
      "offset": 2532.24,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "stuff has changed like I didn't really",
      "offset": 2534.079,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "see products that were super focused on",
      "offset": 2536,
      "duration": 5.599
    },
    {
      "lang": "en",
      "text": "this like episodic temporal memory. It",
      "offset": 2538.72,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "was a lot of stuff that was focused on",
      "offset": 2541.599,
      "duration": 2.881
    },
    {
      "lang": "en",
      "text": "like these knowledge graphs, which is",
      "offset": 2542.96,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "great for for use cases that are like",
      "offset": 2544.48,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "much more workflow driven where it's",
      "offset": 2546.24,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "like I want the agent to do a certain",
      "offset": 2547.76,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "thing but also know a bunch of stuff",
      "offset": 2549.2,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "about doing the thing. Um, but like that",
      "offset": 2550.72,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "just wasn't as important to us.",
      "offset": 2553.44,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "Um, and so we basically decided like,",
      "offset": 2555.44,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "hey, let's just build our own temporal",
      "offset": 2558.72,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "agent memory because like the naive",
      "offset": 2559.92,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "solution to this, which we thought was",
      "offset": 2562.24,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "kind of naive, but um, turns out like",
      "offset": 2564.079,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "required a lot of nuance, ended up being",
      "offset": 2566.48,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "really good. Um, and so because we",
      "offset": 2568.319,
      "duration": 5.441
    },
    {
      "lang": "en",
      "text": "didn't care as much about like knowing",
      "offset": 2571.92,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "what was happening exactly, you know, a",
      "offset": 2573.76,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "year ago and like this the exact",
      "offset": 2576.56,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "specifics of that, we ended up building",
      "offset": 2578.72,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "what we call decaying resolution memory.",
      "offset": 2580.48,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "Um, which is like kind of exactly what",
      "offset": 2582.96,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "it sounds like. We basically take all of",
      "offset": 2585.04,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "the ground truth of what has happened",
      "offset": 2586.64,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "and it's this diagram is actually a",
      "offset": 2589.599,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "little bit out of date right now. Um, we",
      "offset": 2591.28,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "don't take the actual messages, we do",
      "offset": 2593.119,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "summaries of the direct interactions. So",
      "offset": 2594.8,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "like after one um one like run of the",
      "offset": 2596.72,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "agent",
      "offset": 2600.88,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "it's uh",
      "offset": 2602,
      "duration": 5.56
    },
    {
      "lang": "en",
      "text": "someone question",
      "offset": 2604.079,
      "duration": 3.481
    },
    {
      "lang": "en",
      "text": "uh Jerry mute that person.",
      "offset": 2607.76,
      "duration": 5.079
    },
    {
      "lang": "en",
      "text": "How you doing?",
      "offset": 2609.52,
      "duration": 3.319
    },
    {
      "lang": "en",
      "text": "I think he's on the phone or something.",
      "offset": 2613.119,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "All right, Jerry is now muted. Jerry,",
      "offset": 2615.599,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "come off mute if you have a question.",
      "offset": 2617.44,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "Cool.",
      "offset": 2619.839,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "um we we take all of the the direct",
      "offset": 2620.96,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "summaries of um kind of what we call",
      "offset": 2623.119,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "like interactions, but there doesn't",
      "offset": 2625.44,
      "duration": 2.879
    },
    {
      "lang": "en",
      "text": "actually have to be a human on the other",
      "offset": 2626.8,
      "duration": 3.039
    },
    {
      "lang": "en",
      "text": "side of the interactions. Like our agent",
      "offset": 2628.319,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "can wake up and not be talking to you.",
      "offset": 2629.839,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "It can just be doing stuff behind the",
      "offset": 2631.68,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "scenes. Um we take a summary of all of",
      "offset": 2633.119,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "those things and that's like our ground",
      "offset": 2636.48,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "truth of what happened, but like the",
      "offset": 2637.839,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "agent might be waking up 10 or 20 times",
      "offset": 2639.839,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "every day depending on like the",
      "offset": 2642.079,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "relationship. And so that's like way too",
      "offset": 2644.4,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "much to just throw into a, you know, the",
      "offset": 2646.96,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "front of your prompt and do context",
      "offset": 2649.28,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "engineering that way. And so we have",
      "offset": 2650.56,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "this like memory system that as memories",
      "offset": 2652.64,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "get older, it clumps them into like",
      "offset": 2655.92,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "lower resolution summarized chunks,",
      "offset": 2658.72,
      "duration": 6.96
    },
    {
      "lang": "en",
      "text": "right? And so we're like a year ago, um,",
      "offset": 2662.16,
      "duration": 5.439
    },
    {
      "lang": "en",
      "text": "you'll have monthly chunks of like,",
      "offset": 2665.68,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "okay, last June we were doing this,",
      "offset": 2667.599,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "right? and kind of this is all the",
      "offset": 2670.48,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "notable stuff about last June which",
      "offset": 2672.16,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "isn't going to be that big right it's",
      "offset": 2674,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "like we're going to it's very lossy but",
      "offset": 2675.2,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "our hypothesis is that like most of the",
      "offset": 2677.68,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "stuff from last June is like not that",
      "offset": 2679.839,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "important to remember you know but then",
      "offset": 2681.52,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "as you get closer to now like for this",
      "offset": 2684.56,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "week you have much more specific",
      "offset": 2687.52,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "granular data about like I think right",
      "offset": 2689.76,
      "duration": 6.559
    },
    {
      "lang": "en",
      "text": "now we do um I can look at this in a",
      "offset": 2692.56,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "minute but I think right now we do um",
      "offset": 2696.319,
      "duration": 6.241
    },
    {
      "lang": "en",
      "text": "like every for the for today we store",
      "offset": 2698.8,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "the direct interactions and that gets",
      "offset": 2702.56,
      "duration": 6.32
    },
    {
      "lang": "en",
      "text": "fed into the model. But then for uh",
      "offset": 2704.64,
      "duration": 7.12
    },
    {
      "lang": "en",
      "text": "like the last week's worth of days um we",
      "offset": 2708.88,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "have daily summaries and then for like",
      "offset": 2711.76,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "the last n weeks we have weekly",
      "offset": 2713.359,
      "duration": 3.121
    },
    {
      "lang": "en",
      "text": "summaries and the last forever we have",
      "offset": 2714.56,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "monthly summaries for the rest of it.",
      "offset": 2716.48,
      "duration": 3.119
    },
    {
      "lang": "en",
      "text": "And so what that becomes is like this",
      "offset": 2718,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "kind of chunked up pretty like",
      "offset": 2719.599,
      "duration": 5.121
    },
    {
      "lang": "en",
      "text": "compressed context window of what kind",
      "offset": 2722.16,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "of is like the the background of this",
      "offset": 2724.72,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "agent's memory. And the nice part is",
      "offset": 2727.2,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "that this scales sublinearly, right?",
      "offset": 2729.359,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "Like that's the most important piece is",
      "offset": 2731.68,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "that as as you have more interactions",
      "offset": 2733.119,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "and more and more time with this agent,",
      "offset": 2735.119,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "uh your number of tokens to keep the",
      "offset": 2737.359,
      "duration": 5.041
    },
    {
      "lang": "en",
      "text": "memory like uh up to date is not a",
      "offset": 2738.88,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "linear scaling anymore. Um and there's",
      "offset": 2742.4,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "bunch of chats in here. Um no, this code",
      "offset": 2744.72,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "is not open source, but I'm happy to to",
      "offset": 2747.52,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "share snippets of people if if you know",
      "offset": 2749.599,
      "duration": 6.161
    },
    {
      "lang": "en",
      "text": "they would like that. Um",
      "offset": 2751.68,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "Josh,",
      "offset": 2755.76,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "would you not summarize",
      "offset": 2756.16,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "Go ahead.",
      "offset": 2758.64,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "Again.",
      "offset": 2759.28,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "No, go ahead.",
      "offset": 2761.44,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "Uh Josh, we we we don't summarize um by",
      "offset": 2762,
      "duration": 6.64
    },
    {
      "lang": "en",
      "text": "topic. This is specifically for the",
      "offset": 2765.68,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "agents memory. And I'll go into a bit",
      "offset": 2768.64,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "more about like the how we store topic",
      "offset": 2770.56,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "stuff later is like we make heavy heavy",
      "offset": 2772.48,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "use of stateful tooling which is again",
      "offset": 2774.96,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "kind of like a reversed like spun around",
      "offset": 2776.88,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "version of rag. Right.",
      "offset": 2779.92,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "This was so cool. Right. You're using",
      "offset": 2781.44,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "the tools themselves to offload some of",
      "offset": 2783.359,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "the context management and they have",
      "offset": 2785.68,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "their own kind of stateful memory like",
      "offset": 2787.44,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "like isolated to the context of each",
      "offset": 2790.16,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "agent instance that is might be using",
      "offset": 2792.16,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "those tools. Right.",
      "offset": 2793.92,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "Yeah. So to me this this was like",
      "offset": 2796.079,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "obvious. I didn't know this is like not",
      "offset": 2798.079,
      "duration": 2.801
    },
    {
      "lang": "en",
      "text": "a thing that people were doing a lot of",
      "offset": 2799.599,
      "duration": 3.121
    },
    {
      "lang": "en",
      "text": "yet. But it's like it's very obvious",
      "offset": 2800.88,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "that like I don't remember all of the",
      "offset": 2802.72,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "emails in my inbox. That's why I have an",
      "offset": 2804.64,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "inbox, right? Like there there are tasks",
      "offset": 2806.88,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "like this that I just am not able to do.",
      "offset": 2809.44,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "I can't compute those in my head, you",
      "offset": 2811.92,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "know, and so I know that like my",
      "offset": 2813.76,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "workflow for this is check my inbox,",
      "offset": 2817.2,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "look at what's in my inbox and whatever.",
      "offset": 2819.52,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "And so we we distinctly don't actually",
      "offset": 2821.28,
      "duration": 6.559
    },
    {
      "lang": "en",
      "text": "include that data for like inboxes, SMS,",
      "offset": 2823.839,
      "duration": 6.721
    },
    {
      "lang": "en",
      "text": "we don't really do specific stuff in",
      "offset": 2827.839,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "this memory summarizations because we",
      "offset": 2830.56,
      "duration": 3.039
    },
    {
      "lang": "en",
      "text": "don't want them there, right? We want",
      "offset": 2832.24,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "the memory summarizations to only be",
      "offset": 2833.599,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "about like what's happening during the",
      "offset": 2835.2,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "tutoring relationship, where's the",
      "offset": 2837.119,
      "duration": 2.801
    },
    {
      "lang": "en",
      "text": "student and their progress, all that",
      "offset": 2838.64,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "stuff, right?",
      "offset": 2839.92,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "um",
      "offset": 2841.839,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "out of the model design.",
      "offset": 2843.359,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "Yeah.",
      "offset": 2845.92,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "Uh with with long-term versus with",
      "offset": 2847.04,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "variable just on this chart um is with",
      "offset": 2849.04,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "variable that's the DRM the decaying",
      "offset": 2852.24,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "resolution where it just gets more and",
      "offset": 2854.24,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "more summarizy over time.",
      "offset": 2856.079,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "Mhm.",
      "offset": 2859.04,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "Okay. And what is with long-term? Is",
      "offset": 2860.16,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "there a is that like an in between",
      "offset": 2862.48,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "diagram that's actually not in here?",
      "offset": 2864,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "Well, with long-term, it's just like if",
      "offset": 2866.56,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "you make big summaries of what's",
      "offset": 2868.88,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "happening, but it's still just summaries",
      "offset": 2870.56,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "every n like every unit of time. Like",
      "offset": 2872.4,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "maybe you do weekly summaries as your",
      "offset": 2875.2,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "long-term memory, right? Because then",
      "offset": 2876.8,
      "duration": 2.48
    },
    {
      "lang": "en",
      "text": "you can say, &quot;Hold on, we got a",
      "offset": 2878.16,
      "duration": 2.4
    },
    {
      "lang": "en",
      "text": "short-term memory, which is here, and",
      "offset": 2879.28,
      "duration": 3.039
    },
    {
      "lang": "en",
      "text": "then weekly memory, which is here, but",
      "offset": 2880.56,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "it still scales linear linearly with",
      "offset": 2882.319,
      "duration": 5.361
    },
    {
      "lang": "en",
      "text": "weeks.&quot; Um, which is just like better",
      "offset": 2884.56,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "than just having short-term, but it's",
      "offset": 2887.68,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "not quite as good.",
      "offset": 2889.2,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "Um, I see a head up from Steve",
      "offset": 2890.48,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "insight before sleep goes on. I think",
      "offset": 2892.16,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "the real insight. Can you scroll up",
      "offset": 2894.079,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "here, Brian?",
      "offset": 2895.28,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "The real the real problem here is I",
      "offset": 2897.44,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "think a this is like when first people",
      "offset": 2900.64,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "are first introduce like merge sort for",
      "offset": 2902.96,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "the first time like oh [Â __Â ] that works",
      "offset": 2904.8,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "and you keep sorting in a small bit. It",
      "offset": 2906.72,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "actually works really well and it's",
      "offset": 2909.2,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "really elegant and it feels really",
      "offset": 2910.319,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "intuitive the first time you see it and",
      "offset": 2912.16,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "the first time you write it. Um, but I",
      "offset": 2913.76,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "think what you're probably running into",
      "offset": 2916.64,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "here is for the first time like people",
      "offset": 2918.88,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "are like, &quot;How do I get this thing to",
      "offset": 2921.28,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "remember everything I've done over the",
      "offset": 2922.96,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "last two years?&quot; And the answer is you",
      "offset": 2924.96,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "don't.",
      "offset": 2927.2,
      "duration": 2.399
    },
    {
      "lang": "en",
      "text": "That's the thing. The trick is you",
      "offset": 2928.16,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "don't. And once you reframe that problem",
      "offset": 2929.599,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "and say, &quot;How do I get to remember the",
      "offset": 2931.839,
      "duration": 2.881
    },
    {
      "lang": "en",
      "text": "most important things?&quot;",
      "offset": 2933.28,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "Now you have a separate task which is",
      "offset": 2934.72,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "how do I make good monthly summaries",
      "offset": 2936.8,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "which is a separate LM problem that you",
      "offset": 2939.44,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "constrain and say are my monthly",
      "offset": 2941.52,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "summaries good? And you can build an",
      "offset": 2943.68,
      "duration": 2.48
    },
    {
      "lang": "en",
      "text": "eval",
      "offset": 2945.599,
      "duration": 2.801
    },
    {
      "lang": "en",
      "text": "and that will be use case specific too.",
      "offset": 2946.16,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "Um, exactly like you can you can choose",
      "offset": 2948.4,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "what you want to happen and what you",
      "offset": 2950.48,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "want to be in a summaries. Um, and but",
      "offset": 2951.92,
      "duration": 5.919
    },
    {
      "lang": "en",
      "text": "monthly summaries for monthly summaries",
      "offset": 2955.359,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "for a tutoring bot is going to be very",
      "offset": 2957.839,
      "duration": 2.961
    },
    {
      "lang": "en",
      "text": "different than monthly summaries for",
      "offset": 2959.44,
      "duration": 3.119
    },
    {
      "lang": "en",
      "text": "chat GPT than is going to be different",
      "offset": 2960.8,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "for every other use case. And so you can",
      "offset": 2962.559,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "eval this and you can test it and you",
      "offset": 2964.8,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "can stub it out and you can build a",
      "offset": 2966.319,
      "duration": 2.481
    },
    {
      "lang": "en",
      "text": "product around it and you can look at it",
      "offset": 2967.52,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "in isolation which makes it easy to kind",
      "offset": 2968.8,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "of build these different this this is",
      "offset": 2971.119,
      "duration": 5.601
    },
    {
      "lang": "en",
      "text": "the engineering part. How about even",
      "offset": 2972.4,
      "duration": 6.64
    },
    {
      "lang": "en",
      "text": "how about milestone? How about",
      "offset": 2976.72,
      "duration": 5.599
    },
    {
      "lang": "en",
      "text": "milestones summaries?",
      "offset": 2979.04,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "It's all the same.",
      "offset": 2982.319,
      "duration": 2.081
    },
    {
      "lang": "en",
      "text": "What do you mean by that?",
      "offset": 2983.119,
      "duration": 3.121
    },
    {
      "lang": "en",
      "text": "Uh I I think the whole concept here is",
      "offset": 2984.4,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "like you you understand your problem",
      "offset": 2986.24,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "definition and once you understand your",
      "offset": 2988.4,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "problem definition, you then have to go",
      "offset": 2990,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "and scope it out accordingly. So in this",
      "offset": 2991.44,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "case in the tutorbot case, I don't care",
      "offset": 2993.44,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "about details like the dog's name. I",
      "offset": 2996.48,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "care about seeing the academic progress.",
      "offset": 2998.319,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "So what I want to make sure is actual as",
      "offset": 3000.24,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "I funnel down from actual messages to",
      "offset": 3002.88,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "daily summaries to weekly summaries to",
      "offset": 3005.119,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "monthly summaries. I basically am",
      "offset": 3007.04,
      "duration": 5.279
    },
    {
      "lang": "en",
      "text": "expecting a loss of information in each",
      "offset": 3009.28,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "of these steps. It's like a funnel of",
      "offset": 3012.319,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "information being lost. That's by design",
      "offset": 3014.079,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "a good thing. That's what I want. But I",
      "offset": 3016.88,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "want",
      "offset": 3019.28,
      "duration": 2.16
    },
    {
      "lang": "en",
      "text": "you want smaller context. You want",
      "offset": 3019.68,
      "duration": 5.8
    },
    {
      "lang": "en",
      "text": "tighter, more focused context.",
      "offset": 3021.44,
      "duration": 4.04
    },
    {
      "lang": "en",
      "text": "How do I question in zoom?",
      "offset": 3025.76,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "I haven't used Zoom much. Uh why are we",
      "offset": 3028.88,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "focusing why are you focusing so much on",
      "offset": 3031.52,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "time rather than just token size?",
      "offset": 3033.119,
      "duration": 5.281
    },
    {
      "lang": "en",
      "text": "Well, that's the context window that",
      "offset": 3036.319,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "that seems to work well for Brian's use",
      "offset": 3038.4,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "case, I'm guessing. But that's the",
      "offset": 3040.559,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "point. You can decide as a developer",
      "offset": 3042,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "what you want to go use for your thing.",
      "offset": 3044.72,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "So you might say that hey for me token",
      "offset": 3047.44,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "window size matters but semantically",
      "offset": 3050.4,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "what I suspect is for many use cases",
      "offset": 3052.8,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "time is a good orientation because what",
      "offset": 3054.72,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "I'm doing a month ago is probably very",
      "offset": 3057.68,
      "duration": 2.56
    },
    {
      "lang": "en",
      "text": "different than what I'm doing today.",
      "offset": 3059.28,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "It's very very unlikely for it to be",
      "offset": 3060.24,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "continuous. But what I'm doing today,",
      "offset": 3062.24,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "like when I'm doing a cloud code task,",
      "offset": 3064.8,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "every single cloud code session is its",
      "offset": 3067.2,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "own session pretty much. Very rarely do",
      "offset": 3069.119,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "I resume a new session. But whenever I'm",
      "offset": 3071.04,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "doing that, it doesn't matter how big",
      "offset": 3073.52,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "that context is. That is all",
      "offset": 3074.96,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "semantically relevant. And I might still",
      "offset": 3076.96,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "Claude code does something called",
      "offset": 3079.52,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "compacting the context window, which is",
      "offset": 3080.8,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "very relevant. But it's the same thing.",
      "offset": 3083.119,
      "duration": 3.521
    },
    {
      "lang": "en",
      "text": "They realize that the context for them",
      "offset": 3084.96,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "now is getting too big. Let's build a",
      "offset": 3086.64,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "smaller subset of it that we can",
      "offset": 3090.48,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "actually pass in and maintain. But then",
      "offset": 3092.16,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "we go into cloud code rules. If you",
      "offset": 3095.2,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "scroll up again, Brian, sorry, can you",
      "offset": 3097.359,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "scroll up? Yeah. Then we go to cloud",
      "offset": 3099.839,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "code oh into that diagram. Then we go",
      "offset": 3102,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "into cloud code rules. And cloud code",
      "offset": 3103.92,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "rules kind of behave like these monthly",
      "offset": 3105.599,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "summaries which are being injected in",
      "offset": 3107.359,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "every prompt that are almost like static",
      "offset": 3109.68,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "constants. Um, and I say static",
      "offset": 3112,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "constants because they're not changing",
      "offset": 3114.96,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "very often, but they're being injected",
      "offset": 3116.319,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "in and then the model is kind of",
      "offset": 3118.64,
      "duration": 2.4
    },
    {
      "lang": "en",
      "text": "understanding that. And this part is the",
      "offset": 3119.76,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "part that is growing ephemerably as I go",
      "offset": 3121.04,
      "duration": 3.039
    },
    {
      "lang": "en",
      "text": "on. Okay.",
      "offset": 3123.44,
      "duration": 1.919
    },
    {
      "lang": "en",
      "text": "And this is like what the model",
      "offset": 3124.079,
      "duration": 3.121
    },
    {
      "lang": "en",
      "text": "compacted and put on.",
      "offset": 3125.359,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "Stephen,",
      "offset": 3127.2,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "hold on. Let's just I we have a ton of",
      "offset": 3128.96,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "questions piling up. I I want to let",
      "offset": 3131.44,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "Brian finish his walkthrough of the blog",
      "offset": 3133.119,
      "duration": 3.121
    },
    {
      "lang": "en",
      "text": "post and then we'll do all the questions",
      "offset": 3134.8,
      "duration": 2.559
    },
    {
      "lang": "en",
      "text": "because otherwise I don't think we're",
      "offset": 3136.24,
      "duration": 2.16
    },
    {
      "lang": "en",
      "text": "going to finish.",
      "offset": 3137.359,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "That's a good point.",
      "offset": 3138.4,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "Uh, sorry Brian. I'm gonna I'm gonna",
      "offset": 3140.8,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "mute myself too.",
      "offset": 3142.559,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "All good. Um I think the really",
      "offset": 3144.48,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "important here someone asked about",
      "offset": 3146.559,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "context or about token count is that not",
      "offset": 3147.76,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "only does this method scale sublinearly,",
      "offset": 3150.24,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "it actually like tapers off. It's like",
      "offset": 3152.64,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "not quite asic, but it's like pretty",
      "offset": 3154.72,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "close. Um and so like even if you're",
      "offset": 3156.64,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "doing if we're if we're working with a",
      "offset": 3159.599,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "family for five years, you're still only",
      "offset": 3161.839,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "at a max of like maybe call it 60",
      "offset": 3164.559,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "summaries, right?",
      "offset": 3167.119,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "um maybe maybe a little bit like 65,",
      "offset": 3169.04,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "right? And so each summary is like a",
      "offset": 3171.2,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "like maybe a hundred words. That's like",
      "offset": 3173.2,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "still not that much tokens. And so our",
      "offset": 3174.64,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "at least our our bet is like the context",
      "offset": 3176.8,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "windows that are available with these",
      "offset": 3178.8,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "models will scale faster than our memory",
      "offset": 3180,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "will require, right? Um",
      "offset": 3181.92,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "um so yes, all of that memory is being",
      "offset": 3186,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "put in the prompt. Um but it's because",
      "offset": 3187.92,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "it's lossy. It's like it's not all of",
      "offset": 3190.96,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "the information, right? It's like it's",
      "offset": 3193.28,
      "duration": 3.279
    },
    {
      "lang": "en",
      "text": "only the most important stuff from the",
      "offset": 3194.88,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "past that's being put in. Um,",
      "offset": 3196.559,
      "duration": 5.361
    },
    {
      "lang": "en",
      "text": "the other thing I want to harp on",
      "offset": 3199.92,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "Yeah.",
      "offset": 3201.92,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "No, go ahead, Brian, please.",
      "offset": 3202.88,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "The other thing I want to harp on is uh",
      "offset": 3205.28,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "how like what we store in memory and how",
      "offset": 3207.68,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "our to use of tools actually changes",
      "offset": 3210.48,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "this where like like the email inbox",
      "offset": 3212.88,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "stuff like Orin which is our our tutor's",
      "offset": 3215.76,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "name has an email inbox and and that",
      "offset": 3217.68,
      "duration": 5.919
    },
    {
      "lang": "en",
      "text": "email inbox is isolated based on uh kind",
      "offset": 3220.319,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "of every like customer unit which is",
      "offset": 3223.599,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "like a family for us. And so when he's",
      "offset": 3225.2,
      "duration": 5.359
    },
    {
      "lang": "en",
      "text": "querying his email inbox, we only show",
      "offset": 3227.92,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "him the stuff that would be like",
      "offset": 3230.559,
      "duration": 6.721
    },
    {
      "lang": "en",
      "text": "isolated to this family, right? And what",
      "offset": 3233.92,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "that means is that we now don't have to",
      "offset": 3237.28,
      "duration": 3.279
    },
    {
      "lang": "en",
      "text": "store all of that information in memory",
      "offset": 3238.8,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "because whenever he wants to look at his",
      "offset": 3240.559,
      "duration": 2.961
    },
    {
      "lang": "en",
      "text": "email inbox, he can just go look at his",
      "offset": 3242.24,
      "duration": 3.119
    },
    {
      "lang": "en",
      "text": "email inbox, right? And we do the same",
      "offset": 3243.52,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "thing for email. We do it for SMS. He",
      "offset": 3245.359,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "has a calendar that he can use. like all",
      "offset": 3247.76,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "these very basic like just knowledge",
      "offset": 3249.599,
      "duration": 5.281
    },
    {
      "lang": "en",
      "text": "work tools that all of us use um to",
      "offset": 3251.68,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "offload our own memory onto these like",
      "offset": 3254.88,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "software systems we also can give to",
      "offset": 3257.28,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "agents to basically give them the same",
      "offset": 3259.76,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "abilities. Um",
      "offset": 3261.359,
      "duration": 3.121
    },
    {
      "lang": "en",
      "text": "let me let me describe that really",
      "offset": 3262.64,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "quickly just so I I articulate exactly",
      "offset": 3264.48,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "what I think you're saying. Typically",
      "offset": 3266.72,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "when people do like querying a tool",
      "offset": 3268.559,
      "duration": 3.121
    },
    {
      "lang": "en",
      "text": "that's like a tool call or like a",
      "offset": 3270.48,
      "duration": 2.639
    },
    {
      "lang": "en",
      "text": "structured output kind of concept that",
      "offset": 3271.68,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "people are doing and usually they'll",
      "offset": 3273.119,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "have the model output like the user ID",
      "offset": 3274.8,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "of the person they want a structured",
      "offset": 3276.96,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "output. You're saying the only tool that",
      "offset": 3278.4,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "you're giving the model is query search",
      "offset": 3280.64,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "the inbox search the inbox then has an",
      "offset": 3282.88,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "implicit ID attached to it",
      "offset": 3285.76,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "for that user. So the model doesn't have",
      "offset": 3288.079,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "to think it doesn't have to think about",
      "offset": 3290,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "the idea of user permanent users at all.",
      "offset": 3291.599,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "there is only one user from the",
      "offset": 3294.319,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "perspective of the model and is that",
      "offset": 3295.76,
      "duration": 2.48
    },
    {
      "lang": "en",
      "text": "what you're saying?",
      "offset": 3297.68,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "Well, that's not entirely true. Um there",
      "offset": 3298.24,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "there's only one family from the",
      "offset": 3301.839,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "perspective of the model, but when you",
      "offset": 3303.44,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "search the inbox, he also has a contact",
      "offset": 3305.119,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "book that he can use, but that contact",
      "offset": 3307.28,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "book like there's an implicit idea of",
      "offset": 3309.52,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "like every Orin has",
      "offset": 3311.68,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "family's contact. It's it's that",
      "offset": 3313.92,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "family's contact.",
      "offset": 3316,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "It might have like the mom and the dad",
      "offset": 3317.599,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "in the contact book, right? and it will",
      "offset": 3319.04,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "have to give one of those ids as part of",
      "offset": 3321.2,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "the tool call. But the the model itself",
      "offset": 3323.119,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "is like sandboxed into only being able",
      "offset": 3325.44,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "to operate within the scope of this one",
      "offset": 3327.839,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "family. Right.",
      "offset": 3330,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "Yeah. I see that mistake a lot in a lot",
      "offset": 3331.28,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "of prompts when I see them is like",
      "offset": 3334.079,
      "duration": 2.721
    },
    {
      "lang": "en",
      "text": "people are asking it to load some",
      "offset": 3335.44,
      "duration": 3.119
    },
    {
      "lang": "en",
      "text": "database ID and they're literally having",
      "offset": 3336.8,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "the model regurgitate the database ID",
      "offset": 3338.559,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "from the prompt or something that they",
      "offset": 3340.4,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "injected in. I'm like why even do that",
      "offset": 3341.599,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "work? just let the model pretend like it",
      "offset": 3344.16,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "doesn't have to think about that and let",
      "offset": 3345.92,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "just like pass that along to the tool",
      "offset": 3347.52,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "manually",
      "offset": 3349.599,
      "duration": 3.281
    },
    {
      "lang": "en",
      "text": "rather than having to go do that work on",
      "offset": 3350.96,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "in the model side because it it's more",
      "offset": 3352.88,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "complexity on the model side",
      "offset": 3354.96,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "I do still have that where it's like I",
      "offset": 3357.599,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "want to go check my email but from this",
      "offset": 3359.44,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "email address right which is like the",
      "offset": 3361.52,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "dad's email right or the mom's email but",
      "offset": 3363.119,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "there's there's the the scope of valid",
      "offset": 3366.16,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "options is much much smaller than if you",
      "offset": 3368.319,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "were to give it access to the entire",
      "offset": 3370.4,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "database right and there's also like",
      "offset": 3371.92,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "because our customers have direct access",
      "offset": 3374.16,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "to the to the agent, there's huge",
      "offset": 3376.96,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "security concerns",
      "offset": 3378.88,
      "duration": 2.32
    },
    {
      "lang": "en",
      "text": "where like",
      "offset": 3380.16,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "I we we cannot like we cannot put in",
      "offset": 3381.2,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "context information about other",
      "offset": 3384.4,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "families, right? if for your agent that",
      "offset": 3386.16,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "you're working with like you just can't",
      "offset": 3389.76,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "do it. What if it leaks something just",
      "offset": 3391.04,
      "duration": 3.279
    },
    {
      "lang": "en",
      "text": "like a zero you have a zero tolerance of",
      "offset": 3392.72,
      "duration": 2.879
    },
    {
      "lang": "en",
      "text": "risk for that because there's also a lot",
      "offset": 3394.319,
      "duration": 2.721
    },
    {
      "lang": "en",
      "text": "of kid information in there",
      "offset": 3395.599,
      "duration": 2.161
    },
    {
      "lang": "en",
      "text": "and that's",
      "offset": 3397.04,
      "duration": 2.319
    },
    {
      "lang": "en",
      "text": "yeah so there's like we have to like",
      "offset": 3397.76,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "there's fura stuff like there's there's",
      "offset": 3399.359,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "just structurally we cannot have any any",
      "offset": 3401.2,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "leaking risk here and with this there is",
      "offset": 3403.119,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "no leaking risk right",
      "offset": 3405.04,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "yeah because the tool itself is",
      "offset": 3406.559,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "inherently tied only to that ID that you",
      "offset": 3408.16,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "have hardcoded in to your system",
      "offset": 3410.4,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "pretty much yeah",
      "offset": 3413.76,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "do you want to see before we answer the",
      "offset": 3415.68,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "questions is I'd love to see one of your",
      "offset": 3418.16,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "prompts",
      "offset": 3420.079,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "Can you share that with us?",
      "offset": 3420.88,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "Yes, I can share prompts. I can share",
      "offset": 3423.119,
      "duration": 3.281
    },
    {
      "lang": "en",
      "text": "codes.",
      "offset": 3425.599,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "I'm not going to share evals, but I can",
      "offset": 3426.4,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "share everything else.",
      "offset": 3428.319,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "I Yeah, I actually I understand that",
      "offset": 3430.24,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "sentiment. A lot of companies that I've",
      "offset": 3432.88,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "talked to share that same sentiment. The",
      "offset": 3434.4,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "secret sauce is the evals",
      "offset": 3436.24,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "and the prompts and everything else is",
      "offset": 3438.079,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "is like, &quot;Yeah, sure. You can take the",
      "offset": 3441.68,
      "duration": 2.399
    },
    {
      "lang": "en",
      "text": "prompt. We'll have a better prompt like",
      "offset": 3442.96,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "two months from now. It's fine.&quot; Uh,",
      "offset": 3444.079,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "I can even share like how we made the",
      "offset": 3446.319,
      "duration": 2.881
    },
    {
      "lang": "en",
      "text": "evals, but I'm not going to share the",
      "offset": 3448,
      "duration": 2.24
    },
    {
      "lang": "en",
      "text": "evals.",
      "offset": 3449.2,
      "duration": 2.56
    },
    {
      "lang": "en",
      "text": "Yeah.",
      "offset": 3450.24,
      "duration": 2
    },
    {
      "lang": "en",
      "text": "Um,",
      "offset": 3451.76,
      "duration": 2
    },
    {
      "lang": "en",
      "text": "let's take a look at some of the",
      "offset": 3452.24,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "prompts. I'd love to see them.",
      "offset": 3453.76,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "Yeah. So, I I I do think it's worthwhile",
      "offset": 3456.4,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "also briefly going over the productivity",
      "offset": 3458.72,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "piece just people understand like why",
      "offset": 3461.68,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "these things are running without human",
      "offset": 3463.52,
      "duration": 2.24
    },
    {
      "lang": "en",
      "text": "input,",
      "offset": 3464.96,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "right? Um, like basically we let the LLM",
      "offset": 3465.76,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "set their own wake schedules and so Orin",
      "offset": 3469.44,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "can wake up at midnight while you're",
      "offset": 3471.92,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "sleeping and do something if he want if",
      "offset": 3473.44,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "he wants to, which might be creepy, but",
      "offset": 3474.88,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "like it's possible, right? And so what",
      "offset": 3476.799,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "happens is that like when he wakes up,",
      "offset": 3479.44,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "it's not just in response to something",
      "offset": 3481.52,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "that something that the user has done.",
      "offset": 3483.28,
      "duration": 3.279
    },
    {
      "lang": "en",
      "text": "He can wake up and then like text you a",
      "offset": 3484.96,
      "duration": 6.399
    },
    {
      "lang": "en",
      "text": "reminder, right? Um, and I can show uh I",
      "offset": 3486.559,
      "duration": 7.441
    },
    {
      "lang": "en",
      "text": "just I just ran this locally and I can I",
      "offset": 3491.359,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "can show kind of another one in a minute",
      "offset": 3494,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "how how it works. But this is like one",
      "offset": 3495.28,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "one trace of running the actual what we",
      "offset": 3498.16,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "call entity which is like a deployed",
      "offset": 3500.88,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "version of orin for one family.",
      "offset": 3503.2,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "Um,",
      "offset": 3505.44,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "and so what we do is like before",
      "offset": 3506.96,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "anything runs we make sure that all the",
      "offset": 3508.88,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "summaries are updated again. In this",
      "offset": 3510.799,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "case it was all it was all done. And",
      "offset": 3512.64,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "then this is like one of the prompts",
      "offset": 3514.72,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "that would be piped into these models.",
      "offset": 3516.4,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "And so we basically like okay, first",
      "offset": 3518.319,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "thing we tell here's your memory",
      "offset": 3520.72,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "context. Uh and we include the the the",
      "offset": 3522.4,
      "duration": 4.959
    },
    {
      "lang": "en",
      "text": "date and time. Um and then this actually",
      "offset": 3524.64,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "is a bit out of order, but we give it",
      "offset": 3527.359,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "all of the tools with descriptions of",
      "offset": 3529.28,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "what it can do more so than just the the",
      "offset": 3531.04,
      "duration": 6.88
    },
    {
      "lang": "en",
      "text": "schema in um like in in the structured",
      "offset": 3533.599,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "uh outputs kind of tool schema.",
      "offset": 3537.92,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "And this is again why structured outputs",
      "offset": 3540.079,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "is part of context engineering, right?",
      "offset": 3542.319,
      "duration": 2.721
    },
    {
      "lang": "en",
      "text": "is and I've seen a lot of people do this",
      "offset": 3543.92,
      "duration": 2.48
    },
    {
      "lang": "en",
      "text": "where you put the scheme in at the end",
      "offset": 3545.04,
      "duration": 2.799
    },
    {
      "lang": "en",
      "text": "but then you also put the descriptions",
      "offset": 3546.4,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "in the system message.",
      "offset": 3547.839,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "It's more it's also just because like we",
      "offset": 3550.4,
      "duration": 5.919
    },
    {
      "lang": "en",
      "text": "we care about um being very like we want",
      "offset": 3552.16,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "him to be very human in when and how he",
      "offset": 3556.319,
      "duration": 5.601
    },
    {
      "lang": "en",
      "text": "uses these tools, right? Because if if",
      "offset": 3558.64,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "we frame the tool as like here's your",
      "offset": 3561.92,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "email inbox, he has many examples in",
      "offset": 3564.4,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "sample of the model training of when and",
      "offset": 3566.64,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "how to like use a email inbox, right?",
      "offset": 3568.48,
      "duration": 5.119
    },
    {
      "lang": "en",
      "text": "Um, and so like the framing of the tool",
      "offset": 3571.839,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "matters a lot. Um, so yeah, we have a",
      "offset": 3573.599,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "lot of different tools that you can use",
      "offset": 3576.319,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "here. Um, but then like we have a we",
      "offset": 3577.52,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "basically have a big prompt that's just",
      "offset": 3580.4,
      "duration": 5.919
    },
    {
      "lang": "en",
      "text": "like here is um here is like your job",
      "offset": 3582,
      "duration": 7.119
    },
    {
      "lang": "en",
      "text": "manual. This is very high level like",
      "offset": 3586.319,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "what you want to be doing as a tutor",
      "offset": 3589.119,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "like you know what you're supposed to",
      "offset": 3591.119,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "like what what does healthy look like in",
      "offset": 3592.799,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "engagement? What's a healthy customer",
      "offset": 3594.72,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "look like? All that stuff. a bunch of",
      "offset": 3596.079,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "like not really if then instructions but",
      "offset": 3598,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "just general stuff. Um, and I guess this",
      "offset": 3600,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "one actually didn't have any memory",
      "offset": 3602.96,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "because I ran it fresh. Um, but what",
      "offset": 3604,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "happens here is it goes through and let",
      "offset": 3606.48,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "me condense this input out. So it goes",
      "offset": 3610,
      "duration": 5.599
    },
    {
      "lang": "en",
      "text": "through and um now starts to call some",
      "offset": 3612.24,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "tools and we basically do a while loop",
      "offset": 3615.599,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "where it's like it calls tools until it",
      "offset": 3617.28,
      "duration": 6.64
    },
    {
      "lang": "en",
      "text": "calls the sleep tool, you know. Um, so",
      "offset": 3620.319,
      "duration": 7.201
    },
    {
      "lang": "en",
      "text": "like it's going in here and it's saying",
      "offset": 3623.92,
      "duration": 7.6
    },
    {
      "lang": "en",
      "text": "um tool is this.",
      "offset": 3627.52,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "It needs to",
      "offset": 3631.52,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "Yeah. So it needs to like check the",
      "offset": 3633.68,
      "duration": 2.879
    },
    {
      "lang": "en",
      "text": "email threads. So it's checking its",
      "offset": 3635.04,
      "duration": 3.279
    },
    {
      "lang": "en",
      "text": "inbox right now. And then we we come",
      "offset": 3636.559,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "back and at the end of all this we",
      "offset": 3638.319,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "return back the JSON of of the email",
      "offset": 3641.04,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "API, right? U we don't really do a lot",
      "offset": 3643.119,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "of like augmentation in this part of the",
      "offset": 3645.599,
      "duration": 3.281
    },
    {
      "lang": "en",
      "text": "rag. We just kind of get it back like",
      "offset": 3647.599,
      "duration": 3.041
    },
    {
      "lang": "en",
      "text": "JSON responses and that works pretty",
      "offset": 3648.88,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "well. Um, and so it's like, cool, I've",
      "offset": 3650.64,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "called the email tool. I got my response",
      "offset": 3653.2,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "back. Cool. I can see there's emails",
      "offset": 3655.04,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "here. Let me open the test email, right?",
      "offset": 3656.88,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "To see if there's a student. Um, it",
      "offset": 3658.72,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "calls that tool. We append the content",
      "offset": 3660.88,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "of that email to the prompt. And then it",
      "offset": 3663.52,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "says, great. Okay, let's open the second",
      "offset": 3665.52,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "thread, whatever. This uh for this for",
      "offset": 3667.28,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "this version of Orurin that we're",
      "offset": 3671.2,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "looking at right now, there's like",
      "offset": 3672.48,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "pretty much nothing in the stateful",
      "offset": 3673.92,
      "duration": 2.56
    },
    {
      "lang": "en",
      "text": "tools except for like one or two test",
      "offset": 3675.2,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "emails, which is why it's like, you",
      "offset": 3676.48,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "know, kind of just like scrambling to",
      "offset": 3678.48,
      "duration": 3.119
    },
    {
      "lang": "en",
      "text": "figure out what to do.",
      "offset": 3680.079,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "Um, so it keeps opening up email threads",
      "offset": 3681.599,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "and then it's checking its calendar,",
      "offset": 3684.319,
      "duration": 2.881
    },
    {
      "lang": "en",
      "text": "right? Looking for upcoming lessons",
      "offset": 3685.92,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "within the next 48 hours. Um, of which",
      "offset": 3687.2,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "there there are none, right? And so, um,",
      "offset": 3689.839,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "all emails are old, no actual messages,",
      "offset": 3693.44,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "whatever. And then it basically decides",
      "offset": 3695.359,
      "duration": 5.921
    },
    {
      "lang": "en",
      "text": "to, uh, go to sleep. And it has just",
      "offset": 3697.839,
      "duration": 5.041
    },
    {
      "lang": "en",
      "text": "like a tool it can call. It's just go to",
      "offset": 3701.28,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "sleep. And it's going to set a time of",
      "offset": 3702.88,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "like where when it wants to sleep until.",
      "offset": 3706.079,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "And the important part is like none of",
      "offset": 3709.04,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "this happened in response to a user",
      "offset": 3710.64,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "input. We do have certain wake events",
      "offset": 3712.48,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "that come in and and all that stuff, but",
      "offset": 3714.4,
      "duration": 2.959
    },
    {
      "lang": "en",
      "text": "like none of it happened in response to",
      "offset": 3716,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "user input. And then we take all of",
      "offset": 3717.359,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "those messages at of what happened",
      "offset": 3719.44,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "during this conversation. And uh we do",
      "offset": 3721.28,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "what we call enshrine them, which is",
      "offset": 3724.319,
      "duration": 2.961
    },
    {
      "lang": "en",
      "text": "just like summarize them and store them",
      "offset": 3725.599,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "in the longer term memory. And so after",
      "offset": 3727.28,
      "duration": 6.079
    },
    {
      "lang": "en",
      "text": "this one run ran um it goes through this",
      "offset": 3729.76,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "tutoring step, right? which is like it",
      "offset": 3733.359,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "takes in or sorry this summarizing step",
      "offset": 3735.68,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "which is like it takes in all of the",
      "offset": 3737.28,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "messages from this uh this history",
      "offset": 3738.72,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "including all of the data including all",
      "offset": 3741.2,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "that stuff. Um",
      "offset": 3742.799,
      "duration": 6.161
    },
    {
      "lang": "en",
      "text": "and then we basically ask it to uh",
      "offset": 3745.52,
      "duration": 7.039
    },
    {
      "lang": "en",
      "text": "summarize what that would look like and",
      "offset": 3748.96,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "somewhere in here oh here we go wait",
      "offset": 3752.559,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "somewhere in here there is a prompt",
      "offset": 3754.48,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "about how to summarize stuff but if it's",
      "offset": 3757.599,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "not in here I can just show you it in",
      "offset": 3760.24,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "the code. Um",
      "offset": 3761.599,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "yeah, let's see.",
      "offset": 3764.48,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "Code is code is always a lot easier.",
      "offset": 3765.839,
      "duration": 6.081
    },
    {
      "lang": "en",
      "text": "Yeah. So like we we have a lot of",
      "offset": 3768.88,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "examples about uh how to summarize",
      "offset": 3771.92,
      "duration": 5.439
    },
    {
      "lang": "en",
      "text": "different types of uh different types of",
      "offset": 3774.4,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "summaries like like direct interactions,",
      "offset": 3777.359,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "daily summaries, whatever. Um, so for",
      "offset": 3779.04,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "for summarizing this direct interaction",
      "offset": 3781.599,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "of like what happened just now as you",
      "offset": 3783.2,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "did this, we care about like I don't",
      "offset": 3785.04,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "know what what mediums did you interact",
      "offset": 3787.359,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "with, who did you talk to, like what did",
      "offset": 3788.88,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "you do? Um, that kind of stuff.",
      "offset": 3790.319,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "And I also find",
      "offset": 3793.119,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "way a lot of I should pause a lot of",
      "offset": 3794.559,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "people over here don't do the stupid",
      "offset": 3796.88,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "thing. Do the DED thing. Don't",
      "offset": 3799.2,
      "duration": 6.32
    },
    {
      "lang": "en",
      "text": "do the DED thing.",
      "offset": 3801.92,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "That's good. Anyway, I'll",
      "offset": 3805.52,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "do the dead thing.",
      "offset": 3807.52,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "And then we give examples of just like",
      "offset": 3810.48,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "what a good summary for a certain",
      "offset": 3812.079,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "interaction like could look like, right?",
      "offset": 3813.52,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "Just, hey, fot prompt this, you know,",
      "offset": 3815.28,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "here's here's what it might look like.",
      "offset": 3816.96,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "Um,",
      "offset": 3818.88,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "and then we've also like",
      "offset": 3820.559,
      "duration": 2.48
    },
    {
      "lang": "en",
      "text": "I want to note one more thing about your",
      "offset": 3821.68,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "fot prompt really fast. Can you go up?",
      "offset": 3823.039,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "The one thing I noticed about the fot",
      "offset": 3824.96,
      "duration": 2.639
    },
    {
      "lang": "en",
      "text": "prompt is you're not actually telling it",
      "offset": 3826.4,
      "duration": 2.639
    },
    {
      "lang": "en",
      "text": "the input. You're only describing",
      "offset": 3827.599,
      "duration": 3.121
    },
    {
      "lang": "en",
      "text": "outputs, which is good because you're",
      "offset": 3829.039,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "basically helping understand the",
      "offset": 3830.72,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "structure of what kinds what is a good",
      "offset": 3832.079,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "summary without actually telling it",
      "offset": 3834.319,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "here's how you summarize this piece of",
      "offset": 3835.92,
      "duration": 3.119
    },
    {
      "lang": "en",
      "text": "content exactly.",
      "offset": 3837.76,
      "duration": 3.279
    },
    {
      "lang": "en",
      "text": "You're giving it like clues about",
      "offset": 3839.039,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "examples, not really prescribing that",
      "offset": 3841.039,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "this is a summary for this example.",
      "offset": 3843.119,
      "duration": 5.121
    },
    {
      "lang": "en",
      "text": "That's a great few shot example. I I you",
      "offset": 3845.2,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "people on the call know that I harp on",
      "offset": 3848.24,
      "duration": 3.119
    },
    {
      "lang": "en",
      "text": "fuchsia all the time. I think it's ass.",
      "offset": 3849.839,
      "duration": 3.121
    },
    {
      "lang": "en",
      "text": "It's because people do it wrong. This is",
      "offset": 3851.359,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "a great way to do fa.",
      "offset": 3852.96,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "Love it. Love it. And we do this for all",
      "offset": 3856.079,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "the different like uh like types of of",
      "offset": 3858.4,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "how long how long we want to chunk up.",
      "offset": 3860.319,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "But also this has been surprisingly",
      "offset": 3862.559,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "helpful just adding this to every like",
      "offset": 3864.24,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "like almost waterfall of memory where",
      "offset": 3866.319,
      "duration": 2.881
    },
    {
      "lang": "en",
      "text": "it's like",
      "offset": 3867.92,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "when you're summarizing the day what",
      "offset": 3869.2,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "should be remembered for the rest of the",
      "offset": 3871.44,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "week, month and year like include all",
      "offset": 3872.799,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "that stuff too. And like we use 03 to do",
      "offset": 3874.24,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "all of our summaries as well.",
      "offset": 3877.119,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "It's pretty good at that. like it it can",
      "offset": 3879.039,
      "duration": 2.641
    },
    {
      "lang": "en",
      "text": "kind of just like figure it out and be",
      "offset": 3880.48,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "like, &quot;Oh, that's an important thing",
      "offset": 3881.68,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "that I should include in today's summary",
      "offset": 3883.68,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "because it should be remembered for a",
      "offset": 3885.359,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "lot longer than just today, right?&quot; And",
      "offset": 3886.96,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "that ends up waterfalling up into like",
      "offset": 3888.96,
      "duration": 5.839
    },
    {
      "lang": "en",
      "text": "becoming like basically getting put into",
      "offset": 3890.96,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "um like some of these buckets and not",
      "offset": 3894.799,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "just this bucket.",
      "offset": 3897.44,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "And the idea here is in the daily",
      "offset": 3898.799,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "summary, if you highlight that this is a",
      "offset": 3900.48,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "good weekly summary note, then when the",
      "offset": 3902.319,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "weekly summary prompt runs, it's like,",
      "offset": 3904.48,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "&quot;Oh,",
      "offset": 3906.72,
      "duration": 1.359
    },
    {
      "lang": "en",
      "text": "yeah, it has that",
      "offset": 3907.28,
      "duration": 2.079
    },
    {
      "lang": "en",
      "text": "part of the daily summary. this is a",
      "offset": 3908.079,
      "duration": 3.121
    },
    {
      "lang": "en",
      "text": "good weekly summary. I should probably",
      "offset": 3909.359,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "take care of this. And if I see too",
      "offset": 3911.2,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "many, I'll pick the best ones out of all",
      "offset": 3913.2,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "of them.",
      "offset": 3914.88,
      "duration": 2.4
    },
    {
      "lang": "en",
      "text": "And that's kind of the idea.",
      "offset": 3916.16,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "And and and you'll see in the future",
      "offset": 3917.28,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "stuff like our examples get more and",
      "offset": 3919.76,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "more broad kind of like how we would",
      "offset": 3921.92,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "want the summaries to get where it's",
      "offset": 3923.44,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "like, you know, you're not really saying",
      "offset": 3924.64,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "much about like specifically at four",
      "offset": 3926.72,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "o'clock we did this thing. It's just",
      "offset": 3928.559,
      "duration": 2.881
    },
    {
      "lang": "en",
      "text": "like, okay, this is the daily summary.",
      "offset": 3929.92,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "So today we did a session here like the",
      "offset": 3931.44,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "parent reached out in this way and kind",
      "offset": 3934.079,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "of we set up set up a plan, right? And",
      "offset": 3935.68,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "then weekly it gets all the daily",
      "offset": 3937.92,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "summaries of that week. Um and so it's",
      "offset": 3939.44,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "like even more broad, right? Like more",
      "offset": 3941.599,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "focus on student progress, more focus on",
      "offset": 3943.68,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "like the pedagogy of all this, right?",
      "offset": 3946.319,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "And then monthly you get you get very",
      "offset": 3948.559,
      "duration": 2.881
    },
    {
      "lang": "en",
      "text": "broad.",
      "offset": 3950.4,
      "duration": 1.439
    },
    {
      "lang": "en",
      "text": "Yes,",
      "offset": 3951.44,
      "duration": 2.159
    },
    {
      "lang": "en",
      "text": "your examples are listed in different",
      "offset": 3951.839,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "formats for different parts of the",
      "offset": 3953.599,
      "duration": 2.881
    },
    {
      "lang": "en",
      "text": "prompts. There's a part where you do",
      "offset": 3955.039,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "example one, example two, and then",
      "offset": 3956.48,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "another part of it where you do",
      "offset": 3958.319,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "um",
      "offset": 3960.96,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "ah true",
      "offset": 3961.52,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "just calling it out. Please go",
      "offset": 3964.4,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "there's only one for the longer ones",
      "offset": 3966.319,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "there's only one example rather than",
      "offset": 3968.079,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "multiple examples",
      "offset": 3970,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "it looks like right",
      "offset": 3971.92,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "these are examples",
      "offset": 3974.16,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "I just",
      "offset": 3976.559,
      "duration": 2.321
    },
    {
      "lang": "en",
      "text": "but then",
      "offset": 3977.92,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "I just changed it from from this",
      "offset": 3978.88,
      "duration": 6.32
    },
    {
      "lang": "en",
      "text": "yeah uh but none of these are run",
      "offset": 3982.72,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "synchronously so it only ever will see",
      "offset": 3985.2,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "one or the other and like you said it's",
      "offset": 3987.039,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "a staple reducer so it's like",
      "offset": 3988.96,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "you know",
      "offset": 3991.28,
      "duration": 2.24
    },
    {
      "lang": "en",
      "text": "I'm sure I could ease out to figure out",
      "offset": 3992.16,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "which one's better but honestly just",
      "offset": 3993.52,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "haven't",
      "offset": 3996,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "and then like we have a more broad like",
      "offset": 3997.44,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "prompt for everything which is like cool",
      "offset": 3999.68,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "here's a who you are making memory",
      "offset": 4002.079,
      "duration": 2.48
    },
    {
      "lang": "en",
      "text": "summary whatever",
      "offset": 4003.119,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "um we also like we want uh we we allow",
      "offset": 4004.559,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "for like long like longer summaries when",
      "offset": 4007.839,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "it gets to more like weekly and monthly",
      "offset": 4009.599,
      "duration": 5.921
    },
    {
      "lang": "en",
      "text": "stuff um and we also we we plug in the",
      "offset": 4011.28,
      "duration": 7.039
    },
    {
      "lang": "en",
      "text": "existing condensed like like summarized",
      "offset": 4015.52,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "memory we call this whole thing just",
      "offset": 4018.319,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "like the memory context like all of the",
      "offset": 4020.16,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "buckets right we plug in the existing",
      "offset": 4021.76,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "memory memory context in the memory",
      "offset": 4024.079,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "summarization prompts which has actually",
      "offset": 4025.68,
      "duration": 5.119
    },
    {
      "lang": "en",
      "text": "been really important, right? Like it we",
      "offset": 4027.119,
      "duration": 5.361
    },
    {
      "lang": "en",
      "text": "found that if you didn't give the",
      "offset": 4030.799,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "summarization prompt all of the memory",
      "offset": 4032.48,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "from before of what's happened in the",
      "offset": 4034.559,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "entire history of this like this or",
      "offset": 4037.039,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "being deployed, uh it it would miss",
      "offset": 4039.359,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "things that were previously identified",
      "offset": 4042.319,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "as important,",
      "offset": 4044.079,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "right? We",
      "offset": 4045.839,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "we saw similar things when we were",
      "offset": 4047.839,
      "duration": 3.121
    },
    {
      "lang": "en",
      "text": "building the AI content pipeline where",
      "offset": 4049.28,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "we had like there are parts of it where",
      "offset": 4050.96,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "giving it the full transcript was useful",
      "offset": 4052.48,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "but at some point we stopped giving it",
      "offset": 4054.48,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "the full transcript and gave it",
      "offset": 4056.24,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "summaries that we wanted to give away",
      "offset": 4057.44,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "and it's the same idea but some steps we",
      "offset": 4058.96,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "did need both the summary and the",
      "offset": 4060.88,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "transcript and other steps we only",
      "offset": 4062.24,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "needed to just",
      "offset": 4064.16,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "like for example here maybe you have a",
      "offset": 4067.2,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "kid who has a history of like some kind",
      "offset": 4069.359,
      "duration": 5.041
    },
    {
      "lang": "en",
      "text": "of executive function problem and that's",
      "offset": 4070.799,
      "duration": 5.841
    },
    {
      "lang": "en",
      "text": "well documented within in the existing",
      "offset": 4074.4,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "memory that's happened before. But then",
      "offset": 4076.64,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "if you're summarizing what happened",
      "offset": 4078.799,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "today, that might not have come out to",
      "offset": 4080,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "play today, which is actually a notable",
      "offset": 4082.16,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "thing that you should summarize, right?",
      "offset": 4084.16,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "Like you should include the fact that",
      "offset": 4086.96,
      "duration": 2.56
    },
    {
      "lang": "en",
      "text": "there were no executive function",
      "offset": 4088.4,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "problems today in today's summary",
      "offset": 4089.52,
      "duration": 3.519
    },
    {
      "lang": "en",
      "text": "because that's notable bas based on the",
      "offset": 4091.28,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "past summaries. And so if you if you",
      "offset": 4093.039,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "don't have all the past context, then it",
      "offset": 4095.039,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "you know, even your summarization it",
      "offset": 4097.6,
      "duration": 5.679
    },
    {
      "lang": "en",
      "text": "kind of goes goes is off. Um,",
      "offset": 4099.6,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "and then yeah, there's not a lot else to",
      "offset": 4103.279,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "the summarization part here. Um,",
      "offset": 4104.96,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "basically we just like fill up the",
      "offset": 4107.359,
      "duration": 1.92
    },
    {
      "lang": "en",
      "text": "string.",
      "offset": 4108.96,
      "duration": 2.08
    },
    {
      "lang": "en",
      "text": "I just want to say by the way, thank you",
      "offset": 4109.279,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "for sticking with the vibe of today.",
      "offset": 4111.04,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "Like I know Dexter and I usually try and",
      "offset": 4112.48,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "actively write code, but uh today I know",
      "offset": 4114.239,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "we had seen Brian's article and we",
      "offset": 4117.04,
      "duration": 3.119
    },
    {
      "lang": "en",
      "text": "thought it was much more useful to see",
      "offset": 4118.56,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "his code rather than like have us write",
      "offset": 4120.159,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "code like we normally do. So I just want",
      "offset": 4122.4,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "to say firstly big thanks to Brian for",
      "offset": 4124.319,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "actually being bold enough to share the",
      "offset": 4126.96,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "code live. Not a lot of people are",
      "offset": 4128.64,
      "duration": 2.559
    },
    {
      "lang": "en",
      "text": "willing to do that, especially for a",
      "offset": 4130.08,
      "duration": 2.639
    },
    {
      "lang": "en",
      "text": "startup that he's building out there.",
      "offset": 4131.199,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "This is a recording so people have",
      "offset": 4132.719,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "access to these for a while. And I'm",
      "offset": 4134.159,
      "duration": 2.961
    },
    {
      "lang": "en",
      "text": "sure Bri is going to make his props",
      "offset": 4135.92,
      "duration": 3.279
    },
    {
      "lang": "en",
      "text": "better because like you said, emails are",
      "offset": 4137.12,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "the things he can't share.",
      "offset": 4139.199,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "Um um",
      "offset": 4140.88,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "and eval with this type of agent are",
      "offset": 4142.56,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "like especially hard.",
      "offset": 4144.4,
      "duration": 4.959
    },
    {
      "lang": "en",
      "text": "I feel pretty confident in our",
      "offset": 4146.48,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "deliverable is that",
      "offset": 4149.359,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "um we should talk about that. Um but",
      "offset": 4150.719,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "with that, I I know we're 10 minutes",
      "offset": 4153.04,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "over, we're 11 minutes over. I want to",
      "offset": 4154.64,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "take the rest of the time to really help",
      "offset": 4156.08,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "answer questions that people might have",
      "offset": 4157.44,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "uh in the chat. I know people wrote a",
      "offset": 4159.52,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "lot of questions in the middle and we",
      "offset": 4161.679,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "kind of glossed over some of them.",
      "offset": 4163.12,
      "duration": 5.119
    },
    {
      "lang": "en",
      "text": "Um but if people want to go back and ask",
      "offset": 4165.359,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "some questions again or raise your hand",
      "offset": 4168.239,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "really fast. Uh we'll let you come off",
      "offset": 4169.759,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "and then we'll we'd love to have you on",
      "offset": 4171.6,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "to do a quick little",
      "offset": 4173.12,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "uh Q&amp;A session for next 15 or 20 minutes",
      "offset": 4174.799,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "as long as Brian's willing to stay on",
      "offset": 4177.44,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "with us.",
      "offset": 4178.64,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "I'm good.",
      "offset": 4180.64,
      "duration": 6.96
    },
    {
      "lang": "en",
      "text": "Cool. I don't see how I can raise a my",
      "offset": 4183.679,
      "duration": 5.921
    },
    {
      "lang": "en",
      "text": "hand here.",
      "offset": 4187.6,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "It's at the bottom in Zoom. There's a",
      "offset": 4189.6,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "thing called raise hand. There you go.",
      "offset": 4191.279,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "Uh but Radu, you since you spoke up,",
      "offset": 4193.12,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "come on up.",
      "offset": 4195.52,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "I'm sorry. Yeah. So, question to",
      "offset": 4196.719,
      "duration": 7.601
    },
    {
      "lang": "en",
      "text": "uh Dexter. So, Dexter, your diagram, I",
      "offset": 4200.719,
      "duration": 6.881
    },
    {
      "lang": "en",
      "text": "showed um context, engineering context,",
      "offset": 4204.32,
      "duration": 6.8
    },
    {
      "lang": "en",
      "text": "and uh I I was keen to see if there is",
      "offset": 4207.6,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "an intent.",
      "offset": 4211.12,
      "duration": 6.88
    },
    {
      "lang": "en",
      "text": "uh part of that um because as you can",
      "offset": 4213.76,
      "duration": 7.439
    },
    {
      "lang": "en",
      "text": "imagine especially in SLMs",
      "offset": 4218,
      "duration": 7.44
    },
    {
      "lang": "en",
      "text": "you don't want um uh any kind of",
      "offset": 4221.199,
      "duration": 7.601
    },
    {
      "lang": "en",
      "text": "question prompts to do to be sent to the",
      "offset": 4225.44,
      "duration": 8.48
    },
    {
      "lang": "en",
      "text": "SLM but uh the if it's MCP tools it",
      "offset": 4228.8,
      "duration": 8.64
    },
    {
      "lang": "en",
      "text": "knows it can do XYZ",
      "offset": 4233.92,
      "duration": 8.64
    },
    {
      "lang": "en",
      "text": "and also um it could uh say that the",
      "offset": 4237.44,
      "duration": 7.36
    },
    {
      "lang": "en",
      "text": "tool description",
      "offset": 4242.56,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "uh matches or does not match the user",
      "offset": 4244.8,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "question intent. So capturing",
      "offset": 4247.6,
      "duration": 6.559
    },
    {
      "lang": "en",
      "text": "inferring that um how how important that",
      "offset": 4250.96,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "is.",
      "offset": 4254.159,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "You're talking about kind of like",
      "offset": 4256.56,
      "duration": 5.119
    },
    {
      "lang": "en",
      "text": "capturing the user's intent to in terms",
      "offset": 4258.719,
      "duration": 6.561
    },
    {
      "lang": "en",
      "text": "of like protection and guard rails.",
      "offset": 4261.679,
      "duration": 6.241
    },
    {
      "lang": "en",
      "text": "uh not just authorization",
      "offset": 4265.28,
      "duration": 5.919
    },
    {
      "lang": "en",
      "text": "um but also um",
      "offset": 4267.92,
      "duration": 7.52
    },
    {
      "lang": "en",
      "text": "think about that um the user",
      "offset": 4271.199,
      "duration": 9.601
    },
    {
      "lang": "en",
      "text": "uh the entire user base asked um all the",
      "offset": 4275.44,
      "duration": 8.239
    },
    {
      "lang": "en",
      "text": "questions are 75%",
      "offset": 4280.8,
      "duration": 7.04
    },
    {
      "lang": "en",
      "text": "are uh for this kind of intent category",
      "offset": 4283.679,
      "duration": 7.761
    },
    {
      "lang": "en",
      "text": "and the rest is that so you you get a",
      "offset": 4287.84,
      "duration": 7.28
    },
    {
      "lang": "en",
      "text": "feel of what the the end user community",
      "offset": 4291.44,
      "duration": 8.16
    },
    {
      "lang": "en",
      "text": "is is how is it using the SLM?",
      "offset": 4295.12,
      "duration": 8.24
    },
    {
      "lang": "en",
      "text": "And of course, yes, uh that once this",
      "offset": 4299.6,
      "duration": 10.639
    },
    {
      "lang": "en",
      "text": "intent uh um u I would say list or",
      "offset": 4303.36,
      "duration": 9.6
    },
    {
      "lang": "en",
      "text": "taxonomy is",
      "offset": 4310.239,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "um",
      "offset": 4312.96,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "more so fleshed out than it could be as",
      "offset": 4314.48,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "a base become as a base for",
      "offset": 4317.6,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "authorization as well.",
      "offset": 4319.76,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "Yeah. Yeah, I think that's probably",
      "offset": 4321.44,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "slightly off topic for the context",
      "offset": 4324.4,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "engineering thing. I understand the",
      "offset": 4326.64,
      "duration": 2.64
    },
    {
      "lang": "en",
      "text": "value of it. Maybe maybe it's another",
      "offset": 4328,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "episode we could tackle.",
      "offset": 4329.28,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "What do you think?",
      "offset": 4332.08,
      "duration": 1.76
    },
    {
      "lang": "en",
      "text": "I agree.",
      "offset": 4333.36,
      "duration": 1.04
    },
    {
      "lang": "en",
      "text": "Yeah, go ahead,",
      "offset": 4333.84,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "John.",
      "offset": 4334.4,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "Yeah. So, I I think I think I know the",
      "offset": 4336.56,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "obvious answer to this, but I just want",
      "offset": 4339.199,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "to validate my thinking. Um, if there's",
      "offset": 4340.32,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "a memory from three months ago that it",
      "offset": 4342.159,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "realizes from that memory dump is",
      "offset": 4345.28,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "important",
      "offset": 4347.679,
      "duration": 2.801
    },
    {
      "lang": "en",
      "text": "or irrelevant is probably a better",
      "offset": 4349.04,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "answer. it's relevant, but it needs more",
      "offset": 4350.48,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "detail about that memory that is not uh",
      "offset": 4353.44,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "that is not there.",
      "offset": 4356,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "Do you just do a tool call to whatever",
      "offset": 4357.84,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "system is obvious from the memory to go",
      "offset": 4359.92,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "get that information? Like is is the",
      "offset": 4362.4,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "memory dump you're giving an indicator",
      "offset": 4364.48,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "of where to get more details and then it",
      "offset": 4366.32,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "does a tool call to get those details",
      "offset": 4368,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "and then it processes that. Is it that",
      "offset": 4369.28,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "simple?",
      "offset": 4371.28,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "Do you want me to write the code real",
      "offset": 4373.04,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "time? Let's do it.",
      "offset": 4374.64,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "Not not function. I think I'm thinking",
      "offset": 4376.4,
      "duration": 2.4
    },
    {
      "lang": "en",
      "text": "of it more in terms of a line of",
      "offset": 4377.76,
      "duration": 2.24
    },
    {
      "lang": "en",
      "text": "thinking. We're doing this in a business",
      "offset": 4378.8,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "context and like that's how we're",
      "offset": 4380,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "architecting it is the way a human",
      "offset": 4381.92,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "would, right? I'm not going to remember",
      "offset": 4384.64,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "every single term of some enterprise",
      "offset": 4385.92,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "deal we did six months ago, but I'm",
      "offset": 4387.52,
      "duration": 2.56
    },
    {
      "lang": "en",
      "text": "going to remember the name. I'm going to",
      "offset": 4388.96,
      "duration": 2.16
    },
    {
      "lang": "en",
      "text": "remember what this guy's talking about",
      "offset": 4390.08,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "today is relevant to that deal and then",
      "offset": 4391.12,
      "duration": 3.039
    },
    {
      "lang": "en",
      "text": "I'm going to go look up our how we dealt",
      "offset": 4392.8,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "with that one, right? Like is that the",
      "offset": 4394.159,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "Brian?",
      "offset": 4396.96,
      "duration": 2.48
    },
    {
      "lang": "en",
      "text": "Brian, do you have like agentic",
      "offset": 4397.679,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "retrieval tools for like drilling down",
      "offset": 4399.44,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "into memory stuff, I guess, is the",
      "offset": 4401.36,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "question to summarize it. I I I don't",
      "offset": 4403.679,
      "duration": 6.401
    },
    {
      "lang": "en",
      "text": "think that the paradigm of",
      "offset": 4407.12,
      "duration": 5.119
    },
    {
      "lang": "en",
      "text": "giving the agent tools to access its own",
      "offset": 4410.08,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "memory is a very good paradigm because",
      "offset": 4412.239,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "it pro to me it just smells that there",
      "offset": 4414.8,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "are like you should just add tools that",
      "offset": 4418.239,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "are act as surrogate memory that makes",
      "offset": 4420.48,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "semantic sense about what you're trying",
      "offset": 4422.48,
      "duration": 2.719
    },
    {
      "lang": "en",
      "text": "to do. So for example it's like maybe",
      "offset": 4423.76,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "you're in a business meeting and it",
      "offset": 4425.199,
      "duration": 2.881
    },
    {
      "lang": "en",
      "text": "needs to remember details about the",
      "offset": 4426.8,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "business meeting. Cool. You shouldn't",
      "offset": 4428.08,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "give it a tool to let it go access its",
      "offset": 4430.159,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "own memory. You should build grain for",
      "offset": 4431.92,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "your agent,",
      "offset": 4434,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "right? Like you you should build a a",
      "offset": 4436.159,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "meeting recorder for your agent that",
      "offset": 4438.4,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "sits in the meetings and then summarizes",
      "offset": 4440.64,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "that in a notepad for it that it can go",
      "offset": 4442.719,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "query later. So like it's kind of the",
      "offset": 4444.32,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "same thing when you think about it, but",
      "offset": 4446.96,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "the framing for the agent actually makes",
      "offset": 4448.719,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "a lot of difference where it's like the",
      "offset": 4450.8,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "agents don't have a good intuition, at",
      "offset": 4452.48,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "least in my experience, of what exactly",
      "offset": 4454.32,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "memories I should go get and like how",
      "offset": 4457.12,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "those interfaces should work and like",
      "offset": 4458.96,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "there's a lot of insample data about",
      "offset": 4460.719,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "that. But if you have a tool that's like",
      "offset": 4462.48,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "here's your notepad, all of your meeting",
      "offset": 4465.679,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "recordings get auto summarized in this",
      "offset": 4467.92,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "notepad. It knows what to do. Like cool,",
      "offset": 4469.679,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "let me go find the meeting that I had",
      "offset": 4472.159,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "with this guy six months ago. only go",
      "offset": 4473.36,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "like query you know the recording for",
      "offset": 4475.199,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "that meeting and get the transcript or",
      "offset": 4476.96,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "get the summary points",
      "offset": 4478.159,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "what's the difference in that case of",
      "offset": 4479.84,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "making clear that fathom or granola is a",
      "offset": 4482.159,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "tool it can use or pipe drive is the",
      "offset": 4484.32,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "sales CRM or those types of things like",
      "offset": 4486.159,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "what is there a functional difference of",
      "offset": 4488.159,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "why you wouldn't do it like that",
      "offset": 4489.679,
      "duration": 5.281
    },
    {
      "lang": "en",
      "text": "just accessing these third party tools",
      "offset": 4492.159,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "yeah I mean if for us it doesn't doesn't",
      "offset": 4494.96,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "quite work because we have to like build",
      "offset": 4497.12,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "our own internal tools for it but yeah",
      "offset": 4498.4,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "if you can just use Fathom like do that",
      "offset": 4500,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "man like just have Fathom join all your",
      "offset": 4502.56,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "your agents calls and then web hook that",
      "offset": 4504.8,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "in to like store the data somewhere and",
      "offset": 4507.36,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "then just tell it like hey we use Fathom",
      "offset": 4508.96,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "like go get your Fathom stuff and like",
      "offset": 4511.52,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "it will have insample data about what",
      "offset": 4513.76,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "Fathom is and how to use it. Like",
      "offset": 4515.92,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "we have our own little uh like version",
      "offset": 4517.84,
      "duration": 5.359
    },
    {
      "lang": "en",
      "text": "of like a Zoom client that we use.",
      "offset": 4520.32,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "It just wants to use Zoom like it",
      "offset": 4523.199,
      "duration": 3.281
    },
    {
      "lang": "en",
      "text": "continuously tells users that it's just",
      "offset": 4524.88,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "going to send it a Zoom link and like no",
      "offset": 4526.48,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "it's not a Zoom link you know but like",
      "offset": 4527.76,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "it's it's biased toward wanting to use",
      "offset": 4529.44,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "the in sample tool. So if you can do",
      "offset": 4531.679,
      "duration": 3.121
    },
    {
      "lang": "en",
      "text": "that like just give them the example",
      "offset": 4533.28,
      "duration": 4.919
    },
    {
      "lang": "en",
      "text": "tools man.",
      "offset": 4534.8,
      "duration": 3.399
    },
    {
      "lang": "en",
      "text": "Got it. I think that's the part I need",
      "offset": 4538.32,
      "duration": 2.399
    },
    {
      "lang": "en",
      "text": "to dig into.",
      "offset": 4539.6,
      "duration": 2.559
    },
    {
      "lang": "en",
      "text": "How how how will it choose the right",
      "offset": 4540.719,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "one? I I'll dig into that particular",
      "offset": 4542.159,
      "duration": 2.801
    },
    {
      "lang": "en",
      "text": "part.",
      "offset": 4543.679,
      "duration": 2.56
    },
    {
      "lang": "en",
      "text": "John, to highlight that point a little",
      "offset": 4544.96,
      "duration": 2.56
    },
    {
      "lang": "en",
      "text": "bit, we talked about this a little bit",
      "offset": 4546.239,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "in our 12 factor agents episode. And I",
      "offset": 4547.52,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "think this is the part that a lot of",
      "offset": 4550.64,
      "duration": 2.4
    },
    {
      "lang": "en",
      "text": "people think about which is like how do",
      "offset": 4551.84,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "you define these concepts? But I think",
      "offset": 4553.04,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "what you really want to say is there's a",
      "offset": 4555.28,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "decision point that the LLM or some",
      "offset": 4557.36,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "function is going to make that says",
      "offset": 4559.36,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "instead of emitting the final result",
      "offset": 4560.88,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "that I want. I wanted to request more",
      "offset": 4562.48,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "information.",
      "offset": 4564.08,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "So in this case, I have a really simple",
      "offset": 4565.679,
      "duration": 3.281
    },
    {
      "lang": "en",
      "text": "thing like extract a resume. I'm going",
      "offset": 4567.28,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "to use a really really basic example so",
      "offset": 4568.96,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "we can all go over it. It's going to go",
      "offset": 4570.32,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "do this thing and like we have boundary",
      "offset": 4572.64,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "ML over here and what I'll say is",
      "offset": 4574.08,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "company startup",
      "offset": 4576.48,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "or enterprise. I'll just say that",
      "offset": 4578.88,
      "duration": 2.799
    },
    {
      "lang": "en",
      "text": "there's a company type over here and I",
      "offset": 4580.4,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "want to return what company type it has.",
      "offset": 4581.679,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "Um, and I've just wrote the prompt if",
      "offset": 4584,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "information about if information is",
      "offset": 4585.679,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "missing, request more information before",
      "offset": 4587.6,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "continuing,",
      "offset": 4590.56,
      "duration": 5.599
    },
    {
      "lang": "en",
      "text": "right? And when I go run this prompt,",
      "offset": 4592.96,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "you'll see what I'm actually doing. I'm",
      "offset": 4596.159,
      "duration": 2.56
    },
    {
      "lang": "en",
      "text": "literally just telling the model like",
      "offset": 4597.52,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "either dump out the resume",
      "offset": 4598.719,
      "duration": 5.681
    },
    {
      "lang": "en",
      "text": "or request more information. That's it.",
      "offset": 4601.52,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "And I might even put into here like",
      "offset": 4604.4,
      "duration": 5.759
    },
    {
      "lang": "en",
      "text": "action resume uh extract",
      "offset": 4606.08,
      "duration": 6.159
    },
    {
      "lang": "en",
      "text": "complete. And I'm naming this very",
      "offset": 4610.159,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "differently than the previous one. I'm",
      "offset": 4612.239,
      "duration": 3.041
    },
    {
      "lang": "en",
      "text": "naming it extract complete resume",
      "offset": 4613.6,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "intuitively.",
      "offset": 4615.28,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "And this thing is not complete. It",
      "offset": 4616.8,
      "duration": 2.48
    },
    {
      "lang": "en",
      "text": "doesn't have a start date. It doesn't",
      "offset": 4618.4,
      "duration": 1.92
    },
    {
      "lang": "en",
      "text": "have an end date. It doesn't have",
      "offset": 4619.28,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "anything. And I'll make requests um a",
      "offset": 4620.32,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "bunch of this. And I'll even put like a",
      "offset": 4624.08,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "reason kind of thing in here. Why not?",
      "offset": 4625.36,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "Reason string.",
      "offset": 4627.84,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "And I'll say here's all the requests I",
      "offset": 4630.48,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "have. And I should probably name this",
      "offset": 4632.239,
      "duration": 2.561
    },
    {
      "lang": "en",
      "text": "request.",
      "offset": 4633.28,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "And when I go run this in theory, I",
      "offset": 4634.8,
      "duration": 2.56
    },
    {
      "lang": "en",
      "text": "don't know if this will do the right",
      "offset": 4636.48,
      "duration": 2.719
    },
    {
      "lang": "en",
      "text": "thing.",
      "offset": 4637.36,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "It did the right thing.",
      "offset": 4639.199,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "it was able to go through and decide",
      "offset": 4642.159,
      "duration": 2.481
    },
    {
      "lang": "en",
      "text": "that it doesn't have it doesn't have",
      "offset": 4643.44,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "experience for the company name the full",
      "offset": 4644.64,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "name associated with initial VBV is",
      "offset": 4646.48,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "different from vivov gupta just go",
      "offset": 4648.56,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "double check everything so now you can",
      "offset": 4650,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "imagine what I would do in my python",
      "offset": 4652.64,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "code is I can then opt into what I want",
      "offset": 4654.239,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "to go into here so I can do something",
      "offset": 4657.199,
      "duration": 4.441
    },
    {
      "lang": "en",
      "text": "like",
      "offset": 4658.64,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "and I think it it ends up being the same",
      "offset": 4663.28,
      "duration": 2.959
    },
    {
      "lang": "en",
      "text": "concept to what you're talking about",
      "offset": 4665.199,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "from client import bh",
      "offset": 4666.239,
      "duration": 6.241
    },
    {
      "lang": "en",
      "text": "sorry import",
      "offset": 4669.36,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "So when I get like a res is it going to",
      "offset": 4672.48,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "be extract resume and I actually pass in",
      "offset": 4675.28,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "like the string into here whatever the",
      "offset": 4677.76,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "string ends up being this thing is going",
      "offset": 4679.44,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "to be either request more information",
      "offset": 4681.28,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "type or resume type. So now I have a",
      "offset": 4682.64,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "choice of what to do",
      "offset": 4685.12,
      "duration": 7.039
    },
    {
      "lang": "en",
      "text": "if it's a client.types types",
      "offset": 4688.08,
      "duration": 8
    },
    {
      "lang": "en",
      "text": "import request more information if now I",
      "offset": 4692.159,
      "duration": 5.361
    },
    {
      "lang": "en",
      "text": "can basically go ahead and handle these",
      "offset": 4696.08,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "requests in some way and that can be an",
      "offset": 4697.52,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "agentic loop it can be something else it",
      "offset": 4699.28,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "can be and now what I have is like",
      "offset": 4701.12,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "initial resume",
      "offset": 4704.8,
      "duration": 5.919
    },
    {
      "lang": "en",
      "text": "uh state is going to be like a",
      "offset": 4707.28,
      "duration": 5.919
    },
    {
      "lang": "en",
      "text": "resume stir",
      "offset": 4710.719,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "why I pass in a resume stir into this",
      "offset": 4713.199,
      "duration": 5.761
    },
    {
      "lang": "en",
      "text": "and this can just be like a state how do",
      "offset": 4714.96,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "I do",
      "offset": 4718.96,
      "duration": 1.84
    },
    {
      "lang": "en",
      "text": "I mean,",
      "offset": 4720.64,
      "duration": 1.36
    },
    {
      "lang": "en",
      "text": "you're basically building a tool calling",
      "offset": 4720.8,
      "duration": 2.48
    },
    {
      "lang": "en",
      "text": "loop. I think this is kind of what John",
      "offset": 4722,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "proposed is like, do I give it tools or",
      "offset": 4723.28,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "structured outputs that it can give and",
      "offset": 4725.199,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "like yes, your code can decide whatever",
      "offset": 4726.8,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "you want to do with those outputs,",
      "offset": 4728.4,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "whether it's just direct query from Zoom",
      "offset": 4730.239,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "and stuff it back in the context window",
      "offset": 4732.32,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "or you want to send a Slack message to",
      "offset": 4734.159,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "somebody who can help or whatever it is.",
      "offset": 4735.92,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "Is like the model's only going to just",
      "offset": 4737.6,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "say, &quot;Hey, I need more information about",
      "offset": 4739.28,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "this stuff.&quot; You can decide how you",
      "offset": 4740.56,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "handle that.",
      "offset": 4742.48,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "And and the thing that decides whether",
      "offset": 4743.52,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "you need more information or not is just",
      "offset": 4745.44,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "the function. The fact that this",
      "offset": 4747.36,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "function happens to use a model is an",
      "offset": 4748.8,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "implementation detail of how I did this.",
      "offset": 4750.56,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "You could write a different function",
      "offset": 4753.12,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "that is not an LM. For example, you",
      "offset": 4754.48,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "could write a simple function that's if",
      "offset": 4756.48,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "the word bank statement is in there like",
      "offset": 4757.679,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "a simple reax match, then I will always",
      "offset": 4760.56,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "request information about the Chase bank",
      "offset": 4762.88,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "account associated with this account.",
      "offset": 4764.64,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "I will just require that. In fact, I'll",
      "offset": 4766.719,
      "duration": 2.881
    },
    {
      "lang": "en",
      "text": "tell the user if you haven't connected",
      "offset": 4768.48,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "the plaid can't do this action. I won't",
      "offset": 4769.6,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "even go to an LLM.",
      "offset": 4772.08,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "But that's the whole point of thinking",
      "offset": 4773.679,
      "duration": 2.721
    },
    {
      "lang": "en",
      "text": "about everything as a function",
      "offset": 4775.36,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "signature.",
      "offset": 4776.4,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "Does that answer the question really",
      "offset": 4778.08,
      "duration": 1.76
    },
    {
      "lang": "en",
      "text": "well?",
      "offset": 4779.28,
      "duration": 1.84
    },
    {
      "lang": "en",
      "text": "Yeah. Yeah, that makes sense. I think",
      "offset": 4779.84,
      "duration": 2.319
    },
    {
      "lang": "en",
      "text": "we're on the right track with the",
      "offset": 4781.12,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "pattern. Um, I appreciate it. Thank you.",
      "offset": 4782.159,
      "duration": 6.281
    },
    {
      "lang": "en",
      "text": "Uh, Derek,",
      "offset": 4785.12,
      "duration": 3.32
    },
    {
      "lang": "en",
      "text": "thanks. Took me a second to come off",
      "offset": 4790.64,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "mute. Um, just a quick comment that's",
      "offset": 4792.239,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "worked well for us is um, owl time. Uh,",
      "offset": 4794.56,
      "duration": 6.72
    },
    {
      "lang": "en",
      "text": "specifically the Allen algebra with",
      "offset": 4798.719,
      "duration": 5.841
    },
    {
      "lang": "en",
      "text": "temporal um, like time intervals. So if",
      "offset": 4801.28,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "you have multiple agents working as a",
      "offset": 4804.56,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "team uh you know with the turns in the",
      "offset": 4806.64,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "conversation it can help to keep track",
      "offset": 4809.36,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "of what happens in what sequence.",
      "offset": 4811.76,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "Um so I know you've got a different",
      "offset": 4814.239,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "notion of the roll up with time. Um I'd",
      "offset": 4815.84,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "just be curious if you've thought about",
      "offset": 4818.64,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "any of the temporal or if you've got",
      "offset": 4820.239,
      "duration": 6.401
    },
    {
      "lang": "en",
      "text": "agents working together. Um I appreciate",
      "offset": 4822.719,
      "duration": 6.721
    },
    {
      "lang": "en",
      "text": "what you said. I think it was not we use",
      "offset": 4826.64,
      "duration": 5.599
    },
    {
      "lang": "en",
      "text": "mentoring but you used uh the term",
      "offset": 4829.44,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "tutoring. Uh so we've got like you know",
      "offset": 4832.239,
      "duration": 5.841
    },
    {
      "lang": "en",
      "text": "a whole thing with that. Uh but the",
      "offset": 4835.36,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "larger the teams get that are working",
      "offset": 4838.08,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "collaboratively",
      "offset": 4840.159,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "uh we found it to be pretty complex.",
      "offset": 4841.679,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "We've maxed out at about kind of 25 on",
      "offset": 4843.52,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "one team to be able to manage that. So",
      "offset": 4846.159,
      "duration": 5.761
    },
    {
      "lang": "en",
      "text": "I'd be curious of any aspects about uh",
      "offset": 4849.44,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "the temporal relationships you have.",
      "offset": 4851.92,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "Thank you for Thank you for today. This",
      "offset": 4853.76,
      "duration": 5.479
    },
    {
      "lang": "en",
      "text": "has been really great.",
      "offset": 4855.44,
      "duration": 3.799
    },
    {
      "lang": "en",
      "text": "Could you repeat the question again?",
      "offset": 4859.76,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "Sorry. Uh",
      "offset": 4861.04,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "so you said owl time which is I believe",
      "offset": 4862.64,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "the like ontology knowledge graphy like",
      "offset": 4864.88,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "expressions of time right?",
      "offset": 4867.52,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "Yeah specifically and in fact using that",
      "offset": 4870.08,
      "duration": 6.159
    },
    {
      "lang": "en",
      "text": "with BAML and Neo4j as as well but uh of",
      "offset": 4872.48,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "ontology of temporal relationships with",
      "offset": 4876.239,
      "duration": 5.681
    },
    {
      "lang": "en",
      "text": "time there are 13 time intervals that",
      "offset": 4878.64,
      "duration": 8
    },
    {
      "lang": "en",
      "text": "are defined uh uh you know during",
      "offset": 4881.92,
      "duration": 7.6
    },
    {
      "lang": "en",
      "text": "etc. Uh yeah, so this has been we've",
      "offset": 4886.64,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "integrated this over the last like six",
      "offset": 4889.52,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "months uh to to for so everything kind",
      "offset": 4891.6,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "of happens those ones at the bottom the",
      "offset": 4895.12,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "relationships and their inverses is a",
      "offset": 4897.04,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "clear way to look at it. Um so when you",
      "offset": 4899.36,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "have multiple agents running together",
      "offset": 4901.92,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "kind of in stasis and then coming back",
      "offset": 4904.64,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "where they're active. uh with",
      "offset": 4906.88,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "multi-threading. This is how we keep",
      "offset": 4909.36,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "those async operations amongst a",
      "offset": 4911.04,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "collaborative team",
      "offset": 4913.52,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "uh all working together with different",
      "offset": 4915.28,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "roles uh and instantiations of those",
      "offset": 4917.04,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "threads because we don't just like",
      "offset": 4920.96,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "instantiate one and run it. We run like",
      "offset": 4922.48,
      "duration": 6.159
    },
    {
      "lang": "en",
      "text": "the full context window uh typically um",
      "offset": 4924.48,
      "duration": 6.88
    },
    {
      "lang": "en",
      "text": "and have a whole onboarding process",
      "offset": 4928.639,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "for for an agent. So I just was curious",
      "offset": 4931.36,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "if you've incorporated anything. I",
      "offset": 4934.159,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "became aware a few months ago of of what",
      "offset": 4936.08,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "you were doing with these different",
      "offset": 4938.08,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "grouped rollups. It was a nice nice",
      "offset": 4940,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "thing or just take a different approach.",
      "offset": 4942.4,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "Um but I' I'd be curious. Thank thanks",
      "offset": 4945.199,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "again for your time.",
      "offset": 4947.28,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "I haven't personally tried that",
      "offset": 4949.28,
      "duration": 2.959
    },
    {
      "lang": "en",
      "text": "approach. Uh it looks really",
      "offset": 4950.719,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "interesting. I think um in general all",
      "offset": 4952.239,
      "duration": 5.761
    },
    {
      "lang": "en",
      "text": "these attempts that I see are always",
      "offset": 4955.679,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "around like this idea of like somehow",
      "offset": 4958,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "there's this loose structure of like",
      "offset": 4959.84,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "what is a prompt and it's these really",
      "offset": 4961.84,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "sequence of tokens and all these",
      "offset": 4964.08,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "attempts are just attempts to like put",
      "offset": 4965.52,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "into some structured format that is a",
      "offset": 4967.36,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "little bit less loose than just like raw",
      "offset": 4969.6,
      "duration": 3.119
    },
    {
      "lang": "en",
      "text": "strings and that looks like another",
      "offset": 4970.96,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "really interesting format like I would",
      "offset": 4972.719,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "be really keen to hear more about your",
      "offset": 4974.159,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "feedback as you've been doing this and",
      "offset": 4976.719,
      "duration": 3.041
    },
    {
      "lang": "en",
      "text": "like what learnings you had maybe in",
      "offset": 4978.4,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "another episode Derek and it' be really",
      "offset": 4979.76,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "especially",
      "offset": 4981.6,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "if you're willing to share some of the",
      "offset": 4983.28,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "code. I think that the idea of",
      "offset": 4984.4,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "connecting it from knowledge graphs to",
      "offset": 4986.8,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "entities and relationships over time is",
      "offset": 4989.199,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "a problem that a lot of people are going",
      "offset": 4991.12,
      "duration": 5.079
    },
    {
      "lang": "en",
      "text": "to have long term.",
      "offset": 4992.56,
      "duration": 3.639
    },
    {
      "lang": "en",
      "text": "Oh god. Um, one thing I just want to tap",
      "offset": 4996.88,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "into that was brought up a couple times",
      "offset": 4998.8,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "and um, that Brian mentioned is this",
      "offset": 5000.4,
      "duration": 3.279
    },
    {
      "lang": "en",
      "text": "idea. It's actually in 12 factor agents",
      "offset": 5002.08,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "is factor 13 is um, I think Brian",
      "offset": 5003.679,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "mentioned something about like why why",
      "offset": 5006.159,
      "duration": 3.121
    },
    {
      "lang": "en",
      "text": "let me know if I'm paraphrasing this",
      "offset": 5007.92,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "right um, but like the idea of like why",
      "offset": 5009.28,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "wait for the agent to call a tool to get",
      "offset": 5011.92,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "information it needs when like if you",
      "offset": 5013.92,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "know the agent will always need that",
      "offset": 5015.679,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "information or will always be useful",
      "offset": 5017.12,
      "duration": 3.039
    },
    {
      "lang": "en",
      "text": "just inject just fetch it yourself and",
      "offset": 5018.719,
      "duration": 2.881
    },
    {
      "lang": "en",
      "text": "inject it into the context like",
      "offset": 5020.159,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "deterministically.",
      "offset": 5021.6,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "Yes.",
      "offset": 5024.08,
      "duration": 1.04
    },
    {
      "lang": "en",
      "text": "Is that right?",
      "offset": 5024.48,
      "duration": 2.159
    },
    {
      "lang": "en",
      "text": "Yes.",
      "offset": 5025.12,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "Yeah. This is why I'm like not always",
      "offset": 5026.639,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "the biggest fan of of like letting the",
      "offset": 5028.719,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "agent control what like memory is in",
      "offset": 5031.36,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "context because",
      "offset": 5034.639,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "all of the stuff that are in our like",
      "offset": 5036.8,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "canonical memory should be in context",
      "offset": 5038.48,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "all the time. It's important, right? If",
      "offset": 5040.48,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "it shouldn't be in context all the time,",
      "offset": 5043.199,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "it should probably be in a separate",
      "offset": 5044.719,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "siloed state full tool is our that's",
      "offset": 5046.639,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "kind of our philosophy around it where",
      "offset": 5049.04,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "it's like it's like why do you think",
      "offset": 5050.4,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "that should be only in there sometimes",
      "offset": 5052.48,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "and like what about that information",
      "offset": 5054.159,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "can't be stored in like a separate tool",
      "offset": 5056.08,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "that makes like semantic sense.",
      "offset": 5057.76,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "Nice. 100%.",
      "offset": 5060.48,
      "duration": 5.64
    },
    {
      "lang": "en",
      "text": "All right, we'll do one more question.",
      "offset": 5062.4,
      "duration": 3.72
    },
    {
      "lang": "en",
      "text": "Josh,",
      "offset": 5066.48,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "hey, thanks. Hey, thanks um Brian. Um",
      "offset": 5066.88,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "it's usually helpful. So, two questions.",
      "offset": 5070.4,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "one um in the um in your blog you",
      "offset": 5072.48,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "started with the hey we saw problem. So",
      "offset": 5075.92,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "uh first question is that the after you",
      "offset": 5077.84,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "implemented the this memory what did you",
      "offset": 5079.52,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "see what behavior change did you see or",
      "offset": 5082.159,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "benefit did you see and second question",
      "offset": 5084.159,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "is that the um at the bottom now next",
      "offset": 5085.76,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "steps you kind of highlight that the uh",
      "offset": 5088.4,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "better like um more persistent temporal",
      "offset": 5090.48,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "memory why did you select that as a next",
      "offset": 5092.88,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "step as versus like slicing by different",
      "offset": 5095.28,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "topics or whatever so I'm kind of",
      "offset": 5097.679,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "curious about those two",
      "offset": 5100.159,
      "duration": 6.161
    },
    {
      "lang": "en",
      "text": "we kind of do with with topical things",
      "offset": 5103.199,
      "duration": 5.761
    },
    {
      "lang": "en",
      "text": "um Again, topical information should be",
      "offset": 5106.32,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "in context some of the time. It's kind",
      "offset": 5108.96,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "of our viewpoint. It's like it's useful",
      "offset": 5110.639,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "for some things and not for others. And",
      "offset": 5112.32,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "so, like we we've basically we're",
      "offset": 5114,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "building an internal LMS right now for",
      "offset": 5116.239,
      "duration": 5.121
    },
    {
      "lang": "en",
      "text": "our agent to use which stores topical",
      "offset": 5118.56,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "information about like the students",
      "offset": 5121.36,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "progress and kind of the frontier of",
      "offset": 5123.28,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "their knowledge, all that stuff, right?",
      "offset": 5125.199,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "I mean,",
      "offset": 5128.159,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "if you had like a a kind of like a tutor",
      "offset": 5129.76,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "who was like maybe had a memory issue,",
      "offset": 5131.76,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "like a human tutor that just couldn't",
      "offset": 5134.239,
      "duration": 2.561
    },
    {
      "lang": "en",
      "text": "remember stuff very well, like you'd",
      "offset": 5135.44,
      "duration": 2.719
    },
    {
      "lang": "en",
      "text": "want to give them all these tools to",
      "offset": 5136.8,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "enable them to to be able to still tutor",
      "offset": 5138.159,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "somebody effectively, right? And so we",
      "offset": 5140.08,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "want the temporal side of like",
      "offset": 5142.08,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "relationship management, which is",
      "offset": 5143.52,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "honestly mostly for for like",
      "offset": 5145.04,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "interactions with the family and not",
      "offset": 5147.44,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "actually for the the tutoring quality",
      "offset": 5148.88,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "itself. And then we're gonna we're",
      "offset": 5150.8,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "building like tools around storing",
      "offset": 5152.32,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "certain information about what the",
      "offset": 5154.719,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "person knows. Um that like maybe you",
      "offset": 5156.239,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "could use a graph to represent that in",
      "offset": 5159.679,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "like a different a different context. Um",
      "offset": 5161.199,
      "duration": 5.281
    },
    {
      "lang": "en",
      "text": "but like the point is like silo out kind",
      "offset": 5164,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "of what your memory should be. We care",
      "offset": 5166.48,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "about the temporal part because we want",
      "offset": 5168.8,
      "duration": 2.56
    },
    {
      "lang": "en",
      "text": "it to be like a relationship that you",
      "offset": 5170,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "have for months and years, but then also",
      "offset": 5171.36,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "we care about things like uh like",
      "offset": 5173.6,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "topical information, but we just store",
      "offset": 5176,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "that somewhere else, right?",
      "offset": 5177.52,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "Hopefully that answers the question.",
      "offset": 5180.48,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "Got it. And the first question is what",
      "offset": 5182.239,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "benefit did you see after you implement",
      "offset": 5183.76,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "this?",
      "offset": 5186.08,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "Well, we didn't really anything before",
      "offset": 5188,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "this like this was our we started",
      "offset": 5190.159,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "building this from scratch basically. Um",
      "offset": 5191.92,
      "duration": 5.279
    },
    {
      "lang": "en",
      "text": "but the biggest benefit is like for us",
      "offset": 5194.8,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "parents actually feel like this thing is",
      "offset": 5197.199,
      "duration": 6.96
    },
    {
      "lang": "en",
      "text": "responsible and will like like it feels",
      "offset": 5200.32,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "less like a tool and more like a service",
      "offset": 5204.159,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "because it has this temporal memory. It",
      "offset": 5206,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "can be proactive.",
      "offset": 5207.679,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "um like it'll hit you up and be like,",
      "offset": 5209.36,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "&quot;Hey, do you want to do this stuff for",
      "offset": 5211.199,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "your kid this month or like you know,",
      "offset": 5213.6,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "you mentioned going on vacation, I'll",
      "offset": 5215.6,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "I'll give you this week off, you know,",
      "offset": 5217.12,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "stuff like that.&quot; Things that would have",
      "offset": 5219.199,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "had to been built into like very",
      "offset": 5221.44,
      "duration": 2.719
    },
    {
      "lang": "en",
      "text": "specific workflows without temporal",
      "offset": 5222.639,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "memory are now just they just happen,",
      "offset": 5224.159,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "right?",
      "offset": 5226.88,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "I think I think one of the similar ideas",
      "offset": 5228.08,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "is uh like when when we first do like",
      "offset": 5231.6,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "use like something like Google search or",
      "offset": 5234.08,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "like any sort of new technology for the",
      "offset": 5235.6,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "first time there's these wow factors",
      "offset": 5237.84,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "that we look for that make it really",
      "offset": 5239.199,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "amazing and Brian's technique that he",
      "offset": 5241.52,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "applied here gave his parents the first",
      "offset": 5243.84,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "wow factor in AI which is proactivity",
      "offset": 5245.92,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "I think when everyone knows that you can",
      "offset": 5248.639,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "go try and have it go do something the",
      "offset": 5250.239,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "magic is when somebody texts you and",
      "offset": 5252.719,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "says does the right thing happen it's",
      "offset": 5254.239,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "like kind of like the email when we The",
      "offset": 5256.08,
      "duration": 3.119
    },
    {
      "lang": "en",
      "text": "fact that the email doesn't feel super",
      "offset": 5257.679,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "automated. It feels somewhat human",
      "offset": 5259.199,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "written.",
      "offset": 5261.04,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "That's what makes AI feel good. And this",
      "offset": 5262.159,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "temporal memory strategy",
      "offset": 5264.719,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "did that for Brian in his use case for",
      "offset": 5266.719,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "the constraints that he defined for the",
      "offset": 5268.96,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "evals that he wrote. And it looks like",
      "offset": 5270.48,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "they did a lot of work to make it work",
      "offset": 5272.88,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "for their problem, but that required a",
      "offset": 5275.84,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "deep understanding of their problem and",
      "offset": 5277.44,
      "duration": 3.279
    },
    {
      "lang": "en",
      "text": "their users to go give them that wow",
      "offset": 5278.8,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "factor. For everyone else that's going",
      "offset": 5280.719,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "into this, it's not to say that DRM is",
      "offset": 5282.8,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "going to solve your problems.",
      "offset": 5285.199,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "The point here is understand your",
      "offset": 5287.36,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "problem. Understand that one wow moment",
      "offset": 5289.12,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "that you really truly need for your",
      "offset": 5291.6,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "business situation to work and figure",
      "offset": 5293.44,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "out what data structures you need to",
      "offset": 5296.08,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "solve that problem. These are all just",
      "offset": 5297.92,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "fundamentally data structures and",
      "offset": 5300.159,
      "duration": 2.961
    },
    {
      "lang": "en",
      "text": "algorithms problems. We're going to have",
      "offset": 5301.44,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "leak code. Leak code is not going away.",
      "offset": 5303.12,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "Leak code is just about to get a whole",
      "offset": 5304.8,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "lot [Â __Â ] harder. That's all that's",
      "offset": 5306.08,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "gonna happen. Um,",
      "offset": 5307.84,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "yeah. I think",
      "offset": 5310.159,
      "duration": 7.04
    },
    {
      "lang": "en",
      "text": "I think this like DRM productivity thing",
      "offset": 5313.36,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "is probably going to hurt your business",
      "offset": 5317.199,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "if you're not selling a service",
      "offset": 5318.88,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "honestly. Like like if you're trying to",
      "offset": 5320.719,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "sell a tool, I would stay away from",
      "offset": 5322.48,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "that, right? Like the people who are",
      "offset": 5324.159,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "using tools want like the kind of cursor",
      "offset": 5326.48,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "feel where it's like it's not quite like",
      "offset": 5328.96,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "I don't like think about like an",
      "offset": 5331.36,
      "duration": 2.799
    },
    {
      "lang": "en",
      "text": "individual that I'm working with, you",
      "offset": 5332.719,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "know? like I'm I'm able to to mess with",
      "offset": 5334.159,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "stuff and like just just do it at my",
      "offset": 5337.199,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "speed, right? But if you're trying to",
      "offset": 5339.12,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "sell this like full stack service,",
      "offset": 5340.8,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "temporal memory is like super important",
      "offset": 5343.04,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "for just continuity, right? Of like",
      "offset": 5345.199,
      "duration": 5.361
    },
    {
      "lang": "en",
      "text": "like just like the normal like PM [Â __Â ]",
      "offset": 5347.76,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "Like",
      "offset": 5350.56,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "if I have an EA for example, like that",
      "offset": 5351.76,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "EA if it's an AI agent,",
      "offset": 5354.719,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "it better not need to be reminded what",
      "offset": 5357.12,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "the [Â __Â ] BAMO is every single week. Like",
      "offset": 5358.96,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "that just doesn't work. and it should",
      "offset": 5361.04,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "know the vision of what your company is",
      "offset": 5362.8,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "doing every single week and like be able",
      "offset": 5364.56,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "to like understand semantically what the",
      "offset": 5366.239,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "notion is going on on top of my head and",
      "offset": 5368.32,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "what's top of mind today and the best EA",
      "offset": 5370.08,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "are the ones that proactively go do",
      "offset": 5373.44,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "things so whoever builds that service is",
      "offset": 5374.96,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "going to have to go build something like",
      "offset": 5378,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "this whether it's CRM or some other data",
      "offset": 5379.92,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "structure that ends up being even better",
      "offset": 5382.32,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "or not but this is a pretty good start",
      "offset": 5383.679,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "um",
      "offset": 5386.96,
      "duration": 2.56
    },
    {
      "lang": "en",
      "text": "the other warning I will give about",
      "offset": 5387.6,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "proactive stuff just just real quick on",
      "offset": 5389.52,
      "duration": 2.719
    },
    {
      "lang": "en",
      "text": "the last",
      "offset": 5391.44,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "is uh if you want to be proactive and",
      "offset": 5392.239,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "you're giving the keys of like",
      "offset": 5396.159,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "scheduling to the agent itself, you now",
      "offset": 5397.44,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "have to work with time zones again. And",
      "offset": 5399.84,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "that is a whole mode in itself because",
      "offset": 5403.04,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "the models are not good at time zones",
      "offset": 5405.52,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "and you have to do a lot of",
      "offset": 5407.6,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "normalization and augmentation of your",
      "offset": 5409.04,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "context windows to make it look nice and",
      "offset": 5411.12,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "have and basically cater to the model",
      "offset": 5413.679,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "about what like how they should output",
      "offset": 5415.199,
      "duration": 2.401
    },
    {
      "lang": "en",
      "text": "stuff.",
      "offset": 5416.719,
      "duration": 2.561
    },
    {
      "lang": "en",
      "text": "And would that I can write a whole",
      "offset": 5417.6,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "article about that. with that.",
      "offset": 5419.28,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "Don't try to solve time zones in a",
      "offset": 5421.199,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "prompt, right?",
      "offset": 5422.639,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "Yeah.",
      "offset": 5425.76,
      "duration": 2.16
    },
    {
      "lang": "en",
      "text": "Sorry. What did you say?",
      "offset": 5426.08,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "I said with that, we're back to context",
      "offset": 5427.92,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "engineering. It's all context",
      "offset": 5429.52,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "engineering. Put the right tokens in,",
      "offset": 5430.96,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "you get really good tokens out, but",
      "offset": 5432.719,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "understand the problem. Put great tokens",
      "offset": 5434.56,
      "duration": 2.4
    },
    {
      "lang": "en",
      "text": "in.",
      "offset": 5436.32,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "We're two minutes over. Um",
      "offset": 5436.96,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "we",
      "offset": 5439.76,
      "duration": 1.6
    },
    {
      "lang": "en",
      "text": "This was awesome, y'all.",
      "offset": 5440.32,
      "duration": 1.76
    },
    {
      "lang": "en",
      "text": "We normally do.",
      "offset": 5441.36,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "This is super fun.",
      "offset": 5442.08,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "Uh yeah, thank you guys. Uh we'll send",
      "offset": 5444.08,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "the recording like we normally do on",
      "offset": 5446,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "Fridays. Expect to go see it. If you",
      "offset": 5447.04,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "want to continue the conversation, join",
      "offset": 5448.88,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "the Discord. Uh send us an email if you",
      "offset": 5450.239,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "have any questions. Uh see you guys next",
      "offset": 5453.12,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "week. Next week is going to be a lot",
      "offset": 5455.679,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "more coding. I hope that we can do some",
      "offset": 5456.88,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "of this context engineering tips and",
      "offset": 5458.88,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "actually start writing some code next",
      "offset": 5460.639,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "week. So we'll maybe we can do a part",
      "offset": 5462,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "two where we actually do the coding from",
      "offset": 5464.159,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "this week and actually real live code it",
      "offset": 5466.32,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "from scratch. Thank you guys. It was a",
      "offset": 5468.32,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "pleasure. Thank you Dexter. Thank you",
      "offset": 5471.199,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "Brian for making time uh and everyone",
      "offset": 5472.48,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "else that steps steps aside. See you",
      "offset": 5474.719,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "guys next week.",
      "offset": 5476.719,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "Luck. Catch y'all later.",
      "offset": 5478.239,
      "duration": 3.44
    }
  ],
  "cleanText": "Um, do you want to?\nI know a couple folks have notetakers in here. Do you want to make me host and I can? We're going to drop the notetakers because we'll be pushing this to YouTube.\nYes, I generally do that. But yes, we should do that.\nI hate the summaries that they put in the chat sometimes.\nUm.\nOh, yeah.\nI think today's episode is going to be really, really fun. Uh, we'll start at 10:05 as per usual. But while we're um, while we're catching up on this, a few of you probably got our email from last time, uh, that was coming through it. What did everyone think? Uh, did it feel good? Did it feel too AI generated? Give us feedback so we can make it better.\nYou can type in the chat or just hop off mute and just ask.\nYou're outsourcing your 5e valves. I see.\nI am outsourcing my 5e valves.\nYeah. I mean, my feedback, I guess, is that it was very human. I'm currently disappointed to hear that it was not human written.\n[Laughter]\nHe got you. Vibbov got you.\nYeah.\nThat's a hard problem I think facing our industry, which is it does not feel good to be, you know, reading AI content.\nWell, one of the things that we do is, I agree, but one of the things like we do is we do this content live. So for people that want the human content, they can come out here, but it does take like 10 or so hours to do a lot of the follow-up work that comes from the content afterwards, like send you everything.\nAnd like, just... No, I'm not saying I don't understand, like, just the pure human hours saved. Yeah.\nRight. That's why it's a problem. If, if it was just like, why are you doing this? Stop. Then it wouldn't be a problem, you know?\nYeah. I also like, I've definitely, I, I definitely feel the, like, exhaustion from AI content. I was having like a conversation with a guy on X last week, and every question I had, he would respond to me with like a link to like a notion page that was clearly just like entirely written by 03, and it was like 10 pages of slop, and I was just like, \"All right, man. If you're not even going to try, then I'm out of here.\"\nYeah.\nYeah.\nI think the, I think the slop is what really bothers me. We actually have a thing in our team where, like, we don't want to discourage people from using AI for writing docs, but one of the things that we do is we put a little snippet in the notion that says this was written by Chad GBT, and then it's like a collapsible section that people can read. So it's very upfront, very, and like, no one has to guess whether you chat GPT your notes or not, because that's the worst when it feels like you ask them to write a doc and like [Â __Â ] did you just chat GBT this whole thing and like not even read it, where, but if they...\nYou start with this was AI generated content and you let someone know that, I think it's a lot better of a dynamic and a relationship between both people, because both people can opt...\nWell, and you did that in the email, right? The email says at the end, I mean, we were talking about doing an episode on like tactics for authenticity, but here's... Here's your, here's your mini episode, which is, uh, yeah, if you declare that something is AI generated, it becomes a lot easier to, um...\nI think if I can find it...\nget away with, uh, I'm not saying get away with it, but it's just like, it, it becomes a lot more authentic of like, hey, I made this with AI, like, let me know if it sucks, like, I actually put in time and we reviewed it, but it was, it was originally AI generated. I think you polish these a little bit before you paste them in, right?\nUm, no, this one was actually directly past...\nThis one was just raw. Huh?\nOh, I did do some, I did do some editing.\nUm, let's see if I can find the unicorn emoji.\nI, I actually just went through in zero inbox this morning. Um,\nI can, I can find the email from you.\nUh, I can pull up on, just to give people context, like, this is a problem that I think is really, really hard to go solve. So, we spent about two episodes talking about this in the last two episodes, which was specifically how can you go send content that doesn't feel as bad.\nLet's see if I can see the preview. So, this is what the email was listed as, like, hello coder. We linked to the actual repo. We didn't get the exact full path in the repo. That's something we want to improve. We got the YouTube link in there. We got a little summary with things, uh, in there. The bolding didn't work. I did have to do that manually. I, it gave me double asterisk, but markdown.\nUh, we have the next session link that was AI generated, uh, thanks to the work that Dexter did that we talked about. And then this thing was, uh, manually added by me, and that was done for that reason, which was just to let people know that's an AI generated email using our content pipeline that we built over the last two episodes and just shared that with folks.\nSo if you, if you haven't gotten this email, sign up. You'll get an email this week from this week's recording. Um, but with that, uh, let's get back into today's...\nLet's talk memory.\nLet's talk Context Engineering. But yeah, it's gonna be really fun. Um, so for those of you that don't know, I'm VIBO. I'm one of the co-creators of BAML. And this is Dexter, who is...\nI'm Dexter. I am, uh, I work on, uh, a company called Human Layer. Um, build tools to help people build better AI agents and agents that feel more like real people. And we have a very special guest with us today. Um, we got Brian, who's another very awesome YC founder who is actually building products that all the things we've been talking about for the last two months. Brian is building these into a real product that real people use and get value out of. So, Brian, you want to, uh, any further, further intro, or you want to talk about kind of what you've been working on briefly?\nSure. I mean, we're gonna go more into depth later on, but, um, I'm Brian. Um, no, Dexter, no viab. Um, but yeah, we're, I work at a company called Orin. Um, we're basically building all of these, like, AI principles into, like, proactive, um, first AI tutors for mostly middle schoolers these days. So, it's a very, very hands-on application of all of these things into education space. So, excited to jump into it.\nCool. Cool. Thanks, Brian.\nWith that, let's, let's talk about the thing that we're going to talk about today, which is Context Engineering. The word has been going around a lot.\nDexter, you gave a great talk about it at the AI engineer summit, but what the heck is Context Engineering? What are all these terms that people toss around? Is it a real thing? Is it more just like vibes? I... Let's talk about it in detail today. And I think the thing I want to start with is actually the diagram that Dexter has made, because I think it really solidifies what this is.\nYeah. You want to pull it up?\nPull it up. Go ahead and screen share. I'll let you drive.\nUm, so this is a chapter from...\nOne second.\nYeah, go ahead.\nUh, and just so everyone knows, the benefit of the Zoom call is you're welcome to chime in, ask questions as you go on. Otherwise, you can always watch the recording on Fridays as it goes live.\nUm, but if you have questions, just hop off, uh, hop off mute, or if it gets to be too wieldy, uh, just like raise your hand and then, or type in the chat. We'll keep an eye on it.\nYeah, this is not meant to be totally, this is not meant to be totally one way. Uh, drop in your questions, come off mute, ask, tell us, tell us what you think is wrong, tell us what you've seen in the real world. Um, we're all figuring this out together. Uh, which is, uh, leads us into speaking of figuring this out together. So this is a chapter from 12actor agents, um, which was published in April, and one of the diagrams, I think, I think I remember actually when I vibar reviewed an early version of this in like late March, and he saw this diagram, said this is the best diagram in the entire paper. Um, and it's basically this idea that, like, LMS are completely stateless functions. Um, and the only thing that affects the quality of your AI application is the quality of the tokens that are coming out of your model. And the only thing that affects the quality of the tokens that are coming out, other than, like, tuning things like temperature and, uh, training your own model, is the tokens you put in. And so, while people will say that RAG and memory and agentic history and prompt engineering are all kind of like different, like, disciplines, and you need different tools, and they're all like, have their own, like, blackboxy implementations. Um, at the end of the day, it's all just strings going into models and tokens in and tokens out. Um, there's a little bit of the structured output thing, which I think, uh, is, uh, probably part of why Vibb really likes this, because structured outputs is not just what the model outputs, but what you do with it and how you parse it and how you turn it into data for a program to use. Um, but this really popped off. I think it was like June 12th that, uh, the cognition guy did a blog post. I'm actually going to put a, I'm going to post something soon of, like, the history of the term Context Engineering and where it came from and all the different people who kind of like, I think, kind of came to the same conclusion around the same time. Um, but there was a tweet from Toby Lutkkey in June, um, talking about he loves Context Engineering. Andre Karpathy jumped on the train. There's now a hundred blog posts about Context Engineering that you can go read about. Um, but today we're going to go deep into code, and we're going to talk about the anatomy of a prompt and the anatomy of a context and how you can build this stuff programmatically.\nYeah. And I think, um, do you want to pull up the whiteboard really fast?\nYeah, let's do it.\nAnd just to give everyone a summary of all the concepts that we'll be covering today, uh, just as a brief high-level overview. The agenda for today is we're going to go share, um, how to build each of these contexts into the prompt. We'll do a very high-level overview of exactly how RAG is context, how prompt and during is context, how state and history are context, and how memory is context. So we'll actually build out some of these prompts, write them by hand, talk about some of the analogies in there. Um, after that, we're gonna deep dive into memory and focus a little bit more on memory because we've got this awesome guest Brian, um, on the show, and he is going to, he's generous enough to talk about some his blog post that both Dex and I were super impressed by, uh, that really outlines some details around memory that I think a lot of people overlook. Um, and but I think that level of Context Engineering of how you go do that is the key to making applications feel like magic when using LMS. It's like the same thing, like when you, when you first use an app and like you use a gesture on it, it just feels right. Like for those of you that don't like reverse scrolling on your Mac, you're wrong. But for the rest of us, it feels natural when you scrolling.\nAnd that's the whole point of it. Um, it just feels right when you try a UX pattern for the first time. And the same way when you use an L1 and to do that, to make it feel right, you got to context engineer, right? So with that, let's write some prompts and look at how all this fits into the actual Context Engineering piece.\nSo in the whiteboard, we've got a few things.\nYeah.\nThe first one is then that, like, what is your prompt going to end up looking like? So I think the first thing that people write is a prompt that has a bunch of, like, a you are kind of role with a system instruction or something up here. Um, I have opinions about that. I will refrain from sharing that right now. Um, because I think that's irrelevant to the point that we're trying to make here, uh, which is...\nWell, it's just like prompt engineering is one, one element of your context that you pass in, whether it's the system prompt or the user message, and I think it's kind of like we're going to stay a little more high level than like advice on any particular one, and we're going to drill down memory because I think it's the most underserved, the most hotly debated, and the most like valuable thing to get right.\nExactly. So this UR concept is, uh, is like a prompted juring trick that we're... So, every single thing you've heard from like, I'll tip you $10 to do things, or you must be correct, or do not hallucinate. All of those are prompt engineering tips. And whether you choose to put this message in a pro in a system message or whether you choose to put it in a user message, um, is really also prompt engineering. The fact that Enthropic only allows you to send a system message if you send at least one user message. That's a prompt engineering constraint that they've applied on their system. The fact that images can't be passed into user messages or can't be passed into system messages for a lot of things. That's again another constraint applied by the model that you have to go do. So whenever people do prompt engineering, I think they stop at just the words that they're using. But I think one important thing to think about when you do prompt engineering, when you think about the context building around it, is the kind of model that you're using and what affinities it has and what you should be doing. So for example, OpenAI is clearly doing some work to capture prompt injections and stuff that disobey the system message and try and train the model for that benefit. So you might benefit towards trusting OpenAI more and more and using system messages for them. On the other hand, Enthropic seems to have guidance that you should just use the user message whenever possible instead of a system message. So you should just do that because the model people are they're biasing the way that they train the model to go do that. And part of Context Engineering isn't just assembling this. It's actually being aware of what affinities each model has and actually learning that along the way. And that that's just vocab that we have to learn. It's kind of like when we use databases, we decide that Reddus is really good for a quick in-memory system, but you would never put your entire database purely on Reddus if you need cold storage that can never die. It's just the wrong database.\n\n\nTo use for that tool.\nAnd most people don't have to know that, but your database engineer at your company better know that.\nSimilarly true for models.\nAt some point, the best context engineers are going to know the nuances about the models, and that's just part of the job.\nAnything you want to add on to that, Dex?\n\nUm, no.\nI think that makes a ton of sense.\nSorry, I'm just going to pause for a sec while I go close a bunch of tabs.\n\nOkay.\nYeah, go for it.\n\nCool.\nWith that, let's talk about the next thing: RAG.\nCool.\nI think the way that most people do RAG is also very um very interesting because RAG is I think RAG actually in my opinion RAG lives in a hybrid both in and outside of the model.\nUh it's a little bit it it kind of behaves like structured outputs in that sense.\nUm and I think the reason for that actually with that let's talk about structured outputs first then go into RAG.\nUm, I wanted to go in the circle, but I think because RAG behaves slightly differently, I want to talk about structured outputs.\nIt's like why is structured outputs in and out of the model?\nWell, we've probably all seen these things about like where you can use like a whole bunch of different tools.\nYou can use XML, you can use like we can use BAML's format, we can use like JSON, we can use like uh we can have it emit Python code that we then run into a Python data model directly.\nThere's so many different ways of doing structured outputs and All of these methods involve two parts.\nThey involve a method that involves both.\nActually, I have a great diagram for this.\nDrop it in.\nLet me Yeah, that's what I'm meant to do.\nAre you able to just paste the image into the whiteboard?\nI can I can\ncool.\nUm,\nI think when it comes to structured outputs, there's like these three diagrams that a lot of people um Let's see.\nOh, I cannot I I can screenshot and then paste the image in.\nWhen it comes to structured outputs, there's like three parts that we basically have in the model.\nWe have the prompt that we actually have.\nWe have the actual model itself and then we have the code that we run outside of it.\nThe context\nso JSON schema.\nSo JSON schema and this is part of it is like when you when you use the OpenAI SDK and you put tools in like if you read their docs essentially or any any of these model SDKs, right?\nWhen you when you pass tools into a method or push them into an API endpoint, you're passing JSON schemas that then get encoded and injected into the system message in a way that like the model has been trained on basically.\nYeah.\nAnd I'm going to I'm going to put like a couple different tools in here just so we have the screenshots um as I go talk really fast.\nand go capture these.\nYeah.\nNo, these are great.\nUm, and just so people know how context engineering works with structured outputs because it is very nuanced and it's not to say that there's one right answer.\nI think that's the whole point of these conversations.\nThere is no right answer.\nUm, all of it is really really based around uh just understanding the tools that you have at your leisure to then go ahead and go make these things actually better along the way.\nOkay.\nSo I think I have all the\nthis one is constrain generation, right?\nUh\nyes, exactly.\nYeah.\nWhere it's like the output is actually like the sampler gets struck out for anything that's not valid JSON.\nExactly.\nSo like how is structured output context engineering?\nWell, the point is I think in the very beginning at the thing at the very top if you want to scroll up.\nYeah.\nYeah.\nThere you go.\nThis is the most trivial way to do structure generation.\nYou write the prompt, the model does its thing and you run JSON.parse.\nThere's different tuning things that we can do.\nSo for example, we can do context engineering by using some prompt engineering to say your output should do exactly that.\nThe model does whatever.\nThen we run JSON.parse.\nWe can do better.\nJason, you got a question.\nYeah, I didn't want to interrupt by Bob there, but go for it.\nWould you consider like MCP a structured output like uh the the schema that goes along with an MCP server and then the tools that go with it?\nWould you consider that like a structured output of sorts for the for the model that then gives back a different output of its own?\nUm I think MCP falls into it's like a higher level concept than just like purely context engineering.\nAnd if you're using an MCP server as an input into a prompt, then yes, you're basically doing structured outputs with an MCP server.\nThat's all you're doing.\nGo ahead.\nYeah, the canonical way to use MCP is to use the list tools endpoint, get back the JSON schema in a format that then you can just send to the uh model endpoint in the same way that like any normal nonMCP client would request a tool call and tell it about the schemas.\nFor what it's worth, it might work.\nIt might work a little differently with anthropics models because it's like the protocol's opinionated about anthropics models, but that's how everything else implements it like Gemini or whatever and cursor is it's yeah it's just calling list tools and then passing them in as like normal nonMCP tools like the model doesn't know that it's that it's MCP only anthropics like models are aware of MCP as a thing which exists separately from normal tools and prompt context.\nOh, they're actually aware of that in the prompt API.\nYeah.\nSo, this is like why um anthropics models support other primitives like resources for example.\num is because and like why cursor and one surf don't support resources is because it's a separate MCP like primitive or API or whatever you want to call it that yeah it's different from tools but it's difficult to model as tools and so most and so it's not portable to other models because they don't have a predefined way to understand it in their system prompt and so you would kind of have to educate the model about what they are and what they do or kind of like wrap them as tools and So it's not portable.\nSo people just generally use tools.\nOkay, cool.\nThanks, Kyle.\nCool.\nUm, all right.\nSo go to go back into that.\nThe next thing that we could do is we could do better prompt engineering where we say say I don't know if it doesn't know the answer and then we'll literally say if I don't know is in the output, return something else otherwise JSON.parse.\nAnd you can clearly see how we're doing context engineering on the left side and doing something else on the right side.\nAnd that's the point of this.\nThis is not all context engineering.\nThe other thing you can do is you can do constraints.\nSo you're saying this stuff on the left is like the tokens you send into the model is context engineering but there's a little bit part of this where like your parsing afterwards needs to kind of know about what you put into the context and so it's kind of bridges those two sides.\nExactly.\nYou're kind of touching both ends.\nThe next part of the approach is something like this where you actually modify the model.\nyou send you you do send some tokens in like the JSON scheme or something else and then the model basically limits you to only follow JSON grammar specifically.\nSo we're not we are doing context engineering in the fact that we're giving it JSON schema or some spec.\nBut then instead of our parser having knowledge the model has knowledge of what to go do and this has trade-offs that we've talked about many times.\nBasically constraint generation probably hurts that output quality but it is a technique that you can leverage.\nThe next thing that\nyeah we'll link to the other episode where we talked about that a little bit more in depth especially for codegen.\nYeah the other episode the other thing you can do over here is you can use tool calling.\nTool calling is very similar.\nYou do you do a special kind of context engineering where you send in the tools in a special field into your API call as special tokens into the model.\nAnd now the model limits what tools you return using the JSON struct using the JSON approach that they have over there as well.\nthe same approach as this constraint generation and then you also have to change your parser to read output.tools instead of just the raw string from now on.\nSo now we have we've modified both the context and the model and also the parser.\nAnd then you have the last technique which is one that we do which is you do context engineering to inject the data type into the prompt but you don't change the model and you basically just run a special parser that is able to go ahead and take again take knowledge of what you put into the prompt and parse it out.\nXML is the same way.\nYou have to know that you gave the prompt XML format to then run XML.parse instead.\nIt doesn't just magically work.\nAnd that's exactly why the thing at the top that Dexter is showing you, you go to the top.\nThat's why structured outputs lives on this bridge.\nYour context has to know about structured outputs, but so do many other things, including possibly the model and the supporting code around it.\nUm, I saw that there's a question in the middle.\nSorry to have uh gone on for a bit.\nDo you want to hop on and ask your question?\nYeah, come on.\nYes.\nSo I I was just thinking of context as different sections or different types.\nOne could be knowledge again or you have memory and etc.\nthat ven diagram actually explains it better.\nSo when you talk about context I think giving some nuance or basically where we uh give tags or different identifiers for the context is helping it break down.\nSo that also sort sort of helps us map out what the context is.\nRight?\nFor example, you might give knowledge, you might give instructions.\nUh for example, even a structure for its output or input is also again an instruction for the LLM to follow.\nDo you agree?\nKind of.\nI think the fact that you're labeling a section as some section is just a prompt engineering technique.\nSo like if you're labeling this as like if you add an instruction here that says like structure uh is like schema and you like let's say you did some tags like this where you you put some tags like this in your prompt.\nThat's a prompt engineering trick you're employing to go do this.\nThe thing that is context is the full prompt that you send in.\nThe full prompt as is the actual API request you make fully is your context.\nThe sections of it are an abstract concept that we are making up right now.\nBut really the only context that matters is the tokens in and the tokens out even.\nAnd you could put this stuff in any order.\nYou could put your structured output instructions in the system message or the user message.\nThey're kind of orthogonal concepts.\nYou could put it at the beginning, you could put at the end.\nLike all of these are totally different.\nYou could scatter it four times in your prompt, repeating it different times.\nYou could put it without these tags, without these tags.\nor or with these tags it doesn't really matter but you have to try different things to get the right model to understand.\nDoes that answer your question BJ?\nIt does.\nThank you.\nAnd so we split this up into\nYeah, sorry.\nGo ahead.\nI was going to say we split this up into kind of parsing and instructions basically.\nThat's how kind of would clarify this.\nAnd it's kind of like when you're writing an essay, like there's a general guiding for writing a good essay.\nYou have an introduction paragraph, you have three body paragraphs, and you have a conclusion paragraph.\nThat's a pretty good structure in general, but not all essays have to follow that format.\nIt's the same with prompts.\nThere's probably good guidelines for prompts, and you should probably follow them, especially if the model providers are giving you some suggestions.\nBut they're not the end all beall.\nAnd you have to remember that everyone is discovering the capabilities of these models at roughly the same time.\nEven the model providers are also discovering them at roughly the same time.\nLike all this emergent behavior is new to them as well.\nSo whenever a company whenever someone comes out and says this is the best prompting technique and they happen to work at openthropic, they just discovered that for the first time.\nNo one has all the context right now because the space is moving so fast.\nSo just remember like no one designed the models to be good at XML or be good at parsing XML.\npeople just over time we're trying lots of things and realize oh if you put XML into the model it's really good at focusing the types of attention on the certain stanzas that you want and the reason by the way that people seem to perceive XML is better I don't actually think it is is because the parsing of XML is much more tolerant to escape tokens than the parsing of JSON so YAML is whitespace specific JSON doesn't even allow most types of whites space XML is a lot more forgiving and so like if you have the model writing code and the code has new lines, it's very hard for the it's much harder for the model to write slashn instead of an actual new line character because most of the code in the training set is written with real new lines, not with the slash and the n.\nExactly.\nSo when that happens, what ends up happening is you're just biasing towards certain directions.\nAnyway, we got we're Yeah, we're almost 20 minutes in.\nWe got to we got to hit the other categories quickly.\nYes.\nWhat's up, John?\nSorry, I couldn't figure out the how to raise a hand, so I'm just dropping in.\nUm, so I love this diagram, but one of the thing I'm kind of struggling with is that the right now it's kind of like a mixing up both um behavioral instruction like a how to with the factual information like either RAG or memory we're going to talk about.\nIs it is it helpful to like even split out that Yeah.\nYeah.\nthere like a instructional versus factual like a context.\nDepends on the model.\nIt honestly depends on the model.\nSome models benefit from having it in different places.\nUm I will we'll show some prompts in a second that show how to go write this, but it really really really varies.\nUm I wish I think about machine learning that a lot of people I think have a hard thing is like we all in software engineering we have llinters that make us say like this is the right way to name our variables.\nthis is the right way to structure our code and there's often a right answer for so many things.\nBut this whole context engineering piece is much more like a system design interview question where it's all about trade-offs in different dimensions and just understanding the trade-offs and that's how you and like observability as well.\nI'll use that more generally obviously in in LM world it's more like eval but like the idea of like how will you know when you try I I work with a\n\n\nA lot of teams, and they end up like just trying every technique and every paper without knowing how they will know what's better and what's not. And, like, I don't know what's better, and every problem domain and every model is different. So, when you think of this as all kind of like overlapping parts of this challenge of how do I get the right tokens into the LLM so I get the best quality answer, it's a mindset shift. This encourages you to kind of like open your awareness and your approaches. I don't know what's best, but I know the more things you can try and the more flexible you are with how you mix this and interleave this information together, the more likely you are to stumble on to and discover some emergent property of, like, \"Oh, when we do it this way, it's actually like 20% better.\" And that makes our agent go from like kind of just a toy to like, \"Oh, this actually solves real problems reliably.\"\n\nExactly.\n\nYeah. That's kind of like John's comment right now is that how to measure best is like, is it more factual? Is there more like a usable for next like software to use? So anyway, let's keep going. I'll probably follow up with you guys.\n\nYeah, we did a chat on eval like a couple episodes back. If you go on the YouTube channel, you'll find it. Um, let's talk about RAG, and I'll talk about why RAG is very similar to structured outputs in this sense and why we should update this diagram accordingly. Um, well, the way RAG works is also very similar. You have a bunch of context that lives in some database somewhere, and we're going to want to pull parts of this. Oops, sorry. Pulled the wrong diagram. I'm not as fast at Excel Draw as Dexter is. Um, he's a beast at this stuff. Uh, we have a bunch of...\n\nI just love... I played too many RTS games. I'm obsessed with hotkeys.\n\nI can tell. Um, and what we will want to do is we'll want to pull some of these contexts and put them into our prompt. That is naive RAG. That is the most naive form of RAG that you can do. It's just like in structured outputs. If you scroll down, Dexter, the most naive form of structured outputs that we can do is literally dump the, uh, is literally dump the JSON schema into the problem and just JSON pars.\n\nYou're right.\n\nAnd hope it didn't put markdown backticks around the JSON. Hope. Pray.\n\nRight. Exactly.\n\nThat it didn't.\n\nYeah.\n\nSo, but on the other hand, what we really want to do with RAG is RAG is a problem. And the way that I think about the problem is it's basically a problem that looks at this function signature. It takes in some query of some type T and returns a ve of another of a data type. That's all a RAG function is. Whether I use a vector database or anything else for that doesn't really matter. It's really up to me as an engineer to decide.\n\nRetrieve retrieval. This is just the retrieval part, right? Like I would say like this is the R, this is the A, augmentation, augmenting your context with more info, and then the generation happens when you send it all to the model.\n\nExactly. And a lot of people think RAG is about the model. RAG is not about the model. RAG is purely about retrieving the right data, and then the way you augment it into the prompt is the way that you're rendering into the prompt. So you could dump it in directly. You could dump it in reformatting it in a way that's different for the model. So we talked about like rendering XML schemas instead of JSON schema or rendering BAML schema instead of JSON schema or wrapping with these schema tags around your schema part of it. It's the same thing. The way you render is up to you. There's a thousand ways to render the same context in your prompt, and you have to decide which way that is. And that's why RAG sits on the border of context engineering because you have to choose how you augment the model and you have to choose how you get the context. And the parameters that you choose to get the context may not even be ones that are fed directly into the model. It may be a totally different query than the direct user message you're putting in. It's just a function that takes in a query, spits out some data, and then you have a function that has a...\n\nI should write this this way. Dev render to prompt.\n\nYep.\n\nThat takes in a data object of type data and spits out, really, this should speak, spit out a ve of tokens, but I will just say it's a string.\n\nI'll say it's a ve of tokens. And the reason I'll say that specifically is because one RAG context doesn't have to be a string. It can easily meet... it can easily just be, like, for example, each RAG context could be its own user message. It could be a user and system message pair. It could be a system message. It could be a user assistant message. If a new model comes out with a new message type, it could be that.\n\nAnd that's why there's overlap here. That's why the prompt engineering is overlapped with RAG because how you use these system and user tokens might be interled with how you pass context to the model. And I've seen a lot of things where it's like you go retrieve all the documents, but actually the thing that works best is user asks a question and then the system returns \"I need this,\" and then you put it in as a new user message for the answer. And that's a better way to get the model to pay attention to the data in the RAG chunks.\n\nExactly. Um, and as you can probably guess, state and memory are the exact same thing. People call it RAG, people call it state, people call it history, people call it memory. All of them are basically the same thing. You're using context to pull some data in, and then you're rendering it into the model. The rendering part is the context engineering. The pulling part is some excess code that you will write that looks like this. And that is...\n\nI think the naive...\n\nSorry, I was going to say, like, the, the naive, uh, the naive RAG is basically, uh, just embed the query and send it in. We did a long thing about embeddings and RAGs and stuff like that. Memory can just be thought of as like RAG against past conversations. And again, that's the naive thing. But it doesn't have to just be an embedding comparison in a cosine distance, right? This function, this query of T, this T object could be a string that the user asked. It could be a string that the user asked plus the user's ID plus, like, the date of the query and a time range to query memory for because all of that could be stuff in your UI where they say, \"I only want to consider memory.\" So, like, memory becomes this more complex thing where, like, when you get your memory context, your object that you use to fetch it could be a combination of, like, \"Okay, we're going to filter only things from a certain time range. We're going to summarize them before,\" but at the end of the day, we're going to come back with data that's going to get pushed into the prompt.\n\nYeah. And I think, like, conversationally, the way that we use RAG is RAG is usually a backed against factual data, and that's what people nomenclaturely call RAG. Memory is usually the same kind of conversation but against previous, uh, like information about the, about the person that's interacting with the system or the entity that's interacting with the system. And then state and history usually refers to, like, conversations more focused on the actual, uh, stuff that's going on currently in the or more focused on the context that, like, the past interactions that that system has had. memory.\n\nIt's like your actual, like, chat history, basically.\n\nYeah. And it's very different than memory because memory to me is more like factualized information that are already aggated in some summarized. So, for example, if I go to chat GBT...\n\nLet's just see what it says. Um, uh, am I screen sharing? I don't know if you want to screen share.\n\nYou are not. You want to screen share?\n\nUh, did you put a link in?\n\nUm, I'm screen sharing.\n\nNo, I... I don't know if you want to share. Yeah, I am now.\n\nStop share.\n\nI did. Yeah, go for it. Uh, it is sharing. Yeah, if you go to the top tabs, if you don't see Vibe off screen, you can go to the Zoom top tab and select it.\n\nOkay. So, like, for example, it's like, \"Well, I like, what are five words that describe me?\"\n\nIf I put this on here, this is going to be a memory task. And this is slightly different, uh, nice. I'm playful. Uh, this is slightly different, uh, than than a lot of other, uh, uh, things as opposed to state in history. Stay in history is me asking it about the same conversation two or three times. That's stay in history. But memory is more just like using factualized information having being able to have a conversation about this.\n\nUm...\n\nyeah, Brian, that's why I did that, and I didn't ask Desk to do it because I wasn't sure. I didn't want to make him say no in case he didn't want to do it. I was like, \"Whatever. We'll see what it says.\" Impatient and irritable would probably be mine. I like... I like to yell at GPT in all caps. It seems to work well.\n\nUm,\n\ncool. Um, that's the whole point of engineering.\n\nDoes this make sense to you in terms of, like, assistant and user as part of state and history? Like this is like your chat history and how you tell the model. And, like, again, you could use assistant and user messages, or you could just put, \"Hey, here's the past conversation,\" and stuff it all into one user message. And this is the context engineering part.\n\nYes. So, what I want to do for the next, uh, 10 minutes, uh, really fast, is I want to do some live prompting, but before we do that, I want to check really fast with Brian and see if we want to get him on first to talk about some of the stuff that he's been doing in memory and talk about his blog post. And then after that, we'll do some of the actual prompting in the second part of the show, which goes over. So, you can watch the recording if you'd like or stay on where we do live coding. But I think what we should be doing right now is I'd love to have Brian come on and actually... Deck, do you want to pull up his blog post while he does it?\n\nYeah. So, Brian sent me this a while ago. I thought this was one of the best, uh, like practical guides on how to build agents. It goes through all this proactive stuff, and when to trigger things, and there's a really nice, like, concise zero slop section on, like, how they do short-term versus long-term memory and stuff like this. So, I saw this, and I was like, \"We're going to do a memory episode.\" I would love to have Brian come in and talk about kind of how he thinks about this and sort of the lessons they learned along the way and maybe... maybe if we can convince him, if he's feeling generous, to, uh, share some code.\n\nDo you want to blog post and Brian can walk us through?\n\nYeah, I can... I can share screen with the blog post and should be able to go show some, some code and some logs and stuff just for local data.\n\nCan you guys see the blog post on my screen?\n\nYes.\n\nAre we good? Wonderful. Um, I think, like, all the stuff you're saying about memory is correct, where it's like it's really just a form of context engineering, but how you do it, I think, is very use case specific. Um, like, for example, for, for us, for tutoring, it's okay, like, we want to have this agent kind of exist and and kind of be a partner to families throughout months and hopefully years as well. And so it's okay if, like, it can't remember the exact thing that happened, like, 14 months ago, right? Like, if it can just get the general, like, what were we doing roughly in that season, you know, that's fine, right? Um, and but we need to be very, like, very episodic where it understands, like, how has the students progressed through time, right? Um, we care less about, like, you know, your dog's name is Charlie versus, uh, what were you, like, bad at a month ago, right? We want to know more of the episodic stuff than the factual things.\n\nAnd so we basically had to had to figure out a way to, like...\n\nHow did we... Yeah. Go ahead.\n\nTo interrupt you. It sounds like the first thing that you guys did is you defined really clearly what was the success criteria for your thing. Like it sounds like very clearly, you know, you don't care about knowing your dog's name. You do care about knowing, like, educational progress of the student over time. And I know that seems obvious, obviously, like hearing that, that seems super obvious to me about your problem statement. and it's like, \"Oh, you're brilliant tutoring app. Obviously, you don't care about your dog's name.\" Um, but I can also see how someone did an email or like showed their boss that didn't do that. Someone could be upset by it, but like you've clearly made a very hard stance. So, like, this is like it seems like you communicate across your team even what is a success criteria with a pretty good, like, herring test that someone else can run and be like, \"Oh, does should this pass? Should this not pass?\" Which I kind of really like about what you just said there.\n\nYeah. It's less about, like, the dog's name or, like, that specific fact, but more like what type of information do we care about having in the long run, right? So, uh, a lot of times, like, the core insight here is that when people are buying or paying for tutoring, um, they need to feel like they're able to offload the cognitive workload of of managing somebody's learning, right? Like that's kind of the value that you're you're you're actually purchasing. And so if if you can't trust that the solution that you're purchasing is, um, like, able to keep track of what your kid is doing in the long term and, like, how they're progressing and all that stuff and, like, maybe, like, remembers, \"Oh, you're on vacation this week, and so let's not do lessons,\" but also, like, \"You know, last year we had trouble in math, like, let's let's hammer in the algebra this year,\" right? Like, stuff like that is kind of what you're paying for. And so if we'd obviously love to know what your dog's name is and and remember that, but, like, if we had to pick one, we have to optimize for one first.\n\nUm,\n\nand I'm not having the chat up, so if there's questions in there, let me know.\n\nYeah, show off the diagrams that you... I think those are sick.\n\nWell, I like also this, like, conversation about, like, how you decided to, like, basically, like, what made...\n\n\nYou decide to want to build this yourself versus like outsource it to one of the frameworks out there that are all quite popular and in broad use.\nYeah.\nLike honestly, like after doing a deep dive, and this was a few months ago, stuff has changed, like I didn't really see products that were super focused on this like episodic temporal memory.\nIt was a lot of stuff that was focused on like these knowledge graphs, which is great for for use cases that are like much more workflow driven, where it's like I want the agent to do a certain thing, but also know a bunch of stuff about doing the thing.\nUm, but like that just wasn't as important to us.\nUm, and so we basically decided like, hey, let's just build our own temporal agent memory because like the naive solution to this, which we thought was kind of naive, but um, turns out like required a lot of nuance, ended up being really good.\nUm, and so because we didn't care as much about like knowing what was happening exactly, you know, a year ago and like this the exact specifics of that, we ended up building what we call decaying resolution memory.\nUm, which is like kind of exactly what it sounds like.\nWe basically take all of the ground truth of what has happened, and it's this diagram is actually a little bit out of date right now.\nUm, we don't take the actual messages, we do summaries of the direct interactions.\nSo like after one um one like run of the agent, it's uh someone question, uh Jerry mute that person.\nHow you doing?\nI think he's on the phone or something.\nAll right, Jerry is now muted.\nJerry, come off mute if you have a question.\nCool.\nUm, we we take all of the the direct summaries of um kind of what we call like interactions, but there doesn't actually have to be a human on the other side of the interactions.\nLike our agent can wake up and not be talking to you.\nIt can just be doing stuff behind the scenes.\nUm, we take a summary of all of those things, and that's like our ground truth of what happened, but like the agent might be waking up 10 or 20 times every day depending on like the relationship.\nAnd so that's like way too much to just throw into a, you know, the front of your prompt and do context engineering that way.\nAnd so we have this like memory system that as memories get older, it clumps them into like lower resolution summarized chunks, right?\nAnd so we're like a year ago, um, you'll have monthly chunks of like, okay, last June we were doing this, right?\nAnd kind of this is all the notable stuff about last June, which isn't going to be that big, right?\nIt's like we're going to it's very lossy, but our hypothesis is that like most of the stuff from last June is like not that important to remember, you know, but then as you get closer to now, like for this week, you have much more specific granular data about like I think right now we do um I can look at this in a minute, but I think right now we do um like every for the for today, we store the direct interactions and that gets fed into the model.\nBut then for uh like the last week's worth of days, um we have daily summaries and then for like the last n weeks, we have weekly summaries and the last forever, we have monthly summaries for the rest of it.\nAnd so what that becomes is like this kind of chunked up pretty like compressed context window of what kind of is like the the background of this agent's memory.\nAnd the nice part is that this scales sublinearly, right?\nLike that's the most important piece is that as as you have more interactions and more and more time with this agent, uh your number of tokens to keep the memory like uh up to date is not a linear scaling anymore.\nUm and there's bunch of chats in here.\nUm no, this code is not open source, but I'm happy to to share snippets of people if if you know they would like that.\nUm Josh, would you not summarize?\nGo ahead.\nAgain.\nNo, go ahead.\nUh Josh, we we we don't summarize um by topic.\nThis is specifically for the agents memory.\nAnd I'll go into a bit more about like the how we store topic stuff later is like we make heavy heavy use of stateful tooling, which is again kind of like a reversed like spun around version of rag.\nRight.\nThis was so cool.\nRight.\nYou're using the tools themselves to offload some of the context management and they have their own kind of stateful memory like like isolated to the context of each agent instance that is might be using those tools.\nRight.\nYeah.\nSo to me this this was like obvious.\nI didn't know this is like not a thing that people were doing a lot of yet.\nBut it's like it's very obvious that like I don't remember all of the emails in my inbox.\nThat's why I have an inbox, right?\nLike there there are tasks like this that I just am not able to do.\nI can't compute those in my head, you know, and so I know that like my workflow for this is check my inbox, look at what's in my inbox and whatever.\nAnd so we we distinctly don't actually include that data for like inboxes, SMS, we don't really do specific stuff in this memory summarizations because we don't want them there, right?\nWe want the memory summarizations to only be about like what's happening during the tutoring relationship, where's the student and their progress, all that stuff, right?\nUm, out of the model design.\nYeah.\nUh with with long-term versus with variable just on this chart um is with variable that's the DRM, the decaying resolution, where it just gets more and more summarizy over time.\nMhm.\nOkay.\nAnd what is with long-term?\nIs there a is that like an in between diagram that's actually not in here?\nWell, with long-term, it's just like if you make big summaries of what's happening, but it's still just summaries every n like every unit of time.\nLike maybe you do weekly summaries as your long-term memory, right?\nBecause then you can say, &quot;Hold on, we got a short-term memory, which is here, and then weekly memory, which is here, but it still scales linear linearly with weeks.&quot;\nUm, which is just like better than just having short-term, but it's not quite as good.\nUm, I see a head up from Steve insight before sleep goes on.\nI think the real insight.\nCan you scroll up here, Brian?\nThe real the real problem here is I think a this is like when first people are first introduce like merge sort for the first time, like oh [Â __Â ] that works and you keep sorting in a small bit.\nIt actually works really well and it's really elegant and it feels really intuitive the first time you see it and the first time you write it.\nUm, but I think what you're probably running into here is for the first time, like people are like, &quot;How do I get this thing to remember everything I've done over the last two years?&quot;\nAnd the answer is you don't.\nThat's the thing.\nThe trick is you don't.\nAnd once you reframe that problem and say, &quot;How do I get to remember the most important things?&quot;\nNow you have a separate task, which is how do I make good monthly summaries, which is a separate LM problem that you constrain and say are my monthly summaries good?\nAnd you can build an eval, and that will be use case specific too.\nUm, exactly, like you can you can choose what you want to happen and what you want to be in a summaries.\nUm, and but monthly summaries for monthly summaries for a tutoring bot is going to be very different than monthly summaries for chat GPT than is going to be different for every other use case.\nAnd so you can eval this and you can test it and you can stub it out and you can build a product around it and you can look at it in isolation, which makes it easy to kind of build these different this this is the engineering part.\nHow about even how about milestone?\nHow about milestones summaries?\nIt's all the same.\nWhat do you mean by that?\nUh I I think the whole concept here is like you you understand your problem definition and once you understand your problem definition, you then have to go and scope it out accordingly.\nSo in this case in the tutorbot case, I don't care about details like the dog's name.\nI care about seeing the academic progress.\nSo what I want to make sure is actual as I funnel down from actual messages to daily summaries to weekly summaries to monthly summaries.\nI basically am expecting a loss of information in each of these steps.\nIt's like a funnel of information being lost.\nThat's by design a good thing.\nThat's what I want.\nBut I want you want smaller context.\nYou want tighter, more focused context.\nHow do I question in zoom?\nI haven't used Zoom much.\nUh why are we focusing why are you focusing so much on time rather than just token size?\nWell, that's the context window that that seems to work well for Brian's use case, I'm guessing.\nBut that's the point.\nYou can decide as a developer what you want to go use for your thing.\nSo you might say that hey for me token window size matters, but semantically what I suspect is for many use cases time is a good orientation because what I'm doing a month ago is probably very different than what I'm doing today.\nIt's very, very unlikely for it to be continuous.\nBut what I'm doing today, like when I'm doing a cloud code task, every single cloud code session is its own session pretty much.\nVery rarely do I resume a new session.\nBut whenever I'm doing that, it doesn't matter how big that context is.\nThat is all semantically relevant.\nAnd I might still Claude code does something called compacting the context window, which is very relevant.\nBut it's the same thing.\nThey realize that the context for them now is getting too big.\nLet's build a smaller subset of it that we can actually pass in and maintain.\nBut then we go into cloud code rules.\nIf you scroll up again, Brian, sorry, can you scroll up?\nYeah.\nThen we go to cloud code oh into that diagram.\nThen we go into cloud code rules.\nAnd cloud code rules kind of behave like these monthly summaries which are being injected in every prompt that are almost like static constants.\nUm, and I say static constants because they're not changing very often, but they're being injected in and then the model is kind of understanding that.\nAnd this part is the part that is growing ephemerably as I go on.\nOkay.\nAnd this is like what the model compacted and put on.\nStephen, hold on.\nLet's just I we have a ton of questions piling up.\nI I want to let Brian finish his walkthrough of the blog post and then we'll do all the questions because otherwise I don't think we're going to finish.\nThat's a good point.\nUh, sorry Brian.\nI'm gonna I'm gonna mute myself too.\nAll good.\nUm I think the really important here someone asked about context or about token count is that not only does this method scale sublinearly, it actually like tapers off.\nIt's like not quite asic, but it's like pretty close.\nUm and so like even if you're doing if we're if we're working with a family for five years, you're still only at a max of like maybe call it 60 summaries, right?\nUm maybe maybe a little bit like 65, right?\nAnd so each summary is like a like maybe a hundred words.\nThat's like still not that much tokens.\nAnd so our at least our our bet is like the context windows that are available with these models will scale faster than our memory will require, right?\nUm um so yes, all of that memory is being put in the prompt.\nUm but it's because it's lossy.\nIt's like it's not all of the information, right?\nIt's like it's only the most important stuff from the past that's being put in.\nUm, the other thing I want to harp on.\nYeah.\nNo, go ahead, Brian, please.\nThe other thing I want to harp on is uh how like what we store in memory and how our to use of tools actually changes this where like like the email inbox stuff like Orin, which is our our tutor's name, has an email inbox and and that email inbox is isolated based on uh kind of every like customer unit, which is like a family for us.\nAnd so when he's querying his email inbox, we only show him the stuff that would be like isolated to this family, right?\nAnd what that means is that we now don't have to store all of that information in memory because whenever he wants to look at his email inbox, he can just go look at his email inbox, right?\nAnd we do the same thing for email.\nWe do it for SMS.\nHe has a calendar that he can use.\nLike all these very basic like just knowledge work tools that all of us use um to offload our own memory onto these like software systems we also can give to agents to basically give them the same abilities.\nUm let me let me describe that really quickly just so I I articulate exactly what I think you're saying.\nTypically when people do like querying a tool, that's like a tool call or like a structured output kind of concept that people are doing and usually they'll have the model output like the user ID of the person they want a structured output.\nYou're saying the only tool that you're giving the model is query search the inbox, search the inbox then has an implicit ID attached to it for that user.\nSo the model doesn't have to think, it doesn't have to think about the idea of user permanent users at all.\nThere is only one user from the perspective of the model, and is that what you're saying?\nWell, that's not entirely true.\nUm there there's only one family from the perspective of the model, but when you search the inbox, he also has a contact book that he can use, but that contact book like there's an implicit idea of like every Orin has family's contact.\nIt's it's that family's contact.\nIt might have like the mom and the dad in the contact book, right?\nAnd it will have to give one of those ids as part of the tool call.\nBut the the model itself is like sandboxed into only being able to operate within the scope of this one family.\nRight.\nYeah.\nI see that mistake a lot in a lot of prompts when I see them is like people are asking it to load some database ID and they're literally having the model regurgitate the database ID from the prompt or something that they injected in.\nI'm like why even do that work?\nJust let the model pretend like it doesn't have to think about that and let just like pass that along to the tool manually rather than having to go do that work on in the model side because it it's more complexity on the model side.\nI do still have that where it's like I want to go check my email, but from this email address, right, which is like the dad's email, right, or the mom's email, but there's there's the the scope of valid\n\n\nOptions is much, much smaller than if you were to give it access to the entire database, right? And there's also, like, because our customers have direct access to the agent, there's huge security concerns where, like, I, we, we cannot, like, we cannot put in context information about other families, right? If for your agent that you're working with, like, you just can't do it. What if it leaks something? Just like a zero, you have a zero tolerance of risk for that because there's also a lot of kid information in there. And that's, yeah, so there's, like, we have to, like, there's FERPA stuff, like, there's, there's just structurally, we cannot have any, any leaking risk here, and with this, there is no leaking risk, right?\n\nYeah, because the tool itself is inherently tied only to that ID that you have hardcoded into your system.\n\nPretty much, yeah.\n\nDo you want to see before we answer the questions? I'd love to see one of your prompts. Can you share that with us?\n\nYes, I can share prompts. I can share codes. I'm not going to share evals, but I can share everything else. I, yeah, I actually, I understand that sentiment. A lot of companies that I've talked to share that same sentiment. The secret sauce is the evals and the prompts, and everything else is, is like, \"Yeah, sure. You can take the prompt. We'll have a better prompt like two months from now. It's fine.\" Uh, I can even share, like, how we made the evals, but I'm not going to share the evals.\n\nYeah.\n\nUm, let's take a look at some of the prompts. I'd love to see them.\n\nYeah. So, I, I, I do think it's worthwhile also briefly going over the productivity piece just people understand, like, why these things are running without human input, right? Um, like, basically, we let the LLM set their own wake schedules, and so Orin can wake up at midnight while you're sleeping and do something if he want, if he wants to, which might be creepy, but, like, it's possible, right? And so what happens is that, like, when he wakes up, it's not just in response to something that something that the user has done. He can wake up and then, like, text you a reminder, right? Um, and I can show, uh, I just, I just ran this locally, and I can, I can show kind of another one in a minute how, how it works. But this is, like, one, one trace of running the actual, what we call entity, which is, like, a deployed version of Orin for one family.\n\nUm, and so what we do is, like, before anything runs, we make sure that all the summaries are updated again. In this case, it was all, it was all done. And then this is, like, one of the prompts that would be piped into these models. And so we basically, like, okay, first thing, we tell, here's your memory context. Uh, and we include the, the, the date and time. Um, and then this actually is a bit out of order, but we give it all of the tools with descriptions of what it can do more so than just the, the schema in, um, like, in, in the structured, uh, outputs kind of tool schema. And this is again why structured outputs is part of Context Engineering, right? Is, and I've seen a lot of people do this where you put the scheme in at the end, but then you also put the descriptions in the system message. It's more, it's also just because, like, we, we care about, um, being very, like, we want him to be very human in when and how he uses these tools, right? Because if, if we frame the tool as, like, here's your email inbox, he has many examples in sample of the model training of when and how to, like, use a email inbox, right? Um, and so, like, the framing of the tool matters a lot. Um, so yeah, we have a lot of different tools that you can use here. Um, but then, like, we have a, we basically have a big prompt that's just like, here is, um, here is, like, your job manual. This is very high level, like, what you want to be doing as a tutor, like, you know, what you're supposed to, like, what, what does healthy look like in engagement? What's a healthy customer look like? All that stuff. A bunch of, like, not really if-then instructions, but just general stuff. Um, and I guess this one actually didn't have any memory because I ran it fresh. Um, but what happens here is it goes through and let me condense this input out. So it goes through and, um, now starts to call some tools, and we basically do a while loop where it's like, it calls tools until it calls the sleep tool, you know. Um, so, like, it's going in here and it's saying, um, tool is this. It needs to...\n\nYeah. So it needs to, like, check the email threads. So it's checking its inbox right now. And then we, we come back, and at the end of all this, we return back the JSON of, of the email API, right? Uh, we don't really do a lot of, like, augmentation in this part of the RAG. We just kind of get it back like JSON responses, and that works pretty well. Um, and so it's like, cool, I've called the email tool. I got my response back. Cool. I can see there's emails here. Let me open the test email, right? To see if there's a student. Um, it calls that tool. We append the content of that email to the prompt. And then it says, great. Okay, let's open the second thread, whatever. This, uh, for this, for this version of Orin that we're looking at right now, there's, like, pretty much nothing in the stateful tools except for, like, one or two test emails, which is why it's, like, you know, kind of just, like, scrambling to figure out what to do.\n\nUm, so it keeps opening up email threads, and then it's checking its calendar, right? Looking for upcoming lessons within the next 48 hours. Um, of which there, there are none, right? And so, um, all emails are old, no actual messages, whatever. And then it basically decides to, uh, go to sleep. And it has just, like, a tool it can call. It's just go to sleep. And it's going to set a time of, like, where when it wants to sleep until. And the important part is, like, none of this happened in response to a user input. We do have certain wake events that come in and and all that stuff, but, like, none of it happened in response to user input. And then we take all of those messages at of what happened during this conversation. And, uh, we do what we call enshrine them, which is just, like, summarize them and store them in the longer term memory. And so after this one run ran, um, it goes through this tutoring step, right? Which is, like, it takes in, or sorry, this summarizing step, which is, like, it takes in all of the messages from this, uh, this history, including all of the data, including all that stuff. Um, and then we basically ask it to, uh, summarize what that would look like, and somewhere in here, oh, here we go, wait, somewhere in here, there is a prompt about how to summarize stuff, but if it's not in here, I can just show you it in the code.\n\nUm, yeah, let's see. Code is code is always a lot easier.\n\nYeah. So, like, we, we have a lot of examples about, uh, how to summarize different types of, uh, different types of summaries, like, like direct interactions, daily summaries, whatever. Um, so for, for summarizing this direct interaction of, like, what happened just now as you did this, we care about, like, I don't know, what, what mediums did you interact with, who did you talk to, like, what did you do? Um, that kind of stuff.\n\nAnd I also find...\n\nWay a lot of, I should pause a lot of people over here don't do the stupid thing. Do the DED thing. Don't do the DED thing.\n\nThat's good. Anyway, I'll do the dead thing.\n\nAnd then we give examples of just, like, what a good summary for a certain interaction, like, could look like, right? Just, hey, fot prompt this, you know, here's, here's what it might look like.\n\nUm, and then we've also, like...\n\nI want to note one more thing about your fot prompt really fast. Can you go up? The one thing I noticed about the fot prompt is you're not actually telling it the input. You're only describing outputs, which is good because you're basically helping understand the structure of what kinds what is a good summary without actually telling it, here's how you summarize this piece of content exactly. You're giving it, like, clues about examples, not really prescribing that this is a summary for this example.\n\nThat's a great few shot example. I, I, you people on the call know that I harp on fuchsia all the time. I think it's ass. It's because people do it wrong. This is a great way to do FA.\n\nLove it. Love it. And we do this for all the different, like, uh, like types of, of how long, how long we want to chunk up. But also this has been surprisingly helpful just adding this to every, like, like almost waterfall of memory where it's like, when you're summarizing the day, what should be remembered for the rest of the week, month, and year, like, include all that stuff, too. And, like, we use 03 to do all of our summaries as well. It's pretty good at that. Like, it, it can kind of just, like, figure it out and be like, \"Oh, that's an important thing that I should include in today's summary because it should be remembered for a lot longer than just today, right?\" And that ends up waterfalling up into, like, becoming, like, basically getting put into, um, like, some of these buckets and not just this bucket. And the idea here is in the daily summary, if you highlight that this is a good weekly summary note, then when the weekly summary prompt runs, it's like, \"Oh, yeah, it has that part of the daily summary. This is a good weekly summary. I should probably take care of this. And if I see too many, I'll pick the best ones out of all of them. And that's kind of the idea. And, and, and you'll see in the future stuff, like, our examples get more and more broad, kind of like how we would want the summaries to get, where it's like, you know, you're not really saying much about, like, specifically at four o'clock we did this thing. It's just like, okay, this is the daily summary. So today we did a session here, like, the parent reached out in this way, and kind of we set up, set up a plan, right? And then weekly, it gets all the daily summaries of that week. Um, and so it's, like, even more broad, right? Like, more focus on student progress, more focus on, like, the pedagogy of all this, right? And then monthly, you get, you get very broad.\n\nYes, your examples are listed in different formats for different parts of the prompts. There's a part where you do example one, example two, and then another part of it where you do...\n\nUm, ah, true, just calling it out. Please go... there's only one for the longer ones, there's only one example rather than multiple examples, it looks like, right? These are examples. I just... but then... I just changed it from, from this...\n\nYeah, uh, but none of these are run synchronously, so it only ever will see one or the other, and, like, you said, it's a staple reducer, so it's like, you know, I'm sure I could ease out to figure out which one's better, but honestly, just haven't. And then, like, we have a more broad, like, prompt for everything, which is, like, cool, here's a who you are making memory summary, whatever. Um, we also, like, we want, uh, we, we allow for, like, long, like, longer summaries when it gets to more, like, weekly and monthly stuff. Um, and we also, we, we plug in the existing condensed, like, like, summarized memory, we call this whole thing just, like, the memory context, like, all of the buckets, right? We plug in the existing memory, memory context in the memory summarization prompts, which has actually been really important, right? Like, it, we found that if you didn't give the summarization prompt all of the memory from before of what's happened in the entire history of this, like, this or being deployed, uh, it, it would miss things that were previously identified as important, right? We, we saw similar things when we were building the AI content pipeline where we had, like, there are parts of it where giving it the full transcript was useful, but at some point, we stopped giving it the full transcript and gave it summaries that we wanted to give away, and it's the same idea, but some steps we did need both the summary and the transcript, and other steps we only needed to just... like, for example, here, maybe you have a kid who has a history of, like, some kind of executive function problem, and that's well documented within in the existing memory that's happened before. But then if you're summarizing what happened today, that might not have come out to play today, which is actually a notable thing that you should summarize, right? Like, you should include the fact that there were no executive function problems today in today's summary because that's notable based on the past summaries. And so if you, if you don't have all the past context, then it, you know, even your summarization, it kind of goes, goes is off. Um, and then, yeah, there's not a lot else to the summarization part here. Um, basically, we just, like, fill up the string.\n\nI just want to say, by the way, thank you for sticking with the vibe of today. Like, I know Dexter and I usually try and actively write code, but, uh, today I know we had seen Brian's article, and we thought it was much more useful to see his code rather than, like, have us write code like we normally do. So I just want to say firstly, big thanks to Brian for actually being bold enough to share the code live. Not a lot of people are willing to do that, especially for a startup that he's building out there. This is a recording, so people have access to these for a while. And I'm sure Bri is going to make his props better because, like you said, emails are the things he can't share.\n\nUm, um, and eval with this type of agent are, like, especially hard. I feel pretty confident in our deliverable is that... um, we should talk about that. Um, but with that, I, I know we're 10 minutes over, we're 11 minutes over. I want to take the rest of the time to really help answer questions that people might have, uh, in the chat. I know people wrote a lot of questions in the middle, and we kind of glossed over some of them. Um, but if people want to go back and ask some questions again or raise your hand really fast, uh, we'll let you come off, and then we'll, we'd love to have you on to do a quick little, uh, Q&A session for next 15 or 20 minutes as long as Brian's willing to stay on with us.\n\nI'm good.\n\nCool. I don't see how I can raise a my hand here. It's at the bottom in Zoom. There's a thing called raise hand. There you go.\n\n\nBut Radu, since you spoke up, come on up.\nI'm sorry.\nYeah.\nSo, question to uh Dexter.\nSo, Dexter, your diagram, I showed um context engineering context, and uh I I was keen to see if there is an intent part of that um because as you can imagine especially in SLMs, you don't want um uh any kind of question prompts to do to be sent to the SLM, but uh the if it's MCP tools, it knows it can do XYZ, and also um it could uh say that the tool description uh matches or does not match the user question intent.\nSo capturing, inferring that um how how important that is.\nYou're talking about kind of like capturing the user's intent to in terms of like protection and guard rails.\nUh not just authorization, um but also um think about that um the user uh the entire user base asked um all the questions are 75% are uh for this kind of intent category and the rest is that.\nSo you you get a feel of what the the end user community is is how is it using the SLM?\nAnd of course, yes, uh that once this intent uh um u I would say list or taxonomy is um more so fleshed out than it could be as a base become as a base for authorization as well.\nYeah.\nYeah, I think that's probably slightly off topic for the context engineering thing.\nI understand the value of it.\nMaybe maybe it's another episode we could tackle.\nWhat do you think?\nI agree.\nYeah, go ahead, John.\nYeah.\nSo, I I think I think I know the obvious answer to this, but I just want to validate my thinking.\nUm, if there's a memory from three months ago that it realizes from that memory dump is important or irrelevant is probably a better answer.\nIt's relevant, but it needs more detail about that memory that is not uh that is not there.\nDo you just do a tool call to whatever system is obvious from the memory to go get that information?\nLike is is the memory dump you're giving an indicator of where to get more details and then it does a tool call to get those details and then it processes that.\nIs it that simple?\nDo you want me to write the code real time?\nLet's do it.\nNot not function.\nI think I'm thinking of it more in terms of a line of thinking.\nWe're doing this in a business context and like that's how we're architecting it is the way a human would, right?\nI'm not going to remember every single term of some enterprise deal we did six months ago, but I'm going to remember the name.\nI'm going to remember what this guy's talking about today is relevant to that deal and then I'm going to go look up our how we dealt with that one, right?\nLike is that the Brian?\nBrian, do you have like agentic retrieval tools for like drilling down into memory stuff, I guess, is the question to summarize it.\nI I I don't think that the paradigm of giving the agent tools to access its own memory is a very good paradigm because it pro to me it just smells that there are like you should just add tools that are act as surrogate memory that makes semantic sense about what you're trying to do.\nSo for example it's like maybe you're in a business meeting and it needs to remember details about the business meeting.\nCool.\nYou shouldn't give it a tool to let it go access its own memory.\nYou should build grain for your agent, right?\nLike you you should build a a meeting recorder for your agent that sits in the meetings and then summarizes that in a notepad for it that it can go query later.\nSo like it's kind of the same thing when you think about it, but the framing for the agent actually makes a lot of difference where it's like the agents don't have a good intuition, at least in my experience, of what exactly memories I should go get and like how those interfaces should work and like there's a lot of insample data about that.\nBut if you have a tool that's like here's your notepad, all of your meeting recordings get auto summarized in this notepad.\nIt knows what to do.\nLike cool, let me go find the meeting that I had with this guy six months ago.\nOnly go like query you know the recording for that meeting and get the transcript or get the summary points.\nWhat's the difference in that case of making clear that fathom or granola is a tool it can use or pipe drive is the sales CRM or those types of things like what is there a functional difference of why you wouldn't do it like that just accessing these third party tools?\nYeah, I mean if for us it doesn't doesn't quite work because we have to like build our own internal tools for it, but yeah, if you can just use Fathom, like do that man, like just have Fathom join all your your agents calls and then web hook that in to like store the data somewhere and then just tell it like hey we use Fathom, like go get your Fathom stuff and like it will have insample data about what Fathom is and how to use it.\nLike we have our own little uh like version of like a Zoom client that we use.\nIt just wants to use Zoom, like it continuously tells users that it's just going to send it a Zoom link and like no it's not a Zoom link, you know, but like it's it's biased toward wanting to use the in sample tool.\nSo if you can do that, like just give them the example tools man.\nGot it.\nI think that's the part I need to dig into.\nHow how how will it choose the right one?\nI I'll dig into that particular part.\nJohn, to highlight that point a little bit, we talked about this a little bit in our 12 factor agents episode.\nAnd I think this is the part that a lot of people think about which is like how do you define these concepts?\nBut I think what you really want to say is there's a decision point that the LLM or some function is going to make that says instead of emitting the final result that I want, I wanted to request more information.\nSo in this case, I have a really simple thing like extract a resume.\nI'm going to use a really really basic example so we can all go over it.\nIt's going to go do this thing and like we have boundary ML over here and what I'll say is company startup or enterprise.\nI'll just say that there's a company type over here and I want to return what company type it has.\nUm, and I've just wrote the prompt if information about if information is missing, request more information before continuing, right?\nAnd when I go run this prompt, you'll see what I'm actually doing.\nI'm literally just telling the model like either dump out the resume or request more information.\nThat's it.\nAnd I might even put into here like action resume uh extract complete.\nAnd I'm naming this very differently than the previous one.\nI'm naming it extract complete resume intuitively.\nAnd this thing is not complete.\nIt doesn't have a start date.\nIt doesn't have an end date.\nIt doesn't have anything.\nAnd I'll make requests um a bunch of this.\nAnd I'll even put like a reason kind of thing in here.\nWhy not?\nReason string.\nAnd I'll say here's all the requests I have.\nAnd I should probably name this request.\nAnd when I go run this in theory, I don't know if this will do the right thing.\nIt did the right thing.\nIt was able to go through and decide that it doesn't have it doesn't have experience for the company name the full name associated with initial VBV is different from vivov gupta just go double check everything so now you can imagine what I would do in my python code is I can then opt into what I want to go into here so I can do something like and I think it it ends up being the same concept to what you're talking about from client import bh, sorry import.\nSo when I get like a res is it going to be extract resume and I actually pass in like the string into here whatever the string ends up being this thing is going to be either request more information type or resume type.\nSo now I have a choice of what to do if it's a client.types types import request more information if now I can basically go ahead and handle these requests in some way and that can be an agentic loop it can be something else it can be and now what I have is like initial resume uh state is going to be like a resume stir why I pass in a resume stir into this and this can just be like a state how do I do.\nI mean, you're basically building a tool calling loop.\nI think this is kind of what John proposed is like, do I give it tools or structured outputs that it can give and like yes, your code can decide whatever you want to do with those outputs, whether it's just direct query from Zoom and stuff it back in the context window or you want to send a Slack message to somebody who can help or whatever it is.\nIs like the model's only going to just say, \"Hey, I need more information about this stuff.\"\nYou can decide how you handle that.\nAnd and the thing that decides whether you need more information or not is just the function.\nThe fact that this function happens to use a model is an implementation detail of how I did this.\nYou could write a different function that is not an LM.\nFor example, you could write a simple function that's if the word bank statement is in there like a simple reax match, then I will always request information about the Chase bank account associated with this account.\nI will just require that.\nIn fact, I'll tell the user if you haven't connected the plaid can't do this action.\nI won't even go to an LLM.\nBut that's the whole point of thinking about everything as a function signature.\nDoes that answer the question really well?\nYeah.\nYeah, that makes sense.\nI think we're on the right track with the pattern.\nUm, I appreciate it.\nThank you.\nUh, Derek, thanks.\nTook me a second to come off mute.\nUm, just a quick comment that's worked well for us is um, owl time.\nUh, specifically the Allen algebra with temporal um, like time intervals.\nSo if you have multiple agents working as a team uh you know with the turns in the conversation it can help to keep track of what happens in what sequence.\nUm so I know you've got a different notion of the roll up with time.\nUm I'd just be curious if you've thought about any of the temporal or if you've got agents working together.\nUm I appreciate what you said.\nI think it was not we use mentoring but you used uh the term tutoring.\nUh so we've got like you know a whole thing with that.\nUh but the larger the teams get that are working collaboratively uh we found it to be pretty complex.\nWe've maxed out at about kind of 25 on one team to be able to manage that.\nSo I'd be curious of any aspects about uh the temporal relationships you have.\nThank you for Thank you for today.\nThis has been really great.\nCould you repeat the question again?\nSorry.\nUh so you said owl time which is I believe the like ontology knowledge graphy like expressions of time right?\nYeah specifically and in fact using that with BAML and Neo4j as as well but uh of ontology of temporal relationships with time there are 13 time intervals that are defined uh uh you know during etc.\nUh yeah, so this has been we've integrated this over the last like six months uh to to for so everything kind of happens those ones at the bottom the relationships and their inverses is a clear way to look at it.\nUm so when you have multiple agents running together kind of in stasis and then coming back where they're active.\nUh with multi-threading.\nThis is how we keep those async operations amongst a collaborative team uh all working together with different roles uh and instantiations of those threads because we don't just like instantiate one and run it.\nWe run like the full context window uh typically um and have a whole onboarding process for for an agent.\nSo I just was curious if you've incorporated anything.\nI became aware a few months ago of of what you were doing with these different grouped rollups.\nIt was a nice nice thing or just take a different approach.\nUm but I' I'd be curious.\nThank thanks again for your time.\nI haven't personally tried that approach.\nUh it looks really interesting.\nI think um in general all these attempts that I see are always around like this idea of like somehow there's this loose structure of like what is a prompt and it's these really sequence of tokens and all these attempts are just attempts to like put into some structured format that is a little bit less loose than just like raw strings and that looks like another really interesting format like I would be really keen to hear more about your feedback as you've been doing this and like what learnings you had maybe in another episode Derek and it' be really especially if you're willing to share some of the code.\nI think that the idea of connecting it from knowledge graphs to entities and relationships over time is a problem that a lot of people are going to have long term.\nOh god.\nUm, one thing I just want to tap into that was brought up a couple times and um, that Brian mentioned is this idea.\nIt's actually in 12 factor agents is factor 13 is um, I think Brian mentioned something about like why why let me know if I'm paraphrasing this right um, but like the idea of like why wait for the agent to call a tool to get information it needs when like if you know the agent will always need that information or will always be useful just inject just fetch it yourself and inject it into the context like deterministically.\nYes.\nIs that right?\nYes.\nYeah.\nThis is why I'm like not always the biggest fan of of like letting the agent control what like memory is in context because all of the stuff that are in our like canonical memory should be in context all the time.\nIt's important, right?\nIf it shouldn't be in context all the time, it should probably be in a separate siloed state full tool is our that's kind of our philosophy around it where it's like it's like why do you think that should be only in there sometimes and like what about that information can't be stored in like a separate tool that makes like semantic sense.\nNice.\n100%.\nAll right, we'll do one more question.\nJosh, hey, thanks.\nHey, thanks um Brian.\nUm it's usually helpful.\nSo, two questions.\nOne um in the um in your blog you started with the hey we saw problem.\nSo uh first question is that the after you implemented the this memory what did you see what behavior change did you see or benefit did you see and second question is that the um at the bottom now next steps you kind of highlight that the uh better like um more persistent temporal memory why did you select that as a next step as versus like slicing by different topics or whatever so I'm kind of curious about those two we kind of do with with topical things um Again, topical information should be in context some of the time.\nIt's kind of our viewpoint.\nIt's like it's\n\n\nUseful\nfor some things and not for others.\nAnd so, like we, we've basically, we're building an internal LMS right now for our agent to use, which stores topical information about like the students' progress and kind of the frontier of their knowledge, all that stuff, right?\nI mean, if you had like a, a kind of like a tutor who was like maybe had a memory issue, like a human tutor that just couldn't remember stuff very well, like you'd want to give them all these tools to enable them to be able to still tutor somebody effectively, right?\nAnd so we want the temporal side of like relationship management, which is honestly mostly for interactions with the family and not actually for the tutoring quality itself.\nAnd then we're gonna, we're building like tools around storing certain information about what the person knows.\nUm that like maybe you could use a graph to represent that in like a different, a different context.\nUm but like the point is like silo out kind of what your memory should be.\nWe care about the temporal part because we want it to be like a relationship that you have for months and years, but then also we care about things like uh like topical information, but we just store that somewhere else, right?\nHopefully that answers the question.\nGot it.\nAnd the first question is what benefit did you see after you implement this?\nWell, we didn't really anything before this, like this was our, we started building this from scratch basically.\nUm but the biggest benefit is like for us, parents actually feel like this thing is responsible and will like, like it feels less like a tool and more like a service because it has this temporal memory.\nIt can be proactive.\nUm like it'll hit you up and be like, \"Hey, do you want to do this stuff for your kid this month or like you know, you mentioned going on vacation, I'll I'll give you this week off, you know, stuff like that.\"\nThings that would have had to been built into like very specific workflows without temporal memory are now just they just happen, right?\nI think I think one of the similar ideas is uh like when when we first do like use like something like Google search or like any sort of new technology for the first time there's these wow factors that we look for that make it really amazing and Brian's technique that he applied here gave his parents the first wow factor in AI which is proactivity.\nI think when everyone knows that you can go try and have it go do something, the magic is when somebody texts you and says does the right thing happen, it's like kind of like the email when we The fact that the email doesn't feel super automated.\nIt feels somewhat human written.\nThat's what makes AI feel good.\nAnd this temporal memory strategy did that for Brian in his use case for the constraints that he defined for the evals that he wrote.\nAnd it looks like they did a lot of work to make it work for their problem, but that required a deep understanding of their problem and their users to go give them that wow factor.\nFor everyone else that's going into this, it's not to say that DRM is going to solve your problems.\nThe point here is understand your problem.\nUnderstand that one wow moment that you really truly need for your business situation to work and figure out what data structures you need to solve that problem.\nThese are all just fundamentally data structures and algorithms problems.\nWe're going to have leak code.\nLeak code is not going away.\nLeak code is just about to get a whole lot [Â __Â ] harder.\nThat's all that's gonna happen.\nUm, yeah.\nI think I think this like DRM productivity thing is probably going to hurt your business if you're not selling a service honestly.\nLike like if you're trying to sell a tool, I would stay away from that, right?\nLike the people who are using tools want like the kind of cursor feel where it's like it's not quite like I don't like think about like an individual that I'm working with, you know?\nlike I'm I'm able to to mess with stuff and like just just do it at my speed, right?\nBut if you're trying to sell this like full stack service, temporal memory is like super important for just continuity, right?\nOf like like just like the normal like PM [Â __Â ]\nLike if I have an EA for example, like that EA, if it's an AI agent, it better not need to be reminded what the [Â __Â ] BAMO is every single week.\nLike that just doesn't work.\nand it should know the vision of what your company is doing every single week and like be able to like understand semantically what the notion is going on on top of my head and what's top of mind today and the best EA are the ones that proactively go do things so whoever builds that service is going to have to go build something like this whether it's CRM or some other data structure that ends up being even better or not but this is a pretty good start.\nUm the other warning I will give about proactive stuff just just real quick on the last is uh if you want to be proactive and you're giving the keys of like scheduling to the agent itself, you now have to work with time zones again.\nAnd that is a whole mode in itself because the models are not good at time zones and you have to do a lot of normalization and augmentation of your context windows to make it look nice and have and basically cater to the model about what like how they should output stuff.\nAnd would that I can write a whole article about that.\nwith that.\nDon't try to solve time zones in a prompt, right?\nYeah.\nSorry.\nWhat did you say?\nI said with that, we're back to context engineering.\nIt's all context engineering.\nPut the right tokens in, you get really good tokens out, but understand the problem.\nPut great tokens in.\nWe're two minutes over.\nUm we This was awesome, y'all.\nWe normally do.\nThis is super fun.\nUh yeah, thank you guys.\nUh we'll send the recording like we normally do on Fridays.\nExpect to go see it.\nIf you want to continue the conversation, join the Discord.\nUh send us an email if you have any questions.\nUh see you guys next week.\nNext week is going to be a lot more coding.\nI hope that we can do some of this context engineering tips and actually start writing some code next week.\nSo we'll maybe we can do a part two where we actually do the coding from this week and actually real live code it from scratch.\nThank you guys.\nIt was a pleasure.\nThank you Dexter.\nThank you Brian for making time uh and everyone else that steps steps aside.\nSee you guys next week.\nLuck.\nCatch y'all later.\n",
  "dumpedAt": "2025-07-21T18:43:26.217Z"
}