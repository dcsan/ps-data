{
  "episodeId": "Xece-W7Xf48",
  "channelSlug": "@boundaryml",
  "title": "Using LLMs to go from 60+ min Youtube video to email / X posts: ðŸ¦„ #11",
  "publishedAt": "2025-06-27T14:45:08.000Z",
  "rawLines": [
    {
      "lang": "en",
      "text": "And we are",
      "offset": 0.32,
      "duration": 6.479
    },
    {
      "lang": "en",
      "text": "all right. Um, let's get to it. 10:05.",
      "offset": 3.76,
      "duration": 5.759
    },
    {
      "lang": "en",
      "text": "Um, so welcome everyone to our next",
      "offset": 6.799,
      "duration": 6.241
    },
    {
      "lang": "en",
      "text": "episode of AI that works. Our whole goal",
      "offset": 9.519,
      "duration": 5.761
    },
    {
      "lang": "en",
      "text": "here is always the same. Write code that",
      "offset": 13.04,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "works and see if we can do some",
      "offset": 15.28,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "incredible things with AI that doesn't",
      "offset": 17.44,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "uh that doesn't depend on waiting for",
      "offset": 19.92,
      "duration": 5.599
    },
    {
      "lang": "en",
      "text": "GBD 26. My name is Vive. I am one of the",
      "offset": 21.199,
      "duration": 6.481
    },
    {
      "lang": "en",
      "text": "creators of BAML and my wonderful",
      "offset": 25.519,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "co-host is Dexter. I'll let him",
      "offset": 27.68,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "introduce himself.\n Uh, I am working on",
      "offset": 29.679,
      "duration": 7.121
    },
    {
      "lang": "en",
      "text": "My name is Dexter. I build things. Uh,",
      "offset": 33.68,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "that's probably all you need to know.",
      "offset": 36.8,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "Uh, try to help people build very cool",
      "offset": 38.399,
      "duration": 5.361
    },
    {
      "lang": "en",
      "text": "AI stuff. Um, whether it's agents,",
      "offset": 40.719,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "whether it's pipelines, whether it's",
      "offset": 43.76,
      "duration": 4.959
    },
    {
      "lang": "en",
      "text": "fullstack applications. Uh,",
      "offset": 45.44,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "yeah, that's that's I'm not going to go",
      "offset": 48.719,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "deep into it. Um, real quick, uh,",
      "offset": 51.12,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "announcements, just updates. Um, for",
      "offset": 53.44,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "those of y'all, I'll just say it again",
      "offset": 55.28,
      "duration": 2.48
    },
    {
      "lang": "en",
      "text": "one more time in the stream is like, so",
      "offset": 56.48,
      "duration": 2.16
    },
    {
      "lang": "en",
      "text": "we're going to do these, we do these",
      "offset": 57.76,
      "duration": 3.119
    },
    {
      "lang": "en",
      "text": "every Tuesday at 10 a.m. Um, and then we",
      "offset": 58.64,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "launch the I guess this is good context",
      "offset": 60.879,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "to lead into where we're going. Um, we",
      "offset": 62.48,
      "duration": 6.4
    },
    {
      "lang": "en",
      "text": "publish the videos Fridays at 8 a.m. Um,",
      "offset": 64.479,
      "duration": 6.481
    },
    {
      "lang": "en",
      "text": "and we will push up everything in this",
      "offset": 68.88,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "show will be in the GitHub repo here.",
      "offset": 70.96,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "Uh, I'll put in the Zoom chat. So, all",
      "offset": 73.36,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "the notes from previous episodes.",
      "offset": 75.04,
      "duration": 5.119
    },
    {
      "lang": "en",
      "text": "Everything will be live by Friday. Um,",
      "offset": 76.56,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "I have a short link, but this is just",
      "offset": 80.159,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "the GitHub repo. Oh, that's not right.",
      "offset": 81.759,
      "duration": 6.521
    },
    {
      "lang": "en",
      "text": "Sorry, hlyr.devitw.",
      "offset": 84.159,
      "duration": 4.121
    },
    {
      "lang": "en",
      "text": "Um, that'll take you to all the previous",
      "offset": 88.479,
      "duration": 2.801
    },
    {
      "lang": "en",
      "text": "sessions and you can watch the",
      "offset": 89.92,
      "duration": 2.64
    },
    {
      "lang": "en",
      "text": "recordings. You can see the notes and",
      "offset": 91.28,
      "duration": 2.479
    },
    {
      "lang": "en",
      "text": "the notes from this session will be",
      "offset": 92.56,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "there and any useful links and things",
      "offset": 93.759,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "like that and all of the code.",
      "offset": 95.52,
      "duration": 5.599
    },
    {
      "lang": "en",
      "text": "Yeah.\n And it takes a lot of work.\n The",
      "offset": 98.32,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "reason that we try and share all the",
      "offset": 101.119,
      "duration": 2.64
    },
    {
      "lang": "en",
      "text": "code is honestly because there's a lot",
      "offset": 102.4,
      "duration": 2.64
    },
    {
      "lang": "en",
      "text": "of people out there talking about how to",
      "offset": 103.759,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "go how you can theoretically build",
      "offset": 105.04,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "things.",
      "offset": 106.96,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "We just want to see the code. We're all",
      "offset": 108.72,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "engineers or eventually everyone will be",
      "offset": 110,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "an engineer. Today everyone needs Excel.",
      "offset": 111.68,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "Tomorrow everyone's gonna write code.",
      "offset": 113.52,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "It's just the way it is. We might as",
      "offset": 115.04,
      "duration": 2.719
    },
    {
      "lang": "en",
      "text": "well share all the code and share what",
      "offset": 116.56,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "we can learn along the way. Um with that",
      "offset": 117.759,
      "duration": 5.121
    },
    {
      "lang": "en",
      "text": "in mind, today's topic is one Dra and I",
      "offset": 120.64,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "have we're very very very very excited",
      "offset": 122.88,
      "duration": 5.439
    },
    {
      "lang": "en",
      "text": "about. Um all this work that we do every",
      "offset": 125.68,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "week is really freaking annoying",
      "offset": 128.319,
      "duration": 5.681
    },
    {
      "lang": "en",
      "text": "sometimes. This actual\n we love doing it.",
      "offset": 130.16,
      "duration": 7.76
    },
    {
      "lang": "en",
      "text": "We love doing it. But it's\n I agree. And",
      "offset": 134,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "I think the part that's the most",
      "offset": 137.92,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "annoying is the manual part of like",
      "offset": 139.04,
      "duration": 3.279
    },
    {
      "lang": "en",
      "text": "downloading the videos, uploading them",
      "offset": 140.8,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "to YouTube, getting the transcript,",
      "offset": 142.319,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "sending out the email afterwards. And",
      "offset": 144.4,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "sometimes you probably notice like four",
      "offset": 146.4,
      "duration": 2.559
    },
    {
      "lang": "en",
      "text": "of our episodes didn't send an email",
      "offset": 147.76,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "out. That's because that's not what we",
      "offset": 148.959,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "love doing. We just like writing code.",
      "offset": 150.64,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "That's the thing that we love the most.",
      "offset": 152.319,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "So we thought, why don't we actually go",
      "offset": 154.08,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "put some of the practices that we talked",
      "offset": 155.92,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "about in the previous episodes into",
      "offset": 157.84,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "practice and build a pipeline that can",
      "offset": 159.92,
      "duration": 4.959
    },
    {
      "lang": "en",
      "text": "go do something. And the thing that we",
      "offset": 162.56,
      "duration": 6.319
    },
    {
      "lang": "en",
      "text": "thought was worth doing was really",
      "offset": 164.879,
      "duration": 6.321
    },
    {
      "lang": "en",
      "text": "building the AI content pipeline. So",
      "offset": 168.879,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "what we're going to start off with today",
      "offset": 171.2,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "is we're going to screen share. We're",
      "offset": 173.2,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "going to show you a live demo of what",
      "offset": 175.92,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "works today, what doesn't work today.",
      "offset": 177.36,
      "duration": 2.879
    },
    {
      "lang": "en",
      "text": "Then we're going to talk about the",
      "offset": 179.28,
      "duration": 3.039
    },
    {
      "lang": "en",
      "text": "architecture diagram on a whiteboard.",
      "offset": 180.239,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "And then afterwards, we're going to go",
      "offset": 182.319,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "make the pipeline better because I think",
      "offset": 183.68,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "those are the steps that I find to be",
      "offset": 185.68,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "personally really interesting.",
      "offset": 187.76,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "So with that, let's kick it off. Um,",
      "offset": 190.4,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "today's is going to be even more",
      "offset": 192.8,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "interactive than normal. So, if you have",
      "offset": 194.159,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "questions, please hop off mute. Just",
      "offset": 196,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "come and ask yourselves. That's why we",
      "offset": 197.76,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "do this over Zoom and not a podcast",
      "offset": 199.44,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "recording software or anything else",
      "offset": 201.36,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "because it's just way faster and people",
      "offset": 203.12,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "get to ask real questions along the way.",
      "offset": 204.959,
      "duration": 7.56
    },
    {
      "lang": "en",
      "text": "Um, with that, let's get started.",
      "offset": 207.599,
      "duration": 4.92
    },
    {
      "lang": "en",
      "text": "And I will say I am one of the least",
      "offset": 212.72,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "vibe coded people out there. I don't",
      "offset": 214.959,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "really vibe code. Um,",
      "offset": 217.519,
      "duration": 6.72
    },
    {
      "lang": "en",
      "text": "but thanks to Dexter and actually uh",
      "offset": 220.879,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "credit to my cousin as well,",
      "offset": 224.239,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "I'm a big believer everyone should be vi",
      "offset": 226.959,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "coding uh a lot more than they think",
      "offset": 229.04,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "they should. If you were vioding before,",
      "offset": 230.959,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "you probably should v code more than you",
      "offset": 233.04,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "thought you were. And here's what we",
      "offset": 234.64,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "were able to do with this pipeline.",
      "offset": 236.48,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "So, right over here, the first thing",
      "offset": 239.2,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "you'll notice is we are able to sync",
      "offset": 240.48,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "with Zoom and actually record all the",
      "offset": 242.08,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "previous recordings up here. So, we can",
      "offset": 244,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "actually see any of the previous",
      "offset": 246,
      "duration": 2.959
    },
    {
      "lang": "en",
      "text": "recordings. You can see one right over",
      "offset": 247.12,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "here. This current one is literally",
      "offset": 248.959,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "going on right now. We started the",
      "offset": 250.48,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "recording and it pops in immediately. We",
      "offset": 252.159,
      "duration": 4.001
    },
    {
      "lang": "en",
      "text": "can see our previous recordings and we",
      "offset": 254.56,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "can just import and process them. What",
      "offset": 256.16,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "happens when you import and process the",
      "offset": 258.479,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "recording? I'll just kick one off. Um is",
      "offset": 260.079,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "I'll just real quick call out. It says",
      "offset": 263.12,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "the title is still says cracking the",
      "offset": 264.72,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "prompting interview. I think that's like",
      "offset": 266.88,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "inherited from from BAML, but this is",
      "offset": 268.16,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "this current episode.",
      "offset": 270.4,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "Uh oh.\n Or inherited from Luma or",
      "offset": 272.8,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "whatever from Yeah. When Luma created",
      "offset": 275.44,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "the Zoom, it left the title.",
      "offset": 278.08,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "Exactly. So, we'll just start off this",
      "offset": 280.56,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "one. AI that works designing emails.",
      "offset": 282.479,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "We'll create the import.\n So, now you can",
      "offset": 284.24,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "see the video process.\n This is one we",
      "offset": 286.4,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "already did all the work in the email",
      "offset": 289.44,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "for, but um the idea is you going",
      "offset": 291.199,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "forward we would use this.\n So, while",
      "offset": 293.84,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "it's processing, I'll let it kick itself",
      "offset": 296.479,
      "duration": 3.121
    },
    {
      "lang": "en",
      "text": "off and you can actually see what it's",
      "offset": 298.24,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "doing. When it when it's processing,",
      "offset": 299.6,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "it's actually going ahead and",
      "offset": 301.6,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "downloading the full video. And it's",
      "offset": 302.56,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "going to go ahead and eventually it'll",
      "offset": 304.56,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "get to a point where once the video is",
      "offset": 305.84,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "downloaded, it'll actually upload itself",
      "offset": 307.6,
      "duration": 5.599
    },
    {
      "lang": "en",
      "text": "to uh YouTube automatically. It'll",
      "offset": 310.639,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "download the full transcript. Then it'll",
      "offset": 313.199,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "start producing key points and video",
      "offset": 315.6,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "summaries. And then after it's done,",
      "offset": 317.36,
      "duration": 3.279
    },
    {
      "lang": "en",
      "text": "it'll actually produce like an email",
      "offset": 319.36,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "draft. I guess I don't have that here.",
      "offset": 320.639,
      "duration": 6.641
    },
    {
      "lang": "en",
      "text": "This one didn't finish. Let me",
      "offset": 323.28,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "once it's done, it'll actually produce",
      "offset": 327.28,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "like an email draft of what to go send",
      "offset": 328.479,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "out. I'll produce like an exp post that",
      "offset": 330.24,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "we can go copy and paste and post out",
      "offset": 332.8,
      "duration": 3.119
    },
    {
      "lang": "en",
      "text": "there and it'll produce like LinkedIn",
      "offset": 334.479,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "content as well. So, this is what we",
      "offset": 335.919,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "mean by multi- channelannel content.",
      "offset": 338.08,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "Download a 60-minute video. Uh, go for",
      "offset": 340,
      "duration": 5.759
    },
    {
      "lang": "en",
      "text": "it. Go summarize it. Get the key topics",
      "offset": 343.68,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "and takeaways and then go draft things.",
      "offset": 345.759,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "Now, what you'll notice here is that",
      "offset": 347.919,
      "duration": 5.921
    },
    {
      "lang": "en",
      "text": "this email does read like AI still. It's",
      "offset": 351.199,
      "duration": 5.361
    },
    {
      "lang": "en",
      "text": "still not\n Yo, this email sucks, dude.",
      "offset": 353.84,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "Exactly. So, we're gonna talk about how",
      "offset": 356.56,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "to make it. Did you even did you even",
      "offset": 358,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "look at the prompt?\n No, I didn't. I vip",
      "offset": 359.6,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "code the whole thing because the point",
      "offset": 362.8,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "what I really I think this is the point",
      "offset": 364.4,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "I want to stress today is before you can",
      "offset": 366.08,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "even work on your AI part of your",
      "offset": 368.319,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "pipeline, you're not even at the point",
      "offset": 370.08,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "where you can start working on that.",
      "offset": 372.8,
      "duration": 3.119
    },
    {
      "lang": "en",
      "text": "What you need is infrastructure that you",
      "offset": 374.479,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "can actually iterate on. And if we",
      "offset": 375.919,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "hadn't built this whole system out that",
      "offset": 377.84,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "could do the glue code, building the AI",
      "offset": 379.52,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "part is completely useless because it",
      "offset": 382.24,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "doesn't really matter. I still need to",
      "offset": 385.68,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "build the rest of it. And the rest of it",
      "offset": 387.6,
      "duration": 3.039
    },
    {
      "lang": "en",
      "text": "is actually critical to my iteration",
      "offset": 388.88,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "loop. And we talk about this in our eval",
      "offset": 390.639,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "video a lot where if you don't if you're",
      "offset": 393.039,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "evaling, you should spin up a quick",
      "offset": 395.44,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "little UI to go determine if your evals",
      "offset": 397.12,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "are correct or not. Go compare the",
      "offset": 399.68,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "differences. If you're building a",
      "offset": 401.12,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "pipeline, you should be doing the same",
      "offset": 402.72,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "thing. And right here, you can see the",
      "offset": 404.08,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "same thing. It actually is going ahead",
      "offset": 406.08,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "and let me see if the socket connection",
      "offset": 407.759,
      "duration": 3.521
    },
    {
      "lang": "en",
      "text": "works. Yeah, there we go.",
      "offset": 409.44,
      "duration": 3.039
    },
    {
      "lang": "en",
      "text": "It's actually going to it's actually",
      "offset": 411.28,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "able to also stream out the outputs if I",
      "offset": 412.479,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "wanted to, which is really useful for me",
      "offset": 414.4,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "to iterate a little bit faster along the",
      "offset": 416.08,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "way.",
      "offset": 418.56,
      "duration": 7.56
    },
    {
      "lang": "en",
      "text": "I'll pause really fast questions so far.",
      "offset": 420.639,
      "duration": 5.481
    },
    {
      "lang": "en",
      "text": "Cool. Let's keep on going.\n How long was",
      "offset": 427.36,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "it end to end? Did you say three hours",
      "offset": 430.24,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "or something to build this?\n I logged off",
      "offset": 431.84,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "after three hours. I don't know how late",
      "offset": 434.96,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "five was up last night.",
      "offset": 436.96,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "It took a total of 6 pm to about 2:30",
      "offset": 439.28,
      "duration": 7.359
    },
    {
      "lang": "en",
      "text": "a.m. um combined with an hour and a half",
      "offset": 442.8,
      "duration": 6.239
    },
    {
      "lang": "en",
      "text": "break in the middle.",
      "offset": 446.639,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "I",
      "offset": 449.039,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "pne",
      "offset": 450.56,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "you're new to the session.\n Pne pr you",
      "offset": 452.8,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "got to go watch the other episodes. We",
      "offset": 454.96,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "don't we don't do frameworks here.",
      "offset": 456.56,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "And the reason we don't do them is more",
      "offset": 459.199,
      "duration": 2.881
    },
    {
      "lang": "en",
      "text": "often than not they just get in the way",
      "offset": 460.72,
      "duration": 3.039
    },
    {
      "lang": "en",
      "text": "and they make it harder to go do things",
      "offset": 462.08,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "rather than easier. With that, let's",
      "offset": 463.759,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "talk about what this architecture",
      "offset": 466.08,
      "duration": 5.839
    },
    {
      "lang": "en",
      "text": "diagram ends up looking like. Um,",
      "offset": 467.759,
      "duration": 5.601
    },
    {
      "lang": "en",
      "text": "unless there's a couple more questions",
      "offset": 471.919,
      "duration": 5.521
    },
    {
      "lang": "en",
      "text": "about this itself.\n Um,\n yeah, let's let's",
      "offset": 473.36,
      "duration": 5.839
    },
    {
      "lang": "en",
      "text": "uh let's draw it out and then let's",
      "offset": 477.44,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "let's make the let's make the actual AI",
      "offset": 479.199,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "generated stuff not suck.\n I agree. All",
      "offset": 481.28,
      "duration": 6.72
    },
    {
      "lang": "en",
      "text": "right, pull this down.",
      "offset": 485.039,
      "duration": 7.241
    },
    {
      "lang": "en",
      "text": "Oops. Why is my mouse not working?",
      "offset": 488,
      "duration": 4.28
    },
    {
      "lang": "en",
      "text": "We did record the whole thing. You can",
      "offset": 494,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "watch it at 2 to speed. We'll post it.",
      "offset": 496.16,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "Well, not the whole thing.\n Yeah, we're",
      "offset": 498.08,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "we're gonna post the recording. I uh",
      "offset": 499.52,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "Yeah, there's I got to we got to edit a",
      "offset": 501.919,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "couple things out, but yeah, you can you",
      "offset": 503.599,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "can watch the the beginning.",
      "offset": 505.36,
      "duration": 5.119
    },
    {
      "lang": "en",
      "text": "All right, so let's talk about what the",
      "offset": 508.639,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "architecture of this whole system is and",
      "offset": 510.479,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "how we actually did this. And we will",
      "offset": 512,
      "duration": 3.039
    },
    {
      "lang": "en",
      "text": "probably try and record more of our",
      "offset": 513.599,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "coding sessions along the way. uh if",
      "offset": 515.039,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "people find it interesting, we can",
      "offset": 517.76,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "invite you to them and watch us code in",
      "offset": 518.8,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "real time as well uh along with just uh",
      "offset": 520.56,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "going to go do that. So there's actually",
      "offset": 523.279,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "there's actually three main parts to the",
      "offset": 526.56,
      "duration": 4.68
    },
    {
      "lang": "en",
      "text": "system",
      "offset": 528.24,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "and I think it's it's useful to draw",
      "offset": 531.68,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "them out totally different systems.",
      "offset": 533.279,
      "duration": 6.081
    },
    {
      "lang": "en",
      "text": "This one is the database.",
      "offset": 535.68,
      "duration": 7.56
    },
    {
      "lang": "en",
      "text": "Then we have the back end",
      "offset": 539.36,
      "duration": 3.88
    },
    {
      "lang": "en",
      "text": "and then we have the front end.",
      "offset": 547.12,
      "duration": 7.12
    },
    {
      "lang": "en",
      "text": "Okay. So another thing that we did is",
      "offset": 550.8,
      "duration": 6.32
    },
    {
      "lang": "en",
      "text": "what we said is",
      "offset": 554.24,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "we will actually not allow the uh front",
      "offset": 557.12,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "end to really get data from the back",
      "offset": 559.6,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "end. It's not allowed to. It's allowed",
      "offset": 561.2,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "to issue requests to the back end but it",
      "offset": 562.72,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "can't get data from the back end. The",
      "offset": 564.64,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "back end is allowed to read and write",
      "offset": 566.88,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "from the database and the front end is",
      "offset": 569.04,
      "duration": 5.359
    },
    {
      "lang": "en",
      "text": "allowed to read from the database.",
      "offset": 571.6,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "So we get really really nice queries of",
      "offset": 574.399,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "how things are being built out. And the",
      "offset": 576.88,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "reason that this is so important is",
      "offset": 579.12,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "because if you're doing something like",
      "offset": 581.6,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "streaming or interactive UIs, building",
      "offset": 582.8,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "out the system to from your back end",
      "offset": 585.76,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "that communicates with your front end is",
      "offset": 587.68,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "a pain in the ass. And if you're doing",
      "offset": 589.44,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "it on a lot of different places all the",
      "offset": 592.56,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "time, this is what real-time databases",
      "offset": 594.64,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "were made for. Just use them and you can",
      "offset": 597.6,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "solve a lot of your pain and you might",
      "offset": 600.56,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "run into\n and so basically the the data",
      "offset": 603.04,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "API is just your database schema. The",
      "offset": 606,
      "duration": 3.519
    },
    {
      "lang": "en",
      "text": "front end just needs to know the raw",
      "offset": 608.16,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "schema and it can query whatever you",
      "offset": 609.519,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "want to expose to it, right?",
      "offset": 611.04,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "Yes. Exactly. So as long as you have",
      "offset": 614.079,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "like a view in your database that is",
      "offset": 616.079,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "accessible uh that is read readable and",
      "offset": 618,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "you have like some materialized view on",
      "offset": 621.279,
      "duration": 2.881
    },
    {
      "lang": "en",
      "text": "it that is secure doesn't have anything",
      "offset": 622.8,
      "duration": 3.279
    },
    {
      "lang": "en",
      "text": "like PII data or something along that or",
      "offset": 624.16,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "at least requires like authenticated",
      "offset": 626.079,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "users to access it then you can go ahead",
      "offset": 627.6,
      "duration": 6.239
    },
    {
      "lang": "en",
      "text": "and go access this along the way. Um it",
      "offset": 630.56,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "does require some consideration of",
      "offset": 633.839,
      "duration": 2.721
    },
    {
      "lang": "en",
      "text": "exactly how you do it. The easiest way",
      "offset": 635.279,
      "duration": 2.881
    },
    {
      "lang": "en",
      "text": "to do it is a materialized view if you",
      "offset": 636.56,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "want to do it securely on your actual",
      "offset": 638.16,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "data. So you can go configure that in a",
      "offset": 640.24,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "good way.",
      "offset": 641.92,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "Um but once you can go do that now",
      "offset": 643.839,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "you're able to build a pipeline that is",
      "offset": 646.399,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "mostly just workflows in your back end",
      "offset": 647.92,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "that is issuing work at one after",
      "offset": 650.32,
      "duration": 4.959
    },
    {
      "lang": "en",
      "text": "another. So for example\n and you kind of",
      "offset": 652.88,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "get you get rid of this like state",
      "offset": 655.279,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "machiny thing where the front end's like",
      "offset": 657.12,
      "duration": 3.279
    },
    {
      "lang": "en",
      "text": "pushing data and then fetching it back",
      "offset": 658.72,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "out of the back end. It's kind of this",
      "offset": 660.399,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "unidirectional flow. And I think a lot",
      "offset": 661.92,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "of like this happened in React. We had",
      "offset": 664.32,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "React for like a year and a year and a",
      "offset": 666,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "half. And then people realized like the",
      "offset": 667.76,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "current backend paradigm in React, which",
      "offset": 669.76,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "at the time was things like Backbone.js",
      "offset": 672.48,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "and and things like that, just didn't",
      "offset": 674.32,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "work well for the amount of complexity",
      "offset": 676.399,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "people were using to add React into",
      "offset": 678.64,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "their apps. And all this stuff like Flux",
      "offset": 680.56,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "came out and there was explosion of like",
      "offset": 682.56,
      "duration": 3.519
    },
    {
      "lang": "en",
      "text": "six different frameworks all came out",
      "offset": 684.399,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "that one summer of like how do you do",
      "offset": 686.079,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "this unidirectional data flow because it",
      "offset": 687.76,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "was what worked for real-time dynamic",
      "offset": 690.48,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "applications.",
      "offset": 692.399,
      "duration": 3.041
    },
    {
      "lang": "en",
      "text": "Yeah. And this is a pattern that we've",
      "offset": 693.92,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "seen happen a lot like we just find it",
      "offset": 695.44,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "way easier what you and this is similar",
      "offset": 697.44,
      "duration": 2.48
    },
    {
      "lang": "en",
      "text": "to video games and all these other",
      "offset": 698.8,
      "duration": 3.279
    },
    {
      "lang": "en",
      "text": "things that you might want to do uh",
      "offset": 699.92,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "where what you really want to go do is",
      "offset": 702.079,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "you have some what I would call like",
      "offset": 704.16,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "state of truth that is represented here",
      "offset": 705.76,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "and all you want to do is you want to",
      "offset": 708.56,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "render the truth as fast as possible to",
      "offset": 710,
      "duration": 5.519
    },
    {
      "lang": "en",
      "text": "the front end and it's really easy to do",
      "offset": 712.16,
      "duration": 6.32
    },
    {
      "lang": "en",
      "text": "that when you're able to go ahead and",
      "offset": 715.519,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "actually model the truth in a way that",
      "offset": 718.48,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "is representative in the way that the",
      "offset": 720.24,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "front end can use it and the challenge",
      "offset": 722.399,
      "duration": 3.041
    },
    {
      "lang": "en",
      "text": "challenge then becomes building a schema",
      "offset": 723.839,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "that everything else can go reuse",
      "offset": 725.44,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "correctly. So you do have to plan that",
      "offset": 727.68,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "out a little bit, but I think it pays",
      "offset": 729.6,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "for itself. Um, personally in terms of",
      "offset": 731.68,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "like the developer ease and how easy",
      "offset": 734,
      "duration": 4.959
    },
    {
      "lang": "en",
      "text": "this ends up being. This also does",
      "offset": 736.399,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "really nice componentization for both",
      "offset": 738.959,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "like paralyzing workflows in terms of",
      "offset": 740.8,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "how many people can work on it and not",
      "offset": 742.399,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "just people but agents. It makes it way",
      "offset": 744.48,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "easier for my uh front end to actually",
      "offset": 746.88,
      "duration": 6.399
    },
    {
      "lang": "en",
      "text": "understand what it's rendering. If now",
      "offset": 750.48,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "the one part that we run into is in",
      "offset": 753.279,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "order to go do this, what you'll find is",
      "offset": 755.76,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "you have data models. Um,",
      "offset": 757.76,
      "duration": 6.8
    },
    {
      "lang": "en",
      "text": "sorry, I'll draw another rectangle. You",
      "offset": 762.24,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "have some data models that you're saving",
      "offset": 764.56,
      "duration": 6.36
    },
    {
      "lang": "en",
      "text": "into your database",
      "offset": 767.44,
      "duration": 3.48
    },
    {
      "lang": "en",
      "text": "into here. And these data models both",
      "offset": 771.44,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "have to be used by Python and written",
      "offset": 773.2,
      "duration": 2.879
    },
    {
      "lang": "en",
      "text": "from Python. And then they also have to",
      "offset": 774.72,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "be used and written by Type.",
      "offset": 776.079,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "Having some way to keep these in sync is",
      "offset": 778.72,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "also really important. Otherwise, you",
      "offset": 780.48,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "end up again in a world of pain and that",
      "offset": 781.92,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "doesn't really work. And figuring out",
      "offset": 784.16,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "which side you're on is really, really",
      "offset": 786.72,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "important of how you go do that. But",
      "offset": 788.399,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "once you can nail that down, your",
      "offset": 790.399,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "iteration loop is going to be really",
      "offset": 792.24,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "fast. And adding new features, as we'll",
      "offset": 793.839,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "see today, when we add another feature",
      "offset": 795.839,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "such as chapter summary into our system,",
      "offset": 797.76,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "becomes a thing that you can just buy",
      "offset": 800.639,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "code trivially",
      "offset": 802.16,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "and you don't have to think about it",
      "offset": 804.079,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "anymore. uh because your agent kind of",
      "offset": 805.44,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "understands the architecture of what",
      "offset": 807.519,
      "duration": 3.041
    },
    {
      "lang": "en",
      "text": "you're building out and it's so nicely",
      "offset": 808.88,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "componentized that it doesn't have room",
      "offset": 810.56,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "for making mistakes along the way. It's",
      "offset": 812.16,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "like an a you basically built in an API",
      "offset": 815.04,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "contract. If your front end was both",
      "offset": 817.12,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "reading and writing to the database, you",
      "offset": 818.8,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "now have a choice ever the agent now has",
      "offset": 821.2,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "to make an implicit decision of do I do",
      "offset": 823.519,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "this in the front end or do I do this in",
      "offset": 825.36,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "the back end and that's a choice",
      "offset": 826.639,
      "duration": 5.041
    },
    {
      "lang": "en",
      "text": "and it might make one choice correctly,",
      "offset": 830.079,
      "duration": 3.281
    },
    {
      "lang": "en",
      "text": "one choice wrong. But the more you go",
      "offset": 831.68,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "along, the more choices that will be",
      "offset": 833.36,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "made incorrectly just by volume and by",
      "offset": 835.519,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "the nature of it happening",
      "offset": 837.839,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "automatically. By doing this, we make a",
      "offset": 838.959,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "very clear direction of what Dexter is",
      "offset": 841.279,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "saying. It's very easy what it does. The",
      "offset": 843.44,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "front end's job is to render the content",
      "offset": 845.04,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "and issue new background tasks. The",
      "offset": 846.639,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "backend job is to plum and churn on data",
      "offset": 848.8,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "and eventually write updates to the",
      "offset": 851.04,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "database. The database's job is to send",
      "offset": 852.56,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "data out to the front end as soon as",
      "offset": 854.959,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "possible. And now it's very clear what",
      "offset": 856.56,
      "duration": 5.279
    },
    {
      "lang": "en",
      "text": "we have to go represent along the way.",
      "offset": 858.56,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "Um, with that, let's talk about some of",
      "offset": 861.839,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "our routes. Um,",
      "offset": 864.8,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "I do have a very opinionated stance on",
      "offset": 867.279,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "what's a great data model agnostic",
      "offset": 870,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "library as we'll see today. Um, but",
      "offset": 871.76,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "we'll show that in a second. We'll show",
      "offset": 874.48,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "the full code.",
      "offset": 875.76,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "Um, so what's a what's the APIs that we",
      "offset": 877.68,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "actually have? We have a couple APIs",
      "offset": 880,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "that we defined. Um,",
      "offset": 881.76,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "we have Dexter. Do you remember what",
      "offset": 884.32,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "they are by",
      "offset": 885.92,
      "duration": 7.279
    },
    {
      "lang": "en",
      "text": "video? We have one API to like uh submit",
      "offset": 888.079,
      "duration": 7.841
    },
    {
      "lang": "en",
      "text": "a new request.\n Uh did you push the",
      "offset": 893.199,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "latest code? I can pull it down and and",
      "offset": 895.92,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "pull that out real quick.\n Yeah, let me",
      "offset": 897.44,
      "duration": 6.68
    },
    {
      "lang": "en",
      "text": "do that. Actually, I don't think I did.",
      "offset": 899.76,
      "duration": 4.36
    },
    {
      "lang": "en",
      "text": "Um Oh, I might have.",
      "offset": 904.48,
      "duration": 6.359
    },
    {
      "lang": "en",
      "text": "I guess I did.",
      "offset": 907.279,
      "duration": 3.56
    },
    {
      "lang": "en",
      "text": "Uh cool. I will get the API endpoints.",
      "offset": 912.16,
      "duration": 6.479
    },
    {
      "lang": "en",
      "text": "Turns out there's uh AI is pretty good",
      "offset": 916.48,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "at generating a list of API endpoints.",
      "offset": 918.639,
      "duration": 6.64
    },
    {
      "lang": "en",
      "text": "Provide feedback. Um and then we what we",
      "offset": 921.68,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "do is we submit a new request, provide",
      "offset": 925.279,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "feedback. Um get list of videos. So",
      "offset": 926.8,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "there are some things that we do from",
      "offset": 930.48,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "the back end. Um get list of videos,",
      "offset": 931.839,
      "duration": 5.041
    },
    {
      "lang": "en",
      "text": "provide feedback, and then what's the",
      "offset": 935.519,
      "duration": 2.56
    },
    {
      "lang": "en",
      "text": "other stuff? I think there's one more.",
      "offset": 936.88,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "Get title. Oh, there we go. I don't know",
      "offset": 938.079,
      "duration": 6.281
    },
    {
      "lang": "en",
      "text": "why I didn't do that.",
      "offset": 940.72,
      "duration": 3.64
    },
    {
      "lang": "en",
      "text": "Okay, so these are basically all the",
      "offset": 944.56,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "APIs that we have. Um, we did have to",
      "offset": 947.04,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "build one integration because we get",
      "offset": 949.36,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "actual Zoom API calls. We don't want to",
      "offset": 950.8,
      "duration": 2.959
    },
    {
      "lang": "en",
      "text": "do that in a real-time database and we",
      "offset": 952.48,
      "duration": 2.719
    },
    {
      "lang": "en",
      "text": "want the back end to have access. So",
      "offset": 953.759,
      "duration": 2.481
    },
    {
      "lang": "en",
      "text": "this is something that the front end",
      "offset": 955.199,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "queries from directly. This is a way to",
      "offset": 956.24,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "add feedback to a draft. So, if you",
      "offset": 958.8,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "don't like an email, um what we're able",
      "offset": 960.24,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "to do is we can just go through and say",
      "offset": 962.32,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "like, &quot;Oh,",
      "offset": 966,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "um I can just leave feedback and say,",
      "offset": 967.68,
      "duration": 7.36
    },
    {
      "lang": "en",
      "text": "eh, this is",
      "offset": 970,
      "duration": 7.92
    },
    {
      "lang": "en",
      "text": "not long enough.&quot;",
      "offset": 975.04,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "Dude, you got to make it more uh Jan",
      "offset": 977.92,
      "duration": 6.8
    },
    {
      "lang": "en",
      "text": "alpha\n enough uh emojis.",
      "offset": 979.759,
      "duration": 6.88
    },
    {
      "lang": "en",
      "text": "And I'll just refine the email. And what",
      "offset": 984.72,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "this will do is this will commit a task",
      "offset": 986.639,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "and eventually the email will get",
      "offset": 988.48,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "refined along the way. Uh and I did not",
      "offset": 989.839,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "I think I don't know whether I don't",
      "offset": 993.04,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "know if you're planning on um getting",
      "offset": 994.8,
      "duration": 2.64
    },
    {
      "lang": "en",
      "text": "this but I think it would be really",
      "offset": 996.56,
      "duration": 2.16
    },
    {
      "lang": "en",
      "text": "helpful to kind of see the sequence",
      "offset": 997.44,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "diagram or the flowchart of like how how",
      "offset": 998.72,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "stuff flows through the system.\n Yeah, I",
      "offset": 1001.279,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "was going to get to that once I show all",
      "offset": 1004.8,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "the routes.\n Okay,\n that's a good idea.",
      "offset": 1006,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "So and then we basically have all these",
      "offset": 1008.88,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "routes. So let's um let's look at what",
      "offset": 1010.24,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "this actually ends up looking like in a",
      "offset": 1012.079,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "sequence flow in that case. So we have",
      "offset": 1014,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "all these things. So the first thing",
      "offset": 1016.079,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "that we have is Oops. I can't draw it",
      "offset": 1017.44,
      "duration": 5.079
    },
    {
      "lang": "en",
      "text": "there.",
      "offset": 1019.519,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "I will try and do the side by side.\n Oh,",
      "offset": 1023.12,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "I'm missing a couple end points. One",
      "offset": 1026.079,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "sec.\n Oh, you are. Okay. That's what I",
      "offset": 1028,
      "duration": 6.12
    },
    {
      "lang": "en",
      "text": "thought. It looks like you don't\n Yeah.",
      "offset": 1029.679,
      "duration": 4.441
    },
    {
      "lang": "en",
      "text": "Yeah. The first five are actually the",
      "offset": 1034.88,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "most important.",
      "offset": 1036.4,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "Oh, did I lose this?",
      "offset": 1039.6,
      "duration": 5.599
    },
    {
      "lang": "en",
      "text": "Oh, sorry. I have to adjust my Okay,",
      "offset": 1042.4,
      "duration": 6.399
    },
    {
      "lang": "en",
      "text": "there we go. Um, so what we have here",
      "offset": 1045.199,
      "duration": 5.761
    },
    {
      "lang": "en",
      "text": "looks like this. The first step that",
      "offset": 1048.799,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "happens is the back end submits a task",
      "offset": 1050.96,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "to say like I want to process this",
      "offset": 1052.72,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "specific meeting ID.",
      "offset": 1054.4,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "Uh, the front end submits the task to",
      "offset": 1057.52,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "submit I want to process meeting ID. The",
      "offset": 1058.96,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "back end then gets all the submits a",
      "offset": 1060.96,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "background task. So it responds very",
      "offset": 1062.88,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "very fast to the front end saying cool I",
      "offset": 1064.88,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "got you. And then the background task",
      "offset": 1067.12,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "will go ahead and kick off and then say",
      "offset": 1069.6,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "something like",
      "offset": 1072,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "the following where it will say",
      "offset": 1074,
      "duration": 6.32
    },
    {
      "lang": "en",
      "text": "something like um download",
      "offset": 1076.08,
      "duration": 7.04
    },
    {
      "lang": "en",
      "text": "download video plus transcript.",
      "offset": 1080.32,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "Um finding the video is a little bit",
      "offset": 1083.12,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "more complicated I presume because not",
      "offset": 1084.799,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "all Zoom videos will only have one video",
      "offset": 1086.799,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "say sometimes have multiple because",
      "offset": 1088.64,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "people start and stop the recording and",
      "offset": 1090,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "other things along the way. So we have",
      "offset": 1091.44,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "to go do that.",
      "offset": 1093.6,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "Then we will kick off a couple jobs in",
      "offset": 1095.679,
      "duration": 5.681
    },
    {
      "lang": "en",
      "text": "parallel. One of them is upload to",
      "offset": 1097.76,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "YouTube",
      "offset": 1101.36,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "and then simultaneously",
      "offset": 1104,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "we also kick off the",
      "offset": 1106.32,
      "duration": 6.88
    },
    {
      "lang": "en",
      "text": "summarized uh summarized task. Once the",
      "offset": 1109.76,
      "duration": 5.919
    },
    {
      "lang": "en",
      "text": "summarized task is done, we kick off",
      "offset": 1113.2,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "three tasks in parallel again.",
      "offset": 1115.679,
      "duration": 7.24
    },
    {
      "lang": "en",
      "text": "Uh, draft email,",
      "offset": 1118.64,
      "duration": 4.279
    },
    {
      "lang": "en",
      "text": "draft Twitter,",
      "offset": 1123.679,
      "duration": 3.401
    },
    {
      "lang": "en",
      "text": "draft X, and then draft um, LinkedIn.",
      "offset": 1127.76,
      "duration": 5.64
    },
    {
      "lang": "en",
      "text": "Does the summarize task also output the",
      "offset": 1133.6,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "like key points and stuff like that?",
      "offset": 1135.84,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "Exactly. That that's what it output. It",
      "offset": 1137.76,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "outputs like a general summary of",
      "offset": 1139.28,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "everything. Um,\n so the database updates",
      "offset": 1140.48,
      "duration": 6.319
    },
    {
      "lang": "en",
      "text": "from each of these is like basically",
      "offset": 1144.24,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "like",
      "offset": 1146.799,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "mark uploaded in DB.",
      "offset": 1148.32,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "Exactly.",
      "offset": 1151.44,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "And then this is like add summary points",
      "offset": 1153.12,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "to DB.",
      "offset": 1156.24,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "Exactly. And actually this one streams",
      "offset": 1158.08,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "in. So I actually like save the whole",
      "offset": 1160.32,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "thing on every update of the tick of the",
      "offset": 1161.84,
      "duration": 6.68
    },
    {
      "lang": "en",
      "text": "stream.\n Yep.\n Um,",
      "offset": 1163.6,
      "duration": 4.92
    },
    {
      "lang": "en",
      "text": "exactly. Um, and technically there's a",
      "offset": 1170.08,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "special thing that I Oops.",
      "offset": 1172.64,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "There's one last thing that I do over",
      "offset": 1175.6,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "here, which is like",
      "offset": 1177.12,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "Whoops.",
      "offset": 1179.52,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "I I mark I I mark the database as it's",
      "offset": 1182.48,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "done summarizing.",
      "offset": 1185.44,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "It's important I can like mark the state",
      "offset": 1187.84,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "at each single one of them. And that's",
      "offset": 1189.6,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "that that's something else I had to do",
      "offset": 1191.44,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "because once I was done some\n that's",
      "offset": 1192.96,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "actually I I think that's actually worth",
      "offset": 1194.48,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "maybe taking a sec to drill into is just",
      "offset": 1197.12,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "kind of like having your kind of job",
      "offset": 1198.96,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "system just be a table with a bunch of",
      "offset": 1201.919,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "nullable columns and you can tell what's",
      "offset": 1204.32,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "been done just by what's set in the",
      "offset": 1206.16,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "database. And in that way, I mean, it's",
      "offset": 1208,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "it's not as like robust as something",
      "offset": 1210.24,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "like big and like chunky like temporal",
      "offset": 1212.08,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "or something like that, but it is",
      "offset": 1214.799,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "honestly probably good enough,",
      "offset": 1217.28,
      "duration": 2.56
    },
    {
      "lang": "en",
      "text": "especially if you're building like",
      "offset": 1218.72,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "internal tools for yourself. Um, which",
      "offset": 1219.84,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "is, uh, super super high leverage I",
      "offset": 1222.16,
      "duration": 6.32
    },
    {
      "lang": "en",
      "text": "think these days. Um, just that idea of",
      "offset": 1225.84,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "like a really simple job system.",
      "offset": 1228.48,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "The problem that I ran into when I was",
      "offset": 1231.2,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "doing purely nullable fields is that it",
      "offset": 1233.039,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "wasn't I wasn't doing JSON parsing. Uh,",
      "offset": 1235.2,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "sorry. It like once I was streaming, I",
      "offset": 1237.76,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "had something in there when I wasn't",
      "offset": 1239.76,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "done. So, I actually needed to keep a",
      "offset": 1241.12,
      "duration": 5.919
    },
    {
      "lang": "en",
      "text": "status of it a little bit more.\n I see.",
      "offset": 1243.6,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "Because like I needed to know the status",
      "offset": 1247.039,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "of like is it started, is it streaming,",
      "offset": 1248.48,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "or is it done\n and the status was really",
      "offset": 1251.12,
      "duration": 5.919
    },
    {
      "lang": "en",
      "text": "important",
      "offset": 1254.24,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "and having that available was important",
      "offset": 1257.039,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "to actually show the UI there.\n Um, and",
      "offset": 1258.96,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "then the other thing that I did was I",
      "offset": 1261.44,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "got bored so I added one last thing. I",
      "offset": 1263.12,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "added uh while I was at it, I was like,",
      "offset": 1265.12,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "&quot;Okay, let's just why not?&quot; We did these",
      "offset": 1266.72,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "three things. So, I just asked it to",
      "offset": 1268.96,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "literally follow this example along the",
      "offset": 1271.2,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "way. And I said, &quot;Draft a title for me",
      "offset": 1272.96,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "after you're done.&quot;",
      "offset": 1277.12,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "Uh, so that became the next thing that",
      "offset": 1279.12,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "we did. So, now we had four pipelines",
      "offset": 1280.64,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "that could trigger and process. Once",
      "offset": 1283.039,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "this was done, I have another thing that",
      "offset": 1285.36,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "I could do, which was a user,\n Do",
      "offset": 1287.12,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "Yeah.\n Sorry. Do me a favor real quick.",
      "offset": 1290.72,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "Hit five.",
      "offset": 1292.48,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "and just change the arrow to this one",
      "offset": 1295.44,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "over here\n because those macaroni those",
      "offset": 1297.2,
      "duration": 7.04
    },
    {
      "lang": "en",
      "text": "macaroni errors arrows suck.\n I agree.",
      "offset": 1300.4,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "Oh, so you got Yeah, if you hit five",
      "offset": 1304.24,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "twice, I think it cycles through. It",
      "offset": 1306,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "used to happen to me all the time. Still",
      "offset": 1307.6,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "happens to me all the time.\n Thank you.",
      "offset": 1309.36,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "This is why I'm really bad at Excel.\n Um,",
      "offset": 1311.76,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "then we get another\n pretty [Â __Â ] good,",
      "offset": 1314.72,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "dude.",
      "offset": 1317.2,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "Now we have another request that the",
      "offset": 1319.44,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "user can do once all these are done. And",
      "offset": 1321.12,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "remember all this is streaming to the",
      "offset": 1322.799,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "real-time database. So it's very very",
      "offset": 1324.4,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "easy for my front end to render this",
      "offset": 1326.88,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "because it's just rendering a blob of",
      "offset": 1329.6,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "JSON that is strongly typed and I know",
      "offset": 1331.44,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "how to render it. So it becomes trivial",
      "offset": 1333.28,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "for me to go render. I just build react",
      "offset": 1334.799,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "components for all those types.",
      "offset": 1336.48,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "Then I built a second part of the",
      "offset": 1339.44,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "pipeline which said like if the user",
      "offset": 1340.799,
      "duration": 8.721
    },
    {
      "lang": "en",
      "text": "will make like a draft ID content type",
      "offset": 1344.159,
      "duration": 7.76
    },
    {
      "lang": "en",
      "text": "and then their feedback. So the user",
      "offset": 1349.52,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "wants to commit feedback to one of these",
      "offset": 1351.919,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "emails. So each of these drafts has an",
      "offset": 1354,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "ID. They have a version history and",
      "offset": 1356.4,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "everything attached to it. There's",
      "offset": 1357.76,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "different content types and medium. So",
      "offset": 1359.52,
      "duration": 5.519
    },
    {
      "lang": "en",
      "text": "like email, X, and LinkedIn.",
      "offset": 1361.36,
      "duration": 5.439
    },
    {
      "lang": "en",
      "text": "And once this happens, we do the same",
      "offset": 1365.039,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "exact thing that we were doing before.",
      "offset": 1366.799,
      "duration": 4.921
    },
    {
      "lang": "en",
      "text": "Oops.",
      "offset": 1368.72,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "Where this time instead of our normal",
      "offset": 1372.559,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "background, we kick off another",
      "offset": 1375.919,
      "duration": 3.521
    },
    {
      "lang": "en",
      "text": "background process.",
      "offset": 1377.36,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "And this background process again",
      "offset": 1379.44,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "responds very very fast to the OM saying",
      "offset": 1381.28,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "okay cool I'll I'll take care of that",
      "offset": 1384.72,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "for you.",
      "offset": 1386.24,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "And once that is done, it goes to the",
      "offset": 1388.159,
      "duration": 5.921
    },
    {
      "lang": "en",
      "text": "real-time database, pulls out the draft,",
      "offset": 1389.84,
      "duration": 7.68
    },
    {
      "lang": "en",
      "text": "pulls out the transcript,",
      "offset": 1394.08,
      "duration": 6.64
    },
    {
      "lang": "en",
      "text": "uh, draft,",
      "offset": 1397.52,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "transcript,",
      "offset": 1400.72,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "and then issues that to a prompt, saves",
      "offset": 1402.96,
      "duration": 5.839
    },
    {
      "lang": "en",
      "text": "a new version, and then writes that to",
      "offset": 1405.76,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "the database. And that's all this",
      "offset": 1408.799,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "pipeline is. There's nothing fancy about",
      "offset": 1410.799,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "this and it's really just kicking off",
      "offset": 1413.52,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "background workers constantly to go do",
      "offset": 1416,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "work for me while I go and allow myself",
      "offset": 1418.08,
      "duration": 6.64
    },
    {
      "lang": "en",
      "text": "to iterate fast along the way.",
      "offset": 1421.12,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "Does this give everyone a good idea of",
      "offset": 1424.72,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "what the background pipeline looks like",
      "offset": 1426,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "and how we're able to go and iterate on",
      "offset": 1428.159,
      "duration": 6.121
    },
    {
      "lang": "en",
      "text": "this pipeline along the way?",
      "offset": 1430.159,
      "duration": 4.121
    },
    {
      "lang": "en",
      "text": "Uh there's a question from Vijay.",
      "offset": 1436.72,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "So in the first instance when you",
      "offset": 1440.32,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "created the draft, did you use the",
      "offset": 1442.48,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "transcript or did you use the uh summary",
      "offset": 1444.88,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "of the transcript?\n I just used I I I at",
      "offset": 1447.12,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "the time I didn't actually think about",
      "offset": 1451.12,
      "duration": 2.48
    },
    {
      "lang": "en",
      "text": "using the transcript. So I just used the",
      "offset": 1452.4,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "transcript. We can update that to go use",
      "offset": 1453.6,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "the transcript as well. This is why the",
      "offset": 1455.36,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "summary. Got it.\n Yeah, I only use the",
      "offset": 1458.159,
      "duration": 5.041
    },
    {
      "lang": "en",
      "text": "summary, not the transcript. Sorry. Uh",
      "offset": 1460.72,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "this is why the these initial drafts are",
      "offset": 1463.2,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "bad.",
      "offset": 1465.76,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "That's uh so like when we go look on",
      "offset": 1468.64,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "this and we actually look at the UI on",
      "offset": 1470.4,
      "duration": 5.399
    },
    {
      "lang": "en",
      "text": "here. Where'd it go?",
      "offset": 1472,
      "duration": 3.799
    },
    {
      "lang": "en",
      "text": "Oops.",
      "offset": 1476.159,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "Find the right screen.",
      "offset": 1480.4,
      "duration": 3.56
    },
    {
      "lang": "en",
      "text": "Don't know where it went.",
      "offset": 1484.64,
      "duration": 7.48
    },
    {
      "lang": "en",
      "text": "I don't know where Arc is. Sorry.",
      "offset": 1487.76,
      "duration": 4.36
    },
    {
      "lang": "en",
      "text": "Okay. Okay, I guess it's here. Um, this",
      "offset": 1495.6,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "is why the original draft was really",
      "offset": 1497.919,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "bad. And like now that I told it to go",
      "offset": 1499.44,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "and add more feedback with like emojis,",
      "offset": 1501.12,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "you can see it added more emojis in this",
      "offset": 1502.72,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "one, but the first one didn't have any",
      "offset": 1504.24,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "of that. And like it's going to be bad.",
      "offset": 1506.48,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "It doesn't have any of this. It writes",
      "offset": 1508.08,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "your name. I have to fill this stuff",
      "offset": 1509.6,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "out. It's kind of annoying. Um, but",
      "offset": 1511.12,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "that's because I have really given it no",
      "offset": 1514.08,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "context. And the AI part of this is not",
      "offset": 1516,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "even close to done.\n Okay, cool. So, so",
      "offset": 1518.08,
      "duration": 4.959
    },
    {
      "lang": "en",
      "text": "to test this, so to test this, what",
      "offset": 1521.039,
      "duration": 2.801
    },
    {
      "lang": "en",
      "text": "we're going to do is we're going to",
      "offset": 1523.039,
      "duration": 2.161
    },
    {
      "lang": "en",
      "text": "change the prompt and then we're going",
      "offset": 1523.84,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "to reimpport a whole video from scratch",
      "offset": 1525.2,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "and then we're going to see if the",
      "offset": 1527.76,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "drafts are better, right?\n Well, we can",
      "offset": 1528.72,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "just regenerate the summary. But yes, uh",
      "offset": 1531.36,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "we don't actually have to\n No, no, I was",
      "offset": 1533.84,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "No, that was your leadin. You're This is",
      "offset": 1536.32,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "This is where we uh we go way down into",
      "offset": 1538.4,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "the interloop and we say, &quot;Hey, okay,",
      "offset": 1540.64,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "cool. Let's go get a real record from",
      "offset": 1542.24,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "the database and then like write a test",
      "offset": 1544,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "for it.&quot;\n Yeah. Let's go do that.",
      "offset": 1545.76,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "Actually,\n that's the uh that's the old",
      "offset": 1548.159,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "way. That's how I would have done this a",
      "offset": 1550.64,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "year ago is just kind of test the thing",
      "offset": 1552,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "end to end, right?\n Well, I I think",
      "offset": 1553.52,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "that's the point like let's show the end",
      "offset": 1556.24,
      "duration": 3.039
    },
    {
      "lang": "en",
      "text": "to end testing loop and let's show the",
      "offset": 1557.679,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "faster loop once we're able to go do it.",
      "offset": 1559.279,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "So, in this case, does that answer your",
      "offset": 1561.84,
      "duration": 5.079
    },
    {
      "lang": "en",
      "text": "question, BJ?",
      "offset": 1563.76,
      "duration": 3.159
    },
    {
      "lang": "en",
      "text": "Yes, it does. Thank you.\n Perfect. Yes.",
      "offset": 1568.24,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "So, now let's go ahead and actually go",
      "offset": 1571.36,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "and try and make some of this pipeline",
      "offset": 1573.279,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "better. Now, there's a couple questions",
      "offset": 1574.88,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "asked about how do we keep all the state",
      "offset": 1576.559,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "in sync and everything. So, I'll just",
      "offset": 1578,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "show you a little bit more of the code",
      "offset": 1579.279,
      "duration": 2.721
    },
    {
      "lang": "en",
      "text": "and what it ends up looking like because",
      "offset": 1580.72,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "I think it is useful to have some dive",
      "offset": 1582,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "through before we go and change a bunch",
      "offset": 1584.24,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "of things. I'm also going to show how to",
      "offset": 1585.919,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "go make this work with claw code and how",
      "offset": 1588.32,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "to go edit it to add a new pip part of",
      "offset": 1590.88,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "the pipeline because the problem that I",
      "offset": 1592.64,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "actually noticed the most if we actually",
      "offset": 1594.559,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "look at some of the prompts,",
      "offset": 1596.72,
      "duration": 6.319
    },
    {
      "lang": "en",
      "text": "they're just long. it just like doesn't",
      "offset": 1600.159,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "really talk about the real part of it.",
      "offset": 1603.039,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "Like it adds a feedback. It dumps a",
      "offset": 1604.799,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "transcript in there, which is fine, but",
      "offset": 1606.48,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "it's like",
      "offset": 1607.919,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "it it doesn't really really really",
      "offset": 1609.76,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "understand what I mean by actually",
      "offset": 1612.88,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "making the prompt better uh and what",
      "offset": 1614.48,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "makes a good email. It's same thing with",
      "offset": 1617.12,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "this. is like it doesn't really",
      "offset": 1618.88,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "understand about what is a good Twitter",
      "offset": 1620.96,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "thread when it does this along the way",
      "offset": 1622.72,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "cuz I don't think this",
      "offset": 1624.96,
      "duration": 5.599
    },
    {
      "lang": "en",
      "text": "it's it's giving it instructions about",
      "offset": 1628.48,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "what to do not so much about like what",
      "offset": 1630.559,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "looks good like what good looks like",
      "offset": 1633.2,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "exactly and like but we can't focus on",
      "offset": 1635.919,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "what good looks like until we have the",
      "offset": 1638.72,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "mechanics of what to do really plumbed",
      "offset": 1640.24,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "in because I will spend a lot more time",
      "offset": 1642.24,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "iterating on the infrastructure like I",
      "offset": 1644.64,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "said this took about eight hours of work",
      "offset": 1646.24,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "to go from nothing to a fully generating",
      "offset": 1648.08,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "SAS flow that does this. Um, and that's",
      "offset": 1650.159,
      "duration": 6.4
    },
    {
      "lang": "en",
      "text": "worth doing. But if I just built the AI",
      "offset": 1653.6,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "part, I wouldn't even if I thought this",
      "offset": 1656.559,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "was useful, I would Dexture and I",
      "offset": 1658.799,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "wouldn't actually use this app. It would",
      "offset": 1660.559,
      "duration": 3.521
    },
    {
      "lang": "en",
      "text": "be unusable. So, we need to build a flow",
      "offset": 1662.159,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "that made sense for us in a way. And",
      "offset": 1664.08,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "then we can go and make the AI part",
      "offset": 1666.88,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "better. Otherwise, you just have a bunch",
      "offset": 1668.4,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "of Python scripts that you don't",
      "offset": 1669.76,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "actually use and now they're trash. And",
      "offset": 1671.36,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "that was a waste of time. So building",
      "offset": 1674.08,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "the whole thing out is worth it because",
      "offset": 1676.24,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "then you can decide how to actually",
      "offset": 1678.08,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "design the AI along the way. Um with",
      "offset": 1679.6,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "that let's talk about some of this",
      "offset": 1682.799,
      "duration": 3.281
    },
    {
      "lang": "en",
      "text": "stuff. So this data model called Twitter",
      "offset": 1684,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "thread. This is what we're rendering",
      "offset": 1686.08,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "both in the back end and the front end.",
      "offset": 1688.559,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "I'll show you how this gets used in the",
      "offset": 1690.96,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "front end.",
      "offset": 1693.44,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "Uh and I'm just going to exclude all the",
      "offset": 1695.12,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "val code.",
      "offset": 1696.64,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "Oops.",
      "offset": 1699.679,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "Um all we do is we just regenerate",
      "offset": 1701.76,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "types. and we generate types for both",
      "offset": 1703.919,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "front end and backend types and now we",
      "offset": 1705.679,
      "duration": 3.281
    },
    {
      "lang": "en",
      "text": "just use them here. So in this case we",
      "offset": 1707.279,
      "duration": 2.961
    },
    {
      "lang": "en",
      "text": "have a Twitter thread. This gets",
      "offset": 1708.96,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "imported and now my UI component just",
      "offset": 1710.24,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "renders this. So what's really nice",
      "offset": 1712.159,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "about this is if I tell the model to go",
      "offset": 1714,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "change what it means to be a Twitter",
      "offset": 1716.159,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "thread, my front end will break at",
      "offset": 1717.52,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "compile time and now it will go actually",
      "offset": 1719.679,
      "duration": 5.041
    },
    {
      "lang": "en",
      "text": "cloud code can go fix it for me. So I",
      "offset": 1722.48,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "actually get types that are in sync",
      "offset": 1724.72,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "perfectly along the way rather than",
      "offset": 1726.24,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "anything else. Um and the way that I'm",
      "offset": 1728.559,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "able to do this is I just codegen both",
      "offset": 1731.52,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "types. I cogen the Python versions that",
      "offset": 1733.52,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "my back end uses and writes to the",
      "offset": 1735.52,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "database and then I co-gen the React",
      "offset": 1737.52,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "versions that my front end uses and I",
      "offset": 1739.679,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "just use the same types to basically",
      "offset": 1742.72,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "issue the glue code along the way. So I",
      "offset": 1744.64,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "think Joe asked that question earlier I",
      "offset": 1747.2,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "use to go do that and this is what I",
      "offset": 1749.2,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "found worked for me.",
      "offset": 1750.799,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "Um,\n okay. So, you have the back end. The",
      "offset": 1752.559,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "back end is getting the transcript and",
      "offset": 1755.12,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "then it's passing it to methods in BAML",
      "offset": 1756.88,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "in Python with using Pyantic. And then",
      "offset": 1759.2,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "when it gets them back, it's storing",
      "offset": 1761.12,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "those to the database. And because it's",
      "offset": 1762.32,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "all generated from the same BAML kind of",
      "offset": 1764.559,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "contract API specification, when those",
      "offset": 1766.559,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "same kind of JSON objects get served to",
      "offset": 1769.36,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "the front end, the front end can use the",
      "offset": 1771.76,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "generated client uh that that they got",
      "offset": 1773.52,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "made from the exact same specification",
      "offset": 1776.799,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "to render those things and have type",
      "offset": 1778.96,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "safety around them and know exactly what",
      "offset": 1780.88,
      "duration": 2.399
    },
    {
      "lang": "en",
      "text": "they look like when they're being",
      "offset": 1782.32,
      "duration": 2.479
    },
    {
      "lang": "en",
      "text": "rendered.",
      "offset": 1783.279,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "Exactly. Exactly.\n Cool.\n So, you get",
      "offset": 1784.799,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "really, really convenient type systems",
      "offset": 1788,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "along the way. Um, with that, let's",
      "offset": 1789.52,
      "duration": 5.519
    },
    {
      "lang": "en",
      "text": "focus on actually making some of these",
      "offset": 1793.76,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "prompts better. Um, so the first thing",
      "offset": 1795.039,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "that we'll notice is generate a custom",
      "offset": 1797.44,
      "duration": 6.479
    },
    {
      "lang": "en",
      "text": "Twitter thread. Um, one thing that I did",
      "offset": 1799.76,
      "duration": 8.399
    },
    {
      "lang": "en",
      "text": "not do because I was voting, um,",
      "offset": 1803.919,
      "duration": 6.401
    },
    {
      "lang": "en",
      "text": "I found this helpful writing the UI much",
      "offset": 1808.159,
      "duration": 6.161
    },
    {
      "lang": "en",
      "text": "better. Um, the V0 UI is okay, but the",
      "offset": 1810.32,
      "duration": 5.359
    },
    {
      "lang": "en",
      "text": "problem is when you're actually syncing",
      "offset": 1814.32,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "a back end and front end together, using",
      "offset": 1815.679,
      "duration": 5.921
    },
    {
      "lang": "en",
      "text": "Vzero is a really really it's hard. Uh,",
      "offset": 1817.84,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "because what I need\n you have to find a",
      "offset": 1821.6,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "way to well, you have to find a way to",
      "offset": 1823.039,
      "duration": 5.041
    },
    {
      "lang": "en",
      "text": "make sure vzero fully understands the",
      "offset": 1825.6,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "like data contract and the models that",
      "offset": 1828.08,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "are available. Right.\n Exactly.",
      "offset": 1829.84,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "Otherwise, it just changes things and",
      "offset": 1832.08,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "then I'm stuck in a I'm iterating way",
      "offset": 1833.76,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "slower. And it's fine if your entire app",
      "offset": 1835.6,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "is purely built in Typescript. But if",
      "offset": 1839.12,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "you have longunning workflows, it's not",
      "offset": 1840.799,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "going to be built purely in like your",
      "offset": 1842.48,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "Nex.js app won't define everything.",
      "offset": 1844.48,
      "duration": 5.919
    },
    {
      "lang": "en",
      "text": "Um, and like that just doesn't work. So",
      "offset": 1847.679,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "that's why I had to go down this road.",
      "offset": 1850.399,
      "duration": 2.801
    },
    {
      "lang": "en",
      "text": "And I think a lot of people are using",
      "offset": 1852,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "like non Typescript backends or they",
      "offset": 1853.2,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "have non-typescript back ends already",
      "offset": 1855.52,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "that they want to pull data from. And",
      "offset": 1856.96,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "it's just useful to have one data",
      "offset": 1860.159,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "architecture that does this. You can",
      "offset": 1861.679,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "still do it in v 0ero, but it's just the",
      "offset": 1863.36,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "it doesn't have the context and if it",
      "offset": 1865.44,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "breaks the types, then my whole app is",
      "offset": 1867.44,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "broken. That sucks.",
      "offset": 1868.799,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "Um,",
      "offset": 1870.96,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "with that, let's focus on specifically",
      "offset": 1872.64,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "one of it. Um, I think the thing that I",
      "offset": 1874.96,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "dislike the most is actually the email",
      "offset": 1876.72,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "draft. And Dexter and I have a couple",
      "offset": 1878.48,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "emails that we've sent out. We like",
      "offset": 1881.36,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "those. So, why don't we just do",
      "offset": 1883.12,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "something silly",
      "offset": 1885.44,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "and see if we can make the email draft",
      "offset": 1887.36,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "better by doing the dumbest thing",
      "offset": 1889.44,
      "duration": 6.8
    },
    {
      "lang": "en",
      "text": "possible, which is first\n Oh, wow. Is the",
      "offset": 1891.52,
      "duration": 7.92
    },
    {
      "lang": "en",
      "text": "never use prompting guy going to use fot",
      "offset": 1896.24,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "prompting in this episode.\n I'm going to",
      "offset": 1899.44,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "add a transcript actually first.\n Okay.",
      "offset": 1902,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "Okay. Now, this is not going to work.",
      "offset": 1904.64,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "So, what I'm going to do is I'm going to",
      "offset": 1906.48,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "go to cloud code and I will tell it",
      "offset": 1907.84,
      "duration": 7.76
    },
    {
      "lang": "en",
      "text": "um to um update",
      "offset": 1911.84,
      "duration": 8.16
    },
    {
      "lang": "en",
      "text": "the Python code that calls",
      "offset": 1915.6,
      "duration": 9.64
    },
    {
      "lang": "en",
      "text": "email draft to pass in the transcript.",
      "offset": 1920,
      "duration": 5.24
    },
    {
      "lang": "en",
      "text": "And this is I mean coming back to that",
      "offset": 1927.12,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "idea of like VZ needing more context",
      "offset": 1928.64,
      "duration": 3.519
    },
    {
      "lang": "en",
      "text": "about all your other systems to work",
      "offset": 1930.48,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "well. Um I think it becomes like your",
      "offset": 1932.159,
      "duration": 5.281
    },
    {
      "lang": "en",
      "text": "job as the prompt engineer to or",
      "offset": 1935.44,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "actually we'll say context engineer",
      "offset": 1937.44,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "which is like to give it a good prompt",
      "offset": 1939.12,
      "duration": 3.039
    },
    {
      "lang": "en",
      "text": "and then give it all the context that it",
      "offset": 1940.32,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "needs. Uh and some of these like coding",
      "offset": 1942.159,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "agents that run in the CLI like cloud",
      "offset": 1944.399,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "code or even cursor agent like they tend",
      "offset": 1946.159,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "to be pretty good depending on the size",
      "offset": 1949.519,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "of the project. Um it gets a little uh",
      "offset": 1951.279,
      "duration": 6.321
    },
    {
      "lang": "en",
      "text": "uh tricky harder. You just have to be",
      "offset": 1954.399,
      "duration": 5.041
    },
    {
      "lang": "en",
      "text": "more thoughtful if the project's huge.",
      "offset": 1957.6,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "But especially for small projects, they",
      "offset": 1959.44,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "can be very good at finding all the",
      "offset": 1961.039,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "right context for you. So that and kind",
      "offset": 1963.039,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "of taking that job of like getting all",
      "offset": 1964.96,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "the right files and pasting them into",
      "offset": 1966.48,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "the right place. Um, take that off your",
      "offset": 1968.24,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "plate basically.",
      "offset": 1971.039,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "Exactly. And now you can see it's",
      "offset": 1973.36,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "passing the transcript for me",
      "offset": 1975.039,
      "duration": 2.961
    },
    {
      "lang": "en",
      "text": "automatically.",
      "offset": 1976.48,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "And in theory, uh, do you want to make",
      "offset": 1978,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "this? Uh, sure. I don't want to ask",
      "offset": 1980.96,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "again.\n YOLO.\n I just gave it all.\n Let's",
      "offset": 1982.559,
      "duration": 6.321
    },
    {
      "lang": "en",
      "text": "go.",
      "offset": 1986.159,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "Um, and now that's actually going to go",
      "offset": 1988.88,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "ahead and go update the transcript here.",
      "offset": 1991.679,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "Uh, I haven't done a session on cloud",
      "offset": 1994.32,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "code and cursor, but we probably will at",
      "offset": 1996.24,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "some point. Dexter was on this yesterday",
      "offset": 1998.64,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "and I was not about it, but cloud code",
      "offset": 2000.559,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "is in my opinion much superior to",
      "offset": 2002.799,
      "duration": 6.321
    },
    {
      "lang": "en",
      "text": "cursor's agent. Um, a relative.\n Yeah, we",
      "offset": 2004.72,
      "duration": 6.64
    },
    {
      "lang": "en",
      "text": "will we will do in the next couple weeks",
      "offset": 2009.12,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "a deep dive probably into vibe coding.",
      "offset": 2011.36,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "It'll it'll be a little bit different",
      "offset": 2014.24,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "than like the like, hey, let's go let's",
      "offset": 2015.919,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "go like really fine-tune, refine a bunch",
      "offset": 2018.24,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "of pipelines, but I think it could be a",
      "offset": 2020.24,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "fun episode.",
      "offset": 2021.76,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "Um, what do I not like about fuchsia",
      "offset": 2023.76,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "prompting? The reason I think I",
      "offset": 2025.6,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "specifically say this so egregiously and",
      "offset": 2027.44,
      "duration": 5.599
    },
    {
      "lang": "en",
      "text": "so aggressively about fat prompting is a",
      "offset": 2030.24,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "lot of people use freeot prompting as",
      "offset": 2033.039,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "like a crutch to try and get the model",
      "offset": 2034.72,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "to do what they want without",
      "offset": 2036.399,
      "duration": 2.721
    },
    {
      "lang": "en",
      "text": "understanding that they're actually",
      "offset": 2037.919,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "injecting a lot of bias in the model. So",
      "offset": 2039.12,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "let's say you're building a healthcare",
      "offset": 2041.84,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "company and you're using Fshot prompting",
      "offset": 2043.039,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "to teach the model how to do like um um",
      "offset": 2045.039,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "like detect the right verbiage from a",
      "offset": 2048.639,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "doctor. That could be good or it could",
      "offset": 2050.399,
      "duration": 5.121
    },
    {
      "lang": "en",
      "text": "be really really bad. Like let's say for",
      "offset": 2053.04,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "example you wanted to go tell that you",
      "offset": 2055.52,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "want to talk about like some sort of",
      "offset": 2057.44,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "like weird liver disease that a person",
      "offset": 2058.879,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "has and that's the shot example you use.",
      "offset": 2060.72,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "It's a really rare disease. One in like",
      "offset": 2062.56,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "a million people have it. So it's really",
      "offset": 2064.48,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "unlikely to impact anything. and then a",
      "offset": 2066.399,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "customer comes in with the same exact",
      "offset": 2068.639,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "name as the example in your prompt in",
      "offset": 2070.72,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "your future example,",
      "offset": 2073.679,
      "duration": 5.841
    },
    {
      "lang": "en",
      "text": "you're gonna have a hard time not",
      "offset": 2076.72,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "telling telling model this is truly an",
      "offset": 2079.52,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "example and not an example that's",
      "offset": 2081.52,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "related to that customer from a",
      "offset": 2083.2,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "different doctor. It's just a hard\n it's",
      "offset": 2084.8,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "gonna infer meaning from things that you",
      "offset": 2087.599,
      "duration": 5.361
    },
    {
      "lang": "en",
      "text": "didn't intend for it to like include as",
      "offset": 2090.32,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "part of the example like that people",
      "offset": 2092.96,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "named you know people named Sarah are",
      "offset": 2094.72,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "likely to have a liver a rare liver",
      "offset": 2097.68,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "condition or something but like when you",
      "offset": 2099.52,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "give it that example it's going to learn",
      "offset": 2101.68,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "to a different doctor\n right even a",
      "offset": 2103.119,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "simple assumption just the person Sarah",
      "offset": 2105.2,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "is going to a different is going to a",
      "offset": 2106.88,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "different doctor now and it's an updated",
      "offset": 2108.56,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "conversation you're giving me an example",
      "offset": 2110.079,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "but it's also an example about the same",
      "offset": 2111.92,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "person and whether or not the model",
      "offset": 2113.44,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "understands that or not doesn't really",
      "offset": 2115.68,
      "duration": 8
    },
    {
      "lang": "en",
      "text": "matter.\n It's just more so just that like",
      "offset": 2118.56,
      "duration": 6.799
    },
    {
      "lang": "en",
      "text": "you just have to know the biases you're",
      "offset": 2123.68,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "injecting. In our case, we know Dexter",
      "offset": 2125.359,
      "duration": 5.041
    },
    {
      "lang": "en",
      "text": "and I are recording these videos. We",
      "offset": 2128.4,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "know our tone is what we want to go",
      "offset": 2130.4,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "embed. So if we put a few shot example",
      "offset": 2132.079,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "in here, it's just going to do what we",
      "offset": 2134.64,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "want exactly because we're not going I'm",
      "offset": 2136.8,
      "duration": 2.799
    },
    {
      "lang": "en",
      "text": "not going to run this on any other",
      "offset": 2138.4,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "videos but the AI that works videos. Now",
      "offset": 2139.599,
      "duration": 5.281
    },
    {
      "lang": "en",
      "text": "we do recordings for our team meeting.",
      "offset": 2142.88,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "Sometimes if I use the same prompt in",
      "offset": 2144.88,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "here, it will not generate a good email",
      "offset": 2147.2,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "because I've fought example it for that",
      "offset": 2149.52,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "specific scenario. So what I really",
      "offset": 2151.76,
      "duration": 5.599
    },
    {
      "lang": "en",
      "text": "really like is this thing that I like to",
      "offset": 2154.4,
      "duration": 6.32
    },
    {
      "lang": "en",
      "text": "call dynamic fshot prompting uh",
      "offset": 2157.359,
      "duration": 6.561
    },
    {
      "lang": "en",
      "text": "what is it workshop and that works. It's",
      "offset": 2160.72,
      "duration": 6.72
    },
    {
      "lang": "en",
      "text": "just that most people don't do it. Um or",
      "offset": 2163.92,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "like partial fot prompting. So like",
      "offset": 2167.44,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "instead of actually telling the model",
      "offset": 2169.119,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "that like oh in this case if I want the",
      "offset": 2170.24,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "model to really understand that",
      "offset": 2172.56,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "engineering is only people that actively",
      "offset": 2173.76,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "code. So like a VP of engineering isn't",
      "offset": 2175.44,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "engineering their product. I can write",
      "offset": 2177.359,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "that like this in my prompt where I",
      "offset": 2179.839,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "don't actually give the full output in",
      "offset": 2181.839,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "my prompt. I only tell it like oh",
      "offset": 2183.52,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "because they don't code the category is",
      "offset": 2185.839,
      "duration": 2.881
    },
    {
      "lang": "en",
      "text": "product. I missed out on everything",
      "offset": 2187.44,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "else. I didn't add any other fields. So",
      "offset": 2188.72,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "I'm letting the model really understand",
      "offset": 2190.8,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "this is truly an example. It's even if",
      "offset": 2192.48,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "there's another person named by Gupta",
      "offset": 2194.8,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "this even to a human would be clear that",
      "offset": 2197.76,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "this is an example without having to",
      "offset": 2200,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "read too much into it. infer the the",
      "offset": 2202,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "default inference as an example and also",
      "offset": 2204.72,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "you can do more dynamic stuff like for",
      "offset": 2207.28,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "example dynamically change the name of",
      "offset": 2208.88,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "the if for whatever reason I know the",
      "offset": 2210.64,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "person has a name vibupa change this",
      "offset": 2212.32,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "name",
      "offset": 2214.64,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "to be different for that example right",
      "offset": 2216.16,
      "duration": 6.32
    },
    {
      "lang": "en",
      "text": "and that's just things that you can do",
      "offset": 2219.44,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "to go make your fa prompts a lot better",
      "offset": 2222.48,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "because because if your prompt uses vibb",
      "offset": 2224.96,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "gupta and then you pass in a new thing",
      "offset": 2226.96,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "and the person's name is also vibb gupta",
      "offset": 2228.64,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "then it might assume that it's the same",
      "offset": 2230.88,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "that follows that pattern when really",
      "offset": 2232.96,
      "duration": 3.119
    },
    {
      "lang": "en",
      "text": "you'd basically want to say like okay",
      "offset": 2234.88,
      "duration": 2.64
    },
    {
      "lang": "en",
      "text": "whatever the person's name we're testing",
      "offset": 2236.079,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "on make sure the name in the fshot",
      "offset": 2237.52,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "example is different before we launch",
      "offset": 2239.68,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "the prompt.\n Exactly. So you want to kind",
      "offset": 2241.28,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "of find an you want to make it I think",
      "offset": 2244.4,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "the idea of fshot prompting thing is you",
      "offset": 2246.72,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "want to guarantee the model thinks of",
      "offset": 2248.24,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "the thing you're providing as an example",
      "offset": 2249.68,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "and that is a\n cool tangent.",
      "offset": 2252.56,
      "duration": 7.2
    },
    {
      "lang": "en",
      "text": "Sorry uh let's uh let's test this thing.",
      "offset": 2255.76,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "So, let's add the trans. We added the",
      "offset": 2259.76,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "transcript part in there. Um, and like",
      "offset": 2261.2,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "now I'll read summarize.\n So, the only",
      "offset": 2263.28,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "thing we changed So, the only thing we",
      "offset": 2264.8,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "changed was we're using the exact same",
      "offset": 2266.72,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "prompt. We're just also passing in the",
      "offset": 2268,
      "duration": 5.599
    },
    {
      "lang": "en",
      "text": "transcript.\n Exactly. Um, okay.\n And this",
      "offset": 2269.76,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "is going to be really annoying because",
      "offset": 2273.599,
      "duration": 3.041
    },
    {
      "lang": "en",
      "text": "it's going to it just takes a while to",
      "offset": 2274.96,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "go run. And while we're doing that, I'm",
      "offset": 2276.64,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "actually going to go add something and",
      "offset": 2278.88,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "it's streaming this out. So, we'll see",
      "offset": 2280.88,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "if this is better or worse.\n So, sorry.",
      "offset": 2282.4,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "Did you just re-trigger all the",
      "offset": 2285.44,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "processing for all the drafts?",
      "offset": 2287.28,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "Basically, resum\n just for this one.",
      "offset": 2288.72,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "Yeah. Yeah, exactly. For all the drafts",
      "offset": 2291.599,
      "duration": 3.041
    },
    {
      "lang": "en",
      "text": "and the summary. So now it's going to",
      "offset": 2293.2,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "cost me.",
      "offset": 2294.64,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "Yeah. Go ahead.\n So when you when you do",
      "offset": 2296.88,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "reummarize, it's going to repull out the",
      "offset": 2298.64,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "bullet points for takeaways and key",
      "offset": 2300.32,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "topics and then it's going to go because",
      "offset": 2302.079,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "the upstream thing changed, it's going",
      "offset": 2304.56,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "to go regenerate all the drafts.",
      "offset": 2305.839,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "Exactly.",
      "offset": 2308,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "Um that's what I have kind of defined in",
      "offset": 2309.52,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "here. Um and it just takes a while. I",
      "offset": 2311.599,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "don't know why the malls are so slow",
      "offset": 2315.04,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "this morning. Uh but they are. Now, as",
      "offset": 2316.32,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "you can see, my iteration loop here is",
      "offset": 2319.68,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "kind of hosed. It's very, very, very",
      "offset": 2321.04,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "slow. So, I'm going to try and make this",
      "offset": 2323.52,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "iteration loop a little bit faster by",
      "offset": 2325.44,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "writing some test cases. Now, the",
      "offset": 2327.04,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "problem is writing test cases is",
      "offset": 2328.88,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "actually kind of annoying because I",
      "offset": 2331.2,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "don't actually want to do this. But",
      "offset": 2332.8,
      "duration": 3.279
    },
    {
      "lang": "en",
      "text": "these prompts are better. I actually",
      "offset": 2334.16,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "think these key points are better than",
      "offset": 2336.079,
      "duration": 2.321
    },
    {
      "lang": "en",
      "text": "the previous one. Like, it actually got",
      "offset": 2337.28,
      "duration": 2.24
    },
    {
      "lang": "en",
      "text": "one of the key things we talked about",
      "offset": 2338.4,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "last time, which is RTFP, read the FN",
      "offset": 2339.52,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "prompt. Um,\n sorry. Did you Did you",
      "offset": 2341.599,
      "duration": 6.161
    },
    {
      "lang": "en",
      "text": "change the summarization as well? I",
      "offset": 2345.44,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "thought I thought we left the",
      "offset": 2347.76,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "summarization itself unchanged.",
      "offset": 2348.64,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "Um, oh, sorry. Whenever I do re",
      "offset": 2351.44,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "summarize, I actually regenerate the",
      "offset": 2353.44,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "whole summary as well, not just the",
      "offset": 2355.119,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "Okay, so this is just a second a second",
      "offset": 2357.76,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "pass over the same data, right? You",
      "offset": 2360,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "didn't change the inputs to the summary.",
      "offset": 2361.52,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "Exactly. So, I actually I just changed",
      "offset": 2364,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "the email draft one, but I also ch I did",
      "offset": 2365.68,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "the resummarize button along the way.",
      "offset": 2368.56,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "Yeah. Okay. But but you're talking about",
      "offset": 2371.04,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "hey these the fact that these points are",
      "offset": 2373.2,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "better in this case is just at this this",
      "offset": 2375.119,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "time the the model happened to pick",
      "offset": 2377.2,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "better summary points.\n Okay.\n Yes. So",
      "offset": 2379.44,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "once this email is done take it out. And",
      "offset": 2382.079,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "this is why I added the re summarize",
      "offset": 2384.88,
      "duration": 2.56
    },
    {
      "lang": "en",
      "text": "button because like sometimes the",
      "offset": 2386.56,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "summary is just off",
      "offset": 2387.44,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "and I don't really want to think too",
      "offset": 2389.52,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "hard about it. Uh so like the easiest",
      "offset": 2391.2,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "way\n you just want to try\n I just want to",
      "offset": 2392.72,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "try again. So I wanted a way to",
      "offset": 2395.2,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "re-trigger the pipeline\n and you could do",
      "offset": 2396.32,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "this actually in because we wrote all",
      "offset": 2398.8,
      "duration": 3.039
    },
    {
      "lang": "en",
      "text": "the code and we have full control over",
      "offset": 2400.48,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "the pipeline. You could actually do this",
      "offset": 2401.839,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "in the code and say like hey just run it",
      "offset": 2403.68,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "three times and show three summaries and",
      "offset": 2405.44,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "I'll just pick one and then go do the",
      "offset": 2407.119,
      "duration": 3.041
    },
    {
      "lang": "en",
      "text": "rest of the pipeline. Like you could you",
      "offset": 2408.64,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "could implement that pretty easily.\n Yes.",
      "offset": 2410.16,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "Like cloud code could probably build",
      "offset": 2413.04,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "that for you in five minutes. Uh and",
      "offset": 2414.24,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "then the email summary at some point",
      "offset": 2417.359,
      "duration": 5.401
    },
    {
      "lang": "en",
      "text": "will finish up.",
      "offset": 2419.2,
      "duration": 3.56
    },
    {
      "lang": "en",
      "text": "All the content is generated. So I'm",
      "offset": 2423.52,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "have to refresh. This is the sinking",
      "offset": 2425.92,
      "duration": 5.72
    },
    {
      "lang": "en",
      "text": "point that I did not get perfect.",
      "offset": 2427.599,
      "duration": 4.041
    },
    {
      "lang": "en",
      "text": "Did this work?",
      "offset": 2434.48,
      "duration": 6.639
    },
    {
      "lang": "en",
      "text": "Um did not. Let me try rerunning.",
      "offset": 2437.359,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "I think OpenAI for some reason has been",
      "offset": 2441.119,
      "duration": 5.601
    },
    {
      "lang": "en",
      "text": "not working very well. Oh,",
      "offset": 2443.119,
      "duration": 7.321
    },
    {
      "lang": "en",
      "text": "I have an error here.",
      "offset": 2446.72,
      "duration": 3.72
    },
    {
      "lang": "en",
      "text": "This\n Oh, Claude didn't quite nail it.\n It",
      "offset": 2451.28,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "didn't nail it. It's fine. Um, but while",
      "offset": 2454.32,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "it's doing that, I'm going to do",
      "offset": 2456.56,
      "duration": 1.759
    },
    {
      "lang": "en",
      "text": "something. I'm going to add some",
      "offset": 2457.52,
      "duration": 2.24
    },
    {
      "lang": "en",
      "text": "observability to my pipeline so I can",
      "offset": 2458.319,
      "duration": 2.561
    },
    {
      "lang": "en",
      "text": "actually iterate on it a little bit",
      "offset": 2459.76,
      "duration": 2.4
    },
    {
      "lang": "en",
      "text": "faster because the promp thing is going",
      "offset": 2460.88,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "to be so freaking slow otherwise.",
      "offset": 2462.16,
      "duration": 6.52
    },
    {
      "lang": "en",
      "text": "Um yeah, I generated",
      "offset": 2464.88,
      "duration": 3.8
    },
    {
      "lang": "en",
      "text": "um I'm just going to dump thing.",
      "offset": 2469.839,
      "duration": 7.161
    },
    {
      "lang": "en",
      "text": "Um",
      "offset": 2474,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "pass new project.",
      "offset": 2477.04,
      "duration": 3.72
    },
    {
      "lang": "en",
      "text": "Uh I guess it doesn't really matter if",
      "offset": 2484.16,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "you have API keys. You can just send",
      "offset": 2485.52,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "data here",
      "offset": 2486.96,
      "duration": 5.48
    },
    {
      "lang": "en",
      "text": "with X.",
      "offset": 2489.119,
      "duration": 3.321
    },
    {
      "lang": "en",
      "text": "AI that works is built on trust, folks.",
      "offset": 2495.359,
      "duration": 5.681
    },
    {
      "lang": "en",
      "text": "Indeed it is. I will stop screen sharing",
      "offset": 2499.04,
      "duration": 6.52
    },
    {
      "lang": "en",
      "text": "that while I edit my end file.",
      "offset": 2501.04,
      "duration": 4.52
    },
    {
      "lang": "en",
      "text": "We have some trust but not infinite",
      "offset": 2505.599,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "trust.",
      "offset": 2507.76,
      "duration": 4.599
    },
    {
      "lang": "en",
      "text": "Yeah.",
      "offset": 2509.359,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "Okay. So what you're doing is adding the",
      "offset": 2516,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "API keys so that every every AI call is",
      "offset": 2517.68,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "basically going to be tracked and so we",
      "offset": 2520.72,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "can use\n Yeah. And I really what I really",
      "offset": 2522.319,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "the main thing I really want to be",
      "offset": 2524.64,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "honest is I just want to go see the logs",
      "offset": 2526.24,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "so that when it goes ahead I can go",
      "offset": 2528.88,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "ahead and just like turn it into a",
      "offset": 2530.88,
      "duration": 7.04
    },
    {
      "lang": "en",
      "text": "um\n yeah turn it into a test case.",
      "offset": 2534.079,
      "duration": 5.601
    },
    {
      "lang": "en",
      "text": "And basically without that what you",
      "offset": 2537.92,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "could do is I guess you could like",
      "offset": 2539.68,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "handcraft a test case by like going and",
      "offset": 2541.119,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "looking in the database and like",
      "offset": 2543.28,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "assembling the inputs yourself and then",
      "offset": 2544.8,
      "duration": 5.279
    },
    {
      "lang": "en",
      "text": "like writing a test case by hand.\n Yeah.",
      "offset": 2547.28,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "But I don't really want to do that. I",
      "offset": 2550.079,
      "duration": 2.481
    },
    {
      "lang": "en",
      "text": "just want to use like the real test case",
      "offset": 2551.2,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "along the way.",
      "offset": 2552.56,
      "duration": 7
    },
    {
      "lang": "en",
      "text": "Um there we go.",
      "offset": 2555.119,
      "duration": 4.441
    },
    {
      "lang": "en",
      "text": "This should I think be done now. It",
      "offset": 2563.68,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "should have made some diffs along the",
      "offset": 2566,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "way. Um,",
      "offset": 2567.359,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "oh yeah, we forgot to pass in",
      "offset": 2570.56,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "transcript. Generate email draft. Okay,",
      "offset": 2571.839,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "cool.",
      "offset": 2574.88,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "Okay, now what we should be able to do,",
      "offset": 2576.8,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "I also made this other reset processing",
      "offset": 2580.4,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "bug because like sometimes this button",
      "offset": 2582.16,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "gets grayed out like sometimes it's in a",
      "offset": 2584.4,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "bad state like it is right now because I",
      "offset": 2586,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "restarted my server. So, this just",
      "offset": 2587.44,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "allows me to restart processing really",
      "offset": 2589.92,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "really fast.",
      "offset": 2591.52,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "And now in theory,",
      "offset": 2594.079,
      "duration": 7.681
    },
    {
      "lang": "en",
      "text": "oh, it just resets the state. In theory,",
      "offset": 2597.52,
      "duration": 7.76
    },
    {
      "lang": "en",
      "text": "this will work beautifully, but we'll",
      "offset": 2601.76,
      "duration": 7.04
    },
    {
      "lang": "en",
      "text": "see. Um,\n starting BAML similarization",
      "offset": 2605.28,
      "duration": 5.839
    },
    {
      "lang": "en",
      "text": "for video.\n There we go.\n Here we go.\n So,",
      "offset": 2608.8,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "the summarization is happening. Um, so",
      "offset": 2611.119,
      "duration": 3.521
    },
    {
      "lang": "en",
      "text": "the real thing that I really want to do",
      "offset": 2613.28,
      "duration": 2.64
    },
    {
      "lang": "en",
      "text": "now is what I want to do is I want to",
      "offset": 2614.64,
      "duration": 2.64
    },
    {
      "lang": "en",
      "text": "focus on two parts. I want to make the",
      "offset": 2615.92,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "summary as good as possible and I want",
      "offset": 2617.28,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "to make the email as good as possible.",
      "offset": 2618.96,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "The only way I can really make the email",
      "offset": 2620.96,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "very good is by being a really fast",
      "offset": 2622.48,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "iteration loop on actually iterating on",
      "offset": 2624.4,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "the prop. Now, iterating on the UI layer",
      "offset": 2625.76,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "is just too slow. I can't do that. I I",
      "offset": 2628.079,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "don't want to build state management",
      "offset": 2630.16,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "record replay on every single aspect of",
      "offset": 2631.52,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "this. It's just going to be too slow.",
      "offset": 2633.2,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "So, I'm just going to go iterate on just",
      "offset": 2634.72,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "the email. Now, I on the summary,\n you",
      "offset": 2636.4,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "can apply the same concepts to the",
      "offset": 2639.359,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "Twitter and the LinkedIn and all the",
      "offset": 2641.44,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "other stuff.\n And the reason that we're",
      "offset": 2643.119,
      "duration": 5.041
    },
    {
      "lang": "en",
      "text": "not\n we should also take ahead.\n Sorry. Go",
      "offset": 2645.04,
      "duration": 6.4
    },
    {
      "lang": "en",
      "text": "ahead. I was",
      "offset": 2648.16,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "Oh, go ahead. Sorry. Go ahead, sir.\n We",
      "offset": 2651.44,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "got a little bit of lag. I just want to",
      "offset": 2654,
      "duration": 2.48
    },
    {
      "lang": "en",
      "text": "know like we should take some time at",
      "offset": 2655.359,
      "duration": 2.24
    },
    {
      "lang": "en",
      "text": "the end just to talk about like, hey,",
      "offset": 2656.48,
      "duration": 2.56
    },
    {
      "lang": "en",
      "text": "what would be next for this thing? And",
      "offset": 2657.599,
      "duration": 3.281
    },
    {
      "lang": "en",
      "text": "like I I imagine we keep working on this",
      "offset": 2659.04,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "thing and like what other things do we",
      "offset": 2660.88,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "want this to do in addition to email and",
      "offset": 2662.72,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "Twitter? How can we make it a little",
      "offset": 2664.24,
      "duration": 2.879
    },
    {
      "lang": "en",
      "text": "more backgroundy? How can we catch web",
      "offset": 2665.52,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "hooks instead of like having to come",
      "offset": 2667.119,
      "duration": 3.041
    },
    {
      "lang": "en",
      "text": "here and click on stuff? There's a whole",
      "offset": 2668.56,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "like next generation of this thing.\n Oh",
      "offset": 2670.16,
      "duration": 6.72
    },
    {
      "lang": "en",
      "text": "yeah, 100%. Um, Vid's got a question",
      "offset": 2673.76,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "really fast.",
      "offset": 2676.88,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "So, we are currently building the end to",
      "offset": 2678.72,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "end pipeline to test this, but are you",
      "offset": 2680.96,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "also going to do the peacewise uh test",
      "offset": 2683.44,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "as well?\n I'm literally about to do that",
      "offset": 2685.52,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "right now. Now, what I want to do for",
      "offset": 2688.24,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "the peacewise testing is rather than",
      "offset": 2690.16,
      "duration": 6.439
    },
    {
      "lang": "en",
      "text": "running the peacewise testing on my own.",
      "offset": 2691.92,
      "duration": 4.679
    },
    {
      "lang": "en",
      "text": "Oh, you got another you got another",
      "offset": 2696.88,
      "duration": 4.439
    },
    {
      "lang": "en",
      "text": "error.",
      "offset": 2698.319,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "Okay, let me find this",
      "offset": 2703.839,
      "duration": 5.601
    },
    {
      "lang": "en",
      "text": "cloud code isn't doing this right. So, I",
      "offset": 2707.839,
      "duration": 5.081
    },
    {
      "lang": "en",
      "text": "will find this myself",
      "offset": 2709.44,
      "duration": 3.48
    },
    {
      "lang": "en",
      "text": "and just like fix it.",
      "offset": 2714.48,
      "duration": 4.119
    },
    {
      "lang": "en",
      "text": "Uh, command C, command B. Back end.",
      "offset": 2718.96,
      "duration": 7.639
    },
    {
      "lang": "en",
      "text": "Hide my terminal.",
      "offset": 2730.319,
      "duration": 3.561
    },
    {
      "lang": "en",
      "text": "Let me turn on my Python",
      "offset": 2738.8,
      "duration": 6.799
    },
    {
      "lang": "en",
      "text": "script mode really fast and then it'll",
      "offset": 2742.64,
      "duration": 6.36
    },
    {
      "lang": "en",
      "text": "yell at me.",
      "offset": 2745.599,
      "duration": 3.401
    },
    {
      "lang": "en",
      "text": "Why is he on me? Oh, is this not a",
      "offset": 2752.56,
      "duration": 5.64
    },
    {
      "lang": "en",
      "text": "waitable?",
      "offset": 2755.2,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "There we go. That's why it wasn't",
      "offset": 2764.319,
      "duration": 4.681
    },
    {
      "lang": "en",
      "text": "working.",
      "offset": 2766,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "I wasn't calling the right code. That's",
      "offset": 2774.8,
      "duration": 6.039
    },
    {
      "lang": "en",
      "text": "good to know.",
      "offset": 2777.52,
      "duration": 3.319
    },
    {
      "lang": "en",
      "text": "Async. Where's the other one that I",
      "offset": 2783.119,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "changed?",
      "offset": 2784.8,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "Um, sorry about watching me live code.",
      "offset": 2787.119,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "Um, I was hoping to have most of the",
      "offset": 2789.68,
      "duration": 5.08
    },
    {
      "lang": "en",
      "text": "stuff done.",
      "offset": 2791.44,
      "duration": 3.32
    },
    {
      "lang": "en",
      "text": "Okay, I think it should be good now.",
      "offset": 2796.64,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "But while this is done, let's talk about",
      "offset": 2800.96,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "what the actual thing looks like. So I",
      "offset": 2802.56,
      "duration": 3.039
    },
    {
      "lang": "en",
      "text": "should be able to actually see the",
      "offset": 2804.72,
      "duration": 2.16
    },
    {
      "lang": "en",
      "text": "actual test case and I should be able to",
      "offset": 2805.599,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "go and edit it. There you go. So\n yeah,",
      "offset": 2806.88,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "maybe we just do not the email one. We",
      "offset": 2810.16,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "do a different one.\n Exactly. So we can",
      "offset": 2811.92,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "just say like the summarize for example.",
      "offset": 2814.56,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "Let's just make sure the summary is",
      "offset": 2816.88,
      "duration": 2.56
    },
    {
      "lang": "en",
      "text": "actually good. So let's just make this a",
      "offset": 2818,
      "duration": 3.119
    },
    {
      "lang": "en",
      "text": "test case. Let's work on this. And now",
      "offset": 2819.44,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "we can go to",
      "offset": 2821.119,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "So that was really fast. Uh so what you",
      "offset": 2823.599,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "did was you took a recording of an input",
      "offset": 2825.52,
      "duration": 7.52
    },
    {
      "lang": "en",
      "text": "output pair for one of these calls and",
      "offset": 2828.8,
      "duration": 6.64
    },
    {
      "lang": "en",
      "text": "generated a test case so that you can go",
      "offset": 2833.04,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "change the prompt and see how it affects",
      "offset": 2835.44,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "the output basically.",
      "offset": 2836.88,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "So what I'm finding now is I have the",
      "offset": 2839.359,
      "duration": 3.041
    },
    {
      "lang": "en",
      "text": "transcript I can go play around with",
      "offset": 2840.96,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "this like fundamentally what I really",
      "offset": 2842.4,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "need is I need a way to actually go see",
      "offset": 2844.319,
      "duration": 2.881
    },
    {
      "lang": "en",
      "text": "if this is actually going to work or",
      "offset": 2845.92,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "not. Um, how do I hide\n without having to",
      "offset": 2847.2,
      "duration": 7.04
    },
    {
      "lang": "en",
      "text": "go run the entire pipeline end to end",
      "offset": 2850.319,
      "duration": 7.441
    },
    {
      "lang": "en",
      "text": "exactly? So now what I'm able to do is I",
      "offset": 2854.24,
      "duration": 5.119
    },
    {
      "lang": "en",
      "text": "can go see the pipeline, see what it's",
      "offset": 2857.76,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "doing, and then I can just go like",
      "offset": 2859.359,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "firstly I can just see what the model",
      "offset": 2861.839,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "did. It's not like the model's key",
      "offset": 2863.2,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "takeaways are just smaller than I what I",
      "offset": 2865.599,
      "duration": 3.281
    },
    {
      "lang": "en",
      "text": "want them to be. I want them to be like",
      "offset": 2867.76,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "paragraphs. So let's just change that a",
      "offset": 2868.88,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "little bit. Let's add like a Let's see",
      "offset": 2871.44,
      "duration": 6.84
    },
    {
      "lang": "en",
      "text": "what the model actually does.",
      "offset": 2874.48,
      "duration": 3.8
    },
    {
      "lang": "en",
      "text": "It doesn't seem like the model is",
      "offset": 2878.4,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "actually listening to me. It feels like",
      "offset": 2879.839,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "what it's actually doing is jumping it",
      "offset": 2881.599,
      "duration": 2.881
    },
    {
      "lang": "en",
      "text": "out even though I told it to give me an",
      "offset": 2883.04,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "outline of a very very dense summary",
      "offset": 2884.48,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "um in the system prompt. So let's try a",
      "offset": 2888.24,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "different model. Uh let's just swap out",
      "offset": 2890.64,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "to like enthropic um I think I have a",
      "offset": 2892.48,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "custom sonnet",
      "offset": 2896.48,
      "duration": 6.52
    },
    {
      "lang": "en",
      "text": "and see if sonnet will work",
      "offset": 2898.96,
      "duration": 4.04
    },
    {
      "lang": "en",
      "text": "along the way.",
      "offset": 2904.16,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "And so basically you want it before it",
      "offset": 2907.359,
      "duration": 3.521
    },
    {
      "lang": "en",
      "text": "starts dumping out JSON, you want it to",
      "offset": 2909.04,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "kind of like outline its thoughts.",
      "offset": 2910.88,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "Exactly. Outline a dead summary of the",
      "offset": 2913.599,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "video. Uh then",
      "offset": 2915.599,
      "duration": 9
    },
    {
      "lang": "en",
      "text": "answer then fill out the summary.",
      "offset": 2919.119,
      "duration": 5.48
    },
    {
      "lang": "en",
      "text": "Um so example",
      "offset": 2926.64,
      "duration": 4.6
    },
    {
      "lang": "en",
      "text": "Let's do that. Sure.",
      "offset": 2937.44,
      "duration": 3.56
    },
    {
      "lang": "en",
      "text": "Um,",
      "offset": 2943.599,
      "duration": 5.041
    },
    {
      "lang": "en",
      "text": "schema. Sure. Let's just try that. Maybe",
      "offset": 2945.44,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "my prompt was bad. So, let's just",
      "offset": 2948.64,
      "duration": 2.719
    },
    {
      "lang": "en",
      "text": "iterate and see if I can get the model",
      "offset": 2950.319,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "to spit this out.",
      "offset": 2951.359,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "It's really wild how slow the models",
      "offset": 2954.64,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "are. I feel like I'm being rate limited.",
      "offset": 2956.16,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "And this is yeah this is the um this is",
      "offset": 2958.64,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "that like we covered this in depth in",
      "offset": 2961.28,
      "duration": 3.039
    },
    {
      "lang": "en",
      "text": "the reasoning topic right of like how",
      "offset": 2962.8,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "can you get a model to kind of like",
      "offset": 2964.319,
      "duration": 2.721
    },
    {
      "lang": "en",
      "text": "think about it before it starts",
      "offset": 2965.68,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "generating the data that our program is",
      "offset": 2967.04,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "actually going to use.\n Exactly. So now",
      "offset": 2968.64,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "it actually is generating a pretty good",
      "offset": 2971.119,
      "duration": 3.281
    },
    {
      "lang": "en",
      "text": "depth response of what it wanted and",
      "offset": 2972.72,
      "duration": 3.119
    },
    {
      "lang": "en",
      "text": "then it's outputting the summary and I",
      "offset": 2974.4,
      "duration": 2.719
    },
    {
      "lang": "en",
      "text": "think we'll see if this is actually",
      "offset": 2975.839,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "better or not. I'll try with open AI",
      "offset": 2977.119,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "again just to see what it does. Open IP",
      "offset": 2979.04,
      "duration": 4.76
    },
    {
      "lang": "en",
      "text": "portal",
      "offset": 2980.8,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "and you can see how fast I'm iterating",
      "offset": 2984.079,
      "duration": 3.121
    },
    {
      "lang": "en",
      "text": "here. And what I'm really trying to do",
      "offset": 2985.599,
      "duration": 3.121
    },
    {
      "lang": "en",
      "text": "here is I'm trying to figure out is it",
      "offset": 2987.2,
      "duration": 2.879
    },
    {
      "lang": "en",
      "text": "doing what I want. Ah, that looks",
      "offset": 2988.72,
      "duration": 6.52
    },
    {
      "lang": "en",
      "text": "actually better than other things. Um,",
      "offset": 2990.079,
      "duration": 5.161
    },
    {
      "lang": "en",
      "text": "and I can even guide it. Par one,",
      "offset": 2996.559,
      "duration": 6.081
    },
    {
      "lang": "en",
      "text": "um, par two, par three. I can even make",
      "offset": 3000,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "it write multiple paragraphs by telling",
      "offset": 3002.64,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "it what I want it to go do because it",
      "offset": 3004.48,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "this this doesn't seem very dense.",
      "offset": 3006.319,
      "duration": 6.081
    },
    {
      "lang": "en",
      "text": "So, let's let it go do more along the",
      "offset": 3009.76,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "way.",
      "offset": 3012.4,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "Ah, this looks way it looks like it's",
      "offset": 3014.64,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "better on the right track because now",
      "offset": 3017.44,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "I'm telling it to do multiple",
      "offset": 3019.28,
      "duration": 4.12
    },
    {
      "lang": "en",
      "text": "paragraphs.",
      "offset": 3020.4,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "So, let's see if it actually understood",
      "offset": 3024.48,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "it. It still really did do multiple",
      "offset": 3025.68,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "paragraphs.",
      "offset": 3027.44,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "Um,",
      "offset": 3028.96,
      "duration": 6.72
    },
    {
      "lang": "en",
      "text": "so let's like fill this out. Um,",
      "offset": 3030.96,
      "duration": 7.04
    },
    {
      "lang": "en",
      "text": "topic par",
      "offset": 3035.68,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "um topic one",
      "offset": 3038,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "para",
      "offset": 3041.52,
      "duration": 5.799
    },
    {
      "lang": "en",
      "text": "Topic one.",
      "offset": 3043.52,
      "duration": 3.799
    },
    {
      "lang": "en",
      "text": "See,",
      "offset": 3047.52,
      "duration": 7.079
    },
    {
      "lang": "en",
      "text": "topic two parag",
      "offset": 3049.92,
      "duration": 4.679
    },
    {
      "lang": "en",
      "text": "since the videos are 60 plus minutes",
      "offset": 3056.48,
      "duration": 10.2
    },
    {
      "lang": "en",
      "text": "long. Try and have time ranges.",
      "offset": 3061.04,
      "duration": 5.64
    },
    {
      "lang": "en",
      "text": "Time range. Cool.",
      "offset": 3069.76,
      "duration": 4.599
    },
    {
      "lang": "en",
      "text": "It'll get the gist of what I want. So,",
      "offset": 3081.04,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "I'm trying to force it to basically",
      "offset": 3083.2,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "reason more along the way and reason the",
      "offset": 3084.64,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "way that I really want it to go do.",
      "offset": 3087.04,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "Nope, that didn't work at all. Um, I'll",
      "offset": 3089.76,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "rerun this one more time.",
      "offset": 3092.8,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "Went fast.",
      "offset": 3095.28,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "And I think the other thing that I want",
      "offset": 3096.96,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "to do",
      "offset": 3098.4,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "is I want to see",
      "offset": 3100.16,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "if I can get it to actually listen to me",
      "offset": 3102.72,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "by making the temp. I had originally",
      "offset": 3106.16,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "deliberately tried to keep the",
      "offset": 3108.48,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "temperature uh higher for this kind of",
      "offset": 3109.76,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "task. But I'm going to make it",
      "offset": 3112.8,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "temperature zero now and try this out",
      "offset": 3114.72,
      "duration": 5.32
    },
    {
      "lang": "en",
      "text": "one more time.",
      "offset": 3116.64,
      "duration": 3.4
    },
    {
      "lang": "en",
      "text": "Nice.",
      "offset": 3120.079,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "Oh, this is sick.",
      "offset": 3122.24,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "Okay. And then what are we going to go",
      "offset": 3124.4,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "update the structured outputs to create",
      "offset": 3125.839,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "timestamps or like chapter summaries?\n We",
      "offset": 3128,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "might not even need that.\n I just wanted",
      "offset": 3130.559,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "to go have this because I wanted to use",
      "offset": 3132.559,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "this as the summary first before it",
      "offset": 3134.16,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "actually goes ahead and fills out main",
      "offset": 3136,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "takeaways and everything else here,\n but",
      "offset": 3137.44,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "we could update the structured output to",
      "offset": 3140.24,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "also encode encode time uh time things",
      "offset": 3141.92,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "if you think it's useful. So let's go do",
      "offset": 3144.559,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "that. Let's change main takeaways to",
      "offset": 3146.24,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "include time data,",
      "offset": 3148.72,
      "duration": 6.399
    },
    {
      "lang": "en",
      "text": "right? Um",
      "offset": 3152,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "uh let's add another field.",
      "offset": 3155.119,
      "duration": 6.081
    },
    {
      "lang": "en",
      "text": "Um main takeaways",
      "offset": 3157.68,
      "duration": 8.24
    },
    {
      "lang": "en",
      "text": "uh timed data",
      "offset": 3161.2,
      "duration": 7.96
    },
    {
      "lang": "en",
      "text": "time data",
      "offset": 3165.92,
      "duration": 3.24
    },
    {
      "lang": "en",
      "text": "uh summary",
      "offset": 3173.44,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "string array.",
      "offset": 3175.68,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "Well, the summary can be\n can just be one",
      "offset": 3178.64,
      "duration": 6.4
    },
    {
      "lang": "en",
      "text": "string. Exactly.\n Okay.",
      "offset": 3180.8,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "And you can actually see what I was able",
      "offset": 3185.04,
      "duration": 3.039
    },
    {
      "lang": "en",
      "text": "to do. It took me a little bit of work",
      "offset": 3186.64,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "to go get this. And now the model isn't",
      "offset": 3188.079,
      "duration": 2.641
    },
    {
      "lang": "en",
      "text": "actually going to go do this because",
      "offset": 3189.839,
      "duration": 2.401
    },
    {
      "lang": "en",
      "text": "it's like I can just reason the data",
      "offset": 3190.72,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "model. So it's using the flexibility to",
      "offset": 3192.24,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "go do this on its own without really",
      "offset": 3195.2,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "having to think too hard about this. Um,",
      "offset": 3197.119,
      "duration": 5.121
    },
    {
      "lang": "en",
      "text": "but\n and I'm my question, I guess, and I",
      "offset": 3200.4,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "think the answer is probably just vibes,",
      "offset": 3202.24,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "but like how are you deciding whether",
      "offset": 3203.76,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "like what's what's the real trade-off",
      "offset": 3206.88,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "between having it do this in the",
      "offset": 3208.88,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "preamble versus having it do it in the",
      "offset": 3210.319,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "data model?",
      "offset": 3212.16,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "What's the real trade-off? It's a couple",
      "offset": 3214.559,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "of things. Uh, when it does in the",
      "offset": 3216.64,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "preamble, it's not polluting the data",
      "offset": 3218.16,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "model yet.\n So, it allows it to like just",
      "offset": 3219.76,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "like kind of have like scratch work when",
      "offset": 3222.079,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "I'm actually doing work along the way to",
      "offset": 3223.68,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "go produce some results. And that's the",
      "offset": 3225.52,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "real benefit. Um, what I find is in",
      "offset": 3228.319,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "general, Enthropic listens to",
      "offset": 3230.559,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "instructions a little bit better for",
      "offset": 3231.92,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "this kind of stuff than OpenAI does. So",
      "offset": 3233.76,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "like I'll try to run this again with",
      "offset": 3235.44,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "like Enthropic",
      "offset": 3237.599,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "and it should start sending the data.",
      "offset": 3240.16,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "Now we're on here. I think we should",
      "offset": 3242.48,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "have gone another one for email.",
      "offset": 3244,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "Where's the email stuff? Uh oh, the",
      "offset": 3247.68,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "email one didn't run because I re\n I",
      "offset": 3249.76,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "think you probably still have an error.",
      "offset": 3251.2,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "Yeah. Um, so now we're actually able to",
      "offset": 3253.76,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "go do this. It's generating time codes,",
      "offset": 3255.599,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "generating a little bit more takeaways,",
      "offset": 3257.599,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "but it still seems a little very",
      "offset": 3259.359,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "one-dimensional compared to this, which",
      "offset": 3260.8,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "is okay. I might actually be okay with",
      "offset": 3262.079,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "like a onelevel uh summarization here.",
      "offset": 3264.16,
      "duration": 5.679
    },
    {
      "lang": "en",
      "text": "It's not it might not be a bad thing,",
      "offset": 3266.96,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "but I think I got I will get slightly",
      "offset": 3269.839,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "better key topics and bullet points",
      "offset": 3272.48,
      "duration": 5.119
    },
    {
      "lang": "en",
      "text": "along the way to go do this. Now, I may",
      "offset": 3274.8,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "not want to call this bullet points.",
      "offset": 3277.599,
      "duration": 3.041
    },
    {
      "lang": "en",
      "text": "It's like let's call this like aliases",
      "offset": 3278.64,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "like takeaways.",
      "offset": 3280.64,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "Takeaways,",
      "offset": 3282.88,
      "duration": 5.439
    },
    {
      "lang": "en",
      "text": "right? description. Action items. Action",
      "offset": 3285.2,
      "duration": 7.04
    },
    {
      "lang": "en",
      "text": "items. Uh listeners",
      "offset": 3288.319,
      "duration": 8.841
    },
    {
      "lang": "en",
      "text": "can action items listeners",
      "offset": 3292.24,
      "duration": 4.92
    },
    {
      "lang": "en",
      "text": "uh can do",
      "offset": 3297.28,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "to improve their skills. And you can see",
      "offset": 3300.88,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "what this does to the prompt. It changes",
      "offset": 3303.2,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "it to instead of bullet points, we now",
      "offset": 3304.72,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "have a field called takeaways. It's an",
      "offset": 3306.8,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "action item listeners can do to improve",
      "offset": 3308.16,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "their prompt, which I think in general",
      "offset": 3309.68,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "is going to be very, very nice.",
      "offset": 3311.52,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "So, let's go run this.\n I'm also noticing",
      "offset": 3315.04,
      "duration": 5.279
    },
    {
      "lang": "en",
      "text": "that the the time data from anthropic is",
      "offset": 3316.88,
      "duration": 6.32
    },
    {
      "lang": "en",
      "text": "uh looks kind of hallucinated. It's just",
      "offset": 3320.319,
      "duration": 6.081
    },
    {
      "lang": "en",
      "text": "doing 0 to 15, 15 to 30, 30 to 45.",
      "offset": 3323.2,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "That's true.\n Which is probably not",
      "offset": 3326.4,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "probably not good enough.",
      "offset": 3328.079,
      "duration": 5.121
    },
    {
      "lang": "en",
      "text": "Yeah, I agree. Um, let's see if it did",
      "offset": 3330.559,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "it this way.\n Yeah, it really just broke",
      "offset": 3333.2,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "it up to almost like 4 hour chunks. And",
      "offset": 3335.44,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "it's probably I bet what it's doing is",
      "offset": 3337.28,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "biasing up to 60 minutes. Videos",
      "offset": 3338.72,
      "duration": 7.92
    },
    {
      "lang": "en",
      "text": "are pretty long.",
      "offset": 3342.319,
      "duration": 10.961
    },
    {
      "lang": "en",
      "text": "Try have time synced to the transcript.",
      "offset": 3346.64,
      "duration": 7.84
    },
    {
      "lang": "en",
      "text": "And this is again the thing we talk",
      "offset": 3353.28,
      "duration": 2.64
    },
    {
      "lang": "en",
      "text": "about all the time is like a tight",
      "offset": 3354.48,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "iteration iteration loop of like if you",
      "offset": 3355.92,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "were doing this end to end testing or",
      "offset": 3357.839,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "even with piest without like dedicated",
      "offset": 3359.2,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "tools, I think this probably take longer",
      "offset": 3361.28,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "to kind of fiddle with.\n Exactly. It",
      "offset": 3362.88,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "sounds like it's still using those. So",
      "offset": 3365.44,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "like it this might just be like a thing",
      "offset": 3367.2,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "Enthropic is going to do unless I",
      "offset": 3369.04,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "specifically break down the transcript",
      "offset": 3371.359,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "into like a shorter chunk.\n So we can",
      "offset": 3373.2,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "just try that.\n Oh, you want to try a",
      "offset": 3375.28,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "separate method that chunks the",
      "offset": 3378.16,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "transcript.",
      "offset": 3379.359,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "Like let's just like I don't know. It's",
      "offset": 3382.319,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "like 45 minutes. Well, I'm just going to",
      "offset": 3384.24,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "do like the dumb thing",
      "offset": 3386.559,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "and just like command X",
      "offset": 3388.799,
      "duration": 5.681
    },
    {
      "lang": "en",
      "text": "and just run that uh and just see what",
      "offset": 3391.839,
      "duration": 5.801
    },
    {
      "lang": "en",
      "text": "it does.",
      "offset": 3394.48,
      "duration": 3.16
    },
    {
      "lang": "en",
      "text": "It sounds like a It looks like a little",
      "offset": 3398.559,
      "duration": 6.481
    },
    {
      "lang": "en",
      "text": "bit better uh along the way uh but not",
      "offset": 3400.16,
      "duration": 6.72
    },
    {
      "lang": "en",
      "text": "too much. So like I just leave it for",
      "offset": 3405.04,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "now and like we'll come back to this in",
      "offset": 3406.88,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "a second.",
      "offset": 3408.4,
      "duration": 4.959
    },
    {
      "lang": "en",
      "text": "But I think the summarizer looks",
      "offset": 3411.04,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "significantly better at this point.",
      "offset": 3413.359,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "um in terms of what we're going to go",
      "offset": 3415.92,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "get. So now what we'll do is",
      "offset": 3417.119,
      "duration": 9.401
    },
    {
      "lang": "en",
      "text": "update my UI is show the new",
      "offset": 3420.559,
      "duration": 5.961
    },
    {
      "lang": "en",
      "text": "Yeah, exactly. It's kind of like if you",
      "offset": 3427.28,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "wanted to take do a pre-processing step",
      "offset": 3429.839,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "on the transcript and really focus on",
      "offset": 3432.48,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "splitting it up into time chunks and",
      "offset": 3434.72,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "maybe like chunk it deterministically",
      "offset": 3436.72,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "and then pass those chunks into models.",
      "offset": 3439.04,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "You could do that. update my UI to show",
      "offset": 3440.88,
      "duration": 8.56
    },
    {
      "lang": "en",
      "text": "the new time data in the um in summary.",
      "offset": 3443.68,
      "duration": 7.84
    },
    {
      "lang": "en",
      "text": "Um, cool. Let's go do this. One thing I",
      "offset": 3449.44,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "like to do when I go do all this stuff",
      "offset": 3451.52,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "is like this stuff is done. So, I'm I'm",
      "offset": 3452.72,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "just going to stage this and then I'm",
      "offset": 3455.119,
      "duration": 5.931
    },
    {
      "lang": "en",
      "text": "going to go tell it to go do this.",
      "offset": 3457.28,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "[Music]",
      "offset": 3461.05,
      "duration": 3.59
    },
    {
      "lang": "en",
      "text": "And now it should go be able to go find",
      "offset": 3463.2,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "this. And the time data should pop up in",
      "offset": 3464.64,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "there. So, I tried to guide it a little",
      "offset": 3466.88,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "bit along the way.",
      "offset": 3468.48,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "Um,",
      "offset": 3470.96,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "and it should hopefully be able to go",
      "offset": 3473.92,
      "duration": 6.12
    },
    {
      "lang": "en",
      "text": "ahead and see if he can find it.",
      "offset": 3475.839,
      "duration": 4.201
    },
    {
      "lang": "en",
      "text": "Oh,",
      "offset": 3482.88,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "oh, it's too big for Claude to read it.",
      "offset": 3484.799,
      "duration": 5.681
    },
    {
      "lang": "en",
      "text": "Yeah, because of this test case. So,",
      "offset": 3488.48,
      "duration": 5.119
    },
    {
      "lang": "en",
      "text": "what I will do,\n yeah,\n I'll help it and",
      "offset": 3490.48,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "split up the test case into a",
      "offset": 3493.599,
      "duration": 6.161
    },
    {
      "lang": "en",
      "text": "summarize test.baml,",
      "offset": 3496.48,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "which makes sense like we will have a",
      "offset": 3499.76,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "bunch of issues with long-term tests.",
      "offset": 3502.4,
      "duration": 8.36
    },
    {
      "lang": "en",
      "text": "Continue. I made the file smaller.",
      "offset": 3504.88,
      "duration": 5.88
    },
    {
      "lang": "en",
      "text": "Um, so now that I can just put my test",
      "offset": 3510.799,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "in here, now this file should be",
      "offset": 3512.4,
      "duration": 2.399
    },
    {
      "lang": "en",
      "text": "actually readable. That's actually one",
      "offset": 3513.599,
      "duration": 2.401
    },
    {
      "lang": "en",
      "text": "good thing for me to note in general",
      "offset": 3514.799,
      "duration": 2.721
    },
    {
      "lang": "en",
      "text": "from now on because transcripts are",
      "offset": 3516,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "going to be long. I should keep all my",
      "offset": 3517.52,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "tests in different files. In fact, I",
      "offset": 3519.04,
      "duration": 3.519
    },
    {
      "lang": "en",
      "text": "probably want each test to be its own",
      "offset": 3520.88,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "file because I know transcripts will be",
      "offset": 3522.559,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "long.",
      "offset": 3524.48,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "Um otherwise like my AI code gen stuff",
      "offset": 3526.16,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "will not be able to do it very well.",
      "offset": 3528.079,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "Yeah. And that's like another thing that",
      "offset": 3530.64,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "I think um would be cool for a vibe",
      "offset": 3532.48,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "coding episode is like how do you make",
      "offset": 3534,
      "duration": 5.599
    },
    {
      "lang": "en",
      "text": "your code base um easy for AI to use?",
      "offset": 3536.079,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "And there's actually I think a really",
      "offset": 3539.599,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "important balance between like if you",
      "offset": 3540.88,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "have lots of little files like really",
      "offset": 3543.119,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "small files then you're going to end up",
      "offset": 3545.28,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "with like the AI has to remember to read",
      "offset": 3547.119,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "every single file and the prompting and",
      "offset": 3549.119,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "the tuning is going to set it to be a",
      "offset": 3551.04,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "little bit kind of like balanced in how",
      "offset": 3552.72,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "much it reads. But if your files are too",
      "offset": 3554.96,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "long, it's also trained to be a little",
      "offset": 3556.799,
      "duration": 7.121
    },
    {
      "lang": "en",
      "text": "bit um kind of uh thrifty with how much",
      "offset": 3559.52,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "of a file it reads, right? The models,",
      "offset": 3563.92,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "the the agents generally try to read",
      "offset": 3565.76,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "like, okay, if I can find the right 100",
      "offset": 3567.52,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "lines, I'm just going to read those. And",
      "offset": 3569.599,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "that can be an issue as well. And I",
      "offset": 3571.92,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "think we found a lot of success in um",
      "offset": 3573.52,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "what we're doing is like and you'll is",
      "offset": 3576.48,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "is in prompting the model to like make",
      "offset": 3578.48,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "sure it always reads a,000 or 1500. If",
      "offset": 3580.24,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "it's a long file, read the whole file",
      "offset": 3583.44,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "because otherwise it ends up like I I'm",
      "offset": 3585.2,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "sure we've seen this uh in vibe coding",
      "offset": 3587.119,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "where it's like reimplements a method",
      "offset": 3588.96,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "that already existed somewhere else just",
      "offset": 3591.119,
      "duration": 2.881
    },
    {
      "lang": "en",
      "text": "because it didn't read enough of the",
      "offset": 3592.799,
      "duration": 2.56
    },
    {
      "lang": "en",
      "text": "file to see that that function was",
      "offset": 3594,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "already there.",
      "offset": 3595.359,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "Exactly. So now I think it should be",
      "offset": 3597.92,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "almost done. Um and it uh it regen the",
      "offset": 3599.599,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "code which it didn't have to but it did.",
      "offset": 3602.24,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "That's okay. Um and now it should be",
      "offset": 3603.76,
      "duration": 7.319
    },
    {
      "lang": "en",
      "text": "able to update the UI as well.",
      "offset": 3606.96,
      "duration": 4.119
    },
    {
      "lang": "en",
      "text": "Um, this is the only part about VIP.",
      "offset": 3614.88,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "It's kind of annoying. I don't I can't",
      "offset": 3617.28,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "tell all the stuff. I think it's all",
      "offset": 3618.4,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "done. Cool. Let's go back to the UI and",
      "offset": 3620.16,
      "duration": 6.04
    },
    {
      "lang": "en",
      "text": "see what it does.",
      "offset": 3622.799,
      "duration": 3.401
    },
    {
      "lang": "en",
      "text": "So, now that we have this, now we can",
      "offset": 3628.88,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "just resummize.",
      "offset": 3630.799,
      "duration": 6.52
    },
    {
      "lang": "en",
      "text": "Oh, it fail.",
      "offset": 3634,
      "duration": 3.319
    },
    {
      "lang": "en",
      "text": "Um, oh, I don't have my API key for",
      "offset": 3637.92,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "entropic. Give me one second. Let me buy",
      "offset": 3639.92,
      "duration": 5.8
    },
    {
      "lang": "en",
      "text": "that in really fast.",
      "offset": 3642.079,
      "duration": 3.641
    },
    {
      "lang": "en",
      "text": "It would be quite funny if um I did not",
      "offset": 3648.48,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "realize, but I was secretly just sharing",
      "offset": 3652.4,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "all my API keys anyway because I had the",
      "offset": 3654.16,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "screen up, but I think I have my other",
      "offset": 3656.16,
      "duration": 5.32
    },
    {
      "lang": "en",
      "text": "screen.",
      "offset": 3658.48,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "Um",
      "offset": 3664.88,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "and API key.",
      "offset": 3673.52,
      "duration": 4.76
    },
    {
      "lang": "en",
      "text": "Okay.",
      "offset": 3679.76,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "Nobody back in again. And now I'll try",
      "offset": 3685.92,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "some more time.",
      "offset": 3689.839,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "resummarize it is processing",
      "offset": 3692,
      "duration": 6.559
    },
    {
      "lang": "en",
      "text": "and once it's processing we will be able",
      "offset": 3695.839,
      "duration": 6.081
    },
    {
      "lang": "en",
      "text": "to go see everything soon.",
      "offset": 3698.559,
      "duration": 5.121
    },
    {
      "lang": "en",
      "text": "Um,",
      "offset": 3701.92,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "while we're going to go do this, um, why",
      "offset": 3703.68,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "don't we use frameworks here? Um, I want",
      "offset": 3705.68,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "to show you the code that we're actually",
      "offset": 3707.76,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "using to call AI models, just to tell",
      "offset": 3709.28,
      "duration": 2.799
    },
    {
      "lang": "en",
      "text": "you an idea of why we don't use",
      "offset": 3710.96,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "frameworks. Um,",
      "offset": 3712.079,
      "duration": 6.641
    },
    {
      "lang": "en",
      "text": "I actually hate this bud code.",
      "offset": 3715.52,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "The fact that it consumes an editor slot",
      "offset": 3718.72,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "is really annoying.",
      "offset": 3720.559,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "Um, but when I go down into this, all",
      "offset": 3723.119,
      "duration": 6.401
    },
    {
      "lang": "en",
      "text": "I'm really doing here is this. I'm just",
      "offset": 3726.559,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "calling a method that calls summarize",
      "offset": 3729.52,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "video. The summarize video is just a",
      "offset": 3731.04,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "function that I've already defined that",
      "offset": 3733.359,
      "duration": 3.041
    },
    {
      "lang": "en",
      "text": "takes in a transcript, takes out a",
      "offset": 3735.04,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "title, and spits out a data model. The",
      "offset": 3736.4,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "fact that this uses an LLM doesn't",
      "offset": 3738.24,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "really matter, and I don't really need a",
      "offset": 3740.319,
      "duration": 5.681
    },
    {
      "lang": "en",
      "text": "framework to go deal with this. Um, in",
      "offset": 3742.88,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "order to orchestrate it, all I do at",
      "offset": 3746,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "this point is",
      "offset": 3748.079,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "once the summary is done, I feed the",
      "offset": 3750.48,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "summary into these three things and I",
      "offset": 3752.24,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "just async.io.gather it. So like the",
      "offset": 3753.839,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "benefit that I'm getting from a",
      "offset": 3756.96,
      "duration": 2.879
    },
    {
      "lang": "en",
      "text": "framework just isn't worth the debugging",
      "offset": 3758.079,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "headache that I would have to deal with.",
      "offset": 3759.839,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "All I really want is I just want to have",
      "offset": 3762.24,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "functions after one after another. And",
      "offset": 3763.839,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "this is one of like the points I think",
      "offset": 3767.119,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "we make a lot in um in 12 factor agents",
      "offset": 3768.559,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "is basically the idea that like code is",
      "offset": 3771.2,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "already a directed graph. And if you",
      "offset": 3773.28,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "really like thinking in graphs, you can",
      "offset": 3775.68,
      "duration": 3.119
    },
    {
      "lang": "en",
      "text": "use frameworks that give you kind of",
      "offset": 3777.359,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "this like nodes and edge edges",
      "offset": 3778.799,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "structure. But what you're looking at",
      "offset": 3780.559,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "can you go back to that real quick?",
      "offset": 3782.24,
      "duration": 5.119
    },
    {
      "lang": "en",
      "text": "what you're looking at on the screen",
      "offset": 3784.48,
      "duration": 6.079
    },
    {
      "lang": "en",
      "text": "is is a DAG. It's like do this thing and",
      "offset": 3787.359,
      "duration": 5.521
    },
    {
      "lang": "en",
      "text": "then fan it out and do these things and",
      "offset": 3790.559,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "Okay, can you can you go back to the",
      "offset": 3792.88,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "code real quick?\n You do this thing and",
      "offset": 3794.64,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "then",
      "offset": 3796.96,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "um you you do this thing and then you",
      "offset": 3799.76,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "fan it out to these three things and",
      "offset": 3802.16,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "then when they're all done then you go",
      "offset": 3803.599,
      "duration": 3.281
    },
    {
      "lang": "en",
      "text": "to another step. Like code is already a",
      "offset": 3804.88,
      "duration": 4.959
    },
    {
      "lang": "en",
      "text": "way of expressing directed graphs. Um",
      "offset": 3806.88,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "and if you are able to write code and do",
      "offset": 3809.839,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "the basics and get maybe get help from a",
      "offset": 3812.96,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "model and writing the code itself then",
      "offset": 3815.119,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "um you should leave yourself open to",
      "offset": 3817.92,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "whatever approaches and structures and",
      "offset": 3820.559,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "like now because we control all this",
      "offset": 3822.72,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "code. If we wanted to like add another",
      "offset": 3824.64,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "step here that is pre-summary and then",
      "offset": 3826.319,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "pass the output of that in in addition",
      "offset": 3828.319,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "to our summary here. We have the",
      "offset": 3830.24,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "flexibility and the freedom to kind of",
      "offset": 3832.319,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "architect this however we want and by",
      "offset": 3833.76,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "doing weird custom stuff. That's kind of",
      "offset": 3836.24,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "how you find the boundary of what the",
      "offset": 3838.96,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "model is capable of and kind of push the",
      "offset": 3841.28,
      "duration": 5.759
    },
    {
      "lang": "en",
      "text": "limits of what is possible with AI. Um,",
      "offset": 3843.599,
      "duration": 5.041
    },
    {
      "lang": "en",
      "text": "if you're just kind of like using a",
      "offset": 3847.039,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "generic approach, you're going to be",
      "offset": 3848.64,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "your quality is going to be capped at,",
      "offset": 3850.319,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "you know, whatever everyone else using",
      "offset": 3852.24,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "the generic approach is able to do",
      "offset": 3854.16,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "versus if you open the box and start",
      "offset": 3855.839,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "tinkering with the wires, you may be",
      "offset": 3857.52,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "able to get better results.",
      "offset": 3859.039,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "Yeah. Exactly. And then the other really",
      "offset": 3861.599,
      "duration": 3.281
    },
    {
      "lang": "en",
      "text": "thing important thing to note is like",
      "offset": 3863.119,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "most frameworks don't actually give you",
      "offset": 3864.88,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "like full fundamental control of the",
      "offset": 3866.319,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "full full full prompt. They add some",
      "offset": 3868.16,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "preamles they modify how your tools are",
      "offset": 3870.319,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "being presented because part of",
      "offset": 3873.039,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "standardizing things like passing",
      "offset": 3875.039,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "between open anthropic is standardizing",
      "offset": 3876.799,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "something. And while it's true that you",
      "offset": 3878.64,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "may want to go do that the problem is",
      "offset": 3881.2,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "the fundamental thing that you're",
      "offset": 3883.359,
      "duration": 2.641
    },
    {
      "lang": "en",
      "text": "passing into the model is these tokens.",
      "offset": 3884.4,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "Like these token oh it's going to take",
      "offset": 3886,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "that's going to take forever to render.",
      "offset": 3887.44,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "I [Â __Â ] up. um",
      "offset": 3888.96,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "uh running a model on tokenizer in a",
      "offset": 3891.92,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "browser is not a good call on very very",
      "offset": 3893.92,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "long text but at some point you're going",
      "offset": 3896.24,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "to call the model in some way and you're",
      "offset": 3897.92,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "going to want a model to see those exact",
      "offset": 3899.839,
      "duration": 5.041
    },
    {
      "lang": "en",
      "text": "tokens. Your quality is strictly bound",
      "offset": 3901.44,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "by the tokens you send in and out of the",
      "offset": 3904.88,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "model. The only thing that configures",
      "offset": 3906.64,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "what the model spits out is the tokens",
      "offset": 3908.16,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "in the tokens out. So if you can't",
      "offset": 3909.76,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "control every token out there, you can't",
      "offset": 3912.4,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "get it. So I think we've had some people",
      "offset": 3913.92,
      "duration": 2.639
    },
    {
      "lang": "en",
      "text": "for example that live from like other",
      "offset": 3915.28,
      "duration": 3.519
    },
    {
      "lang": "en",
      "text": "countries and like if they if they use a",
      "offset": 3916.559,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "library and they build a pipeline that's",
      "offset": 3918.799,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "supposed to speak let's say Japanese or",
      "offset": 3920,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "like um Persian or some uh or any other",
      "offset": 3921.839,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "language along the way like Hindi",
      "offset": 3924.4,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "anything if the library injects a word",
      "offset": 3925.68,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "of English the model's going to speak in",
      "offset": 3928.16,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "English um because it thinks it's",
      "offset": 3930.96,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "allowed to do that and it's really",
      "offset": 3933.68,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "important that if you have control you",
      "offset": 3936,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "can actually have that access along the",
      "offset": 3938,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "full way.",
      "offset": 3939.839,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "Hopefully that answered the question,",
      "offset": 3941.68,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "VJ.",
      "offset": 3942.799,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "Um, and now I think I canceled this",
      "offset": 3945.44,
      "duration": 5.119
    },
    {
      "lang": "en",
      "text": "again. Okay, third time to\n What do we",
      "offset": 3947.359,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "want to get through before we kind of",
      "offset": 3950.559,
      "duration": 6.161
    },
    {
      "lang": "en",
      "text": "wrap for today and go to questions?",
      "offset": 3952.799,
      "duration": 6.081
    },
    {
      "lang": "en",
      "text": "Um, I think we can go to questions. I'm",
      "offset": 3956.72,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "just going to go I showed kind of how to",
      "offset": 3958.88,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "make the summary summary slightly better",
      "offset": 3960.319,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "by going to go down this road. I'm I the",
      "offset": 3962.24,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "last thing I was going to do was",
      "offset": 3964.96,
      "duration": 2.48
    },
    {
      "lang": "en",
      "text": "actually feed in the email and the",
      "offset": 3966.079,
      "duration": 2.641
    },
    {
      "lang": "en",
      "text": "transcript to the email to make it",
      "offset": 3967.44,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "slightly better as well. So, I'll try",
      "offset": 3968.72,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "and get that wired up and if I don't get",
      "offset": 3970.64,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "it wired up on the call, I'll do it",
      "offset": 3972.079,
      "duration": 3.121
    },
    {
      "lang": "en",
      "text": "afterwards and go send out the content",
      "offset": 3973.52,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "as well so people can poke around with",
      "offset": 3975.2,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "the prompt. But you saw the iteration",
      "offset": 3976.48,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "loop of what I do. I take the prompt",
      "offset": 3978,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "once I have some data. I literally just",
      "offset": 3980.88,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "go ahead and like make a test case from",
      "offset": 3982.64,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "real data because I need to run the",
      "offset": 3984.4,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "pipeline once. I don't want to have fake",
      "offset": 3986.079,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "data. Fake data is the worst thing I can",
      "offset": 3988.16,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "do because if I use AI synthetic data,",
      "offset": 3990,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "it's it's going to work and then it",
      "offset": 3992.319,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "won't work for my real problem. So, I'll",
      "offset": 3993.92,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "just run the pipeline once, pay the",
      "offset": 3995.68,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "cost, download the test case, and then",
      "offset": 3997.359,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "I'll just iterate on the test case alone",
      "offset": 3999.44,
      "duration": 5.119
    },
    {
      "lang": "en",
      "text": "until I'm satisfied, then have the AI",
      "offset": 4001.599,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "model go fix all the downstream",
      "offset": 4004.559,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "pipelines along the way.",
      "offset": 4006.559,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "Yeah. And this is kind of what we did on",
      "offset": 4009.039,
      "duration": 2.721
    },
    {
      "lang": "en",
      "text": "the Evals episode, too, right? Which is",
      "offset": 4010.16,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "like push real data, push a little bit",
      "offset": 4011.76,
      "duration": 3.279
    },
    {
      "lang": "en",
      "text": "of real data through the pipeline and",
      "offset": 4013.44,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "then go look at it by hand and find,",
      "offset": 4015.039,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "okay, this would make a good test case",
      "offset": 4016.64,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "or this is a good example of something",
      "offset": 4018.24,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "the model's not good at. And like part",
      "offset": 4020,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "of baking golden files is about like,",
      "offset": 4022.079,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "oh, this is a thing it did right and I",
      "offset": 4024.16,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "like everything about this. Let's cement",
      "offset": 4025.68,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "that as a test case. So as I change",
      "offset": 4027.68,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "things, I know that that one will",
      "offset": 4029.68,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "continue to work. And then also, hey,",
      "offset": 4031.2,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "let me go find a thing that doesn't work",
      "offset": 4033.359,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "and let me go take that down and put it",
      "offset": 4035.2,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "on the workbench and tinker with the",
      "offset": 4036.72,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "prompt and iterate on it.\n Exactly. And I",
      "offset": 4038,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "think if I go look at the new summaries,",
      "offset": 4040.96,
      "duration": 2.48
    },
    {
      "lang": "en",
      "text": "like we've generated quite a few",
      "offset": 4042.4,
      "duration": 2.32
    },
    {
      "lang": "en",
      "text": "summaries at this point. I think this is",
      "offset": 4043.44,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "the most recent one. We're literally",
      "offset": 4044.72,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "generating time codes and everything",
      "offset": 4046.88,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "else. So the fact that the UI isn't",
      "offset": 4048.16,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "rendering it means that cloud code just",
      "offset": 4049.92,
      "duration": 3.119
    },
    {
      "lang": "en",
      "text": "messed up in the UI and didn't render",
      "offset": 4051.44,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "this correctly. So I just need to go",
      "offset": 4053.039,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "debug that and figure this out. But it's",
      "offset": 4054.24,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "really easy for me to like cool the time",
      "offset": 4056.48,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "codes look good and this is this might",
      "offset": 4057.839,
      "duration": 3.121
    },
    {
      "lang": "en",
      "text": "be good enough for the next step of the",
      "offset": 4059.76,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "pipeline. So it allows me to just debug",
      "offset": 4060.96,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "really really quickly to see what's",
      "offset": 4064.16,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "actually happening under the hood. Um",
      "offset": 4065.68,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "with that let's break for questions",
      "offset": 4068.88,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "really fast and if we don't have any",
      "offset": 4070.24,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "we'll just keep by coding for like the",
      "offset": 4072.079,
      "duration": 5.24
    },
    {
      "lang": "en",
      "text": "next 10 minutes or so.",
      "offset": 4073.44,
      "duration": 3.879
    },
    {
      "lang": "en",
      "text": "Um,",
      "offset": 4078.96,
      "duration": 3.119
    },
    {
      "lang": "en",
      "text": "what do you think about tools like the",
      "offset": 4080.64,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "Verscell AI SDK that still give you all",
      "offset": 4082.079,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "that flexibility to do whatever you",
      "offset": 4084.72,
      "duration": 2.879
    },
    {
      "lang": "en",
      "text": "want, but also give you tons of helpful",
      "offset": 4086,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "stuff for streaming and tools and UI and",
      "offset": 4087.599,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "React integration, other helpers and",
      "offset": 4089.52,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "utilities.",
      "offset": 4091.28,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "What do I personally think about the",
      "offset": 4093.44,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "Versell AI SDK? Um,",
      "offset": 4094.88,
      "duration": 5.839
    },
    {
      "lang": "en",
      "text": "I think the Versell AI SDK is okay to be",
      "offset": 4098,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "completely honest. Um, I think the",
      "offset": 4100.719,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "biggest problem that I have with the",
      "offset": 4103.92,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "Verscell AI SDK is that one, it's too",
      "offset": 4105.44,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "deeply tied to a lot of native Versel",
      "offset": 4108.319,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "stuff, and two, I don't actually think",
      "offset": 4110.96,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "it captures the nuance of what streaming",
      "offset": 4112.799,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "actually is. Um, a lot of people think",
      "offset": 4114.719,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "of streaming for structure data to be",
      "offset": 4117.279,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "trivial. U but it's actually a really,",
      "offset": 4119.04,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "really, really hard problem. And I'll",
      "offset": 4121.6,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "show you why it's hard. Um, and why\n of",
      "offset": 4123.359,
      "duration": 5.521
    },
    {
      "lang": "en",
      "text": "why it's hard, I I wrote the tool",
      "offset": 4126.96,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "streaming for VLM",
      "offset": 4128.88,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "So I like I I'm familiar with I've",
      "offset": 4131.6,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "actually contributed to the streaming in",
      "offset": 4133.679,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "the AI SDK too. Um I guess like I I've",
      "offset": 4135.04,
      "duration": 5.279
    },
    {
      "lang": "en",
      "text": "used it on projects where I've never",
      "offset": 4138.64,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "used Versol at all. Like it doesn't",
      "offset": 4140.319,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "force you into Versell's ecosystem. It's",
      "offset": 4142,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "actually like totally independent of",
      "offset": 4143.92,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "their platform.\n Yeah. Yeah.",
      "offset": 4145.44,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "So I was just curious like because it",
      "offset": 4147.92,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "gives you lots of great stuff for like",
      "offset": 4149.6,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "generating like if you're doing like",
      "offset": 4151.839,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "tool calls or structure generation it",
      "offset": 4153.679,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "lets you stream that object to the you",
      "offset": 4155.52,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "know your react app or whatever and get",
      "offset": 4158.48,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "a hook so that you can see okay well",
      "offset": 4160,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "this is this field has been generated",
      "offset": 4161.52,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "and now this field has been generated",
      "offset": 4163.04,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "and so you can like make your user",
      "offset": 4164.56,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "interface interactive as um you know",
      "offset": 4166.4,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "like for example if you're generating a",
      "offset": 4169.359,
      "duration": 3.041
    },
    {
      "lang": "en",
      "text": "complicated object you can make your",
      "offset": 4171.04,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "time to interactivity and time to user",
      "offset": 4172.4,
      "duration": 5.359
    },
    {
      "lang": "en",
      "text": "feedback faster. Um, but I like it's the",
      "offset": 4174,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "one the one one thing it does force you",
      "offset": 4177.759,
      "duration": 3.281
    },
    {
      "lang": "en",
      "text": "into is using Zod. Like it's very",
      "offset": 4179.52,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "opinionated about using ZOD for schema",
      "offset": 4181.04,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "definition, which is fine. Zod's a great",
      "offset": 4183.359,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "library, but like I love all the great",
      "offset": 4184.88,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "things that BAML gives you for that. So",
      "offset": 4187.359,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "I guess like my question is like more",
      "offset": 4189.359,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "along the lines of like is that do you",
      "offset": 4192,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "ever see there being like some type of",
      "offset": 4193.52,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "integration there of using like\n with",
      "offset": 4194.96,
      "duration": 5.279
    },
    {
      "lang": "en",
      "text": "actually have a BAML integration with",
      "offset": 4198.239,
      "duration": 5.041
    },
    {
      "lang": "en",
      "text": "Versell? Um, all you have to do is like",
      "offset": 4200.239,
      "duration": 5.041
    },
    {
      "lang": "en",
      "text": "so BMAL under the hood. I mean nextJS",
      "offset": 4203.28,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "right\n yeah we have a function like",
      "offset": 4205.28,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "summarize video so what you can do in",
      "offset": 4207.679,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "your VS in your code directly let's say",
      "offset": 4209.6,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "you wanted to do this all in your front",
      "offset": 4211.92,
      "duration": 3.279
    },
    {
      "lang": "en",
      "text": "end code I didn't actually do this here",
      "offset": 4213.12,
      "duration": 6.559
    },
    {
      "lang": "en",
      "text": "but I could um we also do the thing for",
      "offset": 4215.199,
      "duration": 5.921
    },
    {
      "lang": "en",
      "text": "you where we just give you a hook that",
      "offset": 4219.679,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "you can just use import um",
      "offset": 4221.12,
      "duration": 10.76
    },
    {
      "lang": "en",
      "text": "uh let me do this from atlantreact",
      "offset": 4226.159,
      "duration": 5.721
    },
    {
      "lang": "en",
      "text": "hooks So we have a function here called",
      "offset": 4232.48,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "summarize video. So you got to use",
      "offset": 4234.96,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "summarize video hook and then what you",
      "offset": 4236.4,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "can do is const",
      "offset": 4239.12,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "equals use summarize video and then you",
      "offset": 4242.08,
      "duration": 6.639
    },
    {
      "lang": "en",
      "text": "can have like a data as a summary or you",
      "offset": 4245.28,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "can have a mutate function and all the",
      "offset": 4248.719,
      "duration": 3.041
    },
    {
      "lang": "en",
      "text": "other state associated with it and you",
      "offset": 4250.48,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "can just go call this with whatever data",
      "offset": 4251.76,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "you want. The summary is guaranteed to",
      "offset": 4253.44,
      "duration": 6.4
    },
    {
      "lang": "en",
      "text": "be a version of this type along the way.",
      "offset": 4256.32,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "So you can do like\n and the and the",
      "offset": 4259.84,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "partial type is the thing that has like",
      "offset": 4262.8,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "based on your BAML code of like what's",
      "offset": 4264.88,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "what's nullable in a stream basically.",
      "offset": 4267.76,
      "duration": 6.959
    },
    {
      "lang": "en",
      "text": "Right.\n Exactly. So you call like mutate",
      "offset": 4270.08,
      "duration": 6.4
    },
    {
      "lang": "en",
      "text": "um and when you call mutate you get like",
      "offset": 4274.719,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "a stream object along this um along the",
      "offset": 4276.48,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "way but the data object just updates",
      "offset": 4279.199,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "along the way. So data become\n okay. I",
      "offset": 4280.8,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "didn't know you had that. So,",
      "offset": 4283.44,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "so you and and like when you do summary",
      "offset": 4286.32,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "dot you get like bullet points and you",
      "offset": 4288.56,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "notice that all these things get like",
      "offset": 4290.08,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "optionalized along the way. But what I",
      "offset": 4291.28,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "could do is I could say even during",
      "offset": 4293.92,
      "duration": 3.279
    },
    {
      "lang": "en",
      "text": "streaming I want to guarantee that video",
      "offset": 4295.36,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "summary key point is going to be I",
      "offset": 4297.199,
      "duration": 5.121
    },
    {
      "lang": "en",
      "text": "actually want to guarantee that hey when",
      "offset": 4300.88,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "I go send the video summary I want key",
      "offset": 4302.32,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "points to individually be stream.done",
      "offset": 4304.4,
      "duration": 6.56
    },
    {
      "lang": "en",
      "text": "the the key points field has to be done.",
      "offset": 4307.28,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "So all right I\n so that would mean that",
      "offset": 4310.96,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "like this the hook would not start",
      "offset": 4312.56,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "streaming data until the key points were",
      "offset": 4314.96,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "all processed and then you would start",
      "offset": 4317.6,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "getting the partial object for all the",
      "offset": 4319.36,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "rest of the fields\n until main takeaways",
      "offset": 4321.04,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "is done. So now main takeaways becomes",
      "offset": 4323.199,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "guaranteed to be a string array versus",
      "offset": 4324.88,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "if you're trivially streaming you now",
      "offset": 4327.6,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "have to go do this and now it becomes",
      "offset": 4329.6,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "like a very\n everything is now everything",
      "offset": 4331.199,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "everything is nullable versus hey I",
      "offset": 4333.52,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "guarantee that time data and main",
      "offset": 4335.52,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "takeaways will always be true before I",
      "offset": 4337.28,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "hand it to my renderer.",
      "offset": 4338.96,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "want to stream just this array. Each",
      "offset": 4340.96,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "string should be individually streamed.",
      "offset": 4343.44,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "And now you get a slightly different",
      "offset": 4345.199,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "type. You get an array. Oh, whoops.",
      "offset": 4346.48,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "This will render in a second. You get an",
      "offset": 4349.44,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "array of like",
      "offset": 4351.84,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "this should actually work. I'm surprised",
      "offset": 4354.32,
      "duration": 2.64
    },
    {
      "lang": "en",
      "text": "that this doesn't work. Maybe the code",
      "offset": 4355.76,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "gen is wrong for this. Let me check one",
      "offset": 4356.96,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "more time. Point. We're actually in the",
      "offset": 4359.6,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "middle like redefining our types. Some",
      "offset": 4362.64,
      "duration": 3.039
    },
    {
      "lang": "en",
      "text": "stuff works with Go, which requires",
      "offset": 4364.239,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "perfectly strict types and doesn't allow",
      "offset": 4365.679,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "for any flexibility. So, we're just",
      "offset": 4368,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "finding a couple bugs in there, but the",
      "offset": 4369.52,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "idea is like you you when you're",
      "offset": 4371.36,
      "duration": 2.64
    },
    {
      "lang": "en",
      "text": "streaming, you actually live in a",
      "offset": 4372.88,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "duality of type systems. And that is the",
      "offset": 4374,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "part that people really miss out on. You",
      "offset": 4376.56,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "really have two type trades you have to",
      "offset": 4378.159,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "navigate, which is really really",
      "offset": 4379.6,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "complicated. So, having utilities like",
      "offset": 4381.36,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "you can't actually use TypeScript's",
      "offset": 4384.08,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "partial type to go do that. So then when",
      "offset": 4385.44,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "you go ahead and actually build",
      "offset": 4387.92,
      "duration": 2.48
    },
    {
      "lang": "en",
      "text": "something out, you want to go build like",
      "offset": 4388.88,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "really interactive component.",
      "offset": 4390.4,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "Doing this should be trivial.",
      "offset": 4393.199,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "where I built a React component that",
      "offset": 4396.48,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "analyzes and does this, but you can't",
      "offset": 4398,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "really do that because this is working",
      "offset": 4400.08,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "off of two type trees and that is a hard",
      "offset": 4401.84,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "part and making that easy I think is",
      "offset": 4404.48,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "something else AICK doesn't actually do",
      "offset": 4406.48,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "when you get to real complicated AI real",
      "offset": 4408.719,
      "duration": 5.601
    },
    {
      "lang": "en",
      "text": "complicated JSON structures because if",
      "offset": 4411.84,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "you have recursive types you have nested",
      "offset": 4414.32,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "objects you can't define that well in",
      "offset": 4416.159,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "ZOD you really need two ZOD schemas to",
      "offset": 4418.08,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "do that correctly.",
      "offset": 4420.32,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "Okay, thanks.",
      "offset": 4422.88,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "Damn.",
      "offset": 4425.04,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "Um,\n that was uh that was that was uh",
      "offset": 4427.679,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "welcome. That's that's why we call it",
      "offset": 4430.32,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "advanced AI engineering, folks.",
      "offset": 4431.679,
      "duration": 5.441
    },
    {
      "lang": "en",
      "text": "That was sick.",
      "offset": 4434.64,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "Um,",
      "offset": 4437.12,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "there's another question. Uh, you want",
      "offset": 4439.76,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "to talk about the email optimization?",
      "offset": 4441.44,
      "duration": 2.719
    },
    {
      "lang": "en",
      "text": "Yeah, I want to figure out why that's",
      "offset": 4442.8,
      "duration": 2.56
    },
    {
      "lang": "en",
      "text": "not working, but like what we can do",
      "offset": 4444.159,
      "duration": 2.641
    },
    {
      "lang": "en",
      "text": "here is like we can just make this test",
      "offset": 4445.36,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "case really fast. I think Abby mind you",
      "offset": 4446.8,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "had the question. Do you want to",
      "offset": 4448.8,
      "duration": 2.32
    },
    {
      "lang": "en",
      "text": "summarize this question for me, Dex,",
      "offset": 4449.84,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "while I get this working?",
      "offset": 4451.12,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "Yeah. No, I don't question like uh when",
      "offset": 4453.28,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "he mentioned about the AI SDK, I felt",
      "offset": 4455.6,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "that we uh while we were stripping bolt",
      "offset": 4458.159,
      "duration": 7.281
    },
    {
      "lang": "en",
      "text": "new which uses AI SDK uh we kind of uh",
      "offset": 4460.8,
      "duration": 8.08
    },
    {
      "lang": "en",
      "text": "touched upon that part of the uh the",
      "offset": 4465.44,
      "duration": 6.239
    },
    {
      "lang": "en",
      "text": "ecosystem and I personally found out",
      "offset": 4468.88,
      "duration": 6.64
    },
    {
      "lang": "en",
      "text": "that there's no way to control what uh",
      "offset": 4471.679,
      "duration": 7.04
    },
    {
      "lang": "en",
      "text": "the LLM produces out when you use Versal",
      "offset": 4475.52,
      "duration": 7.6
    },
    {
      "lang": "en",
      "text": "SDK versus when you use BAML. So um with",
      "offset": 4478.719,
      "duration": 7.361
    },
    {
      "lang": "en",
      "text": "bold new the problem always existed that",
      "offset": 4483.12,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "the model the LLM could generate",
      "offset": 4486.08,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "anything and versal there's no way to",
      "offset": 4488.159,
      "duration": 6.161
    },
    {
      "lang": "en",
      "text": "control it. Um so I was just\n structured",
      "offset": 4490.159,
      "duration": 7.601
    },
    {
      "lang": "en",
      "text": "object generation.\n What\n you mean like",
      "offset": 4494.32,
      "duration": 5.839
    },
    {
      "lang": "en",
      "text": "structured object generation\n like",
      "offset": 4497.76,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "generating?\n Yeah, it totally has that.",
      "offset": 4500.159,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "It like generate object and stream",
      "offset": 4502.56,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "object. You just give it a ZOD schema",
      "offset": 4504.32,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "and tell it generate an object. No, but",
      "offset": 4506.159,
      "duration": 5.601
    },
    {
      "lang": "en",
      "text": "like how do you evaluate the output of",
      "offset": 4508.719,
      "duration": 6.241
    },
    {
      "lang": "en",
      "text": "like uh imagine you have you are gener",
      "offset": 4511.76,
      "duration": 8.24
    },
    {
      "lang": "en",
      "text": "generating a prompt. How do you uh",
      "offset": 4514.96,
      "duration": 9.44
    },
    {
      "lang": "en",
      "text": "how do you make sure that the",
      "offset": 4520,
      "duration": 7.679
    },
    {
      "lang": "en",
      "text": "response is is sort of consistent as in",
      "offset": 4524.4,
      "duration": 7.839
    },
    {
      "lang": "en",
      "text": "like um what we did was basically tried",
      "offset": 4527.679,
      "duration": 7.52
    },
    {
      "lang": "en",
      "text": "to give examples of uh prompt",
      "offset": 4532.239,
      "duration": 6.081
    },
    {
      "lang": "en",
      "text": "engineering to the LLM itself while",
      "offset": 4535.199,
      "duration": 6.561
    },
    {
      "lang": "en",
      "text": "using the SD AI SDK. I didn't see a way",
      "offset": 4538.32,
      "duration": 7.44
    },
    {
      "lang": "en",
      "text": "of uh the SDK controlling the response",
      "offset": 4541.76,
      "duration": 7.2
    },
    {
      "lang": "en",
      "text": "generated from the LLM in in a way that",
      "offset": 4545.76,
      "duration": 6.479
    },
    {
      "lang": "en",
      "text": "you can do with BAML like if you if you",
      "offset": 4548.96,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "get what I mean.",
      "offset": 4552.239,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "Yeah.\n The biggest I think a lot of",
      "offset": 4554.4,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "people ask like why can't you just use",
      "offset": 4555.84,
      "duration": 2.399
    },
    {
      "lang": "en",
      "text": "structure generation? I think that's",
      "offset": 4557.12,
      "duration": 2.64
    },
    {
      "lang": "en",
      "text": "what Kyle's question here is like why",
      "offset": 4558.239,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "doesn't that just work? The reason that",
      "offset": 4559.76,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "structure generation really doesn't work",
      "offset": 4561.44,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "fundamentally is that like for example",
      "offset": 4562.96,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "check out this email. this email that we",
      "offset": 4564.88,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "have is literally outputting JSON with",
      "offset": 4566.719,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "escape characters with new lines that",
      "offset": 4568.4,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "itself is going to lead to like a bias",
      "offset": 4571.12,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "in the model. So what I really want to",
      "offset": 4573.28,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "do is I want to somehow tell this draft",
      "offset": 4574.96,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "to say at description use triple quote",
      "offset": 4577.44,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "strings",
      "offset": 4580.8,
      "duration": 5.359
    },
    {
      "lang": "en",
      "text": "u for multi-line strings.",
      "offset": 4582.64,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "And now when I go run this it doesn't",
      "offset": 4586.159,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "generate escape characters anymore. the",
      "offset": 4588.32,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "email will just be better because",
      "offset": 4590.239,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "instead of sampling for the best token",
      "offset": 4592.08,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "that is parsible, I allow the model to",
      "offset": 4593.84,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "output the best token actually thinks is",
      "offset": 4595.679,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "the best. And that makes a huge",
      "offset": 4597.76,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "difference in quality for the model",
      "offset": 4600.4,
      "duration": 2.64
    },
    {
      "lang": "en",
      "text": "because when you're not doing this,",
      "offset": 4601.92,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "Dexter had a great great visual on this",
      "offset": 4603.04,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "last time which was basically saying",
      "offset": 4605.36,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "something like this which is when the",
      "offset": 4607.36,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "model goes and generates a this is the",
      "offset": 4608.88,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "token it wants it wants to generate a",
      "offset": 4610.48,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "new line character and then there are",
      "offset": 4612.08,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "other characters that wants to go",
      "offset": 4614.4,
      "duration": 3.279
    },
    {
      "lang": "en",
      "text": "generate like oh a backslash or a back",
      "offset": 4615.52,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "slashn or something else along the way.",
      "offset": 4617.679,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "When you go do this, when you do",
      "offset": 4620,
      "duration": 2.719
    },
    {
      "lang": "en",
      "text": "structure generation, you basically say",
      "offset": 4621.6,
      "duration": 2.4
    },
    {
      "lang": "en",
      "text": "you are not allowed to generate the new",
      "offset": 4622.719,
      "duration": 2.401
    },
    {
      "lang": "en",
      "text": "line character because you're inside of",
      "offset": 4624,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "a quotation mark. That's not valid JSON",
      "offset": 4625.12,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "and you invalidate those tokens. We did",
      "offset": 4627.199,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "this a lot in computer vision in the",
      "offset": 4629.52,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "past last 10 years where you like tried",
      "offset": 4630.88,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "to outsmart the model.\n It turns out just",
      "offset": 4632.64,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "like let the model do its thing. The",
      "offset": 4635.28,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "only reason that we have to do structure",
      "offset": 4637.12,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "generation with JSON is because we don't",
      "offset": 4638.64,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "have a better parser. If we had a better",
      "offset": 4640.4,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "schema that we could go and like parse,",
      "offset": 4642.32,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "then it would just do the trick for us",
      "offset": 4644.159,
      "duration": 2.641
    },
    {
      "lang": "en",
      "text": "and we could let the model do whatever",
      "offset": 4645.52,
      "duration": 2.719
    },
    {
      "lang": "en",
      "text": "it wanted.",
      "offset": 4646.8,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "Yeah, JSON was like totally the wrong",
      "offset": 4648.239,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "decision for for structured generation",
      "offset": 4650.08,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "just in general. Like JSON should never",
      "offset": 4652.239,
      "duration": 3.041
    },
    {
      "lang": "en",
      "text": "have been the primitive for that. It",
      "offset": 4654.08,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "should have been like toml or YAML or",
      "offset": 4655.28,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "something where it's much more like",
      "offset": 4657.04,
      "duration": 7.52
    },
    {
      "lang": "en",
      "text": "everyone uses XML, right?\n Well, XML",
      "offset": 4659.04,
      "duration": 7.04
    },
    {
      "lang": "en",
      "text": "and YAML have the same problems. Toml",
      "offset": 4664.56,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "and Yam are basically in disccernible",
      "offset": 4666.08,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "for where they start and their ambiguity",
      "offset": 4667.6,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "is a starting point. It's really really",
      "offset": 4669.679,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "hard to mclarify because almost every",
      "offset": 4671.28,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "file is a valid YAML file. So parsing",
      "offset": 4674.159,
      "duration": 5.121
    },
    {
      "lang": "en",
      "text": "that is virtually impossible because",
      "offset": 4677.52,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "there's no there's not enough",
      "offset": 4679.28,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "constraints on it. XML has a similar",
      "offset": 4680.64,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "problem where you run into escape",
      "offset": 4683.04,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "characters with XML and like the actual",
      "offset": 4684.08,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "way that you do XML is so nasty. If",
      "offset": 4686.08,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "you've seen how JSON looks in XML, like",
      "offset": 4688.4,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "type systems and XML don't really make",
      "offset": 4690.64,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "sense. They're too verbose. So it's not",
      "offset": 4692.32,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "that there's one answer for everything.",
      "offset": 4694.96,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "It's just that you want the flexibility",
      "offset": 4696.719,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "as the app designer to choose what you",
      "offset": 4698.4,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "want. If your if your goal as a",
      "offset": 4700.4,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "developer is to generate HTML code, XML",
      "offset": 4702.4,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "will be a bad format for you. Guaranteed",
      "offset": 4704.96,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "definition.\n All right. All right. All",
      "offset": 4707.44,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "right. I'm going to pause this on the",
      "offset": 4709.36,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "structured output. This is a dope deep",
      "offset": 4711.04,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "dive. We have seven minutes left and we",
      "offset": 4712.719,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "have some really good questions in the",
      "offset": 4714.48,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "chat. Um I we have Derek has a hand",
      "offset": 4715.679,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "raised. Um so, uh and that I really like",
      "offset": 4717.92,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "Sean's question as well, which is like",
      "offset": 4720.56,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "how do you keep your database schemas",
      "offset": 4722.159,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "consistent with your BAML generated",
      "offset": 4723.92,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "types? Um I think that one is quick,",
      "offset": 4725.52,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "right? We're just dumping JSON B into",
      "offset": 4728,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "the database and and the front end is",
      "offset": 4729.679,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "parsing it using the BAML library.",
      "offset": 4731.36,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "Um, but then I want to hear D's question",
      "offset": 4734.159,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "as well.\n Literally the types that I save",
      "offset": 4735.84,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "are just text and I think I just do JS.",
      "offset": 4738.8,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "I think I up actually I have this is not",
      "offset": 4740.56,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "the latest table. I had to do some",
      "offset": 4742.32,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "migrations. Uh, where's my migrations",
      "offset": 4744.48,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "table?",
      "offset": 4747.36,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "Um, I added uh all of these became JSON",
      "offset": 4749.52,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "views. They're just JSON views. So I",
      "offset": 4752.96,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "just open and close JSON V files uh JSON",
      "offset": 4754.64,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "B columns along the way and then I just",
      "offset": 4757.52,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "cast it because I know it's correct. The",
      "offset": 4759.76,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "hard part about this is versioning",
      "offset": 4761.679,
      "duration": 2.881
    },
    {
      "lang": "en",
      "text": "tables over time. But this is the same",
      "offset": 4763.04,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "problem that you'd have in versioning at",
      "offset": 4764.56,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "any other point in your codebase. Um we",
      "offset": 4765.92,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "will have a solution for that really",
      "offset": 4768.719,
      "duration": 2.241
    },
    {
      "lang": "en",
      "text": "soon actually. It's going to be really",
      "offset": 4769.76,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "exciting. That's several conversation.",
      "offset": 4770.96,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "Uh versioning schemas is really really",
      "offset": 4773.76,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "hard.",
      "offset": 4775.52,
      "duration": 6.4
    },
    {
      "lang": "en",
      "text": "All right Derek what you got?",
      "offset": 4777.679,
      "duration": 5.681
    },
    {
      "lang": "en",
      "text": "Just a quick question. And you've made",
      "offset": 4781.92,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "me think um earlier when you were doing",
      "offset": 4783.36,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "the summarize I had put a suggestion",
      "offset": 4785.28,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "maybe synthesize all of the variable",
      "offset": 4787.28,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "names functions aside really it's that",
      "offset": 4789.679,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "first sentence where you say summarize",
      "offset": 4791.76,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "this but then now I was first going to",
      "offset": 4794.159,
      "duration": 5.281
    },
    {
      "lang": "en",
      "text": "ask about eval like an approach if you",
      "offset": 4796.64,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "wanted to try a few different versions",
      "offset": 4799.44,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "of that sentence how would you eval it",
      "offset": 4801.84,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "but then now it's I'm thinking it's more",
      "offset": 4804.56,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "complex because synthesis because of the",
      "offset": 4806.719,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "way you've blocked this into those time",
      "offset": 4809.199,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "chunks",
      "offset": 4811.679,
      "duration": 2.801
    },
    {
      "lang": "en",
      "text": "then it may not even you know like",
      "offset": 4812.64,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "synthesize across them. So if this",
      "offset": 4814.48,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "question is making any sense, how would",
      "offset": 4816.4,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "you handle that to kind of evalid uh",
      "offset": 4818.96,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "given that you've taken a chunked",
      "offset": 4821.199,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "approach now?",
      "offset": 4822.96,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "Well, I would just the pipeline\n like I",
      "offset": 4825.04,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "think\n chunked meaning the chunk meaning",
      "offset": 4828.56,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "the transcript time chunks. So like now",
      "offset": 4830.56,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "you'd have to look across those chunks",
      "offset": 4833.04,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "to do a synthesis as opposed to",
      "offset": 4834.96,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "summarizing each one. That that's what I",
      "offset": 4836.64,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "meant. So hopefully I'm making sense",
      "offset": 4839.199,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "with my question.\n No, that okay. So this",
      "offset": 4841.04,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "was I mean we haven't actually I don't",
      "offset": 4843.92,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "think we've actually done that yet. Um",
      "offset": 4845.04,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "but the proposal was like hey we could",
      "offset": 4846.88,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "make the transcript a little bit shorter",
      "offset": 4848.48,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "and that may actually not be uh be the",
      "offset": 4849.92,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "right idea.\n Um yeah\n but then you have to",
      "offset": 4852.4,
      "duration": 5.839
    },
    {
      "lang": "en",
      "text": "basically have a\n you basically have to",
      "offset": 4856,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "have a separate eval that is like okay",
      "offset": 4858.239,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "based on our chunking prompt and our",
      "offset": 4860,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "chunking algorithm and how do we take",
      "offset": 4861.84,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "that those chunks and pass it back in?",
      "offset": 4863.6,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "We're no longer evaluating just input",
      "offset": 4865.28,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "output pairs. We're now evaluating kind",
      "offset": 4867.84,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "of a few steps of the pipeline which is",
      "offset": 4869.76,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "also really important to do right the",
      "offset": 4872.08,
      "duration": 2.639
    },
    {
      "lang": "en",
      "text": "best eval is the one that actually tell",
      "offset": 4873.28,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "runs your pipeline end to end and tells",
      "offset": 4874.719,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "you if it's still working and then you",
      "offset": 4877.28,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "can go refine down into the smaller",
      "offset": 4879.36,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "level ones smaller chunks of your",
      "offset": 4881.6,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "pipeline and then individual prompts to",
      "offset": 4882.96,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "narrow down like okay what's the thing",
      "offset": 4885.36,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "that broke this but um I think the",
      "offset": 4887.04,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "answer is like that's probably not what",
      "offset": 4890,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "we would do",
      "offset": 4891.92,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "that was just a proposal like because",
      "offset": 4894.4,
      "duration": 2.799
    },
    {
      "lang": "en",
      "text": "you have control of the pipeline you can",
      "offset": 4895.84,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "do whatever you",
      "offset": 4897.199,
      "duration": 2.561
    },
    {
      "lang": "en",
      "text": "I don't know if that's a great answer.",
      "offset": 4900,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "Oh, thumbs up. All right, we'll take it.",
      "offset": 4901.84,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "Cool. So, let's actually make the email",
      "offset": 4904.8,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "better really fast. By the way, while",
      "offset": 4906.48,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "I'm here,",
      "offset": 4907.92,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "uh, email example, we're going to do",
      "offset": 4909.679,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "this. Um, an example. An example. Great.",
      "offset": 4912,
      "duration": 7.12
    },
    {
      "lang": "en",
      "text": "Email was this.\n Sahil's question is now",
      "offset": 4915.679,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "that we have an automated pipeline, are",
      "offset": 4919.12,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "they are we are they going to get the uh",
      "offset": 4920.639,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "video upload now instead of Friday?",
      "offset": 4922.48,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "No. Um because we're going to make the",
      "offset": 4925.52,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "pipeline good. I think one of the things",
      "offset": 4928.4,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "I hate as an engineer is I [Â __Â ] hate",
      "offset": 4929.679,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "emails. I want emails to be useful or",
      "offset": 4932,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "not at all received. Uh so Dexter and I",
      "offset": 4934.56,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "actually do some good work to actually",
      "offset": 4936.639,
      "duration": 2.801
    },
    {
      "lang": "en",
      "text": "go edit the pipeline to make the emails",
      "offset": 4937.76,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "good. So like in this case, I literally",
      "offset": 4939.44,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "an example of here's what it is. Fed in",
      "offset": 4941.92,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "the example and it's going to go ahead",
      "offset": 4944.32,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "and",
      "offset": 4946.32,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "um and it's going to go ahead and not",
      "offset": 4948,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "read the prompt. I actually told it a",
      "offset": 4950.4,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "great exampam a great email was this. a",
      "offset": 4952.08,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "great email for",
      "offset": 4955.04,
      "duration": 5.639
    },
    {
      "lang": "en",
      "text": "a prior",
      "offset": 4957.28,
      "duration": 3.399
    },
    {
      "lang": "en",
      "text": "uh video was this. And now I'll notice a",
      "offset": 4961.44,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "small things that I noticed immediately,",
      "offset": 4963.679,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "which is these bullet points are labeled",
      "offset": 4966.239,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "wrong. So I want to go and like format",
      "offset": 4968.48,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "these better.",
      "offset": 4969.92,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "So let's go do that or",
      "offset": 4971.679,
      "duration": 5.921
    },
    {
      "lang": "en",
      "text": "like let's just make the prompt better",
      "offset": 4975.28,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "and like that's great. I don't need to",
      "offset": 4977.6,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "tell I don't need to sick.",
      "offset": 4979.76,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "um",
      "offset": 4983.12,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "topic in this again. Let's go read this.",
      "offset": 4985.76,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "Looks good. Main takeaways. Looks good.",
      "offset": 4988.88,
      "duration": 7.359
    },
    {
      "lang": "en",
      "text": "Uh transcript. Complete transcript.",
      "offset": 4991.76,
      "duration": 7.28
    },
    {
      "lang": "en",
      "text": "Full transcript.",
      "offset": 4996.239,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "Um we'll go do this. I will probably",
      "offset": 4999.04,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "move the transcript above the main take.",
      "offset": 5000.96,
      "duration": 6.88
    },
    {
      "lang": "en",
      "text": "Yep.\n Because again biased mostly by the",
      "offset": 5003.92,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "most recent tokens. So I'm going to go",
      "offset": 5007.84,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "do that. So I want the main takeaways to",
      "offset": 5009.52,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "be last. It's going to go do this. I'm",
      "offset": 5010.96,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "using GP4 mini. Let's just try and see",
      "offset": 5013.199,
      "duration": 5.241
    },
    {
      "lang": "en",
      "text": "what happens.",
      "offset": 5015.199,
      "duration": 3.241
    },
    {
      "lang": "en",
      "text": "I've heard GPT I've experimented a",
      "offset": 5019.76,
      "duration": 6.399
    },
    {
      "lang": "en",
      "text": "little bit. The GPT4.5 is is um very",
      "offset": 5021.76,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "good these sort of writing tasks.\n A",
      "offset": 5026.159,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "couple of things. If you notice it said",
      "offset": 5028,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "your name, that's",
      "offset": 5030,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "a a private video. Um create a",
      "offset": 5032.239,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "professional announcement on behalf",
      "offset": 5035.04,
      "duration": 7.52
    },
    {
      "lang": "en",
      "text": "of five and Dexter. Let's tell it that.",
      "offset": 5038.719,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "Let's tell it that on behalf of us so it",
      "offset": 5042.56,
      "duration": 5.119
    },
    {
      "lang": "en",
      "text": "can have more context again. And this is",
      "offset": 5045.199,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "why we can use a fshot example because",
      "offset": 5047.679,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "this example is a really really good",
      "offset": 5049.36,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "example for what we want. In this case,",
      "offset": 5050.88,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "they didn't put a call to action in the",
      "offset": 5053.44,
      "duration": 2.799
    },
    {
      "lang": "en",
      "text": "data model. The call to action is",
      "offset": 5054.639,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "probably dumb.\n Are you hitting context",
      "offset": 5056.239,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "limits on 40 mini? Maybe because it's so",
      "offset": 5058.88,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "long.",
      "offset": 5061.44,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "I'm not. There's no way.\n Okay. Okay.",
      "offset": 5062.96,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "Let's go do this. Let's make call to",
      "offset": 5067.52,
      "duration": 4.679
    },
    {
      "lang": "en",
      "text": "action optional.",
      "offset": 5068.88,
      "duration": 3.319
    },
    {
      "lang": "en",
      "text": "And it actually seems like it figured it",
      "offset": 5072.32,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "out. Um, we can go read this really",
      "offset": 5075.12,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "fast. So, this was about dynamic",
      "offset": 5076.88,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "strategies for effective prompting. Uh,",
      "offset": 5078.08,
      "duration": 6.639
    },
    {
      "lang": "en",
      "text": "this is weird. Um, and last time's video",
      "offset": 5081.28,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "title was actually we already had a",
      "offset": 5084.719,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "title. The title was called um cracking",
      "offset": 5086.08,
      "duration": 5.599
    },
    {
      "lang": "en",
      "text": "the prompting interview. So, before I go",
      "offset": 5089.44,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "add in parameter, let's say like let's",
      "offset": 5091.679,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "just like assume that the title that I",
      "offset": 5093.76,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "passed in I can just crack change this",
      "offset": 5095.36,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "in my test case.",
      "offset": 5097.36,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "uh summary.",
      "offset": 5100.32,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "Let's assume that I gave it the right",
      "offset": 5102.48,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "title.\n Oh, at the bottom.\n Yeah, your",
      "offset": 5104,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "title says Zoom meeting",
      "offset": 5106.32,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "cracking the prompting interview. So,",
      "offset": 5108.88,
      "duration": 6.52
    },
    {
      "lang": "en",
      "text": "I'll go run this",
      "offset": 5111.76,
      "duration": 3.64
    },
    {
      "lang": "en",
      "text": "new video release. Cracking the",
      "offset": 5119.44,
      "duration": 2.4
    },
    {
      "lang": "en",
      "text": "prompting interview. This is pretty",
      "offset": 5120.719,
      "duration": 2.801
    },
    {
      "lang": "en",
      "text": "good. It actually got rid of the Zoom",
      "offset": 5121.84,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "meeting. Gave a slightly better title.",
      "offset": 5123.52,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "It actually almost got the exact same",
      "offset": 5125.52,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "email that we actually really wanted.",
      "offset": 5127.6,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "Use indexes instead of relying on long",
      "offset": 5130.08,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "URLs. That's a great thing. Natural",
      "offset": 5131.84,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "output. Let it go in the most natural",
      "offset": 5133.28,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "format instead of constraining them to",
      "offset": 5134.96,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "JSON, which thing that we should do.",
      "offset": 5136.4,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "Really format the structure as you want.",
      "offset": 5138.8,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "Read the prompt. Balancing",
      "offset": 5140.719,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "effectiveness. That looks like spam. Uh",
      "offset": 5142.56,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "real world applications that looks like",
      "offset": 5146.159,
      "duration": 5.121
    },
    {
      "lang": "en",
      "text": "spam. Um, so what we can do now is we",
      "offset": 5148,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "can just like compare this to the actual",
      "offset": 5151.28,
      "duration": 5.439
    },
    {
      "lang": "en",
      "text": "thing that we sent out and we can go see",
      "offset": 5152.719,
      "duration": 7.361
    },
    {
      "lang": "en",
      "text": "what it feels like.",
      "offset": 5156.719,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "And this was the one that we wrote by",
      "offset": 5160.08,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "hand for this one. Um, there's a couple",
      "offset": 5161.679,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "of things that didn't have like these.",
      "offset": 5164.56,
      "duration": 2.56
    },
    {
      "lang": "en",
      "text": "One of the things that we probably like",
      "offset": 5165.92,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "is we like having code in the emails.",
      "offset": 5167.12,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "Code snip like small little code",
      "offset": 5169.12,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "snippets in back text in the emails. It",
      "offset": 5170.8,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "makes I think makes it much more",
      "offset": 5172.719,
      "duration": 2.881
    },
    {
      "lang": "en",
      "text": "authentic.",
      "offset": 5174.08,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "Didn't seem to capture that. So let's",
      "offset": 5175.6,
      "duration": 7.599
    },
    {
      "lang": "en",
      "text": "just tell it that we like that. Um",
      "offset": 5178.4,
      "duration": 6.72
    },
    {
      "lang": "en",
      "text": "uh where'd it go? We can tell this in",
      "offset": 5183.199,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "the draft. Um include",
      "offset": 5185.12,
      "duration": 6.079
    },
    {
      "lang": "en",
      "text": "code snippets",
      "offset": 5189.199,
      "duration": 6.161
    },
    {
      "lang": "en",
      "text": "include small code snippets in the",
      "offset": 5191.199,
      "duration": 7.121
    },
    {
      "lang": "en",
      "text": "emails.",
      "offset": 5195.36,
      "duration": 6.839
    },
    {
      "lang": "en",
      "text": "And now let's go run this.",
      "offset": 5198.32,
      "duration": 3.879
    },
    {
      "lang": "en",
      "text": "And let's see if it does something.",
      "offset": 5207.36,
      "duration": 3.96
    },
    {
      "lang": "en",
      "text": "And I might actually have a hard time",
      "offset": 5212.08,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "including code snippets because it's",
      "offset": 5213.679,
      "duration": 3.121
    },
    {
      "lang": "en",
      "text": "just from an audio snippet. So what I",
      "offset": 5214.88,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "might need to do is I might actually",
      "offset": 5216.8,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "need to feed in the video into this as",
      "offset": 5217.92,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "well. So I can actually have video as",
      "offset": 5219.84,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "context while it does this. So we should",
      "offset": 5221.44,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "get the YouTube URL for the YouTube",
      "offset": 5223.84,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "video and just pass into like a Gemini",
      "offset": 5225.36,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "model and go pass that in. And that",
      "offset": 5227.28,
      "duration": 2.399
    },
    {
      "lang": "en",
      "text": "would be like the next thing that I",
      "offset": 5228.719,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "would go iterate on along the way.",
      "offset": 5229.679,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "Are you guys understanding we iterate on",
      "offset": 5232.48,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "this pipeline and make it better to make",
      "offset": 5234.4,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "it really feel like what's good? What's",
      "offset": 5236.239,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "good here I want you all to notice is",
      "offset": 5238.159,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "Dexra and I already have done this work",
      "offset": 5239.679,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "a few times. So we have a golden target",
      "offset": 5241.36,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "in mind. If we're working on a data set",
      "offset": 5243.679,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "that neither of us understands, there's",
      "offset": 5246.08,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "no golden target and we'll be stuck and",
      "offset": 5247.84,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "we won't be able to get an answer really",
      "offset": 5249.679,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "well.",
      "offset": 5251.199,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "So we can iterate fast without an eval",
      "offset": 5252.719,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "set because we kind of understand the",
      "offset": 5254.88,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "domain of the problem.",
      "offset": 5256.4,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "Um",
      "offset": 5258.96,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "is there any",
      "offset": 5260.56,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "other questions along the way. Um",
      "offset": 5263.28,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "texture, anything else you want to",
      "offset": 5265.76,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "mention?",
      "offset": 5266.88,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "Um no, the only the other reason why we",
      "offset": 5269.84,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "can't send this out right away because",
      "offset": 5271.92,
      "duration": 2.799
    },
    {
      "lang": "en",
      "text": "Zoom takes like four hours to process",
      "offset": 5273.04,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "the video and create the transcript. So",
      "offset": 5274.719,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "that's uh to answer Seil's other",
      "offset": 5277.6,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "question, we will no matter how much AI",
      "offset": 5279.44,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "we have, we will never send the video",
      "offset": 5281.52,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "immediately after the call.\n Yes, because",
      "offset": 5283.199,
      "duration": 5.121
    },
    {
      "lang": "en",
      "text": "of Zoom. Um and we might be able to get",
      "offset": 5286.159,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "get around the one day turnaround as",
      "offset": 5288.32,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "well. But again, the whole point of this",
      "offset": 5289.92,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "is I am\n Don't you go present to the AI",
      "offset": 5291.36,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "team at Zoom? Don't you have like",
      "offset": 5295.36,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "influence over how they can't you make",
      "offset": 5297.04,
      "duration": 5.119
    },
    {
      "lang": "en",
      "text": "it faster?\n Yeah, let me ping them. Um,",
      "offset": 5298.56,
      "duration": 6.639
    },
    {
      "lang": "en",
      "text": "but I think the",
      "offset": 5302.159,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "um yeah, I think the takeaway here is",
      "offset": 5305.199,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "like build the tools that you need,",
      "offset": 5307.36,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "build the workflow first. Once you've",
      "offset": 5308.88,
      "duration": 3.279
    },
    {
      "lang": "en",
      "text": "built the workflow, add the AI part",
      "offset": 5310.56,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "because you can decouple that",
      "offset": 5312.159,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "development really fast. and whatever I",
      "offset": 5313.36,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "have to do to generate the email",
      "offset": 5315.84,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "address. My real suspicion to make this",
      "offset": 5316.96,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "email really good is actually I should",
      "offset": 5319.04,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "actually break down the structure of",
      "offset": 5321.199,
      "duration": 2.881
    },
    {
      "lang": "en",
      "text": "this email into like separate sections",
      "offset": 5322.48,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "that we have and write a two-step",
      "offset": 5324.08,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "pipeline where the first step generates",
      "offset": 5326.8,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "all the structure, then the second step",
      "offset": 5328.48,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "puts it into the like the language and",
      "offset": 5330,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "the floweriness of what the email needs.",
      "offset": 5333.28,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "And I think that will make it a lot a",
      "offset": 5336.08,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "lot better.",
      "offset": 5337.6,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "Um, but I think that's it. We went a lot",
      "offset": 5340.32,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "a little bit over than what we planned",
      "offset": 5342.96,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "on doing today. Um, but this was tons of",
      "offset": 5344.8,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "fun. I personally learned a lot building",
      "offset": 5347.679,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "this whole thing out. Way more than I",
      "offset": 5349.92,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "normally do. Um, because I learned from",
      "offset": 5351.679,
      "duration": 7.601
    },
    {
      "lang": "en",
      "text": "Draode. It was amazing. Um,\n you were you",
      "offset": 5355.36,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "were pretty good at Vive coding before",
      "offset": 5359.28,
      "duration": 5.439
    },
    {
      "lang": "en",
      "text": "uh when we got started.\n No, I'm I'm not.",
      "offset": 5361.28,
      "duration": 7.6
    },
    {
      "lang": "en",
      "text": "I think I do. My v coating is",
      "offset": 5364.719,
      "duration": 4.161
    }
  ],
  "cleanText": "And we are all right. Um, let's get to it. 10:05.\n\nUm, so welcome everyone to our next episode of AI that works. Our whole goal here is always the same. Write code that works and see if we can do some incredible things with AI that doesn't uh that doesn't depend on waiting for GBD 26. My name is Vive. I am one of the creators of BAML and my wonderful co-host is Dexter. I'll let him introduce himself.\n Uh, I am working on My name is Dexter. I build things. Uh, that's probably all you need to know. Uh, try to help people build very cool AI stuff. Um, whether it's agents, whether it's pipelines, whether it's fullstack applications. Uh, yeah, that's that's I'm not going to go deep into it. Um, real quick, uh, announcements, just updates. Um, for those of y'all, I'll just say it again one more time in the stream is like, so we're going to do these, we do these every Tuesday at 10 a.m. Um, and then we launch the I guess this is good context to lead into where we're going. Um, we publish the videos Fridays at 8 a.m. Um, and we will push up everything in this show will be in the github repo here. Uh, I'll put in the Zoom chat. So, all the notes from previous episodes. Everything will be live by Friday. Um, I have a short link, but this is just the github repo. Oh, that's not right. Sorry, hlyr.devitw.\nUm, that'll take you to all the previous sessions and you can watch the recordings. You can see the notes and the notes from this session will be there and any useful links and things like that and all of the code.\nYeah.\n And it takes a lot of work. The reason that we try and share all the code is honestly because there's a lot of people out there talking about how to go how you can theoretically build things. We just want to see the code. We're all engineers or eventually everyone will be an engineer. Today everyone needs Excel. Tomorrow everyone's gonna write code. It's just the way it is. We might as well share all the code and share what we can learn along the way. Um with that in mind, today's topic is one Dra and I have we're very very very very excited about. Um all this work that we do every week is really freaking annoying sometimes. This actual we love doing it. We love doing it. But it's I agree. And I think the part that's the most annoying is the manual part of like downloading the videos, uploading them to Youtube, getting the transcript, sending out the email afterwards. And sometimes you probably notice like four of our episodes didn't send an email out. That's because that's not what we love doing. We just like writing code. That's the thing that we love the most. So we thought, why don't we actually go put some of the practices that we talked about in the previous episodes into practice and build a pipeline that can go do something. And the thing that we thought was worth doing was really building the AI content pipeline. So what we're going to start off with today is we're going to screen share. We're going to show you a live demo of what works today, what doesn't work today. Then we're going to talk about the architecture diagram on a whiteboard. And then afterwards, we're going to go make the pipeline better because I think those are the steps that I find to be personally really interesting.\nSo with that, let's kick it off. Um, today's is going to be even more interactive than normal. So, if you have questions, please hop off mute. Just come and ask yourselves. That's why we do this over Zoom and not a podcast recording software or anything else because it's just way faster and people get to ask real questions along the way.\nUm, with that, let's get started.\nAnd I will say I am one of the least vibe coded people out there. I don't really vibe code. Um, but thanks to Dexter and actually uh credit to my cousin as well, I'm a big believer everyone should be vi coding uh a lot more than they think they should. If you were vioding before, you probably should v code more than you thought you were. And here's what we were able to do with this pipeline.\nSo, right over here, the first thing you'll notice is we are able to sync with Zoom and actually record all the previous recordings up here. So, we can actually see any of the previous recordings. You can see one right over here. This current one is literally going on right now. We started the recording and it pops in immediately. We can see our previous recordings and we can just import and process them. What happens when you import and process the recording? I'll just kick one off. Um is I'll just real quick call out. It says the title is still says cracking the prompting interview. I think that's like inherited from from BAML, but this is this current episode.\nUh oh.\n Or inherited from Luma or whatever from Yeah. When Luma created the Zoom, it left the title.\nExactly. So, we'll just start off this one. AI that works designing emails. We'll create the import.\n So, now you can see the video process. This is one we already did all the work in the email for, but um the idea is you going forward we would use this.\n So, while it's processing, I'll let it kick itself off and you can actually see what it's doing. When it when it's processing, it's actually going ahead and downloading the full video. And it's going to go ahead and eventually it'll get to a point where once the video is downloaded, it'll actually upload itself to uh Youtube automatically. It'll download the full transcript. Then it'll start producing key points and video summaries. And then after it's done, it'll actually produce like an email draft. I guess I don't have that here. This one didn't finish. Let me once it's done, it'll actually produce like an email draft of what to go send out. I'll produce like an exp post that we can go copy and paste and post out there and it'll produce like LinkedIn content as well. So, this is what we mean by multi- channelannel content. Download a 60-minute video. Uh, go for it. Go summarize it. Get the key topics and takeaways and then go draft things.\nNow, what you'll notice here is that this email does read like AI still. It's still not\n Yo, this email sucks, dude.\nExactly. So, we're gonna talk about how to make it. Did you even did you even look at the prompt?\n No, I didn't. I vip code the whole thing because the point what I really I think this is the point I want to stress today is before you can even work on your AI part of your pipeline, you're not even at the point where you can start working on that. What you need is infrastructure that you can actually iterate on. And if we hadn't built this whole system out that could do the glue code, building the AI part is completely useless because it doesn't really matter. I still need to build the rest of it. And the rest of it is actually critical to my iteration loop. And we talk about this in our eval video a lot where if you don't if you're evaling, you should spin up a quick little UI to go determine if your evals are correct or not. Go compare the differences. If you're building a pipeline, you should be doing the same thing. And right here, you can see the same thing. It actually is going ahead and let me see if the socket connection works. Yeah, there we go.\nIt's actually going to it's actually able to also stream out the outputs if I wanted to, which is really useful for me to iterate a little bit faster along the way.\nI'll pause really fast questions so far.\nCool. Let's keep on going.\n How long was it end to end? Did you say three hours or something to build this?\n I logged off after three hours. I don't know how late five was up last night.\nIt took a total of 6 pm to about 2:30 a.m. um combined with an hour and a half break in the middle.\nI pne you're new to the session.\n Pne pr you got to go watch the other episodes. We don't we don't do frameworks here.\nAnd the reason we don't do them is more often than not they just get in the way and they make it harder to go do things rather than easier. With that, let's talk about what this architecture diagram ends up looking like. Um, unless there's a couple more questions about this itself.\n Um, yeah, let's let's uh let's draw it out and then let's let's make the let's make the actual AI generated stuff not suck.\n I agree. All right, pull this down.\nOops. Why is my mouse not working?\nWe did record the whole thing. You can watch it at 2 to speed. We'll post it.\nWell, not the whole thing.\n Yeah, we're we're gonna post the recording. I uh Yeah, there's I got to we got to edit a couple things out, but yeah, you can you can watch the the beginning.\nAll right, so let's talk about what the architecture of this whole system is and how we actually did this. And we will probably try and record more of our coding sessions along the way. uh if people find it interesting, we can invite you to them and watch us code in real time as well uh along with just uh going to go do that. So there's actually there's actually three main parts to the system and I think it's it's useful to draw them out totally different systems.\nThis one is the database.\nThen we have the back end and then we have the front end.\nOkay. So another thing that we did is what we said is we will actually not allow the uh front end to really get data from the back end. It's not allowed to. It's allowed to issue requests to the back end but it can't get data from the back end. The back end is allowed to read and write from the database and the front end is allowed to read from the database.\nSo we get really really nice queries of how things are being built out. And the reason that this is so important is because if you're doing something like streaming or interactive UIs, building out the system to from your back end that communicates with your front end is a pain in the ass. And if you're doing it on a lot of different places all the time, this is what real-time databases were made for. Just use them and you can solve a lot of your pain and you might run into and so basically the the data API is just your database schema. The front end just needs to know the raw schema and it can query whatever you want to expose to it, right?\nYes. Exactly. So as long as you have like a view in your database that is accessible uh that is read readable and you have like some materialized view on it that is secure doesn't have anything like PII data or something along that or at least requires like authenticated users to access it then you can go ahead and go access this along the way. Um it does require some consideration of exactly how you do it. The easiest way to do it is a materialized view if you want to do it securely on your actual data. So you can go configure that in a good way.\nUm but once you can go do that now you're able to build a pipeline that is mostly just workflows in your back end that is issuing work at one after another. So for example and you kind of get you get rid of this like state machiny thing where the front end's like pushing data and then fetching it back out of the back end. It's kind of this unidirectional flow. And I think a lot of like this happened in React. We had React for like a year and a year and a half. And then people realized like the current backend paradigm in React, which at the time was things like Backbone.js and and things like that, just didn't work well for the amount of complexity people were using to add React into their apps. And all this stuff like Flux came out and there was explosion of like six different frameworks all came out that one summer of like how do you do this unidirectional data flow because it was what worked for real-time dynamic applications.\nYeah. And this is a pattern that we've seen happen a lot like we just find it way easier what you and this is similar to video games and all these other things that you might want to do uh where what you really want to go do is you have some what I would call like state of truth that is represented here and all you want to do is you want to render the truth as fast as possible to the front end and it's really easy to do that when you're able to go ahead and actually model the truth in a way that is representative in the way that the front end can use it and the challenge challenge then becomes building a schema that everything else can go reuse correctly. So you do have to plan that out a little bit, but I think it pays for itself. Um, personally in terms of like the developer ease and how easy this ends up being. This also does really nice componentization for both like paralyzing workflows in terms of how many people can work on it and not just people but agents. It makes it way easier for my uh front end to actually understand what it's rendering. If now the one part that we run into is in order to go do this, what you'll find is you have data models. Um,\nsorry, I'll draw another rectangle. You have some data models that you're saving into your database into here. And these data models both have to be used by Python and written from Python. And then they also have to be used and written by Type.\nHaving some way to keep these in sync is also really important. Otherwise, you end up again in a world of pain and that doesn't really work. And figuring out which side you're on is really, really important of how you go do that. But once you can nail that down, your iteration loop is going to be really fast. And adding new features, as we'll see today, when we add another feature such as chapter summary into our system, becomes a thing that you can just buy code trivially and you don't have to think about it anymore. uh because your agent kind of understands the architecture of what you're building out and it's so nicely componentized that it doesn't have room for making mistakes along the way. It's like an a you basically built in an API contract. If your front end was both reading and writing to the database, you now have a choice ever the agent now has to make an implicit decision of do I do this in the front end or do I do this in the back end and that's a choice and it might make one choice correctly, one choice wrong. But the more you go along, the more choices that will be made incorrectly just by\n\n\nVolume and by the nature of it happening automatically.\nBy doing this, we make a very clear direction of what Dexter is saying.\nIt's very easy what it does.\nThe front end's job is to render the content and issue new background tasks.\nThe backend job is to plum and churn on data and eventually write updates to the database.\nThe database's job is to send data out to the front end as soon as possible.\nAnd now it's very clear what we have to go represent along the way.\nUm, with that, let's talk about some of our routes.\nUm, I do have a very opinionated stance on what's a great data model agnostic library as we'll see today.\nUm, but we'll show that in a second.\nWe'll show the full code.\nUm, so what's the APIs that we actually have?\nWe have a couple APIs that we defined.\nUm, we have Dexter.\nDo you remember what they are by video?\nWe have one API to like uh submit a new request.\nUh did you push the latest code?\nI can pull it down and and pull that out real quick.\nYeah, let me do that.\nActually, I don't think I did.\nUm Oh, I might have.\nI guess I did.\nUh cool.\nI will get the API endpoints.\nTurns out there's uh AI is pretty good at generating a list of API endpoints.\nProvide feedback.\nUm and then we what we do is we submit a new request, provide feedback.\nUm get list of videos.\nSo there are some things that we do from the back end.\nUm get list of videos, provide feedback, and then what's the other stuff?\nI think there's one more.\nGet title.\nOh, there we go.\nI don't know why I didn't do that.\nOkay, so these are basically all the APIs that we have.\nUm, we did have to build one integration because we get actual Zoom API calls.\nWe don't want to do that in a real-time database and we want the back end to have access.\nSo this is something that the front end queries from directly.\nThis is a way to add feedback to a draft.\nSo, if you don't like an email, um what we're able to do is we can just go through and say like, &quot;Oh, um I can just leave feedback and say, eh, this is not long enough.&quot;\nDude, you got to make it more uh Jan alpha enough uh emojis.\nAnd I'll just refine the email.\nAnd what this will do is this will commit a task and eventually the email will get refined along the way.\nUh and I did not I think I don't know whether I don't know if you're planning on um getting this but I think it would be really helpful to kind of see the sequence diagram or the flowchart of like how how stuff flows through the system.\nYeah, I was going to get to that once I show all the routes.\nOkay, that's a good idea.\nSo and then we basically have all these routes.\nSo let's um let's look at what this actually ends up looking like in a sequence flow in that case.\nSo we have all these things.\nSo the first thing that we have is Oops.\nI can't draw it there.\nI will try and do the side by side.\nOh, I'm missing a couple end points.\nOne sec.\nOh, you are.\nOkay.\nThat's what I thought.\nIt looks like you don't\nYeah.\nYeah.\nThe first five are actually the most important.\nOh, did I lose this?\nOh, sorry.\nI have to adjust my Okay, there we go.\nUm, so what we have here looks like this.\nThe first step that happens is the back end submits a task to say like I want to process this specific meeting ID.\nUh, the front end submits the task to submit I want to process meeting ID.\nThe back end then gets all the submits a background task.\nSo it responds very very fast to the front end saying cool I got you.\nAnd then the background task will go ahead and kick off and then say something like the following where it will say something like um download download video plus transcript.\nUm finding the video is a little bit more complicated I presume because not all Zoom videos will only have one video say sometimes have multiple because people start and stop the recording and other things along the way.\nSo we have to go do that.\nThen we will kick off a couple jobs in parallel.\nOne of them is upload to YouTube and then simultaneously we also kick off the summarized uh summarized task.\nOnce the summarized task is done, we kick off three tasks in parallel again.\nUh, draft email, draft Twitter, draft X, and then draft um, LinkedIn.\nDoes the summarize task also output the like key points and stuff like that?\nExactly.\nThat that's what it output.\nIt outputs like a general summary of everything.\nUm, so the database updates from each of these is like basically like mark uploaded in DB.\nExactly.\nAnd then this is like add summary points to DB.\nExactly.\nAnd actually this one streams in.\nSo I actually like save the whole thing on every update of the tick of the stream.\nYep.\nUm, exactly.\nUm, and technically there's a special thing that I Oops.\nThere's one last thing that I do over here, which is like Whoops.\nI I mark I I mark the database as it's done summarizing.\nIt's important I can like mark the state at each single one of them.\nAnd that's that that's something else I had to do because once I was done some that's actually I I think that's actually worth maybe taking a sec to drill into is just kind of like having your kind of job system just be a table with a bunch of nullable columns and you can tell what's been done just by what's set in the database.\nAnd in that way, I mean, it's it's not as like robust as something like big and like chunky like temporal or something like that, but it is honestly probably good enough, especially if you're building like internal tools for yourself.\nUm, which is, uh, super super high leverage I think these days.\nUm, just that idea of like a really simple job system.\nThe problem that I ran into when I was doing purely nullable fields is that it wasn't I wasn't doing JSON parsing.\nUh, sorry.\nIt like once I was streaming, I had something in there when I wasn't done.\nSo, I actually needed to keep a status of it a little bit more.\nI see.\nBecause like I needed to know the status of like is it started, is it streaming, or is it done and the status was really important and having that available was important to actually show the UI there.\nUm, and then the other thing that I did was I got bored so I added one last thing.\nI added uh while I was at it, I was like, &quot;Okay, let's just why not?&quot;\nWe did these three things.\nSo, I just asked it to literally follow this example along the way.\nAnd I said, &quot;Draft a title for me after you're done.&quot;\nUh, so that became the next thing that we did.\nSo, now we had four pipelines that could trigger and process.\nOnce this was done, I have another thing that I could do, which was a user,\nDo Yeah.\nSorry.\nDo me a favor real quick.\nHit five.\nand just change the arrow to this one over here because those macaroni those macaroni errors arrows suck.\nI agree.\nOh, so you got Yeah, if you hit five twice, I think it cycles through.\nIt used to happen to me all the time.\nStill happens to me all the time.\nThank you.\nThis is why I'm really bad at Excel.\nUm, then we get another pretty [Â __Â ] good, dude.\nNow we have another request that the user can do once all these are done.\nAnd remember all this is streaming to the real-time database.\nSo it's very very easy for my front end to render this because it's just rendering a blob of JSON that is strongly typed and I know how to render it.\nSo it becomes trivial for me to go render.\nI just build react components for all those types.\nThen I built a second part of the pipeline which said like if the user will make like a draft ID content type and then their feedback.\nSo the user wants to commit feedback to one of these emails.\nSo each of these drafts has an ID.\nThey have a version history and everything attached to it.\nThere's different content types and medium.\nSo like email, X, and LinkedIn.\nAnd once this happens, we do the same exact thing that we were doing before.\nOops.\nWhere this time instead of our normal background, we kick off another background process.\nAnd this background process again responds very very fast to the OM saying okay cool I'll I'll take care of that for you.\nAnd once that is done, it goes to the real-time database, pulls out the draft, pulls out the transcript, uh, draft, transcript, and then issues that to a prompt, saves a new version, and then writes that to the database.\nAnd that's all this pipeline is.\nThere's nothing fancy about this and it's really just kicking off background workers constantly to go do work for me while I go and allow myself to iterate fast along the way.\nDoes this give everyone a good idea of what the background pipeline looks like and how we're able to go and iterate on this pipeline along the way?\nUh there's a question from Vijay.\nSo in the first instance when you created the draft, did you use the transcript or did you use the uh summary of the transcript?\nI just used I I I at the time I didn't actually think about using the transcript.\nSo I just used the transcript.\nWe can update that to go use the transcript as well.\nThis is why the summary.\nGot it.\nYeah, I only use the summary, not the transcript.\nSorry.\nUh this is why the these initial drafts are bad.\nThat's uh so like when we go look on this and we actually look at the UI on here.\nWhere'd it go?\nOops.\nFind the right screen.\nDon't know where it went.\nI don't know where Arc is.\nSorry.\nOkay.\nOkay, I guess it's here.\nUm, this is why the original draft was really bad.\nAnd like now that I told it to go and add more feedback with like emojis, you can see it added more emojis in this one, but the first one didn't have any of that.\nAnd like it's going to be bad.\nIt doesn't have any of this.\nIt writes your name.\nI have to fill this stuff out.\nIt's kind of annoying.\nUm, but that's because I have really given it no context.\nAnd the AI part of this is not even close to done.\nOkay, cool.\nSo, so to test this, so to test this, what we're going to do is we're going to change the prompt and then we're going to reimpport a whole video from scratch and then we're going to see if the drafts are better, right?\nWell, we can just regenerate the summary.\nBut yes, uh we don't actually have to\nNo, no, I was No, that was your leadin.\nYou're This is This is where we uh we go way down into the interloop and we say, &quot;Hey, okay, cool.\nLet's go get a real record from the database and then like write a test for it.&quot;\nYeah.\nLet's go do that.\nActually, that's the uh that's the old way.\nThat's how I would have done this a year ago is just kind of test the thing end to end, right?\nWell, I I think that's the point like let's show the end to end testing loop and let's show the faster loop once we're able to go do it.\nSo, in this case, does that answer your question, BJ?\nYes, it does.\nThank you.\nPerfect.\nYes.\nSo, now let's go ahead and actually go and try and make some of this pipeline better.\nNow, there's a couple questions asked about how do we keep all the state in sync and everything.\nSo, I'll just show you a little bit more of the code and what it ends up looking like because I think it is useful to have some dive through before we go and change a bunch of things.\nI'm also going to show how to go make this work with claw code and how to go edit it to add a new pip part of the pipeline because the problem that I actually noticed the most if we actually look at some of the prompts, they're just long.\nIt just like doesn't really talk about the real part of it.\nLike it adds a feedback.\nIt dumps a transcript in there, which is fine, but it's like it it doesn't really really really understand what I mean by actually making the prompt better uh and what makes a good email.\nIt's same thing with this.\nis like it doesn't really understand about what is a good Twitter thread when it does this along the way cuz I don't think this it's it's giving it instructions about what to do not so much about like what looks good like what good looks like exactly and like but we can't focus on what good looks like until we have the mechanics of what to do really plumbed in because I will spend a lot more time iterating on the infrastructure like I said this took about eight hours of work to go from nothing to a fully generating SAS flow that does this.\nUm, and that's worth doing.\nBut if I just built the AI part, I wouldn't even if I thought this was useful, I would Dexture and I wouldn't actually use this app.\nIt would be unusable.\nSo, we need to build a flow that made sense for us in a way.\nAnd then we can go and make the AI part better.\nOtherwise, you just have a bunch of Python scripts that you don't actually use and now they're trash.\nAnd that was a waste of time.\nSo building the whole thing out is worth it because then you can decide how to actually design the AI along the way.\nUm with that let's talk about some of this stuff.\nSo this data model called Twitter thread.\nThis is what we're rendering both in the back end and the front end.\nI'll show you how this gets used in the front end.\nUh and I'm just going to exclude all the val code.\nOops.\nUm all we do is we just regenerate types.\nAnd we generate types for both front end and backend types and now we just use them here.\nSo in this case we have a Twitter thread.\nThis gets imported and now my UI component just renders this.\nSo what's really nice about this is if I tell the model to go change what it means to be a Twitter thread, my front end will break at compile time and now it will go actually cloud code can go fix it for me.\nSo I actually get types that are in sync perfectly along the way rather than anything else.\nUm and the way that I'm able to do this is I just codegen both types.\nI cogen the Python versions that my back end uses and writes to the\n\n\nDatabase and then I co-gen the React\nversions that my front end uses and I\njust use the same types to basically\nissue the glue code along the way.\nSo I think Joe asked that question earlier.\nI use to go do that, and this is what I\nfound worked for me.\n\nUm, okay.\nSo, you have the back end.\nThe back end is getting the transcript, and\nthen it's passing it to methods in BAML\nin Python with using Pyantic.\nAnd then when it gets them back, it's storing\nthose to the database.\nAnd because it's all generated from the same BAML kind of\ncontract API specification, when those\nsame kind of JSON objects get served to\nthe front end, the front end can use the\ngenerated client uh that that they got\nmade from the exact same specification\nto render those things and have type\nsafety around them and know exactly what\nthey look like when they're being\nrendered.\n\nExactly.\nExactly.\nCool.\nSo, you get\nreally, really convenient type systems\nalong the way.\nUm, with that, let's\nfocus on actually making some of these\nprompts better.\nUm, so the first thing\nthat we'll notice is generate a custom\nTwitter thread.\nUm, one thing that I did\nnot do because I was voting, um,\nI found this helpful writing the UI much\nbetter.\nUm, the V0 UI is okay, but the\nproblem is when you're actually syncing\na back end and front end together, using\nVzero is a really, really, it's hard.\nUh, because what I need\n you have to find a\nway to well, you have to find a way to\nmake sure vzero fully understands the\nlike data contract and the models that\nare available.\nRight.\nExactly.\nOtherwise, it just changes things, and\nthen I'm stuck in a I'm iterating way\nslower.\nAnd it's fine if your entire app\nis purely built in Typescript.\nBut if\nyou have longunning workflows, it's not\ngoing to be built purely in like your\nNex.js app won't define everything.\nUm, and like that just doesn't work.\nSo\nthat's why I had to go down this road.\nAnd I think a lot of people are using\nlike non Typescript backends or they\nhave non-typescript back ends already\nthat they want to pull data from.\nAnd\nit's just useful to have one data\narchitecture that does this.\nYou can\nstill do it in v 0ero, but it's just the\nit doesn't have the context, and if it\nbreaks the types, then my whole app is\nbroken.\nThat sucks.\nUm,\nwith that, let's focus on specifically\none of it.\nUm, I think the thing that I\ndislike the most is actually the email\ndraft.\nAnd Dexter and I have a couple\nemails that we've sent out.\nWe like\nthose.\nSo, why don't we just do\nsomething silly\nand see if we can make the email draft\nbetter by doing the dumbest thing\npossible, which is first\n Oh, wow.\nIs the\nnever use prompting guy going to use fot\nprompting in this episode?\n I'm going to\nadd a transcript actually first.\n Okay.\nOkay.\nNow, this is not going to work.\nSo, what I'm going to do is I'm going to\ngo to cloud code, and I will tell it\num to um update\nthe Python code that calls\nemail draft to pass in the transcript.\nAnd this is I mean coming back to that\nidea of like VZ needing more context\nabout all your other systems to work\nwell.\nUm I think it becomes like your\njob as the prompt engineer to or\nactually we'll say context engineer\nwhich is like to give it a good prompt\nand then give it all the context that it\nneeds.\nUh and some of these like coding\nagents that run in the CLI like cloud\ncode or even cursor agent like they tend\nto be pretty good depending on the size\nof the project.\nUm it gets a little uh\nuh tricky harder.\nYou just have to be\nmore thoughtful if the project's huge.\nBut especially for small projects, they\ncan be very good at finding all the\nright context for you.\nSo that and kind\nof taking that job of like getting all\nthe right files and pasting them into\nthe right place.\nUm, take that off your\nplate basically.\nExactly.\nAnd now you can see it's\npassing the transcript for me\nautomatically.\nAnd in theory, uh, do you want to make\nthis?\nUh, sure.\nI don't want to ask\nagain.\nYOLO.\n I just gave it all.\n Let's\ngo.\nUm, and now that's actually going to go\nahead and go update the transcript here.\nUh, I haven't done a session on cloud\ncode and cursor, but we probably will at\nsome point.\nDexter was on this yesterday,\nand I was not about it, but cloud code\nis in my opinion much superior to\ncursor's agent.\nUm, a relative.\n Yeah, we\nwill we will do in the next couple weeks\na deep dive probably into vibe coding.\nIt'll it'll be a little bit different\nthan like the like, hey, let's go let's\ngo like really fine-tune, refine a bunch\nof pipelines, but I think it could be a\nfun episode.\nUm, what do I not like about fuchsia\nprompting?\nThe reason I think I\nspecifically say this so egregiously and\nso aggressively about fat prompting is a\nlot of people use freeot prompting as\nlike a crutch to try and get the model\nto do what they want without\nunderstanding that they're actually\ninjecting a lot of bias in the model.\nSo\nlet's say you're building a healthcare\ncompany, and you're using Fshot prompting\nto teach the model how to do like um um\nlike detect the right verbiage from a\ndoctor.\nThat could be good or it could\nbe really, really bad.\nLike let's say for\nexample you wanted to go tell that you\nwant to talk about like some sort of\nlike weird liver disease that a person\nhas, and that's the shot example you use.\nIt's a really rare disease.\nOne in like\na million people have it.\nSo it's really\nunlikely to impact anything.\nAnd then a\ncustomer comes in with the same exact\nname as the example in your prompt in\nyour future example,\nyou're gonna have a hard time not\ntelling telling model this is truly an\nexample and not an example that's\nrelated to that customer from a\ndifferent doctor.\nIt's just a hard\n it's\ngonna infer meaning from things that you\ndidn't intend for it to like include as\npart of the example like that people\nnamed you know people named Sarah are\nlikely to have a liver a rare liver\ncondition or something, but like when you\ngive it that example it's going to learn\nto a different doctor\n right even a\nsimple assumption just the person Sarah\nis going to a different is going to a\ndifferent doctor now, and it's an updated\nconversation you're giving me an example\nbut it's also an example about the same\nperson, and whether or not the model\nunderstands that or not doesn't really\nmatter.\n It's just more so just that like\nyou just have to know the biases you're\ninjecting.\nIn our case, we know Dexter\nand I are recording these videos.\nWe\nknow our tone is what we want to go\nembed.\nSo if we put a few shot example\nin here, it's just going to do what we\nwant exactly because we're not going I'm\nnot going to run this on any other\nvideos but the AI that works videos.\nNow\nwe do recordings for our team meeting.\nSometimes if I use the same prompt in\nhere, it will not generate a good email\nbecause I've fought example it for that\nspecific scenario.\nSo what I really\nreally like is this thing that I like to\ncall dynamic fshot prompting uh\nwhat is it workshop and that works.\nIt's\njust that most people don't do it.\nUm or\nlike partial fot prompting.\nSo like\ninstead of actually telling the model\nthat like oh in this case if I want the\nmodel to really understand that\nengineering is only people that actively\ncode.\nSo like a VP of engineering isn't\nengineering their product.\nI can write\nthat like this in my prompt where I\ndon't actually give the full output in\nmy prompt.\nI only tell it like oh\nbecause they don't code the category is\nproduct.\nI missed out on everything\nelse.\nI didn't add any other fields.\nSo\nI'm letting the model really understand\nthis is truly an example.\nIt's even if\nthere's another person named by Gupta\nthis even to a human would be clear that\nthis is an example without having to\nread too much into it.\ninfer the the\ndefault inference as an example and also\nyou can do more dynamic stuff like for\nexample dynamically change the name of\nthe if for whatever reason I know the\nperson has a name vibupa change this\nname\nto be different for that example right\nand that's just things that you can do\nto go make your fa prompts a lot better\nbecause because if your prompt uses vibb\ngupta and then you pass in a new thing\nand the person's name is also vibb gupta\nthen it might assume that it's the same\nthat follows that pattern when really\nyou'd basically want to say like okay\nwhatever the person's name we're testing\non make sure the name in the fshot\nexample is different before we launch\nthe prompt.\nExactly.\nSo you want to kind\nof find an you want to make it I think\nthe idea of fshot prompting thing is you\nwant to guarantee the model thinks of\nthe thing you're providing as an example\nand that is a\n cool tangent.\nSorry uh let's uh let's test this thing.\nSo, let's add the trans.\nWe added the\ntranscript part in there.\nUm, and like\nnow I'll read summarize.\n So, the only\nthing we changed.\nSo, the only thing we\nchanged was we're using the exact same\nprompt.\nWe're just also passing in the\ntranscript.\n Exactly.\nUm, okay.\n And this\nis going to be really annoying because\nit's going to it just takes a while to\ngo run.\nAnd while we're doing that, I'm\nactually going to go add something, and\nit's streaming this out.\nSo, we'll see\nif this is better or worse.\n So, sorry.\nDid you just re-trigger all the\nprocessing for all the drafts?\nBasically, resum\n just for this one.\nYeah.\nYeah, exactly.\nFor all the drafts\nand the summary.\nSo now it's going to\ncost me.\nYeah.\nGo ahead.\n So when you when you do\nreummarize, it's going to repull out the\nbullet points for takeaways and key\ntopics, and then it's going to go because\nthe upstream thing changed, it's going\nto go regenerate all the drafts.\nExactly.\nUm that's what I have kind of defined in\nhere.\nUm and it just takes a while.\nI\ndon't know why the malls are so slow\nthis morning.\nUh but they are.\nNow, as\nyou can see, my iteration loop here is\nkind of hosed.\nIt's very, very, very\nslow.\nSo, I'm going to try and make this\niteration loop a little bit faster by\nwriting some test cases.\nNow, the\nproblem is writing test cases is\nactually kind of annoying because I\ndon't actually want to do this.\nBut\nthese prompts are better.\nI actually\nthink these key points are better than\nthe previous one.\nLike, it actually got\none of the key things we talked about\nlast time, which is RTFP, read the FN\nprompt.\nUm,\n sorry.\nDid you\nDid you\nchange the summarization as well?\nI\nthought I thought we left the\nsummarization itself unchanged.\nUm, oh, sorry.\nWhenever I do re\nsummarize, I actually regenerate the\nwhole summary as well, not just the\nOkay, so this is just a second a second\npass over the same data, right?\nYou\ndidn't change the inputs to the summary.\nExactly.\nSo, I actually I just changed\nthe email draft one, but I also ch I did\nthe resummarize button along the way.\nYeah.\nOkay.\nBut but you're talking about\nhey these the fact that these points are\nbetter in this case is just at this this\ntime the the model happened to pick\nbetter summary points.\n Okay.\n Yes.\nSo\nonce this email is done take it out.\nAnd\nthis is why I added the re summarize\nbutton because like sometimes the\nsummary is just off\nand I don't really want to think too\nhard about it.\nUh so like the easiest\nway\n you just want to try\n I just want to\ntry again.\nSo I wanted a way to\nre-trigger the pipeline\n and you could do\nthis actually in because we wrote all\nthe code and we have full control over\nthe pipeline.\nYou could actually do this\nin the code and say like hey just run it\nthree times and show three summaries and\nI'll just pick one and then go do the\nrest of the pipeline.\nLike you could you\ncould implement that pretty easily.\n Yes.\nLike cloud code could probably build\nthat for you in five minutes.\nUh and\nthen the email summary at some point\nwill finish up.\nAll the content is generated.\nSo I'm\nhave to refresh.\nThis is the sinking\npoint that I did not get perfect.\nDid this work?\nUm did not.\nLet me try rerunning.\nI think OpenAI for some reason has been\nnot working very well.\nOh,\nI have an error here.\nThis\n Oh, Claude didn't quite nail it.\n It\ndidn't nail it.\nIt's fine.\nUm, but while\nit's doing that, I'm going to do\nsomething.\nI'm going to add some\nobservability to my pipeline so I can\nactually iterate on it a little bit\nfaster because the promp thing is going\nto be so freaking slow otherwise.\nUm yeah, I generated\num I'm just going to dump thing.\nUm\npass new project.\nUh I guess it doesn't really matter if\nyou have API keys.\nYou can just send\ndata here\nwith X.\nAI that works is built on trust, folks.\nIndeed it is.\nI will stop screen sharing\nthat while I edit my end file.\nWe have some trust but not infinite\ntrust.\nYeah.\nOkay.\nSo what you're doing is adding the\nAPI keys so that every every AI call is\nbasically going to be tracked and so we\ncan use\n Yeah.\nAnd I really what I really\nthe main thing I really want to be\nhonest is I just want to go see the logs\nso that when it goes ahead I can go\nahead and just like turn it into a\num\n yeah turn it into a test case.\nAnd basically without that what you\ncould do is I guess you could like\nhandcraft a test case by like going and\nlooking in the database and like\nassembling the inputs yourself and then\nlike writing a test case by hand.\n Yeah.\nBut I don't really want to do that.\nI\njust want to use like the real test case\nalong the way.\nUm there we go.\nThis should I think be done now.\nIt\nshould have made some diffs along the\nway.\nUm,\noh yeah, we forgot to pass in\ntranscript.\nGenerate email draft.\nOkay,\ncool.\nOkay, now what we should be able to do,\nI also made this other reset processing\nbug because like sometimes this button\ngets grayed out like sometimes it's in a\nbad state like it is right now because I\nrestarted my server.\nSo, this just\nallows me to restart processing really\nreally fast.\nAnd now in theory,\noh, it just resets the state.\nIn theory,\nthis will work beautifully, but we'll\nsee.\nUm,\n starting BAML similarization\nfor video.\n There we go.\n Here we go.\n So,\nthe summarization is happening.\nUm, so\nthe real thing that I really want to do\nnow is what I want to do is I want to\nfocus on two parts.\nI want to make the\nsummary as good as possible, and I want\nto make the email as good as possible.\nThe only way I can really make the email\n\n\nVery good is by being a really fast iteration loop on actually iterating on the prop.\nNow, iterating on the UI layer is just too slow.\nI can't do that.\nI don't want to build state management record replay on every single aspect of this.\nIt's just going to be too slow.\nSo, I'm just going to go iterate on just the email.\nNow, on the summary, you can apply the same concepts to the Twitter and the LinkedIn and all the other stuff.\nAnd the reason that we're not, we should also take ahead.\nSorry.\nGo ahead.\nI was, oh, go ahead.\nSorry.\nGo ahead, sir.\nWe got a little bit of lag.\nI just want to know like we should take some time at the end just to talk about like, hey, what would be next for this thing?\nAnd like I imagine we keep working on this thing and like what other things do we want this to do in addition to email and Twitter?\nHow can we make it a little more backgroundy?\nHow can we catch web hooks instead of like having to come here and click on stuff?\nThere's a whole like next generation of this thing.\nOh yeah, 100%.\nUm, Vid's got a question really fast.\nSo, we are currently building the end to end pipeline to test this, but are you also going to do the peacewise uh test as well?\nI'm literally about to do that right now.\nNow, what I want to do for the peacewise testing is rather than running the peacewise testing on my own.\nOh, you got another you got another error.\nOkay, let me find this cloud code isn't doing this right.\nSo, I will find this myself and just like fix it.\nUh, command C, command B.\nBack end.\nHide my terminal.\nLet me turn on my Python script mode really fast and then it'll yell at me.\nWhy is he on me?\nOh, is this not a waitable?\nThere we go.\nThat's why it wasn't working.\nI wasn't calling the right code.\nThat's good to know.\nAsync.\nWhere's the other one that I changed?\nUm, sorry about watching me live code.\nUm, I was hoping to have most of the stuff done.\nOkay, I think it should be good now.\nBut while this is done, let's talk about what the actual thing looks like.\nSo I should be able to actually see the actual test case and I should be able to go and edit it.\nThere you go.\nSo yeah, maybe we just do not the email one.\nWe do a different one.\nExactly.\nSo we can just say like the summarize for example.\nLet's just make sure the summary is actually good.\nSo let's just make this a test case.\nLet's work on this.\nAnd now we can go to.\nSo that was really fast.\nUh so what you did was you took a recording of an input output pair for one of these calls and generated a test case so that you can go change the prompt and see how it affects the output basically.\nSo what I'm finding now is I have the transcript I can go play around with this like fundamentally what I really need is I need a way to actually go see if this is actually going to work or not.\nUm, how do I hide without having to go run the entire pipeline end to end exactly?\nSo now what I'm able to do is I can go see the pipeline, see what it's doing, and then I can just go like firstly I can just see what the model did.\nIt's not like the model's key takeaways are just smaller than I what I want them to be.\nI want them to be like paragraphs.\nSo let's just change that a little bit.\nLet's add like a Let's see what the model actually does.\nIt doesn't seem like the model is actually listening to me.\nIt feels like what it's actually doing is jumping it out even though I told it to give me an outline of a very very dense summary um in the system prompt.\nSo let's try a different model.\nUh let's just swap out to like enthropic.\nUm I think I have a custom sonnet and see if sonnet will work along the way.\nAnd so basically you want it before it starts dumping out JSON, you want it to kind of like outline its thoughts.\nExactly.\nOutline a dead summary of the video.\nUh then answer then fill out the summary.\nUm so example.\nLet's do that.\nSure.\nUm, schema.\nSure.\nLet's just try that.\nMaybe my prompt was bad.\nSo, let's just iterate and see if I can get the model to spit this out.\nIt's really wild how slow the models are.\nI feel like I'm being rate limited.\nAnd this is yeah this is the um this is that like we covered this in depth in the reasoning topic right of like how can you get a model to kind of like think about it before it starts generating the data that our program is actually going to use.\nExactly.\nSo now it actually is generating a pretty good depth response of what it wanted and then it's outputting the summary and I think we'll see if this is actually better or not.\nI'll try with open AI again just to see what it does.\nOpen IP portal and you can see how fast I'm iterating here.\nAnd what I'm really trying to do here is I'm trying to figure out is it doing what I want.\nAh, that looks actually better than other things.\nUm, and I can even guide it.\nPar one, um, par two, par three.\nI can even make it write multiple paragraphs by telling it what I want it to go do because it this this doesn't seem very dense.\nSo, let's let it go do more along the way.\nAh, this looks way it looks like it's better on the right track because now I'm telling it to do multiple paragraphs.\nSo, let's see if it actually understood it.\nIt still really did do multiple paragraphs.\nUm, so let's like fill this out.\nUm, topic par um topic one para Topic one.\nSee, topic two parag since the videos are 60 plus minutes long.\nTry and have time ranges.\nTime range.\nCool.\nIt'll get the gist of what I want.\nSo, I'm trying to force it to basically reason more along the way and reason the way that I really want it to go do.\nNope, that didn't work at all.\nUm, I'll rerun this one more time.\nWent fast.\nAnd I think the other thing that I want to do is I want to see if I can get it to actually listen to me by making the temp.\nI had originally deliberately tried to keep the temperature uh higher for this kind of task.\nBut I'm going to make it temperature zero now and try this out one more time.\nNice.\nOh, this is sick.\nOkay.\nAnd then what are we going to go update the structured outputs to create timestamps or like chapter summaries?\nWe might not even need that.\nI just wanted to go have this because I wanted to use this as the summary first before it actually goes ahead and fills out main takeaways and everything else here, but we could update the structured output to also encode encode time uh time things if you think it's useful.\nSo let's go do that.\nLet's change main takeaways to include time data, right?\nUm uh let's add another field.\nUm main takeaways uh timed data time data uh summary string array.\nWell, the summary can be can just be one string.\nExactly.\nOkay.\nAnd you can actually see what I was able to do.\nIt took me a little bit of work to go get this.\nAnd now the model isn't actually going to go do this because it's like I can just reason the data model.\nSo it's using the flexibility to go do this on its own without really having to think too hard about this.\nUm, but and I'm my question, I guess, and I think the answer is probably just vibes, but like how are you deciding whether like what's what's the real trade-off between having it do this in the preamble versus having it do it in the data model?\nWhat's the real trade-off?\nIt's a couple of things.\nUh, when it does in the preamble, it's not polluting the data model yet.\nSo, it allows it to like just like kind of have like scratch work when I'm actually doing work along the way to go produce some results.\nAnd that's the real benefit.\nUm, what I find is in general, Enthropic listens to instructions a little bit better for this kind of stuff than OpenAI does.\nSo like I'll try to run this again with like Enthropic and it should start sending the data.\nNow we're on here.\nI think we should have gone another one for email.\nWhere's the email stuff?\nUh oh, the email one didn't run because I re I think you probably still have an error.\nYeah.\nUm, so now we're actually able to go do this.\nIt's generating time codes, generating a little bit more takeaways, but it still seems a little very one-dimensional compared to this, which is okay.\nI might actually be okay with like a onelevel uh summarization here.\nIt's not it might not be a bad thing, but I think I got I will get slightly better key topics and bullet points along the way to go do this.\nNow, I may not want to call this bullet points.\nIt's like let's call this like aliases like takeaways.\nTakeaways, right?\nDescription.\nAction items.\nAction items.\nUh listeners can action items listeners uh can do to improve their skills.\nAnd you can see what this does to the prompt.\nIt changes it to instead of bullet points, we now have a field called takeaways.\nIt's an action item listeners can do to improve their prompt, which I think in general is going to be very, very nice.\nSo, let's go run this.\nI'm also noticing that the the time data from anthropic is uh looks kind of hallucinated.\nIt's just doing 0 to 15, 15 to 30, 30 to 45.\nThat's true.\nWhich is probably not probably not good enough.\nYeah, I agree.\nUm, let's see if it did it this way.\nYeah, it really just broke it up to almost like 4 hour chunks.\nAnd it's probably I bet what it's doing is biasing up to 60 minutes.\nVideos are pretty long.\nTry have time synced to the transcript.\nAnd this is again the thing we talk about all the time is like a tight iteration iteration loop of like if you were doing this end to end testing or even with piest without like dedicated tools, I think this probably take longer to kind of fiddle with.\nExactly.\nIt sounds like it's still using those.\nSo like it this might just be like a thing Enthropic is going to do unless I specifically break down the transcript into like a shorter chunk.\nSo we can just try that.\nOh, you want to try a separate method that chunks the transcript.\nLike let's just like I don't know.\nIt's like 45 minutes.\nWell, I'm just going to do like the dumb thing and just like command X and just run that uh and just see what it does.\nIt sounds like a It looks like a little bit better uh along the way uh but not too much.\nSo like I just leave it for now and like we'll come back to this in a second.\nBut I think the summarizer looks significantly better at this point.\num in terms of what we're going to go get.\nSo now what we'll do is update my UI is show the new Yeah, exactly.\nIt's kind of like if you wanted to take do a pre-processing step on the transcript and really focus on splitting it up into time chunks and maybe like chunk it deterministically and then pass those chunks into models.\nYou could do that.\nUpdate my UI to show the new time data in the um in summary.\nUm, cool.\nLet's go do this.\nOne thing I like to do when I go do all this stuff is like this stuff is done.\nSo, I'm I'm just going to stage this and then I'm going to go tell it to go do this.\n[Music]\nAnd now it should go be able to go find this.\nAnd the time data should pop up in there.\nSo, I tried to guide it a little bit along the way.\nUm, and it should hopefully be able to go ahead and see if he can find it.\nOh, oh, it's too big for Claude to read it.\nYeah, because of this test case.\nSo, what I will do, yeah, I'll help it and split up the test case into a summarize test.baml, which makes sense like we will have a bunch of issues with long-term tests.\nContinue.\nI made the file smaller.\nUm, so now that I can just put my test in here, now this file should be actually readable.\nThat's actually one good thing for me to note in general from now on because transcripts are going to be long.\nI should keep all my tests in different files.\nIn fact, I probably want each test to be its own file because I know transcripts will be long.\nUm otherwise like my AI code gen stuff will not be able to do it very well.\nYeah.\nAnd that's like another thing that I think um would be cool for a vibe coding episode is like how do you make your code base um easy for AI to use?\nAnd there's actually I think a really important balance between like if you have lots of little files like really small files then you're going to end up with like the AI has to remember to read every single file and the prompting and the tuning is going to set it to be a little bit kind of like balanced in how much it reads.\nBut if your files are too long, it's also trained to be a little bit um kind of uh thrifty with how much of a file it reads, right?\nThe models, the the agents generally try to read like, okay, if I can find the right 100 lines, I'm just going to read those.\nAnd that can be an issue as well.\nAnd I think we found a lot of success in um what we're doing is like and you'll is is in prompting the model to like make sure it always reads a,000 or 1500.\nIf it's a long file, read the whole file because otherwise it ends up like I I'm sure we've seen this uh in vibe coding where it's like reimplements a method that already existed somewhere else just because it didn't read enough of the file to see that that function was already there.\nExactly.\nSo now I think it should be almost done.\nUm and it uh it regen the code which it didn't have to but it did.\nThat's okay.\nUm and now it should be able to update the UI as well.\nUm, this is the only part about VIP.\nIt's kind of annoying.\nI don't I can't tell all the stuff.\nI think it's all done.\nCool.\nLet's go back to the UI and see what it does.\nSo, now that we have this, now we can just resummize.\nOh, it fail.\nUm, oh, I don't have my API key for entropic.\nGive me one second.\nLet me buy that in really fast.\nIt would be quite funny if\n\n\nUm, I did not realize, but I was secretly just sharing all my API keys anyway because I had the screen up, but I think I have my other screen.\nUm, and API key.\nOkay.\nNobody back in again.\nAnd now I'll try some more time.\nResummarize, it is processing, and once it's processing, we will be able to go see everything soon.\nUm, while we're going to go do this, um, why don't we use frameworks here?\nUm, I want to show you the code that we're actually using to call AI models, just to tell you an idea of why we don't use frameworks.\nUm, I actually hate this bud code.\nThe fact that it consumes an editor slot is really annoying.\nUm, but when I go down into this, all I'm really doing here is this.\nI'm just calling a method that calls summarize video.\nThe summarize video is just a function that I've already defined that takes in a transcript, takes out a title, and spits out a data model.\nThe fact that this uses an LLM doesn't really matter, and I don't really need a framework to go deal with this.\nUm, in order to orchestrate it, all I do at this point is once the summary is done, I feed the summary into these three things, and I just async.io.gather it.\nSo, like the benefit that I'm getting from a framework just isn't worth the debugging headache that I would have to deal with.\nAll I really want is I just want to have functions after one after another.\nAnd this is one of like the points I think we make a lot in um in 12 factor agents is basically the idea that like code is already a directed graph.\nAnd if you really like thinking in graphs, you can use frameworks that give you kind of this like nodes and edge edges structure.\nBut what you're looking at, can you go back to that real quick?\nWhat you're looking at on the screen is a DAG.\nIt's like do this thing and then fan it out and do these things and Okay, can you can you go back to the code real quick?\nYou do this thing and then um you you do this thing and then you fan it out to these three things, and then when they're all done, then you go to another step.\nLike code is already a way of expressing directed graphs.\nUm, and if you are able to write code and do the basics and get maybe get help from a model and writing the code itself, then um you should leave yourself open to whatever approaches and structures and like now because we control all this code.\nIf we wanted to like add another step here that is pre-summary and then pass the output of that in in addition to our summary here, we have the flexibility and the freedom to kind of architect this however we want and by doing weird custom stuff.\nThat's kind of how you find the boundary of what the model is capable of and kind of push the limits of what is possible with AI.\nUm, if you're just kind of like using a generic approach, you're going to be your quality is going to be capped at, you know, whatever everyone else using the generic approach is able to do versus if you open the box and start tinkering with the wires, you may be able to get better results.\nYeah.\nExactly.\nAnd then the other really thing important thing to note is like most frameworks don't actually give you like full fundamental control of the full full full prompt.\nThey add some preamles, they modify how your tools are being presented because part of standardizing things like passing between open anthropic is standardizing something.\nAnd while it's true that you may want to go do that, the problem is the fundamental thing that you're passing into the model is these tokens.\nLike these token oh, it's going to take that's going to take forever to render.\nI [Â __Â ] up.\nUm, uh, running a model on tokenizer in a browser is not a good call on very, very long text, but at some point you're going to call the model in some way, and you're going to want a model to see those exact tokens.\nYour quality is strictly bound by the tokens you send in and out of the model.\nThe only thing that configures what the model spits out is the tokens in the tokens out.\nSo if you can't control every token out there, you can't get it.\nSo I think we've had some people, for example, that live from like other countries, and like if they if they use a library and they build a pipeline that's supposed to speak, let's say Japanese or like um Persian or some uh or any other language along the way, like Hindi, anything, if the library injects a word of English, the model's going to speak in English, um because it thinks it's allowed to do that, and it's really important that if you have control, you can actually have that access along the full way.\nHopefully that answered the question, VJ.\nUm, and now I think I canceled this again.\nOkay, third time to What do we want to get through before we kind of wrap for today and go to questions?\nUm, I think we can go to questions.\nI'm just going to go I showed kind of how to make the summary summary slightly better by going to go down this road.\nI'm I the last thing I was going to do was actually feed in the email and the transcript to the email to make it slightly better as well.\nSo, I'll try and get that wired up, and if I don't get it wired up on the call, I'll do it afterwards and go send out the content as well so people can poke around with the prompt.\nBut you saw the iteration loop of what I do.\nI take the prompt once I have some data.\nI literally just go ahead and like make a test case from real data because I need to run the pipeline once.\nI don't want to have fake data.\nFake data is the worst thing I can do because if I use AI synthetic data, it's it's going to work and then it won't work for my real problem.\nSo, I'll just run the pipeline once, pay the cost, download the test case, and then I'll just iterate on the test case alone until I'm satisfied, then have the AI model go fix all the downstream pipelines along the way.\nYeah.\nAnd this is kind of what we did on the Evals episode, too, right?\nWhich is like push real data, push a little bit of real data through the pipeline and then go look at it by hand and find, okay, this would make a good test case or this is a good example of something the model's not good at.\nAnd like part of baking golden files is about like, oh, this is a thing it did right and I like everything about this.\nLet's cement that as a test case.\nSo as I change things, I know that that one will continue to work.\nAnd then also, hey, let me go find a thing that doesn't work and let me go take that down and put it on the workbench and tinker with the prompt and iterate on it.\nExactly.\nAnd I think if I go look at the new summaries, like we've generated quite a few summaries at this point.\nI think this is the most recent one.\nWe're literally generating time codes and everything else.\nSo the fact that the UI isn't rendering it means that cloud code just messed up in the UI and didn't render this correctly.\nSo I just need to go debug that and figure this out.\nBut it's really easy for me to like cool the time codes look good and this is this might be good enough for the next step of the pipeline.\nSo it allows me to just debug really, really quickly to see what's actually happening under the hood.\nUm, with that, let's break for questions really fast, and if we don't have any, we'll just keep by coding for like the next 10 minutes or so.\nUm, what do you think about tools like the Verscell AI SDK that still give you all that flexibility to do whatever you want, but also give you tons of helpful stuff for streaming and tools and UI and React integration, other helpers and utilities?\nWhat do I personally think about the Versell AI SDK?\nUm, I think the biggest problem that I have with the Verscell AI SDK is that one, it's too deeply tied to a lot of native Versel stuff, and two, I don't actually think it captures the nuance of what streaming actually is.\nUm, a lot of people think of streaming for structured data to be trivial.\nU but it's actually a really, really, really hard problem.\nAnd I'll show you why it's hard.\nUm, and why of why it's hard, I I wrote the tool streaming for VLM\nSo I like I I'm familiar with I've actually contributed to the streaming in the AI SDK too.\nUm I guess like I I've used it on projects where I've never used Versol at all.\nLike it doesn't force you into Versell's ecosystem.\nIt's actually like totally independent of their platform.\nYeah.\nYeah.\nSo I was just curious like because it gives you lots of great stuff for like generating like if you're doing like tool calls or structure generation it lets you stream that object to the you know your react app or whatever and get a hook so that you can see okay well this is this field has been generated and now this field has been generated and so you can like make your user interface interactive as um you know like for example if you're generating a complicated object you can make your time to interactivity and time to user feedback faster.\nUm, but I like it's the one the one one thing it does force you into is using Zod.\nLike it's very opinionated about using ZOD for schema definition, which is fine.\nZod's a great library, but like I love all the great things that BAML gives you for that.\nSo I guess like my question is like more along the lines of like is that do you ever see there being like some type of integration there of using like with actually have a BAML integration with Versell?\nUm, all you have to do is like so BMAL under the hood.\nI mean nextJS right\n yeah we have a function like summarize video so what you can do in your VS in your code directly let's say you wanted to do this all in your front end code I didn't actually do this here but I could um we also do the thing for you where we just give you a hook that you can just use import um\nuh let me do this from atlantreact hooks So we have a function here called summarize video.\nSo you got to use summarize video hook and then what you can do is const\nequals use summarize video and then you can have like a data as a summary or you can have a mutate function and all the other state associated with it and you can just go call this with whatever data you want.\nThe summary is guaranteed to be a version of this type along the way.\nSo you can do like and the and the partial type is the thing that has like based on your BAML code of like what's what's nullable in a stream basically.\nRight.\nExactly.\nSo you call like mutate um and when you call mutate you get like a stream object along this um along the way but the data object just updates along the way.\nSo data become okay.\nI didn't know you had that.\nSo, so you and and like when you do summary dot you get like bullet points and you notice that all these things get like optionalized along the way.\nBut what I could do is I could say even during streaming I want to guarantee that video summary key point is going to be I actually want to guarantee that hey when I go send the video summary I want key points to individually be stream.done the the key points field has to be done.\nSo all right I so that would mean that like this the hook would not start streaming data until the key points were all processed and then you would start getting the partial object for all the rest of the fields until main takeaways is done.\nSo now main takeaways becomes guaranteed to be a string array versus if you're trivially streaming you now have to go do this and now it becomes like a very everything is now everything everything is nullable versus hey I guarantee that time data and main takeaways will always be true before I hand it to my renderer.\nwant to stream just this array.\nEach string should be individually streamed.\nAnd now you get a slightly different type.\nYou get an array.\nOh, whoops.\nThis will render in a second.\nYou get an array of like this should actually work.\nI'm surprised that this doesn't work.\nMaybe the code gen is wrong for this.\nLet me check one more time.\nPoint.\nWe're actually in the middle like redefining our types.\nSome stuff works with Go, which requires perfectly strict types and doesn't allow for any flexibility.\nSo, we're just finding a couple bugs in there, but the idea is like you you when you're streaming, you actually live in a duality of type systems.\nAnd that is the part that people really miss out on.\nYou really have two type trades you have to navigate, which is really, really complicated.\nSo, having utilities like you can't actually use TypeScript's partial type to go do that.\nSo then when you go ahead and actually build something out, you want to go build like really interactive component.\nDoing this should be trivial.\nwhere I built a React component that analyzes and does this, but you can't really do that because this is working off of two type trees and that is a hard part and making that easy I think is something else AICK doesn't actually do when you get to real complicated AI real complicated JSON structures because if you have recursive types you have nested objects you can't define that well in ZOD you really need two ZOD schemas to do that correctly.\nOkay, thanks.\nDamn.\nUm, that was uh that was that was uh welcome.\nThat's that's why we call it advanced AI engineering, folks.\nThat was sick.\nUm, there's another question.\nUh, you want to talk about the email optimization?\nYeah, I want to figure out why that's not working, but like what we can do here is like we can just make this test case really fast.\nI think Abby mind you had the question.\nDo you want to summarize this question for me, Dex, while I get this working?\nYeah.\nNo, I don't question like uh when he mentioned about the AI SDK, I felt that we uh while we were stripping bolt new which uses AI SDK uh we kind of uh touched upon that part of the uh the ecosystem and I personally found out that there's no way to control what uh the LLM produces out when you use Versal SDK versus when you use BAML.\nSo um with bold new the problem always existed that the model the LLM could generate anything and versal there's no way to control it.\nUm so I was just structured object generation.\nWhat you mean like structured object generation like generating?\nYeah, it totally has that.\nIt like generate object and stream object.\nYou just give it a ZOD schema and tell it generate an object.\nNo, but like how do you evaluate the output of like uh imagine you have you are gener generating a prompt.\nHow do you uh\n\n\nHow do you make sure that the response is sort of consistent? As in, like, um, what we did was basically tried to give examples of uh prompt engineering to the LLM itself while using the SD AI SDK. I didn't see a way of uh the SDK controlling the response generated from the LLM in a way that you can do with BAML, like if you if you get what I mean.\nYeah.\nThe biggest, I think, a lot of people ask, like, why can't you just use structure generation? I think that's what Kyle's question here is, like, why doesn't that just work? The reason that structure generation really doesn't work fundamentally is that, like, for example, check out this email. This email that we have is literally outputting JSON with escape characters with new lines that itself is going to lead to, like, a bias in the model. So what I really want to do is I want to somehow tell this draft to say at description, use triple quote strings.\nU for multi-line strings.\nAnd now when I go run this, it doesn't generate escape characters anymore. The email will just be better because instead of sampling for the best token that is parsible, I allow the model to output the best token actually thinks is the best. And that makes a huge difference in quality for the model because when you're not doing this, Dexter had a great, great visual on this last time, which was basically saying something like this, which is when the model goes and generates a, this is the token it wants, it wants to generate a new line character, and then there are other characters that wants to go generate, like, oh, a backslash or a backslash n or something else along the way. When you go do this, when you do structure generation, you basically say you are not allowed to generate the new line character because you're inside of a quotation mark. That's not valid JSON, and you invalidate those tokens. We did this a lot in computer vision in the past, last 10 years, where you like tried to outsmart the model.\nIt turns out, just like, let the model do its thing. The only reason that we have to do structure generation with JSON is because we don't have a better parser. If we had a better schema that we could go and like parse, then it would just do the trick for us, and we could let the model do whatever it wanted.\nYeah, JSON was like totally the wrong decision for for structured generation just in general. Like JSON should never have been the primitive for that. It should have been like TOML or YAML or something where it's much more like everyone uses XML, right?\nWell, XML and YAML have the same problems. TOML and YAML are basically indiscernible for where they start, and their ambiguity is a starting point. It's really, really hard to clarify because almost every file is a valid YAML file. So parsing that is virtually impossible because there's no, there's not enough constraints on it. XML has a similar problem where you run into escape characters with XML, and like the actual way that you do XML is so nasty. If you've seen how JSON looks in XML, like type systems and XML don't really make sense. They're too verbose. So it's not that there's one answer for everything. It's just that you want the flexibility as the app designer to choose what you want. If your, if your goal as a developer is to generate HTML code, XML will be a bad format for you. Guaranteed definition.\nAll right. All right. All right. I'm going to pause this on the structured output. This is a dope deep dive. We have seven minutes left, and we have some really good questions in the chat. Um, I, we have Derek has a hand raised. Um, so, uh, and that I really like Sean's question as well, which is like, how do you keep your database schemas consistent with your BAML generated types? Um, I think that one is quick, right? We're just dumping JSON B into the database, and and the front end is parsing it using the BAML library.\nUm, but then I want to hear D's question as well.\nLiterally the types that I save are just text, and I think I just do JS. I think I up, actually, I have, this is not the latest table. I had to do some migrations. Uh, where's my migrations table?\nUm, I added uh, all of these became JSON views. They're just JSON views. So I just open and close JSON V files, uh, JSON B columns along the way, and then I just cast it because I know it's correct. The hard part about this is versioning tables over time. But this is the same problem that you'd have in versioning at any other point in your codebase. Um, we will have a solution for that really soon, actually. It's going to be really exciting. That's several conversation.\nUh, versioning schemas is really, really hard.\nAll right, Derek, what you got?\nJust a quick question. And you've made me think, um, earlier when you were doing the summarize, I had put a suggestion, maybe synthesize all of the variable names, functions aside, really, it's that first sentence where you say summarize this, but then now I was first going to ask about eval, like an approach if you wanted to try a few different versions of that sentence, how would you eval it, but then now it's I'm thinking it's more complex because synthesis because of the way you've blocked this into those time chunks, then it may not even, you know, like, synthesize across them. So if this question is making any sense, how would you handle that to kind of eval, uh, given that you've taken a chunked approach now?\nWell, I would just the pipeline, like I think chunked, meaning the chunk meaning the transcript time chunks. So like now you'd have to look across those chunks to do a synthesis as opposed to summarizing each one. That, that's what I meant. So hopefully I'm making sense with my question.\nNo, that okay. So this was, I mean, we haven't actually, I don't think we've actually done that yet. Um, but the proposal was like, hey, we could make the transcript a little bit shorter, and that may actually not be uh, be the right idea.\nUm, yeah, but then you have to basically have a, you basically have to have a separate eval that is like, okay, based on our chunking prompt and our chunking algorithm, and how do we take that, those chunks and pass it back in? We're no longer evaluating just input output pairs. We're now evaluating kind of a few steps of the pipeline, which is also really important to do, right? The best eval is the one that actually tell runs your pipeline end to end and tells you if it's still working, and then you can go refine down into the smaller level ones, smaller chunks of your pipeline and then individual prompts to narrow down, like, okay, what's the thing that broke this, but, um, I think the answer is like, that's probably not what we would do.\nThat was just a proposal, like, because you have control of the pipeline, you can do whatever you.\nI don't know if that's a great answer.\nOh, thumbs up. All right, we'll take it.\nCool. So, let's actually make the email better really fast. By the way, while I'm here, uh, email example, we're going to do this. Um, an example. An example. Great. Email was this.\nSahil's question is now that we have an automated pipeline, are they, are we, are they going to get the uh video upload now instead of Friday?\nNo. Um, because we're going to make the pipeline good. I think one of the things I hate as an engineer is I [Â __Â ] hate emails. I want emails to be useful or not at all received. Uh, so Dexter and I actually do some good work to actually go edit the pipeline to make the emails good. So like in this case, I literally an example of here's what it is. Fed in the example, and it's going to go ahead and, um, and it's going to go ahead and not read the prompt. I actually told it a great exampam, a great email was this. A great email for a prior uh video was this. And now I'll notice a small things that I noticed immediately, which is these bullet points are labeled wrong. So I want to go and like format these better.\nSo let's go do that, or like, let's just make the prompt better, and like, that's great. I don't need to tell, I don't need to sick.\nUm, topic in this again. Let's go read this. Looks good. Main takeaways. Looks good. Uh, transcript. Complete transcript. Full transcript.\nUm, we'll go do this. I will probably move the transcript above the main take.\nYep.\nBecause again, biased mostly by the most recent tokens. So I'm going to go do that. So I want the main takeaways to be last. It's going to go do this. I'm using GP4 mini. Let's just try and see what happens.\nI've heard GPT, I've experimented a little bit. The GPT4.5 is is um very good these sort of writing tasks.\nA couple of things. If you notice it said your name, that's a a private video. Um, create a professional announcement on behalf of five and Dexter. Let's tell it that. Let's tell it that on behalf of us so it can have more context again. And this is why we can use a fshot example because this example is a really, really good example for what we want. In this case, they didn't put a call to action in the data model. The call to action is probably dumb.\nAre you hitting context limits on 40 mini? Maybe because it's so long.\nI'm not. There's no way.\nOkay. Okay.\nLet's go do this. Let's make call to action optional.\nAnd it actually seems like it figured it out. Um, we can go read this really fast. So, this was about dynamic strategies for effective prompting. Uh, this is weird. Um, and last time's video title was actually, we already had a title. The title was called um cracking the prompting interview. So, before I go add in parameter, let's say like, let's just like assume that the title that I passed in, I can just crack change this in my test case.\nUh, summary.\nLet's assume that I gave it the right title.\nOh, at the bottom.\nYeah, your title says Zoom meeting cracking the prompting interview. So, I'll go run this.\nNew video release. Cracking the prompting interview. This is pretty good. It actually got rid of the Zoom meeting. Gave a slightly better title. It actually almost got the exact same email that we actually really wanted.\nUse indexes instead of relying on long URLs. That's a great thing. Natural output. Let it go in the most natural format instead of constraining them to JSON, which thing that we should do. Really format the structure as you want. Read the prompt. Balancing effectiveness. That looks like spam. Uh, real world applications, that looks like spam. Um, so what we can do now is we can just like compare this to the actual thing that we sent out, and we can go see what it feels like.\nAnd this was the one that we wrote by hand for this one. Um, there's a couple of things that didn't have like these. One of the things that we probably like is we like having code in the emails. Code snip, like small little code snippets in back text in the emails. It makes, I think, makes it much more authentic.\nDidn't seem to capture that. So let's just tell it that we like that. Um, uh, where'd it go? We can tell this in the draft. Um, include code snippets, include small code snippets in the emails.\nAnd now let's go run this.\nAnd let's see if it does something.\nAnd I might actually have a hard time including code snippets because it's just from an audio snippet. So what I might need to do is I might actually need to feed in the video into this as well. So I can actually have video as context while it does this. So we should get the YouTube URL for the YouTube video and just pass into like a Gemini model and go pass that in. And that would be like the next thing that I would go iterate on along the way.\nAre you guys understanding we iterate on this pipeline and make it better to make it really feel like what's good? What's good here I want you all to notice is Dexra and I already have done this work a few times. So we have a golden target in mind. If we're working on a data set that neither of us understands, there's no golden target and we'll be stuck and we won't be able to get an answer really well.\nSo we can iterate fast without an eval set because we kind of understand the domain of the problem.\nUm, is there any other questions along the way? Um, texture, anything else you want to mention?\nUm, no, the only the other reason why we can't send this out right away because Zoom takes like four hours to process the video and create the transcript. So that's uh to answer Seil's other question, we will no matter how much AI we have, we will never send the video immediately after the call.\nYes, because of Zoom. Um, and we might be able to get get around the one day turnaround as well. But again, the whole point of this is I am.\nDon't you go present to the AI team at Zoom? Don't you have like influence over how they can't you make it faster?\nYeah, let me ping them. Um, but I think the um, yeah, I think the takeaway here is like build the tools that you need, build the workflow first. Once you've built the workflow, add the AI part because you can decouple that development really fast. And whatever I have to do to generate the email address. My real suspicion to make this email really good is actually I should actually break down the structure of this email into like separate sections that we have and write a two-step pipeline where the first step generates all the structure, then the second step puts it into the like the language and the floweriness of what the email needs.\nAnd I think that will make it a lot, a lot better.\nUm, but I think that's it. We went a lot a little bit over than what we planned on doing today. Um, but this was tons of fun. I personally learned a lot building this whole thing out. Way more than I normally do. Um, because I learned from Draode. It was amazing. Um, you were, you were pretty good at Vive coding before uh when we got started.\nNo, I'm I'm not.\nI think I do. My v coating is\n",
  "dumpedAt": "2025-07-21T18:43:26.039Z"
}