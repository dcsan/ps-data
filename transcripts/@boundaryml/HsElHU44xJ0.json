{
  "episodeId": "HsElHU44xJ0",
  "channelSlug": "@boundaryml",
  "title": "Getting Tone Just right with LLMs: ðŸ¦„ #12",
  "publishedAt": "2025-07-04T17:56:15.000Z",
  "rawLines": [
    {
      "lang": "en",
      "text": "And I'm gonna shoot you the",
      "offset": 0.32,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "All right.\n I'm gonna send you the",
      "offset": 3.6,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "whiteboard.",
      "offset": 4.88,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "Perfect.",
      "offset": 6.96,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "How's it going everyone? Um, we're back",
      "offset": 8.72,
      "duration": 6.32
    },
    {
      "lang": "en",
      "text": "to episode number 12 13. Uh, and today",
      "offset": 11.12,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "our whole goal is we're going to talk",
      "offset": 15.04,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "about how to get the tone just right",
      "offset": 16.32,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "when we're prompting. I think a lot of",
      "offset": 17.76,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "times when we're building a prompt, we",
      "offset": 19.84,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "talked about this last time when we",
      "offset": 21.68,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "built the AI content pipeline.",
      "offset": 22.8,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "It's really first it's essential to get",
      "offset": 25.359,
      "duration": 5.441
    },
    {
      "lang": "en",
      "text": "the information correct and presented",
      "offset": 28.4,
      "duration": 3.999
    },
    {
      "lang": "en",
      "text": "the way you want to get it presented,",
      "offset": 30.8,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "but then you want to go do part two. And",
      "offset": 32.399,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "part two is actually making the prompts",
      "offset": 34.239,
      "duration": 5.921
    },
    {
      "lang": "en",
      "text": "do what you want them to do. And I think",
      "offset": 36.32,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "last time we spent a lot of time talking",
      "offset": 40.16,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "about the infrastructure, the tooling,",
      "offset": 41.84,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "and also just getting the data across",
      "offset": 42.96,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "and some by coding processes that Dexter",
      "offset": 44.96,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "and I use when we go do this.",
      "offset": 47.12,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "I think today our goal is we want to",
      "offset": 49.68,
      "duration": 5.359
    },
    {
      "lang": "en",
      "text": "talk a little bit more about exactly how",
      "offset": 51.84,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "we're going to go ahead and make these",
      "offset": 55.039,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "prompts better. So just to recap",
      "offset": 56.96,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "everyone, Dexter, do you want to show",
      "offset": 59.199,
      "duration": 2.961
    },
    {
      "lang": "en",
      "text": "everyone what like we were do what we",
      "offset": 60.32,
      "duration": 3.039
    },
    {
      "lang": "en",
      "text": "were doing yesterday with yesterday's",
      "offset": 62.16,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "pipeline\n or last week's sorry?\n Yeah. So",
      "offset": 63.359,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "like I have the whiteboard here. I just",
      "offset": 66.96,
      "duration": 2.56
    },
    {
      "lang": "en",
      "text": "made a copy of the one from last week",
      "offset": 68.159,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "because we're going to add to it. Um but",
      "offset": 69.52,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "we built this system with a back end and",
      "offset": 72.08,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "a front end and a data bot base and kind",
      "offset": 73.68,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "of an API in between them. um and these",
      "offset": 75.439,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "kind of pipelines where we would take a",
      "offset": 77.68,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "Zoom meeting idea, we download the video",
      "offset": 79.28,
      "duration": 3.519
    },
    {
      "lang": "en",
      "text": "and send it to YouTube and summarize the",
      "offset": 80.96,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "transcript and then draft a bunch of",
      "offset": 82.799,
      "duration": 5.121
    },
    {
      "lang": "en",
      "text": "content uh based on that video. Um and",
      "offset": 84.56,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "we had a little bit of like a human loop",
      "offset": 87.92,
      "duration": 5.519
    },
    {
      "lang": "en",
      "text": "refinement workflow. Um",
      "offset": 89.52,
      "duration": 6.88
    },
    {
      "lang": "en",
      "text": "let me see",
      "offset": 93.439,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "if I I think I have it running. Um",
      "offset": 96.4,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "actually I'm\n you can just run the new",
      "offset": 98.799,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "one. It's fine too. Yeah, there you go.",
      "offset": 100.159,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "Yeah. So this is this is the updated",
      "offset": 102.479,
      "duration": 3.521
    },
    {
      "lang": "en",
      "text": "version of it. Um, so you can see we've",
      "offset": 104.159,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "kind of done some imports. Um, we've",
      "offset": 106,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "added a bunch of stuff, um, like",
      "offset": 107.84,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "connecting the Luma events to the Zoom",
      "offset": 109.84,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "meetings using some mix of AI and not",
      "offset": 112.56,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "AI. Um, we've added, uh, what is it?",
      "offset": 115.28,
      "duration": 6.479
    },
    {
      "lang": "en",
      "text": "This be the ability to create a draft PR",
      "offset": 119.04,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "and GitHub, which I'll show you what one",
      "offset": 121.759,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "of those looks like. Um,",
      "offset": 123.2,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "but then it's, you know, we've got the",
      "offset": 126.64,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "summarization of the key points. We have",
      "offset": 127.759,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "this timestamp stump that we looked at",
      "offset": 129.36,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "and then it will generate a Twitter",
      "offset": 131.36,
      "duration": 8.08
    },
    {
      "lang": "en",
      "text": "thread and a LinkedIn post and an email",
      "offset": 133.84,
      "duration": 7.92
    },
    {
      "lang": "en",
      "text": "for\n and some of you from last time",
      "offset": 139.44,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "probably got this email and you probably",
      "offset": 141.76,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "noticed that we had a little bug in",
      "offset": 143.68,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "there where it said hello first name. It",
      "offset": 145.36,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "turns out when you do generate content",
      "offset": 147.92,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "go ahead.\n Did we send the email with",
      "offset": 150.08,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "hello first name? we did because the",
      "offset": 152.4,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "email templating system that we have",
      "offset": 155.44,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "doesn't actually it needs a variable",
      "offset": 157.44,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "plugged in in a very specific way for",
      "offset": 160.48,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "the actual first name and you know parts",
      "offset": 162.16,
      "duration": 6.32
    },
    {
      "lang": "en",
      "text": "of doing AI generated content is that",
      "offset": 166.4,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "mistakes like this happen I mean that",
      "offset": 168.48,
      "duration": 2.399
    },
    {
      "lang": "en",
      "text": "would happen even if you're a human so",
      "offset": 169.68,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "it's not just restricted to humans are",
      "offset": 170.879,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "just AI pipelines but this is like a bug",
      "offset": 173.12,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "that we have to be aware of because to",
      "offset": 175.68,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "take it to like that last degree we're",
      "offset": 177.36,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "always going to run into these kinds of",
      "offset": 180.16,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "edge cases where like maybe the AI gener",
      "offset": 181.36,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "and we'll inject first name later in the",
      "offset": 183.44,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "pipeline as well. So, we have to think",
      "offset": 185.36,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "about that and think about how we can",
      "offset": 187.44,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "make that error better along the way.",
      "offset": 188.72,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "Yeah. Um and then yeah, the other the",
      "offset": 192.319,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "other cool feature was basically um and",
      "offset": 195.2,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "we can get into this code as well, I",
      "offset": 197.28,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "think. Um but if we go to I can show you",
      "offset": 198.64,
      "duration": 6.56
    },
    {
      "lang": "en",
      "text": "um",
      "offset": 203.44,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "this auto here's an autogenerated pull",
      "offset": 205.2,
      "duration": 5.599
    },
    {
      "lang": "en",
      "text": "request for last week's episode. Um, and",
      "offset": 207.44,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "so it's using a different title and a",
      "offset": 210.799,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "couple other\n click on the top right",
      "offset": 212.48,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "instead of code instead of the markdown",
      "offset": 214.319,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "view through the full proper view on the",
      "offset": 215.92,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "bottom. Oh, not there. Sorry. Uh, I'll",
      "offset": 218.159,
      "duration": 6.561
    },
    {
      "lang": "en",
      "text": "draw on your screen right here.\n Oh. Oh,",
      "offset": 221.44,
      "duration": 5.359
    },
    {
      "lang": "en",
      "text": "I see. Yeah.",
      "offset": 224.72,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "Um, so it's done a pretty good job. If",
      "offset": 226.799,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "you've seen the other readmes in this",
      "offset": 228.799,
      "duration": 5.681
    },
    {
      "lang": "en",
      "text": "repo, it's looks a lot like the other",
      "offset": 230.239,
      "duration": 6.401
    },
    {
      "lang": "en",
      "text": "episodes that we've done. Oh, that's a",
      "offset": 234.48,
      "duration": 5.119
    },
    {
      "lang": "en",
      "text": "workshop. Uh, if we look at one of these",
      "offset": 236.64,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "and you come look at the read Oh, okay.",
      "offset": 239.599,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "Oh, this one. Yeah,\n that was your",
      "offset": 241.76,
      "duration": 3.119
    },
    {
      "lang": "en",
      "text": "episode. You were supposed to upload the",
      "offset": 243.519,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "read me for that one.\n I know, I know, I",
      "offset": 244.879,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "know.",
      "offset": 246.799,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "Um, but so here's like our normal readme",
      "offset": 248.319,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "format where we'll have some whiteboards",
      "offset": 251.04,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "and then we'll have some code and then",
      "offset": 252.879,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "we'll talk about how to run it. So, it's",
      "offset": 254.56,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "it's it's able to and we'll look at how",
      "offset": 256.239,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "this context gets built, but basically",
      "offset": 258.239,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "looking at previous readmemes, it's able",
      "offset": 259.68,
      "duration": 6.64
    },
    {
      "lang": "en",
      "text": "to draft a um what this what this should",
      "offset": 262.079,
      "duration": 6.161
    },
    {
      "lang": "en",
      "text": "look like and leave we've prompted to",
      "offset": 266.32,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "like leave certain sections blank",
      "offset": 268.24,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "because we'll just the goal is always to",
      "offset": 269.759,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "like reduce the amount of human effort.",
      "offset": 271.44,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "Um I don't think we'll ever eliminate it",
      "offset": 274,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "and if we if we if we if we hold off on",
      "offset": 275.759,
      "duration": 5.041
    },
    {
      "lang": "en",
      "text": "deploying a system until everything is",
      "offset": 278.72,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "perfect, then we'll never get anything",
      "offset": 280.8,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "out. Um, and I think this adds a lot of",
      "offset": 282.479,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "value with a couple while still, you",
      "offset": 284.32,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "know, leaving a couple things.\n It goes",
      "offset": 287.12,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "to a couple Can you go up really fast",
      "offset": 288.96,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "that?\n Yeah,\n it goes to like a couple",
      "offset": 290.32,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "things. For example, the whiteboards,",
      "offset": 292.88,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "then we'd have to build a Excal",
      "offset": 294.4,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "integration that can go pull data from",
      "offset": 297.44,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "there. And that just takes time and",
      "offset": 299.12,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "getting an LLM to exactly take the",
      "offset": 300.56,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "perfect screenshots. Like, that's a",
      "offset": 302.4,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "problem that's easier for Dexter and I",
      "offset": 304.96,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "to do than to write the code for doing",
      "offset": 306.56,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "this. The goal of projects when you",
      "offset": 308.16,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "build AI pipelines is not to automate",
      "offset": 309.919,
      "duration": 2.961
    },
    {
      "lang": "en",
      "text": "everything.",
      "offset": 311.759,
      "duration": 3.041
    },
    {
      "lang": "en",
      "text": "The goal of a project is to get a good",
      "offset": 312.88,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "value of time for uh good value for the",
      "offset": 314.8,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "time that you put in. So Dexter and I",
      "offset": 317.36,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "take roughly five to eight hours to",
      "offset": 319.039,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "automate this to do this work every",
      "offset": 320.88,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "week.",
      "offset": 323.44,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "If we're going to go do that, spending",
      "offset": 325.039,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "10 hours to automate it isn't that bad,",
      "offset": 326.639,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "assuming that we keep doing more",
      "offset": 329.759,
      "duration": 3.121
    },
    {
      "lang": "en",
      "text": "content, which I think we will for a",
      "offset": 330.96,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "while.\n Yeah. Um,\n but the whiteboard",
      "offset": 332.88,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "stuff is, you know, I can literally just",
      "offset": 334.96,
      "duration": 3.519
    },
    {
      "lang": "en",
      "text": "come here and grab one of these and copy",
      "offset": 336.56,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "it and then paste it into GitHub and",
      "offset": 338.479,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "that takes me 30 seconds. And to write",
      "offset": 340.24,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "the code, it's like the easiest part of",
      "offset": 342.24,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "the whole workflow. So, we're like,",
      "offset": 344.08,
      "duration": 2.16
    },
    {
      "lang": "en",
      "text": "cool, we're not going to write code for",
      "offset": 345.28,
      "duration": 2
    },
    {
      "lang": "en",
      "text": "that. We're going to let the LM do",
      "offset": 346.24,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "things the LM is really good at and",
      "offset": 347.28,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "we're going to write, we're going to",
      "offset": 348.96,
      "duration": 2.56
    },
    {
      "lang": "en",
      "text": "just manually do the things that we can",
      "offset": 350.08,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "do.",
      "offset": 351.52,
      "duration": 3.519
    },
    {
      "lang": "en",
      "text": "And it's not that we can't do the AI",
      "offset": 353.28,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "part. It's just not worth the time. And",
      "offset": 355.039,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "I think that's what we go into because",
      "offset": 357.52,
      "duration": 2.959
    },
    {
      "lang": "en",
      "text": "if the whiteboards are bad, then we have",
      "offset": 358.8,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "to go in and go edit it. And that is",
      "offset": 360.479,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "where it breaks down. Kyle, then your",
      "offset": 362.4,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "point about what to do is different.\n I",
      "offset": 365.039,
      "duration": 5.521
    },
    {
      "lang": "en",
      "text": "will also say that I think we do quite a",
      "offset": 368.16,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "bit of uh customizing the whiteboards.",
      "offset": 370.56,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "So like this whiteboard is actually four",
      "offset": 373.68,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "different screenshots that are like",
      "offset": 375.84,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "ordered and annotated in a specific way",
      "offset": 377.36,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "because the the full whiteboard would be",
      "offset": 379.6,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "unintelligible.",
      "offset": 381.68,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "Exactly. So it's not that we couldn't do",
      "offset": 383.6,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "it. We could definitely build a pipeline",
      "offset": 385.84,
      "duration": 2.479
    },
    {
      "lang": "en",
      "text": "that automates this. It's just not worth",
      "offset": 387.039,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "the time. It's faster for Dexter and I",
      "offset": 388.319,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "to spend five minutes doing that work",
      "offset": 389.759,
      "duration": 6.081
    },
    {
      "lang": "en",
      "text": "than uh not doing it",
      "offset": 392.56,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "with that. I think I want to talk about",
      "offset": 395.84,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "I want to talk a little bit more about",
      "offset": 397.84,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "the architecture here and the kinds of",
      "offset": 399.6,
      "duration": 2.879
    },
    {
      "lang": "en",
      "text": "data that we were able to plum through.",
      "offset": 401.199,
      "duration": 3.521
    },
    {
      "lang": "en",
      "text": "Dexture drew an amazing whiteboard that",
      "offset": 402.479,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "actually talks about how to collect some",
      "offset": 404.72,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "of this data for this pipeline. Talks",
      "offset": 405.919,
      "duration": 2.881
    },
    {
      "lang": "en",
      "text": "about some more of the problems that we",
      "offset": 407.52,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "have because before we can get the tone",
      "offset": 408.8,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "right again uh let's show and let's show",
      "offset": 410.4,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "the email that we have really fasture",
      "offset": 413.6,
      "duration": 3.039
    },
    {
      "lang": "en",
      "text": "just so people have it. I think you",
      "offset": 415.44,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "should have it as well.\n Not this one the",
      "offset": 416.639,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "other one that we actually sent out. the",
      "offset": 419.44,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "one that actually got sent. Uh I think I",
      "offset": 420.88,
      "duration": 5.48
    },
    {
      "lang": "en",
      "text": "have that one.",
      "offset": 422.96,
      "duration": 3.4
    },
    {
      "lang": "en",
      "text": "Uh let's see.\n And let's talk about some",
      "offset": 427.36,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "of the details that we need in order to",
      "offset": 429.599,
      "duration": 2.961
    },
    {
      "lang": "en",
      "text": "get the tone right because again before",
      "offset": 431.12,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "we can actually do the prompting, we all",
      "offset": 432.56,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "have to understand what are we aiming",
      "offset": 434,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "for and what is the golden data set that",
      "offset": 435.28,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "would really really be good.",
      "offset": 436.96,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "If you look hello first name, you'll",
      "offset": 440.16,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "find it.\n Yeah, there we go.\n There you",
      "offset": 441.599,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "go.",
      "offset": 443.36,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "Uh",
      "offset": 445.28,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "so scroll down. So, there's a couple uh",
      "offset": 447.12,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "sorry, a little bit up. I realize that I",
      "offset": 450.72,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "can't control your screen. There's a",
      "offset": 452.56,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "couple of things that we need. One, if",
      "offset": 454.16,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "you go up, uh",
      "offset": 455.68,
      "duration": 6.959
    },
    {
      "lang": "en",
      "text": "we need the title to be correct.",
      "offset": 458.4,
      "duration": 7.68
    },
    {
      "lang": "en",
      "text": "Uh this is very, very important. We need",
      "offset": 462.639,
      "duration": 5.441
    },
    {
      "lang": "en",
      "text": "some sort of nice line here that feels",
      "offset": 466.08,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "like a flow that is kind of different",
      "offset": 468.08,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "every single time probably because it'd",
      "offset": 471.28,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "be boring to read the same thing or we'd",
      "offset": 473.12,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "rather not have that line.",
      "offset": 474.639,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "Um, we probably don't need anything",
      "offset": 476.8,
      "duration": 4.959
    },
    {
      "lang": "en",
      "text": "special here, but we do need the links",
      "offset": 479.84,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "to be correct for the YouTube video and",
      "offset": 481.759,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "for the GitHub repo. Ideally, we would",
      "offset": 484.4,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "put the right link to the folder there.",
      "offset": 487.919,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "The only reason we don't do that is",
      "offset": 489.84,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "because it takes work to find that",
      "offset": 491.68,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "folder, and that's annoying, but the",
      "offset": 493.199,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "email would be a better email if it took",
      "offset": 496.24,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "you right to that folder. So, we'd like",
      "offset": 497.84,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "to go do that.\n Oh, way to find a folder.",
      "offset": 500.16,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "I can show you that. We'll talk about I",
      "offset": 503.68,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "know we'll talk about that. I saw your",
      "offset": 505.36,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "code.",
      "offset": 507.039,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "We want to have a little summary here",
      "offset": 508.56,
      "duration": 5.359
    },
    {
      "lang": "en",
      "text": "that is not too long um and really",
      "offset": 510.319,
      "duration": 5.121
    },
    {
      "lang": "en",
      "text": "varies based on the task that we're",
      "offset": 513.919,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "doing.",
      "offset": 515.44,
      "duration": 4.959
    },
    {
      "lang": "en",
      "text": "And then do you want to scroll down?",
      "offset": 517.839,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "Yeah.",
      "offset": 520.399,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "And then after that, what we want is",
      "offset": 522.56,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "we'll really want like this one key",
      "offset": 524.64,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "takeaway because like we said, it's just",
      "offset": 526.399,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "useful to have one good summary rather",
      "offset": 528.88,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "than five bullet points written over",
      "offset": 530.64,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "here. And then we'll want to And then",
      "offset": 532.16,
      "duration": 7.119
    },
    {
      "lang": "en",
      "text": "the last part go down.\n Yep.",
      "offset": 534.32,
      "duration": 8.32
    },
    {
      "lang": "en",
      "text": "Clear my screen.\n There we go.\n Sorry. Um",
      "offset": 539.279,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "and the last part we'll want is really",
      "offset": 542.64,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "about like action items for what someone",
      "offset": 544.24,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "can do next. That means the Luma link",
      "offset": 546.24,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "has to be right. The title has to be",
      "offset": 547.76,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "right. And then we probably want to tell",
      "offset": 549.2,
      "duration": 2.639
    },
    {
      "lang": "en",
      "text": "people the right date of when they can",
      "offset": 550.56,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "go go join this nest event. So there's a",
      "offset": 551.839,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "lot of components to this that if we",
      "offset": 554.959,
      "duration": 3.521
    },
    {
      "lang": "en",
      "text": "don't get it perfectly right, it's going",
      "offset": 557.04,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "to be annoying. The part where tone is",
      "offset": 558.48,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "the most important is really this middle",
      "offset": 560.56,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "section about the content and the",
      "offset": 562.08,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "summary. We don't want it to be too",
      "offset": 564,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "long. We don't want it to be too",
      "offset": 565.519,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "redundant. We don't want to use like AI",
      "offset": 566.8,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "hype words. We wanted to really just",
      "offset": 568.399,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "nail like the content part of it which",
      "offset": 570.399,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "is hard uh because if you if you",
      "offset": 572.959,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "trivially prompt a model, at least from",
      "offset": 575.6,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "what I have tried, most of the time it",
      "offset": 577.12,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "doesn't really do that correctly.",
      "offset": 578.72,
      "duration": 6.64
    },
    {
      "lang": "en",
      "text": "So getting that tone of like content to",
      "offset": 582,
      "duration": 5.279
    },
    {
      "lang": "en",
      "text": "uh uh content to quality is going to be",
      "offset": 585.36,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "a little bit tricky to get. But really a",
      "offset": 587.279,
      "duration": 3.281
    },
    {
      "lang": "en",
      "text": "lot of it is about plumbing in the right",
      "offset": 588.959,
      "duration": 2.961
    },
    {
      "lang": "en",
      "text": "data into the right places, the right",
      "offset": 590.56,
      "duration": 3.279
    },
    {
      "lang": "en",
      "text": "YouTube link, the right folder names,",
      "offset": 591.92,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "the right next uh workshops, the right",
      "offset": 593.839,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "oneline takeaway, the right titles. A",
      "offset": 596.56,
      "duration": 3.279
    },
    {
      "lang": "en",
      "text": "lot of things are really just about",
      "offset": 598.64,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "assembling the right data.\n So in order",
      "offset": 599.839,
      "duration": 6.401
    },
    {
      "lang": "en",
      "text": "to do that, go ahead.\n Yeah. So so so I",
      "offset": 602.64,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "think this is where you're going. Um,",
      "offset": 606.24,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "are you are you kind of driving towards",
      "offset": 607.76,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "My instinct as we talk about this is",
      "offset": 610,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "almost like creating a plain text",
      "offset": 612,
      "duration": 6.56
    },
    {
      "lang": "en",
      "text": "template and then filling it in with",
      "offset": 616,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "smaller bits of LM generated content or",
      "offset": 618.56,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "do you still want the because we talk",
      "offset": 621.04,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "about also like these longer strings",
      "offset": 622.48,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "with lots of like random characters in",
      "offset": 624.16,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "them are a little bit hard for AI to",
      "offset": 626.16,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "reproduce super reliably. I mean I think",
      "offset": 627.839,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "a short URL like this I've never seen a",
      "offset": 629.839,
      "duration": 5.841
    },
    {
      "lang": "en",
      "text": "Frontier model ever screw that up.",
      "offset": 632.079,
      "duration": 6.081
    },
    {
      "lang": "en",
      "text": "I think we toss in a big model like two",
      "offset": 635.68,
      "duration": 5.279
    },
    {
      "lang": "en",
      "text": "Gemini 25 GT40",
      "offset": 638.16,
      "duration": 6.88
    },
    {
      "lang": "en",
      "text": "uh like Sonnet um or Sonnet or something",
      "offset": 640.959,
      "duration": 5.521
    },
    {
      "lang": "en",
      "text": "like that, it'll probably not mess up a",
      "offset": 645.04,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "short URL like that that doesn't have",
      "offset": 646.48,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "too many weird characters. It should",
      "offset": 647.76,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "almost definitely work for them.\n Yeah.",
      "offset": 649.36,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "But I think the more important thing is",
      "offset": 652.079,
      "duration": 2.241
    },
    {
      "lang": "en",
      "text": "actually that I want to share with",
      "offset": 653.36,
      "duration": 2.719
    },
    {
      "lang": "en",
      "text": "everyone is the thing that we did before",
      "offset": 654.32,
      "duration": 3.519
    },
    {
      "lang": "en",
      "text": "we did all of this work, which is we",
      "offset": 656.079,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "drew that diagram\n and that diagram is",
      "offset": 657.839,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "essential to actually understanding it.",
      "offset": 661.279,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "So, let's pull that up really fast.\n So,",
      "offset": 662.8,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "this is the behind the scenes of how",
      "offset": 665.36,
      "duration": 2.64
    },
    {
      "lang": "en",
      "text": "Dexter and I actually built this",
      "offset": 666.72,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "pipeline, how we talked about it because",
      "offset": 668,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "it not it's not just enough for us to be",
      "offset": 670.079,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "able to build it. We really need to be",
      "offset": 671.76,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "able to talk about this in a really nice",
      "offset": 673.279,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "way that helps us understand what's",
      "offset": 675.12,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "happening.\n Well, and it's like the",
      "offset": 677.519,
      "duration": 3.281
    },
    {
      "lang": "en",
      "text": "dependencies, right? It's like, okay, if",
      "offset": 679.519,
      "duration": 2.56
    },
    {
      "lang": "en",
      "text": "I want to write the email, what are the",
      "offset": 680.8,
      "duration": 3.039
    },
    {
      "lang": "en",
      "text": "pieces I need? Okay, I need the summary",
      "offset": 682.079,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "and I need the next event and I need the",
      "offset": 683.839,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "YouTube URL and I need the thumbnail",
      "offset": 685.76,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "URL. I don't need the thumbnail, but I",
      "offset": 687.68,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "need the YouTube URL to go into the",
      "offset": 689.36,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "summary. So it's like what are all the",
      "offset": 691.44,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "pieces and how do we get them? And this",
      "offset": 693.04,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "was how we sort of like even like broke",
      "offset": 694.64,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "up the work and and worked kind of",
      "offset": 697.2,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "asynchronously on this stuff.\n Exactly.",
      "offset": 698.959,
      "duration": 5.041
    },
    {
      "lang": "en",
      "text": "So like it all starts with the Zoom",
      "offset": 701.839,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "meeting and that's what we had before.",
      "offset": 704,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "Then what we did part two is we added",
      "offset": 705.92,
      "duration": 5.919
    },
    {
      "lang": "en",
      "text": "the Luma integration and that was a",
      "offset": 709.04,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "little bit trickier than most people",
      "offset": 711.839,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "would expect which was what we wanted to",
      "offset": 713.2,
      "duration": 5.759
    },
    {
      "lang": "en",
      "text": "do was using Luma just directly pull out",
      "offset": 715.519,
      "duration": 6.241
    },
    {
      "lang": "en",
      "text": "data and connect it to the Zoom meeting.",
      "offset": 718.959,
      "duration": 5.281
    },
    {
      "lang": "en",
      "text": "And that",
      "offset": 721.76,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "that's that's what looks that's what",
      "offset": 724.24,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "this is over here, right? Is so we we",
      "offset": 726.079,
      "duration": 2.961
    },
    {
      "lang": "en",
      "text": "fetch the list of Zoom recordings and",
      "offset": 727.68,
      "duration": 3.279
    },
    {
      "lang": "en",
      "text": "then we try to match them to a specific",
      "offset": 729.04,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "Luma event. Uh this happens to be",
      "offset": 730.959,
      "duration": 5.281
    },
    {
      "lang": "en",
      "text": "connected to Vibb's uh company Zoom. So",
      "offset": 733.44,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "there's a bunch of random meet Yeah,",
      "offset": 736.24,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "there's a there's a bunch of random uh",
      "offset": 738,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "meetings in here that aren't connected",
      "offset": 740,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "to Luma events. Um, and so being able to",
      "offset": 741.76,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "match those to the event lets us get the",
      "offset": 744.8,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "title in properly and lets us get the",
      "offset": 747.04,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "summary of the event in properly that",
      "offset": 749.68,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "helps us prompt everything else more",
      "offset": 751.6,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "more correctly. This is all just like",
      "offset": 753.44,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "engineering the right context.",
      "offset": 755.519,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "Yes. Do you want to share a little bit",
      "offset": 758.16,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "about um the Luma side of the pipeline",
      "offset": 760.16,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "really fast? How we're able to go do",
      "offset": 763.36,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "that?\n Yeah. Let's go pull the\n problem",
      "offset": 764.639,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "the problems in there are I think are",
      "offset": 767.44,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "not very obvious.",
      "offset": 769.44,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "Yeah. So this function is actually quite",
      "offset": 771.6,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "long. Um we have two things. We have",
      "offset": 774.639,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "fetch the upcoming event which is",
      "offset": 776.8,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "basically like let's look at a list of",
      "offset": 779.279,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "events from Luma and decide which one is",
      "offset": 781.839,
      "duration": 5.521
    },
    {
      "lang": "en",
      "text": "coming soon is the next event because",
      "offset": 785.279,
      "duration": 3.521
    },
    {
      "lang": "en",
      "text": "when we generate the email we want to be",
      "offset": 787.36,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "able to figure that out. And you could",
      "offset": 788.8,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "do a bunch of logic of like okay let's",
      "offset": 791.279,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "look at the start time and then like",
      "offset": 793.76,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "let's filter them to only ones in the",
      "offset": 795.68,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "future. This is deterministic code. I",
      "offset": 797.36,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "mean AI can do this, but this is",
      "offset": 799.04,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "basically just math. And so your code",
      "offset": 800.88,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "should do this for you. And then we'll",
      "offset": 802.959,
      "duration": 5.521
    },
    {
      "lang": "en",
      "text": "sort them by the start time and then",
      "offset": 806.56,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "that's when we go to the AI side. So we",
      "offset": 808.48,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "give a very basic description. This is",
      "offset": 810.959,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "um we have this luma event object which",
      "offset": 813.44,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "is lots and lots of fields that we're",
      "offset": 815.44,
      "duration": 3.519
    },
    {
      "lang": "en",
      "text": "going to use in lots of other places.",
      "offset": 817.04,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "But actually for the thing we pass the",
      "offset": 818.959,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "LM we're doing a custom serializer. So",
      "offset": 820.48,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "we're just sending a subset. We're only",
      "offset": 822.72,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "sending the next 10 events. Um, and then",
      "offset": 824.959,
      "duration": 5.521
    },
    {
      "lang": "en",
      "text": "we're going to ask, yeah,\n I want to",
      "offset": 828.32,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "pause there really fast. This is what",
      "offset": 830.48,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "Dexter often says as context",
      "offset": 832,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "engineering.",
      "offset": 833.92,
      "duration": 5.279
    },
    {
      "lang": "en",
      "text": "The trivial thing to do is pass in the",
      "offset": 836.079,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "Luma event object. We could all do that.",
      "offset": 839.199,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "The right thing to do so we don't have",
      "offset": 841.519,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "to think about it is context engineering",
      "offset": 843.36,
      "duration": 5.599
    },
    {
      "lang": "en",
      "text": "where we decide and know that here's the",
      "offset": 846.72,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "context that is relevant for the LM.",
      "offset": 848.959,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "Let's give it that context. And now",
      "offset": 851.519,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "we're And now we don't have to think",
      "offset": 854.399,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "about it anymore.\n Yeah. And it all ends",
      "offset": 855.76,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "up going into the prompt. So you could",
      "offset": 858.48,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "call this prompt engineering, but since",
      "offset": 860.079,
      "duration": 5.681
    },
    {
      "lang": "en",
      "text": "it's like basically you're ragging,",
      "offset": 862.48,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "you're you're retrieving data in a",
      "offset": 865.76,
      "duration": 3.519
    },
    {
      "lang": "en",
      "text": "custom way and feeding it to the LM in",
      "offset": 867.68,
      "duration": 2.64
    },
    {
      "lang": "en",
      "text": "the way that's going to work best for",
      "offset": 869.279,
      "duration": 2.961
    },
    {
      "lang": "en",
      "text": "the LM.",
      "offset": 870.32,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "Yeah. So we took out we took out as much",
      "offset": 872.24,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "data as possible and then just put stuff",
      "offset": 875.44,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "in. So like if you Oh, go ahead. And if",
      "offset": 877.04,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "you if you haven't seen this picture",
      "offset": 879.519,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "before, um, context engineering is",
      "offset": 880.8,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "basically",
      "offset": 883.76,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "embracing that everything that you pass",
      "offset": 885.279,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "to an LM, whether it's the prompt or the",
      "offset": 887.199,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "agentic history or something called",
      "offset": 889.04,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "memory or rag or how you tell to, it's",
      "offset": 891.12,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "all just tokens to the LM. And so you",
      "offset": 893.519,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "need to think of all of these things",
      "offset": 896,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "together and like even the order in",
      "offset": 897.279,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "which you pass these things in is a",
      "offset": 900.399,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "practice of engineering of trying",
      "offset": 902.639,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "different things and finding what works.",
      "offset": 904.48,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "And the only reason that structured",
      "offset": 906.72,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "outputs lives on the boundary here is",
      "offset": 907.92,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "because you do have to tell the LM how",
      "offset": 909.44,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "to do structured outputs in some way.",
      "offset": 911.839,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "Whether you use JSON mode, whether you",
      "offset": 913.68,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "use any SAP, any other XML, it doesn't",
      "offset": 915.199,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "really matter. That's you telling the LM",
      "offset": 917.839,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "how you want to do it. And at some",
      "offset": 919.76,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "point, you'll write some code that",
      "offset": 921.279,
      "duration": 2.961
    },
    {
      "lang": "en",
      "text": "converts the response of the LM to the",
      "offset": 922.48,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "structured output. And that's why it",
      "offset": 924.24,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "lives across the boundary of both things",
      "offset": 925.839,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "because you need regular code and I'll",
      "offset": 928.16,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "send a way to describe it to the LM as a",
      "offset": 930.959,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "part of its context on what you're",
      "offset": 932.959,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "trying to go do.",
      "offset": 934.959,
      "duration": 2.56
    },
    {
      "lang": "en",
      "text": "But everything that we're going to do",
      "offset": 936.399,
      "duration": 2.56
    },
    {
      "lang": "en",
      "text": "with an LM is always complex",
      "offset": 937.519,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "engineering.",
      "offset": 938.959,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "Cool.\n Yep.",
      "offset": 940.959,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "Uh, cool. Back to the code. You ready to",
      "offset": 943.44,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "look at the prompt?\n Let's show the",
      "offset": 945.36,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "prompt.",
      "offset": 947.519,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "Cool. So, here's our prompt. Um, let's",
      "offset": 949.199,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "see. So, we got to see which event is",
      "offset": 953.199,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "the next AI that works event. Um, so",
      "offset": 955.199,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "we're going to lur loop over there those",
      "offset": 957.12,
      "duration": 3.279
    },
    {
      "lang": "en",
      "text": "events. We're actually going to format",
      "offset": 959.199,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "them even more. Um, and so I think I",
      "offset": 960.399,
      "duration": 6.88
    },
    {
      "lang": "en",
      "text": "have a test for this. Now,",
      "offset": 964.16,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "um,",
      "offset": 967.279,
      "duration": 5.961
    },
    {
      "lang": "en",
      "text": "oh god, just make a test.",
      "offset": 968.8,
      "duration": 4.44
    },
    {
      "lang": "en",
      "text": "Was it summarize ammo?",
      "offset": 977.36,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "Yeah,",
      "offset": 980.639,
      "duration": 5.681
    },
    {
      "lang": "en",
      "text": "I'll probably make a test pretty pretty.",
      "offset": 983.36,
      "duration": 7.599
    },
    {
      "lang": "en",
      "text": "I think it'll it might make a good test.",
      "offset": 986.32,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "Uh but while we go read the prop,",
      "offset": 990.959,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "there's something some nice that we do",
      "offset": 992.32,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "which is like and the reason that Dex",
      "offset": 994.16,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "did this is because our Luma doesn't",
      "offset": 996.079,
      "duration": 5.921
    },
    {
      "lang": "en",
      "text": "only have a couple of uh events. It has",
      "offset": 998.48,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "multiple events. Sometimes we do inerson",
      "offset": 1002,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "ones, sometimes we use virtual ones. And",
      "offset": 1003.519,
      "duration": 3.521
    },
    {
      "lang": "en",
      "text": "rather than thinking about writing the",
      "offset": 1005.68,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "code to only filter for things because",
      "offset": 1007.04,
      "duration": 3.039
    },
    {
      "lang": "en",
      "text": "sometimes we forget the emoji, sometimes",
      "offset": 1008.56,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "we name it uh differently, sometimes we",
      "offset": 1010.079,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "don't call it AI that works. There's so",
      "offset": 1012,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "many reasons why the event might not",
      "offset": 1013.44,
      "duration": 3.519
    },
    {
      "lang": "en",
      "text": "match.",
      "offset": 1014.72,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "We just let it know that these are the",
      "offset": 1016.959,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "general constraints. And now we just let",
      "offset": 1019.519,
      "duration": 3.041
    },
    {
      "lang": "en",
      "text": "the LM go pick this. And this is kind of",
      "offset": 1020.88,
      "duration": 4.959
    },
    {
      "lang": "en",
      "text": "what I think the superpower of LM is,",
      "offset": 1022.56,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "which is you don't actually have to get",
      "offset": 1025.839,
      "duration": 3.041
    },
    {
      "lang": "en",
      "text": "it perfectly right. You can get it",
      "offset": 1027.28,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "mostly right. And now you can go ship",
      "offset": 1028.88,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "something really fast. It's like you",
      "offset": 1031.439,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "don't have to make your code perfectly",
      "offset": 1033.28,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "fast. You can just use Python and that's",
      "offset": 1034.64,
      "duration": 6.399
    },
    {
      "lang": "en",
      "text": "acceptable. Um",
      "offset": 1037.12,
      "duration": 7.12
    },
    {
      "lang": "en",
      "text": "you can put current date or something.",
      "offset": 1041.039,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "Yeah.",
      "offset": 1044.24,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "um",
      "offset": 1045.439,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "601 and start date is 0701.",
      "offset": 1047.28,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "Let's put it on 625.",
      "offset": 1050.799,
      "duration": 7
    },
    {
      "lang": "en",
      "text": "We'll actually do a realistic one.",
      "offset": 1053.28,
      "duration": 4.519
    },
    {
      "lang": "en",
      "text": "And this problem's pretty easy. So,",
      "offset": 1058.64,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "yeah. And this problem's pretty easy, so",
      "offset": 1061.2,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "it should just work. And can you put a",
      "offset": 1063.12,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "past state event really fast just to",
      "offset": 1064.96,
      "duration": 2.64
    },
    {
      "lang": "en",
      "text": "make sure that it does work correctly in",
      "offset": 1066.32,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "case we had no events that were correct?",
      "offset": 1067.6,
      "duration": 6.68
    },
    {
      "lang": "en",
      "text": "Um,",
      "offset": 1071.28,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "bye Bob.",
      "offset": 1076.24,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "Birthday Zoom. I don't know what your",
      "offset": 1078.16,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "birthday is, but",
      "offset": 1081.039,
      "duration": 5.121
    },
    {
      "lang": "en",
      "text": "he's turning what are you, like 22.",
      "offset": 1083.12,
      "duration": 7.88
    },
    {
      "lang": "en",
      "text": "Yep, exactly. And maybe 16 again.",
      "offset": 1086.16,
      "duration": 4.84
    },
    {
      "lang": "en",
      "text": "Um, cool.",
      "offset": 1091.36,
      "duration": 7.28
    },
    {
      "lang": "en",
      "text": "Uh,\n should be probably different. Yeah.",
      "offset": 1093.76,
      "duration": 6.56
    },
    {
      "lang": "en",
      "text": "Yeah, that's interesting. I bet this is",
      "offset": 1098.64,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "specifically a bad test because yeah,",
      "offset": 1100.32,
      "duration": 5.599
    },
    {
      "lang": "en",
      "text": "the LM will see that those IDs are just",
      "offset": 1102.559,
      "duration": 6.681
    },
    {
      "lang": "en",
      "text": "like stubs.",
      "offset": 1105.919,
      "duration": 3.321
    },
    {
      "lang": "en",
      "text": "Um anyway, yeah, that's fine.\n Yeah,",
      "offset": 1109.76,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "cool.\n Um sorry, you were talking about",
      "offset": 1112.799,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "this prompt here.\n Yeah, I want to talk",
      "offset": 1114.64,
      "duration": 3.279
    },
    {
      "lang": "en",
      "text": "about one thing in the prompt. Uh if you",
      "offset": 1116.48,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "scroll up, Dexter, look at what model",
      "offset": 1117.919,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "you use.\n Yeah.\n Uh we use a really small",
      "offset": 1119.36,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "model because this is\n Yeah, it's a super",
      "offset": 1122.32,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "easy task, so we don't have to think",
      "offset": 1124.4,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "about it. So, we just go do this. Um,",
      "offset": 1125.52,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "and then we do our, this is a best",
      "offset": 1128.16,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "effort thing. If we don't find an event,",
      "offset": 1129.919,
      "duration": 2.561
    },
    {
      "lang": "en",
      "text": "we don't find an event. It's probably a",
      "offset": 1131.28,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "user bug, so we can go fix it. It's not",
      "offset": 1132.48,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "really, we don't, this isn't super",
      "offset": 1134.16,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "critical to us. We can just go change",
      "offset": 1135.679,
      "duration": 2.961
    },
    {
      "lang": "en",
      "text": "the Luma event description and make it",
      "offset": 1137.12,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "match manually as well.\n Yep.",
      "offset": 1138.64,
      "duration": 7.52
    },
    {
      "lang": "en",
      "text": "Yeah.\n Yep.\n Where did we pass this in?",
      "offset": 1143.28,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "Sorry, I'm trying to find the",
      "offset": 1146.16,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "trying to go find where we called this",
      "offset": 1149.28,
      "duration": 5.519
    },
    {
      "lang": "en",
      "text": "thing.\n You can just command click",
      "offset": 1150.72,
      "duration": 7.199
    },
    {
      "lang": "en",
      "text": "from here.",
      "offset": 1154.799,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "Oh, not from there. Oh, you have to",
      "offset": 1158.4,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "search this command F. Yeah,\n here we go.",
      "offset": 1159.84,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "Yeah, sorry. So, if we don't get a",
      "offset": 1162.16,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "result, we say, &quot;Hey, we couldn't find",
      "offset": 1163.52,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "the next event.&quot; Um, otherwise, we go",
      "offset": 1165.12,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "pull that event out and we return it.",
      "offset": 1168.4,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "Yep, that's it. So, it's a pretty",
      "offset": 1170.96,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "straightforward thing. We just rely on",
      "offset": 1172.72,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "the ID matching to do the trick. Um, so",
      "offset": 1174,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "we return our original Duma event that",
      "offset": 1176.559,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "is enriched, but the LM only produces",
      "offset": 1178.16,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "the one that is not as enriched.",
      "offset": 1180.24,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "So, it gives us a better Go ahead. Yeah,",
      "offset": 1182.88,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "I was going to say and like I don't",
      "offset": 1185.84,
      "duration": 2
    },
    {
      "lang": "en",
      "text": "think we should spend time on this",
      "offset": 1186.799,
      "duration": 2.721
    },
    {
      "lang": "en",
      "text": "today, but this would feels like a",
      "offset": 1187.84,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "perfect example of like where you're",
      "offset": 1189.52,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "always talking about like don't use the",
      "offset": 1191.52,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "actual native IDs because they can be",
      "offset": 1192.96,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "long, but like replace this with like",
      "offset": 1194.72,
      "duration": 7.199
    },
    {
      "lang": "en",
      "text": "K1, K2, K3 kind of, right?\n Exly it or",
      "offset": 1197.679,
      "duration": 6.161
    },
    {
      "lang": "en",
      "text": "just like event ID one. In this context,",
      "offset": 1201.919,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "I would call it like event underscore 1",
      "offset": 1203.84,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "2 3 4 5 6. It's just going to be easier",
      "offset": 1206.88,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "for the model.\n Yeah. Yeah. Yeah.\n It's",
      "offset": 1208.72,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "one less thing the model has to think",
      "offset": 1211.44,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "about.\n Yeah.",
      "offset": 1212.88,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "Um, cool. Anyways, that's that code.",
      "offset": 1215.36,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "Let's pop.\n And then one last piece of",
      "offset": 1218.48,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "code I want to show, which is the folder",
      "offset": 1220.08,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "code. The folder matching code. This is",
      "offset": 1221.679,
      "duration": 5.681
    },
    {
      "lang": "en",
      "text": "also kind of\n Oh, yeah.\n Yep.\n Actually do",
      "offset": 1223.44,
      "duration": 5.359
    },
    {
      "lang": "en",
      "text": "all of this stuff to go produce the",
      "offset": 1227.36,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "GitHub reviews to go send the right",
      "offset": 1228.799,
      "duration": 3.521
    },
    {
      "lang": "en",
      "text": "email to the right folder.\n Like the",
      "offset": 1230.32,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "email can't really be sent until we have",
      "offset": 1232.32,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "the GitHub uh PR ready as well because",
      "offset": 1233.919,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "we need we want to link you to the right",
      "offset": 1236.96,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "folder ideally. So, how are we going to",
      "offset": 1238.64,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "go do that?",
      "offset": 1240.88,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "Yeah, exactly. Kyle nailed it. It's like",
      "offset": 1243.52,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "it it just it's just it's the token",
      "offset": 1245.76,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "probability thing. Tokens that are",
      "offset": 1248.559,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "random are just not in the data set of",
      "offset": 1250.24,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "the model. So, it's just going to at",
      "offset": 1252,
      "duration": 2.799
    },
    {
      "lang": "en",
      "text": "some point start it's going to be like",
      "offset": 1253.12,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "I'm printing out a UU ID. Let me just",
      "offset": 1254.799,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "print out UU IDs and those are basically",
      "offset": 1256.32,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "random. And you're you're praying that",
      "offset": 1259.12,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "the LM is really really really good at",
      "offset": 1261.44,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "needle hack tests.\n Yep.",
      "offset": 1263.28,
      "duration": 5.759
    },
    {
      "lang": "en",
      "text": "Um, so what we're going to do is we're",
      "offset": 1267.36,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "going to fetch the existing folders and",
      "offset": 1269.039,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "some file contents. We're going to get",
      "offset": 1271.2,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "the next folder name and a couple other",
      "offset": 1272.96,
      "duration": 2.4
    },
    {
      "lang": "en",
      "text": "things. We're going to use the next",
      "offset": 1274.4,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "folder name to create the readme for the",
      "offset": 1275.36,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "new branch. Um, so this readme we had in",
      "offset": 1277.52,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "our pull request. This file the our",
      "offset": 1280.24,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "application needs to know this full",
      "offset": 1282.96,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "exact path. So it needs to know the",
      "offset": 1284.48,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "folder name and that folder may already",
      "offset": 1286,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "exist on the main branch or it might not",
      "offset": 1288.4,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "and we need to generate it. And so we",
      "offset": 1291.039,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "need like this is a thing you could",
      "offset": 1292.88,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "probably write a bunch of code for to",
      "offset": 1294.799,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "determinate like build deterministically",
      "offset": 1296.32,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "every time. It would be really hard to",
      "offset": 1298.4,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "build the actual slug like the the name",
      "offset": 1299.6,
      "duration": 4.959
    },
    {
      "lang": "en",
      "text": "for the folder. Um so what we did was we",
      "offset": 1301.84,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "took the existing list of folders and",
      "offset": 1304.559,
      "duration": 5.681
    },
    {
      "lang": "en",
      "text": "I'll show you the code and then we took",
      "offset": 1307.2,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "the title of the episode and the date",
      "offset": 1310.24,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "and we said hey AI model like either",
      "offset": 1312.08,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "pick the existing folder if it's already",
      "offset": 1314.48,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "there or generate the name of the new",
      "offset": 1316.24,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "folder if it's not there. And all of",
      "offset": 1318.159,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "these get added. We do this manually",
      "offset": 1320.159,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "every every week. They get added to this",
      "offset": 1321.919,
      "duration": 5.841
    },
    {
      "lang": "en",
      "text": "like repo like route the list of",
      "offset": 1324.96,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "episodes.",
      "offset": 1327.76,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "Um\n can you go down really fast and also",
      "offset": 1329.84,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "mention the other part the table?",
      "offset": 1331.76,
      "duration": 6.159
    },
    {
      "lang": "en",
      "text": "Uh oh yeah and then we also update\n Yeah,",
      "offset": 1335.2,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "we also update the table. Yeah, good",
      "offset": 1337.919,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "call. Let me show So let me show what",
      "offset": 1339.679,
      "duration": 2.561
    },
    {
      "lang": "en",
      "text": "that looks like. And there's there's",
      "offset": 1341.12,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "basically every episode every week we",
      "offset": 1342.24,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "push the YouTube link and the code link",
      "offset": 1345.84,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "for the past episode and then we post",
      "offset": 1348.48,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "the description about the next episode",
      "offset": 1350.48,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "and we have the Luma link and so that's",
      "offset": 1352.72,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "why in order to do the GitHub updates we",
      "offset": 1355.039,
      "duration": 6.64
    },
    {
      "lang": "en",
      "text": "need not only the uh past stuff but we",
      "offset": 1358.24,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "also need the next event and its link.",
      "offset": 1361.679,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "So we need the title and the link to the",
      "offset": 1364.4,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "next event so that we can populate",
      "offset": 1365.84,
      "duration": 6.4
    },
    {
      "lang": "en",
      "text": "this uh where is this um",
      "offset": 1368.48,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "this table as well.\n And the whole and",
      "offset": 1372.24,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "the whole point of going really deep",
      "offset": 1374.32,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "into these concepts is talking about how",
      "offset": 1375.84,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "much work it takes to get tone just",
      "offset": 1377.76,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "right. Like in order to get the tone",
      "offset": 1379.6,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "perfectly right on the email, the first",
      "offset": 1381.28,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "thing we really have to get is like we",
      "offset": 1382.72,
      "duration": 2.959
    },
    {
      "lang": "en",
      "text": "have to break down the problem to make",
      "offset": 1384.32,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "sure that the data is correct. If the",
      "offset": 1385.679,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "factual data is wrong, the tone doesn't",
      "offset": 1387.919,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "matter because I'll just read it and it",
      "offset": 1389.76,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "won't work. But we have to actually get",
      "offset": 1391.52,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "all of that correct and if we don't have",
      "offset": 1393.44,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "the right information. Yeah. Well, oh",
      "offset": 1395.12,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "like I just reating what you said like",
      "offset": 1397.6,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "the to generate to like solve this",
      "offset": 1399.679,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "problem we need to know what the",
      "offset": 1402.08,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "boundaries what the walls of the box",
      "offset": 1403.52,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "are. I think I want to say where it's",
      "offset": 1405.84,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "like okay we're going to have to this is",
      "offset": 1407.36,
      "duration": 2.559
    },
    {
      "lang": "en",
      "text": "going to be a hard-coded thing that",
      "offset": 1408.88,
      "duration": 2.159
    },
    {
      "lang": "en",
      "text": "we're going to have to have to force in.",
      "offset": 1409.919,
      "duration": 2.401
    },
    {
      "lang": "en",
      "text": "I don't want to make the AI generate",
      "offset": 1411.039,
      "duration": 2.64
    },
    {
      "lang": "en",
      "text": "that. This is going to be a hard-coded",
      "offset": 1412.32,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "thing. And so that kind of starts to",
      "offset": 1413.679,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "structure. Okay, what are we going to",
      "offset": 1415.76,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "ask the AI to do and what do we want to",
      "offset": 1417.36,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "be deterministic? Because once we have",
      "offset": 1419.919,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "that structure and those pieces, then we",
      "offset": 1422,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "can start to tone engineer, content",
      "offset": 1424.159,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "engineer each of the different parts of",
      "offset": 1426.48,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "that LM's workflow and break down the",
      "offset": 1428.08,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "problem.\n Exactly.\n And then we're",
      "offset": 1430,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "basically just trying to break down",
      "offset": 1431.919,
      "duration": 3.041
    },
    {
      "lang": "en",
      "text": "exactly what said. We just try to break",
      "offset": 1433.44,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "down the problem as much as possible to",
      "offset": 1434.96,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "get it perfect. And that's because we",
      "offset": 1437.12,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "care about the tone of the email going",
      "offset": 1438.72,
      "duration": 2.56
    },
    {
      "lang": "en",
      "text": "out. And we really just want it to be",
      "offset": 1440.08,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "automated, but we also don't we'd rather",
      "offset": 1441.28,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "do the work manually than send out",
      "offset": 1443.76,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "something automated that's shitty. So",
      "offset": 1445.36,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "given that work, we have to put in a lot",
      "offset": 1448,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "more effort to make it happy. It's",
      "offset": 1450.4,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "unlikely that you can get to the perfect",
      "offset": 1452.159,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "tone with just prompting. You will have",
      "offset": 1453.84,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "mistakes like hello first name. You will",
      "offset": 1456.4,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "have um mistakes like wrong YouTube",
      "offset": 1458.559,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "URLs. You'll have mistakes like wrong",
      "offset": 1461.36,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "luma links. And those are basically",
      "offset": 1462.64,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "catastrophic in these scenarios. like it",
      "offset": 1465.12,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "just loses a lot of trust. So, we want",
      "offset": 1466.88,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "to make sure those all things get",
      "offset": 1469.12,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "squared away.",
      "offset": 1470.559,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "Um, answer. Yeah, sorry. Quickly answer",
      "offset": 1472.64,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "one of the questions from the chat,",
      "offset": 1475.76,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "which was where are the where are the",
      "offset": 1477.039,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "diagrams? Um, I did move this file to",
      "offset": 1479.12,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "readme.md.backup",
      "offset": 1482.159,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "just so we could test the PR stuff. But,",
      "offset": 1483.76,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "um, let's let me just put it back so you",
      "offset": 1486.08,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "can see it.\n This will have all of the",
      "offset": 1488.08,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "whiteboards in it. Um, so you can see",
      "offset": 1490.88,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "all the whiteboards from last week in",
      "offset": 1492.799,
      "duration": 3.281
    },
    {
      "lang": "en",
      "text": "the previous episode notes. the new one",
      "offset": 1494.4,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "that you're talking about, uh, Pashant,",
      "offset": 1496.08,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "the actual architecture, that will be",
      "offset": 1498.559,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "coming in in this week's episode.\n Yeah,",
      "offset": 1499.919,
      "duration": 5.801
    },
    {
      "lang": "en",
      "text": "this one.",
      "offset": 1502.48,
      "duration": 3.24
    },
    {
      "lang": "en",
      "text": "Um, cool. Uh, we haven't seen spiral",
      "offset": 1505.84,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "computer. Let's to go back to the folder",
      "offset": 1508.72,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "code deck. You want to pull that up and",
      "offset": 1510.24,
      "duration": 2.48
    },
    {
      "lang": "en",
      "text": "then we'll start prompt engineering",
      "offset": 1511.52,
      "duration": 2.639
    },
    {
      "lang": "en",
      "text": "after we do that and actually talk about",
      "offset": 1512.72,
      "duration": 2.959
    },
    {
      "lang": "en",
      "text": "the email content.\n Yeah. Yeah. Yeah.",
      "offset": 1514.159,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "Yeah.",
      "offset": 1515.679,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "Yeah.\n Go ahead. Are you supporting",
      "offset": 1517.84,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "native Rust? Uh, we will. We're I think",
      "offset": 1519.919,
      "duration": 3.521
    },
    {
      "lang": "en",
      "text": "today we're going to release Go support",
      "offset": 1522.08,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "and then Rust support should be easy.",
      "offset": 1523.44,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "But let's talk about this today. I want",
      "offset": 1525.84,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "to talk about the folder content.\n Yeah.",
      "offset": 1527.52,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "So um the workflow for GitHub is again",
      "offset": 1530.4,
      "duration": 6.879
    },
    {
      "lang": "en",
      "text": "we will figure out the episode repo path",
      "offset": 1534,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "which is like that's how we generate the",
      "offset": 1537.279,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "folder name and then we generate the",
      "offset": 1538.72,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "readme for the episode and then we",
      "offset": 1540.64,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "generate the updates for the root readme",
      "offset": 1542.159,
      "duration": 7.52
    },
    {
      "lang": "en",
      "text": "and then we are going to basically use a",
      "offset": 1545.44,
      "duration": 6.56
    },
    {
      "lang": "en",
      "text": "uh a repo called supersonic from the",
      "offset": 1549.679,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "case folks that lets you just create",
      "offset": 1552,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "GitHub PRs in a single line. So we give",
      "offset": 1553.679,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "the repo name, we give it the files we",
      "offset": 1555.76,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "want to update. Yeah, it's dope.",
      "offset": 1557.84,
      "duration": 6.88
    },
    {
      "lang": "en",
      "text": "Um and then the branch named",
      "offset": 1560.88,
      "duration": 5.279
    },
    {
      "lang": "en",
      "text": "Yeah. A couple of the things that we're",
      "offset": 1564.72,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "doing. Can you go down?\n Yeah.\n Um uh",
      "offset": 1566.159,
      "duration": 5.841
    },
    {
      "lang": "en",
      "text": "which is we're actually hard coding",
      "offset": 1569.76,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "these file names. We're not letting the",
      "offset": 1572,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "LLM make the like go up.",
      "offset": 1573.36,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "Uh sorry, one more. Uh right here in the",
      "offset": 1576.24,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "PR.\n Technically, we could have made this",
      "offset": 1578.08,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "whole thing a tool call.\n This could have",
      "offset": 1580.32,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "been a tool call where the LM decides",
      "offset": 1582.64,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "what PR to create. The reason we didn't",
      "offset": 1584.24,
      "duration": 5.919
    },
    {
      "lang": "en",
      "text": "do that is because we know that the LM",
      "offset": 1587.2,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "should only ever modify two files, two",
      "offset": 1590.159,
      "duration": 3.281
    },
    {
      "lang": "en",
      "text": "readmemes. So, we just won't even give",
      "offset": 1591.919,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "it that choice. And this goes back to",
      "offset": 1593.44,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "what we talked about again. It's like",
      "offset": 1596,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "whenever we build an LM pipeline, we as",
      "offset": 1597.52,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "a team always get a choice of how much",
      "offset": 1599.52,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "flexibility we give the LM. There's no",
      "offset": 1602,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "flexibility, which is we hardcode",
      "offset": 1604.64,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "everything. And then there's full",
      "offset": 1606.32,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "flexibility where the agent is literally",
      "offset": 1608.32,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "making every single decision and you",
      "offset": 1610.159,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "have no programmatic code except mapping",
      "offset": 1612,
      "duration": 5.279
    },
    {
      "lang": "en",
      "text": "the data model to the tool call and",
      "offset": 1614.48,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "nothing else.\n And I I can tell you",
      "offset": 1617.279,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "exactly what would happen if we had the",
      "offset": 1619.36,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "LLM to call this tool is I would spend I",
      "offset": 1621.279,
      "duration": 5.601
    },
    {
      "lang": "en",
      "text": "would be like you have a tool to create",
      "offset": 1625.44,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "PRs and then I would write like a two",
      "offset": 1626.88,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "sentence prompt that would be like only",
      "offset": 1628.96,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "create this file and this file and",
      "offset": 1630.72,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "here's how you should create those",
      "offset": 1632.48,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "files. And then it would [Â __Â ] it up and",
      "offset": 1633.919,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "it would do something wrong and then it",
      "offset": 1635.84,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "would uh and then it would create and",
      "offset": 1638,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "then I would keep making the prompt",
      "offset": 1640.559,
      "duration": 2.881
    },
    {
      "lang": "en",
      "text": "bigger and bigger and I would have all",
      "offset": 1641.919,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "caps, you must only ever create these",
      "offset": 1643.44,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "files. And it's like if you already know",
      "offset": 1645.6,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "exactly what it should be, like why are",
      "offset": 1648.159,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "you trying to turn your own words, push",
      "offset": 1650,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "it through an LLM and output the JSON or",
      "offset": 1652.72,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "the code for what should happen. If you",
      "offset": 1655.039,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "already know exactly what should happen,",
      "offset": 1657.279,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "let the LM do what it does well, which",
      "offset": 1658.96,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "is generate the read me, generate the",
      "offset": 1661.279,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "readme, and generate the folder, and you",
      "offset": 1663.36,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "own everything else. If you know what it",
      "offset": 1665.76,
      "duration": 2.48
    },
    {
      "lang": "en",
      "text": "is, and it's going to be the same every",
      "offset": 1667.2,
      "duration": 2.56
    },
    {
      "lang": "en",
      "text": "time, yeah, it's less flexible, and you",
      "offset": 1668.24,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "can't use it as a general purpose agent",
      "offset": 1669.76,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "that does everything, but like again,",
      "offset": 1671.36,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "but we don't need that here.\n We don't",
      "offset": 1674.96,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "need that. We just need to automate the",
      "offset": 1676.72,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "things that we know we need to automate.",
      "offset": 1678.159,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "Yeah.\n And this goes back to thinking",
      "offset": 1680.32,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "really hard about what is the input span",
      "offset": 1682.399,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "of the problem that you're solving. So",
      "offset": 1684.48,
      "duration": 2.48
    },
    {
      "lang": "en",
      "text": "like the input span of the problem we're",
      "offset": 1685.679,
      "duration": 3.041
    },
    {
      "lang": "en",
      "text": "solving could be everything. But in",
      "offset": 1686.96,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "dextrous and I case we've decided that",
      "offset": 1688.72,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "we only need to automate this part of",
      "offset": 1691.36,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "the workflow which is a very small slice",
      "offset": 1694.32,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "of the world which is not the whole",
      "offset": 1696.72,
      "duration": 4.959
    },
    {
      "lang": "en",
      "text": "thing which is all we want to do is take",
      "offset": 1698,
      "duration": 5.679
    },
    {
      "lang": "en",
      "text": "content from zoom and produce a PR from",
      "offset": 1701.679,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "it given the content that we have. So we",
      "offset": 1703.679,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "can throw away all the other workflows",
      "offset": 1706.08,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "and not spend any time thinking about",
      "offset": 1708.48,
      "duration": 5.439
    },
    {
      "lang": "en",
      "text": "them and that helps us basically",
      "offset": 1710.48,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "constrain what we have to go get the LM",
      "offset": 1713.919,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "to do. If we did have to make the full",
      "offset": 1716,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "PR for everything including the code",
      "offset": 1717.6,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "then we expand it or we can",
      "offset": 1720,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "incrementally expand it over time as",
      "offset": 1721.919,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "well.",
      "offset": 1724,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "And\n and it's like automate with AI,",
      "offset": 1725.679,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "automate with code and automate",
      "offset": 1728.08,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "manually, right?\n Yeah. And Joseph is",
      "offset": 1730.64,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "asking a really interesting question",
      "offset": 1733.76,
      "duration": 2.24
    },
    {
      "lang": "en",
      "text": "like are we generating everything",
      "offset": 1734.88,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "sequentially or partially or like what",
      "offset": 1736,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "does latency look like? Well, the good",
      "offset": 1738.24,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "part here is it's really up to us as a",
      "offset": 1740.159,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "developer how much we want to go ahead",
      "offset": 1742.799,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "and go do.\n Um in our case, I think we do",
      "offset": 1744.48,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "we do the summarization step",
      "offset": 1748.799,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "sequentially because it's a dependency",
      "offset": 1750.72,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "and then we try and do all of these in",
      "offset": 1752.64,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "parallel but now because we know the",
      "offset": 1754.159,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "GitHub fold the email will actually",
      "offset": 1756.72,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "depend on the folder name and the GitHub",
      "offset": 1758.559,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "repo. Now the email has a dependency on",
      "offset": 1760.08,
      "duration": 7.28
    },
    {
      "lang": "en",
      "text": "the actual readme.",
      "offset": 1764.399,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "Now we can remove that dependency from",
      "offset": 1767.36,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "being as strong simply by saying that",
      "offset": 1769.039,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "we'll create an intermediate step which",
      "offset": 1771.279,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "is a new folder name and that becomes a",
      "offset": 1772.64,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "dependency for the email. So now the",
      "offset": 1774.48,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "email and the root can run in parallel",
      "offset": 1775.76,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "and we use really really small steps",
      "offset": 1778.559,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "that are actual dependencies along the",
      "offset": 1780.72,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "way. And this graph starts to look",
      "offset": 1782.799,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "really really complicated because that's",
      "offset": 1785.52,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "software. Software is complicated. And",
      "offset": 1787.279,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "the part about it that's complicated is",
      "offset": 1789.919,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "not the AI part. It's just like these",
      "offset": 1791.44,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "questions that Dex and I ask",
      "offset": 1793.919,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "architecturally that make it complicated",
      "offset": 1795.36,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "and sophisticated over time. Um like the",
      "offset": 1797.679,
      "duration": 5.521
    },
    {
      "lang": "en",
      "text": "parallels question Joseph is asking.",
      "offset": 1801.44,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "That's the question that we have to ask.",
      "offset": 1803.2,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "We could do everything sequentially.",
      "offset": 1804.96,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "We'd like not to, but we could. And this",
      "offset": 1807.44,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "is exactly why this sort of thinking I",
      "offset": 1810.48,
      "duration": 6.319
    },
    {
      "lang": "en",
      "text": "find to be personally very very useful.",
      "offset": 1813.52,
      "duration": 6.56
    },
    {
      "lang": "en",
      "text": "Um with that, let's continue.",
      "offset": 1816.799,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "Uh let's actually talk about uh I think",
      "offset": 1820.08,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "you showed the folder prompt.\n I showed",
      "offset": 1822.559,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "this folder prompt. I was thinking we",
      "offset": 1824.64,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "could look briefly at the readme prompts",
      "offset": 1825.919,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "as well. Uh well so I showed the folder",
      "offset": 1827.84,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "function here. This is where we so this",
      "offset": 1830.32,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "is where we get the path.\n And so here is",
      "offset": 1832.24,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "our examples of repo names. And so what",
      "offset": 1835.36,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "we're going to do is use another repo",
      "offset": 1837.52,
      "duration": 2.879
    },
    {
      "lang": "en",
      "text": "called kit from the same people who do",
      "offset": 1838.72,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "the PR stuff. This is kit is a\n toolkit",
      "offset": 1840.399,
      "duration": 7.121
    },
    {
      "lang": "en",
      "text": "for getting from remote repos. highly",
      "offset": 1843.52,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "recommend for anyone building any GitHub",
      "offset": 1847.52,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "automation. It's very very good.\n Yeah.",
      "offset": 1849.12,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "Um yeah, from kit import repository. So",
      "offset": 1851.84,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "we're going to go like loop over the f",
      "offset": 1854.72,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "file tree and then we basically take the",
      "offset": 1856.48,
      "duration": 5.439
    },
    {
      "lang": "en",
      "text": "folders and it's just f of path. So",
      "offset": 1859.76,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "we're just going to pass in a list of",
      "offset": 1861.919,
      "duration": 5.041
    },
    {
      "lang": "en",
      "text": "folders. Um I'm actually\n notice again we",
      "offset": 1863.2,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "could have passed this whole thing to",
      "offset": 1866.96,
      "duration": 2.959
    },
    {
      "lang": "en",
      "text": "the lm, but Dexter was like this took me",
      "offset": 1868,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "two seconds of code. So I'll limit it to",
      "offset": 1869.919,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "just the folder and only the root code.",
      "offset": 1871.6,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "I won't think about it.\n Yeah. Yeah. Not",
      "offset": 1873.6,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "saying\n literally Claude just Claude just",
      "offset": 1876,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "wrote this function with very little",
      "offset": 1878.32,
      "duration": 2.959
    },
    {
      "lang": "en",
      "text": "input. I was just like here's how we're",
      "offset": 1879.84,
      "duration": 2.319
    },
    {
      "lang": "en",
      "text": "going to do this and it was like all",
      "offset": 1881.279,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "right let's go.\n Yeah. And again and it's",
      "offset": 1882.159,
      "duration": 5.361
    },
    {
      "lang": "en",
      "text": "just slightly better than passing",
      "offset": 1886,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "everything to the model. And then we",
      "offset": 1887.52,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "just ask the model to tell us what given",
      "offset": 1889.12,
      "duration": 3.279
    },
    {
      "lang": "en",
      "text": "the current video title what is the best",
      "offset": 1890.88,
      "duration": 3.039
    },
    {
      "lang": "en",
      "text": "folder.",
      "offset": 1892.399,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "Yeah. So we can say AI the what was the",
      "offset": 1893.919,
      "duration": 6.081
    },
    {
      "lang": "en",
      "text": "video title was like AI content",
      "offset": 1897.279,
      "duration": 8.481
    },
    {
      "lang": "en",
      "text": "pipeline.\n Yeah. And the thing was 06",
      "offset": 1900,
      "duration": 7.84
    },
    {
      "lang": "en",
      "text": "24",
      "offset": 1905.76,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "and the existing folders were you know",
      "offset": 1907.84,
      "duration": 6.88
    },
    {
      "lang": "en",
      "text": "0617 and 0610",
      "offset": 1910.72,
      "duration": 7.559
    },
    {
      "lang": "en",
      "text": "something cool",
      "offset": 1914.72,
      "duration": 3.559
    },
    {
      "lang": "en",
      "text": "something else cooler\n and then you can",
      "offset": 1922.159,
      "duration": 6.081
    },
    {
      "lang": "en",
      "text": "basically run this test. And so what",
      "offset": 1925.679,
      "duration": 5.281
    },
    {
      "lang": "en",
      "text": "this this what this returns actually is",
      "offset": 1928.24,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "a episode path result which is the path",
      "offset": 1930.96,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "and whether it already exists whether",
      "offset": 1933.76,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "it's new or not and we're going to use",
      "offset": 1935.36,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "that.\n Um so it says okay cool episode",
      "offset": 1936.88,
      "duration": 6.159
    },
    {
      "lang": "en",
      "text": "path content pipeline is new true.",
      "offset": 1940.24,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "Great. And then we can make another one",
      "offset": 1943.039,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "to Yeah. Go ahead. And the reason that",
      "offset": 1944.64,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "we don't just use like a standard slug",
      "offset": 1946.72,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "one, by the way, over here",
      "offset": 1948.48,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "is because a standard slug approach is",
      "offset": 1951.12,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "actually going to be a little bit more",
      "offset": 1953.76,
      "duration": 3.039
    },
    {
      "lang": "en",
      "text": "annoying than you'd expect because",
      "offset": 1954.799,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "turning our Zoom titles into slugs",
      "offset": 1956.799,
      "duration": 3.281
    },
    {
      "lang": "en",
      "text": "doesn't actually mean that they make",
      "offset": 1958.96,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "good folder names.\n Mhm.\n So that's why we",
      "offset": 1960.08,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "used an LM here.",
      "offset": 1963.6,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "And actually, I'm just going to use our",
      "offset": 1966,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "real folder names here.\n That's probably",
      "offset": 1967.2,
      "duration": 6.319
    },
    {
      "lang": "en",
      "text": "better.\n Oops. No, no, no, no. Not paste",
      "offset": 1969.84,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "an entire Excalibur into",
      "offset": 1973.519,
      "duration": 6.441
    },
    {
      "lang": "en",
      "text": "Son, let's do that.",
      "offset": 1976.08,
      "duration": 3.88
    },
    {
      "lang": "en",
      "text": "And Dexter is one of the best people I",
      "offset": 1980.399,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "know to actually do all the stuff. He's",
      "offset": 1981.919,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "like he's just leveraging AI to like do",
      "offset": 1984.32,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "codegen really fast, which would have",
      "offset": 1986.24,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "been a copy and paste job. It's just",
      "offset": 1987.679,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "very convenient.",
      "offset": 1990.159,
      "duration": 3.281
    },
    {
      "lang": "en",
      "text": "And then you can get rid of AI content",
      "offset": 1992.08,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "pipeline, too.",
      "offset": 1993.44,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "Well, I want I wanted to detect one that",
      "offset": 1996.64,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "already exists.\n Oh, yes, it did.\n So,",
      "offset": 1998.88,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "here it is. So, it figured out the",
      "offset": 2000.48,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "episode path. So you could do some math",
      "offset": 2001.76,
      "duration": 3.279
    },
    {
      "lang": "en",
      "text": "and be like, &quot;Okay, does the date",
      "offset": 2003.84,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "match?&quot; Then pull that one out. But um",
      "offset": 2005.039,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "and yeah, the giving an explanation on",
      "offset": 2007.12,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "great for this",
      "offset": 2009.2,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "an exact match title also matches the",
      "offset": 2011.519,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "folder topic since there's an exact",
      "offset": 2013.919,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "match. Yeah, exactly.",
      "offset": 2015.44,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "All right. Now, let's do the I think the",
      "offset": 2017.76,
      "duration": 2.799
    },
    {
      "lang": "en",
      "text": "part that most people are here for,",
      "offset": 2019.279,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "which is let's make the prompts actually",
      "offset": 2020.559,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "good.\n Yeah. Do you want to drive for",
      "offset": 2022.64,
      "duration": 5.519
    },
    {
      "lang": "en",
      "text": "this part?\n Yeah, I will. Um so I think",
      "offset": 2024.399,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "one last question. Is there any tooling",
      "offset": 2028.159,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "to hook in evaluation steps? in between",
      "offset": 2029.6,
      "duration": 7.04
    },
    {
      "lang": "en",
      "text": "these steps. Um, yes, I think it's\n Yeah.",
      "offset": 2032.72,
      "duration": 5.839
    },
    {
      "lang": "en",
      "text": "Okay. Yeah, that's okay.\n I mean, not in",
      "offset": 2036.64,
      "duration": 3.519
    },
    {
      "lang": "en",
      "text": "our system. I mean, I So, I'll just I'll",
      "offset": 2038.559,
      "duration": 3.281
    },
    {
      "lang": "en",
      "text": "just uh I'll just say that I've been",
      "offset": 2040.159,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "using the like the the boundary cloud",
      "offset": 2041.84,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "stuff and it's really really good. I",
      "offset": 2043.919,
      "duration": 2.48
    },
    {
      "lang": "en",
      "text": "don't know if that's like publicly",
      "offset": 2045.2,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "available yet, but um you can record",
      "offset": 2046.399,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "every input output pair and use it to",
      "offset": 2049.44,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "generate tests and eval at least at like",
      "offset": 2051.04,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "the per prompt level. um the end to end",
      "offset": 2053.2,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "eval stuff. If you want to learn more,",
      "offset": 2055.839,
      "duration": 2.881
    },
    {
      "lang": "en",
      "text": "we do a bunch of stuff on that on a",
      "offset": 2057.28,
      "duration": 2.799
    },
    {
      "lang": "en",
      "text": "diff. We did an episode a couple weeks",
      "offset": 2058.72,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "ago all about evals and the different",
      "offset": 2060.079,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "like tiers of like really unit testy",
      "offset": 2062.159,
      "duration": 5.361
    },
    {
      "lang": "en",
      "text": "evals and then vibe evals and like human",
      "offset": 2064.48,
      "duration": 5.439
    },
    {
      "lang": "en",
      "text": "evals versus like more structured evals,",
      "offset": 2067.52,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "all that kind of stuff.\n Yeah. Uh that",
      "offset": 2069.919,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "video probably does better, but it eval",
      "offset": 2072.56,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "sadly just take effort and like people I",
      "offset": 2075.119,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "think they want the same prompts where",
      "offset": 2077.2,
      "duration": 2.399
    },
    {
      "lang": "en",
      "text": "they just want a prompt that works and",
      "offset": 2078.48,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "magically makes a perfect email.",
      "offset": 2079.599,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "There's no prompt that will do that",
      "offset": 2083.28,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "magically. At least not yet. Maybe in",
      "offset": 2084.879,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "like another six months, but as with",
      "offset": 2086.48,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "today, not yet. And same thing with",
      "offset": 2088.879,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "emails.\n Um, I'm going to screen share.",
      "offset": 2090.48,
      "duration": 6.879
    },
    {
      "lang": "en",
      "text": "Um, I will attempt to not show API keys.",
      "offset": 2093.919,
      "duration": 5.281
    },
    {
      "lang": "en",
      "text": "Um,",
      "offset": 2097.359,
      "duration": 5.921
    },
    {
      "lang": "en",
      "text": "sorry, this is my ghost stuff.",
      "offset": 2099.2,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "All",
      "offset": 2103.28,
      "duration": 2
    },
    {
      "lang": "en",
      "text": "right, there we go. Okay,",
      "offset": 2111.2,
      "duration": 7.8
    },
    {
      "lang": "en",
      "text": "um vertex",
      "offset": 2115.04,
      "duration": 3.96
    },
    {
      "lang": "en",
      "text": "patient. Sorry, that's testing on",
      "offset": 2120.079,
      "duration": 6.161
    },
    {
      "lang": "en",
      "text": "different models along the way.",
      "offset": 2122.72,
      "duration": 7.639
    },
    {
      "lang": "en",
      "text": "Uh please",
      "offset": 2126.24,
      "duration": 4.119
    },
    {
      "lang": "en",
      "text": "okay.",
      "offset": 2132,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "So last time when we were doing this,",
      "offset": 2138.48,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "one of the things I think we noticed was",
      "offset": 2139.92,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "um",
      "offset": 2142.32,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "the email draft was not very good um for",
      "offset": 2144.56,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "a couple reasons. I think and I think",
      "offset": 2147.44,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "part of the reason that the draft wasn't",
      "offset": 2149.44,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "very good was actually that the summary",
      "offset": 2150.8,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "was quite bad.",
      "offset": 2152.24,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "Oh yo, you got to do real time codes.",
      "offset": 2154.24,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "Remember we did it last week and the",
      "offset": 2157.04,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "time codes were all in just like 15inute",
      "offset": 2158.24,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "increments. Okay.\n Yeah. So, it turns out",
      "offset": 2160,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "one of the best models for doing this is",
      "offset": 2162.4,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "actually Gemini.\n Um, and this is just it",
      "offset": 2164.16,
      "duration": 4.959
    },
    {
      "lang": "en",
      "text": "just takes hacking with a prompt to go",
      "offset": 2167.68,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "do this. Last time when we did this, um,",
      "offset": 2169.119,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "generate email structure. Uh, where's",
      "offset": 2171.92,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "the summary? Sorry.",
      "offset": 2173.68,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "Uh, generate email drafts. I was working",
      "offset": 2175.839,
      "duration": 5.041
    },
    {
      "lang": "en",
      "text": "on this.",
      "offset": 2178.72,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "Last time when we did this, we weren't",
      "offset": 2180.88,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "using Gemini. We were using OpenAI",
      "offset": 2182.24,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "GPT40.",
      "offset": 2184.16,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "And there's a couple things wrong with",
      "offset": 2186.32,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "this, but like I'll run this really fast",
      "offset": 2187.52,
      "duration": 6.04
    },
    {
      "lang": "en",
      "text": "just to show what happens.",
      "offset": 2189.92,
      "duration": 3.64
    },
    {
      "lang": "en",
      "text": "And this just goes to show like really",
      "offset": 2195.68,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "we talk about this stuff all the time,",
      "offset": 2197.839,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "but like I guess Gemma is doing like",
      "offset": 2199.2,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "decent time codes this time,",
      "offset": 2201.599,
      "duration": 6.641
    },
    {
      "lang": "en",
      "text": "but like these times start to get like a",
      "offset": 2204.4,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "as we go into this, but you'll notice",
      "offset": 2208.24,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "this is much better.",
      "offset": 2209.76,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "This is actually much better than we're",
      "offset": 2211.76,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "doing.\n Did you make this prank better or",
      "offset": 2212.96,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "something?",
      "offset": 2214.56,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "I may have also done that, but I don't",
      "offset": 2216.96,
      "duration": 3.119
    },
    {
      "lang": "en",
      "text": "actually think so. Time data is still a",
      "offset": 2218.64,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "string. Um,",
      "offset": 2220.079,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "I think if I swatch it, swap it to time",
      "offset": 2222.079,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "data. I'll try again.",
      "offset": 2224.24,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "Uh, GT40.",
      "offset": 2226.88,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "Let's try mini.",
      "offset": 2228.8,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "That's kind of slow last time, so I",
      "offset": 2231.119,
      "duration": 4.681
    },
    {
      "lang": "en",
      "text": "don't want that.",
      "offset": 2232.56,
      "duration": 3.24
    },
    {
      "lang": "en",
      "text": "Okay. I mean, it just looks like the",
      "offset": 2236,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "model, for some reason, I'm getting sent",
      "offset": 2237.28,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "off to a better version of the model",
      "offset": 2239.28,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "today last time. But in generalizing",
      "offset": 2240.96,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "you,\n they're not quantitizing. What we",
      "offset": 2244.079,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "saw last time, if you go if everyone",
      "offset": 2246.32,
      "duration": 3.039
    },
    {
      "lang": "en",
      "text": "goes back and looks is they were just",
      "offset": 2247.92,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "breaking down like 10 15 minute chunks.",
      "offset": 2249.359,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "But even here, I'm only getting like",
      "offset": 2251.92,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "really nine nine time segments in an",
      "offset": 2253.28,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "hour and a half video. Maybe that's",
      "offset": 2255.68,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "good, maybe that's bad. Um, but what I",
      "offset": 2257.28,
      "duration": 6.76
    },
    {
      "lang": "en",
      "text": "found is personally,",
      "offset": 2260.48,
      "duration": 3.56
    },
    {
      "lang": "en",
      "text": "uh, Gemini works really, really well for",
      "offset": 2264.079,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "this kind of stuff. Um, and I suspect",
      "offset": 2266.64,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "it's because they train more on YouTube",
      "offset": 2268.72,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "than probably other models do. Like you",
      "offset": 2270.24,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "just see the time code here. Like check",
      "offset": 2271.92,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "that out.",
      "offset": 2273.28,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "Uh that's just a lot better than this",
      "offset": 2275.599,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "one.",
      "offset": 2278.16,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "And you're still just sending the raw",
      "offset": 2280.48,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "transcript from Zoom, right?\n I'm I'm not",
      "offset": 2281.839,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "even sending the video. I'm just sending",
      "offset": 2283.92,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "the raw transcript. Like Gemini model",
      "offset": 2285.119,
      "duration": 3.521
    },
    {
      "lang": "en",
      "text": "like actually breaks this down to proper",
      "offset": 2287.119,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "time codes if you can look.\n Those are",
      "offset": 2288.64,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "really those are almost too small. Those",
      "offset": 2291.2,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "are like 20 second chunks.\n Exactly.",
      "offset": 2292.96,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "So it's actually give me 20 second",
      "offset": 2297.04,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "chunks. Um so we can tell it that we can",
      "offset": 2298.8,
      "duration": 8.08
    },
    {
      "lang": "en",
      "text": "say uh not do that description usually",
      "offset": 2302.48,
      "duration": 10.639
    },
    {
      "lang": "en",
      "text": "uh usually uh 20 to 30 second semantic",
      "offset": 2306.88,
      "duration": 8.08
    },
    {
      "lang": "en",
      "text": "chunks",
      "offset": 2313.119,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "uh maybe like since these are longer",
      "offset": 2314.96,
      "duration": 2.879
    },
    {
      "lang": "en",
      "text": "videos let's make it different usually",
      "offset": 2316.48,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "like five to 10\n yeah five 10 minutes I",
      "offset": 2317.839,
      "duration": 9.041
    },
    {
      "lang": "en",
      "text": "think is good\n semantic chunks but exact",
      "offset": 2320.8,
      "duration": 7.92
    },
    {
      "lang": "en",
      "text": "timings",
      "offset": 2326.88,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "transcript.",
      "offset": 2328.72,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "So, let's try this again. And what we'll",
      "offset": 2331.359,
      "duration": 3.281
    },
    {
      "lang": "en",
      "text": "notice here is like the reason that",
      "offset": 2333.52,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "we're focusing so much energy on getting",
      "offset": 2334.64,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "the summary just right",
      "offset": 2336.24,
      "duration": 6.4
    },
    {
      "lang": "en",
      "text": "is because let's see what it does.",
      "offset": 2338.96,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "It may not listen to me and I can't",
      "offset": 2342.64,
      "duration": 5.719
    },
    {
      "lang": "en",
      "text": "really force it to listen to me.",
      "offset": 2344.24,
      "duration": 4.119
    },
    {
      "lang": "en",
      "text": "And I suspect I know what's happening",
      "offset": 2349.28,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "here.",
      "offset": 2350.96,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "What's happening here is that the model",
      "offset": 2353.119,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "is doing its thing. And there was 104",
      "offset": 2355.04,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "chunks last time. Um, and I suspect it",
      "offset": 2356.96,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "actually got cut off because of um",
      "offset": 2359.04,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "context window length.",
      "offset": 2362.24,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "I probably just need to out uh it",
      "offset": 2364.64,
      "duration": 2.719
    },
    {
      "lang": "en",
      "text": "probably just needs to output more",
      "offset": 2366.24,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "tokens or output less tokens.",
      "offset": 2367.359,
      "duration": 5.921
    },
    {
      "lang": "en",
      "text": "Um, this one seems similar. So, let's",
      "offset": 2371.52,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "try and go do this again. Um,",
      "offset": 2373.28,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "instead of adding the time data in here,",
      "offset": 2376.24,
      "duration": 5.119
    },
    {
      "lang": "en",
      "text": "I'll remove this and I'll simply have",
      "offset": 2379.28,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "the LLM,",
      "offset": 2381.359,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "I'll put it as a response format. And I",
      "offset": 2384,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "suspect the reason that this is",
      "offset": 2386.16,
      "duration": 2.24
    },
    {
      "lang": "en",
      "text": "happening is actually because what the",
      "offset": 2387.28,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "LM is doing is that the LM is not",
      "offset": 2388.4,
      "duration": 7.12
    },
    {
      "lang": "en",
      "text": "forgetting my instructions because",
      "offset": 2392.72,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "there's so much in the context window",
      "offset": 2395.52,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "that it's that it's just like getting",
      "offset": 2397.44,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "lost among the transcript itself.",
      "offset": 2399.04,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "Exactly. like just like\n like it knows",
      "offset": 2400.8,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "it's supposed to be summarizing",
      "offset": 2403.839,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "something and you can maybe give it a",
      "offset": 2405.28,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "little bit of format but\n I was running a",
      "offset": 2407.119,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "token visualizer on a prompt that is too",
      "offset": 2409.68,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "big.",
      "offset": 2411.68,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "Oh yeah, don't do that.\n Yeah.",
      "offset": 2413.359,
      "duration": 7.041
    },
    {
      "lang": "en",
      "text": "Yeah. Um but this prompt is just like",
      "offset": 2417.52,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "too big. Like this this transcript is",
      "offset": 2420.4,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "just like",
      "offset": 2422.4,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "where it go",
      "offset": 2425.04,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "where's the test. This test is literally",
      "offset": 2427.68,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "just",
      "offset": 2430.24,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "long. It's a lot of context. And now,",
      "offset": 2433.04,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "and that's probably why it's trying to",
      "offset": 2435.28,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "output like a bajillion um tokens to me",
      "offset": 2436.56,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "because when the model sees this, I bet",
      "offset": 2439.28,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "you when it was outputting time codes,",
      "offset": 2441.44,
      "duration": 2.639
    },
    {
      "lang": "en",
      "text": "it's literally just converting each one",
      "offset": 2442.8,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "of these into a time code.\n I bet there's",
      "offset": 2444.079,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "104 10.\n Oh, wow. Yeah, you're just",
      "offset": 2446.56,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "labeling every single one. Okay. I was",
      "offset": 2449.28,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "gonna I was just gonna zoom out for a",
      "offset": 2450.88,
      "duration": 2.719
    },
    {
      "lang": "en",
      "text": "sec because I know we're gonna supposed",
      "offset": 2452.48,
      "duration": 2.56
    },
    {
      "lang": "en",
      "text": "to talk about tone today and we got",
      "offset": 2453.599,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "about 15 minutes left on the on the on",
      "offset": 2455.04,
      "duration": 6.039
    },
    {
      "lang": "en",
      "text": "the main session.",
      "offset": 2457.68,
      "duration": 3.399
    },
    {
      "lang": "en",
      "text": "Go ahead. I was going to go fix the",
      "offset": 2461.839,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "prompt.\n Yeah. No, I was just going to",
      "offset": 2463.599,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "say the I mean the summarization stuff I",
      "offset": 2465.52,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "think we spent a lot of time on last",
      "offset": 2467.28,
      "duration": 2.799
    },
    {
      "lang": "en",
      "text": "week and I'm I'm wondering if we can go",
      "offset": 2468.48,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "like push this towards like your some of",
      "offset": 2470.079,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "the stuff you're doing with the prompt",
      "offset": 2473.119,
      "duration": 2.881
    },
    {
      "lang": "en",
      "text": "for the email and things like that.\n I",
      "offset": 2474.24,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "could go do that right away, but the",
      "offset": 2476,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "summarization stuff as it turns out I",
      "offset": 2477.2,
      "duration": 2.159
    },
    {
      "lang": "en",
      "text": "think is going to be very",
      "offset": 2478.72,
      "duration": 2.16
    },
    {
      "lang": "en",
      "text": "straightforward. All we have to do is we",
      "offset": 2479.359,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "just change it. We just change our",
      "offset": 2480.88,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "instructions.\n The main takeaway here is",
      "offset": 2483.68,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "like using using models like Gemini can",
      "offset": 2486,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "help you do this long context stuff a",
      "offset": 2488.64,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "lot better. Right.\n Exactly. And also",
      "offset": 2490.319,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "what you'll notice is I actually have",
      "offset": 2493.28,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "the model dumping out stuff before it",
      "offset": 2494.64,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "does anything.",
      "offset": 2496.88,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "Um so I actually change the order what I",
      "offset": 2498.88,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "do. I dump out the transcript first,",
      "offset": 2501.119,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "then I dump out my instructions.",
      "offset": 2502.8,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "And that can make a huge difference in",
      "offset": 2506.079,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "the quality of my answer.",
      "offset": 2507.44,
      "duration": 5.399
    },
    {
      "lang": "en",
      "text": "Um,",
      "offset": 2509.839,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "and",
      "offset": 2513.119,
      "duration": 5.041
    },
    {
      "lang": "en",
      "text": "then the video",
      "offset": 2514.64,
      "duration": 8.52
    },
    {
      "lang": "en",
      "text": "ranges and then let me go take this out.",
      "offset": 2518.16,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "You lost your output format.\n Keep it to",
      "offset": 2524.079,
      "duration": 6.561
    },
    {
      "lang": "en",
      "text": "five uh notes",
      "offset": 2527.359,
      "duration": 6.281
    },
    {
      "lang": "en",
      "text": "max.",
      "offset": 2530.64,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "And then this will probably do it. It",
      "offset": 2534.8,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "was just doing too much of it in there.",
      "offset": 2536.079,
      "duration": 5.081
    },
    {
      "lang": "en",
      "text": "Mhm.",
      "offset": 2538.16,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "There we go. Now it works. And the",
      "offset": 2542.56,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "reason is I basically had the model go",
      "offset": 2545.04,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "ahead and break down the chunks of like",
      "offset": 2546.64,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "here's five chunks that made semantic",
      "offset": 2548,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "sense to it. The beginning, the middle,",
      "offset": 2549.44,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "other middle, and then eventually it had",
      "offset": 2551.92,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "the end",
      "offset": 2553.68,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "and it then built a I built then built a",
      "offset": 2555.599,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "better takeaways. Now let's do the next",
      "offset": 2558.319,
      "duration": 2.961
    },
    {
      "lang": "en",
      "text": "part which is actually generating the",
      "offset": 2560.079,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "email perfectly.",
      "offset": 2561.28,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "So when we go actually generate this",
      "offset": 2563.359,
      "duration": 4.76
    },
    {
      "lang": "en",
      "text": "email,",
      "offset": 2565.119,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "there's a couple of things on here that",
      "offset": 2568.64,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "we want to do. One, we want to give it",
      "offset": 2570.079,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "uh we want to give it the email example",
      "offset": 2573.359,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "of like another email that we liked. The",
      "offset": 2574.96,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "next thing that we want to do is we want",
      "offset": 2577.68,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "to start giving it video summary, email",
      "offset": 2579.119,
      "duration": 5.441
    },
    {
      "lang": "en",
      "text": "structure, and everything else. But when",
      "offset": 2580.8,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "we give it the email structure, it's",
      "offset": 2584.56,
      "duration": 2.48
    },
    {
      "lang": "en",
      "text": "going to go give it all of these",
      "offset": 2585.839,
      "duration": 2.641
    },
    {
      "lang": "en",
      "text": "concepts. And then we're also going to",
      "offset": 2587.04,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "go give it a couple more things, which",
      "offset": 2588.48,
      "duration": 5.32
    },
    {
      "lang": "en",
      "text": "is we're going to give it",
      "offset": 2590.16,
      "duration": 3.64
    },
    {
      "lang": "en",
      "text": "information about the next session",
      "offset": 2593.92,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "that's already in here. And then we're",
      "offset": 2595.44,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "going to uh where this Okay, sorry. Let",
      "offset": 2597.68,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "me pull up the right test case. I",
      "offset": 2599.52,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "apologize for this. There we go.",
      "offset": 2600.72,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "So, let's just copy and paste the",
      "offset": 2603.599,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "summary and",
      "offset": 2604.96,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "replace the summary with this one",
      "offset": 2607.44,
      "duration": 2.56
    },
    {
      "lang": "en",
      "text": "because this is a slightly better",
      "offset": 2608.72,
      "duration": 4.68
    },
    {
      "lang": "en",
      "text": "version of it.",
      "offset": 2610,
      "duration": 3.4
    },
    {
      "lang": "en",
      "text": "Correct.",
      "offset": 2628.96,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "Okay. So, you're using the outputs from",
      "offset": 2637.44,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "the summarization to generate an email",
      "offset": 2639.359,
      "duration": 5.121
    },
    {
      "lang": "en",
      "text": "with",
      "offset": 2642.24,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "the pieces that you're going to build up",
      "offset": 2644.48,
      "duration": 8.119
    },
    {
      "lang": "en",
      "text": "in in the final template.\n Exactly.",
      "offset": 2646.56,
      "duration": 6.039
    },
    {
      "lang": "en",
      "text": "How do we get more BAML in the training",
      "offset": 2652.64,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "set? Dude, I'm tired of deleting colons",
      "offset": 2654.319,
      "duration": 5.361
    },
    {
      "lang": "en",
      "text": "whenever I generate BAML code.\n We will",
      "offset": 2657.119,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "just support colons and that is how we",
      "offset": 2659.68,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "actually solve that problem. We made a",
      "offset": 2661.599,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "mistake.\n Oh, okay.",
      "offset": 2663.359,
      "duration": 7.281
    },
    {
      "lang": "en",
      "text": "That is really the real problem there.",
      "offset": 2666.8,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "Um that was us trying to be clever and",
      "offset": 2670.64,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "make syntax easier, but it turns out",
      "offset": 2672.88,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "that was incorrect.",
      "offset": 2674.4,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "So when we do this, the subject we",
      "offset": 2676.48,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "already get from our old uh prompting",
      "offset": 2677.92,
      "duration": 5.919
    },
    {
      "lang": "en",
      "text": "video. So we we just call this",
      "offset": 2680,
      "duration": 7.52
    },
    {
      "lang": "en",
      "text": "oops my AirPods died. So I'll be on here",
      "offset": 2683.839,
      "duration": 5.601
    },
    {
      "lang": "en",
      "text": "AI that works cracking the prompting",
      "offset": 2687.52,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "interview.",
      "offset": 2689.44,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "The next thing that we'll do is we'll",
      "offset": 2691.839,
      "duration": 6.121
    },
    {
      "lang": "en",
      "text": "just figure out that we covered.",
      "offset": 2694.079,
      "duration": 3.881
    },
    {
      "lang": "en",
      "text": "There we go. Where we dive into where we",
      "offset": 2699.76,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "dive into",
      "offset": 2702.8,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "uh this session. So, this email",
      "offset": 2704.88,
      "duration": 7.76
    },
    {
      "lang": "en",
      "text": "structure, what this is doing now is",
      "offset": 2707.92,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "this is actually going to go ahead and",
      "offset": 2712.64,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "generate this email pretty easily.",
      "offset": 2714.16,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "Oh, whoops.",
      "offset": 2716.8,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "From that structure, you can see what",
      "offset": 2718.72,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "it's actually able to go do. It doesn't",
      "offset": 2720.64,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "actually struggle that much when I just",
      "offset": 2722.96,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "wanted to fill in the blanks. When I",
      "offset": 2724.72,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "wanted to go fill in the blanks to just",
      "offset": 2726.8,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "make it feel good, it can do this really",
      "offset": 2728,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "easily from the main takeaways in the we",
      "offset": 2730,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "covered section. The question that I",
      "offset": 2731.839,
      "duration": 3.121
    },
    {
      "lang": "en",
      "text": "have to ask myself is what Dexter was",
      "offset": 2733.44,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "talking about last time is how can I get",
      "offset": 2734.96,
      "duration": 3.119
    },
    {
      "lang": "en",
      "text": "it to go generate these sections really",
      "offset": 2736.4,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "really accurately",
      "offset": 2738.079,
      "duration": 6.321
    },
    {
      "lang": "en",
      "text": "and that's why we actually built",
      "offset": 2740.64,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "two prompts here to generate the email.",
      "offset": 2744.4,
      "duration": 4.959
    },
    {
      "lang": "en",
      "text": "One is generate the email draft and the",
      "offset": 2746.8,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "other one is actually go generate the",
      "offset": 2749.359,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "email. So generating the email draft",
      "offset": 2751.28,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "comes from a very basic thing where I",
      "offset": 2754,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "give the video summary, I give the",
      "offset": 2755.599,
      "duration": 2.561
    },
    {
      "lang": "en",
      "text": "transcript, I give the video title and",
      "offset": 2756.72,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "then it generates a draft for me. And",
      "offset": 2758.16,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "the draft consists of these five",
      "offset": 2760.079,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "sections. It literally just consists of",
      "offset": 2762.4,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "each of these.",
      "offset": 2764.8,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "And now I'm able to go ahead and break",
      "offset": 2766.96,
      "duration": 3.119
    },
    {
      "lang": "en",
      "text": "down the problem into smaller sections.",
      "offset": 2768.4,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "And what the difference here is the",
      "offset": 2770.079,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "model might choose to respond in each of",
      "offset": 2772.24,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "these sections with a variety of",
      "offset": 2774.4,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "different options. It might choose to",
      "offset": 2776.88,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "respond with a",
      "offset": 2778.88,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "excuse me, it might choose to respond",
      "offset": 2781.359,
      "duration": 2.801
    },
    {
      "lang": "en",
      "text": "with phrases. It might choose to respond",
      "offset": 2782.64,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "with incomplete sentences, things that",
      "offset": 2784.16,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "don't flow with each other. And that's",
      "offset": 2786,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "totally acceptable. So we don't have to",
      "offset": 2787.92,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "think that hard about this. All we",
      "offset": 2789.76,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "really want to happen here is we just",
      "offset": 2791.68,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "want to get the content mostly correct",
      "offset": 2793.04,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "out of this problem. Now again,\n okay, so",
      "offset": 2794.96,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "you're not using deterministic template.",
      "offset": 2797.92,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "You're not like generate the bullet",
      "offset": 2800.16,
      "duration": 2.959
    },
    {
      "lang": "en",
      "text": "points and then I'm going to like glue",
      "offset": 2801.92,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "this all together by hand. You're",
      "offset": 2803.119,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "actually just like, hey, generate it in",
      "offset": 2804.64,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "a structured way and then I'm going to",
      "offset": 2806.8,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "hand that to something whose only job is",
      "offset": 2808.48,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "to like turn it back into a full",
      "offset": 2810.16,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "coherent email.\n Exactly. Don't And the",
      "offset": 2811.92,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "new thing doesn't add new content. It",
      "offset": 2816,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "doesn't do anything. It just makes it",
      "offset": 2818,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "coherent with another template that we",
      "offset": 2819.839,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "have in mind.",
      "offset": 2821.68,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "And the difference here is like that's",
      "offset": 2823.599,
      "duration": 2.641
    },
    {
      "lang": "en",
      "text": "the difference between like it feeling",
      "offset": 2825.04,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "like ma mad libs. Like mad libs they're",
      "offset": 2826.24,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "fun I guess as a joke but no one would",
      "offset": 2829.359,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "actually send mad lib stuff. It feels",
      "offset": 2831.119,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "bad to go read that and it doesn't",
      "offset": 2832.8,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "really read nicely as well because our",
      "offset": 2834.72,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "content is a little bit more dynamic",
      "offset": 2837.44,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "than everything else. So this is I think",
      "offset": 2838.88,
      "duration": 4.959
    },
    {
      "lang": "en",
      "text": "where people can bridge LMS. I still",
      "offset": 2841.119,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "want my content to be broken down to",
      "offset": 2843.839,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "this generic structure.",
      "offset": 2845.68,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "But what I don't want to happen is I",
      "offset": 2848.48,
      "duration": 5.119
    },
    {
      "lang": "en",
      "text": "don't want the email to feel way too",
      "offset": 2850.88,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "loose in the beginning because if I",
      "offset": 2853.599,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "don't give it a structure, then I have",
      "offset": 2854.88,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "to try and prompt my way to getting this",
      "offset": 2856.8,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "email to feel this way every single",
      "offset": 2858.56,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "time. But what I can do now is I can",
      "offset": 2860,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "just have a section that does this. And",
      "offset": 2862.56,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "I can just like put like make this an",
      "offset": 2864.88,
      "duration": 2.959
    },
    {
      "lang": "en",
      "text": "array if I want because I think this is",
      "offset": 2866.4,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "going to be better. Use my Gemini",
      "offset": 2867.839,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "because Gemini in my opinion is a",
      "offset": 2869.839,
      "duration": 5.681
    },
    {
      "lang": "en",
      "text": "superior model for this problem. Um,",
      "offset": 2871.359,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "and then lastly, we're going to go do",
      "offset": 2875.52,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "the same thing, which is again, we're",
      "offset": 2877.119,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "going to have a lot of context in here.",
      "offset": 2878.4,
      "duration": 9.88
    },
    {
      "lang": "en",
      "text": "Um, so I'm just going to move this up",
      "offset": 2882,
      "duration": 6.28
    },
    {
      "lang": "en",
      "text": "and put my instructions at the bottom",
      "offset": 2890.4,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "because Gemini prefers that. And I can",
      "offset": 2891.92,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "do this conditionally as well, so I",
      "offset": 2894.319,
      "duration": 2.401
    },
    {
      "lang": "en",
      "text": "don't have to think that hard about",
      "offset": 2895.76,
      "duration": 2.16
    },
    {
      "lang": "en",
      "text": "this.",
      "offset": 2896.72,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "So, you're doing two user messages in a",
      "offset": 2897.92,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "row there, by the way, which is",
      "offset": 2899.76,
      "duration": 2.64
    },
    {
      "lang": "en",
      "text": "something I haven't actually seen that",
      "offset": 2900.8,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "much. Um, can you talk about why you did",
      "offset": 2902.4,
      "duration": 9.64
    },
    {
      "lang": "en",
      "text": "that?\n Yes, I can. One second. You can",
      "offset": 2905.44,
      "duration": 6.6
    },
    {
      "lang": "en",
      "text": "Okay. Um, generate email structure.",
      "offset": 2915.28,
      "duration": 6.72
    },
    {
      "lang": "en",
      "text": "Sorry. Draft.",
      "offset": 2918.72,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "Why did I do two user messages? Oh, I",
      "offset": 2922,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "moved this in the wrong place. I moved",
      "offset": 2924,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "the wrong prompt.",
      "offset": 2925.52,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "Why did I do two e user messages in a",
      "offset": 2928.24,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "row? Well, I think what the LMS do under",
      "offset": 2930.319,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "the hood, and this is just like my",
      "offset": 2932.559,
      "duration": 2.56
    },
    {
      "lang": "en",
      "text": "assumption of what they do, is they",
      "offset": 2933.68,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "definitely inject tokens in for you. So,",
      "offset": 2935.119,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "when we actually go look at the raw curl",
      "offset": 2937.119,
      "duration": 2.881
    },
    {
      "lang": "en",
      "text": "request, for example, I have the",
      "offset": 2938.559,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "transcript and everything here, but I",
      "offset": 2940,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "basically have like two content parts in",
      "offset": 2941.599,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "here. I like one content part that is",
      "offset": 2943.2,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "the initial one and a second content",
      "offset": 2946,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "part that's the other one. And the",
      "offset": 2947.76,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "reason that I don't do this that I do",
      "offset": 2949.839,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "this is because Gemini",
      "offset": 2951.28,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "um does something weird with system",
      "offset": 2953.52,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "messages. I'll show you what it does.",
      "offset": 2955.52,
      "duration": 5.4
    },
    {
      "lang": "en",
      "text": "Oops.",
      "offset": 2957.92,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "Uh, which is for some reason Gemini",
      "offset": 2963.04,
      "duration": 6.079
    },
    {
      "lang": "en",
      "text": "likes to put system messages not as a",
      "offset": 2966.24,
      "duration": 5.119
    },
    {
      "lang": "en",
      "text": "part of it. Um,",
      "offset": 2969.119,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "let me make this bigger.\n Oh, it comes in",
      "offset": 2971.359,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "a different part of the request.",
      "offset": 2973.359,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "Exactly.",
      "offset": 2975.119,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "And you can see how big this context is.",
      "offset": 2978,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "So, it's kind of annoying. Um, oh yes,",
      "offset": 2979.44,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "Gemini doesn't actually support system",
      "offset": 2982.079,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "messages in this way. If you try and",
      "offset": 2983.76,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "pass in a system message to Gemini in",
      "offset": 2985.44,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "this way, it doesn't actually work. You",
      "offset": 2987.04,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "can do it this way, but like it just",
      "offset": 2988.96,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "does it as a different part of the",
      "offset": 2991.119,
      "duration": 3.121
    },
    {
      "lang": "en",
      "text": "request. And I can pass into the system",
      "offset": 2992.24,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "instructions, but empirically what I",
      "offset": 2994.24,
      "duration": 2.879
    },
    {
      "lang": "en",
      "text": "have found is the model just doesn't",
      "offset": 2995.68,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "listen to that. It seems to listen to",
      "offset": 2997.119,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "the most recent tokens more.",
      "offset": 2998.64,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "So I just embed it at the very bottom",
      "offset": 3001.599,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "down here whenever I'm using Gemini. And",
      "offset": 3003.359,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "what I actually do under the hood is I",
      "offset": 3005.2,
      "duration": 5.639
    },
    {
      "lang": "en",
      "text": "actually usually do something like this.",
      "offset": 3006.72,
      "duration": 4.119
    },
    {
      "lang": "en",
      "text": "Oh, otherwise you would put a system",
      "offset": 3014.72,
      "duration": 5.24
    },
    {
      "lang": "en",
      "text": "message at the top.",
      "offset": 3016.4,
      "duration": 3.56
    },
    {
      "lang": "en",
      "text": "Exactly. So, it does give me a little",
      "offset": 3023.359,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "bit of flexibility over here where it's",
      "offset": 3025.2,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "like I add it in if it's there,",
      "offset": 3026.559,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "otherwise I don't. And like in the case",
      "offset": 3028.4,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "of Gemini, I get I get like a second",
      "offset": 3030.8,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "user message basically. Um, and that for",
      "offset": 3032.64,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "me works quite well. But now when I'm",
      "offset": 3035.92,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "actually going to go run this, what I'll",
      "offset": 3038.64,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "find is the email comes out kind of",
      "offset": 3039.92,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "nicely. So it gives me a quick recap",
      "offset": 3042.079,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "which if you remember the cracking the",
      "offset": 3044.64,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "prompting interview like this was a",
      "offset": 3046,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "thing. Don't avoid long meaningless",
      "offset": 3047.44,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "tokens, sequences, URLs, etc. for",
      "offset": 3049.44,
      "duration": 5.119
    },
    {
      "lang": "en",
      "text": "diorization and mint indexes. This is a",
      "offset": 3052.48,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "pretty good summary of what we have.",
      "offset": 3054.559,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "Utilize inline comments within the",
      "offset": 3056.559,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "model. Practice RTFP. Read the effing",
      "offset": 3058.72,
      "duration": 5.359
    },
    {
      "lang": "en",
      "text": "prompt. Use fshot prompting to define",
      "offset": 3061.2,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "the output structure, not necessarily",
      "offset": 3064.079,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "the content. So it has a pattern to",
      "offset": 3065.52,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "follow and this summary is actually",
      "offset": 3067.599,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "quite good for what we want. And the",
      "offset": 3069.68,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "problem is it doesn't have a next",
      "offset": 3071.76,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "session and that's correct. It cannot",
      "offset": 3073.68,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "possibly have a next session because we",
      "offset": 3075.359,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "haven't given it that input.",
      "offset": 3076.88,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "I just want to call out real quick um",
      "offset": 3079.839,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "you have a function called generate",
      "offset": 3082.079,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "email draft and then you have a function",
      "offset": 3083.52,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "called generate email structure and I",
      "offset": 3085.119,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "think the names might be flipped.\n Yes,",
      "offset": 3086.8,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "that is\n don't try to change it now.",
      "offset": 3089.839,
      "duration": 5.441
    },
    {
      "lang": "en",
      "text": "Um that is almost definitely true and",
      "offset": 3092.16,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "this is probably because I initially had",
      "offset": 3095.28,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "email uh structure\n just draft\n and it was",
      "offset": 3096.96,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "doing the whole thing it was doing the",
      "offset": 3100.64,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "whole email from input to output.\n Um",
      "offset": 3101.76,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "this uh is a good point. I'll rename",
      "offset": 3105.28,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "this before I\n dude. Yeah, don't change",
      "offset": 3107.76,
      "duration": 2.48
    },
    {
      "lang": "en",
      "text": "it now because it's going to break all",
      "offset": 3109.2,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "the code. But\n yeah,",
      "offset": 3110.24,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "uh the next thing that I want to pass",
      "offset": 3113.2,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "into this is I want to pass in uh uh",
      "offset": 3114.48,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "event",
      "offset": 3117.44,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "event details.",
      "offset": 3119.2,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "And the event details will have the",
      "offset": 3121.599,
      "duration": 2.881
    },
    {
      "lang": "en",
      "text": "event name, the date, the time and",
      "offset": 3123.04,
      "duration": 5.279
    },
    {
      "lang": "en",
      "text": "location.\n You need the URL too.",
      "offset": 3124.48,
      "duration": 8.079
    },
    {
      "lang": "en",
      "text": "uh uh ID",
      "offset": 3128.319,
      "duration": 7.24
    },
    {
      "lang": "en",
      "text": "inviting",
      "offset": 3132.559,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "and I'll pass in next event",
      "offset": 3135.68,
      "duration": 5.919
    },
    {
      "lang": "en",
      "text": "next session. We'll go in there and",
      "offset": 3139.359,
      "duration": 6.2
    },
    {
      "lang": "en",
      "text": "we'll just add this into here.",
      "offset": 3141.599,
      "duration": 3.96
    },
    {
      "lang": "en",
      "text": "Um and now my test case will go back to",
      "offset": 3149.599,
      "duration": 8.081
    },
    {
      "lang": "en",
      "text": "merit gun. add a little test case",
      "offset": 3152.96,
      "duration": 6.159
    },
    {
      "lang": "en",
      "text": "and this is kind of the nice thing about",
      "offset": 3157.68,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "this is the next session will be let's",
      "offset": 3159.119,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "say the next session will be like",
      "offset": 3161.76,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "generating uh content AI content",
      "offset": 3162.88,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "pipeline",
      "offset": 3166.079,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "AI powered",
      "offset": 3167.599,
      "duration": 6.96
    },
    {
      "lang": "en",
      "text": "content pipeline July 15th 2025",
      "offset": 3170.4,
      "duration": 6.32
    },
    {
      "lang": "en",
      "text": "cool",
      "offset": 3174.559,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "um yeah we do have a description on",
      "offset": 3176.72,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "those as well if you go look at the Luma",
      "offset": 3178.48,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "client so I would probably add that",
      "offset": 3180.48,
      "duration": 6.68
    },
    {
      "lang": "en",
      "text": "cool let's do that.",
      "offset": 3183.359,
      "duration": 3.801
    },
    {
      "lang": "en",
      "text": "And again, I could dump this as JSON.",
      "offset": 3193.839,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "I'm just choosing not to because it's",
      "offset": 3195.839,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "better to dump it without the quotation",
      "offset": 3198.079,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "marks and everything else if I can avoid",
      "offset": 3199.599,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "it. um",
      "offset": 3201.119,
      "duration": 5.761
    },
    {
      "lang": "en",
      "text": "on this",
      "offset": 3204.559,
      "duration": 5.121
    },
    {
      "lang": "en",
      "text": "at this",
      "offset": 3206.88,
      "duration": 7
    },
    {
      "lang": "en",
      "text": "um and then the invite link",
      "offset": 3209.68,
      "duration": 4.2
    },
    {
      "lang": "en",
      "text": "and I'm just trying to make this easier.",
      "offset": 3214.88,
      "duration": 6.239
    },
    {
      "lang": "en",
      "text": "Uh, next session link invite",
      "offset": 3217.119,
      "duration": 6.161
    },
    {
      "lang": "en",
      "text": "join.",
      "offset": 3221.119,
      "duration": 5.161
    },
    {
      "lang": "en",
      "text": "Oops.",
      "offset": 3223.28,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "Link.",
      "offset": 3226.48,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "I'm just trying to make this easier for",
      "offset": 3228.96,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "the LLM to go read.\n So, if I go read",
      "offset": 3230.559,
      "duration": 6.841
    },
    {
      "lang": "en",
      "text": "this, I keep running this test again.",
      "offset": 3232.96,
      "duration": 4.44
    },
    {
      "lang": "en",
      "text": "Oh, man. I got to do that today, too. I",
      "offset": 3237.52,
      "duration": 6.12
    },
    {
      "lang": "en",
      "text": "know the",
      "offset": 3240,
      "duration": 3.64
    },
    {
      "lang": "en",
      "text": "what we should find is again the recap",
      "offset": 3244.24,
      "duration": 5.359
    },
    {
      "lang": "en",
      "text": "is still pretty good covers roughly the",
      "offset": 3246.88,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "uh same content and boom our next",
      "offset": 3249.599,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "session on July 15th will be about AI",
      "offset": 3251.68,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "generated content. Here's the link. Now",
      "offset": 3253.52,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "obviously the link can be elucinated. So",
      "offset": 3256.72,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "what I would do if I were doing this is",
      "offset": 3258.319,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "I would just validate the link is",
      "offset": 3259.68,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "correct. And really what I would do is",
      "offset": 3261.119,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "like what I would really do is I'd",
      "offset": 3263.04,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "actually put a placeholder link in",
      "offset": 3265.44,
      "duration": 5.919
    },
    {
      "lang": "en",
      "text": "there. Uh, so I do like luma dot uh next",
      "offset": 3266.8,
      "duration": 7.039
    },
    {
      "lang": "en",
      "text": "AI",
      "offset": 3271.359,
      "duration": 7.441
    },
    {
      "lang": "en",
      "text": "works uh like 12.",
      "offset": 3273.839,
      "duration": 8.881
    },
    {
      "lang": "en",
      "text": "A I I do AI that works 12. And the",
      "offset": 3278.8,
      "duration": 5.519
    },
    {
      "lang": "en",
      "text": "reason I don't put a pure placeholder",
      "offset": 3282.72,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "link is because the LM will not",
      "offset": 3284.319,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "hallucinate. We'll be like, &quot;Oh, that's",
      "offset": 3286.559,
      "duration": 2.241
    },
    {
      "lang": "en",
      "text": "a placeholder link. Maybe I'll",
      "offset": 3287.76,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "substitute something for you.&quot; But",
      "offset": 3288.8,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "putting something like this\n gets you",
      "offset": 3290.64,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "something that's like close enough that",
      "offset": 3292.8,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "it looks real but not wrong enough\n uh",
      "offset": 3294.72,
      "duration": 5.599
    },
    {
      "lang": "en",
      "text": "that it can't get it wrong because these",
      "offset": 3298.48,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "are not that many tokens. So now it'll",
      "offset": 3300.319,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "have a lot more reliability that the",
      "offset": 3302.079,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "model will dump out the invite.\n And then",
      "offset": 3303.68,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "what would you just do like a find",
      "offset": 3305.76,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "replace in the final in the final output",
      "offset": 3307.359,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "and replace it with the with with the",
      "offset": 3309.28,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "real link.\n Literally what I would do.",
      "offset": 3311.359,
      "duration": 6.081
    },
    {
      "lang": "en",
      "text": "Cool. And then what I and now what's",
      "offset": 3314.319,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "nice about this is converting this",
      "offset": 3317.44,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "structure to like an email format. I",
      "offset": 3318.8,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "think that's something we all believe an",
      "offset": 3320.8,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "LLM could do. So now we don't have to",
      "offset": 3322.16,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "think about this as much. Now we can go",
      "offset": 3325.119,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "to the Oops. I I don't know why I had",
      "offset": 3326.8,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "that.\n Again, you're just you're just",
      "offset": 3328.48,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "guiding the thought process. Basically,",
      "offset": 3329.92,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "you're forcing it to think about",
      "offset": 3332.079,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "different parts of what the email is",
      "offset": 3333.76,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "going to be. And that makes it really",
      "offset": 3335.44,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "easy for the next the next LM call to",
      "offset": 3337.04,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "just generate an email that matches what",
      "offset": 3339.44,
      "duration": 2.879
    },
    {
      "lang": "en",
      "text": "we want instead of it having to kind of",
      "offset": 3340.88,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "do those two steps of okay, let me break",
      "offset": 3342.319,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "this down into parts and then let me go",
      "offset": 3344.16,
      "duration": 5.439
    },
    {
      "lang": "en",
      "text": "turn it into a nice email.\n Exactly. So,",
      "offset": 3346.559,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "and and now when we when I submit this,",
      "offset": 3349.599,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "I'm going to run the previous draft",
      "offset": 3351.52,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "again. Run email structure just so",
      "offset": 3353.04,
      "duration": 5.279
    },
    {
      "lang": "en",
      "text": "people get an idea what it looks like.",
      "offset": 3355.68,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "And we can actually read this email and",
      "offset": 3358.319,
      "duration": 3.121
    },
    {
      "lang": "en",
      "text": "just see what we feel because I think",
      "offset": 3359.839,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "feel it's this is a feeling based task.",
      "offset": 3361.44,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "Hello, first name. We should fix the",
      "offset": 3364.079,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "first name thing. We have to keep tabs",
      "offset": 3365.92,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "of that. This we from AI from Vib index.",
      "offset": 3367.44,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "We covered a lot of different topics.",
      "offset": 3371.52,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "This looks really good. Actually, when I",
      "offset": 3373.2,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "go look at this, it doesn't have the",
      "offset": 3374.72,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "GitHub link and it doesn't have the URL",
      "offset": 3376.64,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "probably because we didn't give it to",
      "offset": 3378.319,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "it. So, that makes sense. It's not going",
      "offset": 3379.68,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "to have that, but it does have the Luma",
      "offset": 3381.119,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "link. Join us for the next session",
      "offset": 3383.92,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "generate for to generate content for",
      "offset": 3385.599,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "various use cases. That's pretty good.",
      "offset": 3387.28,
      "duration": 3.279
    },
    {
      "lang": "en",
      "text": "If we had a better description, I bet",
      "offset": 3389.04,
      "duration": 3.039
    },
    {
      "lang": "en",
      "text": "this would be better. And I think this",
      "offset": 3390.559,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "email is actually good enough for us to",
      "offset": 3392.079,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "go send out.",
      "offset": 3394,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "And this it doesn't matter that this is",
      "offset": 3396.559,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "perfect or not, but this I can just",
      "offset": 3398.559,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "read. Our Dex can just read and we can",
      "offset": 3400.319,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "like is this good enough? No, we can",
      "offset": 3402,
      "duration": 3.119
    },
    {
      "lang": "en",
      "text": "edit this manually because it's just one",
      "offset": 3403.839,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "sentence we have to go edit now because",
      "offset": 3405.119,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "the rest of this looks really really",
      "offset": 3406.799,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "high quality. And why did I know? And",
      "offset": 3408.319,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "the trick here is we tried GD40, it",
      "offset": 3411.04,
      "duration": 3.279
    },
    {
      "lang": "en",
      "text": "didn't work. We tried Claude, it didn't",
      "offset": 3412.88,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "work. We tried Sonnet. We tried Gemini.",
      "offset": 3414.319,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "Gemini works with Flash. Great. I'm just",
      "offset": 3416.799,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "going to use that because it's dirt",
      "offset": 3418.88,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "cheap and free. Basically, our problem",
      "offset": 3419.839,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "is solved. And now we have a tone that I",
      "offset": 3422.16,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "think feels much better than the",
      "offset": 3424.72,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "original. Can you do you have the drafts",
      "offset": 3426.079,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "of the email running on your machine,",
      "offset": 3428.16,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "Dexter?\n Yeah, I don't think I have this",
      "offset": 3429.92,
      "duration": 5.679
    },
    {
      "lang": "en",
      "text": "code though.\n You don't need this code. I",
      "offset": 3433.52,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "mean, just running the code that we have",
      "offset": 3435.599,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "on your screen. If we look\n just do the",
      "offset": 3436.88,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "resummarize.",
      "offset": 3439.92,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "Well, just look at this. This email just",
      "offset": 3441.44,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "reads that feels like AI generated an",
      "offset": 3443.839,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "email.\n Yeah, this sucks. The one that we",
      "offset": 3446.16,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "were looking\n if you send me this email",
      "offset": 3448.64,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "I'm unsubscribing by above.\n Exactly.\n And",
      "offset": 3450.4,
      "duration": 5.439
    },
    {
      "lang": "en",
      "text": "the one that we see now is much better.",
      "offset": 3453.52,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "So we've been able to fully autoate the",
      "offset": 3455.839,
      "duration": 3.041
    },
    {
      "lang": "en",
      "text": "pipeline get the tone. But the big part",
      "offset": 3457.359,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "about getting the tone today was really",
      "offset": 3458.88,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "about getting the data getting the right",
      "offset": 3460.319,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "data in the right place. And that's all",
      "offset": 3462.559,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "we focused on. And once we got the right",
      "offset": 3464.24,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "data then it was picking the right model",
      "offset": 3465.839,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "that could do the task because we know",
      "offset": 3467.68,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "that models can get toned probably",
      "offset": 3469.599,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "correct. We just have to find the right",
      "offset": 3471.52,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "one that matches our use case. for our",
      "offset": 3473.2,
      "duration": 5.599
    },
    {
      "lang": "en",
      "text": "use case that happened to be uh Gemini",
      "offset": 3475.28,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "and then we broke the problem into two",
      "offset": 3478.799,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "steps. Generate the generate the",
      "offset": 3480.48,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "content, generate the actual email and",
      "offset": 3482.24,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "that seems to work.",
      "offset": 3484.96,
      "duration": 5.119
    },
    {
      "lang": "en",
      "text": "All right. I have I have one question",
      "offset": 3488,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "for you before we jump into questions",
      "offset": 3490.079,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "from the audience.\n Yeah.\n Um the world of",
      "offset": 3491.76,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "AI moves really fast and we're all",
      "offset": 3495.119,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "constantly learning things. Uh, so if I",
      "offset": 3497.28,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "can take a slightly off topic for one",
      "offset": 3499.68,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "second, uh, do you have a fresh AI hot",
      "offset": 3501.599,
      "duration": 5.281
    },
    {
      "lang": "en",
      "text": "take? Something that you've seen a lot,",
      "offset": 3504.64,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "like maybe not even related to this",
      "offset": 3506.88,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "content, but like what's a strong",
      "offset": 3508.48,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "opinion that you've had to voice in the",
      "offset": 3510.319,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "last week that might be valuable to the",
      "offset": 3512.24,
      "duration": 6.079
    },
    {
      "lang": "en",
      "text": "audience here.\n Um, use more AI than you",
      "offset": 3514.559,
      "duration": 6.401
    },
    {
      "lang": "en",
      "text": "think and use less AI than you think at",
      "offset": 3518.319,
      "duration": 5.841
    },
    {
      "lang": "en",
      "text": "the same time. Um, I think it's been a",
      "offset": 3520.96,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "really interesting learning for me",
      "offset": 3524.16,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "personally. Uh, and it really actually",
      "offset": 3525.52,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "came from talking to kids that are much",
      "offset": 3527.599,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "younger uh than uh me. Uh, because I've",
      "offset": 3529.28,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "been coding for a while. I'm kind of set",
      "offset": 3533.359,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "in my ways and I don't really like to",
      "offset": 3534.64,
      "duration": 4.959
    },
    {
      "lang": "en",
      "text": "change how I code. I just code the way I",
      "offset": 3537.2,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "code. Um, but I've been watching some",
      "offset": 3539.599,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "younger kids code and it's interesting",
      "offset": 3541.68,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "what they use AI for. For example, copy",
      "offset": 3543.44,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "and paste. I will never manually paste",
      "offset": 3544.96,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "something ever again. I literally will",
      "offset": 3547.52,
      "duration": 2.799
    },
    {
      "lang": "en",
      "text": "just ask an alarm to do it or have",
      "offset": 3548.72,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "cursor tab autocomplete it. It is",
      "offset": 3550.319,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "really, really freaking good.",
      "offset": 3552.559,
      "duration": 6.641
    },
    {
      "lang": "en",
      "text": "I naturally I was I I could think how to",
      "offset": 3555.52,
      "duration": 5.279
    },
    {
      "lang": "en",
      "text": "do that for AI but it was my first",
      "offset": 3559.2,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "instinct and now copy and paste is just",
      "offset": 3560.799,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "an AI feature. It is not a copy and",
      "offset": 3563.2,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "paste feature. it is a pure AI feature",
      "offset": 3565.119,
      "duration": 5.921
    },
    {
      "lang": "en",
      "text": "for me and like\n interesting\n that changed",
      "offset": 3566.799,
      "duration": 5.681
    },
    {
      "lang": "en",
      "text": "my behavior and I think there are other",
      "offset": 3571.04,
      "duration": 2.799
    },
    {
      "lang": "en",
      "text": "places like for example when we're doing",
      "offset": 3572.48,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "the folder renaming we could pass in all",
      "offset": 3573.839,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "the folders into it but if it's trivial",
      "offset": 3576.24,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "to do the right thing just do the right",
      "offset": 3578.48,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "thing and just pass in only the folder",
      "offset": 3581.44,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "names instead of all the files because",
      "offset": 3582.88,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "you'll just make your own life easier.",
      "offset": 3585.44,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "So I think that's what I mean by use",
      "offset": 3587.04,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "more AI than you think. Like use it for",
      "offset": 3588.48,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "places that you're probably not",
      "offset": 3590.16,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "imagining using for like copy and paste.",
      "offset": 3591.28,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "Oh, and stop trying to use AI for dumb",
      "offset": 3593.68,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "[Â __Â ] that you could vibe code in five",
      "offset": 3596,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "lines.\n Exactly. Like the PR thing. Like",
      "offset": 3597.599,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "why vibe code the perfect readme and a",
      "offset": 3600.48,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "perfect PR creator when I know I only",
      "offset": 3603.359,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "need to do two read me files\n and find a",
      "offset": 3605.2,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "balance of where that is is actually I",
      "offset": 3608.319,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "think a really really artful um",
      "offset": 3609.92,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "position.\n That's cool. Do you want to",
      "offset": 3612.72,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "hear mine?\n Yeah.\n I feel like I keep",
      "offset": 3614.88,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "saying this. Uh and there's a lot I This",
      "offset": 3617.359,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "is inspired. I saw a lot of MCP talk in",
      "offset": 3620.4,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "the chat. Um, I think if you are",
      "offset": 3622.16,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "building agents and you are building",
      "offset": 3623.839,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "code and you're building AI products, I",
      "offset": 3625.52,
      "duration": 7.599
    },
    {
      "lang": "en",
      "text": "I don't think MCP was designed or is",
      "offset": 3628.64,
      "duration": 6.56
    },
    {
      "lang": "en",
      "text": "very good as like a replacement for",
      "offset": 3633.119,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "traditional SDKs. We've said this in our",
      "offset": 3635.2,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "MCP episode as well is like SDK is MCP",
      "offset": 3636.88,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "is not a building block for building",
      "offset": 3640.48,
      "duration": 5.839
    },
    {
      "lang": "en",
      "text": "production AI agents as products. MCP is",
      "offset": 3642,
      "duration": 6.96
    },
    {
      "lang": "en",
      "text": "an incredibly powerful toolkit that is",
      "offset": 3646.319,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "changing the world because it allows",
      "offset": 3648.96,
      "duration": 5.119
    },
    {
      "lang": "en",
      "text": "non-technical or like much less",
      "offset": 3650.96,
      "duration": 6.8
    },
    {
      "lang": "en",
      "text": "technical people to add functionality to",
      "offset": 3654.079,
      "duration": 6.881
    },
    {
      "lang": "en",
      "text": "existing AI user experiences.",
      "offset": 3657.76,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "And like your app, your production",
      "offset": 3660.96,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "agent, the thing you're shipping,",
      "offset": 3663.68,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "something you want people to use should",
      "offset": 3665.119,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "absolutely support and implement MCP,",
      "offset": 3667.04,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "but not in the sense of like, oh, I'm",
      "offset": 3669.839,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "going to use a bunch of MCP servers and",
      "offset": 3671.52,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "like put them under the hood inside my",
      "offset": 3673.52,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "app that no one knows about, but you",
      "offset": 3675.2,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "should implement an MCP client so that",
      "offset": 3676.88,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "anybody using your app can add new",
      "offset": 3678.96,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "functionality whenever they want.\n Yeah,",
      "offset": 3680.72,
      "duration": 5.359
    },
    {
      "lang": "en",
      "text": "exactly. And that functionality will be",
      "offset": 3683.839,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "less reliable than the native",
      "offset": 3686.079,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "functionality you have built in.",
      "offset": 3687.52,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "But it will be more it will allow them",
      "offset": 3690.559,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "to extend the capabilities and that's",
      "offset": 3692.48,
      "duration": 6.56
    },
    {
      "lang": "en",
      "text": "kind of\n cool is",
      "offset": 3695.119,
      "duration": 5.361
    },
    {
      "lang": "en",
      "text": "um all right we got a lot of stuff going",
      "offset": 3699.04,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "on in the chat um specific questions",
      "offset": 3700.48,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "people want to do uh if anyone wants to",
      "offset": 3703.119,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "share their their hot take of the week",
      "offset": 3704.96,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "um happy to hear that as",
      "offset": 3707.2,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "Um,",
      "offset": 3721.119,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "so I guess the long the long thread was",
      "offset": 3723.2,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "I'm trying to decide if this will be",
      "offset": 3725.599,
      "duration": 3.041
    },
    {
      "lang": "en",
      "text": "useful for my code generation and review",
      "offset": 3726.72,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "rag. Is this a good way to orchestrate",
      "offset": 3728.64,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "the agentic workflow or not so much?\n And",
      "offset": 3731.28,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "then I think there's",
      "offset": 3734.4,
      "duration": 5.679
    },
    {
      "lang": "en",
      "text": "what? Show me your t-shirt.",
      "offset": 3736.4,
      "duration": 5.679
    },
    {
      "lang": "en",
      "text": "Oh, we got to get people the human layer",
      "offset": 3740.079,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "t-shirt.",
      "offset": 3742.079,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "Oh, uh, merch layer.dev. And if you ping",
      "offset": 3743.839,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "me privately on Twitter or LinkedIn,",
      "offset": 3746.96,
      "duration": 3.119
    },
    {
      "lang": "en",
      "text": "I'll send you a promo code for a",
      "offset": 3748.64,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "discount.",
      "offset": 3750.079,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "Um, let's talk about where do you see AI",
      "offset": 3752.72,
      "duration": 5.359
    },
    {
      "lang": "en",
      "text": "risk surface expanding? Um, I think the",
      "offset": 3755.04,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "biggest area of AI risk surface is",
      "offset": 3758.079,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "actually data leakage and data security.",
      "offset": 3759.599,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "Um, that's super underrated, especially",
      "offset": 3761.359,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "as more people send more stuff to AI",
      "offset": 3763.28,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "pipelines. When an AI pipeline randomly",
      "offset": 3765.52,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "has a tool injected into it, like SQL",
      "offset": 3768.079,
      "duration": 5.441
    },
    {
      "lang": "en",
      "text": "injections for example,",
      "offset": 3770.16,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "you're going to have a like we figured",
      "offset": 3773.52,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "out how to deal with SQL injection. We",
      "offset": 3775.44,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "just escape them. There's no way to",
      "offset": 3777.359,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "escape inputs to your prompts. So like",
      "offset": 3778.64,
      "duration": 6.88
    },
    {
      "lang": "en",
      "text": "that's going to be a huge huge area uh",
      "offset": 3782.799,
      "duration": 5.361
    },
    {
      "lang": "en",
      "text": "that people have to deal with somehow.",
      "offset": 3785.52,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "Um and I'm not sure exactly how we'll do",
      "offset": 3788.16,
      "duration": 5.48
    },
    {
      "lang": "en",
      "text": "that universally.",
      "offset": 3790.48,
      "duration": 3.16
    },
    {
      "lang": "en",
      "text": "What about you, Dex? What do you think?",
      "offset": 3794.64,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "spending",
      "offset": 3796.64,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "uh",
      "offset": 3798.799,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "we waste time on stupid [Â __Â ] that",
      "offset": 3801.44,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "doesn't work.",
      "offset": 3803.359,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "I think the like I don't know there was",
      "offset": 3805.28,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "this joke of like oh computers will",
      "offset": 3806.96,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "allow us to work 100% faster. They're",
      "offset": 3808.64,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "going to double our productivity. Then",
      "offset": 3810.72,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "it's like oh computers generate 100%",
      "offset": 3812.64,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "more work. So it's like it's kind of a",
      "offset": 3814.64,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "wash.",
      "offset": 3817.119,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "I don't know. There's going to be some",
      "offset": 3818.88,
      "duration": 4.76
    },
    {
      "lang": "en",
      "text": "version of that with AI.",
      "offset": 3820.079,
      "duration": 3.561
    },
    {
      "lang": "en",
      "text": "Yeah. Exactly.\n What's the next one?",
      "offset": 3824.079,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "What's your take on cloud code hooks?",
      "offset": 3826.799,
      "duration": 5.56
    },
    {
      "lang": "en",
      "text": "Yes.",
      "offset": 3829.359,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "Uh I don't know what that is. I have to",
      "offset": 3832.72,
      "duration": 5.8
    },
    {
      "lang": "en",
      "text": "go look at that.",
      "offset": 3835.2,
      "duration": 3.32
    },
    {
      "lang": "en",
      "text": "Yeah. It's like cursor. It's like",
      "offset": 3841.039,
      "duration": 3.121
    },
    {
      "lang": "en",
      "text": "extensions that you can write personally",
      "offset": 3842.48,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "that basically will like give Claude",
      "offset": 3844.16,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "code special commands that you can have.",
      "offset": 3846,
      "duration": 2.64
    },
    {
      "lang": "en",
      "text": "So like for example, you can have like",
      "offset": 3847.52,
      "duration": 3.279
    },
    {
      "lang": "en",
      "text": "rename file that just has instruction on",
      "offset": 3848.64,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "what you do or you can make like an",
      "offset": 3850.799,
      "duration": 2.961
    },
    {
      "lang": "en",
      "text": "agent that does research where you tell",
      "offset": 3851.92,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "cloud code spin up 15 agents.\n Yeah.",
      "offset": 3853.76,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "Wait, we've been using this for weeks.",
      "offset": 3856.319,
      "duration": 2.961
    },
    {
      "lang": "en",
      "text": "This is just commands. This has been",
      "offset": 3857.839,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "around forever.\n Yeah.",
      "offset": 3859.28,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "Yeah, that's my point. So we have been",
      "offset": 3862.24,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "having commands all the time, but these",
      "offset": 3864.319,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "are hooks. Is that I'm not getting the",
      "offset": 3866.319,
      "duration": 6.72
    },
    {
      "lang": "en",
      "text": "difference like how it is different.",
      "offset": 3869.2,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "I will uh I will look into this. We did",
      "offset": 3873.039,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "say that we would do an episode on vibe",
      "offset": 3875.2,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "coding, which would be a little off",
      "offset": 3877.2,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "topic for what we generally talk about,",
      "offset": 3878.559,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "but I think could be a lot of fun. So,",
      "offset": 3880.4,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "maybe we'll talk about this one as well.",
      "offset": 3882.4,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "Yeah. Okay. So, pre-tool use, post tool",
      "offset": 3885.2,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "use, notification.",
      "offset": 3887.68,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "I think,\n yeah, this is super",
      "offset": 3890.16,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "interesting.\n They're experimenting with",
      "offset": 3891.44,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "a way to let other people build on top",
      "offset": 3893.839,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "of Cloud Code because it helps build an",
      "offset": 3895.839,
      "duration": 6.641
    },
    {
      "lang": "en",
      "text": "ecosystem for them. Um, and like",
      "offset": 3897.76,
      "duration": 6.799
    },
    {
      "lang": "en",
      "text": "maybe that happens to be true where like",
      "offset": 3902.48,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "people want to use a clawed agent loop",
      "offset": 3904.559,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "to go do something. Um, and it becomes a",
      "offset": 3906.079,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "slightly more general purpose agent, but",
      "offset": 3909.52,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "I think that's",
      "offset": 3910.96,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "that's kind of what that is for them.",
      "offset": 3912.96,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "Um, whether it's good or not really",
      "offset": 3914.96,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "depends on like the quality of the hooks",
      "offset": 3916.48,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "that people build out and like what",
      "offset": 3917.68,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "unique ideas people come up with. It's",
      "offset": 3919.2,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "like the app store and Apple. Like the",
      "offset": 3920.96,
      "duration": 2.079
    },
    {
      "lang": "en",
      "text": "app store and Apple could have been a",
      "offset": 3922,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "total of flop, but it turned out to be",
      "offset": 3923.039,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "really good because Angry Birds is",
      "offset": 3924.96,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "really really fun.",
      "offset": 3926.24,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "Um again no reason\n I'm just using it to",
      "offset": 3928.16,
      "duration": 7.76
    },
    {
      "lang": "en",
      "text": "you know ring a bell once one of my like",
      "offset": 3932,
      "duration": 6.799
    },
    {
      "lang": "en",
      "text": "agent kind of session that is done or",
      "offset": 3935.92,
      "duration": 5.679
    },
    {
      "lang": "en",
      "text": "waiting for permission.\n So it just like",
      "offset": 3938.799,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "a bell so it was not possible before.",
      "offset": 3941.599,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "Exactly. And now you can go do that and",
      "offset": 3944.559,
      "duration": 2.401
    },
    {
      "lang": "en",
      "text": "like that's what I mean there's small",
      "offset": 3946,
      "duration": 2
    },
    {
      "lang": "en",
      "text": "things like that where it's like if they",
      "offset": 3946.96,
      "duration": 2.56
    },
    {
      "lang": "en",
      "text": "don't get the UX just right allows other",
      "offset": 3948,
      "duration": 3.039
    },
    {
      "lang": "en",
      "text": "people to go make things slightly",
      "offset": 3949.52,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "incrementally better.",
      "offset": 3951.039,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "I think that's interesting.",
      "offset": 3953.52,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "Um, is there any world where we can use",
      "offset": 3956,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "DSPI with BAML in the future? Uh, I",
      "offset": 3957.839,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "think they're right about prompts being",
      "offset": 3960,
      "duration": 2.799
    },
    {
      "lang": "en",
      "text": "the wrong abstraction since they",
      "offset": 3961.52,
      "duration": 3.279
    },
    {
      "lang": "en",
      "text": "basically just automate eval driven",
      "offset": 3962.799,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "iteration on prompts, but I don't like",
      "offset": 3964.799,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "how they do structured output. Um,",
      "offset": 3966.079,
      "duration": 6.881
    },
    {
      "lang": "en",
      "text": "I think there's a really underst thing",
      "offset": 3970,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "about how people think about prompts and",
      "offset": 3972.96,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "context in general. Uh, and I think this",
      "offset": 3974.64,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "goes back to what Dex often says about",
      "offset": 3977.68,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "context engineering. The problem that I",
      "offset": 3979.2,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "see generally with the way that people",
      "offset": 3981.68,
      "duration": 2.879
    },
    {
      "lang": "en",
      "text": "are doing automatic eval driven",
      "offset": 3983.039,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "developments on prompts is they they",
      "offset": 3984.559,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "iterate on the whole prompt at the same",
      "offset": 3986.48,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "time and that is not very useful like",
      "offset": 3988.88,
      "duration": 6.64
    },
    {
      "lang": "en",
      "text": "I'll give you an example really fast",
      "offset": 3992.72,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "just to make it a little bit more",
      "offset": 3995.52,
      "duration": 3.039
    },
    {
      "lang": "en",
      "text": "concrete and this is kind of and like I",
      "offset": 3996.72,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "I like bam will be extensible so like",
      "offset": 3998.559,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "once we provide the a then you can do",
      "offset": 4000.4,
      "duration": 2.959
    },
    {
      "lang": "en",
      "text": "whatever you want within Python we just",
      "offset": 4002,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "haven't built that yet so like in some",
      "offset": 4003.359,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "sense yes but fundamentally like let's",
      "offset": 4005.359,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "just take a look at this so I have a",
      "offset": 4007.44,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "prompt that generates this email",
      "offset": 4009.599,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "structure. We talked about this today.",
      "offset": 4011.28,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "If the thing that is not good is the",
      "offset": 4013.76,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "quick recap, there's many ways that I",
      "offset": 4016.16,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "could have solved this. Before quick",
      "offset": 4018.48,
      "duration": 3.119
    },
    {
      "lang": "en",
      "text": "recap used to be a string and it was",
      "offset": 4020.079,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "just not as good. By making it a string",
      "offset": 4021.599,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "array, I was able to get better recaps",
      "offset": 4023.44,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "coming out of it.",
      "offset": 4025.28,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "Whether you call that optimizing the",
      "offset": 4027.839,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "prompt or optimizing something else, I",
      "offset": 4029.44,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "don't know. I don't think\n Well, that's",
      "offset": 4032.079,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "part of the signature. Like if you're",
      "offset": 4033.68,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "using DSP, you'd literally have to",
      "offset": 4035.28,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "physically change the signature.",
      "offset": 4036.72,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "Exactly. And that's why I think like the",
      "offset": 4038.4,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "actual prompting part isn't actually",
      "offset": 4040.48,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "constrained to what it is. And we might",
      "offset": 4042.319,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "even find that the right way to do this",
      "offset": 4044.24,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "is like uh recap might be like a recap",
      "offset": 4045.52,
      "duration": 5.519
    },
    {
      "lang": "en",
      "text": "might look like this. It might actually",
      "offset": 4048.96,
      "duration": 5.359
    },
    {
      "lang": "en",
      "text": "be like title string uh description",
      "offset": 4051.039,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "string. And like what I really want to",
      "offset": 4054.319,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "do here is like this.",
      "offset": 4055.76,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "And whether and maybe I want to add a",
      "offset": 4058,
      "duration": 5.359
    },
    {
      "lang": "en",
      "text": "description here at most three four to",
      "offset": 4060.079,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "10 words. And now it's getting to be",
      "offset": 4063.359,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "really sophisticated at a way that like",
      "offset": 4066.079,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "you typically don't want to navigate",
      "offset": 4068,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "English itself.",
      "offset": 4069.839,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "And that's I think the main problem that",
      "offset": 4071.76,
      "duration": 3.279
    },
    {
      "lang": "en",
      "text": "I see with most prompt optimization",
      "offset": 4073.359,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "tools is that they try and navigate the",
      "offset": 4075.039,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "middle chunk of the prompt, but they",
      "offset": 4077.68,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "overlook the fact that the prompt itself",
      "offset": 4079.92,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "is comprised of a giant a that is very",
      "offset": 4081.68,
      "duration": 5.119
    },
    {
      "lang": "en",
      "text": "very complex. And as our programs become",
      "offset": 4084.4,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "more complex, we'll become harder and",
      "offset": 4086.799,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "harder to just like edit in the pure",
      "offset": 4089.68,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "prompting side.",
      "offset": 4091.44,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "I will say like just to play devil's",
      "offset": 4093.68,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "advocate like for the DSPI side like",
      "offset": 4095.2,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "rather than prompting like I think and",
      "offset": 4096.96,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "like maybe maybe I don't fully",
      "offset": 4099.52,
      "duration": 2.719
    },
    {
      "lang": "en",
      "text": "understand this but like my my",
      "offset": 4100.88,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "understanding would be like rather than",
      "offset": 4102.239,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "telling the LM to use X number of words",
      "offset": 4103.6,
      "duration": 5.119
    },
    {
      "lang": "en",
      "text": "you would basically set up a metric and",
      "offset": 4107.04,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "then you would count the words that came",
      "offset": 4108.719,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "out and then you would like penalize it",
      "offset": 4110.239,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "for too many words or not enough words",
      "offset": 4112.08,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "and then over time the model the prompt",
      "offset": 4114.319,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "engineering framework would probably",
      "offset": 4116.319,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "learn to say okay the string that gets",
      "offset": 4117.759,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "the right number of words is to say",
      "offset": 4120.08,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "these like use four to 10 words, but you",
      "offset": 4122,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "wouldn't have to write that. You would",
      "offset": 4124.239,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "write the hey, here's the rules, here's",
      "offset": 4125.52,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "the metric.\n Yeah, that's that's exactly",
      "offset": 4127.359,
      "duration": 5.601
    },
    {
      "lang": "en",
      "text": "right. Like you kind of you I mean you",
      "offset": 4129.92,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "give it a signature and then you express",
      "offset": 4132.96,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "kind of an evaluation function that you",
      "offset": 4134.719,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "know can be any combination of like",
      "offset": 4137.279,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "string based you know any properties of",
      "offset": 4139.279,
      "duration": 5.121
    },
    {
      "lang": "en",
      "text": "the generated thing if it's structured.",
      "offset": 4141.6,
      "duration": 5.119
    },
    {
      "lang": "en",
      "text": "Um, it can also use LLM as a judge if",
      "offset": 4144.4,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "you want and then it will kind of",
      "offset": 4146.719,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "iteratively run that against, you know,",
      "offset": 4148.64,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "an LLM with your evaluation function and",
      "offset": 4150.799,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "it'll figure out like which, you know,",
      "offset": 4153.199,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "it'll try update like having the LLM",
      "offset": 4155.759,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "write things to update parts of the",
      "offset": 4158.319,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "prompts that are not performing well and",
      "offset": 4160.64,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "then see how they behave. It's almost",
      "offset": 4163.12,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "like\n like conceptually like a little bit",
      "offset": 4164.719,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "like a genetic algorithm. It'll try",
      "offset": 4166.96,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "different things to improve the parts",
      "offset": 4168.719,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "that the evals um\n determine are",
      "offset": 4170.319,
      "duration": 5.121
    },
    {
      "lang": "en",
      "text": "performing badly and it'll pick whatever",
      "offset": 4173.52,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "helps the most and and kind of add that",
      "offset": 4175.44,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "to the prompt iteratively. So yeah, you",
      "offset": 4177.52,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "just express like what good looks like",
      "offset": 4179.199,
      "duration": 5.281
    },
    {
      "lang": "en",
      "text": "in that evaluation however you want to",
      "offset": 4182.159,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "and then it will",
      "offset": 4184.48,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "figure out okay what actually works.\n The",
      "offset": 4186.719,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "only tricky part about that is that part",
      "offset": 4189.92,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "of doing the application on what you",
      "offset": 4191.839,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "actually want to go change in the prompt",
      "offset": 4194.159,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "is really really varied.",
      "offset": 4195.679,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "So there are some things that you could",
      "offset": 4198.239,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "do. For example, like you could this",
      "offset": 4199.6,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "like the sentence of at most 410 words.",
      "offset": 4201.6,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "Let's assume that a human didn't write",
      "offset": 4204.32,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "that but a model did. You can write that",
      "offset": 4205.52,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "in many different places. You can write",
      "offset": 4207.92,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "that here. You can write that uh here.",
      "offset": 4209.44,
      "duration": 6.799
    },
    {
      "lang": "en",
      "text": "You can write that you can write that as",
      "offset": 4213.52,
      "duration": 6.32
    },
    {
      "lang": "en",
      "text": "a description field here. Oops.",
      "offset": 4216.239,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "Yeah. And so that's kind of what it's",
      "offset": 4219.84,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "figuring out for you is like not just",
      "offset": 4221.199,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "what should be added to it, but where",
      "offset": 4223.199,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "should it be added to it to get really",
      "offset": 4225.12,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "good results.\n Yeah, exactly. So that",
      "offset": 4227.199,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "will become possible once we expose the",
      "offset": 4229.36,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "a but we really need to just expose the",
      "offset": 4230.8,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "a in an iterative way and then like the",
      "offset": 4233.04,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "model can decide how to go inject it",
      "offset": 4235.44,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "because what you're really doing here is",
      "offset": 4237.36,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "you're modifying the code at runtime",
      "offset": 4238.56,
      "duration": 5.599
    },
    {
      "lang": "en",
      "text": "and that part is the part that needs to",
      "offset": 4242.32,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "be uh analyzed and retroactively like",
      "offset": 4244.159,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "you need to be able to walk the syntax",
      "offset": 4246.64,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "that we have and like clearly we can",
      "offset": 4248.08,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "walk the syntax but we just don't expose",
      "offset": 4249.84,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "an SDK to go do that yet but we will",
      "offset": 4251.36,
      "duration": 5.359
    },
    {
      "lang": "en",
      "text": "that we've tried iteration iteration",
      "offset": 4254.8,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "loops like this before and it seems to",
      "offset": 4256.719,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "work pretty Especially if we're",
      "offset": 4258.239,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "generating new schemas, that's where we",
      "offset": 4259.92,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "find that the model is really really",
      "offset": 4262,
      "duration": 2.64
    },
    {
      "lang": "en",
      "text": "good. Like for example, if the recap is",
      "offset": 4263.28,
      "duration": 2.56
    },
    {
      "lang": "en",
      "text": "bad, it will actually just create a",
      "offset": 4264.64,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "schema. Uh or it'll change it like a",
      "offset": 4265.84,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "string array or something else inside.",
      "offset": 4268.08,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "And that seems to work quite well.",
      "offset": 4269.92,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "Cool. Um last question I think we can",
      "offset": 4272.88,
      "duration": 6.319
    },
    {
      "lang": "en",
      "text": "call it. Yeah, go ahead. Um I like the",
      "offset": 4276.4,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "what's the coolest thing you've seen",
      "offset": 4279.199,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "built with BAML?",
      "offset": 4280.239,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "Clearly this guy just came here to get a",
      "offset": 4282.159,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "BAML pitch and I'm happy to oblige.",
      "offset": 4283.6,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "Um what's the oh sorry there's like",
      "offset": 4287.36,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "seven or eight different question let me",
      "offset": 4289.52,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "just go down really fast I the first one",
      "offset": 4290.719,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "I saw was what's the versioning just use",
      "offset": 4292.32,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "git or like have two variables that you",
      "offset": 4294.239,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "pass in and forth like if you have two",
      "offset": 4296.8,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "variables you can just use that the",
      "offset": 4298.56,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "coolest thing that I have seen built are",
      "offset": 4300.159,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "honestly not about the extent of the",
      "offset": 4302.32,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "pipeline it's really about how good they",
      "offset": 4304.08,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "are so for me things are personally",
      "offset": 4305.92,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "really really cool when they actually",
      "offset": 4308.159,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "work like 99.9% of the time like this",
      "offset": 4309.44,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "email content pipeline that we built",
      "offset": 4312.64,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "wasn't that cool last week to me because",
      "offset": 4314.159,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "it didn't actually produce a good email.",
      "offset": 4315.6,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "The email that we just saw today was",
      "offset": 4317.52,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "pretty good. That becomes cool. What",
      "offset": 4319.84,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "I've seen are pipelines that process",
      "offset": 4322.56,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "like bank statements with like 100 pages",
      "offset": 4324,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "plus. Those are really exciting when",
      "offset": 4325.52,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "they don't have errors. Um pipelines",
      "offset": 4326.88,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "processing tax forms, those are really",
      "offset": 4329.44,
      "duration": 3.279
    },
    {
      "lang": "en",
      "text": "exciting because again, very sensitive",
      "offset": 4331.199,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "data and it just works. Uh hospital",
      "offset": 4332.719,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "records. Uh I think those are I think",
      "offset": 4335.52,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "the most interesting problems to me",
      "offset": 4338.159,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "personally. But I think things that",
      "offset": 4339.52,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "other people probably find really cool",
      "offset": 4341.52,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "are probably along the lines of um like",
      "offset": 4343.04,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "there's this agentic framework that",
      "offset": 4345.92,
      "duration": 2.799
    },
    {
      "lang": "en",
      "text": "someone built that not agentic framework",
      "offset": 4347.199,
      "duration": 3.121
    },
    {
      "lang": "en",
      "text": "an AI assistant that someone built that",
      "offset": 4348.719,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "you can just talk to in your car and",
      "offset": 4350.32,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "just does like seven or eight different",
      "offset": 4352.08,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "things and they did that by adding a",
      "offset": 4353.52,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "bunch of MCP tools. I think if you look",
      "offset": 4355.36,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "in our share your share your creations",
      "offset": 4357.36,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "channel will have it in there that I",
      "offset": 4359.28,
      "duration": 5.919
    },
    {
      "lang": "en",
      "text": "thought was really really really wild.",
      "offset": 4361.6,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "Um what about you Dex? I know you've",
      "offset": 4365.199,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "built some really cool things.",
      "offset": 4367.44,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "Yeah. Uh I",
      "offset": 4370,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "I think it's less about like what you",
      "offset": 4373.12,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "can do specifically with BAML and more",
      "offset": 4375.6,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "like it's all about the application,",
      "offset": 4377.84,
      "duration": 2.879
    },
    {
      "lang": "en",
      "text": "right? Like BAML just helps you get the",
      "offset": 4379.04,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "performance pretty good without having",
      "offset": 4380.719,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "to think about tuning and tool calls and",
      "offset": 4382.719,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "all that kind of stuff. Um and it",
      "offset": 4384.96,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "provides a good development experience",
      "offset": 4386.56,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "for testing and evaluating this stuff.",
      "offset": 4388.96,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "So, I mean, I've built two things with",
      "offset": 4390.96,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "Pamel that I really like, which is like",
      "offset": 4393.12,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "a Slackbot that manages all of our",
      "offset": 4394.719,
      "duration": 5.601
    },
    {
      "lang": "en",
      "text": "GitHub deployments and a email assistant",
      "offset": 4396.88,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "that like you can just forward emails to",
      "offset": 4400.32,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "it and it like looks up a list of tools",
      "offset": 4402,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "with BAML and responds and has like just",
      "offset": 4404.159,
      "duration": 5.841
    },
    {
      "lang": "en",
      "text": "like has become it went from being just",
      "offset": 4407.6,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "like trash dumping JSON into the context",
      "offset": 4410,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "window to something that's really really",
      "offset": 4412.32,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "clean and really high reliability and",
      "offset": 4414.239,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "works really fast and doesn't use too",
      "offset": 4416.08,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "many tokens. So, it was fun iterating in",
      "offset": 4417.84,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "that direction with it.\n Yeah. I think",
      "offset": 4420.4,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "the things that I found really cool in",
      "offset": 4423.36,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "general, the ones that you built is just",
      "offset": 4425.199,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "like the UX pattern of like being able",
      "offset": 4426.56,
      "duration": 3.119
    },
    {
      "lang": "en",
      "text": "to interrupt the workflow when it's",
      "offset": 4428.239,
      "duration": 3.121
    },
    {
      "lang": "en",
      "text": "running with like a feedback loop. That",
      "offset": 4429.679,
      "duration": 2.961
    },
    {
      "lang": "en",
      "text": "was not a thought process that really",
      "offset": 4431.36,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "came across my mind until we worked",
      "offset": 4432.64,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "together for the first time, which is",
      "offset": 4434.08,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "idea that like I want human guided",
      "offset": 4436.08,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "agents where I can like that's why",
      "offset": 4438.239,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "cursor is so freaking good. like cursor",
      "offset": 4439.6,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "is really good or any of these systems",
      "offset": 4443.44,
      "duration": 2.799
    },
    {
      "lang": "en",
      "text": "like cloud code is really good because",
      "offset": 4445.04,
      "duration": 2.639
    },
    {
      "lang": "en",
      "text": "while the agent's working I can chat",
      "offset": 4446.239,
      "duration": 3.041
    },
    {
      "lang": "en",
      "text": "with it and it can interrupt change the",
      "offset": 4447.679,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "workload into the direction that I want",
      "offset": 4449.28,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "that UX is really important to build",
      "offset": 4451.04,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "this is Karpathy talked about this in",
      "offset": 4454.159,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "his YC talk he was like like good AI",
      "offset": 4456,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "tools have an autonomy slider there's",
      "offset": 4458.32,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "like zero autonomy where I'm editing the",
      "offset": 4460.8,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "code there's a little bit of autonomy",
      "offset": 4462.64,
      "duration": 3.519
    },
    {
      "lang": "en",
      "text": "where it's like tab complete but it just",
      "offset": 4464.4,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "shows like suggestions and I accept them",
      "offset": 4466.159,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "and then there's like full agent mode",
      "offset": 4468.719,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "and then there's like other there's ask",
      "offset": 4470.64,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "mode, there's like things in the middle.",
      "offset": 4472.56,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "And so like being able to build AI",
      "offset": 4473.92,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "experiences that have that again",
      "offset": 4476.56,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "autonomy slider, different levels of",
      "offset": 4479.12,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "autonomy for different modes of work and",
      "offset": 4480.96,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "like polish versus broadstrokes versus",
      "offset": 4482.96,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "planning versus high level um I think is",
      "offset": 4485.36,
      "duration": 5.359
    },
    {
      "lang": "en",
      "text": "really important.\n And like full agent",
      "offset": 4488.8,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "mode is interesting, but full agent mode",
      "offset": 4490.719,
      "duration": 2.721
    },
    {
      "lang": "en",
      "text": "without being able to stop it and",
      "offset": 4492.159,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "correct course correct along the way is",
      "offset": 4493.44,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "really really really bad.",
      "offset": 4495.76,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "And like that's why that UX is critical.",
      "offset": 4498.56,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "Um, but with that, I think that's",
      "offset": 4501.12,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "today's episode. Uh, hopefully you guys",
      "offset": 4502.8,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "saw the content. We'll push all the",
      "offset": 4504.48,
      "duration": 2.48
    },
    {
      "lang": "en",
      "text": "prompts out, so you'll have access to",
      "offset": 4505.6,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "it. Uh, and today's email should come",
      "offset": 4506.96,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "back in a fully AI generated way. So,",
      "offset": 4509.44,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "um, it's going to be fun to go see and",
      "offset": 4511.84,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "you'll see the ghetto read me launch",
      "offset": 4513.84,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "soon as well. And if this week goes",
      "offset": 4515.12,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "well, then we'll start uploading pretty",
      "offset": 4516.88,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "um, if if things if this week goes well,",
      "offset": 4520,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "then we might start increasing the",
      "offset": 4523.04,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "upload rate to Wednesday because we got",
      "offset": 4524.239,
      "duration": 5.361
    },
    {
      "lang": "en",
      "text": "an automated pipeline. Uh,\n remember when",
      "offset": 4527.199,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "you wanted to hire an intern to do this",
      "offset": 4529.6,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "work?\n Yeah. It turns out eight hours uh",
      "offset": 4531.44,
      "duration": 6.719
    },
    {
      "lang": "en",
      "text": "16 hours of our time does the job, too.",
      "offset": 4534.32,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "Oh, god. Dude, this is super fun. Thanks",
      "offset": 4538.159,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "everybody for the questions. Catch you",
      "offset": 4540.4,
      "duration": 5.64
    },
    {
      "lang": "en",
      "text": "all next week. Bye-bye.",
      "offset": 4542,
      "duration": 4.04
    }
  ],
  "cleanText": "And I'm gonna shoot you the whiteboard.\nAll right. I'm gonna send you the whiteboard.\nPerfect. How's it going everyone? Um, we're back to episode number 12, 13. Uh, and today our whole goal is we're going to talk about how to get the tone just right when we're prompting. I think a lot of times when we're building a prompt, we talked about this last time when we built the AI content pipeline. It's really first, it's essential to get the information correct and presented the way you want to get it presented, but then you want to go do part two. And part two is actually making the prompts do what you want them to do. And I think last time we spent a lot of time talking about the infrastructure, the tooling, and also just getting the data across and some by coding processes that Dexter and I use when we go do this. I think today our goal is we want to talk a little bit more about exactly how we're going to go ahead and make these prompts better. So just to recap everyone, Dexter, do you want to show everyone what like we were do what we were doing yesterday with yesterday's pipeline or last week's, sorry? Yeah. So like I have the whiteboard here. I just made a copy of the one from last week because we're going to add to it. Um, but we built this system with a back end and a front end and a data bot base and kind of an API in between them. Um, and these kind of pipelines where we would take a Zoom meeting idea, we download the video and send it to YouTube and summarize the transcript and then draft a bunch of content, uh, based on that video. Um, and we had a little bit of like a human loop refinement workflow. Um, let me see if I I think I have it running. Um, actually, I'm you can just run the new one. It's fine too. Yeah, there you go. Yeah. So this is this is the updated version of it. Um, so you can see we've kind of done some imports. Um, we've added a bunch of stuff, um, like connecting the Luma events to the Zoom meetings using some mix of AI and not AI. Um, we've added, uh, what is it? This be the ability to create a draft PR in GitHub, which I'll show you what one of those looks like. Um, but then it's, you know, we've got the summarization of the key points. We have this timestamp stump that we looked at and then it will generate a Twitter thread and a LinkedIn post and an email for and some of you from last time probably got this email and you probably noticed that we had a little bug in there where it said hello first name. It turns out when you do generate content, go ahead. Did we send the email with hello first name? We did because the email templating system that we have doesn't actually it needs a variable plugged in in a very specific way for the actual first name and you know, parts of doing AI generated content is that mistakes like this happen. I mean, that would happen even if you're a human, so it's not just restricted to humans are just AI pipelines, but this is like a bug that we have to be aware of because to take it to like that last degree, we're always going to run into these kinds of edge cases where like maybe the AI gener and we'll inject first name later in the pipeline as well. So we have to think about that and think about how we can make that error better along the way. Yeah. Um, and then yeah, the other the other cool feature was basically, um, and we can get into this code as well, I think. Um, but if we go to I can show you, um, this auto here's an autogenerated pull request for last week's episode. Um, and so it's using a different title and a couple other. Click on the top right instead of code, instead of the markdown view, through the full proper view on the bottom. Oh, not there. Sorry. Uh, I'll draw on your screen right here. Oh. Oh, I see. Yeah. Um, so it's done a pretty good job. If you've seen the other readmes in this repo, it's looks a lot like the other episodes that we've done. Oh, that's a workshop. Uh, if we look at one of these and you come look at the read Oh, okay. Oh, this one. Yeah, that was your episode. You were supposed to upload the readme for that one. I know, I know, I know. Um, but so here's like our normal readme format where we'll have some whiteboards and then we'll have some code and then we'll talk about how to run it. So, it's it's it's able to and we'll look at how this context gets built, but basically looking at previous readmes, it's able to draft a um what this what this should look like and leave we've prompted to like leave certain sections blank because we'll just the goal is always to like reduce the amount of human effort. Um, I don't think we'll ever eliminate it and if we if we if we if we hold off on deploying a system until everything is perfect, then we'll never get anything out. Um, and I think this adds a lot of value with a couple while still, you know, leaving a couple things. It goes to a couple. Can you go up really fast? That? Yeah, it goes to like a couple things. For example, the whiteboards, then we'd have to build an Excal integration that can go pull data from there. And that just takes time and getting an LLM to exactly take the perfect screenshots. Like, that's a problem that's easier for Dexter and I to do than to write the code for doing this. The goal of projects when you build AI pipelines is not to automate everything. The goal of a project is to get a good value of time for uh good value for the time that you put in. So Dexter and I take roughly five to eight hours to automate this to do this work every week. If we're going to go do that, spending 10 hours to automate it isn't that bad, assuming that we keep doing more content, which I think we will for a while. Yeah. Um, but the whiteboard stuff is, you know, I can literally just come here and grab one of these and copy it and then paste it into GitHub and that takes me 30 seconds. And to write the code, it's like the easiest part of the whole workflow. So we're like, cool, we're not going to write code for that. We're going to let the LLM do things the LLM is really good at and we're going to write, we're going to just manually do the things that we can do. And it's not that we can't do the AI part. It's just not worth the time. And I think that's what we go into because if the whiteboards are bad, then we have to go in and go edit it. And that is where it breaks down. Kyle, then your point about what to do is different. I will also say that I think we do quite a bit of uh customizing the whiteboards. So like this whiteboard is actually four different screenshots that are like ordered and annotated in a specific way because the the full whiteboard would be unintelligible. Exactly. So it's not that we couldn't do it. We could definitely build a pipeline that automates this. It's just not worth the time. It's faster for Dexter and I to spend five minutes doing that work than uh not doing it. With that, I think I want to talk about I want to talk a little bit more about the architecture here and the kinds of data that we were able to plumb through. Dexter drew an amazing whiteboard that actually talks about how to collect some of this data for this pipeline. Talks about some more of the problems that we have because before we can get the tone right again, uh let's show and let's show the email that we have really fast, Dexter, just so people have it. I think you should have it as well. Not this one, the other one that we actually sent out, the one that actually got sent. Uh, I think I have that one. Uh, let's see. And let's talk about some of the details that we need in order to get the tone right because again, before we can actually do the prompting, we all have to understand what are we aiming for and what is the golden data set that would really, really be good. If you look hello first name, you'll find it. Yeah, there we go. There you go. Uh, so scroll down. So, there's a couple uh, sorry, a little bit up. I realize that I can't control your screen. There's a couple of things that we need. One, if you go up, uh, we need the title to be correct. Uh, this is very, very important. We need some sort of nice line here that feels like a flow that is kind of different every single time, probably because it'd be boring to read the same thing or we'd rather not have that line. Um, we probably don't need anything special here, but we do need the links to be correct for the YouTube video and for the GitHub repo. Ideally, we would put the right link to the folder there. The only reason we don't do that is because it takes work to find that folder, and that's annoying, but the email would be a better email if it took you right to that folder. So we'd like to go do that. Oh, way to find a folder. I can show you that. We'll talk about. I know we'll talk about that. I saw your code. We want to have a little summary here that is not too long, um, and really varies based on the task that we're doing. And then do you want to scroll down? Yeah. And then after that, what we want is we'll really want like this one key takeaway because like we said, it's just useful to have one good summary rather than five bullet points written over here. And then we'll want to. And then the last part, go down. Yep. Clear my screen. There we go. Sorry. Um, and the last part we'll want is really about like action items for what someone can do next. That means the Luma link has to be right. The title has to be right. And then we probably want to tell people the right date of when they can go go join this next event. So there's a lot of components to this that if we don't get it perfectly right, it's going to be annoying. The part where tone is the most important is really this middle section about the content and the summary. We don't want it to be too long. We don't want it to be too redundant. We don't want to use like AI hype words. We wanted to really just nail like the content part of it, which is hard, uh, because if you if you trivially prompt a model, at least from what I have tried, most of the time it doesn't really do that correctly. So getting that tone of like content to, uh, uh, content to quality is going to be a little bit tricky to get. But really a lot of it is about plumbing in the right data into the right places, the right YouTube link, the right folder names, the right next, uh, workshops, the right oneline takeaway, the right titles. A lot of things are really just about assembling the right data. So in order to do that, go ahead. Yeah. So so so I think this is where you're going. Um, are you are you kind of driving towards My instinct as we talk about this is almost like creating a plain text template and then filling it in with smaller bits of LLM generated content or do you still want the because we talk about also like these longer strings with lots of like random characters in them are a little bit hard for AI to reproduce super reliably. I mean, I think a short URL like this, I've never seen a frontier model ever screw that up. I think we toss in a big model like two Gemini 25 GT40, uh, like Sonnet, um, or Sonnet or something like that, it'll probably not mess up a short URL like that that doesn't have too many weird characters. It should almost definitely work for them. Yeah. But I think the more important thing is actually that I want to share with everyone is the thing that we did before we did all of this work, which is we drew that diagram and that diagram is essential to actually understanding it. So, let's pull that up really fast. So, this is the behind the scenes of how Dexter and I actually built this pipeline, how we talked about it because it not it's not just enough for us to be able to build it. We really need to be able to talk about this in a really nice way that helps us understand what's happening. Well, and it's like the dependencies, right? It's like, okay, if I want to write the email, what are the pieces I need? Okay, I need the summary and I need the next event and I need the YouTube URL and I need the thumbnail URL. I don't need the thumbnail, but I need the YouTube URL to go into the summary. So it's like what are all the pieces and how do we get them? And this was how we sort of like even like broke up the work and and worked kind of asynchronously on this stuff. Exactly. So like it all starts with the Zoom meeting and that's what we had before. Then what we did part two is we added the Luma integration and that was a little bit trickier than most people would expect, which was what we wanted to do was using Luma just directly pull out data and connect it to the Zoom meeting. And that that's that's what looks that's what this is over here, right? Is so we we fetch the list of Zoom recordings and then we try to match them to a specific Luma event. Uh, this happens to be connected to Vibb's uh company Zoom. So there's a bunch of random meet. Yeah, there's a there's a bunch of random uh meetings in here that aren't connected to Luma events. Um, and so being able to match those to the event lets us get the title in properly and lets us get the summary of the event in properly that helps us prompt everything else more more correctly. This is all just like engineering the right context. Yes. Do you want to share a little bit about um the Luma side of the pipeline really fast? How we're able to go do that? Yeah, let's go pull the problem. The problems in there are I think are not very obvious. Yeah. So this function is actually quite long. Um, we have two things. We have fetch the upcoming event, which is basically like let's look at a list of events from Luma and decide which one is coming soon is the next event because when we generate the email, we want to be able to figure that out. And you could do a bunch of logic of like, okay, let's look at the start time and then like let's filter them to only ones in the future. This is deterministic code. I mean, AI can do this, but this is basically just math. And so your code should do this for you. And then we'll sort them by the start time and then that's when we go to\n\nThe AI side. So we give a very basic description. This is um, we have this Luma event object, which is lots and lots of fields that we're going to use in lots of other places. But actually for the thing we pass the LM, we're doing a custom serializer. So we're just sending a subset. We're only sending the next 10 events. Um, and then we're going to ask, yeah, I want to pause there really fast. This is what Dexter often says as context engineering. The trivial thing to do is pass in the Luma event object. We could all do that. The right thing to do so we don't have to think about it is context engineering, where we decide and know that here's the context that is relevant for the LM. Let's give it that context. And now we're, and now we don't have to think about it anymore. Yeah, and it all ends up going into the prompt. So you could call this prompt engineering, but since it's like basically you're ragging, you're retrieving data in a custom way and feeding it to the LM in the way that's going to work best for the LM. Yeah. So we took out, we took out as much data as possible and then just put stuff in. So like if you, oh, go ahead. And if you, if you haven't seen this picture before, um, context engineering is basically embracing that everything that you pass to an LM, whether it's the prompt or the agentic history or something called memory or rag or how you tell Tone, it's all just tokens to the LM. And so you need to think of all of these things together and like even the order in which you pass these things in is a practice of engineering of trying different things and finding what works. And the only reason that structured outputs lives on the boundary here is because you do have to tell the LM how to do structured outputs in some way. Whether you use JSON mode, whether you use any SAP, any other XML, it doesn't really matter. That's you telling the LM how you want to do it. And at some point, you'll write some code that converts the response of the LM to the structured output. And that's why it lives across the boundary of both things because you need regular code and I'll send a way to describe it to the LM as a part of its context on what you're trying to go do. But everything that we're going to do with an LM is always complex engineering. Cool. Yep. Uh, cool. Back to the code. You ready to look at the prompt? Let's show the prompt. Cool. So, here's our prompt. Um, let's see. So, we got to see which event is the next AI that works event. Um, so we're going to loop over there those events. We're actually going to format them even more. Um, and so I think I have a test for this. Now, um, oh god, just make a test. Was it summarize ammo? Yeah, I'll probably make a test pretty pretty. I think it'll it might make a good test. Uh, but while we go read the prompt, there's something nice that we do, which is like, and the reason that Dex did this is because our Luma doesn't only have a couple of uh events. It has multiple events. Sometimes we do in-person ones, sometimes we use virtual ones. And rather than thinking about writing the code to only filter for things because sometimes we forget the emoji, sometimes we name it uh differently, sometimes we don't call it AI that works. There's so many reasons why the event might not match. We just let it know that these are the general constraints. And now we just let the LM go pick this. And this is kind of what I think the superpower of LM is, which is you don't actually have to get it perfectly right. You can get it mostly right. And now you can go ship something really fast. It's like you don't have to make your code perfectly fast. You can just use Python and that's acceptable. Um, you can put current date or something. Yeah. Um, 601 and start date is 0701. Let's put it on 625. We'll actually do a realistic one. And this problem's pretty easy. So, yeah. And this problem's pretty easy, so it should just work. And can you put a past state event really fast just to make sure that it does work correctly in case we had no events that were correct? Um, bye Bob. Birthday Zoom. I don't know what your birthday is, but he's turning what are you, like 22? Yep, exactly. And maybe 16 again. Um, cool. Uh, should be probably different. Yeah. Yeah, that's interesting. I bet this is specifically a bad test because yeah, the LM will see that those IDs are just like stubs. Um, anyway, yeah, that's fine. Yeah, cool. Um, sorry, you were talking about this prompt here. Yeah, I want to talk about one thing in the prompt. Uh, if you scroll up, Dexter, look at what model you use. Yeah. Uh, we use a really small model because this is Yeah, it's a super easy task, so we don't have to think about it. So, we just go do this. Um, and then we do our, this is a best effort thing. If we don't find an event, we don't find an event. It's probably a user bug, so we can go fix it. It's not really, we don't, this isn't super critical to us. We can just go change the Luma event description and make it match manually as well. Yep. Yeah. Yep. Where did we pass this in? Sorry, I'm trying to find the trying to go find where we called this thing. You can just command click from here. Oh, not from there. Oh, you have to search this command F. Yeah, here we go. Yeah, sorry. So, if we don't get a result, we say, \"Hey, we couldn't find the next event.\" Um, otherwise, we go pull that event out and we return it. Yep, that's it. So, it's a pretty straightforward thing. We just rely on the ID matching to do the trick. Um, so we return our original Luma event that is enriched, but the LM only produces the one that is not as enriched. So, it gives us a better Go ahead. Yeah, I was going to say, and like I don't think we should spend time on this today, but this would feels like a perfect example of like where you're always talking about like don't use the actual native IDs because they can be long, but like replace this with like K1, K2, K3 kind of, right? Exactly it or just like event ID one. In this context, I would call it like event underscore 123456. It's just going to be easier for the model. Yeah. Yeah. Yeah. It's one less thing the model has to think about. Yeah. Um, cool. Anyways, that's that code. Let's pop. And then one last piece of code I want to show, which is the folder code. The folder matching code. This is also kind of Oh, yeah. Yep. Actually do all of this stuff to go produce the GitHub reviews to go send the right email to the right folder. Like the email can't really be sent until we have the GitHub PR ready as well because we need we want to link you to the right folder ideally. So, how are we going to go do that? Yeah, exactly. Kyle nailed it. It's like it it just it's just it's the token probability thing. Tokens that are random are just not in the data set of the model. So, it's just going to at some point start it's going to be like I'm printing out a UUID. Let me just print out UUIDs and those are basically random. And you're you're praying that the LM is really, really, really good at needle hack tests. Yep. Um, so what we're going to do is we're going to fetch the existing folders and some file contents. We're going to get the next folder name and a couple other things. We're going to use the next folder name to create the readme for the new branch. Um, so this readme we had in our pull request. This file, the our application needs to know this full exact path. So it needs to know the folder name and that folder may already exist on the main branch or it might not and we need to generate it. And so we need like this is a thing you could probably write a bunch of code for to determine like build deterministically every time. It would be really hard to build the actual slug like the the name for the folder. Um, so what we did was we took the existing list of folders and I'll show you the code and then we took the title of the episode and the date and we said, hey AI model, like either pick the existing folder if it's already there or generate the name of the new folder if it's not there. And all of these get added. We do this manually every every week. They get added to this like repo like route the list of episodes. Um, can you go down really fast and also mention the other part the table? Uh oh, yeah, and then we also update. Yeah, we also update the table. Yeah, good call. Let me show. So let me show what that looks like. And there's there's basically every episode, every week we push the YouTube link and the code link for the past episode and then we post the description about the next episode and we have the Luma link. And so that's why in order to do the GitHub updates, we need not only the uh past stuff, but we also need the next event and its link. So we need the title and the link to the next event so that we can populate this uh where is this um this table as well. And the whole and the whole point of going really deep into these concepts is talking about how much work it takes to get Tone just right. Like in order to get the tone perfectly right on the email, the first thing we really have to get is like we have to break down the problem to make sure that the data is correct. If the factual data is wrong, the tone doesn't matter because I'll just read it and it won't work. But we have to actually get all of that correct and if we don't have the right information. Yeah. Well, oh, like I just re-reading what you said like the to generate to like solve this problem, we need to know what the boundaries, what the walls of the box are. I think I want to say where it's like, okay, we're going to have to this is going to be a hard-coded thing that we're going to have to have to force in. I don't want to make the AI generate that. This is going to be a hard-coded thing. And so that kind of starts to structure, okay, what are we going to ask the AI to do and what do we want to be deterministic? Because once we have that structure and those pieces, then we can start to tone engineer, content engineer each of the different parts of that LM's workflow and break down the problem. Exactly. And then we're basically just trying to break down exactly what said. We just try to break down the problem as much as possible to get it perfect. And that's because we care about the tone of the email going out. And we really just want it to be automated, but we also don't we'd rather do the work manually than send out something automated that's shitty. So given that work, we have to put in a lot more effort to make it happy. It's unlikely that you can get to the perfect tone with just prompting. You will have mistakes like hello first name. You will have um mistakes like wrong YouTube URLs. You'll have mistakes like wrong Luma links. And those are basically catastrophic in these scenarios. like it just loses a lot of trust. So, we want to make sure those all things get squared away. Um, answer. Yeah, sorry. Quickly answer one of the questions from the chat, which was where are the where are the diagrams? Um, I did move this file to readme.md.backup just so we could test the PR stuff. But, um, let's let me just put it back so you can see it. This will have all of the whiteboards in it. Um, so you can see all the whiteboards from last week in the previous episode notes. The new one that you're talking about, uh, Pashant, the actual architecture, that will be coming in in this week's episode. Yeah, this one. Um, cool. Uh, we haven't seen spiral computer. Let's to go back to the folder code deck. You want to pull that up and then we'll start prompt engineering after we do that and actually talk about the email content. Yeah. Yeah. Yeah. Yeah. Go ahead. Are you supporting native Rust? Uh, we will. We're I think today we're going to release Go support and then Rust support should be easy. But let's talk about this today. I want to talk about the folder content. Yeah. So, um, the workflow for GitHub is again, we will figure out the episode repo path, which is like that's how we generate the folder name and then we generate the readme for the episode and then we generate the updates for the root readme and then we are going to basically use a uh a repo called supersonic from the case folks that lets you just create GitHub PRs in a single line. So we give the repo name, we give it the files we want to update. Yeah, it's dope. Um, and then the branch named. Yeah. A couple of the things that we're doing. Can you go down? Yeah. Um, uh, which is we're actually hard coding these file names. We're not letting the LLM make the like go up. Uh, sorry, one more. Uh, right here in the PR. Technically, we could have made this whole thing a tool call. This could have been a tool call where the LM decides what PR to create. The reason we didn't do that is because we know that the LM should only ever modify two files, two readmes. So, we just won't even give it that choice. And this goes back to what we talked about again. It's like whenever we build an LM pipeline, we as a team always get a choice of how much flexibility we give the LM. There's no flexibility, which is we hard code everything. And then there's full flexibility where the agent is literally making every single decision and you have no programmatic code except mapping the data model to the tool call and nothing else. And I I can tell you exactly what would happen if we had the LLM to call this tool is I would spend I would be like you have a tool to create PRs and then I would write like a two sentence prompt that would be like only create this file and this file and here's how you should create those files. And then it would [ __ ] it up and it would do something wrong and then\n\nIt would, uh, and then it would create. And then I would keep making the prompt bigger and bigger, and I would have all caps, you must only ever create these files. And it's like, if you already know exactly what it should be, like, why are you trying to turn your own words, push it through an LLM and output the JSON or the code for what should happen? If you already know exactly what should happen, let the LLM do what it does well, which is generate the readme, generate the readme, and generate the folder, and you own everything else. If you know what it is, and it's going to be the same every time, yeah, it's less flexible, and you can't use it as a general purpose agent that does everything, but like again, but we don't need that here. We don't need that. We just need to automate the things that we know we need to automate. Yeah. And this goes back to thinking really hard about what is the input span of the problem that you're solving. So, like, the input span of the problem we're solving could be everything. But in Dexter and I case, we've decided that we only need to automate this part of the workflow, which is a very small slice of the world, which is not the whole thing, which is all we want to do is take content from Zoom and produce a PR from it, given the content that we have. So we can throw away all the other workflows and not spend any time thinking about them, and that helps us basically constrain what we have to go get the LLM to do. If we did have to make the full PR for everything, including the code, then we expand it, or we can incrementally expand it over time as well. And and it's like, automate with AI, automate with code, and automate manually, right? Yeah. And Joseph is asking a really interesting question, like, are we generating everything sequentially or partially, or like, what does latency look like? Well, the good part here is it's really up to us as a developer how much we want to go ahead and go do. Um, in our case, I think we do the summarization step sequentially because it's a dependency, and then we try and do all of these in parallel. But now, because we know the GitHub folder, the email will actually depend on the folder name and the GitHub repo, now the email has a dependency on the actual readme. Now we can remove that dependency from being as strong simply by saying that we'll create an intermediate step, which is a new folder name, and that becomes a dependency for the email. So now the email and the repo can run in parallel, and we use really, really small steps that are actual dependencies along the way. And this graph starts to look really, really complicated because that's software. Software is complicated. And the part about it that's complicated is not the AI part. It's just like these questions that Dex and I ask architecturally that make it complicated and sophisticated over time. Um, like the parallels question Joseph is asking. That's the question that we have to ask. We could do everything sequentially. We'd like not to, but we could. And this is exactly why this sort of thinking I find to be personally very, very useful. Um, with that, let's continue. Uh, let's actually talk about, uh, I think you showed the folder prompt. I showed this folder prompt. I was thinking we could look briefly at the readme prompts as well. Uh, well, so I showed the folder function here. This is where we, so this is where we get the path. And so here are our examples of repo names. And so what we're going to do is use another repo called kit from the same people who do the PR stuff. This is kit is a toolkit for getting from remote repos. Highly recommend for anyone building any GitHub automation. It's very, very good. Yeah. Um, yeah, from kit import repository. So we're going to go like loop over the file tree, and then we basically take the folders, and it's just f of path. So we're just going to pass in a list of folders. Um, I'm actually notice again, we could have passed this whole thing to the LLM, but Dexter was like, this took me two seconds of code. So I'll limit it to just the folder and only the root code. I won't think about it. Yeah. Yeah. Not saying literally Claude just Claude just wrote this function with very little input. I was just like, here's how we're going to do this, and it was like, all right, let's go. Yeah. And again, and it's just slightly better than passing everything to the model. And then we just ask the model to tell us what, given the current video title, what is the best folder? Yeah. So we can say, AI, the what was the video title was like, AI content pipeline. Yeah. And the thing was 0624 and the existing folders were, you know, 0617 and 0610. Something cool, something else cooler. And then you can basically run this test. And so what this, this, what this returns actually is an episode path result, which is the path and whether it already exists, whether it's new or not, and we're going to use that. Um, so it says, okay, cool, episode path content pipeline is new true. Great. And then we can make another one to, yeah. Go ahead. And the reason that we don't just use like a standard slug one, by the way, over here is because a standard slug approach is actually going to be a little bit more annoying than you'd expect because turning our Zoom titles into slugs doesn't actually mean that they make good folder names. Mhm. So that's why we used an LLM here. And actually, I'm just going to use our real folder names here. That's probably better. Oops. No, no, no, no. Not paste an entire Excalibur into. Son, let's do that. And Dexter is one of the best people I know to actually do all this stuff. He's like, he's just leveraging AI to like do codegen really fast, which would have been a copy and paste job. It's just very convenient. And then you can get rid of AI content pipeline, too. Well, I want I wanted to detect one that already exists. Oh, yes, it did. So, here it is. So, it figured out the episode path. So you could do some math and be like, okay, does the date match? Then pull that one out. But um, and yeah, the giving an explanation on great for this an exact match title also matches the folder topic since there's an exact match. Yeah, exactly. All right. Now, let's do the, I think the part that most people are here for, which is, let's make the prompts actually good. Yeah. Do you want to drive for this part? Yeah, I will. Um, so I think one last question. Is there any tooling to hook in evaluation steps in between these steps? Um, yes, I think it's, Yeah. Okay. Yeah, that's okay. I mean, not in our system. I mean, I, so I'll just, I'll just say that I've been using the like the boundary cloud stuff, and it's really, really good. I don't know if that's like publicly available yet, but um, you can record every input output pair and use it to generate tests and eval at least at like the per prompt level. Um, the end to end eval stuff. If you want to learn more, we do a bunch of stuff on that on a diff. We did an episode a couple weeks ago all about evals and the different like tiers of like really unit testy evals and then vibe evals and like human evals versus like more structured evals, all that kind of stuff. Yeah. Uh, that video probably does better, but it eval sadly just take effort and like people I think they want the same prompts where they just want a prompt that works and magically makes a perfect email. There's no prompt that will do that magically. At least not yet. Maybe in like another six months, but as with today, not yet. And same thing with emails. Um, I'm going to screen share. Um, I will attempt to not show API keys. Um, sorry, this is my ghost stuff. All right, there we go. Okay, um, vertex patient. Sorry, that's testing on different models along the way. Uh, please. Okay. So last time when we were doing this, one of the things I think we noticed was, um, the email draft was not very good, um, for a couple reasons. I think, and I think part of the reason that the draft wasn't very good was actually that the summary was quite bad. Oh, yo, you got to do real time codes. Remember we did it last week and the time codes were all in just like 15 minute increments. Okay. Yeah. So, it turns out one of the best models for doing this is actually Gemini. Um, and this is just it just takes hacking with a prompt to go do this. Last time when we did this, um, generate email structure. Uh, where's the summary? Sorry. Uh, generate email drafts. I was working on this. Last time when we did this, we weren't using Gemini. We were using OpenAI GPT40. And there's a couple things wrong with this, but like, I'll run this really fast just to show what happens. And this just goes to show like, really, we talk about this stuff all the time, but like, I guess Gemini is doing like decent time codes this time, but like these times start to get like, as we go into this, but you'll notice this is much better. This is actually much better than we're doing. Did you make this prompt better or something? I may have also done that, but I don't actually think so. Time data is still a string. Um, I think if I swatch it, swap it to time data. I'll try again. Uh, GT40. Let's try mini. That's kind of slow last time, so I don't want that. Okay. I mean, it just looks like the model, for some reason, I'm getting sent off to a better version of the model today last time. But in generalizing, you, they're not quantizing. What we saw last time, if you go, if everyone goes back and looks, is they were just breaking down like 10, 15 minute chunks. But even here, I'm only getting like, really nine, nine time segments in an hour and a half video. Maybe that's good, maybe that's bad. Um, but what I found is personally, uh, Gemini works really, really well for this kind of stuff. Um, and I suspect it's because they train more on YouTube than probably other models do. Like you just see the time code here. Like check that out. Uh, that's just a lot better than this one. And you're still just sending the raw transcript from Zoom, right? I'm, I'm not even sending the video. I'm just sending the raw transcript. Like Gemini model like actually breaks this down to proper time codes if you can look. Those are really, those are almost too small. Those are like 20 second chunks. Exactly. So it's actually give me 20 second chunks. Um, so we can tell it that we can say, uh, not do that description. Usually, uh, usually, uh, 20 to 30 second semantic chunks. Uh, maybe like since these are longer videos, let's make it different. Usually like five to 10. Yeah, five, 10 minutes, I think is good. Semantic chunks, but exact timings. Transcript. So, let's try this again. And what we'll notice here is like, the reason that we're focusing so much energy on getting the summary just right is because, let's see what it does. It may not listen to me, and I can't really force it to listen to me. And I suspect I know what's happening here. What's happening here is that the model is doing its thing. And there was 104 chunks last time. Um, and I suspect it actually got cut off because of, um, context window length. I probably just need to out, uh, it probably just needs to output more tokens or output less tokens. Um, this one seems similar. So, let's try and go do this again. Um, instead of adding the time data in here, I'll remove this, and I'll simply have the LLM, I'll put it as a response format. And I suspect the reason that this is happening is actually because what the LLM is doing is that the LLM is not forgetting my instructions because there's so much in the context window that it's that it's just like getting lost among the transcript itself. Exactly. Like just like, like it knows it's supposed to be summarizing something, and you can maybe give it a little bit of format, but I was running a token visualizer on a prompt that is too big. Oh, yeah, don't do that. Yeah. Yeah. Um, but this prompt is just like too big. Like this, this transcript is just like, where it go, where's the test? This test is literally just long. It's a lot of context. And now, and that's probably why it's trying to output like a bajillion, um, tokens to me, because when the model sees this, I bet you when it was outputting time codes, it's literally just converting each one of these into a time code. I bet there's 104. 10. Oh, wow. Yeah, you're just labeling every single one. Okay. I was gonna, I was just gonna zoom out for a sec because I know we're gonna supposed to talk about tone today, and we got about 15 minutes left on the on the on the main session. Go ahead. I was going to go fix the prompt. Yeah. No, I was just going to say the, I mean, the summarization stuff, I think we spent a lot of time on last week, and I'm, I'm wondering if we can go like push this towards like your some of the stuff you're doing with the prompt for the email and things like that. I could go do that right away, but the summarization stuff, as it turns out, I think is going to be very straightforward. All we have to do is we just change it. We just change our instructions. The main takeaway here is like using using models like Gemini can help you do this long context stuff a lot better. Right. Exactly. And also, what you'll notice is I actually have the model dumping out stuff before it does anything. Um, so I actually change the order what I do. I dump out the transcript first, then I dump out my instructions. And that can make a huge difference in the quality of my answer. Um, and then the video ranges and then let me go take this out. You lost your output format. Keep it to five, uh, notes max. And then this will probably do it. It was just doing too much of it in there. Mhm. There we go. Now it works. And the reason is, I basically had the model go ahead and break down the chunks of like, here's five chunks that made semantic sense to it. The beginning, the middle, other middle, and then eventually it had the end. And it then built, I built, then built a better takeaways.\n\nNow let's do the next part, which is actually generating the email perfectly. So when we go actually generate this email, there's a couple of things on here that we want to do. One, we want to give it, uh, we want to give it the email example of like another email that we liked. The next thing that we want to do is we want to start giving it video summary, email structure, and everything else. But when we give it the email structure, it's going to go give it all of these concepts. And then we're also going to go give it a couple more things, which is we're going to give it information about the next session that's already in here. And then we're going to, uh, where this, okay, sorry. Let me pull up the right test case. I apologize for this. There we go. So, let's just copy and paste the summary and replace the summary with this one because this is a slightly better version of it. Correct. Okay. So, you're using the outputs from the summarization to generate an email with the pieces that you're going to build up in the final template. Exactly. How do we get more BAML in the training set? Dude, I'm tired of deleting colons whenever I generate BAML code. We will just support colons, and that is how we actually solve that problem. We made a mistake. Oh, okay. That is really the real problem there. Um, that was us trying to be clever and make syntax easier, but it turns out that was incorrect. So when we do this, the subject we already get from our old, uh, prompting video. So we, we just call this, oops, my AirPods died. So I'll be on here, AI that works, cracking the prompting interview. The next thing that we'll do is we'll just figure out that we covered. There we go. Where we dive into where we dive into, uh, this session. So, this email structure, what this is doing now is this is actually going to go ahead and generate this email pretty easily. Oh, whoops. From that structure, you can see what it's actually able to go do. It doesn't actually struggle that much when I just wanted to fill in the blanks. When I wanted to go fill in the blanks to just make it feel good, it can do this really easily from the main takeaways in the we covered section. The question that I have to ask myself is what Dexter was talking about last time is how can I get it to go generate these sections really, really accurately? And that's why we actually built two prompts here to generate the email. One is generate the email draft, and the other one is actually go generate the email. So generating the email draft comes from a very basic thing where I give the video summary, I give the transcript, I give the video title, and then it generates a draft for me. And the draft consists of these five sections. It literally just consists of each of these. And now I'm able to go ahead and break down the problem into smaller sections. And what the difference here is the model might choose to respond in each of these sections with a variety of different options. It might choose to respond with, excuse me, it might choose to respond with phrases. It might choose to respond with incomplete sentences, things that don't flow with each other. And that's totally acceptable. So we don't have to think that hard about this. All we really want to happen here is we just want to get the content mostly correct out of this problem. Now again, okay, so you're not using deterministic template. You're not like generate the bullet points and then I'm going to like glue this all together by hand. You're actually just like, hey, generate it in a structured way and then I'm going to hand that to something whose only job is to like turn it back into a full coherent email. Exactly. Don't. And the new thing doesn't add new content. It doesn't do anything. It just makes it coherent with another template that we have in mind. And the difference here is like that's the difference between like it feeling like Mad Libs. Like Mad Libs, they're fun, I guess, as a joke, but no one would actually send Mad Lib stuff. It feels bad to go read that and it doesn't really read nicely as well because our content is a little bit more dynamic than everything else. So this is, I think, where people can bridge LLMs. I still want my content to be broken down to this generic structure. But what I don't want to happen is I don't want the email to feel way too loose in the beginning because if I don't give it a structure, then I have to try and prompt my way to getting this email to feel this way every single time. But what I can do now is I can just have a section that does this. And I can just like put like make this an array if I want because I think this is going to be better. Use my Gemini because Gemini, in my opinion, is a superior model for this problem. Um, and then lastly, we're going to go do the same thing, which is again, we're going to have a lot of context in here. Um, so I'm just going to move this up and put my instructions at the bottom because Gemini prefers that. And I can do this conditionally as well, so I don't have to think that hard about this. So, you're doing two user messages in a row there, by the way, which is something I haven't actually seen that much. Um, can you talk about why you did that? Yes, I can. One second. You can. Okay. Um, generate email structure. Sorry, draft. Why did I do two user messages? Oh, I moved this in the wrong place. I moved the wrong prompt. Why did I do two user messages in a row? Well, I think what the LLMs do under the hood, and this is just like my assumption of what they do, is they definitely inject tokens in for you. So, when we actually go look at the raw curl request, for example, I have the transcript and everything here, but I basically have like two content parts in here. I like one content part that is the initial one and a second content part that's the other one. And the reason that I don't do this that I do this is because Gemini, um, does something weird with system messages. I'll show you what it does. Oops. Uh, which is for some reason Gemini likes to put system messages not as a part of it. Um, let me make this bigger. Oh, it comes in a different part of the request. Exactly. And you can see how big this context is. So, it's kind of annoying. Um, oh yes, Gemini doesn't actually support system messages in this way. If you try and pass in a system message to Gemini in this way, it doesn't actually work. You can do it this way, but like it just does it as a different part of the request. And I can pass into the system instructions, but empirically what I have found is the model just doesn't listen to that. It seems to listen to the most recent tokens more. So I just embed it at the very bottom down here whenever I'm using Gemini. And what I actually do under the hood is I actually usually do something like this. Oh, otherwise you would put a system message at the top. Exactly. So, it does give me a little bit of flexibility over here where it's like I add it in if it's there, otherwise I don't. And like in the case of Gemini, I get I get like a second user message basically. Um, and that for me works quite well. But now when I'm actually going to go run this, what I'll find is the email comes out kind of nicely. So it gives me a quick recap, which if you remember the cracking the prompting interview, like this was a thing. Don't avoid long meaningless tokens, sequences, URLs, etc. for diarization and mint indexes. This is a pretty good summary of what we have. Utilize inline comments within the model. Practice RTFP. Read the effing prompt. Use fshot prompting to define the output structure, not necessarily the content. So it has a pattern to follow, and this summary is actually quite good for what we want. And the problem is it doesn't have a next session, and that's correct. It cannot possibly have a next session because we haven't given it that input. I just want to call out real quick, um, you have a function called generate email draft and then you have a function called generate email structure, and I think the names might be flipped. Yes, that is Don't try to change it now. Um, that is almost definitely true, and this is probably because I initially had email, uh, structure just draft, and it was doing the whole thing, it was doing the whole email from input to output. Um, this, uh, is a good point. I'll rename this before I, Dude. Yeah, don't change it now because it's going to break all the code. But, yeah, uh, the next thing that I want to pass into this is I want to pass in, uh, uh, event event details. And the event details will have the event name, the date, the time, and location. You need the URL too. Uh, uh, ID inviting, and I'll pass in next event, next session. We'll go in there and we'll just add this into here. Um, and now my test case will go back to merit gun. Add a little test case, and this is kind of the nice thing about this is the next session will be, let's say the next session will be like generating, uh, content AI content pipeline, AI powered content pipeline, July 15th, 2025. Cool. Um, yeah, we do have a description on those as well if you go look at the Luma client, so I would probably add that. Cool. Let's do that. And again, I could dump this as JSON. I'm just choosing not to because it's better to dump it without the quotation marks and everything else if I can avoid it. Um, on this, at this, um, and then the invite link, and I'm just trying to make this easier. Uh, next session link invite, join. Oops. Link. I'm just trying to make this easier for the LLM to go read. So, if I go read this, I keep running this test again. Oh, man, I got to do that today, too. I know. What we should find is again, the recap is still pretty good, covers roughly the, uh, same content, and boom, our next session on July 15th will be about AI generated content. Here's the link. Now, obviously, the link can be elucinated. So what I would do if I were doing this is I would just validate the link is correct. And really what I would do is like, what I would really do is I'd actually put a placeholder link in there. Uh, so I do like luma. uh, next AI works, uh, like 12. AI, I do AI that works 12. And the reason I don't put a pure placeholder link is because the LLM will not hallucinate. We'll be like, oh, that's a placeholder link. Maybe I'll substitute something for you. But putting something like this gets you something that's like close enough that it looks real, but not wrong enough that it can't get it wrong because these are not that many tokens. So now it'll have a lot more reliability that the model will dump out the invite. And then what would you just do like a find replace in the final in the final output and replace it with the with with the real link. Literally what I would do. Cool. And then what I, and now what's nice about this is converting this structure to like an email format. I think that's something we all believe an LLM could do. So now we don't have to think about this as much. Now we can go to the, oops, I I don't know why I had that. Again, you're just you're just guiding the thought process. Basically, you're forcing it to think about different parts of what the email is going to be. And that makes it really easy for the next, the next LLM call to just generate an email that matches what we want instead of it having to kind of do those two steps of, okay, let me break this down into parts and then let me go turn it into a nice email. Exactly. So, and and now when we when I submit this, I'm going to run the previous draft again. Run email structure just so people get an idea what it looks like. And we can actually read this email and just see what we feel because I think feel it's this is a feeling based task. Hello, first name. We should fix the first name thing. We have to keep tabs of that. This we from AI from Vib index. We covered a lot of different topics. This looks really good. Actually, when I go look at this, it doesn't have the GitHub link and it doesn't have the URL probably because we didn't give it to it. So, that makes sense. It's not going to have that, but it does have the Luma link. Join us for the next session, generate for to generate content for various use cases. That's pretty good. If we had a better description, I bet this would be better. And I think this email is actually good enough for us to go send out. And this, it doesn't matter that this is perfect or not, but this I can just read. Our Dex can just read and we can like, is this good enough? No, we can edit this manually because it's just one sentence we have to go edit now because the rest of this looks really, really high quality. And why did I know? And the trick here is we tried GPT4, it didn't work. We tried Claude, it didn't work. We tried Sonnet. We tried Gemini. Gemini works with Flash. Great. I'm just going to use that because it's dirt cheap and free. Basically, our problem is solved. And now we have a tone that I think feels much better than the original. Can you, do you have the drafts of the email running on your machine, Dexter? Yeah, I don't think I have this code though. You don't need this code. I mean, just running the code that we have on your screen. If we look, just do the resummarize. Well, just look at this. This email just reads. It feels like AI generated an email. Yeah, this sucks. The one that we were looking, if you send me this email, I'm unsubscribing by above. Exactly. And the one that we see now is much better. So we've been able to fully automate the pipeline, get the tone. But the big part about getting the tone today was really about getting the data, getting the right data in the right place. And that's all we focused on. And once we got the right data, then it was picking the right model that could do the task because we know that models can get tone probably correct. We just have to find the right one that matches our use case. For our use case, that happened to be, uh, Gemini. And then we broke the problem into two steps. Generate the, generate the content, generate.\n\nThe actual email and that seems to work. All right, I have one question for you before we jump into questions from the audience. Um, the world of AI moves really fast, and we're all constantly learning things. Uh, so if I can take a slightly off-topic for one second, uh, do you have a fresh AI hot take? Something that you've seen a lot, like maybe not even related to this content, but like what's a strong opinion that you've had to voice in the last week that might be valuable to the audience here? Um, use more AI than you think and use less AI than you think at the same time. Um, I think it's been a really interesting learning for me personally. Uh, and it really actually came from talking to kids that are much younger uh than uh me. Uh, because I've been coding for a while. I'm kind of set in my ways and I don't really like to change how I code. I just code the way I code. Um, but I've been watching some younger kids code and it's interesting what they use AI for. For example, copy and paste. I will never manually paste something ever again. I literally will just ask an alarm to do it or have cursor tab autocomplete it. It is really, really freaking good. I naturally I was I I could think how to do that for AI, but it was my first instinct and now copy and paste is just an AI feature. It is not a copy and paste feature. It is a pure AI feature for me and like interesting that changed my behavior and I think there are other places like for example, when we're doing the folder renaming, we could pass in all the folders into it, but if it's trivial to do the right thing, just do the right thing and just pass in only the folder names instead of all the files because you'll just make your own life easier. So I think that's what I mean by use more AI than you think. Like use it for places that you're probably not imagining using for, like copy and paste. Oh, and stop trying to use AI for dumb [Â __Â ] that you could vibe code in five lines. Exactly. Like the PR thing. Like why vibe code the perfect readme and a perfect PR creator when I know I only need to do two readme files and find a balance of where that is is actually I think a really, really artful um position. That's cool. Do you want to hear mine? Yeah. I feel like I keep saying this. Uh, and there's a lot I This is inspired. I saw a lot of MCP talk in the chat. Um, I think if you are building agents and you are building code and you're building AI products, I I don't think MCP was designed or is very good as like a replacement for traditional SDKs. We've said this in our MCP episode as well, is like SDK is MCP is not a building block for building production AI agents as products. MCP is an incredibly powerful toolkit that is changing the world because it allows non-technical or like much less technical people to add functionality to existing AI user experiences. And like your app, your production agent, the thing you're shipping, something you want people to use should absolutely support and implement MCP, but not in the sense of like, oh, I'm going to use a bunch of MCP servers and like put them under the hood inside my app that no one knows about, but you should implement an MCP client so that anybody using your app can add new functionality whenever they want. Yeah, exactly. And that functionality will be less reliable than the native functionality you have built in. But it will be more it will allow them to extend the capabilities and that's kind of cool is um all right, we got a lot of stuff going on in the chat um specific questions people want to do uh if anyone wants to share their their hot take of the week um happy to hear that as Um, so I guess the long the long thread was I'm trying to decide if this will be useful for my code generation and review rag. Is this a good way to orchestrate the agentic workflow or not so much? And then I think there's what? Show me your t-shirt. Oh, we got to get people the human layer t-shirt. Oh, uh, merch layer.dev. And if you ping me privately on Twitter or LinkedIn, I'll send you a promo code for a discount. Um, let's talk about where do you see AI risk surface expanding? Um, I think the biggest area of AI risk surface is actually data leakage and data security. Um, that's super underrated, especially as more people send more stuff to AI pipelines. When an AI pipeline randomly has a tool injected into it, like SQL injections for example, you're going to have a like we figured out how to deal with SQL injection. We just escape them. There's no way to escape inputs to your prompts. So like that's going to be a huge huge area uh that people have to deal with somehow. Um, and I'm not sure exactly how we'll do that universally. What about you, Dex? What do you think? spending uh we waste time on stupid [Â __Â ] that doesn't work. I think the like I don't know there was this joke of like, oh, computers will allow us to work 100% faster. They're going to double our productivity. Then it's like, oh, computers generate 100% more work. So it's like it's kind of a wash. I don't know. There's going to be some version of that with AI. Yeah, exactly. What's the next one? What's your take on cloud code hooks? Yes. Uh, I don't know what that is. I have to go look at that. Yeah, it's like cursor. It's like extensions that you can write personally that basically will like give Claude code special commands that you can have. So like, for example, you can have like rename file that just has instruction on what you do or you can make like an agent that does research where you tell Claude code spin up 15 agents. Yeah. Wait, we've been using this for weeks. This is just commands. This has been around forever. Yeah. Yeah, that's my point. So we have been having commands all the time, but these are hooks. Is that I'm not getting the difference like how it is different. I will uh I will look into this. We did say that we would do an episode on vibe coding, which would be a little off topic for what we generally talk about, but I think could be a lot of fun. So, maybe we'll talk about this one as well. Yeah. Okay. So, pre-tool use, post-tool use, notification. I think, yeah, this is super interesting. They're experimenting with a way to let other people build on top of Cloud Code because it helps build an ecosystem for them. Um, and like maybe that happens to be true where like people want to use a Claude agent loop to go do something. Um, and it becomes a slightly more general purpose agent, but I think that's that's kind of what that is for them. Um, whether it's good or not really depends on like the quality of the hooks that people build out and like what unique ideas people come up with. It's like the App Store and Apple. Like the App Store and Apple could have been a total flop, but it turned out to be really good because Angry Birds is really, really fun. Um, again, no reason I'm just using it to, you know, ring a bell once one of my like agent kind of session that is done or waiting for permission. So it just like a bell so it was not possible before. Exactly. And now you can go do that and like that's what I mean, there's small things like that where it's like if they don't get the UX just right, allows other people to go make things slightly incrementally better. I think that's interesting. Um, is there any world where we can use DSPI with BAML in the future? Uh, I think they're right about prompts being the wrong abstraction since they basically just automate eval driven iteration on prompts, but I don't like how they do structured output. Um, I think there's a really underst thing about how people think about prompts and context in general. Uh, and I think this goes back to what Dex often says about context engineering. The problem that I see generally with the way that people are doing automatic eval driven developments on prompts is they they iterate on the whole prompt at the same time, and that is not very useful. Like I'll give you an example really fast just to make it a little bit more concrete, and this is kind of and like I I like BAML to be extensible, so like once we provide the A, then you can do whatever you want within Python. We just haven't built that yet. So like in some sense, yes, but fundamentally, like let's just take a look at this. So I have a prompt that generates this email structure. We talked about this today. If the thing that is not good is the quick recap, there's many ways that I could have solved this. Before quick recap used to be a string, and it was just not as good. By making it a string array, I was able to get better recaps coming out of it. Whether you call that optimizing the prompt or optimizing something else, I don't know. I don't think Well, that's part of the signature. Like if you're using DSP, you'd literally have to physically change the signature. Exactly. And that's why I think like the actual prompting part isn't actually constrained to what it is. And we might even find that the right way to do this is like uh recap might be like a recap might look like this. It might actually be like title string, uh description string. And like what I really want to do here is like this. And whether and maybe I want to add a description here at most three, four to 10 words. And now it's getting to be really sophisticated at a way that like you typically don't want to navigate English itself. And that's I think the main problem that I see with most prompt optimization tools is that they try and navigate the middle chunk of the prompt, but they overlook the fact that the prompt itself is comprised of a giant A that is very, very complex. And as our programs become more complex, we'll become harder and harder to just like edit in the pure prompting side. I will say like just to play devil's advocate like for the DSPI side like rather than prompting like I think and like maybe maybe I don't fully understand this, but like my my understanding would be like rather than telling the LM to use X number of words, you would basically set up a metric and then you would count the words that came out and then you would like penalize it for too many words or not enough words and then over time the model, the prompt engineering framework would probably learn to say, okay, the string that gets the right number of words is to say these like use four to 10 words, but you wouldn't have to write that. You would write the hey, here's the rules, here's the metric. Yeah, that's that's exactly right. Like you kind of you I mean, you give it a signature and then you express kind of an evaluation function that you know can be any combination of like string based, you know, any properties of the generated thing if it's structured. Um, it can also use LLM as a judge if you want, and then it will kind of iteratively run that against, you know, an LLM with your evaluation function, and it'll figure out like which, you know, it'll try update like having the LLM write things to update parts of the prompts that are not performing well and then see how they behave. It's almost like conceptually like a little bit like a genetic algorithm. It'll try different things to improve the parts that the evals um determine are performing badly and it'll pick whatever helps the most and and kind of add that to the prompt iteratively. So yeah, you just express like what good looks like in that evaluation however you want to and then it will figure out okay, what actually works. The only tricky part about that is that part of doing the application on what you actually want to go change in the prompt is really really varied. So there are some things that you could do. For example, like you could this like the sentence of at most four to 10 words. Let's assume that a human didn't write that, but a model did. You can write that in many different places. You can write that here. You can write that uh here. You can write that you can write that as a description field here. Oops. Yeah. And so that's kind of what it's figuring out for you is like not just what should be added to it, but where should it be added to it to get really good results. Yeah, exactly. So that will become possible once we expose the A, but we really need to just expose the A in an iterative way and then like the model can decide how to go inject it because what you're really doing here is you're modifying the code at runtime and that part is the part that needs to be uh analyzed and retroactively like you need to be able to walk the syntax that we have and like clearly we can walk the syntax, but we just don't expose an SDK to go do that yet, but we will. That we've tried iteration iteration loops like this before and it seems to work pretty. Especially if we're generating new schemas, that's where we find that the model is really, really good. Like for example, if the recap is bad, it will actually just create a schema. Uh, or it'll change it like a string array or something else inside. And that seems to work quite well. Cool. Um, last question I think we can call it. Yeah, go ahead. Um, I like the what's the coolest thing you've seen built with BAML? Clearly this guy just came here to get a BAML pitch and I'm happy to oblige. Um, what's the oh, sorry, there's like seven or eight different questions. Let me just go down really fast. I the first one I saw was what's the versioning? Just use Git or like have two variables that you pass in and forth. Like if you have two variables, you can just use that. The coolest thing that I have seen built are honestly not about the extent of the pipeline, it's really about how good they are. So for me, things are personally really, really cool when they actually work like 99.9% of the time. Like this email content pipeline that we built wasn't that cool last week to me because it didn't actually produce a good email. The email that we just saw today was pretty good. That becomes cool. What I've seen are pipelines that process like bank statements with like 100 pages plus. Those are really exciting when they don't have errors. Um pipelines processing tax forms, those are really exciting because again, very sensitive data and it just works. Uh, hospital records. Uh, I think those are I think the most interesting problems to me personally. But I think things that other people probably find really cool are probably along the lines of um like there's this agentic framework that someone built that not agentic framework, an AI assistant that someone built that you can just talk to in your\n\nCar and just does like seven or eight different things, and they did that by adding a bunch of MCP tools. I think if you look in our share your share your creations channel, we'll have it in there that I thought was really, really, really wild. Um, what about you, Dex? I know you've built some really cool things.\n\nYeah. Uh, I think it's less about like what you can do specifically with BAML and more like it's all about the application, right? Like BAML just helps you get the performance pretty good without having to think about tuning and tool calls and all that kind of stuff. Um, and it provides a good development experience for testing and evaluating this stuff. So, I mean, I've built two things with Pamel that I really like, which is like a Slackbot that manages all of our GitHub deployments and an email assistant that like you can just forward emails to it and it like looks up a list of tools with BAML and responds and has like just like has become it went from being just like trash dumping JSON into the context window to something that's really, really clean and really high reliability and works really fast and doesn't use too many tokens. So, it was fun iterating in that direction with it.\n\nYeah. I think the things that I found really cool in general, the ones that you built is just like the UX pattern of like being able to interrupt the workflow when it's running with like a feedback loop. That was not a thought process that really came across my mind until we worked together for the first time, which is idea that like I want human guided agents where I can like that's why cursor is so freaking good. Like cursor is really good or any of these systems like cloud code is really good because while the agent's working, I can chat with it and it can interrupt, change the workload into the direction that I want. That UX is really important to build. This is Karpathy talked about this in his YC talk. He was like, like good AI tools have an autonomy slider. There's like zero autonomy where I'm editing the code. There's a little bit of autonomy where it's like tab complete, but it just shows like suggestions and I accept them. And then there's like full agent mode, and then there's like other, there's ask mode, there's like things in the middle. And so like being able to build AI experiences that have that again, autonomy slider, different levels of autonomy for different modes of work and like polish versus broadstrokes versus planning versus high level, um, I think is really important.\n\nAnd like full agent mode is interesting, but full agent mode without being able to stop it and correct course correct along the way is really, really, really bad. And like that's why that UX is critical. Um, but with that, I think that's today's episode. Uh, hopefully you guys saw the content. We'll push all the prompts out, so you'll have access to it. Uh, and today's email should come back in a fully AI generated way. So, um, it's going to be fun to go see and you'll see the ghetto readme launch soon as well. And if this week goes well, then we'll start uploading pretty, um, if if things if this week goes well, then we might start increasing the upload rate to Wednesday because we got an automated pipeline. Uh, remember when you wanted to hire an intern to do this work?\n\nYeah. It turns out eight hours, uh, 16 hours of our time does the job, too. Oh, god. Dude, this is super fun. Thanks everybody for the questions. Catch you all next week. Bye-bye.",
  "dumpedAt": "2025-07-21T18:43:25.737Z"
}