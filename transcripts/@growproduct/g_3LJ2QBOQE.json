{
  "episodeId": "g_3LJ2QBOQE",
  "channelSlug": "@growproduct",
  "title": "The ONE AI Skill Every Product Manager NEEDS in 2025",
  "publishedAt": "2025-07-11T15:00:41.000Z",
  "rawLines": [
    {
      "lang": "en",
      "text": "Why do PMs need to be good at AI evals?",
      "offset": 0.24,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "Okay, so there's three things that are",
      "offset": 2.96,
      "duration": 4.959
    },
    {
      "lang": "en",
      "text": "really important. One, eval",
      "offset": 4.799,
      "duration": 6.081
    },
    {
      "lang": "en",
      "text": "to inject your taste and your judgment",
      "offset": 7.919,
      "duration": 5.441
    },
    {
      "lang": "en",
      "text": "directly into the critical path of the",
      "offset": 10.88,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "AI product being developed. The second",
      "offset": 13.36,
      "duration": 3.999
    },
    {
      "lang": "en",
      "text": "thing is like evals are really important",
      "offset": 15.28,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "in helping you iterate. The most",
      "offset": 17.359,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "effective way to do that is using eval",
      "offset": 19.039,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "specifically looking at data in a very",
      "offset": 21.92,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "structured way. And then the third thing",
      "offset": 24,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "is scale. By mastering evals, what you",
      "offset": 25.519,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "can do is you can make sure that you can",
      "offset": 28.24,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "scale your taste judgment uh so on and",
      "offset": 30.32,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "so forth user requirements across all",
      "offset": 33.12,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "the AI workloads that are running. When",
      "offset": 35.04,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "it comes to AI evals, Haml Hussein and",
      "offset": 37.36,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "Shrea Shanker are known as the worldwide",
      "offset": 40.239,
      "duration": 5.921
    },
    {
      "lang": "en",
      "text": "leading experts. Companies like OpenAI",
      "offset": 43.36,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "and ARIS go to them and today we're",
      "offset": 46.16,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "going to learn everything you need to",
      "offset": 49.12,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "know about evals from them. What is the",
      "offset": 50.64,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "most critical skill for PMs who want to",
      "offset": 53.52,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "build AI features to develop?",
      "offset": 56,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "Hands down, error analysis. The ability",
      "offset": 57.6,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "to look at your outputs and",
      "offset": 60.399,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "systematically figure out what makes for",
      "offset": 62.96,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "a bad output. Quantify how many of these",
      "offset": 65.519,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "failure modes you see in a big batch of",
      "offset": 68.64,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "traces for your system and then figure",
      "offset": 71.439,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "out how to turn that measurement into a",
      "offset": 73.68,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "continuous flywheel of improving your",
      "offset": 76.32,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "product. If you guys had to build a road",
      "offset": 78.32,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "map for people who wanted to get really",
      "offset": 80.08,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "deep on AI vowels, what topics should",
      "offset": 82.32,
      "duration": 5.119
    },
    {
      "lang": "en",
      "text": "they learn?",
      "offset": 84.56,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "Really quickly, I think a crazy stat is",
      "offset": 87.439,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "that more than 50% of you listening are",
      "offset": 89.68,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "not subscribed. If you can subscribe on",
      "offset": 91.68,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "YouTube, follow on Apple or Spotify",
      "offset": 94.079,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "podcasts, my commitment to you is that",
      "offset": 96.32,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "we'll continue to make this content",
      "offset": 99.28,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "better and better. And now on to today's",
      "offset": 101.04,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "episode.",
      "offset": 103.439,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "Hammel Hussein and Shrea Shanker are the",
      "offset": 105.759,
      "duration": 5.761
    },
    {
      "lang": "en",
      "text": "people who the experts go to for evals,",
      "offset": 108.159,
      "duration": 7.361
    },
    {
      "lang": "en",
      "text": "open AI, Arise AI. Those people are",
      "offset": 111.52,
      "duration": 6.239
    },
    {
      "lang": "en",
      "text": "going to them for evals and we have them",
      "offset": 115.52,
      "duration": 4.959
    },
    {
      "lang": "en",
      "text": "on the podcast today. Welcome Shrey",
      "offset": 117.759,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "Hmel.",
      "offset": 120.479,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "Yay.",
      "offset": 122.56,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "Thank you. Nice to be here.",
      "offset": 123.439,
      "duration": 5.281
    },
    {
      "lang": "en",
      "text": "Why do PMS need to be good at AIE",
      "offset": 126.32,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "valves?",
      "offset": 128.72,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "Okay, so there's three things um that",
      "offset": 130,
      "duration": 6.239
    },
    {
      "lang": "en",
      "text": "are really important. one",
      "offset": 132.8,
      "duration": 7.76
    },
    {
      "lang": "en",
      "text": "uh evals give you a way as a PM to you",
      "offset": 136.239,
      "duration": 7.761
    },
    {
      "lang": "en",
      "text": "know inject your taste and your judgment",
      "offset": 140.56,
      "duration": 7.039
    },
    {
      "lang": "en",
      "text": "directly into the critical path of",
      "offset": 144,
      "duration": 6.879
    },
    {
      "lang": "en",
      "text": "the AI product being de uh developed. So",
      "offset": 147.599,
      "duration": 5.841
    },
    {
      "lang": "en",
      "text": "like you know as we all know like PMs",
      "offset": 150.879,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "they spend a lot of time gaining context",
      "offset": 153.44,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "from customers",
      "offset": 155.519,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "user feedback so on and so forth writing",
      "offset": 157.36,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "PRDS they're you know trying to give",
      "offset": 159.44,
      "duration": 5.439
    },
    {
      "lang": "en",
      "text": "context to engineers and you know",
      "offset": 162.48,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "they're hoping like kind of engineers",
      "offset": 164.879,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "are faithfully carrying out their",
      "offset": 167.68,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "vision. Now what eval give you is you",
      "offset": 169.76,
      "duration": 6.72
    },
    {
      "lang": "en",
      "text": "know you can directly make sure that",
      "offset": 173.68,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "your taste in all of that context if",
      "offset": 176.48,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "done correctly is now on the critical",
      "offset": 180.16,
      "duration": 5.999
    },
    {
      "lang": "en",
      "text": "path when your engineering team is",
      "offset": 182.48,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "developing those AI",
      "offset": 186.159,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "products. The second thing is like",
      "offset": 188.56,
      "duration": 5.599
    },
    {
      "lang": "en",
      "text": "evolves are really important in helping",
      "offset": 191.04,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "you iterate.",
      "offset": 194.159,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "So you know nothing is like set in",
      "offset": 196.239,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "stone. you have to constantly like",
      "offset": 198.08,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "change your requirements, you're",
      "offset": 200.08,
      "duration": 2.879
    },
    {
      "lang": "en",
      "text": "learning more about your customer, so on",
      "offset": 201.519,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "and so forth. The most effective way to",
      "offset": 202.959,
      "duration": 5.601
    },
    {
      "lang": "en",
      "text": "do that is,",
      "offset": 205.36,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "you know, using evals specifically",
      "offset": 208.56,
      "duration": 5.599
    },
    {
      "lang": "en",
      "text": "looking at data in a very structured way",
      "offset": 211.28,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "and which this is one of the things that",
      "offset": 214.159,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "Shrea and I teach. Um, you know, that",
      "offset": 215.68,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "allows you to to refine and have really",
      "offset": 218.48,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "fast feedback loops and really fast",
      "offset": 221.2,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "cycles of feedback. And then the third",
      "offset": 223.12,
      "duration": 7.199
    },
    {
      "lang": "en",
      "text": "thing is scale. So you know by mastering",
      "offset": 225.36,
      "duration": 7.519
    },
    {
      "lang": "en",
      "text": "eval what you can do is you can make",
      "offset": 230.319,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "sure that you can scale your",
      "offset": 232.879,
      "duration": 6.881
    },
    {
      "lang": "en",
      "text": "taste judgment uh so on and so forth",
      "offset": 236.799,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "user requirements across you know all",
      "offset": 239.76,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "the AI workloads that are running in a",
      "offset": 242.239,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "way that you just couldn't before",
      "offset": 246,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "because ultimately there's a lot of you",
      "offset": 248.239,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "know you can bake a lot of these evals",
      "offset": 250.799,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "they're using AI themselves you just",
      "offset": 252.48,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "have to make sure that you do it",
      "offset": 254.56,
      "duration": 2.399
    },
    {
      "lang": "en",
      "text": "correctly",
      "offset": 255.92,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "So you have to make sure that you uh",
      "offset": 256.959,
      "duration": 5.841
    },
    {
      "lang": "en",
      "text": "align the AI with yourself as a PM in a",
      "offset": 260,
      "duration": 7.36
    },
    {
      "lang": "en",
      "text": "very kind of process that we teach. Um",
      "offset": 262.8,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "and as long as you do that correctly and",
      "offset": 267.36,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "you do it in such a way that you develop",
      "offset": 269.04,
      "duration": 7.2
    },
    {
      "lang": "en",
      "text": "trust in the AI that is doing the eval",
      "offset": 272.24,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "and there's a way to do that that",
      "offset": 276.24,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "alignment then you can really scale",
      "offset": 277.6,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "yourself. So a lot of times PMs",
      "offset": 279.52,
      "duration": 7.28
    },
    {
      "lang": "en",
      "text": "or not just PMs but people at large they",
      "offset": 283.6,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "kind of view evals as a very monotonous",
      "offset": 286.8,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "task that you know you just want someone",
      "offset": 289.52,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "else to do it. It's like oh like I have",
      "offset": 292.96,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "to look at data I have to annotate data",
      "offset": 294.8,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "um you know who's going to do this. You",
      "offset": 297.84,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "don't want to give up that leverage",
      "offset": 300.56,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "because when you when you build that",
      "offset": 302.56,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "foundation of evals",
      "offset": 304.4,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "um you develop you have immense leverage",
      "offset": 306.96,
      "duration": 5.519
    },
    {
      "lang": "en",
      "text": "and you can you know it's a really quick",
      "offset": 310.32,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "way to kind of exert lots of influence",
      "offset": 312.479,
      "duration": 5.841
    },
    {
      "lang": "en",
      "text": "over the process and in a good way and",
      "offset": 315.68,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "so this is why I would encourage PMs to",
      "offset": 318.32,
      "duration": 7.2
    },
    {
      "lang": "en",
      "text": "really pay attention to this.",
      "offset": 321.84,
      "duration": 8
    },
    {
      "lang": "en",
      "text": "Can you guys precisely define evals?",
      "offset": 325.52,
      "duration": 7.92
    },
    {
      "lang": "en",
      "text": "Yeah, I can take this one. An eval is",
      "offset": 329.84,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "some systematic measurement of some",
      "offset": 333.44,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "aspect of quality. So what varies in an",
      "offset": 335.6,
      "duration": 7.52
    },
    {
      "lang": "en",
      "text": "eval is what that criterion is. So for",
      "offset": 338.24,
      "duration": 7.12
    },
    {
      "lang": "en",
      "text": "example, maybe it's conciseness of a",
      "offset": 343.12,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "response",
      "offset": 345.36,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "and then how you want to measure it. So",
      "offset": 346.88,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "maybe that is I'm going to define it by",
      "offset": 349.759,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "you know word length. I'm going to find",
      "offset": 352.56,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "it by sentence length. Um maybe it is",
      "offset": 354.4,
      "duration": 6.079
    },
    {
      "lang": "en",
      "text": "some, you know, very very complex",
      "offset": 358.16,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "bespoke human judgment for something",
      "offset": 360.479,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "that's more subjective. Um but those two",
      "offset": 363.12,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "things make up an email eval. And often",
      "offset": 365.68,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "times products actually have a suite of",
      "offset": 368.72,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "evals. I've never seen just one eval",
      "offset": 370.56,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "doing the job. I see three to five,",
      "offset": 373.44,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "sometimes even up to 10 evals that are",
      "offset": 376.56,
      "duration": 5.359
    },
    {
      "lang": "en",
      "text": "really important for a product.",
      "offset": 378.96,
      "duration": 5.359
    },
    {
      "lang": "en",
      "text": "People",
      "offset": 381.919,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "say that if you get eval right, you've",
      "offset": 384.319,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "gotten the hardest part of the AI",
      "offset": 386.56,
      "duration": 6.079
    },
    {
      "lang": "en",
      "text": "product solved. Is that accurate?",
      "offset": 388.96,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "I think it's accurate. Now, what do you",
      "offset": 392.639,
      "duration": 2.961
    },
    {
      "lang": "en",
      "text": "think?",
      "offset": 394.96,
      "duration": 2.32
    },
    {
      "lang": "en",
      "text": "I think it's totally accurate.",
      "offset": 395.6,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "Just like anything else, it's the",
      "offset": 397.28,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "process of creating the evals that",
      "offset": 398.72,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "provides all the value. It's not",
      "offset": 401.12,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "necessarily the eval itself is the",
      "offset": 402.8,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "journey that creates all the value. And",
      "offset": 405.6,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "so once you've done all of that work,",
      "offset": 408.8,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "you've looked at all of your data,",
      "offset": 410.4,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "you've iterated on your system, you've",
      "offset": 411.6,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "thought very carefully and and you know",
      "offset": 413.6,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "often times scientifically about how to",
      "offset": 415.44,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "improve your system, you've already got",
      "offset": 417.6,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "99% of the way there.",
      "offset": 419.599,
      "duration": 5.121
    },
    {
      "lang": "en",
      "text": "The way that I like to think about it is",
      "offset": 422.4,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "if you ever want your product to make it",
      "offset": 424.72,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "past one iteration, you need evals. I've",
      "offset": 426.96,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "never seen somebody make it through",
      "offset": 429.919,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "multiple iterations of their product",
      "offset": 431.759,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "without any evals. But once you have",
      "offset": 433.12,
      "duration": 6.079
    },
    {
      "lang": "en",
      "text": "good evals in place, then evals are not",
      "offset": 435.919,
      "duration": 5.761
    },
    {
      "lang": "en",
      "text": "necessarily the bottleneck for you. But",
      "offset": 439.199,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "that's a good thing. That's how it",
      "offset": 441.68,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "should be, right? You should be able to",
      "offset": 443.12,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "focus on building out other aspects of",
      "offset": 445.36,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "the product, making things faster,",
      "offset": 447.599,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "making things feel better, more",
      "offset": 450.16,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "intuitive, um, you know, everything",
      "offset": 452.08,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "beyond that.",
      "offset": 455.52,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "Why can't you just rely on like human",
      "offset": 457.44,
      "duration": 5.599
    },
    {
      "lang": "en",
      "text": "evals? like the PM looks at the feature,",
      "offset": 460,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "the engineers look at the feature and",
      "offset": 463.039,
      "duration": 3.041
    },
    {
      "lang": "en",
      "text": "they feel like those outputs are good",
      "offset": 464.56,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "enough.",
      "offset": 466.08,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "Oh, I love this question on the vibe",
      "offset": 467.52,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "checks and why. So, so Hamilton and I",
      "offset": 469.68,
      "duration": 4.959
    },
    {
      "lang": "en",
      "text": "teach our course and pitch it in a way",
      "offset": 472.24,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "that we we are helping you codify,",
      "offset": 474.639,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "operationalize, and scale up your vibe",
      "offset": 477.36,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "checks. Your vibe checks are very",
      "offset": 479.599,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "important, but they don't scale, right?",
      "offset": 481.68,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "Cuz they involve you, the human.",
      "offset": 484,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "It's very hard to onboard other people",
      "offset": 486.8,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "to do the vibe checks in the same way as",
      "offset": 488.639,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "you are. So like I would have to observe",
      "offset": 490.56,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "you do this thousands of times, look at",
      "offset": 493.199,
      "duration": 5.041
    },
    {
      "lang": "en",
      "text": "outputs, try to build my own rubric or",
      "offset": 495.599,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "mental model of what you're doing and",
      "offset": 498.24,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "then I have no good way of teaching",
      "offset": 500.4,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "other people of how to do this. So being",
      "offset": 501.84,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "able to do eval just means taking your",
      "offset": 504.4,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "vibe checks and translating them to",
      "offset": 506.4,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "something concrete. In our course we",
      "offset": 508.08,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "define that as a rubric of binary",
      "offset": 510.16,
      "duration": 5.999
    },
    {
      "lang": "en",
      "text": "criteria. Every criteria can be complex.",
      "offset": 513.12,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "That's fine. Can be subjective. That's",
      "offset": 516.159,
      "duration": 3.281
    },
    {
      "lang": "en",
      "text": "fine. But you better have a very precise",
      "offset": 517.76,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "definition for pass fail. Have some",
      "offset": 519.44,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "examples of pass. Have some examples of",
      "offset": 522.399,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "fail. And we also teach people ways to",
      "offset": 524.72,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "measure alignment on those results.",
      "offset": 527.04,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "That's really what this whole process is",
      "offset": 529.76,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "about.",
      "offset": 531.279,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "I think the critical phrase there is",
      "offset": 533.04,
      "duration": 7.2
    },
    {
      "lang": "en",
      "text": "binary criteria. Why binary?",
      "offset": 536,
      "duration": 8.32
    },
    {
      "lang": "en",
      "text": "Yeah. So binary really is a kind of a",
      "offset": 540.24,
      "duration": 7.279
    },
    {
      "lang": "en",
      "text": "heristic in a in a sense like that is",
      "offset": 544.32,
      "duration": 5.519
    },
    {
      "lang": "en",
      "text": "like a simplification for that works for",
      "offset": 547.519,
      "duration": 5.361
    },
    {
      "lang": "en",
      "text": "most people. And the the thing is like",
      "offset": 549.839,
      "duration": 6.161
    },
    {
      "lang": "en",
      "text": "you know a lot of people try to like",
      "offset": 552.88,
      "duration": 6.32
    },
    {
      "lang": "en",
      "text": "assign scores let's say on a rating",
      "offset": 556,
      "duration": 5.279
    },
    {
      "lang": "en",
      "text": "scale 1 to five that's usually a really",
      "offset": 559.2,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "bad idea because no one knows what that",
      "offset": 561.279,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "means. If you have a average score of",
      "offset": 562.959,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "3.2 versus average score of 3.7 what",
      "offset": 565.04,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "does that really mean? And you know that",
      "offset": 567.519,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "can be very hard to calibrate and you",
      "offset": 569.68,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "have to work incredibly hard to you know",
      "offset": 571.6,
      "duration": 6.72
    },
    {
      "lang": "en",
      "text": "make sense of that. So binary judgments",
      "offset": 574.32,
      "duration": 6.4
    },
    {
      "lang": "en",
      "text": "force you to kind of make a past fail",
      "offset": 578.32,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "decision and that tends to also",
      "offset": 580.72,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "correlate with the fact that you're",
      "offset": 582.64,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "going to have to ship this product. Do",
      "offset": 584.959,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "you want to ship it or not? And it",
      "offset": 587.2,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "really distills that decision- making",
      "offset": 589.6,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "down into the annotation. And for the",
      "offset": 591.44,
      "duration": 5.839
    },
    {
      "lang": "en",
      "text": "vast majority of people that's the right",
      "offset": 595.68,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "choice.",
      "offset": 597.279,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "Yeah. And to provide a little bit extra",
      "offset": 599.2,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "context on the background of LLM as",
      "offset": 601.279,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "judge and why you know people have a lot",
      "offset": 603.44,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "of variance in whether they want it to",
      "offset": 605.92,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "be you know binary or rating based",
      "offset": 607.6,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "scale. Um LLM as judge has been around",
      "offset": 609.6,
      "duration": 6.479
    },
    {
      "lang": "en",
      "text": "you know before these foundation models",
      "offset": 613.6,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "even just regular language models",
      "offset": 616.079,
      "duration": 5.041
    },
    {
      "lang": "en",
      "text": "fine-tuning models to serve as judges.",
      "offset": 618.32,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "Um, and in those cases, people a had a",
      "offset": 621.12,
      "duration": 5.839
    },
    {
      "lang": "en",
      "text": "lot of preference data of what is good",
      "offset": 624.48,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "and bad and maybe even a fine grain",
      "offset": 626.959,
      "duration": 6.481
    },
    {
      "lang": "en",
      "text": "scale and b could fine-tune models to be",
      "offset": 629.04,
      "duration": 6.479
    },
    {
      "lang": "en",
      "text": "aligned with that preference data.",
      "offset": 633.44,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "Today's world of LLM judge is very",
      "offset": 635.519,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "different. We don't see people",
      "offset": 637.839,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "fine-tuning judge models as much. We see",
      "offset": 639.44,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "people trying to use off-the-shelf",
      "offset": 641.839,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "models, still want to align with their",
      "offset": 643.279,
      "duration": 5.761
    },
    {
      "lang": "en",
      "text": "complex subjective criteria.",
      "offset": 646.16,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "And now the alignment problem is much",
      "offset": 649.04,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "harder, right? You can't, you know,",
      "offset": 651.68,
      "duration": 5.279
    },
    {
      "lang": "en",
      "text": "steer the LLM in a way that you could",
      "offset": 653.36,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "before. Um, and for that reason, we say",
      "offset": 656.959,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "limit yourself to binary because that is",
      "offset": 659.44,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "what the LLM can do very well. All you",
      "offset": 661.2,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "have to do is provide examples of past,",
      "offset": 663.519,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "provide examples of fail and have, you",
      "offset": 665.519,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "know, very simple or like good rubrics.",
      "offset": 667.92,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "Um, and people find that much easier to",
      "offset": 670.48,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "do than say rating on a scale of one to",
      "offset": 672.64,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "five. Okay, now you need to provide",
      "offset": 675.6,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "examples for one, for two, for three,",
      "offset": 677.04,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "for four, for five. You need to, you",
      "offset": 679.279,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "know, have descriptions of what makes a",
      "offset": 681.68,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "one different from a two. All of these",
      "offset": 683.92,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "things, you know, the pair wise",
      "offset": 686.56,
      "duration": 2.32
    },
    {
      "lang": "en",
      "text": "interactions between all these ratings",
      "offset": 687.6,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "just explode in complexity. And we never",
      "offset": 688.88,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "see people successfully able to",
      "offset": 691.2,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "operationalize that at the rate at which",
      "offset": 692.959,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "they can do binary evals. And a lot of",
      "offset": 694.88,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "times um non-binary evals like ratings",
      "offset": 697.68,
      "duration": 6.56
    },
    {
      "lang": "en",
      "text": "of 1 to five that is a a smell of",
      "offset": 701.12,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "intellectual laziness",
      "offset": 704.24,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "like the work hasn't been done to",
      "offset": 706.88,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "actually",
      "offset": 709.04,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "you know to make a call of like what is",
      "offset": 710.64,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "good enough and what's not good enough",
      "offset": 713.04,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "and it's kind of like h we don't really",
      "offset": 715.2,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "know let's just capture these like rough",
      "offset": 716.72,
      "duration": 6.72
    },
    {
      "lang": "en",
      "text": "things um you know in in this like uh",
      "offset": 720.079,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "score cuz we're going to lose something",
      "offset": 723.44,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "and it doesn't you know like the binary",
      "offset": 724.8,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "ary skill like really forces you to be",
      "offset": 726.639,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "very clear about what you want.",
      "offset": 728.72,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "AI evals are one of the most important",
      "offset": 731.04,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "skills for PMs and I know you know they",
      "offset": 733.76,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "matter. The question is are you doing",
      "offset": 736.24,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "them right? Most teams are winging it",
      "offset": 738.24,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "with basic metrics and hoping for the",
      "offset": 740.56,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "best. Meanwhile, the teams that actually",
      "offset": 743.279,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "ship reliable AI, they've cracked the",
      "offset": 745.36,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "code on systematic evaluation. Today's",
      "offset": 747.68,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "episode is brought to you by the AIE",
      "offset": 750.24,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "evals for engineers and PMS course by HL",
      "offset": 752.399,
      "duration": 6.161
    },
    {
      "lang": "en",
      "text": "Hussein and Shrea Shunker. This live",
      "offset": 756,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "Maven course will teach you the battle",
      "offset": 758.56,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "tested frameworks from HML and Shrea who",
      "offset": 760.88,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "are the engineers behind GitHub",
      "offset": 763.36,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "copilot's evaluation system and 25 plus",
      "offset": 765.04,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "production AI implementations. Four",
      "offset": 768.399,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "weeks live instruction. Next cohort",
      "offset": 770.72,
      "duration": 5.919
    },
    {
      "lang": "en",
      "text": "starts July 21st. Start shipping AI that",
      "offset": 773.839,
      "duration": 5.761
    },
    {
      "lang": "en",
      "text": "actually works. Enroll at maven.com with",
      "offset": 776.639,
      "duration": 6.081
    },
    {
      "lang": "en",
      "text": "my code ag-roduct-growth",
      "offset": 779.6,
      "duration": 8.6
    },
    {
      "lang": "en",
      "text": "for over $800 off. That's ag-pr-g",
      "offset": 782.72,
      "duration": 5.48
    },
    {
      "lang": "en",
      "text": "r o wt. Today's episode is brought to",
      "offset": 788.72,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "you by Jira product discovery. If you're",
      "offset": 791.519,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "like most product managers, you're",
      "offset": 793.6,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "probably in Jira, tracking tickets and",
      "offset": 795.12,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "managing the backlog. But what about",
      "offset": 797.279,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "everything that happens before delivery?",
      "offset": 799.04,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "Jurro product discovery helps you move",
      "offset": 801.68,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "your discovery, prioritization, and even",
      "offset": 803.44,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "road mapping work out of spreadsheets",
      "offset": 805.76,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "and into a purpose-built tool designed",
      "offset": 808.32,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "for product teams. Capture insights,",
      "offset": 811.44,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "prioritize what matters, and create road",
      "offset": 813.92,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "maps you can easily tailor for any",
      "offset": 816.24,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "audience. And because it's built to work",
      "offset": 818.48,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "with Jira, everything stays connected",
      "offset": 820.32,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "from idea to delivery. Used by product",
      "offset": 822.399,
      "duration": 5.521
    },
    {
      "lang": "en",
      "text": "teams at Canva, Deliveroo, and even The",
      "offset": 825.12,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "Economist. Check out why and try it for",
      "offset": 827.92,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "free today at",
      "offset": 830.56,
      "duration": 4.04
    },
    {
      "lang": "en",
      "text": "atlassian.com/roduct-discovery.",
      "offset": 831.6,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "That's a t-san.com/roduct-discovery.",
      "offset": 835.6,
      "duration": 3.72
    },
    {
      "lang": "en",
      "text": "Juro product discovery. Build the right",
      "offset": 842.24,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "thing.",
      "offset": 844.639,
      "duration": 5.041
    },
    {
      "lang": "en",
      "text": "I've heard that LLM are also not very",
      "offset": 846.32,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "good at one to five ratings. Is that",
      "offset": 849.68,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "true?",
      "offset": 852.32,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "They're good at what they're trained on.",
      "offset": 854.32,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "Somewhere out there in the world, I am",
      "offset": 857.68,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "sure there is a task with very clear or",
      "offset": 859.36,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "simple one to five ratings and the LLM",
      "offset": 862,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "is good for that. But to make such a",
      "offset": 864.24,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "blanket statement for all products and",
      "offset": 866.32,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "all use cases is very hard to do. That's",
      "offset": 868.48,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "the thing. That's the message we want to",
      "offset": 870.88,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "hammer home to every single product",
      "offset": 872.32,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "manager who takes the course. Like look,",
      "offset": 874.079,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "you think the LLM might be able to do",
      "offset": 876.16,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "something. You saw an instance of an LLM",
      "offset": 877.839,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "being able to do the task for some other",
      "offset": 880.16,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "domain. That doesn't mean it's going to",
      "offset": 882.079,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "translate to your domain or your use",
      "offset": 883.92,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "case. you still have to put in this",
      "offset": 885.839,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "work. Um, and just don't trust any Haml",
      "offset": 887.68,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "is a great way of saying this. Maybe he",
      "offset": 890.399,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "should talk about it. But he always",
      "offset": 892,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "tells people never trust it. Always put",
      "offset": 894.32,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "on your detective hat. Haml, you want to",
      "offset": 896.48,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "talk about that?",
      "offset": 899.199,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "Yeah. What underlines the entire process",
      "offset": 900.72,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "of evals is the scientific method. It's",
      "offset": 903.92,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "something that we've all learned in high",
      "offset": 906.72,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "school education. Um, but it's really",
      "offset": 908.8,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "applied, you know, in this context. Then",
      "offset": 911.6,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "what you have to do is be very skeptical",
      "offset": 914,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "of everything and do lots of experiments",
      "offset": 916.079,
      "duration": 7.361
    },
    {
      "lang": "en",
      "text": "and prove to yourself that the the thing",
      "offset": 920,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "that you're trying to achieve or some",
      "offset": 923.44,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "new complexity you want to add whatever",
      "offset": 925.68,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "it is that is actually working and try",
      "offset": 928,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "to do it in the simplest way. Um and",
      "offset": 929.68,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "build intuition doing by doing lots of",
      "offset": 932.24,
      "duration": 5.279
    },
    {
      "lang": "en",
      "text": "experiments. Um but the point is to like",
      "offset": 934.56,
      "duration": 6.32
    },
    {
      "lang": "en",
      "text": "measure those and like you know record",
      "offset": 937.519,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "those and go through it in a structured",
      "offset": 940.88,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "way uh rather than those vibe checks.",
      "offset": 942.959,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "You you asked about vibe checks earlier.",
      "offset": 945.12,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "The analogy that I like to use and I can",
      "offset": 947.839,
      "duration": 5.521
    },
    {
      "lang": "en",
      "text": "give this to you uh if you ask me later.",
      "offset": 950.72,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "There's a uh there's a little video of",
      "offset": 953.36,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "my friend Greg Sakarelli playing",
      "offset": 956.16,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "whack-a-ole and it's my favorite meme to",
      "offset": 958.079,
      "duration": 5.361
    },
    {
      "lang": "en",
      "text": "use when telling people about the need",
      "offset": 960.8,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "for evals. It's, you know, it's like",
      "offset": 963.44,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "you're playing whack-a-ole without",
      "offset": 965.839,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "evals, you know. So, you see a problem,",
      "offset": 967.44,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "okay, hammer it over with some tool or a",
      "offset": 970.16,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "prompt change. Then another problem",
      "offset": 973.04,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "comes up, you hammer that and you keep",
      "offset": 974.72,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "going, you don't really make any",
      "offset": 976.88,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "progress. It's really with Eval that you",
      "offset": 978.399,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "can systematically try to solve the",
      "offset": 980.56,
      "duration": 5.519
    },
    {
      "lang": "en",
      "text": "problem without like going in circles.",
      "offset": 982.32,
      "duration": 5.759
    },
    {
      "lang": "en",
      "text": "I want to talk about some stories. So,",
      "offset": 986.079,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "one of the features I implemented in my",
      "offset": 988.079,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "last job, I was VP of product at",
      "offset": 990,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "Apollo.io. It's a unicorn startup that",
      "offset": 992.32,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "does sales technology. So, what do sales",
      "offset": 994.399,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "people need to do, right? They need to",
      "offset": 996.639,
      "duration": 5.841
    },
    {
      "lang": "en",
      "text": "write emails. So, as soon as I think it",
      "offset": 998.399,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "was Chad GPT 3.5 came out, we're like,",
      "offset": 1002.48,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "\"Okay, we're going to use GPT 3.5 to",
      "offset": 1004.639,
      "duration": 5.681
    },
    {
      "lang": "en",
      "text": "write people's emails.\" But the very",
      "offset": 1007.92,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "first thing we found was that it was",
      "offset": 1010.32,
      "duration": 5.519
    },
    {
      "lang": "en",
      "text": "hallucinating all sorts of crazy details",
      "offset": 1012.16,
      "duration": 5.599
    },
    {
      "lang": "en",
      "text": "and ultimately we had to set up a bunch",
      "offset": 1015.839,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "of evals instead of vibe checks. Can you",
      "offset": 1017.759,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "guys give a little bit more context into",
      "offset": 1020.48,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "why eval help solve that hallucination",
      "offset": 1022.72,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "problem for us?",
      "offset": 1025.28,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "Yeah. So with any kind of problem that",
      "offset": 1027.36,
      "duration": 5.439
    },
    {
      "lang": "en",
      "text": "you see um you know hallucination or",
      "offset": 1029.679,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "whatever it is. So the first thing to",
      "offset": 1032.799,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "know about evals where people go off the",
      "offset": 1034.88,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "rails is do not reach for generic",
      "offset": 1037.039,
      "duration": 6.481
    },
    {
      "lang": "en",
      "text": "metrics. So the industry is full of",
      "offset": 1040.48,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "tools and vendors wanting to sell you",
      "offset": 1043.52,
      "duration": 5.519
    },
    {
      "lang": "en",
      "text": "like a magic pill to solve your eval",
      "offset": 1046.48,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "problem. like, hey, don't worry. Just",
      "offset": 1049.039,
      "duration": 5.281
    },
    {
      "lang": "en",
      "text": "buy our tool, plug it in. We'll show you",
      "offset": 1051.12,
      "duration": 5.679
    },
    {
      "lang": "en",
      "text": "a dashboard of all of these metrics and",
      "offset": 1054.32,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "things like that. The problem is it",
      "offset": 1056.799,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "doesn't work because those offtheshelf",
      "offset": 1059.039,
      "duration": 6.801
    },
    {
      "lang": "en",
      "text": "eval hallucination score is not going to",
      "offset": 1061.679,
      "duration": 7.201
    },
    {
      "lang": "en",
      "text": "work. What you need to do is take a look",
      "offset": 1065.84,
      "duration": 6.56
    },
    {
      "lang": "en",
      "text": "at in your case those emails and",
      "offset": 1068.88,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "understand like what exactly is the",
      "offset": 1072.4,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "failure mode and what if you do uh",
      "offset": 1074.32,
      "duration": 6.64
    },
    {
      "lang": "en",
      "text": "observe a hallucination",
      "offset": 1078.88,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "what is the hallucination and kind of",
      "offset": 1080.96,
      "duration": 4.959
    },
    {
      "lang": "en",
      "text": "give more life to the domain specificity",
      "offset": 1083.28,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "of the hallucination so that you can",
      "offset": 1085.919,
      "duration": 6.481
    },
    {
      "lang": "en",
      "text": "then start crafting an LLM as a judge",
      "offset": 1089.28,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "that is prompted in such a way that is",
      "offset": 1092.4,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "very specific to the types of",
      "offset": 1095.12,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "hallucination that you are seeing.",
      "offset": 1096.72,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "Um, and then you go to through an",
      "offset": 1099.919,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "iterative process. So, you kind of",
      "offset": 1101.44,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "handle label when hallucinations happen,",
      "offset": 1103.6,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "you know, how they're happening. You",
      "offset": 1106.96,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "you're building an LLM as a judge and",
      "offset": 1108.799,
      "duration": 3.521
    },
    {
      "lang": "en",
      "text": "you're measuring that judge. You're",
      "offset": 1110.96,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "being skeptical again through a",
      "offset": 1112.32,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "scientific process and you're saying,",
      "offset": 1113.84,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "\"Okay, I have this judge. Can I get it",
      "offset": 1115.679,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "to agree with me? Can I have it be let's",
      "offset": 1118.08,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "say if if it's you Akos making this",
      "offset": 1121.039,
      "duration": 5.121
    },
    {
      "lang": "en",
      "text": "judge this email hallucination thing um",
      "offset": 1123.2,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "you know how do I make this judge a",
      "offset": 1126.16,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "proxy of me and how do I trust it almost",
      "offset": 1128.08,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "like an employee and so the only way to",
      "offset": 1130.72,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "do that is to check it and you can do",
      "offset": 1133.36,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "that iteratively through this process um",
      "offset": 1134.96,
      "duration": 5.839
    },
    {
      "lang": "en",
      "text": "and when you do that then you not only",
      "offset": 1138.16,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "do you have something that scales it's",
      "offset": 1140.799,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "an automated way of checking that",
      "offset": 1142.64,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "problem but it's also something that you",
      "offset": 1144.16,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "trust in that second part that's",
      "offset": 1146.64,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "something that you trust is key because",
      "offset": 1148.72,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "the last thing you want is the whole",
      "offset": 1151.52,
      "duration": 3.519
    },
    {
      "lang": "en",
      "text": "bunch of evals that you put up on a",
      "offset": 1153.28,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "dashboard and then people stop looking",
      "offset": 1155.039,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "at them because they're like h we have",
      "offset": 1157.679,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "these evals but you know what the",
      "offset": 1159.679,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "product doesn't really work.",
      "offset": 1161.039,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "No, that that's the death of your AI",
      "offset": 1163.28,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "product because then no one's going to",
      "offset": 1165.84,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "ever look at eval again and then you",
      "offset": 1167.919,
      "duration": 5.64
    },
    {
      "lang": "en",
      "text": "don't have any leverage.",
      "offset": 1170.08,
      "duration": 3.479
    },
    {
      "lang": "en",
      "text": "That's exactly what we experienced and",
      "offset": 1174.08,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "you weren't even there. I want to talk a",
      "offset": 1176.16,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "little bit about your experiences. I",
      "offset": 1178,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "know you worked at Airbnb on these",
      "offset": 1180,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "products. Can you tell us a little bit",
      "offset": 1183.2,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "more about that? And what was the most",
      "offset": 1184.48,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "difficult part of building evals there?",
      "offset": 1186.24,
      "duration": 6.64
    },
    {
      "lang": "en",
      "text": "So, I didn't work on LMS at Airbnb",
      "offset": 1190,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "because it was prior to, you know, way",
      "offset": 1192.88,
      "duration": 6.72
    },
    {
      "lang": "en",
      "text": "before ChatBT. Um but what I did work on",
      "offset": 1195.76,
      "duration": 7.039
    },
    {
      "lang": "en",
      "text": "my entire career is machine learning and",
      "offset": 1199.6,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "um you know building predictive models",
      "offset": 1202.799,
      "duration": 6.801
    },
    {
      "lang": "en",
      "text": "and a lot of the same machinery of evals",
      "offset": 1205.36,
      "duration": 7.92
    },
    {
      "lang": "en",
      "text": "comes directly from machine learning. Um",
      "offset": 1209.6,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "the reason that is is because machine",
      "offset": 1213.28,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "learning systems are systems that",
      "offset": 1214.799,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "produce stochastic outputs. you know,",
      "offset": 1216.88,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "they'll give you predictions of various",
      "offset": 1219.28,
      "duration": 5.519
    },
    {
      "lang": "en",
      "text": "kinds um or like classified things and",
      "offset": 1220.96,
      "duration": 6.079
    },
    {
      "lang": "en",
      "text": "they're they're non-deterministic",
      "offset": 1224.799,
      "duration": 5.601
    },
    {
      "lang": "en",
      "text": "and you have to evaluate them. You like",
      "offset": 1227.039,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "you know they giving you different",
      "offset": 1230.4,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "outputs every time and they have noise",
      "offset": 1231.76,
      "duration": 6.32
    },
    {
      "lang": "en",
      "text": "and um so how do you do that? That's",
      "offset": 1234.88,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "something that's very wellestablished in",
      "offset": 1238.08,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "machine learning that you know a lot of",
      "offset": 1240.32,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "people haven't been exposed to. So when",
      "offset": 1242.32,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "it comes to like AI more generally now",
      "offset": 1244.72,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "you have really very similar thing. It's",
      "offset": 1247.039,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "like hey you have like a stochastic",
      "offset": 1250.24,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "system you know it's non-deterministic",
      "offset": 1251.919,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "it can output anything like these emails",
      "offset": 1254.559,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "it's like how do you go about measuring",
      "offset": 1256.559,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "that and um yeah and so basically you",
      "offset": 1258.24,
      "duration": 5.919
    },
    {
      "lang": "en",
      "text": "can kind of bring that over and so what",
      "offset": 1262.159,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "we teach is instead of going through the",
      "offset": 1264.159,
      "duration": 5.041
    },
    {
      "lang": "en",
      "text": "entire data science machine learning",
      "offset": 1266.799,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "curriculum how do you have a very",
      "offset": 1269.2,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "focused way of learning that that is",
      "offset": 1272.559,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "contextualized",
      "offset": 1275.28,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "to LLMs",
      "offset": 1276.96,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "That's actually fascinating. How was",
      "offset": 1280.72,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "Airbnb using machine learning models?",
      "offset": 1282.72,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "I'm sure people want to know under the",
      "offset": 1285.36,
      "duration": 2.559
    },
    {
      "lang": "en",
      "text": "hood.",
      "offset": 1286.64,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "Yeah, Airbnb was using it for a lot of",
      "offset": 1287.919,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "things such as detecting fraud in",
      "offset": 1289.76,
      "duration": 5.919
    },
    {
      "lang": "en",
      "text": "payments. Um, also the biggest use case",
      "offset": 1292.72,
      "duration": 5.839
    },
    {
      "lang": "en",
      "text": "at Airbnb was search ranking. So, if",
      "offset": 1295.679,
      "duration": 5.281
    },
    {
      "lang": "en",
      "text": "you're searching for a listing, let me",
      "offset": 1298.559,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "show you other listings that you might",
      "offset": 1300.96,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "be interested in based upon what you've",
      "offset": 1303.36,
      "duration": 2.64
    },
    {
      "lang": "en",
      "text": "been looking at and what you're",
      "offset": 1304.96,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "searching for. So basic recommendation",
      "offset": 1306,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "systems, search ranking, things like",
      "offset": 1307.84,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "that. Um, a lot of like growth marketing",
      "offset": 1310.64,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "initiatives like trying to figure out",
      "offset": 1314.32,
      "duration": 5.599
    },
    {
      "lang": "en",
      "text": "the lifetime value of a specific guest",
      "offset": 1316.4,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "so you can allocate marketing to them",
      "offset": 1319.919,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "appropriately. That's what I worked on.",
      "offset": 1321.84,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "Um, you know, and there's many other use",
      "offset": 1324.48,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "cases, but those are the ones that sort",
      "offset": 1326.96,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "of one of like their bigger ones. So now",
      "offset": 1329.52,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "they have you know now Airbnb is doing",
      "offset": 1331.44,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "generative AI stuff uh as well just like",
      "offset": 1333.52,
      "duration": 5.519
    },
    {
      "lang": "en",
      "text": "any other company.",
      "offset": 1336.08,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "Search ranking is something that",
      "offset": 1339.039,
      "duration": 3.041
    },
    {
      "lang": "en",
      "text": "probably we've been dealing with right",
      "offset": 1340.48,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "for like 30 plus years I guess ever",
      "offset": 1342.08,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "since you know these search engines ever",
      "offset": 1344.64,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "came out. So what can we learn from how",
      "offset": 1346.559,
      "duration": 6.641
    },
    {
      "lang": "en",
      "text": "people are evaluating search and apply",
      "offset": 1350.08,
      "duration": 6.64
    },
    {
      "lang": "en",
      "text": "into how we're evaluating our LLMs?",
      "offset": 1353.2,
      "duration": 7.76
    },
    {
      "lang": "en",
      "text": "The number one difference now is we",
      "offset": 1356.72,
      "duration": 7.12
    },
    {
      "lang": "en",
      "text": "definitely search systems to try to get",
      "offset": 1360.96,
      "duration": 5.599
    },
    {
      "lang": "en",
      "text": "context to improve our LLMs. Sure. But",
      "offset": 1363.84,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "the consumer of the search results is",
      "offset": 1366.559,
      "duration": 5.281
    },
    {
      "lang": "en",
      "text": "now the LLM. In the past, the consumer",
      "offset": 1369.039,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "of search has been the human. And humans",
      "offset": 1371.84,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "and LLMs are good at different things.",
      "offset": 1373.919,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "LLMs are good at finding needle in a",
      "offset": 1376.24,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "haststack in very very long complex",
      "offset": 1378.24,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "windows. Humans have very short",
      "offset": 1380,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "attention spans. They'll read through",
      "offset": 1382.64,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "things, but after a few paragraphs,",
      "offset": 1384.72,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "they're done. So, some of the metrics",
      "offset": 1386.64,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "like how high up a relevant result was",
      "offset": 1389.039,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "ranked",
      "offset": 1391.84,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "are much more important for humans than",
      "offset": 1393.679,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "they are for LLMs, you know, you could",
      "offset": 1396.24,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "try to retrieve like 500 results and as",
      "offset": 1398,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "long as it's, you know, even if it's",
      "offset": 1400.48,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "like 150, 200, the LLM will pick it out",
      "offset": 1402.4,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "and figure out how to give that result",
      "offset": 1406.08,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "to the h the user. So I think the bottom",
      "offset": 1407.52,
      "duration": 5.279
    },
    {
      "lang": "en",
      "text": "line differences are, you know, we still",
      "offset": 1410.64,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "want to use the same metrics, but our",
      "offset": 1412.799,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "tolerance",
      "offset": 1415.52,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "has changed slightly. We still want to",
      "offset": 1417.039,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "prioritize recalling the right",
      "offset": 1419.12,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "information, but now that we have long",
      "offset": 1420.48,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "context windows, it's okay for it to",
      "offset": 1422.48,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "kind of be at the end. Um, as long as we",
      "offset": 1424.799,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "carefully, you know, leverage LLMs to go",
      "offset": 1426.88,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "and uh iteratively refine those search",
      "offset": 1429.2,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "results, pick out the bottom results,",
      "offset": 1431.679,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "and then go and show that back to the",
      "offset": 1433.44,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "human.",
      "offset": 1435.039,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "One thing I'll add on to this is people",
      "offset": 1436.88,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "often",
      "offset": 1438.64,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "wonder okay how do I evaluate rag",
      "offset": 1440.24,
      "duration": 6.72
    },
    {
      "lang": "en",
      "text": "systems. So rag is a big thing and so",
      "offset": 1442.72,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "what what is rag you know retrieval",
      "offset": 1446.96,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "augmented generation there's the",
      "offset": 1448.88,
      "duration": 3.039
    },
    {
      "lang": "en",
      "text": "retrieval part and then there's the",
      "offset": 1450.4,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "generation part and so the retrieval",
      "offset": 1451.919,
      "duration": 6.401
    },
    {
      "lang": "en",
      "text": "part you evaluate that pretty much the",
      "offset": 1454.559,
      "duration": 5.761
    },
    {
      "lang": "en",
      "text": "exact same way that you would evaluate",
      "offset": 1458.32,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "any search system. So all those classic",
      "offset": 1460.32,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "search systems um and informationational",
      "offset": 1462.559,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "retrieval like that entire scientific",
      "offset": 1465.36,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "discipline can be applied onto the",
      "offset": 1467.679,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "retrieval part. So like optimizing that",
      "offset": 1469.84,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "making sure you're getting the right",
      "offset": 1472.48,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "documents, the right context, so on and",
      "offset": 1473.919,
      "duration": 6.481
    },
    {
      "lang": "en",
      "text": "so forth. Um and a lot of it applies and",
      "offset": 1475.84,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "Trey is right there's a lot there is",
      "offset": 1480.4,
      "duration": 3.279
    },
    {
      "lang": "en",
      "text": "some nuance in terms of different",
      "offset": 1481.6,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "tolerances and things like that.",
      "offset": 1483.679,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "Yeah. Yeah, the the number one thing I",
      "offset": 1487.2,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "see that's different is instead of",
      "offset": 1489.039,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "measuring like recall at 10, like we can",
      "offset": 1490.4,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "measure recall at 500 and have a long",
      "offset": 1492.88,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "context model like Gemini be able to",
      "offset": 1494.88,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "consume those results. Uh at the end of",
      "offset": 1496.88,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "the day, maybe you want to measure",
      "offset": 1499.6,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "precision",
      "offset": 1502,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "for the result of the LLM call that",
      "offset": 1503.679,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "feeds in the result to the human. Sure,",
      "offset": 1505.6,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "whatever humans consume, they should all",
      "offset": 1507.44,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "be measured the same way. But if LLMs",
      "offset": 1509.12,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "are there, refining search results,",
      "offset": 1512,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "then we can use that to our advantage to",
      "offset": 1514.72,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "be able to rerank outputs from a search",
      "offset": 1516.48,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "engine.",
      "offset": 1519.2,
      "duration": 4.959
    },
    {
      "lang": "en",
      "text": "So that's search. One of the other sort",
      "offset": 1522,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "of really big things in the AI space",
      "offset": 1524.159,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "that I think HML you worked on a little",
      "offset": 1527.039,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "bit was the precursor to GitHub copilot,",
      "offset": 1528.72,
      "duration": 6.079
    },
    {
      "lang": "en",
      "text": "right? LMS for code generation. What was",
      "offset": 1531.84,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "the biggest problem you guys faced with",
      "offset": 1534.799,
      "duration": 4.921
    },
    {
      "lang": "en",
      "text": "eval?",
      "offset": 1536.72,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "So",
      "offset": 1539.76,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "with okay things like code generation",
      "offset": 1541.6,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "are actually really interesting. So in",
      "offset": 1543.36,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "eval there's more generally in AI",
      "offset": 1545.44,
      "duration": 6.96
    },
    {
      "lang": "en",
      "text": "products more generally is you want your",
      "offset": 1548.48,
      "duration": 7.6
    },
    {
      "lang": "en",
      "text": "domain expert in inside the inner loop.",
      "offset": 1552.4,
      "duration": 5.519
    },
    {
      "lang": "en",
      "text": "So what you don't want to do is give",
      "offset": 1556.08,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "your developer the the task of",
      "offset": 1557.919,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "annotating your data and you know",
      "offset": 1560.4,
      "duration": 6.32
    },
    {
      "lang": "en",
      "text": "writing evals necessarily because they",
      "offset": 1563.919,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "don't know enough they don't have enough",
      "offset": 1566.72,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "context and that really bottlenecks a",
      "offset": 1568.24,
      "duration": 7.039
    },
    {
      "lang": "en",
      "text": "lot of teams because they don't get that",
      "offset": 1572.72,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "right. But there's an exception. The",
      "offset": 1575.279,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "exception is developer tools. That is",
      "offset": 1577.919,
      "duration": 5.041
    },
    {
      "lang": "en",
      "text": "one case where the domain expert is the",
      "offset": 1580.24,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "developer. And so that's why we saw",
      "offset": 1582.96,
      "duration": 7.44
    },
    {
      "lang": "en",
      "text": "developer tools like as the first you",
      "offset": 1586.72,
      "duration": 6.64
    },
    {
      "lang": "en",
      "text": "know AI products because that was the",
      "offset": 1590.4,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "sweet spot that was like where it was",
      "offset": 1593.36,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "easiest to develop. That's one property",
      "offset": 1594.799,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "of you know developer tools. The second",
      "offset": 1597.279,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "property is there's a highly verifiable",
      "offset": 1600,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "domain.",
      "offset": 1601.84,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "So there's some structure to code and it",
      "offset": 1603.44,
      "duration": 6.239
    },
    {
      "lang": "en",
      "text": "turns out like on GitHub um you know you",
      "offset": 1606.4,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "have all this code and you also have",
      "offset": 1609.679,
      "duration": 5.041
    },
    {
      "lang": "en",
      "text": "lots of tests defined against that code.",
      "offset": 1611.6,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "So a lot of time was spent in that",
      "offset": 1614.72,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "verifiable domain and it's excellent",
      "offset": 1616.96,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "when you have a verifiable domain um is",
      "offset": 1618.799,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "to develop kind of a test harness and",
      "offset": 1621.679,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "the test harness was very impressive.",
      "offset": 1624.799,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "Basically what it did is it took all of",
      "offset": 1627.279,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "the code at scale, not all of it, but a",
      "offset": 1629.76,
      "duration": 6.64
    },
    {
      "lang": "en",
      "text": "select like filtered quantity of it and",
      "offset": 1632,
      "duration": 6.559
    },
    {
      "lang": "en",
      "text": "basically recreated the environment that",
      "offset": 1636.4,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "code is going to run in and ran the test",
      "offset": 1638.559,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "at scale. And basically it, you know,",
      "offset": 1640.4,
      "duration": 5.519
    },
    {
      "lang": "en",
      "text": "things like asking the LLM to, you know,",
      "offset": 1642.799,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "fix certain things or complete certain",
      "offset": 1645.919,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "code and it would run all the tests. is",
      "offset": 1647.919,
      "duration": 5.041
    },
    {
      "lang": "en",
      "text": "kind of a very impressive engineering",
      "offset": 1650.72,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "effort because you're talking about",
      "offset": 1652.96,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "running all this random software from",
      "offset": 1654.88,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "everywhere with all kinds of",
      "offset": 1657.6,
      "duration": 6.319
    },
    {
      "lang": "en",
      "text": "dependencies at scale all the time. Um,",
      "offset": 1659.039,
      "duration": 8.24
    },
    {
      "lang": "en",
      "text": "so the the details don't matter. What I",
      "offset": 1663.919,
      "duration": 5.521
    },
    {
      "lang": "en",
      "text": "want to say is like the eval really",
      "offset": 1667.279,
      "duration": 5.121
    },
    {
      "lang": "en",
      "text": "mattered because a lot of upfront work",
      "offset": 1669.44,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "went into",
      "offset": 1672.4,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "constructing the eval system and",
      "offset": 1674.32,
      "duration": 6.719
    },
    {
      "lang": "en",
      "text": "it's really after that that the team was",
      "offset": 1678.88,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "able to iterate really fast like when",
      "offset": 1681.039,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "GitHub copilot was first released",
      "offset": 1683.12,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "internally it it didn't really work you",
      "offset": 1685.76,
      "duration": 5.919
    },
    {
      "lang": "en",
      "text": "know it was something like you know 20%",
      "offset": 1688.64,
      "duration": 6.639
    },
    {
      "lang": "en",
      "text": "or so of you know",
      "offset": 1691.679,
      "duration": 5.921
    },
    {
      "lang": "en",
      "text": "uh like",
      "offset": 1695.279,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "uh acceptance rates of like suggestions,",
      "offset": 1697.6,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "things like that. And then after the",
      "offset": 1700.08,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "evals, the team was able to like iterate",
      "offset": 1701.6,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "really fast and like climb that uh you",
      "offset": 1703.84,
      "duration": 6.719
    },
    {
      "lang": "en",
      "text": "know to where it did work. Um and so you",
      "offset": 1706.399,
      "duration": 7.52
    },
    {
      "lang": "en",
      "text": "know it was really like",
      "offset": 1710.559,
      "duration": 5.281
    },
    {
      "lang": "en",
      "text": "or let me step back. It it was yeah it",
      "offset": 1713.919,
      "duration": 3.281
    },
    {
      "lang": "en",
      "text": "was it was really like the key that",
      "offset": 1715.84,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "unlocked uh you know progress on that.",
      "offset": 1717.2,
      "duration": 6.88
    },
    {
      "lang": "en",
      "text": "um you know I didn't work directly on",
      "offset": 1720.559,
      "duration": 6.561
    },
    {
      "lang": "en",
      "text": "the GitHub copilot like you know in that",
      "offset": 1724.08,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "phase I was kind of working on some",
      "offset": 1727.12,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "research before that um I was actually",
      "offset": 1728.96,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "working on some research that led to",
      "offset": 1731.6,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "some of the benchmarks",
      "offset": 1734.24,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "so um basically one thing I did at",
      "offset": 1736.32,
      "duration": 6.479
    },
    {
      "lang": "en",
      "text": "GitHub is uh you know I worked on this",
      "offset": 1739.36,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "project called code searchnet which is a",
      "offset": 1742.799,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "semantic search of code and basically",
      "offset": 1745.36,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "like you type in like what what code",
      "offset": 1748.159,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "you're looking for and like semantically",
      "offset": 1750.48,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "try to find it and it leveraged a lot of",
      "offset": 1752.24,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "the um there's so a lot of code has",
      "offset": 1754.559,
      "duration": 5.281
    },
    {
      "lang": "en",
      "text": "comments in it in the documentation",
      "offset": 1757.76,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "embedded in that code. So it was just a",
      "offset": 1759.84,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "really large data set. We opened a large",
      "offset": 1761.76,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "data set with benchmarks of",
      "offset": 1764.159,
      "duration": 6.081
    },
    {
      "lang": "en",
      "text": "uh you know code retrieval and so that",
      "offset": 1767.2,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "was like used by open AAI in their very",
      "offset": 1770.24,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "first iteration of codeex which is like",
      "offset": 1772.32,
      "duration": 6.479
    },
    {
      "lang": "en",
      "text": "a very u old model it's not not the",
      "offset": 1775.039,
      "duration": 5.601
    },
    {
      "lang": "en",
      "text": "current codeex this is like a different",
      "offset": 1778.799,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "the same name but different different",
      "offset": 1780.64,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "thing um but it was it was an eval it",
      "offset": 1782.24,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "was an eval that was used back in the",
      "offset": 1784.799,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "day so you can say like been working on",
      "offset": 1786.799,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "LM eval for a really long time",
      "offset": 1788.559,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "trust Trust isn't just earned, it's",
      "offset": 1792.399,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "demanded. Whether you're a startup",
      "offset": 1794.159,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "founder navigating your first audit or a",
      "offset": 1796,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "seasoned professional scaling your GRC",
      "offset": 1798.24,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "program, proving your commitment to",
      "offset": 1800.24,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "security has never been more critical or",
      "offset": 1802.08,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "more complex. That's where Vanta comes",
      "offset": 1804.159,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "in. Businesses use Vanta to establish",
      "offset": 1806.48,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "trust by automating compliance needs",
      "offset": 1808.88,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "across over 35 frameworks like SOCK 2",
      "offset": 1810.72,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "and ISO 2701.",
      "offset": 1813.6,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "centralized security workflows, complete",
      "offset": 1816.24,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "questionnaires up to five times faster,",
      "offset": 1818.48,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "and proactively manage vendor risk.",
      "offset": 1820.559,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "Vanta can help you start or scale your",
      "offset": 1823.039,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "security program by connecting you with",
      "offset": 1825.039,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "auditors and experts to conduct your",
      "offset": 1826.88,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "audit and set up your security program",
      "offset": 1828.64,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "quickly. Plus, with automation and AI",
      "offset": 1830.32,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "throughout the platform, Vant gives you",
      "offset": 1833.039,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "time back so you can focus on building",
      "offset": 1834.64,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "your company. Join over 9,000 global",
      "offset": 1836.559,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "companies like Atlassian, Kora, and",
      "offset": 1839.039,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "Factory who use Vant to manage risk and",
      "offset": 1841.36,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "prove security in real time. For a",
      "offset": 1843.36,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "limited time, my listeners get $1,000",
      "offset": 1845.12,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "off Fanta at vanta.com/acosash.",
      "offset": 1847.279,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "That's v a nta.com/",
      "offset": 1850.159,
      "duration": 7.361
    },
    {
      "lang": "en",
      "text": "a kh for $1,000 off. Today's episode is",
      "offset": 1853.279,
      "duration": 6.801
    },
    {
      "lang": "en",
      "text": "brought to you by the AIPM certification",
      "offset": 1857.52,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "on Maven run by McDad Jaffer, who is a",
      "offset": 1860.08,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "product leader at OpenAI. This is not",
      "offset": 1862.72,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "your typical course. It's 8 weeks of",
      "offset": 1864.72,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "live cohort-based learning with the",
      "offset": 1866.799,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "leader at one of the top companies in",
      "offset": 1868.88,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "tech. OpenAI just doesn't stop shipping,",
      "offset": 1870.559,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "and this is your chance to learn how.",
      "offset": 1872.799,
      "duration": 3.521
    },
    {
      "lang": "en",
      "text": "run along with product faculty and Mo",
      "offset": 1874.32,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "Ali. The course has a 4.9 rating with",
      "offset": 1876.32,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "133 reviews. Former students come from",
      "offset": 1879.2,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "companies like OpenAI, Shopify, Stripe,",
      "offset": 1881.44,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "Google, and Meta. The best part, your",
      "offset": 1884,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "company can probably cover the cost. So,",
      "offset": 1886.08,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "if you want to get $500 off, use my code",
      "offset": 1888.159,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "a AA25",
      "offset": 1890.96,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "and head to maven.com/roduct-faculty.",
      "offset": 1893.279,
      "duration": 6.28
    },
    {
      "lang": "en",
      "text": "That's mavn.com/pect-fac.",
      "offset": 1896.32,
      "duration": 3.239
    },
    {
      "lang": "en",
      "text": "You mentioned hill climbing and I think",
      "offset": 1902.88,
      "duration": 2.799
    },
    {
      "lang": "en",
      "text": "that's a really interesting concept",
      "offset": 1904.32,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "because I was reading Daniel McKinnon's",
      "offset": 1905.679,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "piece on eval. He is a PM on Meta's",
      "offset": 1907.84,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "llama team and he talked about how it's",
      "offset": 1910.48,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "so important for the PM to define the",
      "offset": 1913.2,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "evals so that then the AI engineers and",
      "offset": 1915.2,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "research teams that he works with can",
      "offset": 1918.399,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "hill climb. Can you guys explain that?",
      "offset": 1920.399,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "Engineers are really good at hill",
      "offset": 1923.919,
      "duration": 5.521
    },
    {
      "lang": "en",
      "text": "climbing and especially ML AI data",
      "offset": 1925.519,
      "duration": 6.64
    },
    {
      "lang": "en",
      "text": "science stats people people who are",
      "offset": 1929.44,
      "duration": 7.599
    },
    {
      "lang": "en",
      "text": "trained to go and look at you know ML",
      "offset": 1932.159,
      "duration": 7.36
    },
    {
      "lang": "en",
      "text": "metrics figure out why they're bad and",
      "offset": 1937.039,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "how to improve them.",
      "offset": 1939.519,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "They were good at that in structured",
      "offset": 1941.679,
      "duration": 2.561
    },
    {
      "lang": "en",
      "text": "data in traditional machine learning",
      "offset": 1942.88,
      "duration": 2.799
    },
    {
      "lang": "en",
      "text": "because those metrics are very well",
      "offset": 1944.24,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "defined like accuracy is well defined",
      "offset": 1945.679,
      "duration": 3.521
    },
    {
      "lang": "en",
      "text": "loss is well defined. And if you're like",
      "offset": 1947.6,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "doing a binary classifier and you",
      "offset": 1949.2,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "already have label data, you know, it's",
      "offset": 1951.44,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "all there for you. You're just doing",
      "offset": 1953.44,
      "duration": 2.959
    },
    {
      "lang": "en",
      "text": "your job and trying to improve on those",
      "offset": 1954.96,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "metrics. Now, we're in a role in which",
      "offset": 1956.399,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "none of the metrics are defined. And so,",
      "offset": 1958.96,
      "duration": 4.959
    },
    {
      "lang": "en",
      "text": "there's no way to go and exercise all",
      "offset": 1961.2,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "the skills that the AI engineers have to",
      "offset": 1963.919,
      "duration": 5.281
    },
    {
      "lang": "en",
      "text": "improve those metrics. I think it is so",
      "offset": 1966.96,
      "duration": 6.079
    },
    {
      "lang": "en",
      "text": "spot-on that this article talks about,",
      "offset": 1969.2,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "you know, PMs need to set these metrics",
      "offset": 1973.039,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "because they have the best context to",
      "offset": 1974.88,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "set them. And once you set them once you",
      "offset": 1976.399,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "provide the definition or you know",
      "offset": 1978.64,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "examples of good and bad then engineers",
      "offset": 1980.96,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "will figure out a way to encode this",
      "offset": 1983.679,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "into the product and improve even get to",
      "offset": 1985.44,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "self-improving improving products like",
      "offset": 1988.399,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "that is exactly what I see also in my",
      "offset": 1990.159,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "experience",
      "offset": 1992.399,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "one thing you have to be careful with",
      "offset": 1994.399,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "with hill climbing and when that term is",
      "offset": 1995.679,
      "duration": 5.761
    },
    {
      "lang": "en",
      "text": "said it does tickle my brain is make",
      "offset": 1998.96,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "sure that people don't overfit because",
      "offset": 2001.44,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "you know engineers sometimes they use",
      "offset": 2003.44,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "that term and they are overfitting in",
      "offset": 2005.039,
      "duration": 6.321
    },
    {
      "lang": "en",
      "text": "some sense. Um you if they're not if you",
      "offset": 2008.48,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "know machine learning people know, data",
      "offset": 2011.36,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "science people know but it is a it is a",
      "offset": 2013.279,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "thing.",
      "offset": 2016,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "And what is what does overfitting look",
      "offset": 2017.519,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "like in practice?",
      "offset": 2019.84,
      "duration": 6.4
    },
    {
      "lang": "en",
      "text": "Yeah, overfitting means that you um you",
      "offset": 2022.159,
      "duration": 7.441
    },
    {
      "lang": "en",
      "text": "have hill climbed against some data or",
      "offset": 2026.24,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "something that doesn't generalize. So",
      "offset": 2029.6,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "the key phrases does not generalize",
      "offset": 2031.2,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "and um you know maybe you have some",
      "offset": 2034.32,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "eval. So like a one very trivial way to",
      "offset": 2037.2,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "mess this up and this is from a real",
      "offset": 2040.32,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "client is you have some uh you have a",
      "offset": 2042.08,
      "duration": 6.799
    },
    {
      "lang": "en",
      "text": "test data set of you know cases that you",
      "offset": 2044.88,
      "duration": 7.999
    },
    {
      "lang": "en",
      "text": "want to do well on in your evals and you",
      "offset": 2048.879,
      "duration": 8.081
    },
    {
      "lang": "en",
      "text": "use the same data as fshot examples in",
      "offset": 2052.879,
      "duration": 5.921
    },
    {
      "lang": "en",
      "text": "your prompt.",
      "offset": 2056.96,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "So that is a hilariously",
      "offset": 2058.8,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "kind of straightforward way of",
      "offset": 2061.679,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "overfitting because you have given the",
      "offset": 2063.119,
      "duration": 5.361
    },
    {
      "lang": "en",
      "text": "LLM the answer directly in the prompt.",
      "offset": 2065.04,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "Um and there's you know we can relax",
      "offset": 2068.48,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "that a little bit. There's there's more",
      "offset": 2070.72,
      "duration": 4.959
    },
    {
      "lang": "en",
      "text": "subtle ways that you might do that. Um",
      "offset": 2072.32,
      "duration": 5.279
    },
    {
      "lang": "en",
      "text": "but essentially you know you might",
      "offset": 2075.679,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "overengineer your prompt with those",
      "offset": 2077.599,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "specific cases in mind with with details",
      "offset": 2079.919,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "of those cases that are a little bit too",
      "offset": 2082.48,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "specific. you know, even though you",
      "offset": 2084.879,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "don't have the exact like information",
      "offset": 2087.119,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "there, you know, that might lead to",
      "offset": 2089.599,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "overfitting. Um, and there's many ways",
      "offset": 2091.599,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "you can overfit. And what we Shan and I",
      "offset": 2093.839,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "teach is well, how do you know that you",
      "offset": 2096.56,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "overfit? How can you guard against it?",
      "offset": 2098.24,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "It turns out you can use a lot of the",
      "offset": 2100.48,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "same techniques from machine learning to",
      "offset": 2102.72,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "kind of give yourself a early warning",
      "offset": 2104.96,
      "duration": 6.96
    },
    {
      "lang": "en",
      "text": "system to say, \"Hey, like you overfit.\"",
      "offset": 2108,
      "duration": 6.88
    },
    {
      "lang": "en",
      "text": "What are some of those techniques?",
      "offset": 2111.92,
      "duration": 5.679
    },
    {
      "lang": "en",
      "text": "Uh number one thing is collect label",
      "offset": 2114.88,
      "duration": 6.8
    },
    {
      "lang": "en",
      "text": "data. Reserve some of it for testing as",
      "offset": 2117.599,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "Hamill kind of alluded to. You know, you",
      "offset": 2121.68,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "want to have some test cases that you",
      "offset": 2123.839,
      "duration": 2.801
    },
    {
      "lang": "en",
      "text": "want to try out your product to see if",
      "offset": 2125.28,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "it performs well on. You know, reserve",
      "offset": 2126.64,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "that set and never never look at it.",
      "offset": 2129.119,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "Never look at it when you're developing",
      "offset": 2131.92,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "your product, when you're developing",
      "offset": 2133.44,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "your prompts, even when you're kind of",
      "offset": 2134.96,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "trying to test out your prompts to",
      "offset": 2137.359,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "figure out how to improve it. Just make",
      "offset": 2138.72,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "sure that the test data is like in the",
      "offset": 2141.119,
      "duration": 5.281
    },
    {
      "lang": "en",
      "text": "sandbox that you never never see.",
      "offset": 2143.04,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "That's",
      "offset": 2146.4,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "the number one strategy that I tell",
      "offset": 2148.32,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "people. Um, another thing that I tell",
      "offset": 2150.8,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "people and PMs should also know this is",
      "offset": 2153.28,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "anytime you see a metric that is",
      "offset": 2155.92,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "suspiciously too high like I achieved",
      "offset": 2158.4,
      "duration": 7.199
    },
    {
      "lang": "en",
      "text": "99% alignment or 95 or 100 or whatever,",
      "offset": 2160.72,
      "duration": 7.52
    },
    {
      "lang": "en",
      "text": "immediate smell warning flags that",
      "offset": 2165.599,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "something is off. we're leaking some",
      "offset": 2168.24,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "data into our prompts from the test set.",
      "offset": 2170.16,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "Maybe maybe our test set doesn't have",
      "offset": 2172.48,
      "duration": 5.119
    },
    {
      "lang": "en",
      "text": "enough examples. Um so go back and",
      "offset": 2174.56,
      "duration": 5.279
    },
    {
      "lang": "en",
      "text": "question the process if you see any",
      "offset": 2177.599,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "numbers that are too high.",
      "offset": 2179.839,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "That's absolutely true. One of my",
      "offset": 2181.52,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "favorite examples is there's um",
      "offset": 2183.68,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "there is a interview that's been going",
      "offset": 2187.04,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "around for a while now. It's from a",
      "offset": 2189.44,
      "duration": 5.679
    },
    {
      "lang": "en",
      "text": "company called Caseex who's a you know",
      "offset": 2191.839,
      "duration": 6.321
    },
    {
      "lang": "en",
      "text": "was bought by I believe uh Thompson",
      "offset": 2195.119,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "Routers or maybe Lexus Nexus one of the",
      "offset": 2198.16,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "two big ones uh legal companies and the",
      "offset": 2200.079,
      "duration": 7.04
    },
    {
      "lang": "en",
      "text": "CEO went on the you know and said hey",
      "offset": 2203.44,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "like you need to keep iterating on your",
      "offset": 2207.119,
      "duration": 5.441
    },
    {
      "lang": "en",
      "text": "evals until you get to 100% accuracy",
      "offset": 2208.96,
      "duration": 8
    },
    {
      "lang": "en",
      "text": "and so um you know if you're iterating",
      "offset": 2212.56,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "if you're getting 100% accuracy in your",
      "offset": 2216.96,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "evals it's likely your evals are",
      "offset": 2218.56,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "worthless. They because they are",
      "offset": 2220.96,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "providing you with no signal. Just think",
      "offset": 2223.76,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "about anything else. If every every",
      "offset": 2225.68,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "student in your class is getting a",
      "offset": 2228.16,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "perfect score, is it a good test?",
      "offset": 2229.68,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "You know, any kind of test, if everybody",
      "offset": 2232.64,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "is passing it, if everything is passing,",
      "offset": 2235.44,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "should you be celebrating? No. It means",
      "offset": 2237.76,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "the test doesn't have is not",
      "offset": 2240.24,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "differentiating good and bad, and it",
      "offset": 2242.16,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "probably is not really worth it. And so",
      "offset": 2244.16,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "um it's really good you know you have to",
      "offset": 2247.28,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "keep that in mind",
      "offset": 2249.359,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "you want your test bit of nuance to it",
      "offset": 2251.44,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "you want some tests that are you know",
      "offset": 2254.56,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "basic regression tests or functionality",
      "offset": 2256.48,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "tests that absolutely you want to pass",
      "offset": 2259.04,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "them because otherwise you're shipping a",
      "offset": 2260.72,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "broken product but I think the broader",
      "offset": 2262.16,
      "duration": 5.919
    },
    {
      "lang": "en",
      "text": "point HML and I want to make is you also",
      "offset": 2264.32,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "should have some aspirational",
      "offset": 2268.079,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "eals um and those you know it doesn't",
      "offset": 2270.24,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "need to be 0% or like designed",
      "offset": 2273.28,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "adversarial really but some benchmark",
      "offset": 2275.44,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "that helps you know whether you know the",
      "offset": 2278.48,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "general AI or intelligence in your",
      "offset": 2280.48,
      "duration": 6.84
    },
    {
      "lang": "en",
      "text": "system is getting better.",
      "offset": 2283.44,
      "duration": 3.88
    },
    {
      "lang": "en",
      "text": "In talking to so many AI companies about",
      "offset": 2288.16,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "evals, what's one AI company that sticks",
      "offset": 2290.72,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "out to you as doing it particularly well",
      "offset": 2293.76,
      "duration": 6.359
    },
    {
      "lang": "en",
      "text": "and why?",
      "offset": 2296.8,
      "duration": 3.319
    },
    {
      "lang": "en",
      "text": "Everyone's struggling.",
      "offset": 2302.24,
      "duration": 3.56
    },
    {
      "lang": "en",
      "text": "Okay, I'll I'll talk about some design",
      "offset": 2316.56,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "principles that I see people converging",
      "offset": 2319.52,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "on that are solving parts of their",
      "offset": 2321.359,
      "duration": 2.961
    },
    {
      "lang": "en",
      "text": "problem. So So I don't think anybody is",
      "offset": 2322.72,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "doing it perfectly. I mean, otherwise",
      "offset": 2324.32,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "you would have like a rocket ship AI",
      "offset": 2325.92,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "startup that's like already perfect and",
      "offset": 2328,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "like gone public and everything works.",
      "offset": 2330.48,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "We just don't have that, right? It's",
      "offset": 2332.72,
      "duration": 2.56
    },
    {
      "lang": "en",
      "text": "it's not there. So, like, let's be",
      "offset": 2334,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "honest. Um, some of the things that I've",
      "offset": 2335.28,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "been seeing teams that really seem to",
      "offset": 2337.92,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "give them leverage are one building",
      "offset": 2340.48,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "custom data labeling and annotation",
      "offset": 2343.52,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "interfaces",
      "offset": 2345.92,
      "duration": 6.4
    },
    {
      "lang": "en",
      "text": "um or providing ways for domain experts",
      "offset": 2347.52,
      "duration": 6.88
    },
    {
      "lang": "en",
      "text": "or other people that they trust to be",
      "offset": 2352.32,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "able to give feedback on traces. Um, a",
      "offset": 2354.4,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "lot of the observability tools for LLMs",
      "offset": 2357.119,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "have started to build in these features.",
      "offset": 2359.92,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "They didn't have them even in the last",
      "offset": 2361.52,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "like 2 months, 3 months. Um, so it's",
      "offset": 2363.04,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "going to take some time until we can",
      "offset": 2365.2,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "really see the effect of that. Um,",
      "offset": 2366.4,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "another thing that I've been seeing is",
      "offset": 2368.96,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "these really well scoped LLM judges",
      "offset": 2370.32,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "that are being deployed as part of",
      "offset": 2374.48,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "products. Um, Claude Anthropic just had",
      "offset": 2376,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "a new post on their research agent about",
      "offset": 2379.52,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "how, you know, they really break down",
      "offset": 2382.16,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "the complex task into a multi-agent",
      "offset": 2383.92,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "system and then critically they have a",
      "offset": 2386,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "bunch of LLM judges that are very very",
      "offset": 2388.32,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "well scoped to each task much like we",
      "offset": 2390.4,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "teach in our course. Um, and I'm sure",
      "offset": 2392.56,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "they're making a bunch of revenue off of",
      "offset": 2394.4,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "this product. Um, and then of course I",
      "offset": 2396.48,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "think there's a lot around, you know,",
      "offset": 2399.599,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "cursor famously, you know, very",
      "offset": 2401.28,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "dutifully measures your, you know, next",
      "offset": 2403.44,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "token prediction. They have their own",
      "offset": 2405.28,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "models for this. Um, I think next token",
      "offset": 2407.44,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "prediction for coding is interesting",
      "offset": 2410.16,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "because that metric is very well",
      "offset": 2412,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "defined. People know that it's useful.",
      "offset": 2413.52,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "It directly correlates with product",
      "offset": 2415.2,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "success. Not all products have such a",
      "offset": 2416.8,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "metric like that. Um, PMs need to really",
      "offset": 2418.96,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "go figure that out. But these kinds of",
      "offset": 2421.04,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "things I think are broadly helpful",
      "offset": 2423.04,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "across the board that I've been seeing.",
      "offset": 2424.8,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "But I think we're pretty far out from",
      "offset": 2426.88,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "like you know the product that's perfect",
      "offset": 2428.4,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "right now.",
      "offset": 2429.92,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "What is next token prediction?",
      "offset": 2431.599,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "Uh like in when you're writing code like",
      "offset": 2434.24,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "the next snippet um that you're going to",
      "offset": 2436.56,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "write in your code. If the AI model",
      "offset": 2439.28,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "predicted that correctly, then the",
      "offset": 2441.2,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "metric goes up. If not, metric goes",
      "offset": 2443.599,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "down.",
      "offset": 2445.52,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "Kind of like an advanced autocomplete.",
      "offset": 2446.8,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "Yeah.",
      "offset": 2449.359,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "Why haven't we seen better autocomplete",
      "offset": 2450.96,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "in like regular products like email and",
      "offset": 2453.359,
      "duration": 5.361
    },
    {
      "lang": "en",
      "text": "text message and things like that?",
      "offset": 2456.72,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "It's incredibly frustrating. Um if",
      "offset": 2458.72,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "especially if you're writing code, if",
      "offset": 2460.96,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "you're a programmer using cursor, Shrea",
      "offset": 2463.28,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "is using cursor I know like six hours a",
      "offset": 2465.28,
      "duration": 6.4
    },
    {
      "lang": "en",
      "text": "day. Um, and you know, the developer",
      "offset": 2468.48,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "tools are very far ahead of the other",
      "offset": 2471.68,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "tools. And it's incredibly frustrating",
      "offset": 2474,
      "duration": 5.359
    },
    {
      "lang": "en",
      "text": "if you try to use AI in Gmail or in",
      "offset": 2476.48,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "Google Docs or in PowerPoint or",
      "offset": 2479.359,
      "duration": 3.521
    },
    {
      "lang": "en",
      "text": "anything. And it's like even more",
      "offset": 2481.2,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "frustrating cuz then you open LinkedIn",
      "offset": 2482.88,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "or whatever and it's like, do we have AI",
      "offset": 2484.64,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "and everything? And we're like, no, you",
      "offset": 2487.359,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "don't. Like, you barely do. Um, and um,",
      "offset": 2489.839,
      "duration": 7.76
    },
    {
      "lang": "en",
      "text": "yeah, I'm not really sure why. Um",
      "offset": 2493.599,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "I have some hypothesis.",
      "offset": 2497.599,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "Yeah. So it goes back to Haml's comment",
      "offset": 2499.839,
      "duration": 5.121
    },
    {
      "lang": "en",
      "text": "on verifiable domains.",
      "offset": 2502.56,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "Code is a verifiable domain to some",
      "offset": 2504.96,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "extent, right? You can just make sure",
      "offset": 2507.68,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "the code runs. Like that's already",
      "offset": 2509.04,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "great. The other thing is there's a",
      "offset": 2511.28,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "bunch of data of how people write code",
      "offset": 2513.119,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "that's already there and available on",
      "offset": 2516.24,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "the internet in ways that you know we",
      "offset": 2517.839,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "don't have that for emails. Um emails",
      "offset": 2519.76,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "are not very verifiable, right? like",
      "offset": 2522,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "nobody is telling you like this is",
      "offset": 2524.4,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "ground truth like this is correct or",
      "offset": 2526.319,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "like this is not correct. Um PMS have to",
      "offset": 2528.8,
      "duration": 5.279
    },
    {
      "lang": "en",
      "text": "do this job of figuring out how to",
      "offset": 2532.079,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "design a system around good and bad",
      "offset": 2534.079,
      "duration": 3.121
    },
    {
      "lang": "en",
      "text": "emails.",
      "offset": 2536.24,
      "duration": 2.64
    },
    {
      "lang": "en",
      "text": "Um and that's where it becomes really",
      "offset": 2537.2,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "really bespoke and I think the",
      "offset": 2538.88,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "that it goes also back to Haml's comment",
      "offset": 2540.96,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "around code where it's like developers",
      "offset": 2542.8,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "knew what the eval so they were able to",
      "offset": 2544.64,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "implement that. Um, but when like this",
      "offset": 2547.119,
      "duration": 5.841
    },
    {
      "lang": "en",
      "text": "is why we need AIPMs, we need people to",
      "offset": 2550.56,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "be able to help developers craft these",
      "offset": 2552.96,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "verifiable or even like loosely",
      "offset": 2556.24,
      "duration": 5.4
    },
    {
      "lang": "en",
      "text": "verifiable signals.",
      "offset": 2557.92,
      "duration": 3.72
    },
    {
      "lang": "en",
      "text": "I know you guys have spent some time",
      "offset": 2561.92,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "with uh Open AI. How is OpenAI",
      "offset": 2563.599,
      "duration": 5.361
    },
    {
      "lang": "en",
      "text": "approaching evals and what can we learn",
      "offset": 2567.119,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "from them?",
      "offset": 2568.96,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "So one thing I'll say is um you know",
      "offset": 2571.52,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "there's a really big difference between",
      "offset": 2574.56,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "foundation model benchmarks so MMLU",
      "offset": 2576.4,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "score human eval score these are like",
      "offset": 2579.68,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "general purpose benchmarks which try to",
      "offset": 2582.24,
      "duration": 7.68
    },
    {
      "lang": "en",
      "text": "assess the general capability of models",
      "offset": 2584.56,
      "duration": 7.84
    },
    {
      "lang": "en",
      "text": "and then there's another kind of eval",
      "offset": 2589.92,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "which is your domain specific eval for",
      "offset": 2592.4,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "your business and they are very very",
      "offset": 2594.96,
      "duration": 6.8
    },
    {
      "lang": "en",
      "text": "different Um and it's important that",
      "offset": 2597.68,
      "duration": 8.72
    },
    {
      "lang": "en",
      "text": "people know that um now rightfully so uh",
      "offset": 2601.76,
      "duration": 7.2
    },
    {
      "lang": "en",
      "text": "the foundation model labs like open AAI",
      "offset": 2606.4,
      "duration": 4.959
    },
    {
      "lang": "en",
      "text": "they're very much focused on the former",
      "offset": 2608.96,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "the the general purpose benchmarks",
      "offset": 2611.359,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "because that is you know relevant to",
      "offset": 2612.88,
      "duration": 6.8
    },
    {
      "lang": "en",
      "text": "their product. Um, but I think, you",
      "offset": 2615.44,
      "duration": 6.639
    },
    {
      "lang": "en",
      "text": "know, they're kind of at the same level",
      "offset": 2619.68,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "of playing field as everybody else when",
      "offset": 2622.079,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "it comes to domain specific emails and",
      "offset": 2624.4,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "like how the companies should be",
      "offset": 2626.88,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "defining their emails.",
      "offset": 2628.64,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "The other thing is they're not going to",
      "offset": 2631.2,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "do it right because like your definition",
      "offset": 2632.64,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "of a good email is different from my",
      "offset": 2634.72,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "definition of a good email and we're",
      "offset": 2636.16,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "both building email assistant companies.",
      "offset": 2637.76,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "They're going to be different products",
      "offset": 2639.76,
      "duration": 2.48
    },
    {
      "lang": "en",
      "text": "than they should be because they should",
      "offset": 2640.88,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "reflect our taste and our company's",
      "offset": 2642.24,
      "duration": 5.119
    },
    {
      "lang": "en",
      "text": "vision. um OpenAI just like cannot solve",
      "offset": 2644.48,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "both of them with one model, right? Does",
      "offset": 2647.359,
      "duration": 3.521
    },
    {
      "lang": "en",
      "text": "that doesn't make sense and they're not",
      "offset": 2648.72,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "in that business, right? Like they're",
      "offset": 2650.88,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "trying to make the model that you want",
      "offset": 2652.24,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "to hire, for lack of a better term, to",
      "offset": 2654.24,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "do the job. Um but you need to train,",
      "offset": 2656.72,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "not just not really train in terms of",
      "offset": 2660.16,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "model parameters, but you need to figure",
      "offset": 2661.92,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "out how to make an environment, how to",
      "offset": 2663.68,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "elicit the right signals, how to reject",
      "offset": 2666.56,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "emails that are bad according to your",
      "offset": 2669.68,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "definitions. Like that's all the stuff",
      "offset": 2671.44,
      "duration": 2.879
    },
    {
      "lang": "en",
      "text": "that you've got to build that's domain",
      "offset": 2672.8,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "specific that no foundation model",
      "offset": 2674.319,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "company is going to do for you.",
      "offset": 2676.4,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "Definitely. One example that is top of",
      "offset": 2678.8,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "mind right now is Shrea just wrote this",
      "offset": 2681.2,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "excellent blog post writing in the age",
      "offset": 2683.52,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "of LLMs",
      "offset": 2685.599,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "and you know if I was to so the problem",
      "offset": 2687.28,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "is is like foundation models you know",
      "offset": 2689.28,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "they're trained they have like sets of",
      "offset": 2692.16,
      "duration": 7.439
    },
    {
      "lang": "en",
      "text": "labelers and basically it's the it's by",
      "offset": 2694.64,
      "duration": 7.76
    },
    {
      "lang": "en",
      "text": "definition trained on the average taste",
      "offset": 2699.599,
      "duration": 6.081
    },
    {
      "lang": "en",
      "text": "of some people and you know if you're if",
      "offset": 2702.4,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "for so if you're writing a lot you know",
      "offset": 2705.68,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "like I know you are Gosh, you know, as",
      "offset": 2708.4,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "well is, you know, what comes out of the",
      "offset": 2710.64,
      "duration": 6.32
    },
    {
      "lang": "en",
      "text": "LLM, like almost everybody I know that",
      "offset": 2713.599,
      "duration": 5.601
    },
    {
      "lang": "en",
      "text": "writes a lot kind of hates it. They're",
      "offset": 2716.96,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "like, why can't it be better? Like, why",
      "offset": 2719.2,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "is it giving me like the same like why",
      "offset": 2720.8,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "is it like using so many words and why",
      "offset": 2723.92,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "is it like doing this? Why does it keep",
      "offset": 2726.4,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "using mdash everywhere? Why is it using",
      "offset": 2728.88,
      "duration": 4.959
    },
    {
      "lang": "en",
      "text": "so many emojis? Like, stop it. It's",
      "offset": 2730.88,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "because like you know whoever is",
      "offset": 2733.839,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "labeling the data you know they kind of",
      "offset": 2735.28,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "bias towards like longer explanations",
      "offset": 2738.16,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "and like overusing bullet points and all",
      "offset": 2740.48,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "this stuff. And so like you know if you",
      "offset": 2743.04,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "want to have like a a thing that writers",
      "offset": 2744.96,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "love you know then you have to get",
      "offset": 2748.16,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "someone you know you have to get like",
      "offset": 2750.56,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "perspective like Shrea has and like",
      "offset": 2752.079,
      "duration": 6.561
    },
    {
      "lang": "en",
      "text": "iterate on your product like over time",
      "offset": 2754.48,
      "duration": 7.04
    },
    {
      "lang": "en",
      "text": "and make it do those things that they",
      "offset": 2758.64,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "incorporate the taste that you have you",
      "offset": 2761.52,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "know um and and so that's kind of where",
      "offset": 2764,
      "duration": 7.68
    },
    {
      "lang": "en",
      "text": "it diverges from the foundation models.",
      "offset": 2768,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "So, do those rappers where you're",
      "offset": 2771.68,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "putting in your own taste into the",
      "offset": 2773.92,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "evals, do they actually have some sort",
      "offset": 2775.76,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "of moat where people should consider",
      "offset": 2777.52,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "building those startups?",
      "offset": 2779.359,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "Oh, absolutely. Oh, I think maybe I'm",
      "offset": 2780.96,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "crazy. I think evals are the moat for AI",
      "offset": 2784.079,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "products and like truly nothing else.",
      "offset": 2786.56,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "Like tomorrow, you can use a different",
      "offset": 2788.64,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "model. You can have a different stack",
      "offset": 2789.92,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "for surfing whatever it is. like there",
      "offset": 2792.319,
      "duration": 5.601
    },
    {
      "lang": "en",
      "text": "the secret sauce is your evals and how",
      "offset": 2794.16,
      "duration": 6.4
    },
    {
      "lang": "en",
      "text": "you're able to operationalize or scale",
      "offset": 2797.92,
      "duration": 6.64
    },
    {
      "lang": "en",
      "text": "that out as quickly as possible. Um so",
      "offset": 2800.56,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "that means having good LLM judges for",
      "offset": 2804.56,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "example that are like very well aligned",
      "offset": 2806.64,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "with your preferences that you can just",
      "offset": 2808.4,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "automatically run on everything um and",
      "offset": 2810.56,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "build that flywheel for yourself to like",
      "offset": 2812.88,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "go then look at what failed and improve",
      "offset": 2815.04,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "your product and so forth. And it's not",
      "offset": 2817.04,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "just like it's not just the eval itself.",
      "offset": 2819.839,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "It's the whole system like Shrea is",
      "offset": 2822.8,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "saying around the eval the entire eval",
      "offset": 2824.319,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "pipeline. It basically should be",
      "offset": 2826.72,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "portable. You should be able to switch",
      "offset": 2828.88,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "models or you know switch components and",
      "offset": 2830.8,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "see what the effect of that is. And the",
      "offset": 2833.68,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "key thing is like eval open up a whole",
      "offset": 2836.64,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "lot of doors. So it opens up the door",
      "offset": 2838.8,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "for easy fine-tuning. You know once",
      "offset": 2841.68,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "you've done an eval you've already done",
      "offset": 2844.4,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "99% of the work of fine-tuning.",
      "offset": 2846.079,
      "duration": 5.601
    },
    {
      "lang": "en",
      "text": "um fine-tuning is just a kind of a",
      "offset": 2849.28,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "formality that you can go through after",
      "offset": 2851.68,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "that almost to to an extent because",
      "offset": 2853.599,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "you've already done you already have all",
      "offset": 2855.28,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "the tools for data curation you know how",
      "offset": 2856.96,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "to measure things you know how to like",
      "offset": 2859.04,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "see measure the effects of the",
      "offset": 2861.44,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "fine-tuning so on and so forth um you",
      "offset": 2862.88,
      "duration": 6.479
    },
    {
      "lang": "en",
      "text": "know and I suspect that that will become",
      "offset": 2866.319,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "that just accelerates your advantage",
      "offset": 2869.359,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "like that much more",
      "offset": 2871.52,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "it's kind of funny because I see people",
      "offset": 2874.319,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "tend to focus on fine-tuning",
      "offset": 2876.16,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "a lot more and there's a lot more",
      "offset": 2878.4,
      "duration": 2.959
    },
    {
      "lang": "en",
      "text": "attention given to that. I think it's",
      "offset": 2879.92,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "because like in the open AI and",
      "offset": 2881.359,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "enthropic developer docs for when they",
      "offset": 2883.839,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "give you a model, they're like, \"Yeah,",
      "offset": 2886.24,
      "duration": 2.879
    },
    {
      "lang": "en",
      "text": "this is how you can fine-tune it and",
      "offset": 2887.52,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "things like that.\" How does fine-tuning",
      "offset": 2889.119,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "really fit in with the overall life",
      "offset": 2891.76,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "cycle? When should you be putting",
      "offset": 2893.44,
      "duration": 5.56
    },
    {
      "lang": "en",
      "text": "attention into fine-tuning?",
      "offset": 2895.28,
      "duration": 3.72
    },
    {
      "lang": "en",
      "text": "We we should put it in last. So, you",
      "offset": 2899.359,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "should do evals first because like what",
      "offset": 2901.44,
      "duration": 3.119
    },
    {
      "lang": "en",
      "text": "are you",
      "offset": 2903.28,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "you have to know if you're fine? Yeah.",
      "offset": 2904.559,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "Oh, sorry. Maybe my connection is bad.",
      "offset": 2909.2,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "There's like lag here.",
      "offset": 2911.68,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "Um, yeah, you need to know this. This",
      "offset": 2913.44,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "goes back to you want to iterate on your",
      "offset": 2916.319,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "product. You want to know, okay, my",
      "offset": 2917.839,
      "duration": 2.801
    },
    {
      "lang": "en",
      "text": "product is not good, so I'm going to",
      "offset": 2919.52,
      "duration": 3.039
    },
    {
      "lang": "en",
      "text": "improve it. You don't have any way of",
      "offset": 2920.64,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "knowing if your product is not good",
      "offset": 2922.559,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "unless you have eval. So, eval zero,",
      "offset": 2924.079,
      "duration": 6.401
    },
    {
      "lang": "en",
      "text": "quantify your performance. Then step one",
      "offset": 2927.2,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "is okay, how am I going to improve its",
      "offset": 2930.48,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "weak points? Some lowhanging fruit ways",
      "offset": 2933.2,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "of improving are just use a more",
      "offset": 2936.24,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "powerful model. Switch from GBT40 mini",
      "offset": 2937.92,
      "duration": 5.679
    },
    {
      "lang": "en",
      "text": "to GPT40. See if that works. Because you",
      "offset": 2940.48,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "have good evals, right? You can just",
      "offset": 2943.599,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "make that switch, run it, see if your",
      "offset": 2945.2,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "numbers go up. There are other more",
      "offset": 2947.2,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "complicated strategies you can do like",
      "offset": 2949.28,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "take a complex LLM call, a complex task",
      "offset": 2951.359,
      "duration": 5.361
    },
    {
      "lang": "en",
      "text": "described in them and break them down",
      "offset": 2954.48,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "into multiple LLM calls. So instead of",
      "offset": 2956.72,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "you know having an LLM extracting 10",
      "offset": 2959.2,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "things from uh this document I'm going",
      "offset": 2961.44,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "to have one LLM call each extracting one",
      "offset": 2964,
      "duration": 6.88
    },
    {
      "lang": "en",
      "text": "of those 10 things. Um so like that you",
      "offset": 2967.2,
      "duration": 5.359
    },
    {
      "lang": "en",
      "text": "can kind of do task that's what we call",
      "offset": 2970.88,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "task decomposition. When you exhaust",
      "offset": 2972.559,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "these strategies then it makes sense to",
      "offset": 2974.96,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "move into fine-tuning where it's like I",
      "offset": 2977.839,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "have no other way of solving my problem.",
      "offset": 2980.24,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "The model's just not there yet. I'm",
      "offset": 2983.04,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "going to collect a bunch of data or use",
      "offset": 2985.28,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "the data I've already labeled and",
      "offset": 2987.28,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "fine-tune a model. Fine-tuning has cons",
      "offset": 2988.64,
      "duration": 6.64
    },
    {
      "lang": "en",
      "text": "because now you have to make sure that",
      "offset": 2992.079,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "you're continually fine-tuning that",
      "offset": 2995.28,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "model as you get new data. You learn",
      "offset": 2997.28,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "something new about your preferences of",
      "offset": 3001.04,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "your customers. Um, they don't just",
      "offset": 3002.8,
      "duration": 5.279
    },
    {
      "lang": "en",
      "text": "automatically update if the base model",
      "offset": 3005.359,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "updates, right? GPT 40 or 50 or whatever",
      "offset": 3008.079,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "comes out. Now you have to redo your all",
      "offset": 3011.599,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "fine-tuning all over again. And then if",
      "offset": 3013.359,
      "duration": 5.441
    },
    {
      "lang": "en",
      "text": "you don't use a model provider's",
      "offset": 3016.319,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "fine-tuning service, like you're",
      "offset": 3018.8,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "fine-tuning an open-source model, you",
      "offset": 3020.16,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "need to deal with the MLOps complexity",
      "offset": 3022.4,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "of serving that, making sure it has good",
      "offset": 3024.319,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "uptime. Latency is loan low. This is all",
      "offset": 3026.48,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "actually pretty complicated, right?",
      "offset": 3029.04,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "Costs a lot to maintain this",
      "offset": 3030.88,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "infrastructure both in human personnel",
      "offset": 3032.64,
      "duration": 4.959
    },
    {
      "lang": "en",
      "text": "as well as in money. Um, and and using",
      "offset": 3034.8,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "an off-the-shelf LLM is like pretty much",
      "offset": 3037.599,
      "duration": 5.041
    },
    {
      "lang": "en",
      "text": "the right way to go in most people's use",
      "offset": 3039.839,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "cases. Um, so for that reason, for those",
      "offset": 3042.64,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "reasons, we really try to not encourage",
      "offset": 3045.599,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "people to do fine-tuning unless you",
      "offset": 3047.92,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "really really have a good reason for",
      "offset": 3050.48,
      "duration": 5.119
    },
    {
      "lang": "en",
      "text": "doing your own fine-tuning.",
      "offset": 3052.16,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "One example where I might go to",
      "offset": 3055.599,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "fine-tuning hilariously is this writing",
      "offset": 3057.359,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "thing. Cuz like Sha, we we talk about it",
      "offset": 3060,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "a lot like writing, writing, we bash our",
      "offset": 3061.839,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "head against the wall. even right before",
      "offset": 3063.52,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "this call was like, \"Oh, like do you try",
      "offset": 3065.44,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "4.1?\" Industry made good comments like,",
      "offset": 3066.96,
      "duration": 5.119
    },
    {
      "lang": "en",
      "text": "\"No, like you know, you can't prompt it",
      "offset": 3069.68,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "with all these rules. It's not going to",
      "offset": 3072.079,
      "duration": 2.24
    },
    {
      "lang": "en",
      "text": "follow these rules.\" So, I'm like,",
      "offset": 3073.44,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "\"Okay, there's no this really feels like",
      "offset": 3074.319,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "it's only one avenue left here.\" Um, you",
      "offset": 3076.24,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "know, we have to put a lot of work into",
      "offset": 3078.96,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "it, but it's like if you wanted Shrea",
      "offset": 3080.16,
      "duration": 4.959
    },
    {
      "lang": "en",
      "text": "GPT, that feels like the only way to get",
      "offset": 3081.76,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "it.",
      "offset": 3085.119,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "Fine-tuning is always talked in the same",
      "offset": 3087.2,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "conversation as prompt engineering and",
      "offset": 3089.839,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "rag. So when do you really think about",
      "offset": 3091.839,
      "duration": 6.76
    },
    {
      "lang": "en",
      "text": "using each of those three techniques?",
      "offset": 3094.48,
      "duration": 4.119
    },
    {
      "lang": "en",
      "text": "Um okay so like yeah prompt engineering",
      "offset": 3098.96,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "is basically you know everybody that's",
      "offset": 3101.359,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "is basic you know you're using the LM",
      "offset": 3104.64,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "you have to communicate what you want to",
      "offset": 3107.599,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "it you have to specify what you want in",
      "offset": 3109.04,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "whatever way so you should always be",
      "offset": 3111.359,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "writing prompts you don't even need to",
      "offset": 3113.359,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "wor put word engineering at the end we",
      "offset": 3115.04,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "are kind of you know adding a lot of",
      "offset": 3117.68,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "ceremony um you know we're just you're",
      "offset": 3119.52,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "writing you know write English or you",
      "offset": 3122.319,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "know in some cases you might you can",
      "offset": 3124.96,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "write other languages too but it is",
      "offset": 3126.559,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "mostly English um you know and refine",
      "offset": 3128,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "your thinking and refine your",
      "offset": 3130.48,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "instructions rag is you know anytime",
      "offset": 3132.48,
      "duration": 7.68
    },
    {
      "lang": "en",
      "text": "you're you need external context",
      "offset": 3135.359,
      "duration": 8.081
    },
    {
      "lang": "en",
      "text": "um which there's a very narrow set of",
      "offset": 3140.16,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "use cases where you don't need external",
      "offset": 3143.44,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "context most of the time you do need in",
      "offset": 3144.96,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "many applications you need some external",
      "offset": 3147.04,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "context",
      "offset": 3148.96,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "um you know that's not some general",
      "offset": 3150.72,
      "duration": 6.32
    },
    {
      "lang": "en",
      "text": "knowledge of the world like and so um",
      "offset": 3152.88,
      "duration": 6.479
    },
    {
      "lang": "en",
      "text": "that's when you need rack, you know, and",
      "offset": 3157.04,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "you're not trying to bake all that",
      "offset": 3159.359,
      "duration": 5.281
    },
    {
      "lang": "en",
      "text": "external context into your prompt.",
      "offset": 3160.72,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "Um",
      "offset": 3164.64,
      "duration": 4.959
    },
    {
      "lang": "en",
      "text": "and then yeah, fine-tuning is sort of",
      "offset": 3166.72,
      "duration": 6.8
    },
    {
      "lang": "en",
      "text": "hey like if you can't",
      "offset": 3169.599,
      "duration": 6.561
    },
    {
      "lang": "en",
      "text": "prompt this behavior",
      "offset": 3173.52,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "if the if the model is not doing what",
      "offset": 3176.16,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "you want, Sha has a has created this",
      "offset": 3178,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "like very powerful um model. It's called",
      "offset": 3180.4,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "the three gulfs. Maybe we should bring",
      "offset": 3183.76,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "it up on the screen actually.",
      "offset": 3185.76,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "Yeah, this is the three golfs question.",
      "offset": 3188,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "Yeah, we should dive into the three",
      "offset": 3191.76,
      "duration": 5.16
    },
    {
      "lang": "en",
      "text": "golfs.",
      "offset": 3193.92,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "I think they're in lesson one.",
      "offset": 3197.44,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "Okay.",
      "offset": 3199.92,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "Or great, you have them.",
      "offset": 3200.96,
      "duration": 7.879
    },
    {
      "lang": "en",
      "text": "Oh, you made your own gulfs. Nice.",
      "offset": 3204.559,
      "duration": 4.28
    },
    {
      "lang": "en",
      "text": "Yeah, I I can take a stab. So your",
      "offset": 3211.04,
      "duration": 3.519
    },
    {
      "lang": "en",
      "text": "question was, you know, you have all",
      "offset": 3213.119,
      "duration": 3.521
    },
    {
      "lang": "en",
      "text": "these tools available to you for",
      "offset": 3214.559,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "improving AI products. You have prompt",
      "offset": 3216.64,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "engineering, you have rag, you have",
      "offset": 3218.72,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "fine-tuning. Are they the same? Are they",
      "offset": 3220.8,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "the different? When do I use them? Blah",
      "offset": 3222.88,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "blah blah. This is a good question that",
      "offset": 3224.48,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "trips people up a lot because people are",
      "offset": 3226.48,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "just thinking of them all equally as",
      "offset": 3228.88,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "ways to improve their product. They are",
      "offset": 3231.44,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "all complimentary strategies. You can do",
      "offset": 3233.599,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "multiple of them, you can do none of",
      "offset": 3235.839,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "them, you can do one of them, whatever",
      "offset": 3237.28,
      "duration": 6.88
    },
    {
      "lang": "en",
      "text": "it is. Prompting is very good when you",
      "offset": 3239.119,
      "duration": 7.2
    },
    {
      "lang": "en",
      "text": "have specific requirements that your",
      "offset": 3244.16,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "task follows and you need to communicate",
      "offset": 3246.319,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "that to the LLM, right? So, say you were",
      "offset": 3248.4,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "to hire some human to do some job or",
      "offset": 3250.64,
      "duration": 4.959
    },
    {
      "lang": "en",
      "text": "contract some job for your company. You",
      "offset": 3252.96,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "might give them a task specification.",
      "offset": 3255.599,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "Say do this then do step A then step B",
      "offset": 3258.64,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "then step C. Make sure your output",
      "offset": 3261.839,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "follows these requirements. Okay, that's",
      "offset": 3263.76,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "that's prompting. you know, there's no",
      "offset": 3265.839,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "almost feels like there's no ceiling",
      "offset": 3268.88,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "sometimes to how much value you can get",
      "offset": 3270.24,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "from good prompting. And this is the",
      "offset": 3272.64,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "first thing that you should be doing.",
      "offset": 3274,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "And this solves this gulf of",
      "offset": 3275.68,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "specification problem that Haml and I",
      "offset": 3277.359,
      "duration": 5.681
    },
    {
      "lang": "en",
      "text": "see a lot um in how people build LLM",
      "offset": 3279.359,
      "duration": 5.361
    },
    {
      "lang": "en",
      "text": "products, which is, you know, they have",
      "offset": 3283.04,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "some latent this hidden criteria, this",
      "offset": 3284.72,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "hidden task that they want the LLM to",
      "offset": 3286.96,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "do, but they just don't know how to",
      "offset": 3289.2,
      "duration": 5.919
    },
    {
      "lang": "en",
      "text": "specify it well in a clear way and",
      "offset": 3291.119,
      "duration": 5.921
    },
    {
      "lang": "en",
      "text": "completely. So that LLM fully",
      "offset": 3295.119,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "understands all of those hidden",
      "offset": 3297.04,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "preferences.",
      "offset": 3298.48,
      "duration": 5.359
    },
    {
      "lang": "en",
      "text": "Now fine-tuning rag strategies",
      "offset": 3300.16,
      "duration": 6.56
    },
    {
      "lang": "en",
      "text": "are for this gulf of generalization",
      "offset": 3303.839,
      "duration": 5.121
    },
    {
      "lang": "en",
      "text": "which targets this problem of I have a",
      "offset": 3306.72,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "very good specification",
      "offset": 3308.96,
      "duration": 6.159
    },
    {
      "lang": "en",
      "text": "but the model is simply not good enough",
      "offset": 3311.599,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "for some reason. Maybe the model is not",
      "offset": 3315.119,
      "duration": 5.041
    },
    {
      "lang": "en",
      "text": "powerful enough so I should do",
      "offset": 3318.079,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "fine-tuning.",
      "offset": 3320.16,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "Maybe the model just doesn't have all of",
      "offset": 3321.839,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "the context",
      "offset": 3324.48,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "that it needs to make the decision.",
      "offset": 3326.559,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "That's where rag comes in. I need to",
      "offset": 3329.359,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "pull in external data sources to improve",
      "offset": 3331.2,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "my product.",
      "offset": 3333.359,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "But yeah,",
      "offset": 3335.2,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "very helpful.",
      "offset": 3337.52,
      "duration": 2.799
    },
    {
      "lang": "en",
      "text": "And I will submit to you, I know that",
      "offset": 3338.559,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "you ask this question a lot in your",
      "offset": 3340.319,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "podcast because I think it's confusing.",
      "offset": 3342.4,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "Rightfully so, you asked this question.",
      "offset": 3344.24,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "I would say this might be a good",
      "offset": 3345.76,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "framework for answering that question.",
      "offset": 3349.2,
      "duration": 6.96
    },
    {
      "lang": "en",
      "text": "Um, you know, since it comes up a lot.",
      "offset": 3351.68,
      "duration": 6.32
    },
    {
      "lang": "en",
      "text": "Yeah, I try to play the role of the the",
      "offset": 3356.16,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "watcher, right? And that they keep",
      "offset": 3358,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "asking me that question, so I got to ask",
      "offset": 3359.839,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "the experts.",
      "offset": 3361.599,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "Makes sense.",
      "offset": 3364,
      "duration": 2.559
    },
    {
      "lang": "en",
      "text": "If you guys had to build a road map for",
      "offset": 3364.799,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "people who wanted to get really deep on",
      "offset": 3366.559,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "AI evals, what topics should they learn?",
      "offset": 3368.799,
      "duration": 7.201
    },
    {
      "lang": "en",
      "text": "Great question. So Shrea and I have",
      "offset": 3372.079,
      "duration": 5.681
    },
    {
      "lang": "en",
      "text": "developed this course reader. It's",
      "offset": 3376,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "actually a very extensive set of notes.",
      "offset": 3377.76,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "One might even call it a book. It",
      "offset": 3380.799,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "probably will become a book. It's 150",
      "offset": 3383.04,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "pages. This is the detail that we go",
      "offset": 3384.88,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "through in our course. We arm our",
      "offset": 3387.04,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "students with a lot of information to",
      "offset": 3388.72,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "make sure that they get materials in",
      "offset": 3390.799,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "many different ways including uh you",
      "offset": 3392.96,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "know live instruction office hours but",
      "offset": 3395.52,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "also this very detailed sort of treatis",
      "offset": 3397.839,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "on like how you go through eval",
      "offset": 3400.88,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "step-by-step process. So we start off",
      "offset": 3402.64,
      "duration": 5.679
    },
    {
      "lang": "en",
      "text": "with um you know like what is",
      "offset": 3406.16,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "evaluation. We go through the three",
      "offset": 3408.319,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "gulfs framework that we just described.",
      "offset": 3410.24,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "Um you know we talk about why you need",
      "offset": 3413.359,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "evals. We motivate that. Uh then we kind",
      "offset": 3416,
      "duration": 5.599
    },
    {
      "lang": "en",
      "text": "of do a little bit overview of like okay",
      "offset": 3419.359,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "the strengths and weaknesses of LLMs.",
      "offset": 3421.599,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "You know what kinds of things you need",
      "offset": 3423.92,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "to intuitively understand when you're",
      "offset": 3425.359,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "doing evals before you even get to",
      "offset": 3427.92,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "evals. The third chapter is very",
      "offset": 3429.28,
      "duration": 7.36
    },
    {
      "lang": "en",
      "text": "important. It's probably where we spend",
      "offset": 3432.4,
      "duration": 7.199
    },
    {
      "lang": "en",
      "text": "a lot of our time in practice and a lot",
      "offset": 3436.64,
      "duration": 4.959
    },
    {
      "lang": "en",
      "text": "of people we don't know about this step.",
      "offset": 3439.599,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "It's called error analysis. So what is",
      "offset": 3441.599,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "error analysis?",
      "offset": 3444.24,
      "duration": 6.319
    },
    {
      "lang": "en",
      "text": "So you might hear Shrea and I talk a lot",
      "offset": 3446,
      "duration": 6.559
    },
    {
      "lang": "en",
      "text": "about looking at your data. We keep",
      "offset": 3450.559,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "beating people over the head with this",
      "offset": 3452.559,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "phrase, look at your data. Look at your",
      "offset": 3454.24,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "data. What does that mean? Look at my",
      "offset": 3455.839,
      "duration": 6.161
    },
    {
      "lang": "en",
      "text": "data. What data do you look at? Do you",
      "offset": 3458.4,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "look at all your data? Do you just quit",
      "offset": 3462,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "your job and look at your data and do",
      "offset": 3464.4,
      "duration": 2.719
    },
    {
      "lang": "en",
      "text": "nothing else? Do you don't have time?",
      "offset": 3465.839,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "How do you look at your data? And what",
      "offset": 3467.119,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "do you do when you look at that data?",
      "offset": 3469.68,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "Like how do you make sense of it? How do",
      "offset": 3471.119,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "you make it how is it make it tractable",
      "offset": 3472.96,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "and learn something from your data? What",
      "offset": 3475.28,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "if you don't have any data? Like what",
      "offset": 3477.92,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "data am I talking about? What if you",
      "offset": 3479.52,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "haven't built anything yet? Um the key",
      "offset": 3480.64,
      "duration": 6.64
    },
    {
      "lang": "en",
      "text": "thing is like you need to look at some",
      "offset": 3483.76,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "sort of data. Go through a structured",
      "offset": 3487.28,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "process. It's not as painful as it",
      "offset": 3489.52,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "sounds looking at data. It's actually",
      "offset": 3492.24,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "really",
      "offset": 3494.079,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "um beneficial. A lot of my clients, you",
      "offset": 3495.599,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "know, we have a plan to go through this",
      "offset": 3499.599,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "whole process and they get so much value",
      "offset": 3501.839,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "just out of error analysis that they",
      "offset": 3503.839,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "just they're like, \"This is amazing. I'm",
      "offset": 3505.599,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "done.\" I'm like, \"Wait, what do you mean",
      "offset": 3508.079,
      "duration": 2.961
    },
    {
      "lang": "en",
      "text": "you're done? Like, I can do all this",
      "offset": 3509.92,
      "duration": 2.159
    },
    {
      "lang": "en",
      "text": "other stuff for you.\" Like, \"No, no,",
      "offset": 3511.04,
      "duration": 3.039
    },
    {
      "lang": "en",
      "text": "this is great. Like, I this is like I'm",
      "offset": 3512.079,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "busy for a while. Air analysis has",
      "offset": 3514.079,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "taught me so much.\" And you know error",
      "offset": 3516.319,
      "duration": 5.841
    },
    {
      "lang": "en",
      "text": "analysis is you know to go to error",
      "offset": 3518.799,
      "duration": 5.121
    },
    {
      "lang": "en",
      "text": "analysis",
      "offset": 3522.16,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "um we can scroll through",
      "offset": 3523.92,
      "duration": 7.439
    },
    {
      "lang": "en",
      "text": "um let me see if I can there's a diagram",
      "offset": 3528.24,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "here",
      "offset": 3531.359,
      "duration": 6.081
    },
    {
      "lang": "en",
      "text": "but you know um this is this is like one",
      "offset": 3534.24,
      "duration": 5.599
    },
    {
      "lang": "en",
      "text": "way to generate synthetic queries for",
      "offset": 3537.44,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "example um it's hard to dive into the",
      "offset": 3539.839,
      "duration": 6.401
    },
    {
      "lang": "en",
      "text": "details just without the context but we",
      "offset": 3543.04,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "go through the process of like how",
      "offset": 3546.24,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "generate, you know, synthetic data. Um,",
      "offset": 3547.76,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "you know, and then also we go through",
      "offset": 3550.799,
      "duration": 6.721
    },
    {
      "lang": "en",
      "text": "like how to look at your data. Um,",
      "offset": 3553.76,
      "duration": 5.839
    },
    {
      "lang": "en",
      "text": "uh, and, you know, so we describe it in",
      "offset": 3557.52,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "a lot of detail. Um, a lot of it is",
      "offset": 3559.599,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "supplemented with videos. Let me see if",
      "offset": 3561.68,
      "duration": 6.56
    },
    {
      "lang": "en",
      "text": "I can scroll up. Maybe Shre you can, uh,",
      "offset": 3563.44,
      "duration": 6.639
    },
    {
      "lang": "en",
      "text": "yeah, that diagram is pretty good.",
      "offset": 3568.24,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "Yeah, this one.",
      "offset": 3570.079,
      "duration": 3.121
    },
    {
      "lang": "en",
      "text": "So, there's a concept called axial",
      "offset": 3571.599,
      "duration": 5.121
    },
    {
      "lang": "en",
      "text": "coding or open coding and axial coding.",
      "offset": 3573.2,
      "duration": 6.159
    },
    {
      "lang": "en",
      "text": "Um Trey, you want to tell tell us",
      "offset": 3576.72,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "everyone where that comes from and the",
      "offset": 3579.359,
      "duration": 2.641
    },
    {
      "lang": "en",
      "text": "history behind it?",
      "offset": 3580.72,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "Yeah definitely. So um in creating this",
      "offset": 3582,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "curriculum we took a lot of inspiration",
      "offset": 3584.96,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "from social science research actually",
      "offset": 3586.64,
      "duration": 6.479
    },
    {
      "lang": "en",
      "text": "because we were thinking okay in what",
      "offset": 3590.48,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "field and domain do people need to look",
      "offset": 3593.119,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "at vast amount of unstructured free form",
      "offset": 3594.96,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "text and labels and come up with",
      "offset": 3597.68,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "meaningful actionable insights out of",
      "offset": 3599.599,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "it. Well turns out social science",
      "offset": 3601.52,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "researchers have done this for a very",
      "offset": 3603.28,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "long time. There's this process called",
      "offset": 3604.88,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "grounded theory um that gives this",
      "offset": 3606.88,
      "duration": 7.36
    },
    {
      "lang": "en",
      "text": "systematic structure to the process and",
      "offset": 3611.04,
      "duration": 6.319
    },
    {
      "lang": "en",
      "text": "first what people do is go through their",
      "offset": 3614.24,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "open-ended data co outputs or whatever",
      "offset": 3617.359,
      "duration": 5.521
    },
    {
      "lang": "en",
      "text": "it is they read them and they write free",
      "offset": 3620.48,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "form open codes is what they call it um",
      "offset": 3622.88,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "free form notes on what's good what's",
      "offset": 3626.16,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "bad any themes that emerge whatot on a",
      "offset": 3628.16,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "traceby trace basis then after going",
      "offset": 3631.2,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "through about a hundred of those they",
      "offset": 3634,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "will try to merge similar notes together",
      "offset": 3636.079,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "into clusters and this determines your",
      "offset": 3639.04,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "failure modes. If you find that a bunch",
      "offset": 3641.839,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "of notes around, you know, a specific",
      "offset": 3643.599,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "type of hallucination get merged",
      "offset": 3646.48,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "together, well, looks like that's a huge",
      "offset": 3648.16,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "problem that you need to solve in your",
      "offset": 3650.4,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "product. Um, and this kind of loop keeps",
      "offset": 3651.839,
      "duration": 6.081
    },
    {
      "lang": "en",
      "text": "going until what we call theoretical",
      "offset": 3654.72,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "saturation in qualitative research,",
      "offset": 3657.92,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "which means I've not learned any new",
      "offset": 3660.24,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "failure modes. That's all. I keep",
      "offset": 3662.24,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "looking at data and I just keep adding",
      "offset": 3664.559,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "to my existing failure modes. Um, at",
      "offset": 3666.319,
      "duration": 5.601
    },
    {
      "lang": "en",
      "text": "that point you can kind of stop and then",
      "offset": 3669.28,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "you can move on to okay, how am I going",
      "offset": 3671.92,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "to turn those into automated evaluators",
      "offset": 3673.76,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "so I don't have to do it all the time.",
      "offset": 3676.079,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "Maybe I'll build LLM as judges based on",
      "offset": 3678.16,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "my labeled failure modes. Maybe I'll",
      "offset": 3680.88,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "write some codebased evaluators for",
      "offset": 3682.88,
      "duration": 7.199
    },
    {
      "lang": "en",
      "text": "things that code can check. Um and then",
      "offset": 3685.68,
      "duration": 5.919
    },
    {
      "lang": "en",
      "text": "we find that you know we teach people",
      "offset": 3690.079,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "this this diagram error analysis and",
      "offset": 3691.599,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "then suddenly they're they're like oh",
      "offset": 3694.319,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "this is great okay bye. It's like I mean",
      "offset": 3695.839,
      "duration": 5.681
    },
    {
      "lang": "en",
      "text": "I guess right like it it makes sense.",
      "offset": 3699.599,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "This is where most people are actually",
      "offset": 3701.52,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "bottlenecked right because ML",
      "offset": 3703.2,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "traditional machine learning people",
      "offset": 3706.079,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "didn't have to do this. They didn't when",
      "offset": 3707.839,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "they looked at their data they would",
      "offset": 3709.599,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "just look at all these know tables of",
      "offset": 3711.04,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "features which are all numbers and the",
      "offset": 3713.359,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "outputs are also numbers. And so there",
      "offset": 3715.599,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "are ways to kind of debug those. LLMs",
      "offset": 3718,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "are different because now the inputs are",
      "offset": 3721.359,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "free form text and the outputs are free",
      "offset": 3723.28,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "form text and people are like I don't",
      "offset": 3725.04,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "know how to there it's new techn it's",
      "offset": 3726.72,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "new new new skills that you need to be",
      "offset": 3728.799,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "able to make sense of that um but",
      "offset": 3730.799,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "fortunately once you do this then you",
      "offset": 3732.96,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "can go and implement automated",
      "offset": 3735.599,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "evaluators as you might with traditional",
      "offset": 3736.96,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "machine learning um and then you can do",
      "offset": 3739.119,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "improvement strategies you can do",
      "offset": 3741.839,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "fine-tuning all of these things like a",
      "offset": 3743.28,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "traditional machine learning person",
      "offset": 3745.599,
      "duration": 2
    },
    {
      "lang": "en",
      "text": "would",
      "offset": 3746.96,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "um one thing I want to share that might",
      "offset": 3747.599,
      "duration": 5.601
    },
    {
      "lang": "en",
      "text": "be useful here or interesting perhaps",
      "offset": 3749.92,
      "duration": 6.72
    },
    {
      "lang": "en",
      "text": "for a product manager. Um we have Teresa",
      "offset": 3753.2,
      "duration": 6.159
    },
    {
      "lang": "en",
      "text": "taking our class and you know just today",
      "offset": 3756.64,
      "duration": 6.479
    },
    {
      "lang": "en",
      "text": "she was discussing uh you know you know",
      "offset": 3759.359,
      "duration": 6.561
    },
    {
      "lang": "en",
      "text": "uh okay I love the class too. um you",
      "offset": 3763.119,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "know when open and axial coding are we",
      "offset": 3765.92,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "analyzing opportunities towards an",
      "offset": 3767.839,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "outcome",
      "offset": 3769.44,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "and she responds I've been thinking",
      "offset": 3771.119,
      "duration": 5.921
    },
    {
      "lang": "en",
      "text": "about this a lot um you know and then",
      "offset": 3773.04,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "she has some comments about generating",
      "offset": 3777.04,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "synthetic data you know but she also",
      "offset": 3778.64,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "recognizes and this is really fun this",
      "offset": 3780.799,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "is why we have product managers in our",
      "offset": 3782.559,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "class because you know um these kind of",
      "offset": 3784.079,
      "duration": 5.921
    },
    {
      "lang": "en",
      "text": "like tying together of things like okay",
      "offset": 3787.599,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "identifying opportunities from",
      "offset": 3790,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "interviews is based on grounded theory",
      "offset": 3791.359,
      "duration": 5.041
    },
    {
      "lang": "en",
      "text": "also um you know and then she's",
      "offset": 3793.359,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "experimenting with methods for capturing",
      "offset": 3796.4,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "annotations directly from the customer",
      "offset": 3798.319,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "and you know she also kind of points out",
      "offset": 3800.72,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "that okay these are the scientific",
      "offset": 3802.64,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "method applied look at the data so um I",
      "offset": 3804.559,
      "duration": 5.681
    },
    {
      "lang": "en",
      "text": "think project managers have a lot to",
      "offset": 3808.4,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "offer here you know bringing their",
      "offset": 3810.24,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "customer and user research into the",
      "offset": 3812.319,
      "duration": 5.681
    },
    {
      "lang": "en",
      "text": "whole process that's why we think it's",
      "offset": 3814.799,
      "duration": 5.841
    },
    {
      "lang": "en",
      "text": "really important that they are in the",
      "offset": 3818,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "driver's seat of these evals",
      "offset": 3820.64,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "I will go even further to say that I",
      "offset": 3822.4,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "think that PMs are like vital in",
      "offset": 3824.88,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "building AI products. Like we're not",
      "offset": 3828.24,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "going to have successful AI products",
      "offset": 3830.72,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "across different domains unless we have",
      "offset": 3832.4,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "good AI PMs. Like I cannot emphasize",
      "offset": 3834.48,
      "duration": 6.96
    },
    {
      "lang": "en",
      "text": "that enough um without good AI PMs. Like",
      "offset": 3836.96,
      "duration": 6.159
    },
    {
      "lang": "en",
      "text": "we're just going to have failed AI",
      "offset": 3841.44,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "products. So I hope this is a call to",
      "offset": 3843.119,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "action for PMs. Like this is a skill",
      "offset": 3844.88,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "that you absolutely need to develop. um",
      "offset": 3846.72,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "it's going to set you apart obviously of",
      "offset": 3848.88,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "course from a career perspective but",
      "offset": 3851.2,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "also we need this right in order to",
      "offset": 3853.44,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "really realize this vision of AI",
      "offset": 3855.839,
      "duration": 7
    },
    {
      "lang": "en",
      "text": "products changing people's lives",
      "offset": 3858.079,
      "duration": 4.76
    },
    {
      "lang": "en",
      "text": "um okay so to get back on you know the",
      "offset": 3863.599,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "things to know so once you do error",
      "offset": 3866.319,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "analysis now you have a grounded way of",
      "offset": 3868,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "knowing like what to focus on because",
      "offset": 3871.119,
      "duration": 3.041
    },
    {
      "lang": "en",
      "text": "like the question is like what do you",
      "offset": 3872.799,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "even evaluate you can come up with like",
      "offset": 3874.16,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "millions of metrics like hallucination",
      "offset": 3876.559,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "score, toxicity score, conciseness",
      "offset": 3878.48,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "score. You can name these scores all day",
      "offset": 3880.16,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "long and you can get really confused and",
      "offset": 3881.76,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "overwhelmed and say and say, \"Oh my",
      "offset": 3884.16,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "goodness, like I can't do it.\" So error",
      "offset": 3886.24,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "analysis is very important. Um you know,",
      "offset": 3888.88,
      "duration": 5.919
    },
    {
      "lang": "en",
      "text": "you would be surprised",
      "offset": 3892.24,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "in our discord channel almost every",
      "offset": 3894.799,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "other question is answered with the",
      "offset": 3897.44,
      "duration": 5.679
    },
    {
      "lang": "en",
      "text": "phrase error analysis because um you",
      "offset": 3899.599,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "know one of the main trouble people have",
      "offset": 3903.119,
      "duration": 6.081
    },
    {
      "lang": "en",
      "text": "with eval is what do you eval? You can,",
      "offset": 3905.839,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "you know, you can eval anything. There's",
      "offset": 3909.2,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "an infinite number of things, ideas of",
      "offset": 3911.039,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "things that can go wrong. Um, you know,",
      "offset": 3912.96,
      "duration": 5.359
    },
    {
      "lang": "en",
      "text": "instead of just being paranoid and sort",
      "offset": 3915.92,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "of, you know, working yourself up about",
      "offset": 3918.319,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "like what can go wrong, you should be",
      "offset": 3920.72,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "grounding it in things that are going",
      "offset": 3922.319,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "wrong or the things that will probably",
      "offset": 3924,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "go wrong. Um, and that's what era",
      "offset": 3926.319,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "analysis helps you with. It helps you",
      "offset": 3928.72,
      "duration": 6.8
    },
    {
      "lang": "en",
      "text": "focus on high value things because eval",
      "offset": 3930.64,
      "duration": 6.959
    },
    {
      "lang": "en",
      "text": "are not free.",
      "offset": 3935.52,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "um it takes a little bit of effort and",
      "offset": 3937.599,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "so there's a this entire time that",
      "offset": 3939.68,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "you're doing eval there has to be a",
      "offset": 3941.599,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "costbenefit analysis of around like what",
      "offset": 3943.039,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "do you even",
      "offset": 3945.92,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "um should you be emailing what do you",
      "offset": 3947.68,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "email whatever and so error analysis is",
      "offset": 3949.52,
      "duration": 5.839
    },
    {
      "lang": "en",
      "text": "the answer to all those questions um and",
      "offset": 3952.64,
      "duration": 6.88
    },
    {
      "lang": "en",
      "text": "error analysis is so valuable that",
      "offset": 3955.359,
      "duration": 6.96
    },
    {
      "lang": "en",
      "text": "and you know a sizable portion of my",
      "offset": 3959.52,
      "duration": 5.519
    },
    {
      "lang": "en",
      "text": "clients they sign up with me to go",
      "offset": 3962.319,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "through this whole evaluation process",
      "offset": 3965.039,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "and they do the air analysis part and",
      "offset": 3967.52,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "they're like great Haml we love it we're",
      "offset": 3969.44,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "done like this is I'm like no wait I",
      "offset": 3972.24,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "have all this other stuff like all this",
      "offset": 3974.96,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "other stuff in this table of contests I",
      "offset": 3976.559,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "can take you through they're like no",
      "offset": 3977.92,
      "duration": 5.119
    },
    {
      "lang": "en",
      "text": "this is so much value like we've we're",
      "offset": 3979.839,
      "duration": 5.921
    },
    {
      "lang": "en",
      "text": "so happy and it's true because like you",
      "offset": 3983.039,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "know error analysis is like this thing",
      "offset": 3985.76,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "about looking at your data um it",
      "offset": 3989.039,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "actually drives you to develop a very",
      "offset": 3991.28,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "deep intuition about your system and",
      "offset": 3994,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "what's going wrong with it and it makes",
      "offset": 3996.079,
      "duration": 7.201
    },
    {
      "lang": "en",
      "text": "you develop a nose for everything and um",
      "offset": 3998.559,
      "duration": 6.56
    },
    {
      "lang": "en",
      "text": "kind of gives you a six sense of like",
      "offset": 4003.28,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "what is even going to break if you're",
      "offset": 4005.119,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "doing it enough and so it's extremely",
      "offset": 4006.64,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "powerful probably the most powerful",
      "offset": 4009.119,
      "duration": 5.041
    },
    {
      "lang": "en",
      "text": "technique in eval",
      "offset": 4010.72,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "um you know I can't highlight it more",
      "offset": 4014.16,
      "duration": 6.72
    },
    {
      "lang": "en",
      "text": "than that um but okay once you move past",
      "offset": 4016.64,
      "duration": 7.439
    },
    {
      "lang": "en",
      "text": "error analysis now it's now you know",
      "offset": 4020.88,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "what is wrong and where to focus and",
      "offset": 4024.079,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "what to prioritize. Now, how do you",
      "offset": 4025.92,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "actually go about the process of writing",
      "offset": 4027.76,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "the evals? And so, that's where these",
      "offset": 4029.52,
      "duration": 5.599
    },
    {
      "lang": "en",
      "text": "other chapters come in around, okay, um",
      "offset": 4032.4,
      "duration": 6.32
    },
    {
      "lang": "en",
      "text": "how who does the who writes the evals?",
      "offset": 4035.119,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "Who does the annotations?",
      "offset": 4038.72,
      "duration": 5.839
    },
    {
      "lang": "en",
      "text": "So, we have this uh we have some strong",
      "offset": 4041.599,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "guidance in here. We're not we're not",
      "offset": 4044.559,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "afraid to, you know, cut right to the",
      "offset": 4046.24,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "chase and have opinions. And so like one",
      "offset": 4048.96,
      "duration": 5.599
    },
    {
      "lang": "en",
      "text": "opinion that we have for example of the",
      "offset": 4052.72,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "many opinions but I'll just highlight it",
      "offset": 4054.559,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "here is benevolent dictators. So we say",
      "offset": 4056.079,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "okay people always ask like well who is",
      "offset": 4058.88,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "going to make the final call of whether",
      "offset": 4061.039,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "or not this is a pass or fail or who is",
      "offset": 4062.799,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "going to be the one writing the email?",
      "offset": 4066.079,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "Who's in charge? I have a whole team to",
      "offset": 4067.76,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "just involve the whole team. The answer",
      "offset": 4070.88,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "is no. You need to have a benevolent",
      "offset": 4072.24,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "dictator in most situations.",
      "offset": 4074.48,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "Um and there's some you know there's",
      "offset": 4077.28,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "some reasoning behind that. It just, you",
      "offset": 4079.68,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "know, it's like the binary",
      "offset": 4081.52,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "classification thing. It forces you to",
      "offset": 4083.039,
      "duration": 5.121
    },
    {
      "lang": "en",
      "text": "kind of make a decision and it makes the",
      "offset": 4085.44,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "whole process tractable because it can",
      "offset": 4088.16,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "quickly become intractable if you add",
      "offset": 4089.839,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "various complexity. So going back to the",
      "offset": 4092.079,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "table of context, let me context. Let me",
      "offset": 4094.24,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "scroll back up. Um, feel free to",
      "offset": 4096.239,
      "duration": 6.881
    },
    {
      "lang": "en",
      "text": "interrupt me at any time. Um, so we",
      "offset": 4099.279,
      "duration": 6.161
    },
    {
      "lang": "en",
      "text": "talked through implementing, you know,",
      "offset": 4103.12,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "making this a process automated with LM",
      "offset": 4105.44,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "as a judge. Now, with LM as a judge,",
      "offset": 4107.839,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "you're writing prompts",
      "offset": 4111.04,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "and you're asking an LM to grade",
      "offset": 4113.359,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "something. Now, almost everybody,",
      "offset": 4115.6,
      "duration": 6.96
    },
    {
      "lang": "en",
      "text": "I would say 80% of people who are using",
      "offset": 4119.839,
      "duration": 6.4
    },
    {
      "lang": "en",
      "text": "LM as a judge are just writing a prompt",
      "offset": 4122.56,
      "duration": 6.56
    },
    {
      "lang": "en",
      "text": "and just praying that is doing the right",
      "offset": 4126.239,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "thing.",
      "offset": 4129.12,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "But that's not what you should do. When",
      "offset": 4130.64,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "you The right way to use an LLM as a",
      "offset": 4133.759,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "judge is to label to get your label",
      "offset": 4135.279,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "data, which you already done in a",
      "offset": 4137.44,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "previous step. That's what we're",
      "offset": 4139.359,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "teaching. And measure the LM as a judge",
      "offset": 4140.64,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "against your label data to understand",
      "offset": 4143.359,
      "duration": 5.681
    },
    {
      "lang": "en",
      "text": "is the judge any good? Number one. But",
      "offset": 4146.4,
      "duration": 4.959
    },
    {
      "lang": "en",
      "text": "number two, more importantly, you have",
      "offset": 4149.04,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "to iterate on your LM as a judge. You",
      "offset": 4151.359,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "have to see like where is it wrong? and",
      "offset": 4153.279,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "you're like, \"Oh, it's always the case",
      "offset": 4154.799,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "that not only do you adjust the LM as a",
      "offset": 4157.12,
      "duration": 5.679
    },
    {
      "lang": "en",
      "text": "judge, but Shrea has a lot of research",
      "offset": 4160.319,
      "duration": 6.561
    },
    {
      "lang": "en",
      "text": "that shows that the annotator also",
      "offset": 4162.799,
      "duration": 6.88
    },
    {
      "lang": "en",
      "text": "adjusts their requirements",
      "offset": 4166.88,
      "duration": 5.839
    },
    {
      "lang": "en",
      "text": "seeing the LLM output.\" Shre you want to",
      "offset": 4169.679,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "talk a little bit about that?",
      "offset": 4172.719,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "Yeah, I have a paper called who",
      "offset": 4174.4,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "validates the validators um list 2024 if",
      "offset": 4175.759,
      "duration": 5.681
    },
    {
      "lang": "en",
      "text": "people are interested in the venue. But",
      "offset": 4179.199,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "yeah, one of the things we did was we",
      "offset": 4181.44,
      "duration": 3.279
    },
    {
      "lang": "en",
      "text": "built an interface for people to go and",
      "offset": 4182.96,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "annotate outputs to train LLM judge",
      "offset": 4184.719,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "evaluators. And then we found that",
      "offset": 4187.759,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "people will keep changing the rubric as",
      "offset": 4189.92,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "they encounter new failure modes when",
      "offset": 4192.08,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "looking at the data. And this",
      "offset": 4194.239,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "underscores right how tricky it is to",
      "offset": 4195.92,
      "duration": 6.64
    },
    {
      "lang": "en",
      "text": "figure out what the rubric is. How you",
      "offset": 4200.159,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "need to go through this iterative",
      "offset": 4202.56,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "process. How sometimes when the criteria",
      "offset": 4204.08,
      "duration": 6.72
    },
    {
      "lang": "en",
      "text": "is a little bit too subjective, maybe",
      "offset": 4208.08,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "you want to have multiple people weigh",
      "offset": 4210.8,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "in on what correctness means. And there",
      "offset": 4212.4,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "are ways to do this like the inner",
      "offset": 4215.76,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "annotative inner annotator agreement",
      "offset": 4217.6,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "metric that we talk about um cohence",
      "offset": 4220.239,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "kappa and the course reader a lot of",
      "offset": 4222.64,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "stuff. Um point is that we just provide",
      "offset": 4224.08,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "a lot of frameworks to really",
      "offset": 4227.04,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "systematically solve the problems when",
      "offset": 4228.88,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "you encounter challenges in coming up",
      "offset": 4230.4,
      "duration": 6.52
    },
    {
      "lang": "en",
      "text": "with good labels and ground truth.",
      "offset": 4232.719,
      "duration": 4.201
    },
    {
      "lang": "en",
      "text": "Um and so you know we teach you a lot of",
      "offset": 4237.52,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "things like also how to you know correct",
      "offset": 4239.52,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "the LM as a judge error rate based upon",
      "offset": 4243.36,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "your real error rate and you know all",
      "offset": 4245.76,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "kinds of advanced things. We also uh",
      "offset": 4247.679,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "teach you how to think about multi-turn",
      "offset": 4250.159,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "evaluations like okay if you have a long",
      "offset": 4252.159,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "conversation",
      "offset": 4254.48,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "um that's like multiple turns between an",
      "offset": 4256.159,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "AI and a human how do you actually",
      "offset": 4258.239,
      "duration": 5.121
    },
    {
      "lang": "en",
      "text": "evaluate that um do you evaluate the",
      "offset": 4260,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "entire conversation",
      "offset": 4263.36,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "um you know one piece of advice we have",
      "offset": 4265.36,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "is like you should you know evaluate you",
      "offset": 4267.28,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "should when you're doing open coding for",
      "offset": 4270.159,
      "duration": 2.961
    },
    {
      "lang": "en",
      "text": "example you should stop on the first",
      "offset": 4271.6,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "error that you see because these things",
      "offset": 4273.12,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "have causal relationships and it's",
      "offset": 4275.04,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "usually the case that the first error",
      "offset": 4277.199,
      "duration": 5.441
    },
    {
      "lang": "en",
      "text": "is the blocking uh is the blocking",
      "offset": 4279.44,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "problem. And so, you know, a simplifying",
      "offset": 4282.64,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "heristic is to is to is to anchor on the",
      "offset": 4285.52,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "first error. And there's all kinds of",
      "offset": 4288.64,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "heristics like this that make the entire",
      "offset": 4289.76,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "process tractable. Um but also when it",
      "offset": 4292,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "comes to evaluating multi-ter",
      "offset": 4295.6,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "conversation is like how do you actually",
      "offset": 4296.719,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "have test data sets for that? How do you",
      "offset": 4298.64,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "how do you simulate in multi-turn",
      "offset": 4301.04,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "conversation or do you need to simulate",
      "offset": 4302.719,
      "duration": 3.121
    },
    {
      "lang": "en",
      "text": "a multi-turn conversation? There's a lot",
      "offset": 4304,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "of uh you know there's a lot of things",
      "offset": 4305.84,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "to dive in there. So we cover that. We",
      "offset": 4307.92,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "also cover how to",
      "offset": 4310.32,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "evaluate retrieval augmented generation",
      "offset": 4312.719,
      "duration": 8.721
    },
    {
      "lang": "en",
      "text": "and so we talk about when to treat",
      "offset": 4315.92,
      "duration": 7.12
    },
    {
      "lang": "en",
      "text": "certain aspects of the problem like a",
      "offset": 4321.44,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "search problem when what components of",
      "offset": 4323.04,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "the problem like the generation problem",
      "offset": 4325.52,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "how to think about that separately and",
      "offset": 4327.12,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "you know all the nuances there. Um and",
      "offset": 4329.28,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "then we talk about other specific",
      "offset": 4332.48,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "architectures and components. So like",
      "offset": 4334.96,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "how how to think about tool calling,",
      "offset": 4338.4,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "what you need to know about agents. So",
      "offset": 4340.48,
      "duration": 5.679
    },
    {
      "lang": "en",
      "text": "agents are systems that can be very",
      "offset": 4343.28,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "dynamic. You know, they might have",
      "offset": 4346.159,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "trajectories that are unpredictable and",
      "offset": 4348.56,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "do various things that you can't",
      "offset": 4350.48,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "anticipate. How do you evaluate that?",
      "offset": 4352.719,
      "duration": 6.161
    },
    {
      "lang": "en",
      "text": "How do you tame that monster, that",
      "offset": 4355.12,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "spaghetti?",
      "offset": 4358.88,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "It turns out you can use various",
      "offset": 4360.4,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "analytical tools that simplify the",
      "offset": 4362.56,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "problem and allow you to attack it and",
      "offset": 4364.719,
      "duration": 5.281
    },
    {
      "lang": "en",
      "text": "make sense of you know agentic systems",
      "offset": 4367.36,
      "duration": 5.359
    },
    {
      "lang": "en",
      "text": "and we show you uh things like how to",
      "offset": 4370,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "use transition matrices and other",
      "offset": 4372.719,
      "duration": 5.281
    },
    {
      "lang": "en",
      "text": "analytical tools to help wrangle that",
      "offset": 4374.4,
      "duration": 7.68
    },
    {
      "lang": "en",
      "text": "problem. Um and then we talk about you",
      "offset": 4378,
      "duration": 6.88
    },
    {
      "lang": "en",
      "text": "know how to evaluate specific input data",
      "offset": 4382.08,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "modalities like different types of",
      "offset": 4384.88,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "modalities like not just text.",
      "offset": 4386.56,
      "duration": 6.56
    },
    {
      "lang": "en",
      "text": "Um then we talk about production",
      "offset": 4389.44,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "uh productionizing these things. So like",
      "offset": 4393.12,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "CI/CD",
      "offset": 4395.04,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "um you know kind of automating these and",
      "offset": 4396.8,
      "duration": 5.919
    },
    {
      "lang": "en",
      "text": "running these at scale.",
      "offset": 4400.48,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "And then we talk about also kind of",
      "offset": 4402.719,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "probably my second favorite subject in",
      "offset": 4405.12,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "chapter 10 is interfaces. And I'll give",
      "offset": 4407.199,
      "duration": 5.681
    },
    {
      "lang": "en",
      "text": "it to Shrea to talk about the interfaces",
      "offset": 4410.64,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "actually.",
      "offset": 4412.88,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "Yeah. Chapter 10 and 11. They're the",
      "offset": 4414,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "last week of the course and they're kind",
      "offset": 4416.56,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "of like special advanced topics that you",
      "offset": 4418.159,
      "duration": 5.361
    },
    {
      "lang": "en",
      "text": "will uh all I can pretty I think I can",
      "offset": 4420.32,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "guarantee that you won't find them",
      "offset": 4423.52,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "anywhere out there on the internet. So,",
      "offset": 4424.88,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "we wanted to do something special um for",
      "offset": 4426.48,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "our students. Chapter 10 is about how to",
      "offset": 4429.199,
      "duration": 6.321
    },
    {
      "lang": "en",
      "text": "build perhaps even vibe code your way to",
      "offset": 4432.32,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "an effective interface that has people",
      "offset": 4435.52,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "really really labeling things very",
      "offset": 4438.8,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "quickly maximizing the throughput of how",
      "offset": 4440.56,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "many labels you can get from human",
      "offset": 4442.88,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "reviewers. We talk about principles",
      "offset": 4445.28,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "there to case studies of good bad",
      "offset": 4447.04,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "interfaces. Um and then chapter 11 I",
      "offset": 4448.88,
      "duration": 7.44
    },
    {
      "lang": "en",
      "text": "think is about improvement you know",
      "offset": 4452.64,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "obvious strategies that you might know",
      "offset": 4456.32,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "about like decomposing tasks fine-tuning",
      "offset": 4457.84,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "models so forth but also cost",
      "offset": 4460.239,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "optimization and cost improvement. A lot",
      "offset": 4463.44,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "of people actually say like oh my",
      "offset": 4465.44,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "pipeline is working but it's too",
      "offset": 4467.76,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "expensive especially if it's working on",
      "offset": 4470.159,
      "duration": 5.121
    },
    {
      "lang": "en",
      "text": "really long documents. um how do we keep",
      "offset": 4471.76,
      "duration": 6.479
    },
    {
      "lang": "en",
      "text": "the same quality but reduce the cost by",
      "offset": 4475.28,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "an order of magnitude or even more well",
      "offset": 4478.239,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "we talk about some cost optimization",
      "offset": 4480.64,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "techniques there so that's a very long",
      "offset": 4482.48,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "answer I think to your question on you",
      "offset": 4484.64,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "know what's the road map like here is a",
      "offset": 4486.239,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "road map we think it's pretty good a lot",
      "offset": 4488.48,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "of places you can dive in you can dive",
      "offset": 4490.96,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "into each chapter that you're interested",
      "offset": 4492.32,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "in um and chapters 10 and 11 here I",
      "offset": 4493.84,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "think are pretty open territory um in",
      "offset": 4496.56,
      "duration": 5.599
    },
    {
      "lang": "en",
      "text": "both in research to explore more as well",
      "offset": 4498.96,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "so upon on seeing this book, a natural",
      "offset": 4502.159,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "question is where can I get the book or",
      "offset": 4504,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "are you going to release this book? And",
      "offset": 4506.08,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "so the answer to that question is",
      "offset": 4508.32,
      "duration": 5.919
    },
    {
      "lang": "en",
      "text": "eventually yes, maybe early next year.",
      "offset": 4509.679,
      "duration": 7.201
    },
    {
      "lang": "en",
      "text": "However, um you know, if you want the",
      "offset": 4514.239,
      "duration": 5.761
    },
    {
      "lang": "en",
      "text": "kind of hands-on approach of you know",
      "offset": 4516.88,
      "duration": 7.04
    },
    {
      "lang": "en",
      "text": "doing this in in a very guided way, then",
      "offset": 4520,
      "duration": 7.12
    },
    {
      "lang": "en",
      "text": "of course take our course.",
      "offset": 4523.92,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "That's actually what I wanted to ask",
      "offset": 4527.12,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "next. Hmel is you were working at Airbnb",
      "offset": 4528.48,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "and GitHub. Why did you transition into",
      "offset": 4531.44,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "consulting and courses?",
      "offset": 4533.92,
      "duration": 6.72
    },
    {
      "lang": "en",
      "text": "Good question. Um it kind of happened",
      "offset": 4537.04,
      "duration": 7.679
    },
    {
      "lang": "en",
      "text": "somewhat organically. Um you know I took",
      "offset": 4540.64,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "a sabbatical",
      "offset": 4544.719,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "after",
      "offset": 4546.4,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "uh GitHub for a bit. I worked at some",
      "offset": 4548.32,
      "duration": 5.359
    },
    {
      "lang": "en",
      "text": "startups and then I sorry I I uh after",
      "offset": 4550.719,
      "duration": 6.081
    },
    {
      "lang": "en",
      "text": "GitHub I worked at a a few startups then",
      "offset": 4553.679,
      "duration": 5.761
    },
    {
      "lang": "en",
      "text": "decided to take a sabbatical",
      "offset": 4556.8,
      "duration": 7.439
    },
    {
      "lang": "en",
      "text": "um and a company decided to contract me",
      "offset": 4559.44,
      "duration": 8.08
    },
    {
      "lang": "en",
      "text": "convince me kind of to the name of the",
      "offset": 4564.239,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "company was weights and biases they",
      "offset": 4567.52,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "convinced me to uh do some consulting",
      "offset": 4569.44,
      "duration": 5.759
    },
    {
      "lang": "en",
      "text": "work for them and so I yeah I always",
      "offset": 4571.6,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "thought that like I would hate",
      "offset": 4575.199,
      "duration": 2.801
    },
    {
      "lang": "en",
      "text": "consulting because I did consulting with",
      "offset": 4576.48,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "a large consulting company very early in",
      "offset": 4578,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "my career before tech and and I realized",
      "offset": 4579.92,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "like, hey, I really like it. When you're",
      "offset": 4582.8,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "doing it on your own, becoming like an",
      "offset": 4584.159,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "independent",
      "offset": 4586.159,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "indie kind of entrepreneur, it's very",
      "offset": 4587.679,
      "duration": 7.04
    },
    {
      "lang": "en",
      "text": "enjoyable. Um, it's a lot of freedom and",
      "offset": 4590.88,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "I found that a lot of people found it",
      "offset": 4594.719,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "very valuable like in terms of like",
      "offset": 4596.64,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "helping them sort of navigate",
      "offset": 4598.8,
      "duration": 8.08
    },
    {
      "lang": "en",
      "text": "LLMs and AI. Um and then I just yeah I",
      "offset": 4602.8,
      "duration": 6.96
    },
    {
      "lang": "en",
      "text": "just started really enjoying it and one",
      "offset": 4606.88,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "thing led to another and here I am like",
      "offset": 4609.76,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "uh there's no let's say grand plan as",
      "offset": 4611.6,
      "duration": 7.119
    },
    {
      "lang": "en",
      "text": "such it's just uh kind of you know",
      "offset": 4613.679,
      "duration": 6.641
    },
    {
      "lang": "en",
      "text": "yeah it just like happened to be in this",
      "offset": 4618.719,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "place and it happened that everybody",
      "offset": 4620.32,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "building with AI they're always stuck on",
      "offset": 4622.4,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "eval",
      "offset": 4624.719,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "exact same problem almost every single",
      "offset": 4626.88,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "time I needed to start with error",
      "offset": 4628.8,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "analysis and this is like you know",
      "offset": 4630.48,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "working with 2530 companies",
      "offset": 4632.32,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "um you know and then I just started",
      "offset": 4635.04,
      "duration": 5.119
    },
    {
      "lang": "en",
      "text": "writing about it a lot and it was really",
      "offset": 4636.159,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "apparent to me that there's not that",
      "offset": 4640.159,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "much education on this topic and it's",
      "offset": 4641.679,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "where everyone's struggling um you know",
      "offset": 4643.679,
      "duration": 5.281
    },
    {
      "lang": "en",
      "text": "and consulting is expensive and so",
      "offset": 4646.56,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "that's why we created this course to",
      "offset": 4648.96,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "make this way more accessible to a",
      "offset": 4650.88,
      "duration": 5.279
    },
    {
      "lang": "en",
      "text": "larger audience",
      "offset": 4653.36,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "and do you tell people who want to",
      "offset": 4656.159,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "consult you as a consultant to not reach",
      "offset": 4658.32,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "out if they can't spend $38,000",
      "offset": 4660.64,
      "duration": 5.599
    },
    {
      "lang": "en",
      "text": "Yeah. So I think one thing that you have",
      "offset": 4664.4,
      "duration": 6.4
    },
    {
      "lang": "en",
      "text": "to do as an entrepreneur is to",
      "offset": 4666.239,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "a lot of different things but one is",
      "offset": 4670.8,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "like it's important can be important to",
      "offset": 4672.159,
      "duration": 6.721
    },
    {
      "lang": "en",
      "text": "have a niche so that you customers know",
      "offset": 4674.4,
      "duration": 7.92
    },
    {
      "lang": "en",
      "text": "uh what you can help them with. So in",
      "offset": 4678.88,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "this case mine is eval.",
      "offset": 4682.32,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "And you know just from a practical",
      "offset": 4685.36,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "perspective there's a lot of overhead",
      "offset": 4687.28,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "that comes with being an entrepreneur",
      "offset": 4689.44,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "like you have you're in charge of your",
      "offset": 4691.6,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "own sales your own marketing your own",
      "offset": 4693.04,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "you know all the administrative overhead",
      "offset": 4695.6,
      "duration": 6.72
    },
    {
      "lang": "en",
      "text": "of it and um you know you I don't want",
      "offset": 4697.28,
      "duration": 6.8
    },
    {
      "lang": "en",
      "text": "to be on sales calls all day. You have",
      "offset": 4702.32,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "to qualify people and you know the",
      "offset": 4704.08,
      "duration": 6.4
    },
    {
      "lang": "en",
      "text": "problem has to be painful enough for",
      "offset": 4707.6,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "them to want to solve it. And so that's",
      "offset": 4710.48,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "just a kind of a basic kind of blocking",
      "offset": 4712.8,
      "duration": 5.919
    },
    {
      "lang": "en",
      "text": "tackling approach to say, okay, like how",
      "offset": 4716.32,
      "duration": 7.359
    },
    {
      "lang": "en",
      "text": "do I not drown in Zoom meetings?",
      "offset": 4718.719,
      "duration": 7.841
    },
    {
      "lang": "en",
      "text": "And then the course, it costs roughly",
      "offset": 4723.679,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "$2,000",
      "offset": 4726.56,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "and you have over 600 people in this",
      "offset": 4728.08,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "cohort that me, Teresa, Pavle, other",
      "offset": 4730.159,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "people are participating in. So does",
      "offset": 4732.719,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "that mean you earned over a million",
      "offset": 4734.4,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "dollars on this cohort of the course?",
      "offset": 4736.08,
      "duration": 6.56
    },
    {
      "lang": "en",
      "text": "Um, not",
      "offset": 4739.52,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "no. So, we didn't quite cross the",
      "offset": 4742.64,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "million-dollar threshold because we gave",
      "offset": 4744.96,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "a lot of stuff away for free. A lot of",
      "offset": 4746.4,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "friends. Uh, we, you know, I let all my",
      "offset": 4748.32,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "friends in. Basically, uh, Treyo,",
      "offset": 4751.04,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "we're also very generous with any",
      "offset": 4753.92,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "discounts for people who have any need.",
      "offset": 4756.88,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "Basically, if your company can't",
      "offset": 4759.04,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "reimburse it and you don't want to pay",
      "offset": 4760.32,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "fully out of pocket, you know, just",
      "offset": 4761.76,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "email us with how much your company can",
      "offset": 4763.6,
      "duration": 5.119
    },
    {
      "lang": "en",
      "text": "reimburse. And we are super flexible.",
      "offset": 4766.159,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "like we want people to join the course.",
      "offset": 4768.719,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "We have a steep price point because we",
      "offset": 4770.64,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "find that we want we want to attract",
      "offset": 4773.12,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "people who are serious about building AI",
      "offset": 4775.6,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "products and evals, right? It benefits",
      "offset": 4777.6,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "nobody if our course has 10,000 people",
      "offset": 4779.679,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "and everyone's like only mildly",
      "offset": 4782.32,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "interested and serious, right? We want",
      "offset": 4783.92,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "to focus on the people who are going to",
      "offset": 4785.52,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "go and put these techniques into",
      "offset": 4787.28,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "production. We're going to go build",
      "offset": 4788.719,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "products. Um and for that turns out you",
      "offset": 4790.08,
      "duration": 6.96
    },
    {
      "lang": "en",
      "text": "need to have a steep price point.",
      "offset": 4793.44,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "So almost yeah almost a million. I don't",
      "offset": 4797.04,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "want to dodge the question because I",
      "offset": 4798.8,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "know like whatever like almost a",
      "offset": 4800.239,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "million. It's like 800k is",
      "offset": 4802.32,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "what it was the first cohort",
      "offset": 4805.679,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "which is insane. That's mind-blowing",
      "offset": 4808.48,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "right especially for the first cohort of",
      "offset": 4810.719,
      "duration": 5.281
    },
    {
      "lang": "en",
      "text": "a course. And what's even more insane I",
      "offset": 4813.12,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "think is that your next cohort is going",
      "offset": 4816,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "to be your last. Why?",
      "offset": 4818,
      "duration": 6.159
    },
    {
      "lang": "en",
      "text": "Yes. So, I don't want it to be, you",
      "offset": 4821.6,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "know, how you like watch a movie and",
      "offset": 4824.159,
      "duration": 5.361
    },
    {
      "lang": "en",
      "text": "like the first one is good and usually",
      "offset": 4826.239,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "like it's really hard to make a good",
      "offset": 4829.52,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "sequel, but this is like really",
      "offset": 4830.96,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "impossible",
      "offset": 4832.96,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "like to make it like continue to be",
      "offset": 4834.56,
      "duration": 6.4
    },
    {
      "lang": "en",
      "text": "good. And honestly, like um you know, I",
      "offset": 4837.199,
      "duration": 6.161
    },
    {
      "lang": "en",
      "text": "don't want to",
      "offset": 4840.96,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "you know, I have a lot of different",
      "offset": 4843.36,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "interests, but also um there's a lot of",
      "offset": 4844.4,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "things that we can do with the course.",
      "offset": 4848.08,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "So we could for example you know you see",
      "offset": 4849.199,
      "duration": 5.761
    },
    {
      "lang": "en",
      "text": "this like 150 page this is just a draft",
      "offset": 4851.52,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "honestly I mean we don't we keep",
      "offset": 4854.96,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "iterating on this on this book that we",
      "offset": 4857.52,
      "duration": 6.159
    },
    {
      "lang": "en",
      "text": "have open right now um so one you know",
      "offset": 4859.76,
      "duration": 8.959
    },
    {
      "lang": "en",
      "text": "one sort of motion is to like okay focus",
      "offset": 4863.679,
      "duration": 7.601
    },
    {
      "lang": "en",
      "text": "time on writing a book then can you know",
      "offset": 4868.719,
      "duration": 5.361
    },
    {
      "lang": "en",
      "text": "it's currently it's a $2,300 course",
      "offset": 4871.28,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "we're going to actually be increasing",
      "offset": 4874.08,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "the price to $2,500 on Friday um you",
      "offset": 4875.6,
      "duration": 6.8
    },
    {
      "lang": "en",
      "text": "know can there be a recorded version",
      "offset": 4879.76,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "that doesn't involve us like live that",
      "offset": 4882.4,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "doesn't involve any office hours or",
      "offset": 4885.52,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "anything else um you know that's like",
      "offset": 4887.199,
      "duration": 5.921
    },
    {
      "lang": "en",
      "text": "lower cost okay that's fine so we think",
      "offset": 4890.56,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "like there's different products out",
      "offset": 4893.12,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "there that we can offer or different",
      "offset": 4894.32,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "ways that we can offer this um but also",
      "offset": 4896.08,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "like you know this course takes an",
      "offset": 4899.84,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "immense amount of time to deliver and so",
      "offset": 4901.52,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "one of the things that we like to do is",
      "offset": 4904.4,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "actually build these AI projects like I",
      "offset": 4906.08,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "like to work with customers and build",
      "offset": 4907.76,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "these things. I know Shrea does too. She",
      "offset": 4909.36,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "likes to, you know, do research on this",
      "offset": 4911.6,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "topic. Her research is very applied. She",
      "offset": 4914.239,
      "duration": 5.121
    },
    {
      "lang": "en",
      "text": "works with companies on this. She builds",
      "offset": 4916.639,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "tools in the space. And it's really,",
      "offset": 4919.36,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "yeah, if we would do the course over and",
      "offset": 4922.239,
      "duration": 2.641
    },
    {
      "lang": "en",
      "text": "over again like this, we wouldn't be",
      "offset": 4923.76,
      "duration": 2.56
    },
    {
      "lang": "en",
      "text": "able to do that. It would actually",
      "offset": 4924.88,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "dilute the course. So, we want to keep",
      "offset": 4926.32,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "it special. We want to we want people to",
      "offset": 4928.4,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "feel like okay they were here for this",
      "offset": 4931.6,
      "duration": 2.48
    },
    {
      "lang": "en",
      "text": "course it's something they could",
      "offset": 4933.04,
      "duration": 5.599
    },
    {
      "lang": "en",
      "text": "remember it you know um last year I had",
      "offset": 4934.08,
      "duration": 8.24
    },
    {
      "lang": "en",
      "text": "a course like this um it was not on this",
      "offset": 4938.639,
      "duration": 7.04
    },
    {
      "lang": "en",
      "text": "it was on fine-tuning it was it was",
      "offset": 4942.32,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "provisionally about fine-tuning and then",
      "offset": 4945.679,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "became like basically a conference but",
      "offset": 4947.44,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "it was basically the same scale as this",
      "offset": 4950.48,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "course it was like also $900,000 uh",
      "offset": 4952.08,
      "duration": 6.96
    },
    {
      "lang": "en",
      "text": "revenue course in one cohort um And",
      "offset": 4954.8,
      "duration": 5.919
    },
    {
      "lang": "en",
      "text": "yeah, we decided like we don't want to",
      "offset": 4959.04,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "do that again because how can we",
      "offset": 4960.719,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "possibly recreate that that subject and",
      "offset": 4962.48,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "that feeling of that time, you know, we",
      "offset": 4965.199,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "don't want to do a second one and just",
      "offset": 4968.56,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "have it be underwhelming because we try",
      "offset": 4969.84,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "to repeat something. So like, you know,",
      "offset": 4973.04,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "this is different. This is like that",
      "offset": 4974.88,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "was, you know, kind of a conference in a",
      "offset": 4977.199,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "way. It turned into a conference. It was",
      "offset": 4979.6,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "just like everybody about anything about",
      "offset": 4981.199,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "LMS. It was really fun. Um, you know,",
      "offset": 4983.199,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "this is like really focused on evals.",
      "offset": 4985.6,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "you think that okay there's a lot more",
      "offset": 4987.199,
      "duration": 5.281
    },
    {
      "lang": "en",
      "text": "repeatability to this but you know our",
      "offset": 4989.76,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "goal is not necessarily",
      "offset": 4992.48,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "you know trying to milk all the money",
      "offset": 4995.36,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "out of it per se. We're actually like",
      "offset": 4997.36,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "you know all this this money that made",
      "offset": 4999.52,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "we're actually like going to invested in",
      "offset": 5002.56,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "all these things. So like writing a book",
      "offset": 5004,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "um you know doing this additional like",
      "offset": 5006.56,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "different kinds of courses like that are",
      "offset": 5009.44,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "can be delivered in different ways you",
      "offset": 5011.44,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "know it's all going to be reinvested",
      "offset": 5012.96,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "into that to make this material like",
      "offset": 5014.4,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "accessible um because we really believe",
      "offset": 5017.12,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "we have a lot of conviction in this",
      "offset": 5019.76,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "message and this topic and the impact of",
      "offset": 5021.28,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "of learning this.",
      "offset": 5023.28,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "I think there's a lot of lessons there",
      "offset": 5025.679,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "for aspiring and current course",
      "offset": 5027.199,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "creators, but I think one of the most",
      "offset": 5029.36,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "interesting ones is how of the time",
      "offset": 5030.639,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "you've made this course. Like you're",
      "offset": 5034.239,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "addressing the problems that you're",
      "offset": 5035.6,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "seeing with your consulting clients and",
      "offset": 5037.6,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "with AI engineers and AIPMs that they're",
      "offset": 5039.679,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "facing with eval. So you're able to",
      "offset": 5042.239,
      "duration": 3.521
    },
    {
      "lang": "en",
      "text": "write about it that way. You're not",
      "offset": 5044.639,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "trying to just create timeless content.",
      "offset": 5045.76,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "And I think that makes it really",
      "offset": 5048.56,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "effective.",
      "offset": 5049.92,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "Yeah, I hope so. I mean, uh, you know,",
      "offset": 5052.08,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "people message me, they're like, \"Oh,",
      "offset": 5054.88,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "this is a lot of money for like a month",
      "offset": 5056.4,
      "duration": 5.279
    },
    {
      "lang": "en",
      "text": "or whatever, but then I remind them",
      "offset": 5059.52,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "like, we've been writing about this for",
      "offset": 5061.679,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "2 years.\" I think Shrea has reviewed all",
      "offset": 5063.44,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "of my blog posts throughout the years.",
      "offset": 5065.92,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "Um, you know, I send it to her and she",
      "offset": 5068.159,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "like reviews it. Uh, you know, I'm",
      "offset": 5070,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "really grateful for that. Um, you know,",
      "offset": 5072.4,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "and she's been writing about this for",
      "offset": 5074.96,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "many years as well. Um, and by some",
      "offset": 5076.639,
      "duration": 6.721
    },
    {
      "lang": "en",
      "text": "stroke of luck, I was able to convince",
      "offset": 5080.48,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "her to nerd snipe her also and say like,",
      "offset": 5083.36,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "\"Okay, let's do this course together.\"",
      "offset": 5085.52,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "Um, and I wouldn't have done it if she",
      "offset": 5087.36,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "said no. It was only like if she does",
      "offset": 5089.04,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "it, then I will do it kind of situation.",
      "offset": 5091.28,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "Um, you know, because Freya is, you",
      "offset": 5094.48,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "know, if you look at her writing, it's",
      "offset": 5097.36,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "actually really impressive. Like she",
      "offset": 5099.84,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "Yeah, she like um it's a very good",
      "offset": 5102.239,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "compliment cuz I'm like really focused",
      "offset": 5104.4,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "on consulting and stuff like that. I",
      "offset": 5105.92,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "don't have time to survey the field",
      "offset": 5107.28,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "bring in you know like this like",
      "offset": 5110.239,
      "duration": 5.521
    },
    {
      "lang": "en",
      "text": "generalized perspective and theory but",
      "offset": 5112.88,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "there's a really good compliment there",
      "offset": 5115.76,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "where Shrea brings to you know",
      "offset": 5117.76,
      "duration": 5.439
    },
    {
      "lang": "en",
      "text": "structuring this material bringing in",
      "offset": 5120.48,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "like you know a lot of the kind of I",
      "offset": 5123.199,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "mean I didn't even know what axial",
      "offset": 5126.56,
      "duration": 3.119
    },
    {
      "lang": "en",
      "text": "coding I was just doing axial coding",
      "offset": 5127.84,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "open coding I didn't know it had a",
      "offset": 5129.679,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "history in social sciences so it was",
      "offset": 5130.96,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "really good to know that there's a",
      "offset": 5134.239,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "there's there's actually some like",
      "offset": 5136.56,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "history of this working. So things like",
      "offset": 5139.04,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "that. I mean um I don't even know if I'm",
      "offset": 5141.199,
      "duration": 5.361
    },
    {
      "lang": "en",
      "text": "answering the question anymore.",
      "offset": 5144,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "There was no question. So that was",
      "offset": 5146.56,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "great. If you were to give advice to",
      "offset": 5148.48,
      "duration": 5.679
    },
    {
      "lang": "en",
      "text": "somebody who wanted to create a $800,",
      "offset": 5151.6,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "$900,000 course, what would be your",
      "offset": 5154.159,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "advice to them?",
      "offset": 5156.4,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "Yeah. So one is Okay, so a lot of people",
      "offset": 5158.88,
      "duration": 6.319
    },
    {
      "lang": "en",
      "text": "ask me this question. Uh let me try my",
      "offset": 5162.48,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "best.",
      "offset": 5165.199,
      "duration": 5.281
    },
    {
      "lang": "en",
      "text": "One is is to really find a niche that",
      "offset": 5166.88,
      "duration": 6.64
    },
    {
      "lang": "en",
      "text": "you are passionate about. Like I didn't",
      "offset": 5170.48,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "think I was going to create a course",
      "offset": 5173.52,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "around this honestly. Um you know I've",
      "offset": 5174.639,
      "duration": 5.281
    },
    {
      "lang": "en",
      "text": "been writing about this forever. In fact",
      "offset": 5177.6,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "emails is like one of the most unpopular",
      "offset": 5179.92,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "things you can possibly write about or",
      "offset": 5181.76,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "talk about. It's like deeply deeply",
      "offset": 5183.76,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "unpopular.",
      "offset": 5186.159,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "Yeah. I really want to drive that point.",
      "offset": 5188.08,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "had we talked about something very cool",
      "offset": 5190.719,
      "duration": 5.041
    },
    {
      "lang": "en",
      "text": "and sexy like you know I don't even",
      "offset": 5192.639,
      "duration": 5.841
    },
    {
      "lang": "en",
      "text": "there's so many buzzwords out there like",
      "offset": 5195.76,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "MCP and like agents and stuff right like",
      "offset": 5198.48,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "that is an easy topic to write a course",
      "offset": 5201.92,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "on because everyone wants to learn how",
      "offset": 5204,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "to do it eval is something that",
      "offset": 5205.6,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "everybody knows is a problem but doesn't",
      "offset": 5207.28,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "want to go and put in the effort to",
      "offset": 5209.199,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "doing so I think yeah my number one",
      "offset": 5210.8,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "advice would be to find a if you want to",
      "offset": 5213.92,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "do it the easy way find a problem that",
      "offset": 5216,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "people are excited about going and",
      "offset": 5217.76,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "actually doing the work for cuz you",
      "offset": 5219.52,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "could probably 10x what we did for",
      "offset": 5221.36,
      "duration": 6.799
    },
    {
      "lang": "en",
      "text": "emails if you pick a exciting topic. Um,",
      "offset": 5223.92,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "but Ham, you should continue.",
      "offset": 5228.159,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "Yeah, I mean, I actually went through",
      "offset": 5229.76,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "several nights or weeks or months or",
      "offset": 5231.84,
      "duration": 5.839
    },
    {
      "lang": "en",
      "text": "whatever of thinking to myself, man, am",
      "offset": 5234.4,
      "duration": 5.759
    },
    {
      "lang": "en",
      "text": "I swimming upstream? Like, you know, I",
      "offset": 5237.679,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "talk about evals. built",
      "offset": 5240.159,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "sorry I built my uh consulting around",
      "offset": 5242.639,
      "duration": 6.961
    },
    {
      "lang": "en",
      "text": "evals but like you know",
      "offset": 5245.36,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "you know it's like telling people to eat",
      "offset": 5249.6,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "their vegetables um you know it's not",
      "offset": 5251.04,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "really that popular",
      "offset": 5253.36,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "and um you know it's much easier to talk",
      "offset": 5255.52,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "about agents you know if if you did some",
      "offset": 5258,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "like background research if you try to",
      "offset": 5260.239,
      "duration": 2.801
    },
    {
      "lang": "en",
      "text": "come out of this perspective like hey I",
      "offset": 5261.52,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "should build a course I want to like",
      "offset": 5263.04,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "build a course you would never pick",
      "offset": 5264.32,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "evals you would pick agents",
      "offset": 5266.56,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "you'd be like okay I'm going to sell an",
      "offset": 5268.719,
      "duration": 2.401
    },
    {
      "lang": "en",
      "text": "agent course everyone wants to learn",
      "offset": 5269.92,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "about that. People are actively googling",
      "offset": 5271.12,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "if you do like SEO research like what",
      "offset": 5272.96,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "are like the keywords people like almost",
      "offset": 5275.199,
      "duration": 6.081
    },
    {
      "lang": "en",
      "text": "nobody is like searching for evals",
      "offset": 5277.36,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "there's very little competition for that",
      "offset": 5281.28,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "keyword actually um like if you search",
      "offset": 5282.88,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "for evals like",
      "offset": 5285.92,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "Shrea myself Eugene basically our",
      "offset": 5288.4,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "friends like Shrea me and our friends",
      "offset": 5291.44,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "and I'm talking about our friends",
      "offset": 5294.32,
      "duration": 2.879
    },
    {
      "lang": "en",
      "text": "meaning the friends that Shrea and I",
      "offset": 5295.52,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "have in common we all are come come up",
      "offset": 5297.199,
      "duration": 5.681
    },
    {
      "lang": "en",
      "text": "on like you know at the top of the list.",
      "offset": 5299.76,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "It's pretty insane. It's like that's how",
      "offset": 5302.88,
      "duration": 5.839
    },
    {
      "lang": "en",
      "text": "niche or quote niche it is. But you know",
      "offset": 5305.12,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "at some point I just didn't care. I was",
      "offset": 5308.719,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "like we need to create the category.",
      "offset": 5310.56,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "So you know it's not popular any make it",
      "offset": 5313.36,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "popular.",
      "offset": 5315.92,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "So I don't know if that's good advice.",
      "offset": 5317.679,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "That was my mindset. But I can tell you",
      "offset": 5319.679,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "the only reason that allowed me to get",
      "offset": 5321.679,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "into the mindset is because Shrea",
      "offset": 5323.52,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "partnered with me.",
      "offset": 5325.04,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "And so one thing I would say is like",
      "offset": 5327.04,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "find a good partner to partner with you.",
      "offset": 5328.48,
      "duration": 7.04
    },
    {
      "lang": "en",
      "text": "So like to get into the mechanics like",
      "offset": 5331.44,
      "duration": 8.08
    },
    {
      "lang": "en",
      "text": "you know a lot of course sales has to",
      "offset": 5335.52,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "you know marketing is very important.",
      "offset": 5339.52,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "You need to let people know that the",
      "offset": 5341.36,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "course exists otherwise they can't buy",
      "offset": 5342.8,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "the course who don't know. And uh they",
      "offset": 5345.6,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "not only need to know it exists they",
      "offset": 5348,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "need to know like what it's about. Um",
      "offset": 5349.44,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "they need to get excited about it. They",
      "offset": 5351.52,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "will need to know like it's good. They",
      "offset": 5353.52,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "need to know like you know they need to",
      "offset": 5356,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "have some social proof or they need to",
      "offset": 5358,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "have you know some idea like people like",
      "offset": 5359.6,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "it they their peers get value out of it",
      "offset": 5361.84,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "all of these things and they need to be",
      "offset": 5364.08,
      "duration": 6.159
    },
    {
      "lang": "en",
      "text": "reminded about it constantly because you",
      "offset": 5366.719,
      "duration": 5.361
    },
    {
      "lang": "en",
      "text": "know I need to be reminded about things",
      "offset": 5370.239,
      "duration": 5.361
    },
    {
      "lang": "en",
      "text": "constantly for anything. So, um, you",
      "offset": 5372.08,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "know, all of this is very, very",
      "offset": 5375.6,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "important. And so, uh, that's a",
      "offset": 5377.04,
      "duration": 6.159
    },
    {
      "lang": "en",
      "text": "full-time job,",
      "offset": 5380.8,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "you know, and so",
      "offset": 5383.199,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "if you want a course, you want to sell a",
      "offset": 5385.52,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "course like that, like you have to spend",
      "offset": 5387.44,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "a lot of time doing that. And can you",
      "offset": 5388.8,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "outsource that marketing? You can",
      "offset": 5391.28,
      "duration": 2.64
    },
    {
      "lang": "en",
      "text": "definitely partner with people on",
      "offset": 5392.719,
      "duration": 2.641
    },
    {
      "lang": "en",
      "text": "marketing, but you kind of have to drive",
      "offset": 5393.92,
      "duration": 2.719
    },
    {
      "lang": "en",
      "text": "it because you are the subject matter",
      "offset": 5395.36,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "expert, you're the domain expert. you",
      "offset": 5396.639,
      "duration": 3.121
    },
    {
      "lang": "en",
      "text": "know, you know, you have to build that",
      "offset": 5398.08,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "authority and that connection with your",
      "offset": 5399.76,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "audience because, you know, whatever.",
      "offset": 5403.12,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "Um, you know, it's just relentless focus",
      "offset": 5405.92,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "on that. Um, but",
      "offset": 5408,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "Hamilton is not also talking about all",
      "offset": 5410.159,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "of the work that he spends putting",
      "offset": 5412.32,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "together, you know, like a lot of really",
      "offset": 5414.56,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "great guest speakers really thoughtfully",
      "offset": 5416.96,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "figuring out, okay, you know, we're",
      "offset": 5419.76,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "talking about X, Y, and Z. So, and so is",
      "offset": 5422.8,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "like the world's leading expert on X.",
      "offset": 5425.28,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "like why has been applied in these three",
      "offset": 5427.28,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "companies so let's get somebody from",
      "offset": 5429.36,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "there. Um this the guest speakers are",
      "offset": 5431.04,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "really diverse and I think really",
      "offset": 5434.32,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "distinguish also our course from a lot",
      "offset": 5436.56,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "of other courses out there. It's not",
      "offset": 5439.199,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "just us monotonously lecturing. It's",
      "offset": 5440.48,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "we've got other people coming in showing",
      "offset": 5442.719,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "you how these things are implemented in",
      "offset": 5445.04,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "practice at big companies at small",
      "offset": 5447.28,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "companies at specific domains or",
      "offset": 5449.6,
      "duration": 4.84
    },
    {
      "lang": "en",
      "text": "whatnot.",
      "offset": 5451.44,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "Another thing is focus. Yeah, this is",
      "offset": 5455.44,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "like I get up in the morning, all I",
      "offset": 5457.199,
      "duration": 6.241
    },
    {
      "lang": "en",
      "text": "think about is the course and how can I",
      "offset": 5459.12,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "let more people know about the course or",
      "offset": 5463.44,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "like you know make it better for",
      "offset": 5465.04,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "students. Um how can I bring more",
      "offset": 5467.44,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "perspective into there so on and so",
      "offset": 5469.52,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "forth. Yeah. So all that stuff is really",
      "offset": 5471.6,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "important. Um and just constant",
      "offset": 5474.08,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "experimentation. So kind of going back",
      "offset": 5476.8,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "it's kind of like a you know I'm a data",
      "offset": 5478.639,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "scientist kind of thinking person. I'm",
      "offset": 5480.639,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "just constantly doing experiments and",
      "offset": 5482.239,
      "duration": 5.121
    },
    {
      "lang": "en",
      "text": "looking at the data. Um, and you know, I",
      "offset": 5484.48,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "send Treya like probably average of 200",
      "offset": 5487.36,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "text messages a day or something like",
      "offset": 5490.48,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "that with like different experiments",
      "offset": 5491.92,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "like of like, oh, I'm trying this. I'm",
      "offset": 5494.159,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "doing this thing. Look at these numbers.",
      "offset": 5496.239,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "Okay. Like what do you like this",
      "offset": 5498.32,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "marketing copy this here? I put it like",
      "offset": 5499.84,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "you put an ad here. This what h this is",
      "offset": 5502.48,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "what happens. Do you have any ideas for",
      "offset": 5504.88,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "like how we can go about this",
      "offset": 5507.44,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "differently? Whatever. Um, you know,",
      "offset": 5509.04,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "hope she has do not disturb on. I hope I",
      "offset": 5512.159,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "do.",
      "offset": 5514.96,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "But uh you know so uh yeah I mean this",
      "offset": 5516.159,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "like constant experimentation so it's",
      "offset": 5519.76,
      "duration": 2.879
    },
    {
      "lang": "en",
      "text": "kind of treating it like how I treat",
      "offset": 5521.36,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "evals to be meta is like you know this",
      "offset": 5522.639,
      "duration": 6.801
    },
    {
      "lang": "en",
      "text": "like iterating on you know the business",
      "offset": 5526.88,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "of making the core and the thing that's",
      "offset": 5529.44,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "like really good about making money in a",
      "offset": 5531.199,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "course is like you can I've hired a lot",
      "offset": 5533.36,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "of my friends you know and so like you",
      "offset": 5535.6,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "know a lot of my friends are data",
      "offset": 5538.96,
      "duration": 2.719
    },
    {
      "lang": "en",
      "text": "scientists machine learning engineers",
      "offset": 5540.239,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "and you know a lot of them are involved",
      "offset": 5541.679,
      "duration": 5.121
    },
    {
      "lang": "en",
      "text": "in one way or the other either as",
      "offset": 5544.639,
      "duration": 6.881
    },
    {
      "lang": "en",
      "text": "affiliates or guest speakers or TAs",
      "offset": 5546.8,
      "duration": 7.919
    },
    {
      "lang": "en",
      "text": "and I pay all of them. Uh and it's",
      "offset": 5551.52,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "great. I love paying them and yeah it's",
      "offset": 5554.719,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "like really great like all my friends",
      "offset": 5557.44,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "win too and that's like the good part is",
      "offset": 5559.92,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "you know to create that situation where",
      "offset": 5563.12,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "everybody wins.",
      "offset": 5566,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "What a great place to end it. If people",
      "offset": 5569.04,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "want to find you guys online where",
      "offset": 5571.28,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "should they go?",
      "offset": 5572.96,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "Yeah, we're both active on X. Um, you",
      "offset": 5574.56,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "can pretty much just search us up,",
      "offset": 5577.28,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "you'll find us. I have a blog. HL has a",
      "offset": 5578.8,
      "duration": 8.24
    },
    {
      "lang": "en",
      "text": "blog. Um, email us. Yeah. Nothing super",
      "offset": 5581.44,
      "duration": 7.84
    },
    {
      "lang": "en",
      "text": "fancy, I think.",
      "offset": 5587.04,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "Where do they find your email?",
      "offset": 5589.28,
      "duration": 6.32
    },
    {
      "lang": "en",
      "text": "The best way to do it is just to Google",
      "offset": 5592.8,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "us and our email is at the top of the",
      "offset": 5595.6,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "page.",
      "offset": 5597.679,
      "duration": 5.281
    },
    {
      "lang": "en",
      "text": "Um, but I can also",
      "offset": 5599.44,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "my my email is first name last name",
      "offset": 5602.96,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "shreunker berkeerkley.edu.",
      "offset": 5605.04,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "So,",
      "offset": 5607.76,
      "duration": 2.08
    },
    {
      "lang": "en",
      "text": "and mine is just get in touch with me at",
      "offset": 5608.08,
      "duration": 5.599
    },
    {
      "lang": "en",
      "text": "X on X. It's easy, you know. You can",
      "offset": 5609.84,
      "duration": 5.839
    },
    {
      "lang": "en",
      "text": "send me a DM there. You can message me,",
      "offset": 5613.679,
      "duration": 5.361
    },
    {
      "lang": "en",
      "text": "whatever, and I'll figure it out.",
      "offset": 5615.679,
      "duration": 5.361
    },
    {
      "lang": "en",
      "text": "Amazing. Thank you guys so much for",
      "offset": 5619.04,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "lending your expertise. We'll see",
      "offset": 5621.04,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "everybody in the next one. So, if you",
      "offset": 5623.36,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "want to learn more about how to shift to",
      "offset": 5625.44,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "this way of working, check out our full",
      "offset": 5627.04,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "conversation on Apple or Spotify",
      "offset": 5628.96,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "podcasts. And if you want the actual",
      "offset": 5630.8,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "documents that we showed, the tools and",
      "offset": 5633.36,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "frameworks and public links, be sure to",
      "offset": 5635.92,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "check out my newsletter post with all of",
      "offset": 5638,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "the details. Finally, thank you so much",
      "offset": 5640.4,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "for watching. It would really mean a lot",
      "offset": 5643.199,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "if you could make sure you are",
      "offset": 5645.76,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "subscribed on YouTube, following on",
      "offset": 5647.12,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "Apple or Spotify podcasts, and leave us",
      "offset": 5649.679,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "a review on those platforms. That really",
      "offset": 5652.48,
      "duration": 5.759
    },
    {
      "lang": "en",
      "text": "helps grow the podcast and support our",
      "offset": 5655.199,
      "duration": 5.361
    },
    {
      "lang": "en",
      "text": "work so that we can do bigger and better",
      "offset": 5658.239,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "productions. I'll see you in the next",
      "offset": 5660.56,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "one.",
      "offset": 5662.56,
      "duration": 3
    }
  ],
  "cleanText": "Why do PMs need to be good at AI evals?\n\nOkay, so there's three things that are really important.\nOne, evals give you a way as a PM to inject your taste and your judgment directly into the critical path of the AI product being developed.\nThe second thing is like evals are really important in helping you iterate. The most effective way to do that is using eval, specifically looking at data in a very structured way.\nAnd then the third thing is scale. By mastering evals, what you can do is you can make sure that you can scale your taste judgment, user requirements across all the AI workloads that are running.\n\nWhen it comes to AI evals, Hamel Husain and Shreya Shankar are known as the worldwide leading experts. Companies like OpenAI and Arise go to them, and today we're going to learn everything you need to know about evals from them.\n\nWhat is the most critical skill for PMs who want to build AI features to develop?\n\nHands down, error analysis. The ability to look at your outputs and systematically figure out what makes for a bad output. Quantify how many of these failure modes you see in a big batch of traces for your system and then figure out how to turn that measurement into a continuous flywheel of improving your product.\n\nIf you guys had to build a roadmap for people who wanted to get really deep on AI evals, what topics should they learn?\n\nReally quickly, I think a crazy stat is that more than 50% of you listening are not subscribed. If you can subscribe on YouTube, follow on Apple or Spotify podcasts, my commitment to you is that we'll continue to make this content better and better. And now on to today's episode.\n\nHamel Husain and Shreya Shankar are the people who the experts go to for evals, OpenAI, Arise AI. Those people are going to them for evals, and we have them on the podcast today. Welcome Shreya, Hamel.\n\nYay.\n\nThank you. Nice to be here.\n\nWhy do PMs need to be good at AI evals?\n\nOkay, so there's three things that are really important.\nOne, evals give you a way as a PM to inject your taste and your judgment directly into the critical path of the AI product being developed. So like, you know, as we all know, like PMs, they spend a lot of time gaining context from customers, user feedback, so on and so forth, writing PRDs, they're, you know, trying to give context to engineers, and you know, they're hoping like kind of engineers are faithfully carrying out their vision. Now what eval give you is, you know, you can directly make sure that your taste in all of that context, if done correctly, is now on the critical path when your engineering team is developing those AI products.\nThe second thing is like evolves are really important in helping you iterate. So, you know, nothing is like set in stone. You have to constantly like change your requirements, you're learning more about your customer, so on and so forth. The most effective way to do that is, you know, using evals, specifically looking at data in a very structured way, and which this is one of the things that Shreya and I teach. Um, you know, that allows you to refine and have really fast feedback loops and really fast cycles of feedback.\nAnd then the third thing is scale. So, you know, by mastering eval, what you can do is you can make sure that you can scale your taste judgment, user requirements across, you know, all the AI workloads that are running in a way that you just couldn't before, because ultimately there's a lot of, you know, you can bake a lot of these evals, they're using AI themselves, you just have to make sure that you do it correctly. So you have to make sure that you align the AI with yourself as a PM in a very kind of process that we teach. Um, and as long as you do that correctly and you do it in such a way that you develop trust in the AI that is doing the eval, and there's a way to do that, that alignment, then you can really scale yourself. So a lot of times PMs, or not just PMs, but people at large, they kind of view evals as a very monotonous task that, you know, you just want someone else to do it. It's like, oh, like I have to look at data, I have to annotate data, um, you know, who's going to do this. You don't want to give up that leverage, because when you when you build that foundation of evals, um, you develop, you have immense leverage, and you can, you know, it's a really quick way to kind of exert lots of influence over the process and in a good way, and so this is why I would encourage PMs to really pay attention to this.\n\nCan you guys precisely define evals?\n\nYeah, I can take this one. An eval is some systematic measurement of some aspect of quality. So what varies in an eval is what that criterion is. So for example, maybe it's conciseness of a response and then how you want to measure it. So maybe that is I'm going to define it by, you know, word length. I'm going to find it by sentence length. Um, maybe it is some, you know, very, very complex bespoke human judgment for something that's more subjective. Um, but those two things make up an email eval. And often times products actually have a suite of evals. I've never seen just one eval doing the job. I see three to five, sometimes even up to 10 evals that are really important for a product.\n\nPeople say that if you get eval right, you've gotten the hardest part of the AI product solved. Is that accurate?\n\nI think it's accurate. Now, what do you think?\n\nI think it's totally accurate. Just like anything else, it's the process of creating the evals that provides all the value. It's not necessarily the eval itself is the journey that creates all the value. And so once you've done all of that work, you've looked at all of your data, you've iterated on your system, you've thought very carefully and and you know, often times scientifically about how to improve your system, you've already got 99% of the way there.\n\nThe way that I like to think about it is if you ever want your product to make it past one iteration, you need evals. I've never seen somebody make it through multiple iterations of their product without any evals. But once you have good evals in place, then evals are not necessarily the bottleneck for you. But that's a good thing. That's how it should be, right? You should be able to focus on building out other aspects of the product, making things faster, making things feel better, more intuitive, um, you know, everything beyond that.\n\nWhy can't you just rely on like human evals? Like the PM looks at the feature, the engineers look at the feature, and they feel like those outputs are good enough.\n\nOh, I love this question on the vibe checks and why. So, so Hamilton and I teach our course and pitch it in a way that we we are helping you codify, operationalize, and scale up your vibe checks. Your vibe checks are very important, but they don't scale, right? Cuz they involve you, the human. It's very hard to onboard other people to do the vibe checks in the same way as you are. So like I would have to observe you do this thousands of times, look at outputs, try to build my own rubric or mental model of what you're doing, and then I have no good way of teaching other people of how to do this. So being able to do eval just means taking your vibe checks and translating them to something concrete. In our course we define that as a rubric of binary criteria. Every criteria can be complex. That's fine. Can be subjective. That's fine. But you better have a very precise definition for pass fail. Have some examples of pass. Have some examples of fail. And we also teach people ways to measure alignment on those results. That's really what this whole process is about.\n\nI think the critical phrase there is binary criteria. Why binary?\n\nYeah. So binary really is a kind of a heuristic in a in a sense like that is like a simplification for that works for most people. And the the thing is like, you know, a lot of people try to like assign scores, let's say on a rating scale, 1 to five, that's usually a really bad idea because no one knows what that means. If you have a average score of 3.2 versus average score of 3.7, what does that really mean? And you know, that can be very hard to calibrate and you have to work incredibly hard to you know, make sense of that. So binary judgments force you to kind of make a past fail decision and that tends to also correlate with the fact that you're going to have to ship this product. Do you want to ship it or not? And it really distills that decision- making down into the annotation. And for the vast majority of people that's the right choice.\n\nYeah. And to provide a little bit extra context on the background of LLMs as judge and why, you know, people have a lot of variance in whether they want it to be, you know, binary or rating based scale. Um, LLM as judge has been around, you know, before these foundation models, even just regular language models, fine-tuning models to serve as judges. Um, and in those cases, people a had a lot of preference data of what is good and bad and maybe even a fine grain scale and b could fine-tune models to be aligned with that preference data. Today's world of LLM judge is very different. We don't see people fine-tuning judge models as much. We see people trying to use off-the-shelf models, still want to align with their complex subjective criteria. And now the alignment problem is much harder, right? You can't, you know, steer the LLM in a way that you could before. Um, and for that reason, we say limit yourself to binary because that is what the LLM can do very well. All you have to do is provide examples of past, provide examples of fail and have, you know, very simple or like good rubrics. Um, and people find that much easier to do than say rating on a scale of one to five. Okay, now you need to provide examples for one, for two, for three, for four, for five. You need to, you know, have descriptions of what makes a one different from a two. All of these things, you know, the pair wise interactions between all these ratings just explode in complexity. And we never see people successfully able to operationalize that at the rate at which they can do binary evals. And a lot of times, um, non-binary evals like ratings of 1 to five, that is a a smell of intellectual laziness, like the work hasn't been done to actually, you know, to make a call of like what is good enough and what's not good enough, and it's kind of like, h, we don't really know, let's just capture these like rough things, um, you know, in in this like uh score cuz we're going to lose something, and it doesn't, you know, like the binary skill, like really forces you to be very clear about what you want.\n\nAI evals are one of the most important skills for PMs and I know you know they matter. The question is, are you doing them right? Most teams are winging it with basic metrics and hoping for the best. Meanwhile, the teams that actually ship reliable AI, they've cracked the code on systematic evaluation. Today's episode is brought to you by the AI Evals for engineers and PMs course by HL Husain and Shreya Shankar. This live Maven course will teach you the battle tested frameworks from HML and Shreya, who are the engineers behind GitHub copilot's evaluation system and 25 plus production AI implementations. Four weeks live instruction. Next cohort starts July 21st. Start shipping AI that actually works. Enroll at maven.com with my code ag-roduct-growth for over $800 off. That's ag-pr-g r o wt. Today's episode is brought to you by Jira product discovery. If you're like most product managers, you're probably in Jira, tracking tickets and managing the backlog. But what about everything that happens before delivery? Jurro product discovery helps you move your discovery, prioritization, and even road mapping work out of spreadsheets and into a purpose-built tool designed for product teams. Capture insights, prioritize what matters, and create road maps you can easily tailor for any audience. And because it's built to work with Jira, everything stays connected from idea to delivery. Used by product teams at Canva, Deliveroo, and even The Economist. Check out why and try it for free today at atlassian.com/roduct-discovery. That's a t-san.com/roduct-discovery. Juro product discovery. Build the right thing.\n\nI've heard that LLMs are also not very good at one to five ratings. Is that true?\n\nThey're good at what they're trained on. Somewhere out there in the world, I am sure there is a task with very clear or simple one to five ratings and the LLM is good for that. But to make such a blanket statement for all products and all use cases is very hard to do. That's the thing. That's the message we want to hammer home to every single product manager who takes the course. Like look, you think the LLM might be able to do something. You saw an instance of an LLM being able to do the task for some other domain. That doesn't mean it's going to translate to your domain or your use case. you still have to put in this work. Um, and just don't trust any Hamel is a great way of saying this. Maybe he should talk about it. But he always tells people never trust it. Always put on your detective hat. Hamel, you want to talk about that?\n\nYeah. What underlines the entire process of evals is the scientific method. It's something that we've all learned in high school education. Um, but it's really applied, you know, in this context. Then what you have to do is be very skeptical of everything and do lots of experiments and prove to yourself that the the thing that you're trying to achieve or some new complexity you want to add, whatever it is, that is actually working and try to do it in the simplest way. Um, and build intuition doing by doing lots of experiments. Um, but the point is to like measure those and like, you know, record those and go through it in a structured way, uh, rather than those vibe checks. You you asked about vibe checks earlier. The analogy that I like to use and I can give this to you uh if you ask me later. There's a uh there's a little video of my friend Greg Sakarelli playing whack-a-mole and it's my favorite meme to use when telling people about the need for evals. It's, you know, it's like you're playing whack-a-mole without evals, you know. So, you see a problem, okay, hammer it over with some tool or a prompt change. Then another problem comes up, you hammer that and you keep going, you don't really make any progress. It's really with Eval that you can\n\n\nSystematically try to solve the problem without like going in circles.\nI want to talk about some stories.\nSo, one of the features I implemented in my last job, I was VP of product at Apollo.io.\nIt's a unicorn startup that does sales technology.\nSo, what do sales people need to do, right?\nThey need to write emails.\nSo, as soon as I think it was GPT 3.5 came out, we're like, \"Okay, we're going to use GPT 3.5 to write people's emails.\"\nBut the very first thing we found was that it was hallucinating all sorts of crazy details, and ultimately we had to set up a bunch of evals instead of vibe checks.\nCan you guys give a little bit more context into why eval help solve that hallucination problem for us?\nYeah.\nSo with any kind of problem that you see, um, you know, hallucination or whatever it is.\nSo the first thing to know about evals where people go off the rails is do not reach for generic metrics.\nSo the industry is full of tools and vendors wanting to sell you like a magic pill to solve your eval problem.\nLike, hey, don't worry.\nJust buy our tool, plug it in.\nWe'll show you a dashboard of all of these metrics and things like that.\nThe problem is it doesn't work because those off-the-shelf eval hallucination scores are not going to work.\nWhat you need to do is take a look at, in your case, those emails and understand like what exactly is the failure mode and what if you do uh observe a hallucination, what is the hallucination, and kind of give more life to the domain specificity of the hallucination so that you can then start crafting an LLM as a judge that is prompted in such a way that is very specific to the types of hallucination that you are seeing.\nUm, and then you go to through an iterative process.\nSo, you kind of handle label when hallucinations happen, you know, how they're happening.\nYou're building an LLM as a judge and you're measuring that judge.\nYou're being skeptical again through a scientific process and you're saying, \"Okay, I have this judge.\nCan I get it to agree with me?\nCan I have it be, let's say, if it's you Akos making this judge this email hallucination thing, um, you know, how do I make this judge a proxy of me and how do I trust it almost like an employee?\"\nAnd so the only way to do that is to check it, and you can do that iteratively through this process.\nUm, and when you do that, then you not only do you have something that scales, it's an automated way of checking that problem, but it's also something that you trust.\nAnd that second part, that's something that you trust, is key because the last thing you want is a whole bunch of evals that you put up on a dashboard and then people stop looking at them because they're like, \"We have these evals, but you know what, the product doesn't really work.\"\nNo, that that's the death of your AI product because then no one's going to ever look at eval again and then you don't have any leverage.\nThat's exactly what we experienced, and you weren't even there.\nI want to talk a little bit about your experiences.\nI know you worked at Airbnb on these products.\nCan you tell us a little bit more about that?\nAnd what was the most difficult part of building evals there?\nSo, I didn't work on LLMs at Airbnb because it was prior to, you know, way before ChatBT.\nUm, but what I did work on my entire career is machine learning and um, you know, building predictive models, and a lot of the same machinery of evals comes directly from machine learning.\nUm, the reason that is is because machine learning systems are systems that produce stochastic outputs.\nYou know, they'll give you predictions of various kinds, um, or like classified things, and they're non-deterministic, and you have to evaluate them.\nYou like, you know, they're giving you different outputs every time, and they have noise.\nAnd, um, so how do you do that?\nThat's something that's very well-established in machine learning that, you know, a lot of people haven't been exposed to.\nSo when it comes to like AI more generally now, you have really very similar thing.\nIt's like, hey, you have like a stochastic system, you know, it's non-deterministic, it can output anything like these emails, it's like how do you go about measuring that?\nAnd, um, yeah, and so basically you can kind of bring that over, and so what we teach is instead of going through the entire data science machine learning curriculum, how do you have a very focused way of learning that that is contextualized to LLMs?\nThat's actually fascinating.\nHow was Airbnb using machine learning models?\nI'm sure people want to know under the hood.\nYeah, Airbnb was using it for a lot of things such as detecting fraud in payments.\nUm, also the biggest use case at Airbnb was search ranking.\nSo, if you're searching for a listing, let me show you other listings that you might be interested in based upon what you've been looking at and what you're searching for.\nSo basic recommendation systems, search ranking, things like that.\nUm, a lot of like growth marketing initiatives like trying to figure out the lifetime value of a specific guest so you can allocate marketing to them appropriately.\nThat's what I worked on.\nUm, you know, and there's many other use cases, but those are the ones that sort of one of like their bigger ones.\nSo now they have, you know, now Airbnb is doing generative AI stuff uh as well, just like any other company.\nSearch ranking is something that probably we've been dealing with, right, for like 30 plus years, I guess, ever since, you know, these search engines ever came out.\nSo what can we learn from how people are evaluating search and apply into how we're evaluating our LLMs?\nThe number one difference now is we definitely search systems to try to get context to improve our LLMs.\nSure.\nBut the consumer of the search results is now the LLM.\nIn the past, the consumer of search has been the human.\nAnd humans and LLMs are good at different things.\nLLMs are good at finding needle in a haystack in very, very long complex windows.\nHumans have very short attention spans.\nThey'll read through things, but after a few paragraphs, they're done.\nSo, some of the metrics like how high up a relevant result was ranked are much more important for humans than they are for LLMs, you know, you could try to retrieve like 500 results and as long as it's, you know, even if it's like 150, 200, the LLM will pick it out and figure out how to give that result to the user.\nSo I think the bottom line differences are, you know, we still want to use the same metrics, but our tolerance has changed slightly.\nWe still want to prioritize recalling the right information, but now that we have long context windows, it's okay for it to kind of be at the end.\nUm, as long as we carefully, you know, leverage LLMs to go and uh iteratively refine those search results, pick out the bottom results, and then go and show that back to the human.\nOne thing I'll add on to this is people often wonder, okay, how do I evaluate RAG systems?\nSo RAG is a big thing, and so what what is RAG, you know, retrieval augmented generation?\nThere's the retrieval part and then there's the generation part, and so the retrieval part, you evaluate that pretty much the exact same way that you would evaluate any search system.\nSo all those classic search systems, um, and informationational retrieval, like that entire scientific discipline can be applied onto the retrieval part.\nSo like optimizing that, making sure you're getting the right documents, the right context, so on and so forth.\nUm, and a lot of it applies, and Trey is right, there's a lot, there is some nuance in terms of different tolerances and things like that.\nYeah.\nYeah, the the number one thing I see that's different is instead of measuring like recall at 10, like we can measure recall at 500 and have a long context model like Gemini be able to consume those results.\nUh at the end of the day, maybe you want to measure precision for the result of the LLM call that feeds in the result to the human.\nSure, whatever humans consume, they should all be measured the same way.\nBut if LLMs are there, refining search results, then we can use that to our advantage to be able to rerank outputs from a search engine.\nSo that's search.\nOne of the other sort of really big things in the AI space that I think Hamel, you worked on a little bit, was the precursor to GitHub Copilot, right?\nLLMs for code generation.\nWhat was the biggest problem you guys faced with eval?\nSo with okay, things like code generation are actually really interesting.\nSo in eval, there's more generally in AI products more generally is you want your domain expert in inside the inner loop.\nSo what you don't want to do is give your developer the the task of annotating your data and you know, writing evals necessarily because they don't know enough, they don't have enough context, and that really bottlenecks a lot of teams because they don't get that right.\nBut there's an exception.\nThe exception is developer tools.\nThat is one case where the domain expert is the developer.\nAnd so that's why we saw developer tools like as the first, you know, AI products because that was the sweet spot, that was like where it was easiest to develop.\nThat's one property of you know, developer tools.\nThe second property is there's a highly verifiable domain.\nSo there's some structure to code, and it turns out like on GitHub, um, you know, you have all this code and you also have lots of tests defined against that code.\nSo a lot of time was spent in that verifiable domain, and it's excellent when you have a verifiable domain, um, is to develop kind of a test harness, and the test harness was very impressive.\nBasically what it did is it took all of the code at scale, not all of it, but a select like filtered quantity of it, and basically recreated the environment that code is going to run in and ran the test at scale.\nAnd basically it, you know, things like asking the LLM to, you know, fix certain things or complete certain code, and it would run all the tests.\nIt's kind of a very impressive engineering effort because you're talking about running all this random software from everywhere with all kinds of dependencies at scale all the time.\nUm, so the the details don't matter.\nWhat I want to say is like the eval really mattered because a lot of upfront work went into constructing the eval system, and it's really after that that the team was able to iterate really fast.\nLike when GitHub Copilot was first released internally, it it didn't really work, you know, it was something like, you know, 20% or so of you know, uh, like uh acceptance rates of like suggestions, things like that.\nAnd then after the evals, the team was able to like iterate really fast and like climb that uh, you know, to where it did work.\nUm, and so you know, it was really like, or let me step back.\nIt it was, yeah, it was it was really like the key that unlocked uh, you know, progress on that.\nUm, you know, I didn't work directly on the GitHub Copilot, like, you know, in that phase.\nI was kind of working on some research before that.\nUm, I was actually working on some research that led to some of the benchmarks.\nSo, um, basically one thing I did at GitHub is uh, you know, I worked on this project called Code SearchNet, which is a semantic search of code, and basically like you type in like what what code you're looking for and like semantically try to find it, and it leveraged a lot of the um, there's so a lot of code has comments in it in the documentation embedded in that code.\nSo it was just a really large data set.\nWe opened a large data set with benchmarks of uh, you know, code retrieval, and so that was like used by OpenAI in their very first iteration of CodeEx, which is like a very old model, it's not not the current CodeEx, this is like a different, the same name but different, different thing.\nUm, but it was it was an eval, it was an eval that was used back in the day, so you can say like been working on LLM eval for a really long time.\nTrust.\nTrust isn't just earned, it's demanded.\nWhether you're a startup founder navigating your first audit or a seasoned professional scaling your GRC program, proving your commitment to security has never been more critical or more complex.\nThat's where Vanta comes in.\nBusinesses use Vanta to establish trust by automating compliance needs across over 35 frameworks like SOCK 2 and ISO 2701, centralized security workflows, complete questionnaires up to five times faster, and proactively manage vendor risk.\nVanta can help you start or scale your security program by connecting you with auditors and experts to conduct your audit and set up your security program quickly.\nPlus, with automation and AI throughout the platform, Vant gives you time back so you can focus on building your company.\nJoin over 9,000 global companies like Atlassian, Kora, and Factory who use Vant to manage risk and prove security in real time.\nFor a limited time, my listeners get $1,000 off Vanta at vanta.com/akosash.\nThat's v a n t a.com/aakash for $1,000 off.\nToday's episode is brought to you by the AIPM certification on Maven run by McDad Jaffer, who is a product leader at OpenAI.\nThis is not your typical course.\nIt's 8 weeks of live cohort-based learning with the leader at one of the top companies in tech.\nOpenAI just doesn't stop shipping, and this is your chance to learn how, run along with Product Faculty and Mo Ali.\nThe course has a 4.9 rating with 133 reviews.\nFormer students come from companies like OpenAI, Shopify, Stripe, Google, and Meta.\nThe best part, your company can probably cover the cost.\nSo, if you want to get $500 off, use my code AAK25 and head to maven.com/product-faculty.\nThat's maven.com/product-faculty.\nYou mentioned hill climbing, and I think that's a really interesting concept because I was reading Daniel McKinnon's piece on eval.\nHe is a PM on Meta's llama team, and he talked about how it's so important for the PM to define the evals so that then the AI engineers and research teams that he works with can hill climb.\nCan you guys explain that?\nEngineers are really good at hill climbing and especially ML AI data science stats people, people who are trained to go and look at, you know, ML metrics, figure out why they're bad and how to improve them.\nThey were good at that in structured data in traditional machine learning because those metrics are very well defined, like accuracy is well defined, loss is well defined.\nAnd if you're like doing a binary classifier and you already have label data, you know, it's all there for you.\nYou're just doing your job and trying to improve on those metrics.\nNow, we're in a role in which none of the metrics are defined.\nAnd so, there's no way to go and exercise all the skills that the AI engineers have to\n\n\nImprove those metrics.\nI think it is so spot-on that this article talks about, you know, PMs need to set these metrics because they have the best context to set them.\nAnd once you set them, once you provide the definition or you know, examples of good and bad, then engineers will figure out a way to encode this into the product and improve, even get to self-improving products like that.\nThat is exactly what I see also in my experience.\n\nOne thing you have to be careful with with Hill Climbing, and when that term is said, it does tickle my brain, is make sure that people don't overfit because, you know, engineers sometimes they use that term and they are overfitting in some sense.\nUm, you, if they're not, if you know machine learning people know, data science people know, but it is a thing.\nAnd what is, what does overfitting look like in practice?\nYeah, overfitting means that you, um, you have hill climbed against some data or something that doesn't generalize.\nSo the key phrase is \"does not generalize.\"\nAnd, um, you know, maybe you have some eval.\nSo, like, a one very trivial way to mess this up, and this is from a real client, is you have some, uh, you have a test data set of, you know, cases that you want to do well on in your evals, and you use the same data as few-shot examples in your prompt.\nSo that is a hilariously kind of straightforward way of overfitting because you have given the LLM the answer directly in the prompt.\nUm, and there's, you know, we can relax that a little bit.\nThere's more subtle ways that you might do that.\nUm, but essentially, you know, you might overengineer your prompt with those specific cases in mind, with, with details of those cases that are a little bit too specific.\nYou know, even though you don't have the exact, like, information there, you know, that might lead to overfitting.\nUm, and there's many ways you can overfit.\nAnd what we, Shreya and I, teach is, well, how do you know that you overfit?\nHow can you guard against it?\nIt turns out you can use a lot of the same techniques from machine learning to kind of give yourself an early warning system to say, \"Hey, like, you overfit.\"\nWhat are some of those techniques?\nUh, number one thing is collect label data.\nReserve some of it for testing, as Hamel kind of alluded to.\nYou know, you want to have some test cases that you want to try out your product to see if it performs well on.\nYou know, reserve that set and never, never look at it.\nNever look at it when you're developing your product, when you're developing your prompts, even when you're kind of trying to test out your prompts to figure out how to improve it.\nJust make sure that the test data is like in the sandbox that you never, never see.\nThat's the number one strategy that I tell people.\nUm, another thing that I tell people, and PMs should also know this, is anytime you see a metric that is suspiciously too high, like I achieved 99% alignment or 95 or 100 or whatever, immediate smell warning flags that something is off.\nWe're leaking some data into our prompts from the test set.\nMaybe, maybe our test set doesn't have enough examples.\nUm, so go back and question the process if you see any numbers that are too high.\nThat's absolutely true.\nOne of my favorite examples is there's, um, there is an interview that's been going around for a while now.\nIt's from a company called Caseex, who's a, you know, was bought by, I believe, uh, Thompson Reuters or maybe Lexus Nexus, one of the two big ones, uh, legal companies, and the CEO went on, you know, and said, \"Hey, like, you need to keep iterating on your evals until you get to 100% accuracy.\"\nAnd so, um, you know, if you're iterating, if you're getting 100% accuracy in your evals, it's likely your evals are worthless.\nThey, because they are providing you with no signal.\nJust think about anything else.\nIf every, every student in your class is getting a perfect score, is it a good test?\nYou know, any kind of test, if everybody is passing it, if everything is passing, should you be celebrating?\nNo.\nIt means the test doesn't have, is not differentiating good and bad, and it probably is not really worth it.\nAnd so, um, it's really good, you know, you have to keep that in mind.\nYou want your test, a bit of nuance to it.\nYou want some tests that are, you know, basic regression tests or functionality tests that absolutely you want to pass them because otherwise you're shipping a broken product, but I think the broader point Hamel and I want to make is you also should have some aspirational evals.\nUm, and those, you know, it doesn't need to be 0% or like designed adversarial, really, but some benchmark that helps you know whether, you know, the general AI or intelligence in your system is getting better.\nIn talking to so many AI companies about evals, what's one AI company that sticks out to you as doing it particularly well and why?\nEveryone's struggling.\nOkay, I'll, I'll talk about some design principles that I see people converging on that are solving parts of their problem.\nSo, so I don't think anybody is doing it perfectly.\nI mean, otherwise you would have like a rocket ship AI startup that's like already perfect and like gone public and everything works.\nWe just don't have that, right?\nIt's, it's not there.\nSo, like, let's be honest.\nUm, some of the things that I've been seeing teams that really seem to give them leverage are one, building custom data labeling and annotation interfaces, um, or providing ways for domain experts or other people that they trust to be able to give feedback on traces.\nUm, a lot of the observability tools for LLMs have started to build in these features.\nThey didn't have them even in the last like 2 months, 3 months.\nUm, so it's going to take some time until we can really see the effect of that.\nUm, another thing that I've been seeing is these really well-scoped LLM judges that are being deployed as part of products.\nUm, Claude Anthropic just had a new post on their research agent about how, you know, they really break down the complex task into a multi-agent system, and then critically they have a bunch of LLM judges that are very, very well scoped to each task, much like we teach in our course.\nUm, and I'm sure they're making a bunch of revenue off of this product.\nUm, and then of course, I think there's a lot around, you know, Cursor, famously, you know, very dutifully measures your, you know, next token prediction.\nThey have their own models for this.\nUm, I think next token prediction for coding is interesting because that metric is very well defined.\nPeople know that it's useful.\nIt directly correlates with product success.\nNot all products have such a metric like that.\nUm, PMs need to really go figure that out.\nBut these kinds of things I think are broadly helpful across the board that I've been seeing.\nBut I think we're pretty far out from, like, you know, the product that's perfect right now.\nWhat is next token prediction?\nUh, like in when you're writing code, like the next snippet, um, that you're going to write in your code.\nIf the AI model predicted that correctly, then the metric goes up.\nIf not, metric goes down.\nKind of like an advanced autocomplete.\nYeah.\nWhy haven't we seen better autocomplete in like regular products like email and text message and things like that?\nIt's incredibly frustrating.\nUm, if, especially if you're writing code, if you're a programmer using Cursor, Shreya is using Cursor, I know, like six hours a day.\nUm, and you know, the developer tools are very far ahead of the other tools.\nAnd it's incredibly frustrating if you try to use AI in Gmail or in Google Docs or in PowerPoint or anything.\nAnd it's like even more frustrating because then you open LinkedIn or whatever and it's like, \"Do we have AI in everything?\"\nAnd we're like, \"No, you don't.\nLike, you barely do.\"\nUm, and, um, yeah, I'm not really sure why.\nUm, I have some hypothesis.\nYeah.\nSo it goes back to Hamel's comment on verifiable domains.\nCode is a verifiable domain to some extent, right?\nYou can just make sure the code runs.\nLike that's already great.\nThe other thing is there's a bunch of data of how people write code that's already there and available on the internet in ways that, you know, we don't have that for emails.\nUm, emails are not very verifiable, right?\nLike nobody is telling you like, \"This is ground truth, like this is correct or like this is not correct.\"\nUm, PMs have to do this job of figuring out how to design a system around good and bad emails.\nUm, and that's where it becomes really, really bespoke, and I think the, that it goes also back to Hamel's comment around code where it's like developers knew what the eval, so they were able to implement that.\nUm, but when, like, this is why we need AIPMs, we need people to be able to help developers craft these verifiable or even like loosely verifiable signals.\nI know you guys have spent some time with, uh, OpenAI.\nHow is OpenAI approaching evals and what can we learn from them?\nSo one thing I'll say is, um, you know, there's a really big difference between foundation model benchmarks, so MMLU score, human eval score, these are like general purpose benchmarks which try to assess the general capability of models, and then there's another kind of eval, which is your domain-specific eval for your business, and they are very, very different.\nUm, and it's important that people know that.\nUm, now, rightfully so, uh, the foundation model labs like OpenAI, they're very much focused on the former, the, the general purpose benchmarks because that is, you know, relevant to their product.\nUm, but I think, you know, they're kind of at the same level of playing field as everybody else when it comes to domain-specific emails and like how the companies should be defining their emails.\nThe other thing is they're not going to do it right because like your definition of a good email is different from my definition of a good email, and we're both building email assistant companies.\nThey're going to be different products than they should be because they should reflect our taste and our company's vision.\nUm, OpenAI just like cannot solve both of them with one model, right?\nThat doesn't make sense, and they're not in that business, right?\nLike they're trying to make the model that you want to hire, for lack of a better term, to do the job.\nUm, but you need to train, not just, not really train in terms of model parameters, but you need to figure out how to make an environment, how to elicit the right signals, how to reject emails that are bad according to your definitions.\nLike that's all the stuff that you've got to build that's domain-specific that no foundation model company is going to do for you.\nDefinitely.\nOne example that is top of mind right now is Shreya just wrote this excellent blog post, \"Writing in the Age of LLMs.\"\nAnd you know, if I was to, so the problem is is like foundation models, you know, they're trained, they have like sets of labelers, and basically it's the, it's by definition trained on the average taste of some people.\nAnd you know, if you're, if, for so if you're writing a lot, you know, like I know you are, gosh, you know, as well is, you know, what comes out of the LLM, like almost everybody I know that writes a lot kind of hates it.\nThey're like, \"Why can't it be better?\nLike, why is it giving me like the same, like, why is it like using so many words, and why is it like doing this?\nWhy does it keep using mdash everywhere?\nWhy is it using so many emojis?\"\nLike, stop it.\nIt's because like, you know, whoever is labeling the data, you know, they kind of bias towards like longer explanations and like overusing bullet points and all this stuff.\nAnd so, like, you know, if you want to have like a, a thing that writers love, you know, then you have to get someone, you know, you have to get like perspective like Shreya has and like iterate on your product like over time and make it do those things that they incorporate the taste that you have, you know, um, and, and so that's kind of where it diverges from the foundation models.\nSo, do those wrappers where you're putting in your own taste into the evals, do they actually have some sort of moat where people should consider building those startups?\nOh, absolutely.\nOh, I think maybe I'm crazy.\nI think evals are the moat for AI products and like truly nothing else.\nLike tomorrow, you can use a different model.\nYou can have a different stack for surfing, whatever it is.\nLike there, the secret sauce is your evals and how you're able to operationalize or scale that out as quickly as possible.\nUm, so that means having good LLM judges, for example, that are like very well aligned with your preferences that you can just automatically run on everything, um, and build that flywheel for yourself to like go then look at what failed and improve your product and so forth.\nAnd it's not just, like, it's not just the eval itself.\nIt's the whole system, like Shreya is saying, around the eval, the entire eval pipeline.\nIt basically should be portable.\nYou should be able to switch models or, you know, switch components and see what the effect of that is.\nAnd the key thing is like eval open up a whole lot of doors.\nSo it opens up the door for easy fine-tuning.\nYou know, once you've done an eval, you've already done 99% of the work of fine-tuning.\nUm, fine-tuning is just a kind of a formality that you can go through after that almost to, to an extent because you've already done, you already have all the tools for data curation, you know how to measure things, you know how to like see, measure the effects of the fine-tuning, so on and so forth.\nUm, you know, and I suspect that that will become, that just accelerates your advantage like that much more.\nIt's kind of funny because I see people tend to focus on fine-tuning a lot more, and there's a lot more attention given to that.\nI think it's because like in the OpenAI and Anthropic developer docs for when they give you a model, they're like, \"Yeah, this is how you can fine-tune it and things like that.\"\nHow does fine-tuning really fit in with the overall life cycle?\nWhen should you be putting attention into fine-tuning?\nWe, we should put it in last.\nSo, you should do evals first because like what are you, you have to know if you're fine?\nYeah.\nOh, sorry.\nMaybe my connection is bad.\nThere's like lag here.\nUm, yeah, you need to know this.\nThis goes back to you want to iterate on your product.\nYou want to know, okay, my product is not good, so I'm going to improve it.\nYou don't have any way of knowing if your product is not good unless you have eval.\nSo, eval zero, quantify your performance.\nThen step one is, okay, how am I going to improve its weak points?\nSome low-hanging fruit ways of improving are just use a more powerful model.\n\n\nSwitch from GBT40 mini to GPT40. See if that works. Because you have good evals, right? You can just make that switch, run it, see if your numbers go up. There are other more complicated strategies you can do, like take a complex LLM call, a complex task described in them, and break them down into multiple LLM calls. So instead of, you know, having an LLM extracting 10 things from uh this document, I'm going to have one LLM call each extracting one of those 10 things. Um, so like that, you can kind of do task, that's what we call task decomposition. When you exhaust these strategies, then it makes sense to move into fine-tuning, where it's like, \"I have no other way of solving my problem. The model's just not there yet. I'm going to collect a bunch of data or use the data I've already labeled and fine-tune a model.\" Fine-tuning has cons because now you have to make sure that you're continually fine-tuning that model as you get new data. You learn something new about your preferences of your customers. Um, they don't just automatically update if the base model updates, right? GPT 40 or 50 or whatever comes out. Now you have to redo your all fine-tuning all over again. And then if you don't use a model provider's fine-tuning service, like you're fine-tuning an open-source model, you need to deal with the MLOps complexity of serving that, making sure it has good uptime. Latency is low. This is all actually pretty complicated, right? Costs a lot to maintain this infrastructure, both in human personnel as well as in money. Um, and and using an off-the-shelf LLM is like pretty much the right way to go in most people's use cases. Um, so for that reason, for those reasons, we really try to not encourage people to do fine-tuning unless you really, really have a good reason for doing your own fine-tuning.\n\nOne example where I might go to fine-tuning, hilariously, is this writing thing. Cuz like Sha, we we talk about it a lot, like writing, writing, we bash our head against the wall. Even right before this call was like, \"Oh, like do you try 4.1?\" Industry made good comments like, \"No, like you know, you can't prompt it with all these rules. It's not going to follow these rules.\" So, I'm like, \"Okay, there's no, this really feels like it's only one avenue left here.\" Um, you know, we have to put a lot of work into it, but it's like if you wanted Shreya GPT, that feels like the only way to get it.\n\nFine-tuning is always talked in the same conversation as prompt engineering and RAG. So when do you really think about using each of those three techniques? Um, okay, so like, yeah, prompt engineering is basically, you know, everybody that's is basic, you know, you're using the LM, you have to communicate what you want to it, you have to specify what you want in whatever way, so you should always be writing prompts. You don't even need to put word engineering at the end. We are kind of, you know, adding a lot of ceremony. Um, you know, we're just, you're writing, you know, write English, or you know, in some cases, you might, you can write other languages too, but it is mostly English. Um, you know, and refine your thinking and refine your instructions. RAG is, you know, anytime you're you need external context, um, which there's a very narrow set of use cases where you don't need external context. Most of the time, you do need, in many applications, you need some external context, um, you know, that's not some general knowledge of the world, like, and so, um, that's when you need RAG, you know, and you're not trying to bake all that external context into your prompt.\n\nUm, and then yeah, fine-tuning is sort of, \"Hey, like, if you can't prompt this behavior, if the if the model is not doing what you want,\" Sha has a has created this like very powerful um model. It's called The Three Gulfs. Maybe we should bring it up on the screen actually.\n\nYeah, this is the three gulfs question. Yeah, we should dive into The Three Gulfs.\n\nI think they're in lesson one.\n\nOkay.\n\nOr great, you have them.\n\nOh, you made your own gulfs. Nice.\n\nYeah, I I can take a stab. So your question was, you know, you have all these tools available to you for improving AI products. You have prompt engineering, you have RAG, you have fine-tuning. Are they the same? Are they different? When do I use them? Blah blah blah. This is a good question that trips people up a lot because people are just thinking of them all equally as ways to improve their product. They are all complimentary strategies. You can do multiple of them, you can do none of them, you can do one of them, whatever it is. Prompting is very good when you have specific requirements that your task follows and you need to communicate that to the LLM, right? So, say you were to hire some human to do some job or contract some job for your company. You might give them a task specification. Say do this, then do step A, then step B, then step C. Make sure your output follows these requirements. Okay, that's that's prompting. You know, there's no, almost feels like there's no ceiling sometimes to how much value you can get from good prompting. And this is the first thing that you should be doing. And this solves this gulf of specification problem that Hamel and I see a lot um in how people build LLM products, which is, you know, they have some latent, this hidden criteria, this hidden task that they want the LLM to do, but they just don't know how to specify it well in a clear way and completely. So that LLM fully understands all of those hidden preferences.\n\nNow fine-tuning RAG strategies are for this gulf of generalization, which targets this problem of, \"I have a very good specification, but the model is simply not good enough for some reason.\" Maybe the model is not powerful enough, so I should do fine-tuning. Maybe the model just doesn't have all of the context that it needs to make the decision. That's where RAG comes in. I need to pull in external data sources to improve my product.\n\nBut yeah, very helpful. And I will submit to you, I know that you ask this question a lot in your podcast because I think it's confusing. Rightfully so, you asked this question. I would say this might be a good framework for answering that question. Um, you know, since it comes up a lot.\n\nYeah, I try to play the role of the the watcher, right? And that they keep asking me that question, so I got to ask the experts.\n\nMakes sense.\n\nIf you guys had to build a road map for people who wanted to get really deep on AI Evals, what topics should they learn?\n\nGreat question. So Shreya and I have developed this course reader. It's actually a very extensive set of notes. One might even call it a book. It probably will become a book. It's 150 pages. This is the detail that we go through in our course. We arm our students with a lot of information to make sure that they get materials in many different ways, including uh, you know, live instruction, office hours, but also this very detailed sort of treatise on like how you go through eval step-by-step process. So we start off with um, you know, like what is evaluation. We go through The Three Gulfs framework that we just described. Um, you know, we talk about why you need evals. We motivate that. Uh, then we kind of do a little bit overview of like, okay, the strengths and weaknesses of LLMs. You know, what kinds of things you need to intuitively understand when you're doing evals before you even get to evals. The third chapter is very important. It's probably where we spend a lot of our time in practice and a lot of people we don't know about this step. It's called error analysis. So what is error analysis?\n\nSo you might hear Shreya and I talk a lot about looking at your data. We keep beating people over the head with this phrase, \"Look at your data. Look at your data.\" What does that mean? Look at my data. What data do you look at? Do you look at all your data? Do you just quit your job and look at your data and do nothing else? Do you don't have time? How do you look at your data? And what do you do when you look at that data? Like how do you make sense of it? How do you make it, how is it make it tractable and learn something from your data? What if you don't have any data? Like what data am I talking about? What if you haven't built anything yet? Um, the key thing is like you need to look at some sort of data. Go through a structured process. It's not as painful as it sounds looking at data. It's actually really um beneficial. A lot of my clients, you know, we have a plan to go through this whole process and they get so much value just out of error analysis that they just they're like, \"This is amazing. I'm done.\" I'm like, \"Wait, what do you mean you're done? Like, I can do all this other stuff for you.\" Like, \"No, no, this is great. Like, I, this is like I'm busy for a while. Air analysis has taught me so much.\" And you know error analysis is, you know, to go to error analysis, um, we can scroll through, um, let me see if I can, there's a diagram here, but you know, um, this is this is like one way to generate synthetic queries, for example. Um, it's hard to dive into the details just without the context, but we go through the process of like how generate, you know, synthetic data. Um, you know, and then also we go through like how to look at your data. Um, uh, and, you know, so we describe it in a lot of detail. Um, a lot of it is supplemented with videos. Let me see if I can scroll up. Maybe Shre, you can, uh, yeah, that diagram is pretty good.\n\nYeah, this one.\n\nSo, there's a concept called axial coding or open coding and axial coding. Um, Trey, you want to tell tell us everyone where that comes from and the history behind it?\n\nYeah, definitely. So, um, in creating this curriculum, we took a lot of inspiration from social science research actually, because we were thinking, \"Okay, in what field and domain do people need to look at vast amount of unstructured free form text and labels and come up with meaningful actionable insights out of it?\" Well, turns out social science researchers have done this for a very long time. There's this process called grounded theory, um, that gives this systematic structure to the process. And first, what people do is go through their open-ended data co outputs or whatever it is, they read them and they write free form open codes is what they call it, um, free form notes on what's good, what's bad, any themes that emerge, whatot on a trace-by-trace basis. Then after going through about a hundred of those, they will try to merge similar notes together into clusters, and this determines your failure modes. If you find that a bunch of notes around, you know, a specific type of hallucination get merged together, well, looks like that's a huge problem that you need to solve in your product. Um, and this kind of loop keeps going until what we call theoretical saturation in qualitative research, which means I've not learned any new failure modes. That's all. I keep looking at data and I just keep adding to my existing failure modes. Um, at that point, you can kind of stop and then you can move on to, \"Okay, how am I going to turn those into automated evaluators so I don't have to do it all the time? Maybe I'll build LLM as judges based on my labeled failure modes. Maybe I'll write some codebased evaluators for things that code can check.\" Um, and then we find that, you know, we teach people this this diagram error analysis, and then suddenly they're they're like, \"Oh, this is great, okay, bye.\" It's like, I mean, I guess, right? Like, it it makes sense. This is where most people are actually bottlenecked, right? Because ML, traditional machine learning people didn't have to do this. They didn't, when they looked at their data, they would just look at all these know tables of features, which are all numbers, and the outputs are also numbers. And so there are ways to kind of debug those. LLMs are different because now the inputs are free form text and the outputs are free form text, and people are like, \"I don't know how to, there, it's new techn, it's new, new, new skills that you need to be able to make sense of that.\" Um, but fortunately, once you do this, then you can go and implement automated evaluators as you might with traditional machine learning, um, and then you can do improvement strategies, you can do fine-tuning, all of these things like a traditional machine learning person would.\n\nUm, one thing I want to share that might be useful here or interesting, perhaps, for a product manager. Um, we have Teresa taking our class, and you know, just today she was discussing uh, you know, you know, uh, \"Okay, I love the class too.\" um, you know, when open and axial coding are we analyzing opportunities towards an outcome? And she responds, \"I've been thinking about this a lot.\" um, you know, and then she has some comments about generating synthetic data, you know, but she also recognizes, and this is really fun, this is why we have product managers in our class, because, you know, um, these kind of like tying together of things, like, \"Okay, identifying opportunities from interviews is based on grounded theory,\" also, um, you know, and then she's experimenting with methods for capturing annotations directly from the customer, and you know, she also kind of points out that, \"Okay, these are the scientific method applied, look at the data.\" So, um, I think project managers have a lot to offer here, you know, bringing their customer and user research into the whole process. That's why we think it's really important that they are in the driver's seat of these evals.\n\nI will go even further to say that I think that PMs are like vital in building AI products. Like we're not going to have successful AI products across different domains unless we have good AI PMs. Like I cannot emphasize that enough. Um, without good AI PMs. Like we're just going to have failed AI products. So I hope this is a call to action for PMs. Like this is a skill that you absolutely need to develop. um, it's going to set you apart, obviously, of course, from a career perspective, but also we need this, right, in order to really realize this vision of AI products changing people's lives.\n\nUm, okay, so to get back on, you know, the things to know, so once you do error analysis, now you have a grounded way of knowing like what to focus on, because like the question is like what do you even evaluate? You can come up with like millions of metrics, like hallucination score, toxicity score, conciseness score. You can name these scores all day long and you can get really confused and overwhelmed and say and say, \"Oh my goodness, like I can't do it.\" So error analysis is very\n\n\nImportant.\nUm, you know, you would be surprised.\nIn our Discord channel, almost every other question is answered with the phrase error analysis because, um, you know, one of the main troubles people have with eval is what do you eval?\nYou can, you know, you can eval anything.\nThere's an infinite number of things, ideas of things that can go wrong.\nUm, you know, instead of just being paranoid and sort of, you know, working yourself up about like what can go wrong, you should be grounding it in things that are going wrong or the things that will probably go wrong.\nUm, and that's what error analysis helps you with.\nIt helps you focus on high-value things because evals are not free.\nUm, it takes a little bit of effort, and so there's a this entire time that you're doing eval, there has to be a cost-benefit analysis of around like what do you even, um, should you be emailing, what do you email, whatever.\nAnd so error analysis is the answer to all those questions.\nUm, and error analysis is so valuable that, and you know, a sizable portion of my clients, they sign up with me to go through this whole evaluation process, and they do the air analysis part, and they're like, \"Great, Hamel, we love it, we're done.\"\nLike, this is, I'm like, \"No, wait, I have all this other stuff, like all this other stuff in this table of contests I can take you through.\"\nThey're like, \"No, this is so much value, like we've, we're so happy.\"\nAnd it's true because like, you know, error analysis is like this thing about looking at your data.\nUm, it actually drives you to develop a very deep intuition about your system and what's going wrong with it, and it makes you develop a nose for everything, and, um, kind of gives you a sixth sense of like what is even going to break if you're doing it enough.\nAnd so it's extremely powerful, probably the most powerful technique in eval.\nUm, you know, I can't highlight it more than that.\nUm, but okay, once you move past error analysis, now it's, now you know what is wrong and where to focus and what to prioritize.\nNow, how do you actually go about the process of writing the evals?\nAnd so, that's where these other chapters come in around, okay, um, how, who does the, who writes the evals?\nWho does the annotations?\nSo, we have this, uh, we have some strong guidance in here.\nWe're not, we're not afraid to, you know, cut right to the chase and have opinions.\nAnd so like one opinion that we have, for example, of the many opinions, but I'll just highlight it here, is benevolent dictators.\nSo we say, okay, people always ask like, \"Well, who is going to make the final call of whether or not this is a pass or fail, or who is going to be the one writing the email?\nWho's in charge?\"\nI have a whole team to just involve the whole team.\nThe answer is no.\nYou need to have a benevolent dictator in most situations.\nUm, and there's some, you know, there's some reasoning behind that.\nIt just, you know, it's like the binary classification thing.\nIt forces you to kind of make a decision, and it makes the whole process tractable because it can quickly become intractable if you add various complexity.\nSo going back to the table of context, let me context.\nLet me scroll back up.\nUm, feel free to interrupt me at any time.\nUm, so we talked through implementing, you know, making this a process automated with LM as a judge.\nNow, with LM as a judge, you're writing prompts, and you're asking an LM to grade something.\nNow, almost everybody, I would say 80% of people who are using LM as a judge are just writing a prompt and just praying that is doing the right thing.\nBut that's not what you should do.\nWhen you, the right way to use an LLM as a judge is to label, to get your label data, which you already done in a previous step.\nThat's what we're teaching.\nAnd measure the LM as a judge against your label data to understand, is the judge any good?\nNumber one.\nBut number two, more importantly, you have to iterate on your LM as a judge.\nYou have to see like where is it wrong?\nAnd you're like, \"Oh, it's always the case that not only do you adjust the LM as a judge, but Shreya has a lot of research that shows that the annotator also adjusts their requirements seeing the LLM output.\"\nShreya, you want to talk a little bit about that?\nYeah, I have a paper called \"Who Validates the Validators\" um, list 2024 if people are interested in the venue.\nBut yeah, one of the things we did was we built an interface for people to go and annotate outputs to train LLM judge evaluators.\nAnd then we found that people will keep changing the rubric as they encounter new failure modes when looking at the data.\nAnd this underscores, right, how tricky it is to figure out what the rubric is.\nHow you need to go through this iterative process.\nHow sometimes when the criteria is a little bit too subjective, maybe you want to have multiple people weigh in on what correctness means.\nAnd there are ways to do this like the inner annotative inner annotator agreement metric that we talk about, um, cohence kappa and the course reader, a lot of stuff.\nUm, point is that we just provide a lot of frameworks to really systematically solve the problems when you encounter challenges in coming up with good labels and ground truth.\nUm, and so you know, we teach you a lot of things like also how to, you know, correct the LM as a judge error rate based upon your real error rate, and you know, all kinds of advanced things.\nWe also, uh, teach you how to think about multi-turn evaluations, like, okay, if you have a long conversation, um, that's like multiple turns between an AI and a human, how do you actually evaluate that?\nUm, do you evaluate the entire conversation?\nUm, you know, one piece of advice we have is like, you should, you know, evaluate, you should, when you're doing open coding, for example, you should stop on the first error that you see because these things have causal relationships, and it's usually the case that the first error is the blocking, uh, is the blocking problem.\nAnd so, you know, a simplifying heuristic is to, is to, is to anchor on the first error.\nAnd there's all kinds of heuristics like this that make the entire process tractable.\nUm, but also when it comes to evaluating multi-turn conversation is like, how do you actually have test data sets for that?\nHow do you, how do you simulate in multi-turn conversation, or do you need to simulate a multi-turn conversation?\nThere's a lot of, uh, you know, there's a lot of things to dive in there.\nSo we cover that.\nWe also cover how to evaluate retrieval augmented generation, and so we talk about when to treat certain aspects of the problem like a search problem, when what components of the problem like the generation problem, how to think about that separately, and you know, all the nuances there.\nUm, and then we talk about other specific architectures and components.\nSo like how, how to think about tool calling, what you need to know about agents.\nSo agents are systems that can be very dynamic.\nYou know, they might have trajectories that are unpredictable and do various things that you can't anticipate.\nHow do you evaluate that?\nHow do you tame that monster, that spaghetti?\nIt turns out you can use various analytical tools that simplify the problem and allow you to attack it and make sense of, you know, agentic systems, and we show you, uh, things like how to use transition matrices and other analytical tools to help wrangle that problem.\nUm, and then we talk about, you know, how to evaluate specific input data modalities, like different types of modalities, like not just text.\nUm, then we talk about production, uh, productionizing these things.\nSo like CI/CD, um, you know, kind of automating these and running these at scale.\nAnd then we talk about also kind of, probably my second favorite subject in chapter 10 is interfaces.\nAnd I'll give it to Shreya to talk about the interfaces actually.\nYeah.\nChapter 10 and 11.\nThey're the last week of the course, and they're kind of like special advanced topics that you will, uh, all I can pretty, I think I can guarantee that you won't find them anywhere out there on the internet.\nSo, we wanted to do something special, um, for our students.\nChapter 10 is about how to build, perhaps even vibe code your way to an effective interface that has people really, really labeling things very quickly, maximizing the throughput of how many labels you can get from human reviewers.\nWe talk about principles there to case studies of good, bad interfaces.\nUm, and then chapter 11, I think is about improvement, you know, obvious strategies that you might know about, like decomposing tasks, fine-tuning models, so forth, but also cost optimization and cost improvement.\nA lot of people actually say like, \"Oh, my pipeline is working, but it's too expensive,\" especially if it's working on really long documents.\nUm, how do we keep the same quality but reduce the cost by an order of magnitude or even more?\nWell, we talk about some cost optimization techniques there.\nSo that's a very long answer, I think, to your question on, you know, what's the road map, like here is a road map, we think it's pretty good, a lot of places you can dive in, you can dive into each chapter that you're interested in.\nUm, and chapters 10 and 11 here, I think, are pretty open territory, um, in both in research to explore more as well.\nSo upon on seeing this book, a natural question is, where can I get the book, or are you going to release this book?\nAnd so the answer to that question is, eventually yes, maybe early next year.\nHowever, um, you know, if you want the kind of hands-on approach of, you know, doing this in in a very guided way, then of course, take our course.\nThat's actually what I wanted to ask next.\nHamel, is you were working at Airbnb and GitHub.\nWhy did you transition into consulting and courses?\nGood question.\nUm, it kind of happened somewhat organically.\nUm, you know, I took a sabbatical after GitHub for a bit.\nI worked at some startups, and then I, sorry, I, I, uh, after GitHub, I worked at a few startups, then decided to take a sabbatical.\nUm, and a company decided to contract me, convince me kind of, to the name of the company was Weights and Biases, they convinced me to, uh, do some consulting work for them.\nAnd so I, yeah, I always thought that like I would hate consulting because I did consulting with a large consulting company very early in my career before tech, and and I realized like, hey, I really like it.\nWhen you're doing it on your own, becoming like an independent indie kind of entrepreneur, it's very enjoyable.\nUm, it's a lot of freedom, and I found that a lot of people found it very valuable, like in terms of like helping them sort of navigate LLMs and AI.\nUm, and then I just, yeah, I just started really enjoying it, and one thing led to another, and here I am, like, uh, there's no, let's say, grand plan as such, it's just, uh, kind of, you know, yeah, it just like happened to be in this place, and it happened that everybody building with AI, they're always stuck on eval, exact same problem almost every single time.\nI needed to start with error analysis, and this is like, you know, working with 25, 30 companies.\nUm, you know, and then I just started writing about it a lot, and it was really apparent to me that there's not that much education on this topic, and it's where everyone's struggling.\nUm, you know, and consulting is expensive, and so that's why we created this course to make this way more accessible to a larger audience.\nAnd do you tell people who want to consult you as a consultant to not reach out if they can't spend $38,000?\nYeah.\nSo I think one thing that you have to do as an entrepreneur is to, a lot of different things, but one is like it's important, can be important to have a niche so that you customers know, uh, what you can help them with.\nSo in this case, mine is eval.\nAnd you know, just from a practical perspective, there's a lot of overhead that comes with being an entrepreneur, like you have, you're in charge of your own sales, your own marketing, your own, you know, all the administrative overhead of it, and, um, you know, you, I don't want to be on sales calls all day.\nYou have to qualify people, and you know, the problem has to be painful enough for them to want to solve it.\nAnd so that's just a kind of a basic kind of blocking tackling approach to say, okay, like how do I not drown in Zoom meetings?\nAnd then the course, it costs roughly $2,000, and you have over 600 people in this cohort that me, Teresa, Pavle, other people are participating in.\nSo does that mean you earned over a million dollars on this cohort of the course?\nUm, not, no.\nSo, we didn't quite cross the million-dollar threshold because we gave a lot of stuff away for free.\nA lot of friends.\nUh, we, you know, I let all my friends in.\nBasically, uh, Treyo, we're also very generous with any discounts for people who have any need.\nBasically, if your company can't reimburse it and you don't want to pay fully out of pocket, you know, just email us with how much your company can reimburse, and we are super flexible.\nLike, we want people to join the course.\nWe have a steep price point because we find that we want, we want to attract people who are serious about building AI products and evals, right?\nIt benefits nobody if our course has 10,000 people and everyone's like only mildly interested and serious, right?\nWe want to focus on the people who are going to go and put these techniques into production.\nWe're going to go build products.\nUm, and for that, turns out you need to have a steep price point.\nSo almost, yeah, almost a million.\nI don't want to dodge the question because I know like, whatever, like almost a million.\nIt's like 800k is what it was the first cohort, which is insane.\nThat's mind-blowing, right, especially for the first cohort of a course.\nAnd what's even more insane, I think, is that your next cohort is going to be your last.\nWhy?\nYes.\nSo, I don't want it to be, you know, how you like watch a movie and like the first one is good, and usually like it's really hard to make a good sequel, but this is like really impossible, like to make it like continue to be good.\nAnd honestly, like, um, you know, I don't want to, you know, I have a lot of different interests, but also, um, there's a lot of things that we can do with the course.\nSo we could, for example, you know, you see this like 150-page, this is just a draft, honestly, I mean, we don't, we keep iterating on this on this book that we have open right now.\nUm, so one, you know, one sort of motion is to like, okay, focus time on writing a book, then can you know, it's currently it's a $2,300 course, we're going to actually be increasing the price to $2,500 on Friday.\nUm, you know, can there be a recorded version that doesn't involve us like live, that doesn't involve any office hours or anything else?\nUm, you know, that's like lower cost, okay, that's fine.\nSo we think like there's different products out there that we can offer or different ways that we can offer this, um, but also,\n\n\nLike, you know, this course takes an immense amount of time to deliver, and so one of the things that we like to do is actually build these AI projects. I like to work with customers and build these things. I know Shreya does too. She likes to, you know, do research on this topic. Her research is very applied. She works with companies on this. She builds tools in the space. And it's really, yeah, if we would do the course over and over again like this, we wouldn't be able to do that. It would actually dilute the course. So, we want to keep it special. We want people to feel like, okay, they were here for this course. It's something they could remember.\n\nLast year, I had a course like this. It was not on this; it was on fine-tuning. It was provisionally about fine-tuning and then became like basically a conference, but it was basically the same scale as this course. It was also a $900,000 revenue course in one cohort. And yeah, we decided like we don't want to do that again because how can we possibly recreate that subject and that feeling of that time, you know? We don't want to do a second one and just have it be underwhelming because we try to repeat something. So like, you know, this is different. This is like that was, you know, kind of a conference in a way. It turned into a conference. It was just like everybody about anything about LLMs. It was really fun.\n\nUm, you know, this is like really focused on evals. You think that, okay, there's a lot more repeatability to this, but you know, our goal is not necessarily, you know, trying to milk all the money out of it, per se. We're actually like, you know, all this money that made, we're actually like going to invest it in all these things. So like writing a book, um, you know, doing this additional, like, different kinds of courses that can be delivered in different ways, you know, it's all going to be reinvested into that to make this material accessible, um, because we really believe we have a lot of conviction in this message and this topic and the impact of learning this.\n\nI think there's a lot of lessons there for aspiring and current course creators, but I think one of the most interesting ones is how of the time you've made this course. Like you're addressing the problems that you're seeing with your consulting clients and with AI engineers and AI PMs that they're facing with eval. So you're able to write about it that way. You're not trying to just create timeless content, and I think that makes it really effective.\n\nYeah, I hope so. I mean, uh, you know, people message me, they're like, \"Oh, this is a lot of money for like a month or whatever,\" but then I remind them like, \"We've been writing about this for two years.\" I think Shreya has reviewed all of my blog posts throughout the years. Um, you know, I send it to her and she like reviews it. Uh, you know, I'm really grateful for that. Um, you know, and she's been writing about this for many years as well. Um, and by some stroke of luck, I was able to convince her to nerd snipe her also and say like, \"Okay, let's do this course together.\" Um, and I wouldn't have done it if she said no. It was only like if she does it, then I will do it kind of situation.\n\nUm, you know, because Shreya is, you know, if you look at her writing, it's actually really impressive. Like she...\n\nYeah, she like, um, it's a very good compliment, 'cause I'm like really focused on consulting and stuff like that. I don't have time to survey the field, bring in, you know, like this like generalized perspective and theory, but there's a really good compliment there where Shreya brings to, you know, structuring this material, bringing in like, you know, a lot of the kind of... I mean, I didn't even know what axial coding was. I was just doing axial coding, open coding. I didn't know it had a history in social sciences, so it was really good to know that there's a there's there's actually some like history of this working. So things like that. I mean, um, I don't even know if I'm answering the question anymore.\n\nThere was no question. So that was great. If you were to give advice to somebody who wanted to create an $800, $900,000 course, what would be your advice to them?\n\nYeah. So one is... Okay, so a lot of people ask me this question. Uh, let me try my best.\n\nOne is to really find a niche that you are passionate about. Like I didn't think I was going to create a course around this, honestly. Um, you know, I've been writing about this forever. In fact, evals is like one of the most unpopular things you can possibly write about or talk about. It's like deeply, deeply unpopular.\n\nYeah. I really want to drive that point. Had we talked about something very cool and sexy, like, you know, I don't even... there's so many buzzwords out there, like MCP and like agents and stuff, right? Like that is an easy topic to write a course on because everyone wants to learn how to do it. Eval is something that everybody knows is a problem but doesn't want to go and put in the effort to doing so. I think, yeah, my number one advice would be to find a... if you want to do it the easy way, find a problem that people are excited about going and actually doing the work for, 'cause you could probably 10x what we did for evals if you pick an exciting topic. Um, but Hamel, you should continue.\n\nYeah, I mean, I actually went through several nights or weeks or months or whatever of thinking to myself, \"Man, am I swimming upstream?\" Like, you know, I talk about evals, built... sorry, I built my uh consulting around evals, but like, you know... you know, it's like telling people to eat their vegetables. Um, you know, it's not really that popular.\n\nAnd um, you know, it's much easier to talk about agents, you know, if if you did some like background research, if you try to come out of this perspective, like, \"Hey, I should build a course. I want to like build a course,\" you would never pick evals. You would pick agents. You'd be like, \"Okay, I'm going to sell an agent course. Everyone wants to learn about that.\" People are actively googling. If you do like SEO research, like what are like the keywords? People, like, almost nobody is like searching for evals. There's very little competition for that keyword actually. Um, like if you search for evals, like... Shreya, myself, Eugene, basically our friends, like Shreya, me, and our friends, and I'm talking about our friends, meaning the friends that Shreya and I have in common, we all are come come up on like, you know, at the top of the list. It's pretty insane. It's like that's how niche or quote niche it is. But you know, at some point I just didn't care. I was like, \"We need to create the category.\" So, you know, it's not popular, any make it popular.\n\nSo I don't know if that's good advice. That was my mindset. But I can tell you the only reason that allowed me to get into the mindset is because Shreya partnered with me.\n\nAnd so one thing I would say is like find a good partner to partner with you. So like to get into the mechanics, like, you know, a lot of course sales has to... you know, marketing is very important. You need to let people know that the course exists, otherwise they can't buy the course who don't know. And uh, they not only need to know it exists, they need to know like what it's about. Um, they need to get excited about it. They will need to know like it's good. They need to know like, you know, they need to have some social proof or they need to have, you know, some idea like people like it, they their peers get value out of it, all of these things, and they need to be reminded about it constantly because, you know, I need to be reminded about things constantly for anything. So, um, you know, all of this is very, very important. And so, uh, that's a full-time job, you know, and so if you want a course, you want to sell a course like that, like you have to spend a lot of time doing that. And can you outsource that marketing? You can definitely partner with people on marketing, but you kind of have to drive it because you are the subject matter expert, you're the domain expert. You know, you know, you have to build that authority and that connection with your audience because, you know, whatever.\n\nUm, you know, it's just relentless focus on that. Um, but Hamel is not also talking about all of the work that he spends putting together, you know, like a lot of really great guest speakers, really thoughtfully figuring out, okay, you know, we're talking about X, Y, and Z. So, and so is like the world's leading expert on X. Like why has been applied in these three companies? So let's get somebody from there. Um, this the guest speakers are really diverse and I think really distinguish also our course from a lot of other courses out there. It's not just us monotonously lecturing. It's we've got other people coming in showing you how these things are implemented in practice at big companies, at small companies, at specific domains or whatnot.\n\nAnother thing is focus. Yeah, this is like I get up in the morning, all I think about is the course and how can I let more people know about the course or like, you know, make it better for students. Um, how can I bring more perspective into there, so on and so forth. Yeah. So all that stuff is really important. Um, and just constant experimentation. So kind of going back, it's kind of like a, you know, I'm a data scientist kind of thinking person. I'm just constantly doing experiments and looking at the data. Um, and you know, I send Shreya like probably average of 200 text messages a day or something like that with like different experiments, like, \"Oh, I'm trying this. I'm doing this thing. Look at these numbers. Okay. Like what do you like this marketing copy this here? I put it like you put an ad here. This what h this is what happens. Do you have any ideas for like how we can go about this differently? Whatever.\" Um, you know, hope she has do not disturb on. I hope I do.\n\nBut uh, you know, so uh, yeah, I mean, this like constant experimentation, so it's kind of treating it like how I treat evals to be meta is like, you know, this like iterating on, you know, the business of making the core and the thing that's like really good about making money in a course is like you can... I've hired a lot of my friends, you know, and so like, you know, a lot of my friends are data scientists, machine learning engineers, and you know, a lot of them are involved in one way or the other, either as affiliates or guest speakers or TAs, and I pay all of them. Uh, and it's great. I love paying them, and yeah, it's like really great, like all my friends win too, and that's like the good part is, you know, to create that situation where everybody wins.\n\nWhat a great place to end it. If people want to find you guys online, where should they go?\n\nYeah, we're both active on X. Um, you can pretty much just search us up, you'll find us. I have a blog. Hamel has a blog. Um, email us. Yeah. Nothing super fancy, I think.\n\nWhere do they find your email?\n\nThe best way to do it is just to Google us and our email is at the top of the page.\n\nUm, but I can also... my my email is first name last name shreyashankar.berkeley.edu.\n\nSo, and mine is just get in touch with me at X on X. It's easy, you know. You can send me a DM there. You can message me, whatever, and I'll figure it out.\n\nAmazing. Thank you guys so much for lending your expertise. We'll see everybody in the next one. So, if you want to learn more about how to shift to this way of working, check out our full conversation on Apple or Spotify podcasts. And if you want the actual documents that we showed, the tools and frameworks and public links, be sure to check out my newsletter post with all of the details. Finally, thank you so much for watching. It would really mean a lot if you could make sure you are subscribed on YouTube, following on Apple or Spotify podcasts, and leave us a review on those platforms. That really helps grow the podcast and support our work so that we can do bigger and better productions. I'll see you in the next one.\n",
  "dumpedAt": "2025-07-21T18:43:25.314Z"
}