{
  "episodeId": "7hBG5ShQ2BA",
  "channelSlug": "@sequoiacapital",
  "title": "From DevOps ‘Heart Attacks’ to AI-Powered Diagnostics With Traversal’s AI Agents",
  "publishedAt": "2025-06-24T09:00:08.000Z",
  "rawLines": [
    {
      "lang": "en",
      "text": "If you're in product, if you're in",
      "offset": 0.08,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "design, if you're in core engineering,",
      "offset": 1.36,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "you constantly have to make bets as to",
      "offset": 3.439,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "where AI is going to be six months from",
      "offset": 5.279,
      "duration": 3.121
    },
    {
      "lang": "en",
      "text": "now and you're willing to re-evaluate",
      "offset": 6.64,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "everything six months from now. The good",
      "offset": 8.4,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "news is that it's only going only to get",
      "offset": 10.24,
      "duration": 2.24
    },
    {
      "lang": "en",
      "text": "better. So, it's not like your product's",
      "offset": 11.36,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "going to get worse 6 months from now,",
      "offset": 12.48,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "right? And so, for example, one um",
      "offset": 14.16,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "really interesting thing bet that has",
      "offset": 17.279,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "paid off is Raj and Raz were very",
      "offset": 18.88,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "preient about the fact that uh reasoning",
      "offset": 21.76,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "model is going to get better, right?",
      "offset": 24.4,
      "duration": 3.039
    },
    {
      "lang": "en",
      "text": "They'd made this bet in September,",
      "offset": 25.519,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "right? Just seeing where the world is",
      "offset": 27.439,
      "duration": 2.481
    },
    {
      "lang": "en",
      "text": "going.",
      "offset": 28.88,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "And we architected our system such that",
      "offset": 29.92,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "the reasoning models would get to shine",
      "offset": 32.8,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "and that has really played out dividends",
      "offset": 35.36,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "right now um in the way our entire",
      "offset": 37.2,
      "duration": 5.81
    },
    {
      "lang": "en",
      "text": "architecture is set up.",
      "offset": 39.84,
      "duration": 17.22
    },
    {
      "lang": "en",
      "text": "[Music]",
      "offset": 43.01,
      "duration": 14.05
    },
    {
      "lang": "en",
      "text": "Today we're excited to welcome Anishh",
      "offset": 57.76,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "and Raj, co-founders of Traversal.",
      "offset": 59.44,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "Traversal is building AI agents to",
      "offset": 61.92,
      "duration": 3.519
    },
    {
      "lang": "en",
      "text": "transform the world of DevOps and site",
      "offset": 63.68,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "reliability engineering. Today companies",
      "offset": 65.439,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "have war rooms and armies of DevOps and",
      "offset": 67.92,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "SRRES, troubleshooting their production",
      "offset": 69.84,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "failures and fixing them in the",
      "offset": 71.68,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "codebase. Every minute of production",
      "offset": 72.88,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "downtime is costly and sometimes life or",
      "offset": 74.96,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "death. Keeping a company's application",
      "offset": 77.04,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "running is very hard and valuable work",
      "offset": 79.2,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "and it's a problem that is only getting",
      "offset": 80.88,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "worse with the advent of AI generated",
      "offset": 82.4,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "code and vibe coding. Traversal believes",
      "offset": 84.32,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "that agents are the scalable solution to",
      "offset": 87.04,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "the problem and has seen exciting",
      "offset": 89.04,
      "duration": 3.119
    },
    {
      "lang": "en",
      "text": "results already from the troubleshooter",
      "offset": 90.72,
      "duration": 2.64
    },
    {
      "lang": "en",
      "text": "agents that they've deployed into",
      "offset": 92.159,
      "duration": 3.121
    },
    {
      "lang": "en",
      "text": "production. We're excited to have them",
      "offset": 93.36,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "share more about that vision and their",
      "offset": 95.28,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "results so far.",
      "offset": 96.64,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "Anish and Raj, welcome to training data.",
      "offset": 99.6,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "So wonderful um to have you today. We",
      "offset": 101.84,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "have been working together for a year",
      "offset": 103.84,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "and we can't wait to share with our",
      "offset": 105.52,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "audience how",
      "offset": 107.439,
      "duration": 5.521
    },
    {
      "lang": "en",
      "text": "AI and AI agents can transform the",
      "offset": 110.399,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "worlds of site reliability engineering",
      "offset": 112.96,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "and and DevOps and somehow we need to",
      "offset": 115.36,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "weave negronies in the conversation",
      "offset": 117.92,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "someplace later on but we'll get to",
      "offset": 119.759,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "that. Let's start with some quick hot",
      "offset": 121.52,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "takes. Um",
      "offset": 123.68,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "will DevOps or SR as we know them today",
      "offset": 125.92,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "even exist five years from now?\n Yeah,",
      "offset": 129.84,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "it's a great question. I I think it will",
      "offset": 132,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "but I think it'll look fundamentally",
      "offset": 134.239,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "different from the way it looks now. And",
      "offset": 135.36,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "um I think\n in this world of of DevOps",
      "offset": 138,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "and SR I think healthcare analogies",
      "offset": 141.12,
      "duration": 5.119
    },
    {
      "lang": "en",
      "text": "generally fall quite naturally I find",
      "offset": 143.36,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "and um in this world if you think about",
      "offset": 146.239,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "healthcare and the mass hierarchy of",
      "offset": 148.08,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "needs there imagine um the stage one is",
      "offset": 149.92,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "where let's say you're having a heart",
      "offset": 153.28,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "attack. You have to solve that right",
      "offset": 154.319,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "now. Nothing else matters. Nothing",
      "offset": 156.48,
      "duration": 3.039
    },
    {
      "lang": "en",
      "text": "matters 5 minutes from now other than",
      "offset": 158.08,
      "duration": 2.56
    },
    {
      "lang": "en",
      "text": "solving the heart attack that you're",
      "offset": 159.519,
      "duration": 2.561
    },
    {
      "lang": "en",
      "text": "having. Right? And to me that's",
      "offset": 160.64,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "analogous to dealing with a high",
      "offset": 162.08,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "severity incident.\n Mhm. Um and then",
      "offset": 163.84,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "stage two is where let's say you're",
      "offset": 166.959,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "dealing with some sort of chronic issue",
      "offset": 168.319,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "that you face like you know you have",
      "offset": 170.8,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "sprained ankle or whatever the whatever",
      "offset": 172.4,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "illness that might be affecting you and",
      "offset": 175.519,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "it's very hard to plan 3 months in",
      "offset": 177.92,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "advance because that's something that's",
      "offset": 179.68,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "just affecting you every day right and I",
      "offset": 181.04,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "think that's the analogous thing you do",
      "offset": 183.599,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "in in DevOps is dealing with streams of",
      "offset": 185.599,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "alerts streams of of checking whether a",
      "offset": 188.239,
      "duration": 5.441
    },
    {
      "lang": "en",
      "text": "deployment was safe or not safe right",
      "offset": 190.959,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "and and then stage Three is where you",
      "offset": 193.68,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "know I think of it as like life hacking",
      "offset": 195.92,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "where you're thinking about how do I",
      "offset": 197.84,
      "duration": 3.119
    },
    {
      "lang": "en",
      "text": "optimize my sleep and my nutrients so",
      "offset": 199.12,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "that I have a high quality life that's",
      "offset": 200.959,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "you know and and fulfilling life right",
      "offset": 203.519,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "and I think that's the equivalent of",
      "offset": 205.68,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "like planning out what your next 5 years",
      "offset": 207.2,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "of infrastructure look like and how you",
      "offset": 209.2,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "invest in the right places now if I",
      "offset": 210.799,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "think of what unfortunately people in",
      "offset": 212.72,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "the devops sur on call engineering space",
      "offset": 214.64,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "uh live like right now it's like having",
      "offset": 217.92,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "a heart attack twice a week\n and dealing",
      "offset": 219.68,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "with a debilitating chronic condition",
      "offset": 222.319,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "that you have to handle every day.\n As",
      "offset": 223.92,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "I'm listening to you, I'm just trying to",
      "offset": 226.239,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "picture all the people who I know in",
      "offset": 227.68,
      "duration": 6.479
    },
    {
      "lang": "en",
      "text": "DevOps and SRE in scrubs or wearing CGI",
      "offset": 230.48,
      "duration": 5.839
    },
    {
      "lang": "en",
      "text": "monitors to life hack like you know",
      "offset": 234.159,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "their infrastructure.",
      "offset": 236.319,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "Uh and I think people don't realize it",
      "offset": 238.879,
      "duration": 3.521
    },
    {
      "lang": "en",
      "text": "but when you when you make this",
      "offset": 241.04,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "connection to healthcare you just",
      "offset": 242.4,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "realize how what we've gotten used to",
      "offset": 243.92,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "and what life really should be like in",
      "offset": 245.84,
      "duration": 3.039
    },
    {
      "lang": "en",
      "text": "this world as we think about the",
      "offset": 247.519,
      "duration": 2.64
    },
    {
      "lang": "en",
      "text": "healthcare of large scale software",
      "offset": 248.879,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "systems. And so I think if traversal",
      "offset": 250.159,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "does its job right then that first stage",
      "offset": 252.72,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "and second stage of needs which is uh",
      "offset": 255.04,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "there's really high severity incidents",
      "offset": 257.6,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "and that constant pain by death by a",
      "offset": 259.68,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "thousand cuts should be something that",
      "offset": 262.16,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "AI and AI agents take care of and DevOps",
      "offset": 263.68,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "and SR get to deal with the creative fun",
      "offset": 266.639,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "parts of what is it my infrastructure",
      "offset": 268.32,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "should look like for the next year for",
      "offset": 269.84,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "the next 5 years and it becomes a much",
      "offset": 271.44,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "more fulfilling job\n and many engineering",
      "offset": 272.88,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "teams are nowadays um adopting",
      "offset": 275.44,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "autonomous coding things like cursor",
      "offset": 277.84,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "Windsurf",
      "offset": 280.4,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "do you think that's going to have",
      "offset": 282.4,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "profound impact on how people maintain",
      "offset": 283.84,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "the reliability of their infrastructure",
      "offset": 286.479,
      "duration": 6.321
    },
    {
      "lang": "en",
      "text": "or will it be the case that AI will have",
      "offset": 288.8,
      "duration": 6.399
    },
    {
      "lang": "en",
      "text": "a major role to play in fulfilling that",
      "offset": 292.8,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "vision moving people away from like",
      "offset": 295.199,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "being the intensive care unit surgeons",
      "offset": 298.16,
      "duration": 6.64
    },
    {
      "lang": "en",
      "text": "to being more thoughtful planners. So I",
      "offset": 301.199,
      "duration": 5.041
    },
    {
      "lang": "en",
      "text": "think it's going to be there's a",
      "offset": 304.8,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "short-term answer and there's a",
      "offset": 306.24,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "long-term answer and I think it becomes",
      "offset": 307.6,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "in the short term at least it's a tale",
      "offset": 309.919,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "of two worlds I think. So um there's",
      "offset": 311.52,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "this because of everything happening in",
      "offset": 315.199,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "the world of of cursor and wind surf and",
      "offset": 316.56,
      "duration": 5.359
    },
    {
      "lang": "en",
      "text": "so on so forth. I think the idea of vibe",
      "offset": 318.8,
      "duration": 4.959
    },
    {
      "lang": "en",
      "text": "coding obviously has become very popular",
      "offset": 321.919,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "right and we can write a prompt or a few",
      "offset": 323.759,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "prompts and have something stood up that",
      "offset": 325.919,
      "duration": 5.441
    },
    {
      "lang": "en",
      "text": "people can use and play with. uh but I",
      "offset": 328.32,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "think the analogy here is of of fast",
      "offset": 331.36,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "fashion where you try something on you",
      "offset": 333.28,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "like it you don't like it you discard it",
      "offset": 336.08,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "you know start the wear the next thing",
      "offset": 338.08,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "right and I think in that world actually",
      "offset": 339.919,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "reliability doesn't really matter",
      "offset": 342,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "because you don't really need to take",
      "offset": 343.28,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "care of what you've created there's no",
      "offset": 344.8,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "craft to it you create it you throw it",
      "offset": 346.16,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "away right and I think um but there's",
      "offset": 347.84,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "another world which is where you start",
      "offset": 351.44,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "applying these AI powered software",
      "offset": 352.8,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "engineering techniques to mission",
      "offset": 354.96,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "critical systems in payments in",
      "offset": 356.96,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "financial institution ions in security",
      "offset": 359.52,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "infrastructure streaming and in that",
      "offset": 361.36,
      "duration": 5.679
    },
    {
      "lang": "en",
      "text": "world I think it's going to lead to a",
      "offset": 363.919,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "major issue. It probably already is and",
      "offset": 367.039,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "it's only going to get worse in my",
      "offset": 368.88,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "opinion um because everyone we've seen",
      "offset": 370.24,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "this with the enterprises we work with",
      "offset": 373.44,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "everyone is using uh AI powered software",
      "offset": 374.8,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "engineering tools to guide them as they",
      "offset": 376.8,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "write code and what you actually find is",
      "offset": 378.72,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "that it actually even passes their local",
      "offset": 380.72,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "unit test. So in that local piece of",
      "offset": 382.4,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "code, everything looks perfect, right?",
      "offset": 384.639,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "The problem is that in large scale",
      "offset": 386.72,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "enterprises, things break when different",
      "offset": 387.919,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "pieces of your system interact in a way",
      "offset": 390.56,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "you just didn't realize or you didn't",
      "offset": 392.08,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "couldn't foresee. And when that happens,",
      "offset": 393.36,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "because all of this code is being",
      "offset": 395.919,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "written by an AI system, it's very hard",
      "offset": 397.12,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "to debug it because you just don't have",
      "offset": 399.199,
      "duration": 2.481
    },
    {
      "lang": "en",
      "text": "the context anymore. You didn't write it",
      "offset": 400.4,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "anymore. And I think in that world,",
      "offset": 401.68,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "unless we find ways of using AI systems",
      "offset": 403.52,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "to do software maintenance, we're going",
      "offset": 405.6,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "to be throttled. and either people will",
      "offset": 407.28,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "disallow AI software engineering tools",
      "offset": 409.199,
      "duration": 3.521
    },
    {
      "lang": "en",
      "text": "to be used because there's too much",
      "offset": 411.12,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "downtime um or something of that form",
      "offset": 412.72,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "right and I think in that world uh we're",
      "offset": 415.919,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "going to need new tools and new um",
      "offset": 418.16,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "software to help maintainance of of such",
      "offset": 421.199,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "systems and I think that's where",
      "offset": 423.52,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "traversing can can play a part\n you",
      "offset": 424.96,
      "duration": 3.519
    },
    {
      "lang": "en",
      "text": "definitely have your way with with words",
      "offset": 426.8,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "and analogy so I love uh the tale of two",
      "offset": 428.479,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "worlds you either have Louis Vuitton",
      "offset": 431.28,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "high fashion or you have fast fast",
      "offset": 433.039,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "fashion with sheen or or or or or",
      "offset": 435.599,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "something like that. Um but I think we",
      "offset": 438,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "jumped um into uh into the deep end",
      "offset": 440.24,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "maybe like a bit too quickly. Let's",
      "offset": 442.56,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "let's go back a few steps and maybe um",
      "offset": 444.24,
      "duration": 6.399
    },
    {
      "lang": "en",
      "text": "explain for our audience like what",
      "offset": 448.16,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "actually uh root cause analysis stand",
      "offset": 450.639,
      "duration": 5.761
    },
    {
      "lang": "en",
      "text": "for like what what does it do? Tell us a",
      "offset": 453.36,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "bit more about this wonderful world of",
      "offset": 456.4,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "of troubleshooting and root cause",
      "offset": 457.84,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "analysis.\n Yeah. So I think um if I just",
      "offset": 459.52,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "think about the tale of the life cycle",
      "offset": 462.639,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "of an incident, right? the story of an",
      "offset": 464.16,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "incident. It always surprisingly looks",
      "offset": 465.599,
      "duration": 5.361
    },
    {
      "lang": "en",
      "text": "the same across companies.\n Uh a customer",
      "offset": 467.84,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "will log on to a platform, things not",
      "offset": 470.96,
      "duration": 5.359
    },
    {
      "lang": "en",
      "text": "working exactly right. Um either it's",
      "offset": 474,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "already been recorded or they make a",
      "offset": 476.319,
      "duration": 3.121
    },
    {
      "lang": "en",
      "text": "complaint to customer support. Customer",
      "offset": 477.599,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "support looks at it and says, &quot;Okay,",
      "offset": 479.44,
      "duration": 2.879
    },
    {
      "lang": "en",
      "text": "it's not a user error, it's an actual",
      "offset": 480.639,
      "duration": 3.521
    },
    {
      "lang": "en",
      "text": "issue.&quot; It escalates in this game of",
      "offset": 482.319,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "telephone up like five ladders of of",
      "offset": 484.16,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "different sophistication of engineering",
      "offset": 486.319,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "organizations uh within an enterprise.",
      "offset": 488.16,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "At some point it hits the DevOps SR team",
      "offset": 491.12,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "who look at the issue and decide is this",
      "offset": 493.36,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "is this worthy of an incident based on",
      "offset": 496.4,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "the impact of it based on the severity",
      "offset": 498.56,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "of it the immediiacy of it and if they",
      "offset": 500.08,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "decide it is worthy of an incident then",
      "offset": 502.639,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "suddenly chaos ensues right you'll have",
      "offset": 504.4,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "a slack incident channel that's created",
      "offset": 506.24,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "there's 30 to 50 people in a channel",
      "offset": 508,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "everyone's kind of implicitly blaming",
      "offset": 510.479,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "each other for what happened um and a",
      "offset": 512.24,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "bit of a who done it type thing and",
      "offset": 515.039,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "almost without fail is the same thing",
      "offset": 518.08,
      "duration": 2.959
    },
    {
      "lang": "en",
      "text": "that happens which is there'll be this",
      "offset": 519.76,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "10x engineer who's been at the company",
      "offset": 521.039,
      "duration": 6.12
    },
    {
      "lang": "en",
      "text": "for\n got the Christie",
      "offset": 523.2,
      "duration": 3.959
    },
    {
      "lang": "en",
      "text": "uh out of Sherlock Holmes of the",
      "offset": 527.519,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "situation and like aha like they'll",
      "offset": 529.839,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "figure out exactly what happened right",
      "offset": 532.32,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "and not clear how they got there but",
      "offset": 534.8,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "they got there you roll back the the you",
      "offset": 536.399,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "come up with some sort of hot fix and",
      "offset": 539.44,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "then everyone kind of it looks for an uh",
      "offset": 541.279,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "a longer term fix over time right and",
      "offset": 544,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "that's typically how it all plays out",
      "offset": 545.68,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "and that whole flow is what we call",
      "offset": 547.68,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "um the life cycle of an incident and",
      "offset": 550.64,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "root cause analysis in particular,",
      "offset": 552.24,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "right? Um and I think what I find",
      "offset": 554.16,
      "duration": 5.679
    },
    {
      "lang": "en",
      "text": "incredible is that all of observability,",
      "offset": 556.959,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "right, this these incredible companies",
      "offset": 559.839,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "have been created, um they're the second",
      "offset": 561.36,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "largest software spend typically",
      "offset": 564,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "companies have after your cloud spend",
      "offset": 565.36,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "and yet we're still at the state of root",
      "offset": 567.2,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "cause analysis. Um but anyways, that",
      "offset": 569.279,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "that's my take on what RCA looks like",
      "offset": 571.279,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "nowadays.\n And tell us maybe like a bit",
      "offset": 573.04,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "more color on where does root cause",
      "offset": 574.959,
      "duration": 6.641
    },
    {
      "lang": "en",
      "text": "analysis live. uh with relations to the",
      "offset": 576.959,
      "duration": 7.521
    },
    {
      "lang": "en",
      "text": "plethora of observability tools that",
      "offset": 581.6,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "companies use today. Why is it that if",
      "offset": 584.48,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "still the case if this is the number two",
      "offset": 587.36,
      "duration": 5.919
    },
    {
      "lang": "en",
      "text": "spend in technology why is it that you",
      "offset": 589.68,
      "duration": 6.4
    },
    {
      "lang": "en",
      "text": "have you know 50 people like in a slack",
      "offset": 593.279,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "channel with a who done it kind of plot.",
      "offset": 596.08,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "It speaks to the the importance of the",
      "offset": 598.959,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "problem right and what observability has",
      "offset": 600.88,
      "duration": 3.519
    },
    {
      "lang": "en",
      "text": "done is the best they could have done",
      "offset": 602.88,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "given the technology that was available",
      "offset": 604.399,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "even like six months ago right which is",
      "offset": 606.8,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "so if I think what observability is it's",
      "offset": 609.2,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "fundamentally about the creation of",
      "offset": 610.8,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "telemetry data it's called melt data so",
      "offset": 612.48,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "like metrics and events and logs and",
      "offset": 614.32,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "traces so melt for short and it's about",
      "offset": 615.839,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "the creation of this data storing it and",
      "offset": 618.72,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "then providing a nice visualization",
      "offset": 620.959,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "layer on top of it so people can create",
      "offset": 622.48,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "the right slices to create little",
      "offset": 624.16,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "eyeballs in different parts of their",
      "offset": 626,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "system Right. And I think that's what",
      "offset": 627.44,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "observatory ends at which is a storage",
      "offset": 629.12,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "and visualization layer because that's",
      "offset": 630.64,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "all that could have been done. Right.",
      "offset": 632.32,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "And um the the complex workflow of",
      "offset": 634.16,
      "duration": 6.239
    },
    {
      "lang": "en",
      "text": "troubleshooting remains super manual\n and",
      "offset": 637.68,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "that just because that was what um",
      "offset": 640.399,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "that's all we could have done so far. Um",
      "offset": 642.56,
      "duration": 6.56
    },
    {
      "lang": "en",
      "text": "and I think the promise of what these AI",
      "offset": 644.48,
      "duration": 6.96
    },
    {
      "lang": "en",
      "text": "agent companies can do is that they",
      "offset": 649.12,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "automate complex workflows that we do on",
      "offset": 651.44,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "top of software. And to me this problem",
      "offset": 653.76,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "of troubleshooting root cause analysis",
      "offset": 655.6,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "is one of the most complex workflows",
      "offset": 657.04,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "that any set of humans do in software.",
      "offset": 658.48,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "And I think now there's an opportunity",
      "offset": 660.56,
      "duration": 3.519
    },
    {
      "lang": "en",
      "text": "to kind of move up uh in terms of",
      "offset": 661.839,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "sophistication where we use AI systems",
      "offset": 664.079,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "to actually automate this workflow",
      "offset": 666.079,
      "duration": 3.521
    },
    {
      "lang": "en",
      "text": "versus just stopping at the storage and",
      "offset": 667.92,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "visualization layer.\n Can you talk about",
      "offset": 669.6,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "your product? Uh what is the agent or",
      "offset": 671.92,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "the agents that you're building to kind",
      "offset": 674.16,
      "duration": 3.119
    },
    {
      "lang": "en",
      "text": "of take on this problem and how does it",
      "offset": 675.76,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "work?\n Yeah. And if I double agents",
      "offset": 677.279,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "um so maybe stepping back what what is",
      "offset": 681.6,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "an agent and it's really an LLM",
      "offset": 683.519,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "orchestration of tools and in our world",
      "offset": 685.839,
      "duration": 5.761
    },
    {
      "lang": "en",
      "text": "those tools might be data fetching tools",
      "offset": 688.32,
      "duration": 6.32
    },
    {
      "lang": "en",
      "text": "how to get logs how to get metrics back",
      "offset": 691.6,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "data processing how to format it in a",
      "offset": 694.64,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "way that the agent can really process or",
      "offset": 696.8,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "statistical tools like running anomaly",
      "offset": 699.04,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "detection\n and I think really what we're",
      "offset": 701.36,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "trying to look for here is you need to",
      "offset": 704.64,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "define the tools rich enough so that an",
      "offset": 707.12,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "RCA can be expressed as some combination",
      "offset": 709.44,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "or some sequence of these tool calls.",
      "offset": 711.92,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "And that's really where a lot of the",
      "offset": 715.2,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "complexities and the multi- aents come",
      "offset": 717.36,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "in is how do you piece together these",
      "offset": 719.519,
      "duration": 5.521
    },
    {
      "lang": "en",
      "text": "tools to solve these complex tasks and I",
      "offset": 721.519,
      "duration": 6.961
    },
    {
      "lang": "en",
      "text": "think at least in our world the data is",
      "offset": 725.04,
      "duration": 5.359
    },
    {
      "lang": "en",
      "text": "so big that it needs to be sequential",
      "offset": 728.48,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "because a single trace might not even",
      "offset": 730.399,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "fit an LLM context. So you need to know",
      "offset": 732.639,
      "duration": 6.161
    },
    {
      "lang": "en",
      "text": "how to slowly just build this context of",
      "offset": 735.2,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "the LM to get to the root cause and",
      "offset": 738.8,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "that's why it's really challenging of a",
      "offset": 740.8,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "problem.\n Are you mirroring the cognitive",
      "offset": 742.56,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "architecture of a of a human",
      "offset": 745.36,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "troubleshooter? Is it is it you know is",
      "offset": 747.2,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "a map of operations very similar to what",
      "offset": 749.279,
      "duration": 2.961
    },
    {
      "lang": "en",
      "text": "a human would be doing or is it very",
      "offset": 750.959,
      "duration": 3.281
    },
    {
      "lang": "en",
      "text": "different for an agent? Yeah, I think",
      "offset": 752.24,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "that's where at least when we were first",
      "offset": 754.24,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "thinking of the problem, we tried to",
      "offset": 756,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "mimic how an S sur would debug and it",
      "offset": 758.24,
      "duration": 5.279
    },
    {
      "lang": "en",
      "text": "was very manual, very sequential where",
      "offset": 760.639,
      "duration": 6.241
    },
    {
      "lang": "en",
      "text": "an S sur typically might look at a piece",
      "offset": 763.519,
      "duration": 5.601
    },
    {
      "lang": "en",
      "text": "of evidence and then okay, figure out",
      "offset": 766.88,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "what's the next piece of evidence to",
      "offset": 769.12,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "look. But a lot of times they make these",
      "offset": 770.639,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "hops with system knowledge which the",
      "offset": 773.04,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "agent might not know. And that's really",
      "offset": 775.519,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "where we used a lot more scale to figure",
      "offset": 777.92,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "out how to bypass some of this issue of",
      "offset": 780.959,
      "duration": 6.641
    },
    {
      "lang": "en",
      "text": "system knowledge. So this agent will",
      "offset": 784.16,
      "duration": 6.32
    },
    {
      "lang": "en",
      "text": "basically look in a more systematic",
      "offset": 787.6,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "fashion of there's this Google S",
      "offset": 790.48,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "handbook which tells you here's the key",
      "offset": 792.32,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "golden signals you should be looking at",
      "offset": 794.48,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "latency error rate. So it looks at this",
      "offset": 796.079,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "way of kind of health monitoring to",
      "offset": 799.12,
      "duration": 5.839
    },
    {
      "lang": "en",
      "text": "piece together how to sequentially",
      "offset": 801.839,
      "duration": 5.761
    },
    {
      "lang": "en",
      "text": "search this rich space and it's less",
      "offset": 804.959,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "about having holes in the reasoning",
      "offset": 807.6,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "where a human can just immediately get",
      "offset": 809.6,
      "duration": 5.359
    },
    {
      "lang": "en",
      "text": "maybe to that hop. The agent is making",
      "offset": 812.48,
      "duration": 5.359
    },
    {
      "lang": "en",
      "text": "these sequential flows that get to the",
      "offset": 814.959,
      "duration": 5.361
    },
    {
      "lang": "en",
      "text": "answer.\n Are there particular conditions",
      "offset": 817.839,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "or environments where this agogentic",
      "offset": 820.32,
      "duration": 6.079
    },
    {
      "lang": "en",
      "text": "approach works better or worse? Yeah, I",
      "offset": 822.959,
      "duration": 6.481
    },
    {
      "lang": "en",
      "text": "think um fundamentally it's about data",
      "offset": 826.399,
      "duration": 5.761
    },
    {
      "lang": "en",
      "text": "access. So what we have found and",
      "offset": 829.44,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "surprisingly that's what we've found is",
      "offset": 832.16,
      "duration": 5.119
    },
    {
      "lang": "en",
      "text": "that we sometimes can add less value in",
      "offset": 833.68,
      "duration": 5.599
    },
    {
      "lang": "en",
      "text": "like a series A startup versus as a",
      "offset": 837.279,
      "duration": 3.521
    },
    {
      "lang": "en",
      "text": "large enterprise because when you become",
      "offset": 839.279,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "a large enterprise\n your um observability",
      "offset": 840.8,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "is quite mature. So everything is being",
      "offset": 844.16,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "instrumented in a way where the",
      "offset": 846,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "fundamental data is in place\n but your",
      "offset": 847.68,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "teams are very fragmented. So no one",
      "offset": 850.48,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "team or no one person has enough context",
      "offset": 852.16,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "to piece together what everything about",
      "offset": 854,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "how to debug, right? And that's why you",
      "offset": 855.839,
      "duration": 3.281
    },
    {
      "lang": "en",
      "text": "have these 30 or 50 people in a in a",
      "offset": 857.6,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "Slack incident room. And so what we",
      "offset": 859.12,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "fundamentally found is that um the place",
      "offset": 861.04,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "where we add the most value is when the",
      "offset": 863.36,
      "duration": 5.599
    },
    {
      "lang": "en",
      "text": "reasoning steps for this agent to go",
      "offset": 866.8,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "from a high level trigger to the root",
      "offset": 868.959,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "cause, those steps can be found in the",
      "offset": 871.04,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "data, right? It just is too much data",
      "offset": 873.44,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "for any one human to keep in their head.",
      "offset": 875.36,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "uh when that data is fundamentally not",
      "offset": 877.76,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "there then it's when the agent suffers",
      "offset": 879.519,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "and that tends to not be the case at the",
      "offset": 881.92,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "enterprise is what we found.\n Very",
      "offset": 884,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "interesting and and really like somewhat",
      "offset": 886.24,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "counterintuitive because it's usually",
      "offset": 887.839,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "the case with startups that the way you",
      "offset": 890.24,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "choose your design partners or customers",
      "offset": 892,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "you start at smaller companies or",
      "offset": 893.839,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "mid-market and then you try to graduate",
      "offset": 896.16,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "your way up to um up to enterprise.",
      "offset": 898.639,
      "duration": 7.44
    },
    {
      "lang": "en",
      "text": "And if we're going to use like the L1 to",
      "offset": 902.32,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "L5",
      "offset": 906.079,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "like analogy from self-driving cars, um",
      "offset": 907.92,
      "duration": 8.88
    },
    {
      "lang": "en",
      "text": "how close are you uh with having agents",
      "offset": 911.839,
      "duration": 7.521
    },
    {
      "lang": "en",
      "text": "correctly get to the root cause of of",
      "offset": 916.8,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "incidents? Or maybe more broadly like",
      "offset": 919.36,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "what what counts as success? Like are we",
      "offset": 921.279,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "trying to get to 100%",
      "offset": 923.839,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "correct resolution or what even counts",
      "offset": 926.16,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "as as correct resolution like in this",
      "offset": 928.56,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "context? Yeah, that's a great question.",
      "offset": 930.8,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "So, I think there's really two cases",
      "offset": 933.04,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "here. There's the first case where the",
      "offset": 935.199,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "root cause fundamentally belongs in the",
      "offset": 937.519,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "data. So, it's in some log. It's in some",
      "offset": 939.519,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "PR. And in that case, I would say with",
      "offset": 942,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "traversal, we're at L4. And I won't say",
      "offset": 944.48,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "L5 because for us, we might be able to",
      "offset": 947.36,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "flag that problematic PR or that smoking",
      "offset": 949.68,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "gun log, but then there's still that",
      "offset": 953.04,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "last smile of the fix and the",
      "offset": 955.04,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "remediation. And in cases where the fix",
      "offset": 956.8,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "is not so localized to a specific file",
      "offset": 959.759,
      "duration": 5.681
    },
    {
      "lang": "en",
      "text": "or code change where you need to new do",
      "offset": 962.72,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "basically a bigger systemwide change",
      "offset": 965.44,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "that's we haven't gotten there but I",
      "offset": 968.48,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "think that's where it's really exciting",
      "offset": 970.32,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "to see all the developments with code",
      "offset": 971.68,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "agents because that's where we can get",
      "offset": 973.519,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "to that L5. I think for places where the",
      "offset": 975.759,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "root cause isn't in the data that's",
      "offset": 979.279,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "where we're more at a L2. I think",
      "offset": 981.519,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "traversal finds a lot of the important",
      "offset": 984.079,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "symptoms that really help people debug.",
      "offset": 986.639,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "But sometimes",
      "offset": 989.279,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "it's just not in that data and this",
      "offset": 991.279,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "human kind of needs to make those",
      "offset": 993.36,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "additional hops to get to the actual",
      "offset": 995.92,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "root cause. But still the symptoms",
      "offset": 998,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "really help figure out how to make those",
      "offset": 999.68,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "additional hops. And sometimes right for",
      "offset": 1001.92,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "us when we notice that we can tell",
      "offset": 1004.32,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "customers well maybe you should",
      "offset": 1006.24,
      "duration": 3.519
    },
    {
      "lang": "en",
      "text": "instrument in this way to make the",
      "offset": 1008,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "system more observable\n across the AI",
      "offset": 1009.759,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "landscape right now there's really",
      "offset": 1012,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "interesting kind of AI native companies",
      "offset": 1014,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "being formed like yourselves there's",
      "offset": 1016.079,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "also the incumbents that are you know in",
      "offset": 1017.6,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "many cases not asleep at the at the",
      "offset": 1019.519,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "wheel um how how do you think about the",
      "offset": 1021.839,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "incumbent risk here and and why your",
      "offset": 1024.079,
      "duration": 3.521
    },
    {
      "lang": "en",
      "text": "customers are choosing to go with you",
      "offset": 1025.919,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "yeah I think it's a great question I",
      "offset": 1027.6,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "think the heart of it is observability",
      "offset": 1029.679,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "is expensive, right? It's such an",
      "offset": 1032.64,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "expensive product that people pay for.",
      "offset": 1034.64,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "As a result, it's extremely fragmented,",
      "offset": 1036.64,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "right? You go to any big enterprise,",
      "offset": 1039.679,
      "duration": 2.801
    },
    {
      "lang": "en",
      "text": "they're using data dog, they're using",
      "offset": 1041.12,
      "duration": 2.719
    },
    {
      "lang": "en",
      "text": "Splunk, they're using Datrace, they're",
      "offset": 1042.48,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "using Elastic, they're using Graphana,",
      "offset": 1043.839,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "they're using Service Now. I mean,",
      "offset": 1045.839,
      "duration": 2.641
    },
    {
      "lang": "en",
      "text": "they're using everything, right? And all",
      "offset": 1046.959,
      "duration": 3.281
    },
    {
      "lang": "en",
      "text": "of them are like encroaching in this",
      "offset": 1048.48,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "game of attrition, right? And the",
      "offset": 1050.24,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "problem is that if you just think about",
      "offset": 1053.039,
      "duration": 2.561
    },
    {
      "lang": "en",
      "text": "the pricing models in this world, it's",
      "offset": 1054.16,
      "duration": 2.56
    },
    {
      "lang": "en",
      "text": "all based on the amount of data they",
      "offset": 1055.6,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "store,\n right? And so as a result,",
      "offset": 1056.72,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "company A is not incentivized to give",
      "offset": 1058.799,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "you better insights from anything that's",
      "offset": 1060.96,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "stored in company B, right? And to debug",
      "offset": 1062.4,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "something, if you just look at any of",
      "offset": 1065.679,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "the the S sur or on call engineers,",
      "offset": 1067.12,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "they're calling upon all five six tools",
      "offset": 1069.28,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "they have access to. And it's that",
      "offset": 1071.28,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "fragmentation of this historical",
      "offset": 1073.28,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "industry, I think, which is going to",
      "offset": 1074.88,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "ideally lead to companies such as",
      "offset": 1076.4,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "ourselves that are somewhat agnostic to",
      "offset": 1078.08,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "where the data is stored at least for",
      "offset": 1080.64,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "now. Uh it gives us the chance. Do you",
      "offset": 1081.84,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "have customers deployed on traversal",
      "offset": 1084.4,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "today and what have you found is the",
      "offset": 1086,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "difference between what you kind of",
      "offset": 1088.24,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "expected even coming from an academic",
      "offset": 1089.44,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "background versus what you're actually",
      "offset": 1091.36,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "finding in real world environments?",
      "offset": 1092.48,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "Yeah, it's been it's been quite a",
      "offset": 1094.4,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "journey. I think when we started as one",
      "offset": 1096.16,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "should typically do, we started with",
      "offset": 1098.72,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "like very small companies and um and",
      "offset": 1100.24,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "built something that um worked for them",
      "offset": 1103.76,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "and that typically took the form of we",
      "offset": 1106.08,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "went through the last 100 incidents that",
      "offset": 1108.88,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "they had right in the Slack channels and",
      "offset": 1110.64,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "kind of try to use our brains to figure",
      "offset": 1112.48,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "out what is the meta workflow of how",
      "offset": 1114.4,
      "duration": 3.519
    },
    {
      "lang": "en",
      "text": "they always debug an incident in that",
      "offset": 1115.76,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "particular company right and in that",
      "offset": 1117.919,
      "duration": 5.361
    },
    {
      "lang": "en",
      "text": "situation a very popular framework for",
      "offset": 1121.12,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "agents is something called the React",
      "offset": 1123.28,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "framework right and that as a general",
      "offset": 1124.559,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "idea worked. We could somehow imbue the",
      "offset": 1126.96,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "meta workflow into a react agent system",
      "offset": 1129.039,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "and it was able to do really a really",
      "offset": 1132,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "good job. Um and then at some point we",
      "offset": 1133.44,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "started you know working towards larger",
      "offset": 1135.84,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "companies that had an actual like large",
      "offset": 1137.84,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "scale observability system thousands of",
      "offset": 1139.679,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "microservices that kind of situation and",
      "offset": 1142.72,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "I think I remember there's very clearly",
      "offset": 1145.919,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "this one week where it was the first",
      "offset": 1147.6,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "time we dealt with that kind of scale",
      "offset": 1149.28,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "and um the second we tried to apply our",
      "offset": 1151.52,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "system on on just some historical",
      "offset": 1154.72,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "instant that happened our accuracy was",
      "offset": 1156.64,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "at 0% and we just it would not move",
      "offset": 1158.88,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "whatever we did to the prompts whatever",
      "offset": 1161.679,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "we did to anything. It was stubbornly at",
      "offset": 1163.36,
      "duration": 6.4
    },
    {
      "lang": "en",
      "text": "0%. And that was a a rude awakening. Uh",
      "offset": 1165.679,
      "duration": 9.801
    },
    {
      "lang": "en",
      "text": "I remember Raj and I had had a negro me.",
      "offset": 1169.76,
      "duration": 5.72
    },
    {
      "lang": "en",
      "text": "Um and I think at that point we kind of",
      "offset": 1175.52,
      "duration": 6.96
    },
    {
      "lang": "en",
      "text": "had to think through ways of we made",
      "offset": 1179.12,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "some interesting decisions which played",
      "offset": 1182.48,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "out uh in our in our favor which is we",
      "offset": 1184.16,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "said okay nothing about a specific",
      "offset": 1186.64,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "company will be hardcoded into the",
      "offset": 1188.64,
      "duration": 8.159
    },
    {
      "lang": "en",
      "text": "prompts right um and nothing about a",
      "offset": 1192.72,
      "duration": 6.079
    },
    {
      "lang": "en",
      "text": "workflow that humans do there will be",
      "offset": 1196.799,
      "duration": 5.281
    },
    {
      "lang": "en",
      "text": "hardcoded into the agentic like workflow",
      "offset": 1198.799,
      "duration": 5.441
    },
    {
      "lang": "en",
      "text": "and and that complexity has to go",
      "offset": 1202.08,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "somewhere All right. And eventually",
      "offset": 1204.24,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "where it went to is computation which in",
      "offset": 1206.559,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "the form in our world takes the form of",
      "offset": 1208.24,
      "duration": 2.799
    },
    {
      "lang": "en",
      "text": "spending tokens in the problem like",
      "offset": 1209.76,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "using it at inference time. And once we",
      "offset": 1211.039,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "were able to kind of find an",
      "offset": 1213.28,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "architecture that exploited inference",
      "offset": 1214.559,
      "duration": 2.961
    },
    {
      "lang": "en",
      "text": "time compute which is something now",
      "offset": 1216.24,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "everyone is finding to be important uh",
      "offset": 1217.52,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "accuracy starts shooting up. And what we",
      "offset": 1219.84,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "found then is for if the fundamental",
      "offset": 1221.84,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "answer lies in the data, we get to the",
      "offset": 1224.08,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "answer more than 90% of the time and we",
      "offset": 1225.76,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "get in within two to four minutes,",
      "offset": 1227.76,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "right? And which is amazing because now",
      "offset": 1229.52,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "you just look at these Slack channels,",
      "offset": 1231.679,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "humans are spending most of the time",
      "offset": 1233.36,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "just verifying the answer to what we",
      "offset": 1235.36,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "find versus actually root causing it.",
      "offset": 1237.28,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "And just the time the month-to-month",
      "offset": 1239.52,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "timed resolution has dropped, I'd say,",
      "offset": 1241.679,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "as is the number of people on average in",
      "offset": 1243.76,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "the Slack channel, which are the two",
      "offset": 1245.28,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "things that I think any enterprise cares",
      "offset": 1246.64,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "about. And how do you measure accuracy",
      "offset": 1248.08,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "like in this context? Like for example,",
      "offset": 1250.559,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "there are a lot of companies that focus",
      "offset": 1252.96,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "on LLM valves like does such a thing",
      "offset": 1255.039,
      "duration": 5.361
    },
    {
      "lang": "en",
      "text": "exist in the world of like SR and root",
      "offset": 1258.4,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "cause analysis or like you know how do",
      "offset": 1260.4,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "you how do you know that you're",
      "offset": 1261.76,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "correctly identifying a root cause?\n The",
      "offset": 1263.76,
      "duration": 5.279
    },
    {
      "lang": "en",
      "text": "gold standards honestly trying it on",
      "offset": 1266.559,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "live incidents. I think when you onboard",
      "offset": 1269.039,
      "duration": 6.081
    },
    {
      "lang": "en",
      "text": "to a customer oftentimes incidents can",
      "offset": 1272.159,
      "duration": 5.041
    },
    {
      "lang": "en",
      "text": "happen like two to three times a week",
      "offset": 1275.12,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "and in those scenarios you get the best",
      "offset": 1277.2,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "feedback. Maybe it takes a couple of",
      "offset": 1279.2,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "hours for that incident to complete. You",
      "offset": 1281.12,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "look at the postmortem and you can",
      "offset": 1283.2,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "really evaluate it. Um so I think that's",
      "offset": 1284.559,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "one definitive source. There's other",
      "offset": 1287.12,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "subtasks we do for evaluation such as",
      "offset": 1289.28,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "when people are trying to search for",
      "offset": 1292,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "specific information from their",
      "offset": 1293.84,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "observability. there's smaller chunks of",
      "offset": 1295.28,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "tasks you need to do to do RCA. So we",
      "offset": 1298.08,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "evaluate on those tasks as well which",
      "offset": 1301.12,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "are just higher volume. Um but",
      "offset": 1303.28,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "ultimately real live incidents are the",
      "offset": 1306.4,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "best way for us to evaluate.\n Awesome. It",
      "offset": 1309.36,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "sounds like you know you had this uh",
      "offset": 1312,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "rude awakening at some point where you",
      "offset": 1313.6,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "deployed the product and it was stuck at",
      "offset": 1315.76,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "0% accuracy before you kind of went back",
      "offset": 1318.24,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "to the drawing board and really reought",
      "offset": 1320.799,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "like the entire architecture. So maybe",
      "offset": 1323.44,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "can you give us like a very quick tour",
      "offset": 1325.919,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "under the covers of how the product",
      "offset": 1328,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "works today and kind of what is the",
      "offset": 1329.84,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "magic that enables",
      "offset": 1331.36,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "precise root cause analysis?\n Well, one",
      "offset": 1333.919,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "important decision we made was we only",
      "offset": 1336.24,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "require readonly access to the data and",
      "offset": 1338.64,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "I think that was a decision basically",
      "offset": 1341.28,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "based on enterprises not wanting to just",
      "offset": 1343.44,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "have yet another tool generate more",
      "offset": 1345.919,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "data. So how do we actually do it? Well,",
      "offset": 1348.159,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "I think there's two phases. there's a",
      "offset": 1350.559,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "offline phase and an online phase. So",
      "offset": 1352.559,
      "duration": 5.041
    },
    {
      "lang": "en",
      "text": "during this on offline phase, we're",
      "offset": 1355.44,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "really trying to learn this rich",
      "offset": 1357.6,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "dependency map. How do different",
      "offset": 1359.36,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "functions, how do different logs relate",
      "offset": 1361.919,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "to each other? And one way to do that is",
      "offset": 1363.679,
      "duration": 6.081
    },
    {
      "lang": "en",
      "text": "through LLMs. So LLMs go traverse really",
      "offset": 1365.919,
      "duration": 6.801
    },
    {
      "lang": "en",
      "text": "understand semantically how these logs",
      "offset": 1369.76,
      "duration": 5.279
    },
    {
      "lang": "en",
      "text": "different tags within the logs all",
      "offset": 1372.72,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "relate to each other. And then we also",
      "offset": 1375.039,
      "duration": 5.601
    },
    {
      "lang": "en",
      "text": "use statistics. And statistics comes in",
      "offset": 1377.039,
      "duration": 5.921
    },
    {
      "lang": "en",
      "text": "when for example there's natural",
      "offset": 1380.64,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "variation in time series and that turns",
      "offset": 1382.96,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "out to leave traces of causality which",
      "offset": 1385.36,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "basically Anish and I worked on in grad",
      "offset": 1388.4,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "school of how do you pull causal",
      "offset": 1390.48,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "relationships out of this data and",
      "offset": 1392.32,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "that's really key to build this rich",
      "offset": 1394.559,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "map. We also use selfplay to basically",
      "offset": 1396.48,
      "duration": 5.679
    },
    {
      "lang": "en",
      "text": "prioritize certain paths that are very",
      "offset": 1399.679,
      "duration": 5.601
    },
    {
      "lang": "en",
      "text": "promising for RCA. So now once we've",
      "offset": 1402.159,
      "duration": 5.601
    },
    {
      "lang": "en",
      "text": "constructed this rich dependency map",
      "offset": 1405.28,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "during the online phase when an actual",
      "offset": 1407.76,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "incident comes to us what this agent is",
      "offset": 1409.919,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "doing is it's using that real-time",
      "offset": 1412.559,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "information and this dependency map to",
      "offset": 1414.24,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "basically figure out what hops to make",
      "offset": 1417.679,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "to do the root cause analysis. How long",
      "offset": 1419.84,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "does it take for the offline part to",
      "offset": 1422.4,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "become effective such that in other",
      "offset": 1425.12,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "words between deploying your solution",
      "offset": 1426.799,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "and the first incident that you can",
      "offset": 1429.12,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "actually troubleshoot how long is that",
      "offset": 1431.919,
      "duration": 6.88
    },
    {
      "lang": "en",
      "text": "gestation period?\n It kind of depends on",
      "offset": 1434,
      "duration": 6.64
    },
    {
      "lang": "en",
      "text": "if they want to troubleshoot a live",
      "offset": 1438.799,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "incident or a historical incident. In",
      "offset": 1440.64,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "general, I would say we take about 5 to",
      "offset": 1443.6,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "10 hours to kind of look through all of",
      "offset": 1446.64,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "their codebase, look through their",
      "offset": 1449.44,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "observability, really have that system",
      "offset": 1450.96,
      "duration": 5.599
    },
    {
      "lang": "en",
      "text": "understanding. Um, for larger customers,",
      "offset": 1453.2,
      "duration": 6.32
    },
    {
      "lang": "en",
      "text": "it can take a day, but generally 5 to 10",
      "offset": 1456.559,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "hours.\n So, you've mentioned reasoning",
      "offset": 1459.52,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "and inference time compute a couple",
      "offset": 1461.76,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "times in there. Are you using kind of",
      "offset": 1463.52,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "foundation models and, you know,",
      "offset": 1466.159,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "fine-tuning them for your purposes? are",
      "offset": 1468.48,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "you building a lot of that kind of",
      "offset": 1470.559,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "architecture yourself? Maybe just talk a",
      "offset": 1471.919,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "little bit about you know the",
      "offset": 1473.919,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "architectural decisions you've made.",
      "offset": 1475.279,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "Yeah, one really interesting thing we",
      "offset": 1477.039,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "have learned um is that if you work with",
      "offset": 1478.799,
      "duration": 5.601
    },
    {
      "lang": "en",
      "text": "enterprises they typically have a",
      "offset": 1481.36,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "existing relationship with a LLM",
      "offset": 1484.4,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "provider. So they might have an",
      "offset": 1486.72,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "enterprise contract with OpenAI or",
      "offset": 1488.159,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "Anthropic and if you try to bring your",
      "offset": 1489.52,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "own model or your own fine-tuned model",
      "offset": 1492.08,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "to them, you're going to be stuck in",
      "offset": 1494,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "security hell for about a year. M\n and so",
      "offset": 1495.679,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "you you have to kind of tie your hands",
      "offset": 1497.84,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "where you say you have to be able to use",
      "offset": 1499.84,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "whichever model I give you. Typically",
      "offset": 1501.44,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "open AI is a pretty safe bet enthropic",
      "offset": 1503.039,
      "duration": 5.921
    },
    {
      "lang": "en",
      "text": "as well. Um and so then most of the",
      "offset": 1505.12,
      "duration": 6.799
    },
    {
      "lang": "en",
      "text": "complexity is really about um how do you",
      "offset": 1508.96,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "get the right set of tools that this LLM",
      "offset": 1511.919,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "has access to to orchestrate the RCA",
      "offset": 1514.48,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "step itself. And the other thing you can",
      "offset": 1517.039,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "do is fine-tune within a company's",
      "offset": 1519.12,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "environment. So, you know, let's say",
      "offset": 1521.6,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "they point to their Azure OpenAI um",
      "offset": 1522.96,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "instance, then you can fine-tune that by",
      "offset": 1525.76,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "every time an instant happens, you see",
      "offset": 1528.4,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "how far you got, you saw whatever the",
      "offset": 1529.76,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "pin root cause was, you can see how far",
      "offset": 1531.6,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "away you were, and then the system can",
      "offset": 1533.44,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "fine-tune on on making sure that gaps",
      "offset": 1535.279,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "get smaller and smaller over time,",
      "offset": 1536.96,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "right? So, that's generally how it we've",
      "offset": 1538.72,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "seen it's played out.\n And you also",
      "offset": 1541.12,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "mentioned that you guys had",
      "offset": 1542.96,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "part of the the architecture here is",
      "offset": 1546.24,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "based on years of your academic",
      "offset": 1548.32,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "research. Can you share a bit more about",
      "offset": 1550.48,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "kind of this interplay between your PhD",
      "offset": 1553.039,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "dissertations and and translating that",
      "offset": 1555.36,
      "duration": 5.679
    },
    {
      "lang": "en",
      "text": "now into a company? Yeah, actually. So,",
      "offset": 1558,
      "duration": 5.679
    },
    {
      "lang": "en",
      "text": "one thing that's kind of interesting is",
      "offset": 1561.039,
      "duration": 5.601
    },
    {
      "lang": "en",
      "text": "at least during grad school, I worked um",
      "offset": 1563.679,
      "duration": 5.761
    },
    {
      "lang": "en",
      "text": "closely with the Broad Institute to",
      "offset": 1566.64,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "basically understand with these um",
      "offset": 1569.44,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "crisper interventions. You have these",
      "offset": 1572.08,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "gene regulatory networks. you do a",
      "offset": 1573.6,
      "duration": 2.959
    },
    {
      "lang": "en",
      "text": "crisper intervention and you're trying",
      "offset": 1575.2,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "to understand what is the effect of this",
      "offset": 1576.559,
      "duration": 5.681
    },
    {
      "lang": "en",
      "text": "drug or this knockout experiment on how",
      "offset": 1579.44,
      "duration": 6.32
    },
    {
      "lang": "en",
      "text": "these genes express and the techniques",
      "offset": 1582.24,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "we developed there for learning that",
      "offset": 1585.76,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "causal structure between genes turns out",
      "offset": 1587.36,
      "duration": 5.919
    },
    {
      "lang": "en",
      "text": "to be pretty related to this um problem",
      "offset": 1590,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "we're facing with production systems",
      "offset": 1593.279,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "where if you think of the nodes of",
      "offset": 1595.52,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "swapping them with genes as",
      "offset": 1597.84,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "microservices and then you're learning",
      "offset": 1599.2,
      "duration": 4.959
    },
    {
      "lang": "en",
      "text": "okay what happens when I make a PR",
      "offset": 1602.08,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "change or I break this part of the",
      "offset": 1604.159,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "system, how does that percolate, it",
      "offset": 1606.48,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "becomes almost the identical problem?",
      "offset": 1609.12,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "And I think that was honestly by sheer",
      "offset": 1611.279,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "luck. We didn't know that was going to",
      "offset": 1614.32,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "be the case. And only until we got into",
      "offset": 1615.84,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "the weeds of the problem did we realize",
      "offset": 1617.919,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "like, oh wow, we got really lucky that",
      "offset": 1620.159,
      "duration": 5.041
    },
    {
      "lang": "en",
      "text": "our grad school research played out well",
      "offset": 1622.799,
      "duration": 5.041
    },
    {
      "lang": "en",
      "text": "here. What have been some other",
      "offset": 1625.2,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "surprises",
      "offset": 1627.84,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "on this journey over the past year, year",
      "offset": 1629.6,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "and a half? Well, for one, I think which",
      "offset": 1632.32,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "is part of why it's been such a joyous",
      "offset": 1634.64,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "thing for us is just how it's like the",
      "offset": 1636.4,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "industrial age of AI. And so I think all",
      "offset": 1639.279,
      "duration": 5.121
    },
    {
      "lang": "en",
      "text": "of the most interesting innovation I",
      "offset": 1642.4,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "feel is happening at small research",
      "offset": 1644.4,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "focused uh startups such as the ones",
      "offset": 1646.32,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "that we are part of or have the",
      "offset": 1648.48,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "privilege to be a part of. And so I",
      "offset": 1650.159,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "think just having seen the best of",
      "offset": 1651.84,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "research in some of the best",
      "offset": 1654,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "universities and now actually seeing how",
      "offset": 1655.279,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "it gets played out in a company, it just",
      "offset": 1656.96,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "feels like this is where all the magical",
      "offset": 1659.039,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "special work is happening. And just I",
      "offset": 1662.08,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "think that was surprising to me coming",
      "offset": 1663.919,
      "duration": 3.041
    },
    {
      "lang": "en",
      "text": "from a world of academia becoming a",
      "offset": 1665.36,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "professor. I thought that's where all",
      "offset": 1666.96,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "the innovation will be. And so that's",
      "offset": 1669.279,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "been quite surprising. Um honestly I",
      "offset": 1671.039,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "think the other thing that's been quite",
      "offset": 1673.6,
      "duration": 2.48
    },
    {
      "lang": "en",
      "text": "surprising is just how hungry",
      "offset": 1674.64,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "enterprises are for uh using JVI to",
      "offset": 1676.08,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "solve real problems and you can show",
      "offset": 1679.44,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "that the pace at which you can move with",
      "offset": 1681.6,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "them what typically would take a year or",
      "offset": 1683.44,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "two years to close can you know be",
      "offset": 1685.44,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "rapidly done in a couple of months and",
      "offset": 1687.039,
      "duration": 3.281
    },
    {
      "lang": "en",
      "text": "so I think the hunger of the market and",
      "offset": 1688.88,
      "duration": 3.279
    },
    {
      "lang": "en",
      "text": "also the pace of innovation industry has",
      "offset": 1690.32,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "been I think quite surprising. Speaking",
      "offset": 1692.159,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "of the pace of innovation,",
      "offset": 1694.399,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "things are moving like you know so",
      "offset": 1697.12,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "quickly and you have been working on",
      "offset": 1698.559,
      "duration": 5.041
    },
    {
      "lang": "en",
      "text": "this product for slightly more than a",
      "offset": 1700.96,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "year now. Have you been in situations",
      "offset": 1703.6,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "where you have to go back and rework",
      "offset": 1706,
      "duration": 5.679
    },
    {
      "lang": "en",
      "text": "something? Oh, because now there's MCP",
      "offset": 1708.64,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "or something new on the market that",
      "offset": 1711.679,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "previously you had to engineer yourself,",
      "offset": 1713.679,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "but now it's readily available like off",
      "offset": 1715.36,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "the shelf, so to speak. or or in some",
      "offset": 1717.679,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "ways like how do you futureproof like",
      "offset": 1720,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "your architecture and and and and what",
      "offset": 1722,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "you're working on?\n Honestly, I think",
      "offset": 1724.24,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "that is going to be a constant challenge",
      "offset": 1726.48,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "for companies of this generation. If",
      "offset": 1729.279,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "you're in product, if you're in design,",
      "offset": 1731.279,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "if you're in core engineering, you",
      "offset": 1732.88,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "constantly have to make bets as to where",
      "offset": 1734.72,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "AI is going to be 6 months from now and",
      "offset": 1736.559,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "you're willing to re-evaluate everything",
      "offset": 1738.24,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "six months from now. The good news is",
      "offset": 1739.679,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "that it's only going to get better. So,",
      "offset": 1741.6,
      "duration": 2.079
    },
    {
      "lang": "en",
      "text": "it's not like your product's going to",
      "offset": 1742.799,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "get worse 6 months from now, right? And",
      "offset": 1743.679,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "so for example, one um really",
      "offset": 1745.679,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "interesting thing bet that has paid off",
      "offset": 1748.64,
      "duration": 5.759
    },
    {
      "lang": "en",
      "text": "is um you know Raj and Raj were very",
      "offset": 1750.559,
      "duration": 6.161
    },
    {
      "lang": "en",
      "text": "precient about the fact that uh",
      "offset": 1754.399,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "reasoning models going to get better",
      "offset": 1756.72,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "right they' made this bet in September",
      "offset": 1758,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "right just seeing where the world is",
      "offset": 1760.08,
      "duration": 4.959
    },
    {
      "lang": "en",
      "text": "going and we architected our system such",
      "offset": 1761.52,
      "duration": 5.759
    },
    {
      "lang": "en",
      "text": "that the reasoning models would get to",
      "offset": 1765.039,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "shine and that has really played our",
      "offset": 1767.279,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "dividends right now um in the way our",
      "offset": 1769.2,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "entire architecture is set up and so I",
      "offset": 1772.159,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "think you have to keep making these",
      "offset": 1774.72,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "kinds of six-month bets about where AI",
      "offset": 1776.08,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "is going to be and surf it just right.",
      "offset": 1778.32,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "And I think the companies are able to",
      "offset": 1780.48,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "string together a few of these bets just",
      "offset": 1781.84,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "right, other ones are going to win. Uh I",
      "offset": 1783.44,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "think that's why in my opinion, if you",
      "offset": 1785.84,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "ask me, would you rather be an AI team",
      "offset": 1788.88,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "taking this problem or an observability",
      "offset": 1790.399,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "team? I'm biased, but I'd say I'd rather",
      "offset": 1791.84,
      "duration": 4.959
    },
    {
      "lang": "en",
      "text": "be an observability an AI team because",
      "offset": 1793.76,
      "duration": 5.279
    },
    {
      "lang": "en",
      "text": "um you just have to you get more fluent",
      "offset": 1796.799,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "in making these kinds of bets about",
      "offset": 1799.039,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "where things are going.\n What is your",
      "offset": 1800.559,
      "duration": 3.281
    },
    {
      "lang": "en",
      "text": "team composition like? How many are",
      "offset": 1802.399,
      "duration": 2.961
    },
    {
      "lang": "en",
      "text": "researchers? How many come from the",
      "offset": 1803.84,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "domain? Like what I'm curious about this",
      "offset": 1805.36,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "kind what the shape of a AI native agent",
      "offset": 1807.6,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "company will look like more broadly.",
      "offset": 1809.919,
      "duration": 5.521
    },
    {
      "lang": "en",
      "text": "Yeah, I mean right now I would say we're",
      "offset": 1811.6,
      "duration": 7.52
    },
    {
      "lang": "en",
      "text": "90% engineers. I mean a lot of people,",
      "offset": 1815.44,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "you know, there's a few of us with PhDs",
      "offset": 1819.12,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "who have done PhDs in machine learning.",
      "offset": 1821.44,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "A large fraction come from like",
      "offset": 1823.84,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "traditional software engineering",
      "offset": 1826.159,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "backgrounds who have been very",
      "offset": 1827.6,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "interested in Genai and have used it in",
      "offset": 1829.919,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "their own life. But I think for some of",
      "offset": 1832.24,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "these things at least the barrier right",
      "offset": 1834.48,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "is lower and you can kind of get",
      "offset": 1837.039,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "everyone's trying to learn how to make",
      "offset": 1838.88,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "an agent. And I think it's not like the",
      "offset": 1841.279,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "old days where you might have needed a",
      "offset": 1843.84,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "PhD to you know write out these gradient",
      "offset": 1846.559,
      "duration": 6.561
    },
    {
      "lang": "en",
      "text": "updates on this crazy LLM. I think it's",
      "offset": 1849.84,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "a lot more democratized and that",
      "offset": 1853.12,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "reflects in the way we've made the team",
      "offset": 1854.72,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "composition. Um we also have people",
      "offset": 1856.48,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "working with infrastructure. So how do",
      "offset": 1859.36,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "we scale these agents? And I think",
      "offset": 1861.84,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "that's a really interesting problem of",
      "offset": 1863.44,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "to Anisha's point of inference time",
      "offset": 1865.679,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "compute is how do you really get this AI",
      "offset": 1867.76,
      "duration": 7.519
    },
    {
      "lang": "en",
      "text": "agent to just do a lot more work than a",
      "offset": 1871.279,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "human can possibly do and that involves",
      "offset": 1875.279,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "for us we make thousands of network",
      "offset": 1877.279,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "calls per investigation. So that's",
      "offset": 1879.6,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "reflected in hiring the best",
      "offset": 1882,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "infrastructure engineers. And then I",
      "offset": 1883.6,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "think the other part is we have um",
      "offset": 1885.679,
      "duration": 5.441
    },
    {
      "lang": "en",
      "text": "product managers and some of those folks",
      "offset": 1888.159,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "are coming in the next month. So we're",
      "offset": 1891.12,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "really excited just to bring all of",
      "offset": 1892.96,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "these different peoples with different",
      "offset": 1895.2,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "backgrounds together to solve this hard",
      "offset": 1897.039,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "problem.\n Yeah. I think one thing that",
      "offset": 1899.36,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "unifies everyone at the company is an",
      "offset": 1901.2,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "excitement for generative AI. And I",
      "offset": 1903.039,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "think because of that they're willing to",
      "offset": 1905.039,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "learn and adapt and throw away things",
      "offset": 1906.88,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "that didn't work versus kind of being",
      "offset": 1909.519,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "stuck in your way. because I think those",
      "offset": 1911.279,
      "duration": 2.801
    },
    {
      "lang": "en",
      "text": "are the people that are not that's",
      "offset": 1912.88,
      "duration": 2.56
    },
    {
      "lang": "en",
      "text": "that's the type of culture that is not",
      "offset": 1914.08,
      "duration": 2.319
    },
    {
      "lang": "en",
      "text": "going to survive in this world because",
      "offset": 1915.44,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "it's just moving too quickly.\n And if I",
      "offset": 1916.399,
      "duration": 5.441
    },
    {
      "lang": "en",
      "text": "think about um what it's interesting I",
      "offset": 1918.48,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "think about an AI researcher versus an",
      "offset": 1921.84,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "AI engineer versus an engineer like how",
      "offset": 1923.44,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "do I organize that I I I think it has",
      "offset": 1925.6,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "nothing to do with whether you have a",
      "offset": 1928.72,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "PhD or any sort of credential. It's it's",
      "offset": 1929.679,
      "duration": 6.641
    },
    {
      "lang": "en",
      "text": "about how experimental are you in the",
      "offset": 1933.2,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "way you think. like how uh willing are",
      "offset": 1936.32,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "you to make little bets and hypotheses",
      "offset": 1938.72,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "about how things can be and then",
      "offset": 1941.039,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "quickly, you know, put that to the test,",
      "offset": 1942.88,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "right? I think that mentality of quick",
      "offset": 1946,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "iteration",
      "offset": 1947.919,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "on on a with AI and data is I think what",
      "offset": 1949.6,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "makes someone a good AI researcher or an",
      "offset": 1952.08,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "AI engineer and so it's more a mindset",
      "offset": 1953.679,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "than a than a credential. Speaking of",
      "offset": 1955.76,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "the composition of your team, I find it",
      "offset": 1958.24,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "very interesting that neither one of the",
      "offset": 1959.84,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "two of you or your two other co-founders",
      "offset": 1961.44,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "are actually observability industry",
      "offset": 1964.96,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "insiders and your entire company is very",
      "offset": 1967.12,
      "duration": 7.439
    },
    {
      "lang": "en",
      "text": "heavy on AI and engineering prowess and",
      "offset": 1970.24,
      "duration": 7.2
    },
    {
      "lang": "en",
      "text": "honestly like you know quite short on",
      "offset": 1974.559,
      "duration": 5.281
    },
    {
      "lang": "en",
      "text": "observability like you know domain",
      "offset": 1977.44,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "knowledge and yet you are making waves",
      "offset": 1979.84,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "like you know in this in this domain and",
      "offset": 1983.36,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "and successfully",
      "offset": 1985.12,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "resolving these very complex issues that",
      "offset": 1987.279,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "otherwise take little armies of people",
      "offset": 1989.039,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "scurrying around and dumpster diving to",
      "offset": 1991.12,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "go figure out like what is the smoking",
      "offset": 1993.76,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "gun log. It almost begs the question of",
      "offset": 1995.44,
      "duration": 6.239
    },
    {
      "lang": "en",
      "text": "what do you think the a future",
      "offset": 1998.96,
      "duration": 5.839
    },
    {
      "lang": "en",
      "text": "observability or s team at one of your",
      "offset": 2001.679,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "customers might look like today? You",
      "offset": 2004.799,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "mentioned that there are people who have",
      "offset": 2007.519,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "all the insider knowledge like all the",
      "offset": 2009.519,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "tribal knowledge, the intuition they can",
      "offset": 2011.84,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "make these hoops you know from here to",
      "offset": 2014,
      "duration": 6.399
    },
    {
      "lang": "en",
      "text": "there without without data. Is that",
      "offset": 2016.48,
      "duration": 6.64
    },
    {
      "lang": "en",
      "text": "still a valuable skill set like you know",
      "offset": 2020.399,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "5 10 years from now or would the",
      "offset": 2023.12,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "observability team look very very",
      "offset": 2025.919,
      "duration": 6.081
    },
    {
      "lang": "en",
      "text": "different?\n A couple of answers. One is I",
      "offset": 2027.919,
      "duration": 5.841
    },
    {
      "lang": "en",
      "text": "think",
      "offset": 2032,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "there will be a lot of work to be done",
      "offset": 2033.76,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "about the reliability of AI systems",
      "offset": 2035.36,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "themselves and I think the way you",
      "offset": 2037.36,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "reason about reliability of AI systems",
      "offset": 2038.96,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "will require obviously first principle",
      "offset": 2041.6,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "good S sur principles but also some sort",
      "offset": 2043.519,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "of fluency with AI as well because",
      "offset": 2046.32,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "they'll break in interesting unique ways",
      "offset": 2047.84,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "that we're just not used to and so I",
      "offset": 2049.76,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "think um I think modern SR teams will",
      "offset": 2052,
      "duration": 6.079
    },
    {
      "lang": "en",
      "text": "have to kind of u be fluent in both in",
      "offset": 2054.96,
      "duration": 6.32
    },
    {
      "lang": "en",
      "text": "some ways which is how do LLMs and AI",
      "offset": 2058.079,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "systems break and also how regular",
      "offset": 2061.28,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "systems break and and marrying those two",
      "offset": 2063.359,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "things with what I think will make you a",
      "offset": 2065.04,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "really good DevOps and SR engineer and",
      "offset": 2066.159,
      "duration": 5.361
    },
    {
      "lang": "en",
      "text": "also even simple things like I think one",
      "offset": 2068.8,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "really interesting thing that's going to",
      "offset": 2071.52,
      "duration": 2.079
    },
    {
      "lang": "en",
      "text": "happen is that the data layer of",
      "offset": 2072.399,
      "duration": 2.48
    },
    {
      "lang": "en",
      "text": "observability is going to fundamentally",
      "offset": 2073.599,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "change as well like the way a log looks",
      "offset": 2074.879,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "like is going to look fundamentally",
      "offset": 2077.359,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "different and a part of being a good SR",
      "offset": 2079.119,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "or DevOps person is knowing how to write",
      "offset": 2081.679,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "a good log right so that over time when",
      "offset": 2083.76,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "incident happens you have the right",
      "offset": 2086.48,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "instrumentation there's an art to",
      "offset": 2087.76,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "instrumenting your system the right way",
      "offset": 2089.839,
      "duration": 2.481
    },
    {
      "lang": "en",
      "text": "where you're not flooding it with too",
      "offset": 2091.2,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "many logs but just the right amount",
      "offset": 2092.32,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "right and I think the way we will write",
      "offset": 2094,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "logs will look fundamentally different",
      "offset": 2096.159,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "as well because it's no longer meant to",
      "offset": 2098.079,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "be scrolled by a human but meant to be",
      "offset": 2099.52,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "consumed by an AI system and\n that's a",
      "offset": 2101.68,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "very interesting point like how do you",
      "offset": 2104.32,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "rate cursor today on their ability to",
      "offset": 2106.16,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "write good logs\n I think it does a pretty",
      "offset": 2108.16,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "good job but I mean I think it's\n better",
      "offset": 2110.4,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "than a human\n I think the way it can",
      "offset": 2112.72,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "format like humans just make typos and",
      "offset": 2114.48,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "they don't I mean I make I'm really bad",
      "offset": 2117.119,
      "duration": 5.441
    },
    {
      "lang": "en",
      "text": "at spelling. Like I think cursor does a",
      "offset": 2119.76,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "great job there, but I think a lot of it",
      "offset": 2122.56,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "is still mimicking how traditional",
      "offset": 2124.16,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "logging looks. And I think to Anisha's",
      "offset": 2126.8,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "point, you need less of that. And you",
      "offset": 2129.2,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "really, for example, want to bury as",
      "offset": 2130.96,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "much information in the message field",
      "offset": 2132.88,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "because that's the meat of what is going",
      "offset": 2134.8,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "on. And now when the LM sees that, it",
      "offset": 2136.8,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "can kind of have a better understanding",
      "offset": 2139.52,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "of how to make those hops. But a lot of",
      "offset": 2141.599,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "times people don't like to add so much",
      "offset": 2143.92,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "content because a human can't read such",
      "offset": 2146.16,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "a long error stack trace in a log.\n We",
      "offset": 2148.16,
      "duration": 4.959
    },
    {
      "lang": "en",
      "text": "have shorter context windows than than",
      "offset": 2150.48,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "these LLMs.\n And I think also then",
      "offset": 2153.119,
      "duration": 2.801
    },
    {
      "lang": "en",
      "text": "connecting with the business logic,",
      "offset": 2154.8,
      "duration": 2.48
    },
    {
      "lang": "en",
      "text": "right? A lot of times because typically",
      "offset": 2155.92,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "it may not be engineering system is",
      "offset": 2157.28,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "fine, but what it depends on what fine",
      "offset": 2158.72,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "even means for an engineering system. It",
      "offset": 2160.88,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "has to connect to some sort of business",
      "offset": 2162.32,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "logic for it to be healthy or unhealthy.",
      "offset": 2164.24,
      "duration": 5.119
    },
    {
      "lang": "en",
      "text": "And I think logging in a way that it",
      "offset": 2167.2,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "that connects these two things in the",
      "offset": 2169.359,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "right way is is an art still. Um and I",
      "offset": 2170.88,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "think that will be tough for Corser or",
      "offset": 2174,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "any other AI system to do unless they",
      "offset": 2176.16,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "have full understanding of your business",
      "offset": 2177.76,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "logic which might happen. I don't know.",
      "offset": 2179.92,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "Yeah.\n As you dream about the future of",
      "offset": 2181.28,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "the software engineering market and the",
      "offset": 2183.52,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "role that observability and and",
      "offset": 2185.52,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "traversal will play in that future. You",
      "offset": 2187.119,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "know you started this podcast episode",
      "offset": 2189.04,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "talking about you know people aren't",
      "offset": 2191.2,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "vibes coding a you know payment system",
      "offset": 2192.4,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "or banking system today. Do you think",
      "offset": 2194.16,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "that you know if everything that",
      "offset": 2196.4,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "traversal is building goes right if",
      "offset": 2197.839,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "observability changes in the way you",
      "offset": 2199.68,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "think it might that you know engineers",
      "offset": 2201.04,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "at the largest banks and payments",
      "offset": 2203.76,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "companies and healthcare institutions",
      "offset": 2205.28,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "will be actually vibes coding their",
      "offset": 2207.28,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "their software uh their software",
      "offset": 2209.599,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "programs\n to some extent yes but and I",
      "offset": 2211.599,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "think what we'll have is just a much",
      "offset": 2214.24,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "better sense of",
      "offset": 2216.48,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "um like did it fulfill some function and",
      "offset": 2218.64,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "that will typically take the form of of",
      "offset": 2222.32,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "unit test in some form M right and just",
      "offset": 2224.24,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "much better at testing. And so as long",
      "offset": 2226.48,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "as it it fulfills some test that code is",
      "offset": 2228.24,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "fine and who cares what was written that",
      "offset": 2230.48,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "code\n I think that will somehow in my",
      "offset": 2232,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "opinion is how software engineering is",
      "offset": 2234.32,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "going to evolve\n which will be much more",
      "offset": 2235.52,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "about\n did it fulfill some function",
      "offset": 2237.2,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "versus the way it was written\n which will",
      "offset": 2239.28,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "be good but as we talked about earlier I",
      "offset": 2241.44,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "think also bad because when systems fail",
      "offset": 2243.76,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "it's because they interact in ways you",
      "offset": 2246.079,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "didn't expect right a lot of times\n you",
      "offset": 2247.52,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "are failing because a third party that",
      "offset": 2250,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "you call upon is not hitting the SLA",
      "offset": 2251.44,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "right and that's just going to happen a",
      "offset": 2253.52,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "lot more I think and it's going to be a",
      "offset": 2254.72,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "lot harder to debug because you don't",
      "offset": 2256.8,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "actually know what was the content of a",
      "offset": 2258.48,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "particular code file. Um, and so I think",
      "offset": 2259.76,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "that that's the the problem that we're",
      "offset": 2262.4,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "going to face and that's why I think",
      "offset": 2264.56,
      "duration": 2.559
    },
    {
      "lang": "en",
      "text": "products like traversal will have a big",
      "offset": 2265.68,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "part to play is in solving those kinds",
      "offset": 2267.119,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "of issues which is a lot more subtle.",
      "offset": 2268.8,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "Yeah.\n Because I think that's where",
      "offset": 2271.68,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "humans shine, right? Is right now humans",
      "offset": 2273.119,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "mostly write all of their code. So they",
      "offset": 2276.16,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "have that rich system knowledge. So when",
      "offset": 2277.839,
      "duration": 5.441
    },
    {
      "lang": "en",
      "text": "they're see an incident, they can short",
      "offset": 2280.32,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "circuit a lot of paths because they",
      "offset": 2283.28,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "already kind of have seen something in",
      "offset": 2285.68,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "the past or they kind of already",
      "offset": 2287.2,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "understand this error message they",
      "offset": 2288.96,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "wrote. But if an AI generated that, they",
      "offset": 2290.96,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "don't have that structural advantage and",
      "offset": 2293.52,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "then you really need a AI",
      "offset": 2295.839,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "troubleshooter.\n Okay, we're going to",
      "offset": 2297.52,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "close out with some rapid fire",
      "offset": 2299.119,
      "duration": 2.801
    },
    {
      "lang": "en",
      "text": "questions. You guys ready?\n Yeah, let's",
      "offset": 2300.4,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "do it.\n One sentence or one word answers.",
      "offset": 2301.92,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "Uh maybe first what application category",
      "offset": 2304.56,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "do you think will break out for AI in",
      "offset": 2307.119,
      "duration": 3.521
    },
    {
      "lang": "en",
      "text": "the next 12 months?\n We'll try to say",
      "offset": 2308.72,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "this in two sentences.\n Okay.\n Which is I",
      "offset": 2310.64,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "think anywhere where you're using",
      "offset": 2314.24,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "reasoning models better are where",
      "offset": 2315.359,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "applications are going to shine. And I",
      "offset": 2317.76,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "think diagnostics is one of them.",
      "offset": 2319.599,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "Healthcare being I think a prime example",
      "offset": 2321.68,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "of that.\n Another startup or a founder",
      "offset": 2323.44,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "that you admire.\n I would say Deis",
      "offset": 2325.599,
      "duration": 5.361
    },
    {
      "lang": "en",
      "text": "Hosabus hopefully I didn't butcher the",
      "offset": 2328.56,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "name but I think just amazing",
      "offset": 2330.96,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "researcher. so much foundational work",
      "offset": 2333.119,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "and just always trying to solve the",
      "offset": 2335.68,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "hardest problems.\n Recommended pieces of",
      "offset": 2337.52,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "content uh how to make yourself smart on",
      "offset": 2340.16,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "AI.\n My favorite uh article which you can",
      "offset": 2342.72,
      "duration": 6.32
    },
    {
      "lang": "en",
      "text": "read in five minutes is is is the better",
      "offset": 2346,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "lesson by Rich Sutton who won the Nobel",
      "offset": 2349.04,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "Prize last year for his work on",
      "offset": 2351.04,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "reinforcement learning. Um yeah.\n Any",
      "offset": 2352.32,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "other AI agent companies that you",
      "offset": 2355.44,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "admire? You know, you're one of this",
      "offset": 2357.359,
      "duration": 3.121
    },
    {
      "lang": "en",
      "text": "very first cohort of truly agent native",
      "offset": 2358.64,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "companies. Any others? Uh aside from the",
      "offset": 2360.48,
      "duration": 7.44
    },
    {
      "lang": "en",
      "text": "obvious like uh Glean and uh and I mean",
      "offset": 2363.44,
      "duration": 5.919
    },
    {
      "lang": "en",
      "text": "I would say Perplexity is one for sure",
      "offset": 2367.92,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "that I I think is just doing a fantastic",
      "offset": 2369.359,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "job. Yeah.\n Favorite new AI applications",
      "offset": 2371.119,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "that you're using in your personal life?",
      "offset": 2373.68,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "I honestly don't use anything other than",
      "offset": 2375.599,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "CHP.\n Unfortunately the same for me. It's",
      "offset": 2376.96,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "already it's my best friend at this",
      "offset": 2380.48,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "point.\n Wow. I love it. Uh maybe one last",
      "offset": 2382.48,
      "duration": 6.879
    },
    {
      "lang": "en",
      "text": "uh question. Will we be vibes coding",
      "offset": 2386.64,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "banking apps, payment systems,",
      "offset": 2389.359,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "healthcare systems in five years time?",
      "offset": 2390.88,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "To a some extent, yes.\n Wonderful. Thank",
      "offset": 2393.119,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "you so much for joining us today. We",
      "offset": 2395.76,
      "duration": 2.48
    },
    {
      "lang": "en",
      "text": "really love this conversation and",
      "offset": 2397.119,
      "duration": 2.401
    },
    {
      "lang": "en",
      "text": "congratulations on what you've built so",
      "offset": 2398.24,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "far at Traversal.\n Thank you.\n Thanks so",
      "offset": 2399.52,
      "duration": 3.45
    },
    {
      "lang": "en",
      "text": "much.",
      "offset": 2401.44,
      "duration": 11.449
    },
    {
      "lang": "en",
      "text": "[Music]",
      "offset": 2402.97,
      "duration": 9.919
    },
    {
      "lang": "en",
      "text": "[Music]",
      "offset": 2415.45,
      "duration": 17.32
    }
  ],
  "cleanText": "If you're in product, if you're in design, if you're in core engineering, you constantly have to make bets as to where AI is going to be six months from now, and you're willing to re-evaluate everything six months from now. The good news is that it's only going only to get better. So, it's not like your product's going to get worse six months from now, right? And so, for example, one um really interesting thing bet that has paid off is Raj and Raz were very preient about the fact that uh reasoning model is going to get better, right? They'd made this bet in September, right? Just seeing where the world is going.\nAnd we architected our system such that the reasoning models would get to shine, and that has really played out dividends right now um in the way our entire architecture is set up.\n[Music]\nToday we're excited to welcome Anish Agarwal and Raj Agrawal, co-founders of Traversal. Traversal is building AI Agents to transform the world of DevOps and Site Reliability Engineering. Today companies have war rooms and armies of DevOps and SREs, troubleshooting their production failures and fixing them in the codebase. Every minute of production downtime is costly and sometimes life or death. Keeping a company's application running is very hard and valuable work, and it's a problem that is only getting worse with the advent of AI generated code and vibe coding. Traversal believes that agents are the scalable solution to the problem and has seen exciting results already from the troubleshooter agents that they've deployed into production. We're excited to have them share more about that vision and their results so far.\nAnish and Raj, welcome to training data. So wonderful um to have you today. We have been working together for a year, and we can't wait to share with our audience how AI and AI Agents can transform the worlds of Site Reliability Engineering and DevOps, and somehow we need to weave negronies in the conversation someplace later on, but we'll get to that. Let's start with some quick rapid fire questions. Um will DevOps or SREs as we know them today even exist five years from now?\n Yeah, it's a great question. I I think it will, but I think it'll look fundamentally different from the way it looks now. And um I think in this world of DevOps and SRE, I think healthcare analogies generally fall quite naturally, I find, and um in this world, if you think about healthcare and the mass hierarchy of needs, there imagine um the stage one is where let's say you're having a Heart Attacks. You have to solve that right now. Nothing else matters. Nothing matters five minutes from now other than solving the Heart Attacks that you're having, right? And to me that's analogous to dealing with a high severity incident.\n Mhm. Um and then stage two is where let's say you're dealing with some sort of chronic issue that you face, like you know you have sprained ankle or whatever the whatever illness that might be affecting you, and it's very hard to plan three months in advance because that's something that's just affecting you every day, right? And I think that's the analogous thing you do in DevOps is dealing with streams of alerts, streams of of checking whether a deployment was safe or not safe, right? And and then stage three is where you know I think of it as like life hacking where you're thinking about how do I optimize my sleep and my nutrients so that I have a high quality life that's you know and and fulfilling life, right? And I think that's the equivalent of like planning out what your next five years of infrastructure look like and how you invest in the right places. Now if I think of what unfortunately people in the DevOps SRE on call engineering space uh live like right now, it's like having a Heart Attacks twice a week and dealing with a debilitating chronic condition that you have to handle every day.\n As I'm listening to you, I'm just trying to picture all the people who I know in DevOps and SRE in scrubs or wearing CGI monitors to life hack like you know their infrastructure.\nUh and I think people don't realize it, but when you when you make this connection to healthcare, you just realize how what we've gotten used to and what life really should be like in this world as we think about the healthcare of large scale software systems. And so I think if Traversal does its job right, then that first stage and second stage of needs, which is uh there's really high severity incidents and that constant pain by death by a thousand cuts should be something that AI and AI Agents take care of, and DevOps and SRE get to deal with the creative fun parts of what is it my infrastructure should look like for the next year, for the next five years, and it becomes a much more fulfilling job, and many engineering teams are nowadays um adopting autonomous coding things like cursor, Windsurf.\nDo you think that's going to have profound impact on how people maintain the reliability of their infrastructure, or will it be the case that AI will have a major role to play in fulfilling that vision, moving people away from like being the intensive care unit surgeons to being more thoughtful planners? So I think it's going to be there's a short-term answer and there's a long-term answer, and I think it becomes in the short term at least, it's a tale of two worlds, I think. So um there's this because of everything happening in the world of cursor and wind surf and so on so forth. I think the idea of vibe coding obviously has become very popular, right? And we can write a prompt or a few prompts and have something stood up that people can use and play with. uh but I think the analogy here is of of fast fashion where you try something on, you like it, you don't like it, you discard it, you know, start the wear the next thing, right? And I think in that world actually reliability doesn't really matter because you don't really need to take care of what you've created, there's no craft to it, you create it, you throw it away, right? And I think um but there's another world which is where you start applying these AI-powered Software Engineering techniques to mission critical systems in payments, in financial institution ions, in security infrastructure, streaming, and in that world, I think it's going to lead to a major issue. It probably already is, and it's only going to get worse in my opinion um because everyone, we've seen this with the enterprises we work with, everyone is using uh AI-powered Software Engineering tools to guide them as they write code, and what you actually find is that it actually even passes their local unit test. So in that local piece of code, everything looks perfect, right? The problem is that in large scale enterprises, things break when different pieces of your system interact in a way you just didn't realize or you didn't couldn't foresee. And when that happens, because all of this code is being written by an AI system, it's very hard to debug it because you just don't have the context anymore. You didn't write it anymore. And I think in that world, unless we find ways of using AI systems to do software maintenance, we're going to be throttled, and either people will disallow AI Software Engineering tools to be used because there's too much downtime um or something of that form, right? And I think in that world uh we're going to need new tools and new um software to help maintainance of of such systems, and I think that's where Traversal can can play a part.\n you definitely have your way with with words and analogy, so I love uh the tale of two worlds, you either have Louis Vuitton high fashion or you have fast fast fashion with sheen or or or or or something like that. Um but I think we jumped um into uh into the deep end maybe like a bit too quickly. Let's let's go back a few steps and maybe um explain for our audience like what actually uh root cause analysis stand for, like what what does it do? Tell us a bit more about this wonderful world of of troubleshooting and root cause analysis.\n Yeah. So I think um if I just think about the tale of The Lifecycle of an Incident, right? the story of an incident. It always surprisingly looks the same across companies.\n Uh a customer will log on to a platform, things not working exactly right. Um either it's already been recorded or they make a complaint to customer support. Customer support looks at it and says, &quot;Okay, it's not a user error, it's an actual issue.&quot; It escalates in this game of telephone up like five ladders of of different sophistication of engineering organizations uh within an enterprise. At some point it hits the DevOps SRE team who look at the issue and decide is this is this worthy of an incident based on the impact of it, based on the severity of it, the immediiacy of it, and if they decide it is worthy of an incident, then suddenly chaos ensues, right? You'll have a Slack incident channel that's created, there's 30 to 50 people in a channel, everyone's kind of implicitly blaming each other for what happened um and a bit of a who done it type thing, and almost without fail is the same thing that happens, which is there'll be this 10x engineer who's been at the company for got the Christie uh out of Sherlock Holmes of the situation and like aha, like they'll figure out exactly what happened, right? And not clear how they got there, but they got there, you roll back the the you come up with some sort of hot fix, and then everyone kind of it looks for an uh a longer term fix over time, right? And that's typically how it all plays out, and that whole flow is what we call um The Lifecycle of an Incident and root cause analysis in particular, right? Um and I think what I find incredible is that all of Observability, right, this these incredible companies have been created, um they're the second largest software spend typically companies have after your cloud spend, and yet we're still at the state of root cause analysis. Um but anyways, that that's my take on what RCA looks like nowadays.\n And tell us maybe like a bit more color on where does root cause analysis live. uh with relations to the plethora of observability tools that companies use today. Why is it that if still the case if this is the number two spend in technology, why is it that you have you know 50 people like in a Slack channel with a who done it kind of plot.\nIt speaks to the the importance of the problem, right? And what observability has done is the best they could have done given the technology that was available even like six months ago, right? Which is so if I think what observability is, it's fundamentally about the creation of telemetry data, it's called melt data, so like metrics and events and logs and traces, so melt for short, and it's about the creation of this data, storing it, and then providing a nice visualization layer on top of it so people can create the right slices to create little eyeballs in different parts of their system, right? And I think that's what observatory ends at, which is a storage and visualization layer because that's all that could have been done. Right. And um the the complex workflow of troubleshooting remains super manual and that just because that was what um that's all we could have done so far. Um and I think the promise of what these AI Agent companies can do is that they automate complex workflows that we do on top of software. And to me this problem of troubleshooting root cause analysis is one of the most complex workflows that any set of humans do in software. And I think now there's an opportunity to kind of move up uh in terms of sophistication where we use AI systems to actually automate this workflow versus just stopping at the storage and visualization layer.\n Can you talk about your product? Uh what is the agent or the agents that you're building to kind of take on this problem and how does it work?\n Yeah. And if I double agents um so maybe stepping back, what what is an agent, and it's really an LLM orchestration of tools, and in our world those tools might be data fetching tools, how to get logs, how to get metrics back, data processing, how to format it in a way that the agent can really process or statistical tools like running anomaly detection, and I think really what we're trying to look for here is you need to define the tools rich enough so that an RCA can be expressed as some combination or some sequence of these tool calls. And that's really where a lot of the complexities and the multi- aents come in is how do you piece together these tools to solve these complex tasks, and I think at least in our world the data is so big that it needs to be sequential because a single trace might not even fit an LLM context. So you need to know how to slowly just build this context of the LM to get to the root cause, and that's why it's really challenging of a problem.\n Are you mirroring the cognitive architecture of a of a human troubleshooter? Is it is it you know is a map of operations very similar to what a human would be doing or is it very different for an agent? Yeah, I think that's where at least when we were first thinking of the problem, we tried to mimic how an SRE would debug, and it was very manual, very sequential where an SRE typically might look at a piece of evidence and then okay, figure out what's the next piece of evidence to look. But a lot of times they make these hops with system knowledge which the agent might not know. And that's really where we used a lot more scale to figure out how to bypass some of this issue of system knowledge. So this agent will basically look in a more systematic fashion of there's this Google SRE handbook which tells you here's the key golden signals you should be looking at, latency, error rate. So it looks at this way of kind of health monitoring to piece together how to sequentially search this rich space, and it's less about having holes in the reasoning where a human can just immediately get maybe to that hop. The agent is making these sequential flows that get to the answer.\n Are there particular conditions or environments where this agogentic approach works better or worse? Yeah, I think um fundamentally it's about data access. So what we have found, and surprisingly that's what we've found, is that we sometimes can add less value in like a series A startup versus as a large enterprise because when you become a large enterprise, your um observability is quite mature. So everything is being instrumented in a way where the fundamental data is in place, but your teams are very fragmented. So no one team or no one person has enough context to piece together what everything about how to debug, right? And that's why you have these 30 or 50 people in a in a Slack incident room. And so what we fundamentally found is that um the place where we add the most value is when the reasoning steps for this agent to go from a high level trigger to the root cause, those steps can be found in the data, right? It just is too much data for any one human to keep in their head. uh when that data is fundamentally not there, then it's when the agent suffers, and that tends to not be the case at the enterprise is what we found.\n Very interesting and and really like somewhat counterintuitive because it's usually the case with startups that the way you choose your design partners or customers, you start at smaller companies or\n\n\nMid-market, and then you try to graduate your way up to, um, up to enterprises. And if we're going to use, like, the L1 to L5, like, analogy from self-driving cars, um, how close are you, uh, with having AI Agents correctly get to the root cause of incidents? Or maybe more broadly, like, what counts as success? Like, are we trying to get to 100% correct resolution, or what even counts as correct resolution, like, in this context? Yeah, that's a great question. So, I think there's really two cases here. There's the first case where the root cause fundamentally belongs in the data. So, it's in some log. It's in some PR. And in that case, I would say with Traversal, we're at L4. And I won't say L5 because for us, we might be able to flag that problematic PR or that smoking gun log, but then there's still that last smile of the fix and the remediation. And in cases where the fix is not so localized to a specific file or code change, where you need to do, basically, a bigger system-wide change, that's, we haven't gotten there, but I think that's where it's really exciting to see all the developments with code agents because that's where we can get to that L5. I think for places where the root cause isn't in the data, that's where we're more at a L2. I think Traversal finds a lot of the important symptoms that really help people debug. But sometimes it's just not in that data, and this human kind of needs to make those additional hops to get to the actual root cause. But still, the symptoms really help figure out how to make those additional hops. And sometimes, right, for us, when we notice that, we can tell customers, well, maybe you should instrument in this way to make the system more observable. Across the AI landscape right now, there's really interesting kind of AI native companies being formed, like yourselves. There's also the incumbents that are, you know, in many cases, not asleep at the wheel. Um, how do you think about the incumbent risk here, and why your customers are choosing to go with you? Yeah, I think it's a great question. I think the heart of it is Observability is expensive, right? It's such an expensive product that people pay for. As a result, it's extremely fragmented, right? You go to any big enterprise, they're using DataDog, they're using Splunk, they're using Datadog, they're using Elastic, they're using Graphana, they're using Service Now. I mean, they're using everything, right? And all of them are, like, encroaching in this game of attrition, right? And the problem is that if you just think about the pricing models in this world, it's all based on the amount of data they store, right? And so, as a result, company A is not incentivized to give you better insights from anything that's stored in company B, right? And to debug something, if you just look at any of the SREs or on-call engineers, they're calling upon all five, six tools they have access to. And it's that fragmentation of this historical industry, I think, which is going to ideally lead to companies such as ourselves that are somewhat agnostic to where the data is stored, at least for now. Uh, it gives us the chance. Do you have customers deployed on Traversal today, and what have you found is the difference between what you kind of expected, even coming from an academic background, versus what you're actually finding in real-world environments? Yeah, it's been, it's been quite a journey. I think when we started, as one should typically do, we started with, like, very small companies, and, um, and built something that, um, worked for them, and that typically took the form of, we went through the last 100 incidents that they had, right, in the Slack channels, and kind of try to use our brains to figure out what is the meta workflow of how they always debug an incident in that particular company, right? And in that situation, a very popular framework for agents is something called the React framework, right? And that, as a general idea, worked. We could somehow imbue the meta workflow into a React agent system, and it was able to do really a really good job. Um, and then at some point, we started, you know, working towards larger companies that had an actual, like, large-scale Observability system, thousands of microservices, that kind of situation. And I think I remember there's very clearly this one week where it was the first time we dealt with that kind of scale, and, um, the second we tried to apply our system on, on just some historical incident that happened, our accuracy was at 0%, and we just, it would not move, whatever we did to the prompts, whatever we did to anything. It was stubbornly at 0%. And that was a rude awakening. Uh, I remember Raj and I had a negro me. Um, and I think at that point, we kind of had to think through ways of, we made some interesting decisions which played out, uh, in our, in our favor, which is, we said, okay, nothing about a specific company will be hardcoded into the prompts, right? Um, and nothing about a workflow that humans do there will be hardcoded into the agentic, like, workflow, and, and that complexity has to go somewhere, all right? And eventually, where it went to is Computation, which, in the form, in our world, takes the form of spending tokens in the problem, like, using it at inference time. And once we were able to kind of find an architecture that exploited inference time compute, which is something now everyone is finding to be important, uh, accuracy starts shooting up. And what we found then is, for, if the fundamental answer lies in the data, we get to the answer more than 90% of the time, and we get in within two to four minutes, right? And which is amazing because now you just look at these Slack channels, humans are spending most of the time just verifying the answer to what we find versus actually root causing it. And just the time, the month-to-month timed resolution has dropped, I'd say, as is the number of people on average in the Slack channel, which are the two things that I think any enterprise cares about. And how do you measure accuracy, like, in this context? Like, for example, there are a lot of companies that focus on LLM valves, like, does such a thing exist in the world of, like, SRE and root cause analysis, or, like, you know, how do you, how do you know that you're correctly identifying a root cause? The gold standards, honestly, trying it on live incidents. I think when you onboard to a customer, oftentimes incidents can happen, like, two to three times a week, and in those scenarios, you get the best feedback. Maybe it takes a couple of hours for that incident to complete. You look at the postmortem, and you can really evaluate it. Um, so I think that's one definitive source. There's other subtasks we do for evaluation, such as when people are trying to search for specific information from their Observability. There's smaller chunks of tasks you need to do to do RCA. So we evaluate on those tasks as well, which are just higher volume. Um, but ultimately, real live incidents are the best way for us to evaluate. Awesome. It sounds like, you know, you had this, uh, rude awakening at some point where you deployed the product and it was stuck at 0% accuracy before you kind of went back to the drawing board and really rethought, like, the entire architecture. So, maybe can you give us, like, a very quick tour under the covers of how the product works today, and kind of what is the magic that enables precise root cause analysis? Well, one important decision we made was we only require read-only access to the data, and I think that was a decision basically based on enterprises not wanting to just have yet another tool generate more data. So, how do we actually do it? Well, I think there's two phases. There's a offline phase and an online phase. So, during this offline phase, we're really trying to learn this rich dependency map. How do different functions, how do different logs relate to each other? And one way to do that is through LLMs. So, LLMs go traverse, really understand semantically how these logs, different tags within the logs, all relate to each other. And then we also use statistics. And statistics comes in when, for example, there's natural variation in time series, and that turns out to leave traces of causality, which basically Anish and I worked on in grad school of how do you pull causal relationships out of this data, and that's really key to build this rich map. We also use self-play to basically prioritize certain paths that are very promising for RCA. So now, once we've constructed this rich dependency map, during the online phase, when an actual incident comes to us, what this agent is doing is it's using that real-time information and this dependency map to basically figure out what hops to make to do the root cause analysis. How long does it take for the offline part to become effective, such that, in other words, between deploying your solution and the first incident that you can actually troubleshoot, how long is that gestation period? It kind of depends on if they want to troubleshoot a live incident or a historical incident. In general, I would say we take about 5 to 10 hours to kind of look through all of their codebase, look through their Observability, really have that system understanding. Um, for larger customers, it can take a day, but generally 5 to 10 hours. So, you've mentioned reasoning and inference time compute a couple times in there. Are you using kind of foundation models and, you know, fine-tuning them for your purposes? Are you building a lot of that kind of architecture yourself? Maybe just talk a little bit about, you know, the architectural decisions you've made. Yeah, one really interesting thing we have learned, um, is that if you work with enterprises, they typically have an existing relationship with a LLM provider. So, they might have an enterprise contract with OpenAI or Anthropic, and if you try to bring your own model or your own fine-tuned model to them, you're going to be stuck in security hell for about a year. And so, you, you have to kind of tie your hands where you say, you have to be able to use whichever model I give you. Typically, OpenAI is a pretty safe bet, Anthropic as well. Um, and so then most of the complexity is really about, um, how do you get the right set of tools that this LLM has access to to orchestrate the RCA step itself. And the other thing you can do is fine-tune within a company's environment. So, you know, let's say they point to their Azure OpenAI, um, instance, then you can fine-tune that by every time an incident happens, you see how far you got, you saw whatever the pin root cause was, you can see how far away you were, and then the system can fine-tune on, on making sure that gaps get smaller and smaller over time, right? So, that's generally how it we've seen it's played out. And you also mentioned that you guys had part of the architecture here is based on years of your academic research. Can you share a bit more about kind of this interplay between your PhD dissertations and and translating that now into a company? Yeah, actually. So, one thing that's kind of interesting is, at least during grad school, I worked, um, closely with the Broad Institute to basically understand with these, um, CRISPR interventions, you have these gene regulatory networks. You do a CRISPR intervention, and you're trying to understand what is the effect of this drug or this knockout experiment on how these genes express, and the techniques we developed there for learning that causal structure between genes turns out to be pretty related to this, um, problem we're facing with production systems, where if you think of the nodes of swapping them with genes as microservices, and then you're learning, okay, what happens when I make a PR change or I break this part of the system, how does that percolate, it becomes almost the identical problem? And I think that was honestly by sheer luck. We didn't know that was going to be the case. And only until we got into the weeds of the problem did we realize, like, oh, wow, we got really lucky that our grad school research played out well here. What have been some other surprises on this journey over the past year, year and a half? Well, for one, I think, which is part of why it's been such a joyous thing for us, is just how it's like the industrial age of AI. And so, I think all of the most interesting innovation I feel is happening at small research-focused, uh, startups, such as the ones that we are part of or have the privilege to be a part of. And so, I think just having seen the best of research in some of the best universities and now actually seeing how it gets played out in a company, it just feels like this is where all the magical, special work is happening. And just, I think that was surprising to me coming from a world of academia becoming a professor. I thought that's where all the innovation will be. And so, that's been quite surprising. Um, honestly, I think the other thing that's been quite surprising is just how hungry enterprises are for, uh, using AI to solve real problems, and you can show that the pace at which you can move with them, what typically would take a year or two years to close, can, you know, be rapidly done in a couple of months. And so, I think the hunger of the market and also the pace of innovation industry has been, I think, quite surprising. Speaking of the pace of innovation, things are moving, like, you know, so quickly, and you have been working on this product for slightly more than a year now. Have you been in situations where you have to go back and rework something? Oh, because now there's MCP or something new on the market that previously you had to engineer yourself, but now it's readily available, like, off the shelf, so to speak, or, or in some ways, like, how do you futureproof, like, your architecture and and and and what you're working on? Honestly, I think that is going to be a constant challenge for companies of this generation. If you're in product, if you're in design, if you're in core engineering, you constantly have to make bets as to where AI is going to be six months from now, and you're willing to re-evaluate everything six months from now. The good news is that it's only going to get better. So, it's not like your product's going to get worse six months from now, right? And so, for example, one, um, really interesting thing bet that has paid off is, um, you know, Raj and Raj were very precient about the fact that, uh, reasoning models going to get better, right? They made this bet in September, right, just seeing where the world is going, and we architected our system such that the reasoning models would get to shine, and that has really played our dividends, right now, um, in the way our entire architecture is set up. And so, I think you have to keep making these kinds of six-month bets about where AI is going to be and surf it just right. And I think the companies are able to string together a few of these bets just right, other ones are going to win. Uh, I think that's why, in my opinion, if you ask me, would you rather be an AI team taking this problem\n\n\nOr an observability team?\nI'm biased, but I'd say I'd rather be an observability and an AI team because you just have to get more fluent in making these kinds of bets about where things are going.\nWhat is your team composition like?\nHow many are researchers?\nHow many come from the domain?\nLike what I'm curious about this kind, what the shape of an AI native agent company will look like more broadly.\nYeah, I mean right now I would say we're 90% engineers.\nI mean a lot of people, you know, there's a few of us with PhDs who have done PhDs in machine learning.\nA large fraction come from like traditional Software Engineering backgrounds who have been very interested in GenAI and have used it in their own life.\nBut I think for some of these things at least the barrier right is lower and you can kind of get everyone's trying to learn how to make an agent.\nAnd I think it's not like the old days where you might have needed a PhD to you know write out these gradient updates on this crazy LLM.\nI think it's a lot more democratized and that reflects in the way we've made the team composition.\nWe also have people working with infrastructure.\nSo how do we scale these agents?\nAnd I think that's a really interesting problem of to Anish's point of inference time compute is how do you really get this AI agent to just do a lot more work than a human can possibly do and that involves for us we make thousands of network calls per investigation.\nSo that's reflected in hiring the best infrastructure engineers.\nAnd then I think the other part is we have product managers and some of those folks are coming in the next month.\nSo we're really excited just to bring all of these different peoples with different backgrounds together to solve this hard problem.\nYeah.\nI think one thing that unifies everyone at the company is an excitement for generative AI.\nAnd I think because of that they're willing to learn and adapt and throw away things that didn't work versus kind of being stuck in your way because I think those are the people that are not that's that's the type of culture that is not going to survive in this world because it's just moving too quickly.\nAnd if I think about what it's interesting I think about an AI researcher versus an AI engineer versus an engineer, like how do I organize that?\nI think it has nothing to do with whether you have a PhD or any sort of credential.\nIt's about how experimental are you in the way you think.\nLike how willing are you to make little bets and hypotheses about how things can be and then quickly, you know, put that to the test, right?\nI think that mentality of quick iteration on AI and data is I think what makes someone a good AI researcher or an AI engineer and so it's more a mindset than a than a credential.\nSpeaking of the composition of your team, I find it very interesting that neither one of the two of you or your two other co-founders are actually observability industry insiders and your entire company is very heavy on AI and engineering prowess and honestly like you know quite short on observability like you know domain knowledge and yet you are making waves like you know in this in this domain and and successfully resolving these very complex issues that otherwise take little armies of people scurrying around and dumpster diving to go figure out like what is the smoking gun log.\nIt almost begs the question of what do you think the a future observability or SRE team at one of your customers might look like today?\nYou mentioned that there are people who have all the insider knowledge like all the tribal knowledge, the intuition they can make these hoops you know from here to there without without data.\nIs that still a valuable skill set like you know 5 10 years from now or would the observability team look very very different?\nA couple of answers.\nOne is I think there will be a lot of work to be done about the reliability of AI systems themselves and I think the way you reason about reliability of AI systems will require obviously first principle good SRE principles but also some sort of fluency with AI as well because they'll break in interesting unique ways that we're just not used to and so I think I think modern SRE teams will have to kind of be fluent in both in some ways which is how do LLMs and AI systems break and also how regular systems break and and marrying those two things with what I think will make you a really good DevOps and SRE engineer and also even simple things like I think one really interesting thing that's going to happen is that the data layer of observability is going to fundamentally change as well like the way a log looks like is going to look fundamentally different and a part of being a good SRE or DevOps person is knowing how to write a good log right so that over time when incident happens you have the right instrumentation there's an art to instrumenting your system the right way where you're not flooding it with too many logs but just the right amount right and I think the way we will write logs will look fundamentally different as well because it's no longer meant to be scrolled by a human but meant to be consumed by an AI system.\nThat's a very interesting point, like how do you rate Cursor today on their ability to write good logs?\nI think it does a pretty good job, but I mean I think it's better than a human.\nI think the way it can format, like humans just make typos and they don't, I mean I make, I'm really bad at spelling.\nLike I think Cursor does a great job there, but I think a lot of it is still mimicking how traditional logging looks.\nAnd I think to Anisha's point, you need less of that.\nAnd you really, for example, want to bury as much information in the message field because that's the meat of what is going on.\nAnd now when the LM sees that, it can kind of have a better understanding of how to make those hops.\nBut a lot of times people don't like to add so much content because a human can't read such a long error stack trace in a log.\nWe have shorter context windows than these LLMs.\nAnd I think also then connecting with the business logic, right?\nA lot of times because typically it may not be engineering system is fine, but what it depends on what fine even means for an engineering system.\nIt has to connect to some sort of business logic for it to be healthy or unhealthy.\nAnd I think logging in a way that it that connects these two things in the right way is is an art still.\nUm and I think that will be tough for Cursor or any other AI system to do unless they have full understanding of your business logic which might happen.\nI don't know.\nYeah.\nAs you dream about the future of the Software Engineering market and the role that observability and and traversal will play in that future.\nYou know you started this podcast episode talking about you know people aren't vibes coding a you know payment system or banking system today.\nDo you think that you know if everything that traversal is building goes right if observability changes in the way you think it might that you know engineers at the largest banks and payments companies and Healthcare institutions will be actually vibes coding their their software uh their software programs?\nTo some extent, yes, but and I think what we'll have is just a much better sense of um like did it fulfill some function and that will typically take the form of of unit test in some form, right?\nAnd just much better at testing.\nAnd so as long as it it fulfills some test that code is fine and who cares what was written that code.\nI think that will somehow in my opinion is how Software Engineering is going to evolve which will be much more about did it fulfill some function versus the way it was written which will be good but as we talked about earlier I think also bad because when systems fail it's because they interact in ways you didn't expect, right?\nA lot of times you are failing because a third party that you call upon is not hitting the SLA, right?\nAnd that's just going to happen a lot more I think and it's going to be a lot harder to debug because you don't actually know what was the content of a particular code file.\nUm, and so I think that that's the the problem that we're going to face and that's why I think products like traversal will have a big part to play is in solving those kinds of issues which is a lot more subtle.\nYeah.\nBecause I think that's where humans shine, right?\nIs right now humans mostly write all of their code.\nSo they have that rich system knowledge.\nSo when they're see an incident, they can short circuit a lot of paths because they already kind of have seen something in the past or they kind of already understand this error message they wrote.\nBut if an AI generated that, they don't have that structural advantage and then you really need a AI troubleshooter.\nOkay, we're going to close out with some Rapid Fire Questions.\nYou guys ready?\nYeah, let's do it.\nOne sentence or one word answers.\nUh maybe first, what application category do you think will break out for AI in the next 12 months?\nWe'll try to say this in two sentences.\nOkay.\nWhich is I think anywhere where you're using reasoning models better are where applications are going to shine.\nAnd I think AI-Powered Diagnostics is one of them.\nHealthcare being I think a prime example of that.\nAnother startup or a founder that you admire.\nI would say Deis Hosabus, hopefully I didn't butcher the name, but I think just amazing researcher, so much foundational work and just always trying to solve the hardest problems.\nRecommended pieces of content, uh, how to make yourself smart on AI.\nMy favorite uh article which you can read in five minutes is is is the better lesson by Rich Sutton who won the Nobel Prize last year for his work on reinforcement learning.\nUm yeah.\nAny other AI agent companies that you admire?\nYou know, you're one of this very first cohort of truly agent native companies.\nAny others?\nUh aside from the obvious like uh Glean and uh and I mean I would say Perplexity is one for sure that I I think is just doing a fantastic job.\nYeah.\nFavorite new AI applications that you're using in your personal life?\nI honestly don't use anything other than ChatGPT.\nUnfortunately the same for me.\nIt's already it's my best friend at this point.\nWow.\nI love it.\nUh maybe one last uh question.\nWill we be vibes coding banking apps, payment systems, Healthcare systems in five years time?\nTo a some extent, yes.\nWonderful.\nThank you so much for joining us today.\nWe really love this conversation and congratulations on what you've built so far at Traversal.\nThank you.\nThanks so much.\n[Music]\n[Music]\n",
  "dumpedAt": "2025-07-21T18:43:25.607Z"
}