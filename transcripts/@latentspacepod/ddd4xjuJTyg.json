{
  "episodeId": "ddd4xjuJTyg",
  "channelSlug": "@latentspacepod",
  "title": "Scaling Test Time Compute to Multi-Agent Civilizations â€” Noam Brown, OpenAI",
  "publishedAt": "2025-06-19T19:37:39.000Z",
  "rawLines": [
    {
      "lang": "en",
      "text": "Hey everyone, welcome to the L and Space",
      "offset": 5.2,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "podcast. This is Allesio, partner and",
      "offset": 7.279,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "CTO of Deible and I'm joined by my",
      "offset": 9.28,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "co-host Spooks, founder of Small AI.",
      "offset": 10.88,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "Hello. Hello. And we are here recording",
      "offset": 12.8,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "on a holiday Monday with Nan Brown from",
      "offset": 14.719,
      "duration": 5.041
    },
    {
      "lang": "en",
      "text": "OpenAI. Welcome. Thank you. So glad to",
      "offset": 17.76,
      "duration": 3.519
    },
    {
      "lang": "en",
      "text": "have you finally join us. Uh, a lot a",
      "offset": 19.76,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "lot of people have heard you. You've",
      "offset": 21.279,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "been rather generous of your time on on",
      "offset": 23.199,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "like podcasts. Um, Lex Friedman and",
      "offset": 25.279,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "you've done a you've done a TED talk",
      "offset": 27.76,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "recently just talking about the thinking",
      "offset": 29.039,
      "duration": 5.441
    },
    {
      "lang": "en",
      "text": "paradigm, but I think maybe perhaps your",
      "offset": 31.519,
      "duration": 5.281
    },
    {
      "lang": "en",
      "text": "most interesting recent achievement is",
      "offset": 34.48,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "winning the world diplomacy",
      "offset": 36.8,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "championship. Yeah. In 2022, you you",
      "offset": 38.559,
      "duration": 5.041
    },
    {
      "lang": "en",
      "text": "built like sort of Cicero which was top",
      "offset": 40.8,
      "duration": 5.439
    },
    {
      "lang": "en",
      "text": "10% of human players. I guess my opening",
      "offset": 43.6,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "question is how has your diplomacy",
      "offset": 46.239,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "playing changed since working on Cicero",
      "offset": 48.16,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "and then now personally playing it? When",
      "offset": 51.2,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "you work on these games, you kind of",
      "offset": 53.28,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "have to understand the game well enough",
      "offset": 54.879,
      "duration": 2.881
    },
    {
      "lang": "en",
      "text": "to like be able to debug your bot",
      "offset": 56.32,
      "duration": 2.879
    },
    {
      "lang": "en",
      "text": "because if the bot does something that's",
      "offset": 57.76,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "like really radical and like that t",
      "offset": 59.199,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "humans typically wouldn't do, you're not",
      "offset": 61.12,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "sure if that's like a mistake or if",
      "offset": 62.64,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "that's just uh like if it's a bug in the",
      "offset": 64.72,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "system or it's actually just like the",
      "offset": 66.479,
      "duration": 2.801
    },
    {
      "lang": "en",
      "text": "bot being brilliant. When we were",
      "offset": 67.68,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "working on Diplomacy, I kind of like did",
      "offset": 69.28,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "this deep dive like trying to understand",
      "offset": 71.04,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "the game better. I played in in",
      "offset": 72.479,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "tournaments. I like watched a lot of",
      "offset": 74.88,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "like tutorial videos and commentary",
      "offset": 76.56,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "videos on games and over that process I",
      "offset": 78.08,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "got better. And then also seeing the bot",
      "offset": 81.04,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "like the way it would behave in these",
      "offset": 82.88,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "games like sometimes it would do things",
      "offset": 84.64,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "that humans typically wouldn't do. And",
      "offset": 86.64,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "that taught me about the game as well.",
      "offset": 89.439,
      "duration": 5.121
    },
    {
      "lang": "en",
      "text": "When we released Cicero, we announced it",
      "offset": 91.68,
      "duration": 5.759
    },
    {
      "lang": "en",
      "text": "in like late 2022. I still found the",
      "offset": 94.56,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "game like really fascinating. And so I",
      "offset": 97.439,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "like kept up with it. I like continue to",
      "offset": 99.2,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "play and that led to me winning the",
      "offset": 101.04,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "championship in the world championship",
      "offset": 103.2,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "in 2025. So just a couple months ago.",
      "offset": 104.799,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "There's always a question of like",
      "offset": 106.96,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "centaur systems where humans and",
      "offset": 108,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "machines work together. Like was there",
      "offset": 110.24,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "an equivalent of what happened in Go",
      "offset": 111.92,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "where you updated your play style",
      "offset": 113.68,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "because you're asking if I used Cicero",
      "offset": 115.68,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "when I played in the tournament. The",
      "offset": 117.68,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "answer is the answer is no. Seeing the",
      "offset": 118.88,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "way the bot played and like taking",
      "offset": 120.56,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "inspiration from that I think did help",
      "offset": 121.92,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "me in in the tournament. Yeah. Yeah. Do",
      "offset": 124.159,
      "duration": 5.361
    },
    {
      "lang": "en",
      "text": "people now ask turing questions every",
      "offset": 126.96,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "single time when they're playing",
      "offset": 129.52,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "diplomacy? ask to try to figure out if",
      "offset": 130.8,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "uh if the person they're playing with is",
      "offset": 135.2,
      "duration": 3.119
    },
    {
      "lang": "en",
      "text": "a bot or Yeah, like that's the one thing",
      "offset": 136.64,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "you're worried about when you started.",
      "offset": 138.319,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "It was really interesting when we were",
      "offset": 140.319,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "working on Cisero because like you know",
      "offset": 141.76,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "we didn't have the best language models.",
      "offset": 143.76,
      "duration": 2.32
    },
    {
      "lang": "en",
      "text": "We were really bottlenecked on the",
      "offset": 145.04,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "quality of the language models and",
      "offset": 146.08,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "sometimes the bot would do would say",
      "offset": 148.239,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "like bizarre things like you know 90 99%",
      "offset": 150.48,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "of the time it was fine but then like",
      "offset": 152.879,
      "duration": 2.481
    },
    {
      "lang": "en",
      "text": "every once in a while it would say this",
      "offset": 154.08,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "like really bizarre thing like it would",
      "offset": 155.36,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "just hallucinate about something.",
      "offset": 157.12,
      "duration": 3.119
    },
    {
      "lang": "en",
      "text": "Somebody would reference something that",
      "offset": 159.04,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "they said earlier in a conversation with",
      "offset": 160.239,
      "duration": 2.64
    },
    {
      "lang": "en",
      "text": "the bot and the bot would be like, &quot;I",
      "offset": 161.76,
      "duration": 2.16
    },
    {
      "lang": "en",
      "text": "have no idea we're talking about. I",
      "offset": 162.879,
      "duration": 2.321
    },
    {
      "lang": "en",
      "text": "never said that.&quot; And then the person",
      "offset": 163.92,
      "duration": 2.24
    },
    {
      "lang": "en",
      "text": "would be like, &quot;Look, you could just",
      "offset": 165.2,
      "duration": 2
    },
    {
      "lang": "en",
      "text": "scroll up in the chat.&quot; It's like",
      "offset": 166.16,
      "duration": 2.64
    },
    {
      "lang": "en",
      "text": "literally right there. And the bot would",
      "offset": 167.2,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "be like, &quot;No, you're Windows.&quot;",
      "offset": 168.8,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "And when it does these kinds of things,",
      "offset": 171.92,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "like people just kind of like shrugged",
      "offset": 173.44,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "it off as like, &quot;Oh, that's just, you",
      "offset": 175.44,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "know, the person's tired or they're",
      "offset": 177.28,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "drunk or whatever or they're just like",
      "offset": 178.8,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "trolling me.&quot; But I think like that's",
      "offset": 180.48,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "because people weren't looking for a",
      "offset": 182.48,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "bot. They weren't expecting a bot to be",
      "offset": 184.4,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "in the games. We're actually really",
      "offset": 185.84,
      "duration": 2.64
    },
    {
      "lang": "en",
      "text": "scared because we were afraid that",
      "offset": 187.36,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "people would would figure out at one",
      "offset": 188.48,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "point that that there's a bot in these",
      "offset": 190.56,
      "duration": 2.319
    },
    {
      "lang": "en",
      "text": "games and then they would just like",
      "offset": 191.92,
      "duration": 2.08
    },
    {
      "lang": "en",
      "text": "always be on the lookout for it and they",
      "offset": 192.879,
      "duration": 2.161
    },
    {
      "lang": "en",
      "text": "would always be and if you're if you're",
      "offset": 194,
      "duration": 2.08
    },
    {
      "lang": "en",
      "text": "looking for it, you're able to spot it.",
      "offset": 195.04,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "That's the thing. So, I think now that",
      "offset": 196.08,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "it's announced and that people know to",
      "offset": 198.56,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "look for it, I think they would have an",
      "offset": 200.72,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "easier time spotting it. Now, that said,",
      "offset": 202.4,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "the language models have also gotten a",
      "offset": 204.4,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "lot better since 2022. It's adversarial.",
      "offset": 205.68,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "Yeah. So, at this point, like, you know,",
      "offset": 208,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "the truth is, you know, GP40 and like",
      "offset": 209.2,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "03, these models are like passing the",
      "offset": 211.2,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "touring test. So I don't think they can",
      "offset": 212.72,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "really ask that many touring complete",
      "offset": 215.12,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "questions that would actually make a",
      "offset": 216.799,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "difference. And Cesar was very small",
      "offset": 218.64,
      "duration": 4.959
    },
    {
      "lang": "en",
      "text": "like 2.7b, right? It was a very small uh",
      "offset": 220.48,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "language model. Yeah. This is one of the",
      "offset": 223.599,
      "duration": 2.481
    },
    {
      "lang": "en",
      "text": "things we realized over the course of",
      "offset": 225.04,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "the project that like oh yeah you you",
      "offset": 226.08,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "really benefit a lot from just having",
      "offset": 228.08,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "like larger language models, right? Yep.",
      "offset": 229.36,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "How do you think about today's",
      "offset": 231.76,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "perception of AI and a lot of like maybe",
      "offset": 233.519,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "the safety discourse of like you know",
      "offset": 235.28,
      "duration": 3.039
    },
    {
      "lang": "en",
      "text": "you're kind of built a bot that is",
      "offset": 236.959,
      "duration": 3.121
    },
    {
      "lang": "en",
      "text": "really good at persuading people into",
      "offset": 238.319,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "like helping them win a game and I think",
      "offset": 240.08,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "maybe today labs want to say they don't",
      "offset": 241.92,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "work on that type of problem. How do you",
      "offset": 244.48,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "think about that dichotomy so to speak",
      "offset": 246.08,
      "duration": 3.519
    },
    {
      "lang": "en",
      "text": "between the two? No, honestly like after",
      "offset": 248,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "we released Cicero, a lot of the AI",
      "offset": 249.599,
      "duration": 5.521
    },
    {
      "lang": "en",
      "text": "safety community was really happy with",
      "offset": 252.319,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "the research and like the way it worked",
      "offset": 255.12,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "because it was a very controllable",
      "offset": 256.88,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "system like we conditioned Cicero on",
      "offset": 258.479,
      "duration": 5.681
    },
    {
      "lang": "en",
      "text": "certain concrete actions and that gave",
      "offset": 261.68,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "it a lot of steerability to say like",
      "offset": 264.16,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "okay well it's going to pursue a",
      "offset": 265.68,
      "duration": 4.959
    },
    {
      "lang": "en",
      "text": "behavior that we can like very clearly",
      "offset": 268.56,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "interpret and and very clearly define.",
      "offset": 270.639,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "It's not just like oh it's a language",
      "offset": 272.8,
      "duration": 2.399
    },
    {
      "lang": "en",
      "text": "model like running loose and doing",
      "offset": 274,
      "duration": 2.479
    },
    {
      "lang": "en",
      "text": "whatever it feels like. It's actually",
      "offset": 275.199,
      "duration": 2.481
    },
    {
      "lang": "en",
      "text": "like pretty steerable and there's this",
      "offset": 276.479,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "whole reasoning system that steers the",
      "offset": 277.68,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "way the language model interacts with",
      "offset": 280.08,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "the human. Actually, a lot of",
      "offset": 281.44,
      "duration": 2.56
    },
    {
      "lang": "en",
      "text": "researchers reached out to like reached",
      "offset": 282.8,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "out to me and said like we think this is",
      "offset": 284,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "like potentially a really good way to",
      "offset": 285.52,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "achieve safety with these systems. I",
      "offset": 289.04,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "guess the last diplomacy related",
      "offset": 291.04,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "questions that we might have is have you",
      "offset": 292.8,
      "duration": 5.679
    },
    {
      "lang": "en",
      "text": "updated or tested like Oer models on",
      "offset": 295.36,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "diplomacy and would you would you expect",
      "offset": 298.479,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "a lot more difference? I have not. I",
      "offset": 300.88,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "think I said this on on Twitter at one",
      "offset": 303.12,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "point that I think this would be a great",
      "offset": 304.96,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "benchmark. I would love to see all the",
      "offset": 306.4,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "leading bots play a game of diplomacy",
      "offset": 308.08,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "with each other and see like who does",
      "offset": 309.52,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "best and I think a couple people have",
      "offset": 310.96,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "like taken inspiration from that and are",
      "offset": 312.8,
      "duration": 2.32
    },
    {
      "lang": "en",
      "text": "actually like building out these",
      "offset": 314.08,
      "duration": 2.959
    },
    {
      "lang": "en",
      "text": "benchmarks and like evaling the models.",
      "offset": 315.12,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "My understanding is that they don't do",
      "offset": 317.039,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "very well right now. Um but I think I",
      "offset": 318.24,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "think it really is a fascinating",
      "offset": 320.24,
      "duration": 2.799
    },
    {
      "lang": "en",
      "text": "benchmark and I think it would be um",
      "offset": 321.36,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "yeah, I think it'd be a really cool",
      "offset": 323.039,
      "duration": 2.641
    },
    {
      "lang": "en",
      "text": "thing to to try out. Well, we're going",
      "offset": 324.08,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "to go a little bit into O series now. I",
      "offset": 325.68,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "think the last time you did a lot of",
      "offset": 327.68,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "publicity, you were just launching 01.",
      "offset": 329.12,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "You did your TED talk and everything.",
      "offset": 331.68,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "How has the vibe, how have the vibes",
      "offset": 333.36,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "changed just in general? You said you",
      "offset": 336.4,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "were very excited to learn from domain",
      "offset": 338.32,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "experts like in chemistry like how they",
      "offset": 341.039,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "review the old series models like how",
      "offset": 343.6,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "have you updated since let's say end of",
      "offset": 345.759,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "last year? I think the trajectory was",
      "offset": 348.08,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "pretty clear pretty early on in the",
      "offset": 350.639,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "development cycle. And I think that",
      "offset": 352.32,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "everything that's unfolded since then",
      "offset": 355.36,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "has been pretty on track for what I",
      "offset": 357.52,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "expected. So, I wouldn't say that my",
      "offset": 360.08,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "perception of where where things are",
      "offset": 363.28,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "going or has honestly changed that much.",
      "offset": 364.639,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "Um, I think that we're going to continue",
      "offset": 366.96,
      "duration": 3.519
    },
    {
      "lang": "en",
      "text": "to see I I said before that we're going",
      "offset": 368.08,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "to see this paradigm continue to",
      "offset": 370.479,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "progress rapidly and I think that that's",
      "offset": 372.16,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "true even today that we we saw that with",
      "offset": 374.319,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "like going from 01 preview to 01 to 03",
      "offset": 376.4,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "consistent progress and we're going to",
      "offset": 378.8,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "continue to see that going forward and I",
      "offset": 381.6,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "think that we're going to see a",
      "offset": 383.52,
      "duration": 3.119
    },
    {
      "lang": "en",
      "text": "broadening of what these models can do",
      "offset": 385.199,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "as well. You know, like we're going to",
      "offset": 386.639,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "start seeing agentic behavior. We're",
      "offset": 387.919,
      "duration": 2.881
    },
    {
      "lang": "en",
      "text": "already starting to see agentic",
      "offset": 389.759,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "behavior. Like honestly for me 03 I've",
      "offset": 390.8,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "been using it a ton in my day-to-day",
      "offset": 393.68,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "life. I just find it so useful this",
      "offset": 395.44,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "especially the fact that I can now",
      "offset": 397.039,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "browse the web and like you know do",
      "offset": 398.24,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "meaningful research on my behalf like",
      "offset": 400.72,
      "duration": 2.56
    },
    {
      "lang": "en",
      "text": "it's kind of like a mini deep research",
      "offset": 402.08,
      "duration": 2.399
    },
    {
      "lang": "en",
      "text": "that you can just get a response in",
      "offset": 403.28,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "three minutes. So yeah I think it's just",
      "offset": 404.479,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "going to continue to become more and",
      "offset": 406.88,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "more useful and uh more powerful as time",
      "offset": 408.08,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "goes on and pretty quickly. Yeah. And",
      "offset": 410.08,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "talking about deep research you tweeted",
      "offset": 412.319,
      "duration": 3.521
    },
    {
      "lang": "en",
      "text": "about if you need proof that we can do",
      "offset": 413.919,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "this in number of viable domains. Deep",
      "offset": 415.84,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "research is kind of like a great",
      "offset": 417.6,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "example. Can you maybe talk about if",
      "offset": 418.72,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "there's something that people are",
      "offset": 421.199,
      "duration": 2.56
    },
    {
      "lang": "en",
      "text": "missing? You know I feel like I hear",
      "offset": 422.24,
      "duration": 2.799
    },
    {
      "lang": "en",
      "text": "that repeated a lot. It's like you know",
      "offset": 423.759,
      "duration": 2.801
    },
    {
      "lang": "en",
      "text": "it's easier to do encoding and math but",
      "offset": 425.039,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "like not in these other domains. I",
      "offset": 426.56,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "frequently get this question including",
      "offset": 428.319,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "from pretty established AI researchers",
      "offset": 429.919,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "that okay we're seeing these like",
      "offset": 432.319,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "reasoning models succeed in math and",
      "offset": 434.16,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "coding and these these easily verifiable",
      "offset": 435.919,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "domains but are they ever going to",
      "offset": 438,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "succeed in domains where success is less",
      "offset": 440.16,
      "duration": 5.439
    },
    {
      "lang": "en",
      "text": "well defined? I'm surprised that this is",
      "offset": 443.199,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "such a common perception because we've",
      "offset": 445.599,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "released deep research and people can",
      "offset": 447.759,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "try it out. People do use it. It's very",
      "offset": 449.68,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "popular and that is very clearly a",
      "offset": 451.36,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "domain where you don't have an easily",
      "offset": 453.36,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "verifiable metric for success. It's very",
      "offset": 455.84,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "like what what is the best research",
      "offset": 458.56,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "report that you could generate and yet",
      "offset": 460.4,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "these models are doing extremely well at",
      "offset": 461.84,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "this at this domain. So I think that's",
      "offset": 463.52,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "like an existence proof that these",
      "offset": 465.36,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "models can succeed in tasks that don't",
      "offset": 467.28,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "have as easily verifiable rewards. Is it",
      "offset": 469.599,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "because there's also not necessarily",
      "offset": 471.84,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "like a wrong answer like there's a",
      "offset": 473.759,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "spectrum of deep research quality,",
      "offset": 475.84,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "right? you can have like a report that",
      "offset": 477.84,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "like looks good but the information is",
      "offset": 479.599,
      "duration": 3.121
    },
    {
      "lang": "en",
      "text": "kind of so and so and then you have a",
      "offset": 481.12,
      "duration": 3.519
    },
    {
      "lang": "en",
      "text": "great report. Do you think people have a",
      "offset": 482.72,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "hard time understanding the difference",
      "offset": 484.639,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "when they get the result? My impression",
      "offset": 486.4,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "is that people do understand the",
      "offset": 488.639,
      "duration": 2.56
    },
    {
      "lang": "en",
      "text": "difference when they get a result and",
      "offset": 490.16,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "and I think that they're surprised at",
      "offset": 491.199,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "how good the deep research results are.",
      "offset": 493.12,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "There's certainly it's not not 100%. It",
      "offset": 495.12,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "could be better and we're going to make",
      "offset": 496.96,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "it better, but I think people can tell",
      "offset": 498.16,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "the difference between a good report and",
      "offset": 500.24,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "a bad report and and certainly and a",
      "offset": 501.28,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "good report and a mediocre report and",
      "offset": 503.28,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "that's enough to kind of feed the the",
      "offset": 505.199,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "loop later to build the product and",
      "offset": 507.12,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "improve the model performance. I mean, I",
      "offset": 508.879,
      "duration": 2.801
    },
    {
      "lang": "en",
      "text": "think if you're in a situation where",
      "offset": 510.56,
      "duration": 2.399
    },
    {
      "lang": "en",
      "text": "people can't tell the difference between",
      "offset": 511.68,
      "duration": 2.719
    },
    {
      "lang": "en",
      "text": "the outputs, then it doesn't really",
      "offset": 512.959,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "matter if you're like, you know, hill",
      "offset": 514.399,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "climbing on on progress. Uh these models",
      "offset": 515.839,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "are going to get better at domains where",
      "offset": 517.76,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "there is a measure of success. Now I",
      "offset": 519.599,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "think this idea that it has to be like",
      "offset": 522.479,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "easily verifiable or something like",
      "offset": 524.08,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "that. I don't think that's true. I think",
      "offset": 525.839,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "that you can have you can have these",
      "offset": 527.519,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "models do well even in domains where",
      "offset": 529.04,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "success is a very difficult to define",
      "offset": 531.519,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "thing could sometimes even be",
      "offset": 534.72,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "subjective. people lean on a lot you've",
      "offset": 536.16,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "you've done as well is the thinking fast",
      "offset": 538.399,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "as slow analogy for just uh thinking",
      "offset": 540.24,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "models and I think it's reasonably well",
      "offset": 543.04,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "diffused now the idea of uh that that",
      "offset": 546.08,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "this is kind of the next scaling",
      "offset": 548.64,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "paradigm all analogies are imperfect",
      "offset": 549.839,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "what is one way in which thinking fast",
      "offset": 552.88,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "and slow or system one system two kind",
      "offset": 555.04,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "of doesn't transfer to how we actually",
      "offset": 557.2,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "scale these things one thing that I",
      "offset": 560.56,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "think is underappreciated is that the",
      "offset": 562.88,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "models the pre-trained models need a",
      "offset": 564.48,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "certain level of capability in order to",
      "offset": 566.56,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "really benefit from this like extra",
      "offset": 568.24,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "thinking. This is kind of why you you've",
      "offset": 570.399,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "seen the reasoning paradigm emerge",
      "offset": 572.64,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "around the time that it did. I think it",
      "offset": 574,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "could have happened earlier, but if you",
      "offset": 575.44,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "try to do the reasoning paradigm on top",
      "offset": 577.36,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "of GBD2, I don't think it would have",
      "offset": 579.36,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "gotten you almost anything. Is this",
      "offset": 581.68,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "emergence? Hard to say I if it's",
      "offset": 584,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "emergence necessarily, but like I",
      "offset": 586.56,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "haven't done the um you know the",
      "offset": 588.399,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "measurements to really define that",
      "offset": 590.32,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "clearly. Um, but I think it's pretty",
      "offset": 592.24,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "clear, you know, people tried chain of",
      "offset": 594.399,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "thought with GBD, like really small",
      "offset": 596.08,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "models, and they saw that it just didn't",
      "offset": 597.839,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "really do anything. Then you go to",
      "offset": 599.68,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "bigger models and it starts to to give a",
      "offset": 601.44,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "lift. I think there's a lot of debate",
      "offset": 603.519,
      "duration": 2.481
    },
    {
      "lang": "en",
      "text": "about like the extent to which this kind",
      "offset": 604.72,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "of behavior is emergent, but clearly",
      "offset": 606,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "there is a difference. So, it's not like",
      "offset": 607.92,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "there are these two independent",
      "offset": 610.8,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "paradigms. I think that they are related",
      "offset": 612.16,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "in the sense that you need a certain",
      "offset": 614,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "level of system one capability in your",
      "offset": 615.519,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "models in order to have system two is to",
      "offset": 617.68,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "be able to benefit from system two.",
      "offset": 620.72,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "Yeah, I have play tried to play amateur",
      "offset": 622.16,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "neuroscientists before and try to",
      "offset": 624.56,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "compare it to the evolution of the brain",
      "offset": 626.56,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "and how you have to evolve the cortex",
      "offset": 628.64,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "first before you evolve the other parts",
      "offset": 630.959,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "of the brain. And perhaps that is what",
      "offset": 632.24,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "we're doing here. Yeah. And you could",
      "offset": 635.279,
      "duration": 2.961
    },
    {
      "lang": "en",
      "text": "argue that actually this is not that",
      "offset": 637.04,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "different from like I guess the um the",
      "offset": 638.24,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "system one system two paradigm because",
      "offset": 639.76,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "you know if you ask like a pigeon to",
      "offset": 641.36,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "think really hard about playing chess",
      "offset": 643.36,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "you know it's not going to get that far.",
      "offset": 644.72,
      "duration": 2.799
    },
    {
      "lang": "en",
      "text": "You know it doesn't matter if it like",
      "offset": 646.56,
      "duration": 2.16
    },
    {
      "lang": "en",
      "text": "thinks for a thousand years it's like",
      "offset": 647.519,
      "duration": 2.161
    },
    {
      "lang": "en",
      "text": "not going to be able to be better at",
      "offset": 648.72,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "playing chess. So maybe you do still",
      "offset": 649.68,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "also also in like with animals and",
      "offset": 651.68,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "humans that you need a certain level of",
      "offset": 653.76,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "intellectual ability just in terms of",
      "offset": 656.16,
      "duration": 3.119
    },
    {
      "lang": "en",
      "text": "system one in order to benefit from",
      "offset": 658,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "system two as well. Yeah. Just this t",
      "offset": 659.279,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "side tangent does this also apply to",
      "offset": 661.76,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "visual reasoning. So let's say we have",
      "offset": 663.44,
      "duration": 6.399
    },
    {
      "lang": "en",
      "text": "now we have the 40 like natively",
      "offset": 666.32,
      "duration": 6.4
    },
    {
      "lang": "en",
      "text": "omnimodel type of thing then that also",
      "offset": 669.839,
      "duration": 5.521
    },
    {
      "lang": "en",
      "text": "makes 03 really good at geogesser. Does",
      "offset": 672.72,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "that apply to other modalities too? I I",
      "offset": 675.36,
      "duration": 5.599
    },
    {
      "lang": "en",
      "text": "think the evidence is yes. It depends on",
      "offset": 678.48,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "exactly the kinds of questions that",
      "offset": 680.959,
      "duration": 2.56
    },
    {
      "lang": "en",
      "text": "you're asking. Like there are some",
      "offset": 682.16,
      "duration": 2.64
    },
    {
      "lang": "en",
      "text": "questions that I think don't really",
      "offset": 683.519,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "benefit from system 2. I think geo",
      "offset": 684.8,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "guests are certainly one uh where where",
      "offset": 687.12,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "you do benefit. I think image",
      "offset": 688.56,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "recognition if if I had to guess it's",
      "offset": 690.24,
      "duration": 2.32
    },
    {
      "lang": "en",
      "text": "like one of those things where you",
      "offset": 691.76,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "probably benefit less from system 2",
      "offset": 692.56,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "thinking cuz you know it or you don't.",
      "offset": 694.8,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "Yeah. Exactly. There's no way. Yeah. And",
      "offset": 696.48,
      "duration": 3.039
    },
    {
      "lang": "en",
      "text": "the the thing the thing I typically",
      "offset": 698.32,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "point to is just like information like",
      "offset": 699.519,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "retrieval. If somebody asks you like",
      "offset": 701.36,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "when was this person born and you don't",
      "offset": 703.36,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "have access to the web, then you either",
      "offset": 706,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "know it or you don't and you can sit",
      "offset": 708.079,
      "duration": 1.841
    },
    {
      "lang": "en",
      "text": "there and you can think about it for a",
      "offset": 709.12,
      "duration": 1.6
    },
    {
      "lang": "en",
      "text": "long time. Maybe you can make an",
      "offset": 709.92,
      "duration": 2.24
    },
    {
      "lang": "en",
      "text": "educated guess and you can say like well",
      "offset": 710.72,
      "duration": 3.119
    },
    {
      "lang": "en",
      "text": "this person was like probably lived",
      "offset": 712.16,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "around this time and so this is like a",
      "offset": 713.839,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "rough date. But you're not going to be",
      "offset": 716.079,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "able to like get the date unless you",
      "offset": 718.079,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "actually just just know it. But like",
      "offset": 719.68,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "spatial reasoning like tic-tac-toe might",
      "offset": 721.92,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "be better because you have all the",
      "offset": 724.16,
      "duration": 2.56
    },
    {
      "lang": "en",
      "text": "information there. Yeah. And I think",
      "offset": 725.12,
      "duration": 3.279
    },
    {
      "lang": "en",
      "text": "it's true that like with tic-tac-toe we",
      "offset": 726.72,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "see that like GPD 4.5 falls over. You",
      "offset": 728.399,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "know, it plays decently well. I",
      "offset": 730.959,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "shouldn't say it falls over. It does",
      "offset": 732.32,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "reasonably well. You can draw the board.",
      "offset": 734.079,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "It can make legal moves, but it will",
      "offset": 735.76,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "make mistakes sometimes. And if you",
      "offset": 738.399,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "really need that system too to enable it",
      "offset": 740,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "to play perfectly. Now, it's possible",
      "offset": 742.639,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "that if you got to GBD6 and you just did",
      "offset": 744.88,
      "duration": 2.48
    },
    {
      "lang": "en",
      "text": "system one, it would also play",
      "offset": 746.399,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "perfectly. You know, I guess we'll we'll",
      "offset": 747.36,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "know one day, but I think right now you",
      "offset": 749.519,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "would need the system two to really like",
      "offset": 752.56,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "do well. What do you think are like the",
      "offset": 754.16,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "things that you need in system one? So",
      "offset": 755.76,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "obviously general understanding of like",
      "offset": 757.76,
      "duration": 3.519
    },
    {
      "lang": "en",
      "text": "game rules. Do you also need to",
      "offset": 759.68,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "understand some sort of like metag game",
      "offset": 761.279,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "of like you know usually this is like",
      "offset": 763.279,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "how you value pieces in different games",
      "offset": 765.12,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "even though it's a you know how do you",
      "offset": 766.72,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "generalize in system one so that then in",
      "offset": 768,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "system two you can kind of get to the",
      "offset": 770.24,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "gameplay so to speak. I think the more",
      "offset": 772.16,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "that you have in your in the system one,",
      "offset": 775.2,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "like this is the same thing with humans,",
      "offset": 778,
      "duration": 2.399
    },
    {
      "lang": "en",
      "text": "you know, like humans are when they're",
      "offset": 779.36,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "playing for the first time, uh, a game",
      "offset": 780.399,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "like chess, they can apply a lot of",
      "offset": 782.32,
      "duration": 3.519
    },
    {
      "lang": "en",
      "text": "system two thinking to it. And if you if",
      "offset": 784,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "you apply a ton of system two thinking",
      "offset": 785.839,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "to it, like if if you just present a",
      "offset": 787.36,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "really smart person with a completely",
      "offset": 789.279,
      "duration": 2.961
    },
    {
      "lang": "en",
      "text": "novel game, and you tell them like,",
      "offset": 790.56,
      "duration": 3.519
    },
    {
      "lang": "en",
      "text": "okay, you're going to play this game",
      "offset": 792.24,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "against like an AI or like a human",
      "offset": 794.079,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "that's like mastered this game, and you",
      "offset": 796,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "tell them to like sit there and and",
      "offset": 797.519,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "think about it for like 3 weeks about",
      "offset": 798.88,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "how to play this game. My guess is they",
      "offset": 801.279,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "could actually do pretty well, but it",
      "offset": 803.12,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "certainly helps to build up that system",
      "offset": 805.68,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "one thinking like build up intuition",
      "offset": 808.56,
      "duration": 4.959
    },
    {
      "lang": "en",
      "text": "about about the game because it will",
      "offset": 810.8,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "just make you so much Yeah. so much",
      "offset": 813.519,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "faster. I think the Pokemon example is a",
      "offset": 815.12,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "good one of like the system one kind of",
      "offset": 817.44,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "has maybe all this information about",
      "offset": 819.44,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "games and then once you put it in the",
      "offset": 821.12,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "game it still needs a lot of harnesses",
      "offset": 822.8,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "to work and I'm trying to figure out how",
      "offset": 824.88,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "much of can we take things from the",
      "offset": 827.04,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "harness and have them in system one to",
      "offset": 828.639,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "the system two as harness free as",
      "offset": 830.959,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "possible but I guess that's like the",
      "offset": 833.2,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "question about generalizing games and AI",
      "offset": 834.8,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "yeah I guess I view that as a different",
      "offset": 837.36,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "question I think the question about like",
      "offset": 839.04,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "harnesses in my view is that the ideal",
      "offset": 840.48,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "harness is no harness",
      "offset": 843.6,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "Yeah, right. I think harnesses are like",
      "offset": 845.199,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "a crutch that eventually we're going to",
      "offset": 847.519,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "be able to move beyond. So, only two",
      "offset": 850.079,
      "duration": 3.281
    },
    {
      "lang": "en",
      "text": "costs and you could ask, you know, you",
      "offset": 851.92,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "could just ask O3. And actually, you",
      "offset": 853.36,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "know, it's interesting cuz like when",
      "offset": 855.12,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "this uh playing Pokemon thing kind of",
      "offset": 856.24,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "like emerged as as this like, you know,",
      "offset": 858.32,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "benchmark. I was actually like pretty",
      "offset": 860.32,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "opposed to evaling this with our with",
      "offset": 862.88,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "our like open eye models because my",
      "offset": 865.44,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "feeling is like, okay, if we're going to",
      "offset": 867.92,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "do this eval, let's just do it with 03.",
      "offset": 869.12,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "you know, how far does 03 get without",
      "offset": 872.32,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "any harness? How far does it get playing",
      "offset": 874.16,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "PokÃ©mon? And the answer is like not very",
      "offset": 875.92,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "far, you know? Um, and that's fine. I",
      "offset": 877.44,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "think it's fine to have an eval where",
      "offset": 879.36,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "the models do terribly. And I don't",
      "offset": 881.199,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "think the answer to that should be like,",
      "offset": 883.6,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "well, let's build a really good harness",
      "offset": 885.12,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "so that now it can do well on this eval.",
      "offset": 887.44,
      "duration": 2.959
    },
    {
      "lang": "en",
      "text": "I think the answer is like, okay, well,",
      "offset": 889.12,
      "duration": 2.399
    },
    {
      "lang": "en",
      "text": "let's just like improve the capabilities",
      "offset": 890.399,
      "duration": 3.041
    },
    {
      "lang": "en",
      "text": "of our models so they can do well at",
      "offset": 891.519,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "everything and then they also happen to",
      "offset": 893.44,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "make progress on this eval. Would you",
      "offset": 895.519,
      "duration": 3.521
    },
    {
      "lang": "en",
      "text": "consider things like checking for a",
      "offset": 897.279,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "valid move a harness or is this in the",
      "offset": 899.04,
      "duration": 5.359
    },
    {
      "lang": "en",
      "text": "in the model? You know, like chess. It's",
      "offset": 902.639,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "like you can either have the model learn",
      "offset": 904.399,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "in system one what moves are valid and",
      "offset": 906,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "what it can and cannot do versus in",
      "offset": 908.079,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "system two figuring out. I think I think",
      "offset": 910.24,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "there's like a lot of this is design",
      "offset": 912.8,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "questions. Like for me, I think you",
      "offset": 914.88,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "should give the model the ability to",
      "offset": 916.399,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "check if a move is legal if you want.",
      "offset": 918.16,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "like that that could be an option in the",
      "offset": 919.6,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "environment of like okay here's a you",
      "offset": 920.88,
      "duration": 3.519
    },
    {
      "lang": "en",
      "text": "know an action that you can like a tool",
      "offset": 922.72,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "call that you can make to see if an",
      "offset": 924.399,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "action is legal if it wants to use that",
      "offset": 926.079,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "it it can and then there's like design",
      "offset": 928,
      "duration": 3.519
    },
    {
      "lang": "en",
      "text": "question of like well what do you do if",
      "offset": 929.839,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "the model makes an illegal move and I",
      "offset": 931.519,
      "duration": 3.281
    },
    {
      "lang": "en",
      "text": "think it's totally reasonable to say",
      "offset": 933.76,
      "duration": 2.319
    },
    {
      "lang": "en",
      "text": "like well if they make an illegal move",
      "offset": 934.8,
      "duration": 2.32
    },
    {
      "lang": "en",
      "text": "then they lose the game like I don't",
      "offset": 936.079,
      "duration": 2.241
    },
    {
      "lang": "en",
      "text": "know what what happens when a human",
      "offset": 937.12,
      "duration": 2.959
    },
    {
      "lang": "en",
      "text": "makes an illegal move in a game of chess",
      "offset": 938.32,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "I actually don't know they're just not",
      "offset": 940.079,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "allowed to yeah like do you just lose",
      "offset": 942,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "the game I don't know so if that's if",
      "offset": 943.44,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "that's the case then I think it's",
      "offset": 945.44,
      "duration": 1.92
    },
    {
      "lang": "en",
      "text": "totally reasonable to say like yeah",
      "offset": 946.32,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "we're going to have an eval where that's",
      "offset": 947.36,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "also the criteria for for the AI models.",
      "offset": 949.04,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "Yeah. But I think like maybe one way to",
      "offset": 951.68,
      "duration": 3.519
    },
    {
      "lang": "en",
      "text": "interpret that in sort of researcher",
      "offset": 953.68,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "terms is are you allowed to do search?",
      "offset": 955.199,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "And one of the famous findings from Deep",
      "offset": 957.6,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "Seek is that MCTS wasn't that useful to",
      "offset": 959.44,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "them. But I think like there are a lot",
      "offset": 962.48,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "of engineers trying out search and",
      "offset": 964.079,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "spending a lot of tokens doing that and",
      "offset": 966.079,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "maybe it's not worth it. Well, I'm",
      "offset": 968.079,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "making a distinction here between like a",
      "offset": 969.44,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "tool call to check whether a move is",
      "offset": 971.839,
      "duration": 5.761
    },
    {
      "lang": "en",
      "text": "legal or illegal is different from",
      "offset": 974.72,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "actually making that move and then",
      "offset": 977.6,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "seeing whether it ended up being legal",
      "offset": 979.36,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "or illegal. Right? So, if that tool call",
      "offset": 981.44,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "is available, I think it's totally fine",
      "offset": 983.92,
      "duration": 2.159
    },
    {
      "lang": "en",
      "text": "to make that tool call and check whether",
      "offset": 984.88,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "a move is legal or illegal. I think it's",
      "offset": 986.079,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "different to have the model say, &quot;Oh,",
      "offset": 987.92,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "I'm making this move.&quot; Yeah. And then,",
      "offset": 990,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "you know, it gets feedback that like,",
      "offset": 993.04,
      "duration": 2.56
    },
    {
      "lang": "en",
      "text": "&quot;Oh, you made an illegal move.&quot; And so,",
      "offset": 994.24,
      "duration": 2.48
    },
    {
      "lang": "en",
      "text": "then it's like, &quot;Oh, just kidding. Like,",
      "offset": 995.6,
      "duration": 2.799
    },
    {
      "lang": "en",
      "text": "I'm going to do something else now.&quot; So,",
      "offset": 996.72,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "so that's that's the distinction I'm I'm",
      "offset": 998.399,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "drawing. Some people have tried to",
      "offset": 999.92,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "classify that second type of laying",
      "offset": 1001.519,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "things out as test time compute. You",
      "offset": 1003.759,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "would not classify that as test time",
      "offset": 1006.24,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "compute. There's a lot of reasons why",
      "offset": 1007.92,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "you would not want to rely on that",
      "offset": 1009.6,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "paradigm when you're going to the",
      "offset": 1011.36,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "imagine you have a robot, you know, and",
      "offset": 1013.6,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "your robot like takes some action in the",
      "offset": 1014.88,
      "duration": 2.959
    },
    {
      "lang": "en",
      "text": "world and it like breaks something and",
      "offset": 1016.48,
      "duration": 2.32
    },
    {
      "lang": "en",
      "text": "you're just like, oh, you can't say",
      "offset": 1017.839,
      "duration": 2.161
    },
    {
      "lang": "en",
      "text": "like, oh, just kidding. I didn't mean to",
      "offset": 1018.8,
      "duration": 2.56
    },
    {
      "lang": "en",
      "text": "do that. I'm going to undo that action.",
      "offset": 1020,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "Like the thing is broken. So if you want",
      "offset": 1021.36,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "to simulate what would happen if I move",
      "offset": 1023.6,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "the robot in this way and then in your",
      "offset": 1026,
      "duration": 2.959
    },
    {
      "lang": "en",
      "text": "simulation you saw that this thing broke",
      "offset": 1027.679,
      "duration": 2.4
    },
    {
      "lang": "en",
      "text": "and then you decide not to do that",
      "offset": 1028.959,
      "duration": 3.521
    },
    {
      "lang": "en",
      "text": "action that's totally fine but you can't",
      "offset": 1030.079,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "just like undo actions that you've taken",
      "offset": 1032.48,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "in the world. There's a couple more",
      "offset": 1034.24,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "things I wanted to cover in this rough",
      "offset": 1035.6,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "area. I actually had an answer on the on",
      "offset": 1037.36,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "the thinking fast and slow side which",
      "offset": 1039.199,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "maybe I I'm curious what you think about",
      "offset": 1041.28,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "like a lot of people are trying to put",
      "offset": 1043.12,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "in effectively model router layers let's",
      "offset": 1045.28,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "say between like the the fast response",
      "offset": 1047.919,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "model and the the long thinking model",
      "offset": 1049.919,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "enthropic is explicitly doing that and I",
      "offset": 1052,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "think there's a question about always do",
      "offset": 1054.24,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "you need a smart judge to route or do",
      "offset": 1056.799,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "you need a dumb judge judge to route",
      "offset": 1059.039,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "because it's fast so when you have a",
      "offset": 1060.48,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "model router let's say let's say you're",
      "offset": 1062.799,
      "duration": 3.521
    },
    {
      "lang": "en",
      "text": "passing requests between system one side",
      "offset": 1064.24,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "and system two",
      "offset": 1066.32,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "Does the router need to be as smart as",
      "offset": 1067.76,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "the smart model or dumb to be fast? I",
      "offset": 1070.4,
      "duration": 6.32
    },
    {
      "lang": "en",
      "text": "think it's possible for a dumb model to",
      "offset": 1073.84,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "recognize that a problem is really hard",
      "offset": 1076.72,
      "duration": 2.319
    },
    {
      "lang": "en",
      "text": "and that it won't be able to solve it",
      "offset": 1078,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "and then route it to a a more capable",
      "offset": 1079.039,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "model, but it's also possible for a dumb",
      "offset": 1081.52,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "model to be fooled or to be",
      "offset": 1083.6,
      "duration": 2.959
    },
    {
      "lang": "en",
      "text": "overconfident. I don't know. I think",
      "offset": 1085.12,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "there's a real trade-off there. But I",
      "offset": 1086.559,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "will say like I think I think there are",
      "offset": 1088.08,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "a lot of things that people are building",
      "offset": 1089.919,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "right now that will eventually be washed",
      "offset": 1092.24,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "away by scale. So I think harnesses are",
      "offset": 1094.4,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "a good example where I think eventually",
      "offset": 1097.36,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "the models are going to be and I think",
      "offset": 1099.36,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "this actually happened with the",
      "offset": 1101.36,
      "duration": 1.76
    },
    {
      "lang": "en",
      "text": "reasoning models like before the",
      "offset": 1102.16,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "reasoning models emerged there was like",
      "offset": 1103.12,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "all of this work that went into",
      "offset": 1104.88,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "engineering these like agentic systems",
      "offset": 1106.88,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "that like made a lot of calls to GBD40",
      "offset": 1109.28,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "or like the these non-reasoning models",
      "offset": 1112.64,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "to get reasoning behavior and then it",
      "offset": 1114.32,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "turns out like oh we just like created",
      "offset": 1116.4,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "reasoning models and they you don't need",
      "offset": 1117.919,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "this like complex behavior. In fact, in",
      "offset": 1119.6,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "many ways it makes it worse like you",
      "offset": 1122.16,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "just give the reasoning model the same",
      "offset": 1123.76,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "question without any sort of scaffolding",
      "offset": 1125.76,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "and it just does it now that you can",
      "offset": 1128.48,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "still and so people are building",
      "offset": 1130.64,
      "duration": 2.399
    },
    {
      "lang": "en",
      "text": "scaffolding on top of the reasoning",
      "offset": 1132.08,
      "duration": 2
    },
    {
      "lang": "en",
      "text": "models right now but I think in many",
      "offset": 1133.039,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "ways like those scaffolds will also just",
      "offset": 1134.08,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "be replaced by the reasoning models and",
      "offset": 1136.4,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "models in general becoming more capable",
      "offset": 1138.4,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "and similarly I think things like model",
      "offset": 1140.08,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "uh like these routers you know we've",
      "offset": 1142.08,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "said pretty openly that we want to move",
      "offset": 1144.4,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "to a world where there is a single",
      "offset": 1146,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "unified model and in that world you",
      "offset": 1147.6,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "shouldn't need a router on top of the",
      "offset": 1151.039,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "model. So I think that the router issue",
      "offset": 1152.88,
      "duration": 6.159
    },
    {
      "lang": "en",
      "text": "will eventually be solved. Also like",
      "offset": 1157.039,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "you're building the router into the",
      "offset": 1159.039,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "model kind of weights itself. I don't",
      "offset": 1160.88,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "think there will be a a a benefit for",
      "offset": 1163.52,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "like I I shouldn't say because it's I",
      "offset": 1166.88,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "could be wrong about this like you know",
      "offset": 1168.48,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "and certainly maybe there's um you know",
      "offset": 1169.76,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "reasons to route to different model",
      "offset": 1173.28,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "providers or whatever but I think that",
      "offset": 1175.2,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "routers are going to um eventually go",
      "offset": 1177.6,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "away and I can understand why it's worth",
      "offset": 1180.64,
      "duration": 3.039
    },
    {
      "lang": "en",
      "text": "doing it in the short term because like",
      "offset": 1182.16,
      "duration": 3.519
    },
    {
      "lang": "en",
      "text": "the fact is it is beneficial right now",
      "offset": 1183.679,
      "duration": 5.041
    },
    {
      "lang": "en",
      "text": "and if you're building a product and you",
      "offset": 1185.679,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "you're getting a lift from it then it's",
      "offset": 1188.72,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "it's worth doing right now. One of the",
      "offset": 1190.4,
      "duration": 3.279
    },
    {
      "lang": "en",
      "text": "tricky things I' I'd imagine that a lot",
      "offset": 1192.16,
      "duration": 2.879
    },
    {
      "lang": "en",
      "text": "of developers are facing is that like",
      "offset": 1193.679,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "you kind of have to plan for where these",
      "offset": 1195.039,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "models are going to be in six months and",
      "offset": 1197.28,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "12 months and that's like very hard to",
      "offset": 1198.799,
      "duration": 2.801
    },
    {
      "lang": "en",
      "text": "do because things are progressing very",
      "offset": 1200.4,
      "duration": 2.32
    },
    {
      "lang": "en",
      "text": "quickly. You know, you don't want to",
      "offset": 1201.6,
      "duration": 2.64
    },
    {
      "lang": "en",
      "text": "spend 6 months building something and",
      "offset": 1202.72,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "then just have it be totally washed away",
      "offset": 1204.24,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "by scale. But I I think I would",
      "offset": 1206.48,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "encourage developers like when when",
      "offset": 1208.24,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "they're, you know, building these kinds",
      "offset": 1209.679,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "of things like scaffolds and and",
      "offset": 1211.12,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "routers, keep in mind that the field is",
      "offset": 1212.799,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "evolving very rapidly. You know, things",
      "offset": 1215.36,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "are going to change in 3 months, uh, let",
      "offset": 1216.88,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "alone 6 months. And that might require",
      "offset": 1219.52,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "radically changing these things around",
      "offset": 1222.96,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "or or tossing them out completely. So",
      "offset": 1225.2,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "don't spend 6 months building something",
      "offset": 1226.799,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "that might get tossed out in 6 months.",
      "offset": 1228.08,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "It's so hard though. Everyone says this",
      "offset": 1229.919,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "and then like no one has concrete",
      "offset": 1231.52,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "suggestions on how.",
      "offset": 1233.52,
      "duration": 5.279
    },
    {
      "lang": "en",
      "text": "What about reinforcement fine-tuning? Is",
      "offset": 1236.64,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "this something that obviously you just",
      "offset": 1238.799,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "released it a month ago at Openai. Is",
      "offset": 1240.4,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "that something people should spend time",
      "offset": 1242.4,
      "duration": 3.519
    },
    {
      "lang": "en",
      "text": "on right now or maybe wait until the",
      "offset": 1244,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "next jump? I think reinforcement fine is",
      "offset": 1245.919,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "is pretty cool and I I think it's like",
      "offset": 1248.24,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "worth looking into because it's really",
      "offset": 1250.159,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "about specializing the models for the",
      "offset": 1252.159,
      "duration": 7.041
    },
    {
      "lang": "en",
      "text": "data that you have and I think that um",
      "offset": 1255.919,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "something that's like worth worth",
      "offset": 1259.2,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "looking into for for developers like",
      "offset": 1260.4,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "we're not we're not suddenly going to",
      "offset": 1262.64,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "like have that data baked into the raw",
      "offset": 1263.84,
      "duration": 6.079
    },
    {
      "lang": "en",
      "text": "model a lot of times. So I I think",
      "offset": 1267.28,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "that's kind of like a separate question.",
      "offset": 1269.919,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "Yeah. So creating the environment and",
      "offset": 1271.28,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "the reward model is the best thing",
      "offset": 1272.799,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "people can do right now. I think the",
      "offset": 1274.48,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "question that people have is like should",
      "offset": 1276.559,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "I rush to fine-tune the the model using",
      "offset": 1278.24,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "RFT or should I build the harness to",
      "offset": 1281.12,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "then RFT the models as they get better.",
      "offset": 1283.52,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "I think the difference is that like for",
      "offset": 1286.159,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "reinforcement fine-tuning you're",
      "offset": 1288.96,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "collecting data that's going to be",
      "offset": 1290.64,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "useful as the models improve as well. So",
      "offset": 1292.159,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "if we come out with like future models",
      "offset": 1294.72,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "that are even more capable, you could",
      "offset": 1296.64,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "still fine-tune them on your data.",
      "offset": 1298.48,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "That's I think actually a good example",
      "offset": 1301.2,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "where you're building something that's",
      "offset": 1302.72,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "going to complement the model scaling",
      "offset": 1304.799,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "and becoming more capable rather than",
      "offset": 1306.48,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "necessarily getting washed away by the",
      "offset": 1308.88,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "scale. Y one last question on Ilia. You",
      "offset": 1310.48,
      "duration": 6.559
    },
    {
      "lang": "en",
      "text": "mentioned on I think the Sarah and Elad",
      "offset": 1314.48,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "podcast where you had this conversation",
      "offset": 1317.039,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "with Ilia a few years ago about more RL",
      "offset": 1318.64,
      "duration": 5.919
    },
    {
      "lang": "en",
      "text": "and reasoning and language models. just",
      "offset": 1321.919,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "any speculation or thoughts on why his",
      "offset": 1324.559,
      "duration": 6.161
    },
    {
      "lang": "en",
      "text": "attempt when he tried it, it didn't work",
      "offset": 1327.679,
      "duration": 5.441
    },
    {
      "lang": "en",
      "text": "or the timing wasn't right and why the",
      "offset": 1330.72,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "time is right now. I don't think I I",
      "offset": 1333.12,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "would frame it that way that like this",
      "offset": 1335.44,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "his attempt didn't work in many ways it",
      "offset": 1338,
      "duration": 7.6
    },
    {
      "lang": "en",
      "text": "did. Um so Ilia for me I saw that in all",
      "offset": 1339.84,
      "duration": 7.76
    },
    {
      "lang": "en",
      "text": "of these domains that I had worked on in",
      "offset": 1345.6,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "poker and hannabi and diplomacy having",
      "offset": 1347.6,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "the models think before acting made a",
      "offset": 1350.799,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "huge difference in performance like",
      "offset": 1353.44,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "orders of magnitude difference like",
      "offset": 1354.96,
      "duration": 2.719
    },
    {
      "lang": "en",
      "text": "10,000 times or something. Yeah. Like",
      "offset": 1356.24,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "you know a thousand to 100,000 times",
      "offset": 1357.679,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "like it's the the equivalent of a model",
      "offset": 1359.28,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "that's like a thousand to 100,000 times",
      "offset": 1360.799,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "bigger. And in language models you",
      "offset": 1362.4,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "weren't really seeing that that the",
      "offset": 1365.44,
      "duration": 2.479
    },
    {
      "lang": "en",
      "text": "model the models would just respond",
      "offset": 1366.64,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "instantly. Some people in the field in",
      "offset": 1367.919,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "the LLM field were like convinced that",
      "offset": 1370,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "like, okay, we just keep scaling",
      "offset": 1371.76,
      "duration": 2.159
    },
    {
      "lang": "en",
      "text": "pre-training, we're going to get to",
      "offset": 1372.88,
      "duration": 3.039
    },
    {
      "lang": "en",
      "text": "super intelligence. And I was kind of",
      "offset": 1373.919,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "skeptical of that perspective. In late",
      "offset": 1375.919,
      "duration": 5.601
    },
    {
      "lang": "en",
      "text": "2021, I was having a meal with Ilia. He",
      "offset": 1378.48,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "asked me what my AGI timelines are. Very",
      "offset": 1381.52,
      "duration": 3.279
    },
    {
      "lang": "en",
      "text": "standard SF question. And I told him",
      "offset": 1383.12,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "like, look, I think it's actually quite",
      "offset": 1384.799,
      "duration": 3.281
    },
    {
      "lang": "en",
      "text": "far away because we're going to need to",
      "offset": 1386,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "figure out this reasoning paradigm in a",
      "offset": 1388.08,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "very general way. And with things like",
      "offset": 1389.36,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "LM, LM are very general, but they don't",
      "offset": 1390.96,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "have a reasoning paradigm that's very",
      "offset": 1393.6,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "general. And until they do, they're",
      "offset": 1394.88,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "going to be limited in what they can do.",
      "offset": 1397.919,
      "duration": 2.961
    },
    {
      "lang": "en",
      "text": "Like, we're going to scale it. Sure,",
      "offset": 1400,
      "duration": 1.84
    },
    {
      "lang": "en",
      "text": "we're going to scale these things up by",
      "offset": 1400.88,
      "duration": 2
    },
    {
      "lang": "en",
      "text": "a few more orders of magnitude. They're",
      "offset": 1401.84,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "going to become more capable, but we're",
      "offset": 1402.88,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "not going to see super intelligence from",
      "offset": 1404.72,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "just that. And like, yes, if we had a",
      "offset": 1406.799,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "quadrillion dollars to train these",
      "offset": 1409.44,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "models, then maybe we would, but like",
      "offset": 1411.039,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "you're going to hit the limits of what's",
      "offset": 1412.799,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "economically feasible before you get to",
      "offset": 1414.64,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "super intelligence, unless you have a",
      "offset": 1416.48,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "reasoning paradigm. And I was convinced",
      "offset": 1418.48,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "incorrectly that the reasoning paradigm",
      "offset": 1420.72,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "would take a long time to figure out",
      "offset": 1422.48,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "because it's like this big unanswered",
      "offset": 1423.919,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "research question and you know Ilia",
      "offset": 1425.36,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "agreed with me and he said like yeah you",
      "offset": 1427.919,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "know I think we need this like",
      "offset": 1430,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "additional paradigm but his take was",
      "offset": 1431.6,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "that like maybe it's not that hard. I I",
      "offset": 1433.76,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "didn't know it at the time but like he",
      "offset": 1436,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "and others at OpenAI had also been",
      "offset": 1437.28,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "thinking about this. They had also been",
      "offset": 1439.36,
      "duration": 2.48
    },
    {
      "lang": "en",
      "text": "thinking about RL. They had been working",
      "offset": 1440.32,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "on it and I think they had some success",
      "offset": 1441.84,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "but like you know with most research",
      "offset": 1444.559,
      "duration": 2.641
    },
    {
      "lang": "en",
      "text": "like it does you have to iterate on",
      "offset": 1445.76,
      "duration": 2.399
    },
    {
      "lang": "en",
      "text": "things. You have to try out different",
      "offset": 1447.2,
      "duration": 2.64
    },
    {
      "lang": "en",
      "text": "ideas. you have to yeah try different",
      "offset": 1448.159,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "things and then also as the models",
      "offset": 1449.84,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "become more capable as they become",
      "offset": 1451.679,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "faster it becomes easier to iterate on",
      "offset": 1453.679,
      "duration": 6.081
    },
    {
      "lang": "en",
      "text": "experiments and I think that the work",
      "offset": 1455.679,
      "duration": 7.441
    },
    {
      "lang": "en",
      "text": "that they did even though it didn't like",
      "offset": 1459.76,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "result in a reasoning paradigm it all",
      "offset": 1463.12,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "builds on top of previous work right so",
      "offset": 1464.799,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "they built a lot of things that over",
      "offset": 1467.2,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "over time led to this reasoning paradigm",
      "offset": 1469.679,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "for listeners gnome can talk about this",
      "offset": 1471.84,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "but the rumor is that that thing was",
      "offset": 1473.84,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "cenamed GPT0 if you want to search for",
      "offset": 1475.6,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "that that line of work. I think there",
      "offset": 1477.76,
      "duration": 3.279
    },
    {
      "lang": "en",
      "text": "was a time where like basically Aro kind",
      "offset": 1479.6,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "of went through a dark age when everyone",
      "offset": 1481.039,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "like went all in on it and then nothing",
      "offset": 1483.12,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "happens and they gave up and like now",
      "offset": 1485.679,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "it's like kind of the golden age again.",
      "offset": 1487.919,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "So that's what I'm like trying to",
      "offset": 1490.32,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "identify like why what is it and it",
      "offset": 1492.32,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "could just be that we have smarter base",
      "offset": 1494.32,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "models and better data. I don't think",
      "offset": 1495.84,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "it's just that we have smarter base",
      "offset": 1497.76,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "models. I I think it's that yeah so I we",
      "offset": 1498.96,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "did end up getting a big success with",
      "offset": 1501.44,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "with reasoning and I but I think it was",
      "offset": 1504.159,
      "duration": 5.441
    },
    {
      "lang": "en",
      "text": "in many ways a gradual thing to some",
      "offset": 1506.24,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "extent it was gradual you know like",
      "offset": 1509.6,
      "duration": 2.24
    },
    {
      "lang": "en",
      "text": "there were signs there were signs of",
      "offset": 1510.72,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "life and then we like you know iterated",
      "offset": 1511.84,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "and tried out some more things we got",
      "offset": 1513.84,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "like better signs of life I think it was",
      "offset": 1515.12,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "around like no november 2023 or October",
      "offset": 1517.039,
      "duration": 5.841
    },
    {
      "lang": "en",
      "text": "2023 when I think I was convinced that",
      "offset": 1519.919,
      "duration": 5.281
    },
    {
      "lang": "en",
      "text": "we had like very conclusive signs of",
      "offset": 1522.88,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "life that like oh this this was going to",
      "offset": 1525.2,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "be this is the paradigm and it's going",
      "offset": 1526.559,
      "duration": 2.48
    },
    {
      "lang": "en",
      "text": "to be a big",
      "offset": 1528,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "that was in many ways a a gradual thing.",
      "offset": 1529.039,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "I think what OpenAI did well is like",
      "offset": 1531.6,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "when we got those signs of life, they",
      "offset": 1533.679,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "recognized it for what it was and",
      "offset": 1536.799,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "invested heavily in in scaling it up.",
      "offset": 1538.64,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "And I think that's that's ultimately",
      "offset": 1541.039,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "what what led to reasoning models",
      "offset": 1543.6,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "arriving when they did. Was there any",
      "offset": 1544.799,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "disagreement internally especially",
      "offset": 1547.2,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "because like you know OpenAI kind of",
      "offset": 1549.12,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "pioneer pre-training scaling, you know,",
      "offset": 1550.559,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "and kind of like computers all you need",
      "offset": 1552.64,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "and then you're kind of saying maybe",
      "offset": 1554.32,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "that's not how we get there. Was it",
      "offset": 1555.84,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "clear to everybody that like, okay, this",
      "offset": 1557.52,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "is going to work or was it",
      "offset": 1559.279,
      "duration": 2.961
    },
    {
      "lang": "en",
      "text": "controversial? There's always different",
      "offset": 1560.64,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "opinions about this stuff. I think there",
      "offset": 1562.24,
      "duration": 1.84
    },
    {
      "lang": "en",
      "text": "were some people that felt that",
      "offset": 1563.36,
      "duration": 1.84
    },
    {
      "lang": "en",
      "text": "pre-training was all we need and we",
      "offset": 1564.08,
      "duration": 2.16
    },
    {
      "lang": "en",
      "text": "scaled it up to infinity and we were",
      "offset": 1565.2,
      "duration": 2.479
    },
    {
      "lang": "en",
      "text": "there. I think a lot of the leadership",
      "offset": 1566.24,
      "duration": 3.039
    },
    {
      "lang": "en",
      "text": "actually at OpenAI recognized that there",
      "offset": 1567.679,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "was another paradigm that was needed and",
      "offset": 1569.279,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "that was why they were investing all",
      "offset": 1570.559,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "this like research effort into this like",
      "offset": 1572.64,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "RL um stuff. And I think that's also to",
      "offset": 1575.76,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "the credit of opening eye that like okay",
      "offset": 1578.32,
      "duration": 2.4
    },
    {
      "lang": "en",
      "text": "yes they figured out the pre-training",
      "offset": 1579.6,
      "duration": 2.48
    },
    {
      "lang": "en",
      "text": "paradigm and they were very focused on",
      "offset": 1580.72,
      "duration": 2.48
    },
    {
      "lang": "en",
      "text": "scaling that up. In fact, the vast",
      "offset": 1582.08,
      "duration": 2.4
    },
    {
      "lang": "en",
      "text": "majority of resources were focused on",
      "offset": 1583.2,
      "duration": 2.719
    },
    {
      "lang": "en",
      "text": "scaling that up. But they also",
      "offset": 1584.48,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "recognized the value that that something",
      "offset": 1585.919,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "else was going to be needed and it was",
      "offset": 1587.84,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "worth researching putting researcher",
      "offset": 1589.76,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "effort into into other directions to",
      "offset": 1591.84,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "figure out what that extra paradigm was",
      "offset": 1593.76,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "going to be. There was a lot of debate",
      "offset": 1595.36,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "about first of all like what is that",
      "offset": 1597.44,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "extra paradigm. So I think a lot of the",
      "offset": 1599.2,
      "duration": 6.88
    },
    {
      "lang": "en",
      "text": "researchers looked at reasoning and and",
      "offset": 1602.32,
      "duration": 6.64
    },
    {
      "lang": "en",
      "text": "RL was not really about scaling test",
      "offset": 1606.08,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "time compute. It was more about data",
      "offset": 1608.96,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "efficiency because you know the feeling",
      "offset": 1610.72,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "was that well we have tons and tons of",
      "offset": 1612.48,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "compute but we actually are more limited",
      "offset": 1614.559,
      "duration": 3.281
    },
    {
      "lang": "en",
      "text": "by data. So there's there's the data",
      "offset": 1616.4,
      "duration": 2.879
    },
    {
      "lang": "en",
      "text": "wall and we're going to hit that before",
      "offset": 1617.84,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "we hit limits on on the compute. So how",
      "offset": 1619.279,
      "duration": 3.121
    },
    {
      "lang": "en",
      "text": "do we make these algorithms more data",
      "offset": 1621.12,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "efficient? They are more data efficient",
      "offset": 1622.4,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "but I think that also like um they are",
      "offset": 1624.08,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "also just like the equivalent of scaling",
      "offset": 1626.559,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "up compute also by a ton. That was",
      "offset": 1628.88,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "interesting. There's like a lot of",
      "offset": 1631.2,
      "duration": 2
    },
    {
      "lang": "en",
      "text": "debate around like okay what what",
      "offset": 1632,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "exactly are we doing here? And then I",
      "offset": 1633.2,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "think also even when we got the signs of",
      "offset": 1634.72,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "life I think there was a lot of debate",
      "offset": 1636.24,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "about the significance of it that like",
      "offset": 1638.24,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "okay how much should we invest in",
      "offset": 1640.32,
      "duration": 2.959
    },
    {
      "lang": "en",
      "text": "scaling up this paradigm. I think",
      "offset": 1641.84,
      "duration": 2.64
    },
    {
      "lang": "en",
      "text": "especially when you're when you're in a",
      "offset": 1643.279,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "small company like you know open AI like",
      "offset": 1644.48,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "in 2023 was not as big as it is today",
      "offset": 1646.64,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "and compute was more constrained than it",
      "offset": 1649.2,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "is today. And if you're investing",
      "offset": 1650.88,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "resources in in a direction that's",
      "offset": 1653.6,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "coming at the expense of something else.",
      "offset": 1655.52,
      "duration": 3.279
    },
    {
      "lang": "en",
      "text": "And so if you look at these signs of",
      "offset": 1657.36,
      "duration": 2.48
    },
    {
      "lang": "en",
      "text": "life on reasoning and you're saying",
      "offset": 1658.799,
      "duration": 2.321
    },
    {
      "lang": "en",
      "text": "like, &quot;Okay, well this looks promising.",
      "offset": 1659.84,
      "duration": 2.959
    },
    {
      "lang": "en",
      "text": "We're going to scale this up by a ton",
      "offset": 1661.12,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "and invest a lot more resources into it,",
      "offset": 1662.799,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "where are those resources coming from?&quot;",
      "offset": 1664.48,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "You have to make that tough call about",
      "offset": 1666.24,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "where to where to draw the resources",
      "offset": 1668.08,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "from. And that is a very controversial,",
      "offset": 1669.36,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "very difficult call to make um that",
      "offset": 1672.24,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "makes some people unhappy. And I think",
      "offset": 1674.399,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "there was debate about whether we're",
      "offset": 1676.48,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "focusing too much on this paradigm,",
      "offset": 1678.559,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "whether it's really a big deal, whether",
      "offset": 1681.6,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "we would see it generalize and do",
      "offset": 1683.52,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "various things. And I remember it was",
      "offset": 1685.6,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "interesting that I I talked to somebody",
      "offset": 1687.52,
      "duration": 5.279
    },
    {
      "lang": "en",
      "text": "who left OpenAI after we had discovered",
      "offset": 1689.76,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "the reasoning paradigm but before we",
      "offset": 1692.799,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "announced 01 and they ended up going to",
      "offset": 1694.48,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "a computing lab. I saw them afterwards",
      "offset": 1696.88,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "after we announced um 01 and they told",
      "offset": 1698.88,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "me that like at the time they really",
      "offset": 1701.52,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "didn't think this like reasoning thing",
      "offset": 1703.36,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "like this these O series the strawberry",
      "offset": 1705.76,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "models were like that that big of a",
      "offset": 1707.279,
      "duration": 3.041
    },
    {
      "lang": "en",
      "text": "deal. It was like they thought we were",
      "offset": 1708.72,
      "duration": 2.64
    },
    {
      "lang": "en",
      "text": "making a bigger deal of it than it",
      "offset": 1710.32,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "really deserved to be. And then when we",
      "offset": 1711.36,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "announced 01 and they saw the reaction",
      "offset": 1713.36,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "of their co-workers at this competing",
      "offset": 1715.679,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "lab about how everybody was like, &quot;Oh",
      "offset": 1717.279,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "crap, like this is a big deal.&quot; And they",
      "offset": 1719.52,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "like pivoted the whole research agenda.",
      "offset": 1721.12,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "Oh my god. To focus on this that then",
      "offset": 1724.559,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "they realized like, &quot;Oh, actually like",
      "offset": 1726.72,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "this maybe is a big deal.&quot; You know, a",
      "offset": 1728.559,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "lot of this seems obvious in retrospect,",
      "offset": 1730.799,
      "duration": 5.041
    },
    {
      "lang": "en",
      "text": "but at the time it's actually not so",
      "offset": 1732.559,
      "duration": 6.321
    },
    {
      "lang": "en",
      "text": "obvious and be quite difficult to",
      "offset": 1735.84,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "recognize something for what it is. I",
      "offset": 1738.88,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "mean, OpenAI has like a great history of",
      "offset": 1741.2,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "just making the right bet. I feel GBD",
      "offset": 1743.6,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "models are kind of similar, right? Where",
      "offset": 1745.84,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "like it started with games and RL and",
      "offset": 1747.44,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "then it's like maybe we can just scale",
      "offset": 1750,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "these language models instead. And um",
      "offset": 1751.76,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "I'm just impressed by the leadership and",
      "offset": 1753.679,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "obviously the the research team that",
      "offset": 1756.32,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "keeps coming out with these insights.",
      "offset": 1758,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "Looking back on it today, it it might",
      "offset": 1759.679,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "seem obvious that like, oh, of course,",
      "offset": 1761.52,
      "duration": 3.039
    },
    {
      "lang": "en",
      "text": "like these models get better with",
      "offset": 1763.039,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "scaled, so you should just scale them up",
      "offset": 1764.559,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "a ton and it'll get better. But it it",
      "offset": 1766.159,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "really is the best research is obvious",
      "offset": 1769.039,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "in retrospect and at the time it's it's",
      "offset": 1771.279,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "not as obvious as it might seem today.",
      "offset": 1773.76,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "Follow questions on data efficiency. Uh",
      "offset": 1775.679,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "this is this is a pet topic of mine. It",
      "offset": 1777.76,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "seems that our current methods of",
      "offset": 1780,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "learning are so inefficient still right",
      "offset": 1781.84,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "like compared to the existence proof of",
      "offset": 1784.48,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "humans. We take five samples and we",
      "offset": 1786.159,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "learn learn something. Machines 200",
      "offset": 1788.48,
      "duration": 5.439
    },
    {
      "lang": "en",
      "text": "maybe you know per per like whatever",
      "offset": 1792.159,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "data point you might need. anyone doing",
      "offset": 1793.919,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "anything interesting in data efficiency",
      "offset": 1796,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "or do you think like there's just a",
      "offset": 1797.84,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "fundamental inefficiency that machine",
      "offset": 1799.2,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "learning has that will just always be",
      "offset": 1801.679,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "there compared to humans? I think it's a",
      "offset": 1804.08,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "good point that if you look at the",
      "offset": 1806.08,
      "duration": 2.24
    },
    {
      "lang": "en",
      "text": "amount of data these models are trained",
      "offset": 1807.12,
      "duration": 3.039
    },
    {
      "lang": "en",
      "text": "on and you compare it to like the amount",
      "offset": 1808.32,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "of data that a human observes to get the",
      "offset": 1810.159,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "same performance. I guess pre-training",
      "offset": 1812.48,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "it's a little hard to make an apples",
      "offset": 1814.72,
      "duration": 2.48
    },
    {
      "lang": "en",
      "text": "apples comparison because like I don't",
      "offset": 1816,
      "duration": 2.399
    },
    {
      "lang": "en",
      "text": "know how much how many tokens does a",
      "offset": 1817.2,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "baby actually absorb when they're",
      "offset": 1818.399,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "developing. But I think it's a fair",
      "offset": 1820.24,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "statement to say that these models are",
      "offset": 1822.399,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "are less data efficient than humans. And",
      "offset": 1823.44,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "I think that that's an unsolved research",
      "offset": 1825.6,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "question and probably one of the most",
      "offset": 1828.32,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "important unsolved research questions.",
      "offset": 1830.64,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "Maybe more important than algorithmic",
      "offset": 1832.48,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "improvements cuz you can just you can we",
      "offset": 1834.559,
      "duration": 5.521
    },
    {
      "lang": "en",
      "text": "we can increase the supply of data out",
      "offset": 1837.44,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "of the existing set of the worlds and",
      "offset": 1840.08,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "humans. I guess okay. So a couple",
      "offset": 1842.48,
      "duration": 2.799
    },
    {
      "lang": "en",
      "text": "thoughts on that. Like one is that the",
      "offset": 1844.08,
      "duration": 2.16
    },
    {
      "lang": "en",
      "text": "answer might be an algorithmic",
      "offset": 1845.279,
      "duration": 1.921
    },
    {
      "lang": "en",
      "text": "improvement. Like maybe maybe",
      "offset": 1846.24,
      "duration": 2.799
    },
    {
      "lang": "en",
      "text": "algorithmic improvements do lead to",
      "offset": 1847.2,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "greater data efficiency. And the second",
      "offset": 1849.039,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "thing is that like it's not like humans",
      "offset": 1851.039,
      "duration": 5.601
    },
    {
      "lang": "en",
      "text": "learn from just reading the internet. So",
      "offset": 1853.039,
      "duration": 6.081
    },
    {
      "lang": "en",
      "text": "um I think it's certainly easiest to",
      "offset": 1856.64,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "learn from just like data that's on the",
      "offset": 1859.12,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "internet. Um but I don't think that's",
      "offset": 1861.2,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "like the limit of what data you could",
      "offset": 1862.64,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "collect. The last followup before we",
      "offset": 1865.2,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "change topics to coding. any other just",
      "offset": 1867.039,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "anecdotes or insights from Ilia just in",
      "offset": 1869.44,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "general cuz like you've worked with him",
      "offset": 1871.52,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "so there's not that many people that we",
      "offset": 1873.84,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "can talk to that have worked with him. I",
      "offset": 1875.919,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "think I've just been very very impressed",
      "offset": 1878.08,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "with his vision that I think like",
      "offset": 1879.76,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "especially when I joined and and I saw",
      "offset": 1881.36,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "you know the internal documents at at",
      "offset": 1883.36,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "OpenAI of like what he had been thinking",
      "offset": 1884.72,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "about back in like 2021 2022 even",
      "offset": 1886.799,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "earlier I I was very impressed that he",
      "offset": 1889.919,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "had a clear vision of like where this",
      "offset": 1892.799,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "was all going and what was needed. some",
      "offset": 1894.24,
      "duration": 5.439
    },
    {
      "lang": "en",
      "text": "of his emails from 201617 when they were",
      "offset": 1896.96,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "founding open AI was published and even",
      "offset": 1899.679,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "then he was talking about how he thinks",
      "offset": 1902.24,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "like one big experiment is much more",
      "offset": 1904.88,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "valuable than 100 small ones that was",
      "offset": 1906.48,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "like a core insight that differentiated",
      "offset": 1908.559,
      "duration": 3.521
    },
    {
      "lang": "en",
      "text": "them from brain for example. It just",
      "offset": 1910.32,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "seems very insightful that he just sees",
      "offset": 1912.08,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "things much more clearly than others and",
      "offset": 1914.159,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "I I just wonder what his production",
      "offset": 1915.919,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "function is like how do you make a human",
      "offset": 1918.159,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "like that and how do you improve your",
      "offset": 1920.399,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "own thinking to better model it. I I",
      "offset": 1922.08,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "mean I think it is true that I mean one",
      "offset": 1925.36,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "of openi's big success was betting on",
      "offset": 1926.88,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "the scaling paradigm. It is just kind of",
      "offset": 1929.519,
      "duration": 3.121
    },
    {
      "lang": "en",
      "text": "odd because you know they were not the",
      "offset": 1931.12,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "biggest lab you know it was like",
      "offset": 1932.64,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "difficult for them to scale back then it",
      "offset": 1933.84,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "was much more common to do like a lot of",
      "offset": 1936,
      "duration": 3.519
    },
    {
      "lang": "en",
      "text": "small experiments more academic style",
      "offset": 1937.36,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "people were trying to figure out these",
      "offset": 1939.519,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "um various like algorithmic improvements",
      "offset": 1941.76,
      "duration": 5.519
    },
    {
      "lang": "en",
      "text": "and openi bet pretty early on like large",
      "offset": 1944.08,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "scale. We had David Wan on who I think",
      "offset": 1947.279,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "was VPenge at the time of GPT1 and 2 and",
      "offset": 1949.76,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "he talked about how the differences",
      "offset": 1952.559,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "between Brain and OpenAI was basically",
      "offset": 1953.919,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "the cause of the Google's inability to",
      "offset": 1955.76,
      "duration": 5.759
    },
    {
      "lang": "en",
      "text": "come out with a scaled model like just",
      "offset": 1959.12,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "structurally everyone had allocated",
      "offset": 1961.519,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "compute and you had to pull resources",
      "offset": 1963.919,
      "duration": 3.281
    },
    {
      "lang": "en",
      "text": "together to make bets and you just",
      "offset": 1965.6,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "couldn't. I think that's true that",
      "offset": 1967.2,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "OpenAI was structured differently and I",
      "offset": 1968.96,
      "duration": 2.959
    },
    {
      "lang": "en",
      "text": "think that really helped them like",
      "offset": 1970.799,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "OpenAI functions a lot like a startup",
      "offset": 1971.919,
      "duration": 5.281
    },
    {
      "lang": "en",
      "text": "and other places tended to function more",
      "offset": 1974.399,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "like universities or or you know",
      "offset": 1977.2,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "research labs as they traditionally",
      "offset": 1979.6,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "existed. the way that OpenAI operates",
      "offset": 1981.679,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "more like as a startup with this mission",
      "offset": 1983.919,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "of building AGI and and super",
      "offset": 1985.76,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "intelligence that helped them organize,",
      "offset": 1988.08,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "collaborate, pull resources together,",
      "offset": 1991.84,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "make hard choices about like how to",
      "offset": 1993.84,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "allocate resources and I think a lot of",
      "offset": 1995.44,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "the other labs like have now been trying",
      "offset": 1997.44,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "to adopt paradigms more like that like",
      "offset": 1999.44,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "setups more like that. Let's talk about",
      "offset": 2001.6,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "maybe the killer use case at least in my",
      "offset": 2003.519,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "mind of these models which is coding.",
      "offset": 2005.679,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "Yeah, you released codeex recently, but",
      "offset": 2007.44,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "I would love to talk through the gnome",
      "offset": 2009.679,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "brown coding stack. What models you use,",
      "offset": 2011.12,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "how you interact with them, cursor, wind",
      "offset": 2013.36,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "surf. Uh lately I've been using windsurf",
      "offset": 2015.36,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "and codeex like actually a lot of",
      "offset": 2018.08,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "codecs. I've been having a lot of fun.",
      "offset": 2019.44,
      "duration": 3.119
    },
    {
      "lang": "en",
      "text": "Like you just give it a task and it just",
      "offset": 2020.96,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "goes off and does it and comes back five",
      "offset": 2022.559,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "minutes later with like a you know pull",
      "offset": 2024.08,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "request. And is it core research task or",
      "offset": 2025.519,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "like side stuff that you don't super",
      "offset": 2027.919,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "care about? I wouldn't say it's like",
      "offset": 2029.519,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "side stuff. I would say basically",
      "offset": 2031.679,
      "duration": 6.961
    },
    {
      "lang": "en",
      "text": "anything that I would normally try to",
      "offset": 2034.399,
      "duration": 7.441
    },
    {
      "lang": "en",
      "text": "code up, I try to do it with codeex",
      "offset": 2038.64,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "first. Well, for you it's free, but",
      "offset": 2041.84,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "yeah, for everybody it's free right now.",
      "offset": 2043.519,
      "duration": 2.721
    },
    {
      "lang": "en",
      "text": "And I think that's partly because it's",
      "offset": 2044.96,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "the it's the most effective way for me",
      "offset": 2046.24,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "to do it and also it's good for me to",
      "offset": 2047.76,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "get experience working with this",
      "offset": 2050.32,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "technology and then also like seeing the",
      "offset": 2052.72,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "shortcomings of it. It just helps me",
      "offset": 2054.24,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "like better understand like okay this is",
      "offset": 2056,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "the the limits of these models and like",
      "offset": 2057.839,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "what we need to push on next. Have you",
      "offset": 2059.359,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "felt the AGI? I felt the AJ multiple",
      "offset": 2061.04,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "times. Yes.",
      "offset": 2063.04,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "Like like um how should people push",
      "offset": 2064.96,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "codeex in ways that you've you've done",
      "offset": 2067.76,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "and you know I think you you see it",
      "offset": 2070.24,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "before others cuz obviously you were",
      "offset": 2071.76,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "closer to it. I think anybody can use",
      "offset": 2073.28,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "codecs and feel the AGI. It's kind of",
      "offset": 2075.119,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "funny how like you feel the AGI and then",
      "offset": 2078.48,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "you get used to it very quickly, you",
      "offset": 2080.159,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "know? So So it's really like",
      "offset": 2081.919,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "dissatisfied with like where it's",
      "offset": 2084.879,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "lacking. Yeah, I know. And you know it's",
      "offset": 2086.48,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "it's magical one day. I was actually",
      "offset": 2088.399,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "looking back at the old uh Sora videos",
      "offset": 2089.76,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "when they were announced cuz like you",
      "offset": 2091.919,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "remember when Sora came out it was just",
      "offset": 2093.52,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "like the biggest news ever. It was just",
      "offset": 2094.879,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "magical. You look at that and you're",
      "offset": 2096.72,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "like it's like it's really here like",
      "offset": 2097.839,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "this is AGI. But you look at it now and",
      "offset": 2099.68,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "it's kind of like oh you know the people",
      "offset": 2101.68,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "like don't move like very organically",
      "offset": 2104.48,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "and it's like there's like a lack of",
      "offset": 2106.24,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "consistency in some ways and you see all",
      "offset": 2107.92,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "these flaws in it now that you just",
      "offset": 2109.52,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "didn't really notice when it was first",
      "offset": 2112,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "came out and yeah you get used to this",
      "offset": 2113.599,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "technology very quickly and but I think",
      "offset": 2115.92,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "what's cool about it is that because",
      "offset": 2118,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "it's developing so quickly you get those",
      "offset": 2118.88,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "feely AGI moments like every few months.",
      "offset": 2120.8,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "So something else is going to come out",
      "offset": 2123.599,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "and just like it's magical to you and uh",
      "offset": 2124.96,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "and then you get used to it very",
      "offset": 2127.119,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "quickly. Yeah. What are your winds surf",
      "offset": 2128.16,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "pro tips now that you've immersed in it?",
      "offset": 2130.56,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "I think one thing I'm surprised by is",
      "offset": 2133.44,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "how few people I mean maybe your",
      "offset": 2134.72,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "audience is going to be more comfortable",
      "offset": 2136.32,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "with reasoning models and like use",
      "offset": 2138.48,
      "duration": 3.119
    },
    {
      "lang": "en",
      "text": "reasoning models more. But I'm surprised",
      "offset": 2139.68,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "at how many people don't even know that",
      "offset": 2141.599,
      "duration": 6.161
    },
    {
      "lang": "en",
      "text": "03 exists. Like I've been using it dayto",
      "offset": 2144.88,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "day. It's basically replaced Google",
      "offset": 2147.76,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "search for me. Like I just use it all",
      "offset": 2150.48,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "the time. like and also for things like",
      "offset": 2151.92,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "coding like I I tend to just use the",
      "offset": 2154.64,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "reasoning models. My suggestion is like",
      "offset": 2156.8,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "if people are not have not tried the",
      "offset": 2158.8,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "reasoning models yet because like",
      "offset": 2161.04,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "honestly like we do like people love",
      "offset": 2162.48,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "them people that use it love them",
      "offset": 2164.64,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "obviously a lot more people use GBD40 um",
      "offset": 2166.16,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "and just like the default and what on",
      "offset": 2168.8,
      "duration": 3.039
    },
    {
      "lang": "en",
      "text": "chatbt and that kind of stuff I think",
      "offset": 2170.079,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "it's worth trying the reasoning models",
      "offset": 2171.839,
      "duration": 2.561
    },
    {
      "lang": "en",
      "text": "like I think people would be surprised",
      "offset": 2173.119,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "at what they can do. I use Windsurf",
      "offset": 2174.4,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "daily and they still haven't actually",
      "offset": 2176.88,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "enabled it as like a default in",
      "offset": 2178.88,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "Windsurf. Like I always have to dig up",
      "offset": 2181.359,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "like type in 03 and then it then it's",
      "offset": 2183.2,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "like oh yeah that that exists. It's it's",
      "offset": 2185.2,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "uh it's weird. I would say like my",
      "offset": 2187.359,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "struggle with it has been that it's",
      "offset": 2189.44,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "takes so long to reason and actually",
      "offset": 2191.44,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "break out of flow. I think that is true.",
      "offset": 2193.359,
      "duration": 3.041
    },
    {
      "lang": "en",
      "text": "Yes. And and I think this this is one of",
      "offset": 2195.2,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "the advantages of Codeex that like okay",
      "offset": 2196.4,
      "duration": 2.719
    },
    {
      "lang": "en",
      "text": "you can give it a task that's kind of",
      "offset": 2197.92,
      "duration": 2.4
    },
    {
      "lang": "en",
      "text": "self-contained and like it can go off",
      "offset": 2199.119,
      "duration": 2.401
    },
    {
      "lang": "en",
      "text": "and do its thing and come back 10",
      "offset": 2200.32,
      "duration": 3.039
    },
    {
      "lang": "en",
      "text": "minutes later. And I can see that if",
      "offset": 2201.52,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "you're doing if you're using this thing",
      "offset": 2203.359,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "as like more like a like a pair",
      "offset": 2205.28,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "programmer kind of thing, then yeah, you",
      "offset": 2207.119,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "want to use GP4.1 or something like",
      "offset": 2209.44,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "that. What do you think are the most",
      "offset": 2211.04,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "broken part of the development cycle",
      "offset": 2213.04,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "with AI? Like in my mind, it's like um",
      "offset": 2214.88,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "pull request review. Like for me, like I",
      "offset": 2217.359,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "use codecs all the time and then I got",
      "offset": 2219.28,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "all these pull requests and it's kind of",
      "offset": 2221.119,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "hard to like go through all of them.",
      "offset": 2222.96,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "What other thing would you like people",
      "offset": 2224.88,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "to build to make this CVN more scalable?",
      "offset": 2226.72,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "I think it's really on us to build a lot",
      "offset": 2229.44,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "more stuff. These models are very",
      "offset": 2231.04,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "limited in in in some ways. I think I",
      "offset": 2232.8,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "find it frustrating that, you know, you",
      "offset": 2235.119,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "ask them to do something and they spend",
      "offset": 2237.52,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "10 minutes doing it and then you ask",
      "offset": 2239.119,
      "duration": 5.041
    },
    {
      "lang": "en",
      "text": "them to do something pretty similar and",
      "offset": 2242.16,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "then they go spend 10 minutes doing it",
      "offset": 2244.16,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "and like, you know, it's I I think I",
      "offset": 2245.76,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "describe them as like they're geniuses,",
      "offset": 2247.52,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "but it's their first day on the job, you",
      "offset": 2249.76,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "know, and that's like kind of annoying.",
      "offset": 2251.359,
      "duration": 2.641
    },
    {
      "lang": "en",
      "text": "Like even the the smartest person on",
      "offset": 2252.56,
      "duration": 2.559
    },
    {
      "lang": "en",
      "text": "earth when they're when it's their first",
      "offset": 2254,
      "duration": 2.56
    },
    {
      "lang": "en",
      "text": "day on the job, you know, they're not",
      "offset": 2255.119,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "going to be like as useful as you would",
      "offset": 2256.56,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "like them to be. So I think being able",
      "offset": 2257.839,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "to get more experience and like act like",
      "offset": 2260.32,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "somebody that's actually been on the job",
      "offset": 2263.359,
      "duration": 2.961
    },
    {
      "lang": "en",
      "text": "for like 6 months instead of what one",
      "offset": 2264.56,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "day I think would make them a lot more",
      "offset": 2266.32,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "useful but that's really on us to build",
      "offset": 2268.32,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "to build that capability. Do you think a",
      "offset": 2270.16,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "lot of it is like GPU constraint for",
      "offset": 2272.079,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "you? Like if I think about codeex why is",
      "offset": 2274.079,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "it asking me to set up the environment",
      "offset": 2276.079,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "myself when like the model if I ask code",
      "offset": 2277.68,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "three to like create an environment",
      "offset": 2279.76,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "setup script for a repo I'm sure it'll",
      "offset": 2281.359,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "be able to do it but today in the",
      "offset": 2283.52,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "product I have to do it. So I'm",
      "offset": 2285.28,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "wondering in your mind, could these be a",
      "offset": 2286.88,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "lot more if we just again put more test",
      "offset": 2289.52,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "time compute on them or do you think",
      "offset": 2291.92,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "there's like a fundamental model",
      "offset": 2293.599,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "capability limitation today that we",
      "offset": 2295.04,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "still need a lot of like human harnesses",
      "offset": 2297.2,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "around it? I think that we're in an",
      "offset": 2299.28,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "awkward state right now where like",
      "offset": 2300.64,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "progress is very fast and there's things",
      "offset": 2302.32,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "that are like clearly we could do this",
      "offset": 2303.839,
      "duration": 2.801
    },
    {
      "lang": "en",
      "text": "and the models would be better. We're",
      "offset": 2305.2,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "going to get to it. It's um you're just",
      "offset": 2306.64,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "limited by how many hours there are in",
      "offset": 2309.28,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "the day, you know? So progress can only",
      "offset": 2310.56,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "proceed so quickly. We're trying to get",
      "offset": 2313.2,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "to everything as fast as we can and and",
      "offset": 2315.68,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "I think that the 03 is not where the",
      "offset": 2317.119,
      "duration": 5.041
    },
    {
      "lang": "en",
      "text": "technology will be in six months. I like",
      "offset": 2320.16,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "that question overall in like there's a",
      "offset": 2322.16,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "software development life cycle not just",
      "offset": 2324.079,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "generation of the code like from issue",
      "offset": 2326.16,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "to PR basically is is like the the",
      "offset": 2328.079,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "typical commentary of that and then",
      "offset": 2330.079,
      "duration": 2.881
    },
    {
      "lang": "en",
      "text": "there's the winds surf side which is",
      "offset": 2331.76,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "insider your ID like what else right",
      "offset": 2332.96,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "pull request review is like something",
      "offset": 2334.96,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "that people don't really there are",
      "offset": 2336.72,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "startups that built around it it's not",
      "offset": 2339.119,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "something that codeex does and it could",
      "offset": 2341.52,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "and so like then there's like what else",
      "offset": 2343.92,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "is there you know that is sort of rate",
      "offset": 2346.079,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "limiting the amount of software you",
      "offset": 2348.079,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "could be iterating on it's an open",
      "offset": 2349.28,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "question. I don't I don't I don't know",
      "offset": 2351.359,
      "duration": 2.801
    },
    {
      "lang": "en",
      "text": "if there's an answer. Anything else on",
      "offset": 2352.32,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "on Ace in general? Like where do you",
      "offset": 2354.16,
      "duration": 7.04
    },
    {
      "lang": "en",
      "text": "think this goes just in form factors or",
      "offset": 2356.96,
      "duration": 6.56
    },
    {
      "lang": "en",
      "text": "what will we be looking at this time",
      "offset": 2361.2,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "next year in terms of how things are how",
      "offset": 2363.52,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "what models were able to do that they're",
      "offset": 2366.64,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "not able to today? I don't think it's",
      "offset": 2368.4,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "going to be limited to ASU, you know. I",
      "offset": 2370.16,
      "duration": 2.4
    },
    {
      "lang": "en",
      "text": "think I don't think it's going to be",
      "offset": 2371.599,
      "duration": 2.161
    },
    {
      "lang": "en",
      "text": "limited to software engineering. I think",
      "offset": 2372.56,
      "duration": 3.279
    },
    {
      "lang": "en",
      "text": "it's going to be able to do a lot of",
      "offset": 2373.76,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "remote work kind of tasks. Yeah. Like",
      "offset": 2375.839,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "freelancer type Upwork. Yeah. Yeah. Or",
      "offset": 2378.16,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "just like even things that are not",
      "offset": 2380.72,
      "duration": 2.879
    },
    {
      "lang": "en",
      "text": "necessarily software engineering. Okay.",
      "offset": 2381.839,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "So, the way that I think about it is",
      "offset": 2383.599,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "like anybody that's doing a remote work",
      "offset": 2385.04,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "kind of job, I think it's valuable to",
      "offset": 2387.839,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "become familiar with the technology and",
      "offset": 2389.92,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "like kind of get a sense of like what it",
      "offset": 2391.44,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "can do, what it can't do, what it's good",
      "offset": 2392.8,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "at, what it's not good at because I",
      "offset": 2394.32,
      "duration": 3.519
    },
    {
      "lang": "en",
      "text": "think the the breadth of things that",
      "offset": 2396.16,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "it's going to be able to do is going to",
      "offset": 2397.839,
      "duration": 2.881
    },
    {
      "lang": "en",
      "text": "expand over time as well. I feel like",
      "offset": 2398.96,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "virtual assistants might be the next",
      "offset": 2400.72,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "thing after as then because they're the",
      "offset": 2403.04,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "most easily like you know virtual assist",
      "offset": 2405.2,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "like hire someone in the Philippines",
      "offset": 2407.359,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "someone uh who who just look through",
      "offset": 2409.04,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "your email and all that because that is",
      "offset": 2410.56,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "entirely you can intercept all the",
      "offset": 2412.4,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "inputs and all the outputs and train on",
      "offset": 2413.92,
      "duration": 5.439
    },
    {
      "lang": "en",
      "text": "that and maybe open just buys a virtual",
      "offset": 2416.24,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "assistant company. Yeah, I think what",
      "offset": 2419.359,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "I'm looking forward to is that um for",
      "offset": 2420.72,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "things like virtual assistants, the the",
      "offset": 2422.96,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "models like if they're aligned well,",
      "offset": 2425.119,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "they could end up being like really",
      "offset": 2427.68,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "preferable for that for that kind of",
      "offset": 2429.359,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "work. You know, if there's always this",
      "offset": 2431.04,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "like principal agent problem where if",
      "offset": 2433.44,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "you delegate a task to somebody, then",
      "offset": 2435.119,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "like are they really aligned with like",
      "offset": 2437.04,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "doing it as you would want it to be done",
      "offset": 2438.8,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "and u just as cheaply as quickly as they",
      "offset": 2440.8,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "can. Yeah. Yeah. And so if you have an",
      "offset": 2443.44,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "AI model that's like actually really",
      "offset": 2445.2,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "aligned to you and your preferences,",
      "offset": 2446.56,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "then that could end up doing a way",
      "offset": 2449.76,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "better job than a human could. Well, not",
      "offset": 2451.2,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "not it's doing a better job than a human",
      "offset": 2452.96,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "could, but like it's doing a better job",
      "offset": 2454.4,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "than a human would. That word alignment,",
      "offset": 2455.68,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "by the way, I think there's like an",
      "offset": 2457.52,
      "duration": 5.839
    },
    {
      "lang": "en",
      "text": "interesting overwriting or uh homorphism",
      "offset": 2459.359,
      "duration": 6.401
    },
    {
      "lang": "en",
      "text": "between safety alignment and instruction",
      "offset": 2463.359,
      "duration": 5.441
    },
    {
      "lang": "en",
      "text": "following alignment. And I wonder where",
      "offset": 2465.76,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "they diverge. Okay, so I think where it",
      "offset": 2468.8,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "diverges is like what do you want to",
      "offset": 2470.8,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "align the models to like that? That's I",
      "offset": 2472.72,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "think a difficult question, you know,",
      "offset": 2474.24,
      "duration": 2.48
    },
    {
      "lang": "en",
      "text": "like you could say like you wanted to",
      "offset": 2475.44,
      "duration": 2.639
    },
    {
      "lang": "en",
      "text": "align it to the user. Okay. Well, what",
      "offset": 2476.72,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "happens if the user wants to build a",
      "offset": 2478.079,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "novel virus that's going to wipe out",
      "offset": 2480.4,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "half of humanity? Safety alignment. So",
      "offset": 2481.599,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "there's a question of like I think",
      "offset": 2484.4,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "alignment I think they're related, you",
      "offset": 2486.079,
      "duration": 2.961
    },
    {
      "lang": "en",
      "text": "know, and I think that the big question",
      "offset": 2487.76,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "is like what are you aligning towards?",
      "offset": 2489.04,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "Yeah. There's like humanity goals and",
      "offset": 2490.88,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "then there's your personal goals and",
      "offset": 2492.48,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "everything in between. Mhm. So that's",
      "offset": 2494.16,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "kind of I guess the individual agent and",
      "offset": 2495.92,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "you announced the you're leading the",
      "offset": 2498.8,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "multi- aent team at OpenAI. I haven't",
      "offset": 2500.48,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "really seen many announcements. Maybe I",
      "offset": 2503.119,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "missed them on what you've been working",
      "offset": 2504.88,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "on, but what can you share about",
      "offset": 2506.319,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "interesting research directions or um",
      "offset": 2508,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "anything from there? Yeah, there's uh",
      "offset": 2510.48,
      "duration": 3.119
    },
    {
      "lang": "en",
      "text": "hasn't really been announcements on",
      "offset": 2511.92,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "this. I think we're working on cool",
      "offset": 2513.599,
      "duration": 2.321
    },
    {
      "lang": "en",
      "text": "stuff and I think we'll get to announce",
      "offset": 2514.72,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "some cool stuff uh at some point. I",
      "offset": 2515.92,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "think the team in many ways is actually",
      "offset": 2518.16,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "a misnomer because we're working on more",
      "offset": 2520,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "more than just multi-agent. Multi-agent",
      "offset": 2521.28,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "is one of the things we're working on.",
      "offset": 2522.96,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "Um some other things we're working on is",
      "offset": 2524,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "just like being able to scale up test",
      "offset": 2525.76,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "time compute by by a ton. So how you",
      "offset": 2526.88,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "know we get these models thinking for 15",
      "offset": 2529.04,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "minutes now how do we get them to think",
      "offset": 2531.68,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "for hours how do we get them to think",
      "offset": 2533.52,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "for days even longer and be able to",
      "offset": 2534.72,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "solve incredibly difficult problems so",
      "offset": 2537.76,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "that's one direction that we're pursuing",
      "offset": 2540.8,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "uh multi-agent is another direction and",
      "offset": 2542.72,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "here I think there's a few different",
      "offset": 2545.359,
      "duration": 3.521
    },
    {
      "lang": "en",
      "text": "motivations uh we're interested in like",
      "offset": 2547.119,
      "duration": 2.801
    },
    {
      "lang": "en",
      "text": "both the collaborative and the",
      "offset": 2548.88,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "competitive aspect of multi- aent I",
      "offset": 2549.92,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "think the way that I describe it is",
      "offset": 2552,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "people often say in AI circles that",
      "offset": 2554.48,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "humans occupy this very narrow band of",
      "offset": 2557.2,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "intelligence and AI are just going to",
      "offset": 2559.359,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "like quickly catch up and then surpass",
      "offset": 2562.48,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "like this band of intelligence. And I",
      "offset": 2564.319,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "actually don't think that the band of",
      "offset": 2566.56,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "int of human intelligence is that",
      "offset": 2568.4,
      "duration": 2.56
    },
    {
      "lang": "en",
      "text": "narrow. I think it's actually quite",
      "offset": 2569.92,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "broad because if you compare",
      "offset": 2570.96,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "anatomically identical humans from, you",
      "offset": 2572.64,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "know, caveman times, they didn't get",
      "offset": 2575.599,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "that far in terms of like, you know,",
      "offset": 2578.16,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "what we would consider intelligence",
      "offset": 2580.079,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "today, right? They're not putting a man",
      "offset": 2581.2,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "on the moon, you know, they're not like",
      "offset": 2583.119,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "building semiconductors or nuclear",
      "offset": 2585.04,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "reactors or anything like that. And and",
      "offset": 2587.92,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "we have those today even though we as",
      "offset": 2589.92,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "humans are not anatomically different.",
      "offset": 2592.72,
      "duration": 2.56
    },
    {
      "lang": "en",
      "text": "And so what's the difference? Well, I",
      "offset": 2594,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "think the difference is that you have",
      "offset": 2595.28,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "thousands of years, a lot of humans,",
      "offset": 2597.76,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "billions of humans cooperating and",
      "offset": 2600.319,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "competing with each other, building up",
      "offset": 2602.48,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "civilization over time. And the",
      "offset": 2604.72,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "technology that we're seeing is the",
      "offset": 2606.48,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "product of this civilization. And I",
      "offset": 2607.76,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "think similarly, the AIs that we have",
      "offset": 2611.68,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "today are kind of like the cavemen of",
      "offset": 2613.44,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "AI. And and I think that if you're able",
      "offset": 2615.68,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "to um have them cooperate and compete",
      "offset": 2618.56,
      "duration": 6.88
    },
    {
      "lang": "en",
      "text": "with billions of AIs over a long period",
      "offset": 2621.92,
      "duration": 6.32
    },
    {
      "lang": "en",
      "text": "of time and build up a civilization,",
      "offset": 2625.44,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "essentially the things that they would",
      "offset": 2628.24,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "be able to produce and answer would be",
      "offset": 2629.599,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "far beyond what is possible today with",
      "offset": 2632.88,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "with the AS that we have today. Do you",
      "offset": 2635.839,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "see that being similar to maybe like Jim",
      "offset": 2637.68,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "Fan's Voyager skill library idea",
      "offset": 2640.24,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "re-saving these things or is it just the",
      "offset": 2642.88,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "models then being retrained on this new",
      "offset": 2644.8,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "knowledge because the humans then have",
      "offset": 2646.96,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "it a lot of it in the brain as they",
      "offset": 2648.64,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "grow? I think I'm going to be evasive",
      "offset": 2650.8,
      "duration": 3.279
    },
    {
      "lang": "en",
      "text": "here and say that like we're we're not",
      "offset": 2652.48,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "going to yeah we're not going to",
      "offset": 2654.079,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "we're until we have something to",
      "offset": 2656.4,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "announce which I think that I think that",
      "offset": 2658.16,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "we will in the not too distant future. I",
      "offset": 2659.599,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "think I'm going to uh be a bit vague",
      "offset": 2661.28,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "about like exactly what we're doing. But",
      "offset": 2663.68,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "I will say that the way that we are",
      "offset": 2665.44,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "approaching multi- aent in the details",
      "offset": 2667.76,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "and the way we're actually going about",
      "offset": 2670.72,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "it is I think very different from how",
      "offset": 2672,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "it's been done historically and how it's",
      "offset": 2674.8,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "being done today by by other places. Um",
      "offset": 2676.079,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "I've been in the multi- aent field for a",
      "offset": 2678.56,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "long time. I've kind of felt like the",
      "offset": 2680,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "multi- aent field has been a bit",
      "offset": 2681.76,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "misguided in some ways and the things",
      "offset": 2684.16,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "that the approaches that the field has",
      "offset": 2685.599,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "taken and like the way it's been",
      "offset": 2686.96,
      "duration": 2.879
    },
    {
      "lang": "en",
      "text": "approached. And so I think we're trying",
      "offset": 2688.319,
      "duration": 3.121
    },
    {
      "lang": "en",
      "text": "to take a very principled approach to",
      "offset": 2689.839,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "multi- aent. Sorry, I got to ask like so",
      "offset": 2691.44,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "you you can't talk about what you're",
      "offset": 2693.92,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "doing, but you can say what's misguided.",
      "offset": 2694.96,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "What's misguided? I think that a lot of",
      "offset": 2696.88,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "the approaches that have been taken have",
      "offset": 2698.56,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "been very heristic and haven't really",
      "offset": 2700.72,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "been following like the bitter lesson",
      "offset": 2702.72,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "approach to scaling and research. Okay,",
      "offset": 2705.04,
      "duration": 5.279
    },
    {
      "lang": "en",
      "text": "I think maybe this might be a good a",
      "offset": 2708.72,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "good spot. So obviously you've done a",
      "offset": 2710.319,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "lot of amazing work in in poker and I",
      "offset": 2712.24,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "think as the recent model got better I",
      "offset": 2714.72,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "was talking to one of my friends who",
      "offset": 2716.56,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "used to be a a hardcore poker grinder",
      "offset": 2718,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "and I told them I was going to interview",
      "offset": 2720.319,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "you and uh their question was at the",
      "offset": 2721.68,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "table you can get a lot of information",
      "offset": 2724.319,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "from a small sample size about how a",
      "offset": 2725.839,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "person plays but today GTO is like so",
      "offset": 2728,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "prevalent that sometimes people forget",
      "offset": 2731.359,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "that you can play exploitatively what do",
      "offset": 2733.2,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "you think is the state as you think",
      "offset": 2735.839,
      "duration": 2.721
    },
    {
      "lang": "en",
      "text": "about multi-agent and kind of like",
      "offset": 2737.04,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "competition is it always going to be",
      "offset": 2738.56,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "trying to find the optimal thing or is a",
      "offset": 2740.8,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "lot of it trying to think more in the",
      "offset": 2742.64,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "moment like how to exploit somebody. I'm",
      "offset": 2744.24,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "guessing your audience is probably not",
      "offset": 2747.119,
      "duration": 3.041
    },
    {
      "lang": "en",
      "text": "super familiar with poker terminology.",
      "offset": 2748.48,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "So, I'll just like explain this a bit.",
      "offset": 2750.16,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "Uh,",
      "offset": 2751.68,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "a lot of people think that poker is just",
      "offset": 2753.2,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "like a luck game and that's not true.",
      "offset": 2754.64,
      "duration": 2.64
    },
    {
      "lang": "en",
      "text": "It's actually there's a lot of strategy",
      "offset": 2755.92,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "in poker. So, you can win consistently",
      "offset": 2757.28,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "in poker if you're playing the right",
      "offset": 2759.599,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "strategy. So, there's different",
      "offset": 2761.119,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "approaches to poker. One is game theory",
      "offset": 2763.119,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "optimal. This is like you're playing an",
      "offset": 2766,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "unbeatable strategy and expectation.",
      "offset": 2767.68,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "Like you're just unexploitable. It's",
      "offset": 2769.2,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "kind of like in rock paper scissors. You",
      "offset": 2770.8,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "can be unbeatable in rock paper scissors",
      "offset": 2773.04,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "if you just randomly choose between rock",
      "offset": 2774.56,
      "duration": 2.559
    },
    {
      "lang": "en",
      "text": "paper and scissors with equal",
      "offset": 2776.24,
      "duration": 2.32
    },
    {
      "lang": "en",
      "text": "probability because no matter what the",
      "offset": 2777.119,
      "duration": 3.041
    },
    {
      "lang": "en",
      "text": "other guy does, you know, they're not",
      "offset": 2778.56,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "going to be able to exploit you or",
      "offset": 2780.16,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "you're going to win. You're going to",
      "offset": 2782,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "like not lose an expectation. Now, a lot",
      "offset": 2783.04,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "of people hear that and they think like,",
      "offset": 2785.839,
      "duration": 2.801
    },
    {
      "lang": "en",
      "text": "well, that also means that you're not",
      "offset": 2787.359,
      "duration": 2.48
    },
    {
      "lang": "en",
      "text": "going to win an expectation because",
      "offset": 2788.64,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "you're just playing totally randomly.",
      "offset": 2789.839,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "But in poker, if you play the",
      "offset": 2791.44,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "equilibrium strategy, it's actually",
      "offset": 2793.04,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "really difficult for the opponents to",
      "offset": 2795.04,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "figure out how to tie you, and they're",
      "offset": 2797.119,
      "duration": 3.521
    },
    {
      "lang": "en",
      "text": "going to end up making mistakes that",
      "offset": 2799.04,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "will lead you to win over the long run.",
      "offset": 2800.64,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "It might not be a massive win, but it is",
      "offset": 2803.68,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "going to be a win. If you play enough",
      "offset": 2805.92,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "hands for a long enough period of time,",
      "offset": 2807.119,
      "duration": 2.641
    },
    {
      "lang": "en",
      "text": "you're you're going to win in",
      "offset": 2808.64,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "expectation. Now, there's also",
      "offset": 2809.76,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "exploitative poker, and the idea here is",
      "offset": 2811.44,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "that you're trying to spot weaknesses in",
      "offset": 2813.599,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "how the opponent plays. you know, maybe",
      "offset": 2815.599,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "they're maybe they're not bluffing",
      "offset": 2817.68,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "enough or maybe they fold too easily to",
      "offset": 2819.04,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "a bluff. And so you start adapting from",
      "offset": 2820.96,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "the game theory optimal balance strategy",
      "offset": 2823.28,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "of like you bluff sometimes, you you",
      "offset": 2825.119,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "don't bluff sometimes to then playing a",
      "offset": 2827.119,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "very unbalanced strategy that's like,",
      "offset": 2829.52,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "oh, I'm just going to like bluff a ton",
      "offset": 2831.359,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "against this person because they always",
      "offset": 2832.88,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "fold whenever I bluff. Now, the key is",
      "offset": 2834.079,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "that there's a trade-off here because if",
      "offset": 2836,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "you're taking this exploitative",
      "offset": 2837.68,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "approach, then you're opening yourself",
      "offset": 2839.2,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "up to exploitation as well. And so you",
      "offset": 2840.96,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "have to choose this balance between",
      "offset": 2842.88,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "playing a defensive game theory optimal",
      "offset": 2844.8,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "policy that guarantees you're not going",
      "offset": 2847.28,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "to lose but might not make you as much",
      "offset": 2848.56,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "money as you potentially could versus",
      "offset": 2850.8,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "playing an exploitative strategy that",
      "offset": 2852.96,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "could be much more profitable but also",
      "offset": 2855.119,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "it creates weaknesses that the opponents",
      "offset": 2857.2,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "could take advantage of and trick you.",
      "offset": 2858.56,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "And there's no way to perfectly balance",
      "offset": 2860.16,
      "duration": 2.4
    },
    {
      "lang": "en",
      "text": "the two. It's kind of like in rock paper",
      "offset": 2861.44,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "scissors if you notice somebody is like",
      "offset": 2862.56,
      "duration": 3.519
    },
    {
      "lang": "en",
      "text": "playing paper for five times in a row",
      "offset": 2864.16,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "you might think like oh they're they",
      "offset": 2866.079,
      "duration": 2.641
    },
    {
      "lang": "en",
      "text": "they have a weakness in their strategy.",
      "offset": 2867.359,
      "duration": 2.881
    },
    {
      "lang": "en",
      "text": "I should just be throwing scissors and",
      "offset": 2868.72,
      "duration": 2.639
    },
    {
      "lang": "en",
      "text": "I'm going to take advantage of them. And",
      "offset": 2870.24,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "so on the sixth time you throw scissors,",
      "offset": 2871.359,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "but actually that's the time when they",
      "offset": 2872.96,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "throw rock, you know, so and you never",
      "offset": 2875.04,
      "duration": 2.559
    },
    {
      "lang": "en",
      "text": "really know. So you always have this",
      "offset": 2876.56,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "trade-off. The poker AIs that have been",
      "offset": 2877.599,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "extremely successful and like my",
      "offset": 2880.8,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "background is like I worked on AI for",
      "offset": 2882.16,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "poker for several years during grad",
      "offset": 2883.68,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "school and made the first superhuman no",
      "offset": 2885.52,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "limit poker AIs, the approach that we",
      "offset": 2887.2,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "took was this game theory optimal",
      "offset": 2889.2,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "approach where the AIs would play this",
      "offset": 2890.64,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "unbeatable strategy and they would play",
      "offset": 2892.16,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "against the world's best and beat them.",
      "offset": 2893.52,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "Now that also means they they beat the",
      "offset": 2895.92,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "world's worst. Like they would just beat",
      "offset": 2897.839,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "anybody. But if they were up against a",
      "offset": 2899.119,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "weak opponent, they might not beat them",
      "offset": 2901.599,
      "duration": 5.361
    },
    {
      "lang": "en",
      "text": "as severely as a human expert might",
      "offset": 2903.359,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "because the human expert would know how",
      "offset": 2906.96,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "to adapt from the game optimal policy to",
      "offset": 2908.24,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "be able to exploit these weak players.",
      "offset": 2910.559,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "And so there's this kind of unanswered",
      "offset": 2912.319,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "question of like how do you make an",
      "offset": 2913.839,
      "duration": 5.921
    },
    {
      "lang": "en",
      "text": "exploitative poker AI? And a lot of",
      "offset": 2916.8,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "people had pursued this research",
      "offset": 2919.76,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "direction. I like dabbled in it a little",
      "offset": 2921.2,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "bit during grad school. And I think",
      "offset": 2922.72,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "fundamentally it just comes down to AI",
      "offset": 2924.559,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "not being as sample efficient as humans.",
      "offset": 2927.119,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "You know, we discussed earlier, if a",
      "offset": 2928.72,
      "duration": 3.119
    },
    {
      "lang": "en",
      "text": "human's playing poker, they're able to",
      "offset": 2930.319,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "get a really good sense of of the",
      "offset": 2931.839,
      "duration": 3.041
    },
    {
      "lang": "en",
      "text": "strengths and weaknesses of a player",
      "offset": 2933.119,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "within a dozen hands. It's like honestly",
      "offset": 2934.88,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "really impressive. And back when we were",
      "offset": 2937.119,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "working on AI for poker in like the, you",
      "offset": 2939.68,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "know, mid2010s, you'd have to these AIs",
      "offset": 2941.28,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "would have to play like 10,000 hands of",
      "offset": 2943.76,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "poker to like get a good profile of like",
      "offset": 2945.52,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "who this player is, like how they're",
      "offset": 2947.2,
      "duration": 2.879
    },
    {
      "lang": "en",
      "text": "playing, where their weaknesses are.",
      "offset": 2948.4,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "Now, I think with more recent",
      "offset": 2950.079,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "technology, that has come down. Um, but",
      "offset": 2951.44,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "still the sample efficiency has been a",
      "offset": 2953.599,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "big challenge. Now, what's interesting",
      "offset": 2955.52,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "is that after working on poker, I worked",
      "offset": 2958,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "on diplomacy. I think we talked about",
      "offset": 2961.04,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "this earlier. And diplomacy is this, you",
      "offset": 2962.88,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "know, it's a seven player negotiation",
      "offset": 2964.72,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "game. And when we started working on it,",
      "offset": 2966.079,
      "duration": 6.401
    },
    {
      "lang": "en",
      "text": "I took a very game theory approach to",
      "offset": 2969.599,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "the problem. I I felt like, okay, we're",
      "offset": 2972.48,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "it's kind of like poker. You have to",
      "offset": 2974.4,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "compute this game theory optimal policy,",
      "offset": 2975.76,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "and you just play this, you're going to",
      "offset": 2977.2,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "not lose an expectation. You're going to",
      "offset": 2978.72,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "win in practice. But that actually",
      "offset": 2979.92,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "doesn't work in diplomacy. And it",
      "offset": 2981.52,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "doesn't work. Again, for question of",
      "offset": 2983.52,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "like how how much of a rabbit hole do we",
      "offset": 2985.44,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "want to go down on this, but like",
      "offset": 2987.119,
      "duration": 2.48
    },
    {
      "lang": "en",
      "text": "basically when you're playing like the",
      "offset": 2988.4,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "zero sum games like like poker, game",
      "offset": 2989.599,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "theory optimal works really well. When",
      "offset": 2992,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "you're playing a game like Diplomacy",
      "offset": 2994.16,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "where there's like you need to",
      "offset": 2995.839,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "collaborate and compete and you need",
      "offset": 2997.359,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "there's there's room for collaboration,",
      "offset": 2999.359,
      "duration": 2.881
    },
    {
      "lang": "en",
      "text": "then game theory optimal actually",
      "offset": 3001.04,
      "duration": 2.48
    },
    {
      "lang": "en",
      "text": "doesn't work that well and you have to",
      "offset": 3002.24,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "understand the players and adapt to them",
      "offset": 3003.52,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "much better. So, this ends up being very",
      "offset": 3005.599,
      "duration": 5.041
    },
    {
      "lang": "en",
      "text": "similar to the problem in poker of like",
      "offset": 3008,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "how do you adapt to your opponents. In",
      "offset": 3010.64,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "poker, it's about adapting to their",
      "offset": 3013.04,
      "duration": 2.64
    },
    {
      "lang": "en",
      "text": "weaknesses and take advantage of that.",
      "offset": 3014.24,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "In diplomacy, it's about adapting to",
      "offset": 3015.68,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "their play styles. It's kind of like if",
      "offset": 3017.76,
      "duration": 2.799
    },
    {
      "lang": "en",
      "text": "you're at a table and everybody's",
      "offset": 3019.44,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "speaking French, you don't want to just",
      "offset": 3020.559,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "keep talking in English. You want to",
      "offset": 3022.64,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "adapt to them and speak in French as",
      "offset": 3023.92,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "well. That's the realization that I have",
      "offset": 3025.44,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "with diplomacy that we need to shift",
      "offset": 3027.04,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "away from this game theory optimal",
      "offset": 3028.72,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "paradigm towards modeling the other",
      "offset": 3031.119,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "players, understanding who they are, and",
      "offset": 3033.599,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "then responding accordingly. And so in",
      "offset": 3035.2,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "many ways the techniques that we",
      "offset": 3038.319,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "developed in diplomacy are exploitative",
      "offset": 3039.44,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "like they're not exploitative. They're",
      "offset": 3042.24,
      "duration": 2.879
    },
    {
      "lang": "en",
      "text": "they're really, you know, just adapting",
      "offset": 3043.68,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "to the to the opponents to the other",
      "offset": 3045.119,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "players at at the table. Um but I think",
      "offset": 3047.119,
      "duration": 5.681
    },
    {
      "lang": "en",
      "text": "the same techniques could be used in AI",
      "offset": 3049.44,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "for poker to make exploitative poker",
      "offset": 3052.8,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "eyes. If I didn't get, you know, AGI",
      "offset": 3054.72,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "pill by the incredible progress that we",
      "offset": 3056.88,
      "duration": 2.64
    },
    {
      "lang": "en",
      "text": "were seeing with language models and",
      "offset": 3058.24,
      "duration": 2.879
    },
    {
      "lang": "en",
      "text": "like shifting my whole research agenda",
      "offset": 3059.52,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "to focusing on like general reasoning,",
      "offset": 3061.119,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "probably what I would have worked on",
      "offset": 3064,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "next was making these like exploitative",
      "offset": 3065.119,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "poker AI. It would be a really fun",
      "offset": 3066.72,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "research direction to go down. I think",
      "offset": 3068.72,
      "duration": 2.399
    },
    {
      "lang": "en",
      "text": "it's still there for anybody that wants",
      "offset": 3070,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "to do it. And I think the key would be",
      "offset": 3071.119,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "taking the techniques that we used in in",
      "offset": 3073.44,
      "duration": 2.879
    },
    {
      "lang": "en",
      "text": "diplomacy and applying them to things",
      "offset": 3074.88,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "like poker. I think to me the core piece",
      "offset": 3076.319,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "is when you play online you have a HUD",
      "offset": 3078.88,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "which tells you you know all these stats",
      "offset": 3080.48,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "about the other player like you know how",
      "offset": 3082.079,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "much they participate pre flop blah blah",
      "offset": 3083.839,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "blah and to me it's like a lot of these",
      "offset": 3085.599,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "models from my understanding are not",
      "offset": 3087.599,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "really leveraging the behavior of the",
      "offset": 3089.599,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "other players at the table they're just",
      "offset": 3091.68,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "kind of looking at the board state and",
      "offset": 3093.44,
      "duration": 3.119
    },
    {
      "lang": "en",
      "text": "kind of working from there that's",
      "offset": 3095.119,
      "duration": 2.881
    },
    {
      "lang": "en",
      "text": "correct the way the the way the poker",
      "offset": 3096.559,
      "duration": 3.121
    },
    {
      "lang": "en",
      "text": "eyes work today they're just kind of",
      "offset": 3098,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "like sticking to their precomputed GTO",
      "offset": 3099.68,
      "duration": 7.28
    },
    {
      "lang": "en",
      "text": "GTO strategy and they're not adapting to",
      "offset": 3103.2,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "the other players um at the table and",
      "offset": 3106.96,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "like you can do various like kind of",
      "offset": 3108.88,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "hacky things to get them to adapt but",
      "offset": 3110.48,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "you know they're not they're not very",
      "offset": 3112.4,
      "duration": 2.32
    },
    {
      "lang": "en",
      "text": "principled they're not they don't work",
      "offset": 3113.76,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "super well y okay any grad students",
      "offset": 3114.72,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "listening uh if you want to work on that",
      "offset": 3117.28,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "I I think that is a very very reasonable",
      "offset": 3119.44,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "research direction that'll at least uh",
      "offset": 3121.52,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "get in front of you and you know get",
      "offset": 3123.92,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "some attention at least the other thing",
      "offset": 3126,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "that this conversation brings up for me",
      "offset": 3127.68,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "is yeah well one of the hypothesis for",
      "offset": 3129.76,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "like what is the next step after testime",
      "offset": 3132.16,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "compute is world models is world",
      "offset": 3133.92,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "modeling",
      "offset": 3135.839,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "importance or worthwhile research",
      "offset": 3137.04,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "direction like Yan Lakun has been",
      "offset": 3139.44,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "talking about this non-stop but like",
      "offset": 3141.44,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "basically no LLM have like they have",
      "offset": 3143.599,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "internal world models but like not",
      "offset": 3145.839,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "explicitly a world model I think it's",
      "offset": 3147.44,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "pretty clear that as these models get",
      "offset": 3150.16,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "bigger they have a world model and that",
      "offset": 3151.44,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "world model becomes better uh with scale",
      "offset": 3153.68,
      "duration": 6.32
    },
    {
      "lang": "en",
      "text": "so they are implicitly developing a",
      "offset": 3156.72,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "world model and I don't think it's",
      "offset": 3160,
      "duration": 5.599
    },
    {
      "lang": "en",
      "text": "something that you need to explicitly",
      "offset": 3161.76,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "model. Um, I could be wrong about that.",
      "offset": 3165.599,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "You know, there's when when dealing with",
      "offset": 3167.76,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "people or multi- aents, it might be",
      "offset": 3169.119,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "because you have entities that are not",
      "offset": 3171.44,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "the world and you're resolving",
      "offset": 3173.28,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "hypotheses of what which of the many",
      "offset": 3175.76,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "types of entities you could be dealing",
      "offset": 3178.48,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "with. You know, there was this like long",
      "offset": 3180.16,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "debate in the multi-agent AI community",
      "offset": 3181.68,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "for a long time about and it's still",
      "offset": 3184,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "going on about whether you need to",
      "offset": 3185.92,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "explicitly model other agents like other",
      "offset": 3187.2,
      "duration": 6.8
    },
    {
      "lang": "en",
      "text": "like other people or if they can be",
      "offset": 3191.2,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "implicitly modeled this part of the",
      "offset": 3194,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "environment. For a long time, I was like",
      "offset": 3195.44,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "on the on the took the perspective of",
      "offset": 3197.52,
      "duration": 2.4
    },
    {
      "lang": "en",
      "text": "like of course you have to like",
      "offset": 3198.96,
      "duration": 2.639
    },
    {
      "lang": "en",
      "text": "explicitly model these other agents",
      "offset": 3199.92,
      "duration": 2.639
    },
    {
      "lang": "en",
      "text": "because they're they're behaving",
      "offset": 3201.599,
      "duration": 2
    },
    {
      "lang": "en",
      "text": "differently from the environment. Like",
      "offset": 3202.559,
      "duration": 2.081
    },
    {
      "lang": "en",
      "text": "they they take actions, they're",
      "offset": 3203.599,
      "duration": 2.561
    },
    {
      "lang": "en",
      "text": "unpredictable, you know, they they have",
      "offset": 3204.64,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "agency. But I think I've actually",
      "offset": 3206.16,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "shifted over time to thinking that like",
      "offset": 3208,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "actually if these models become smart",
      "offset": 3209.76,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "enough, they develop things like theory",
      "offset": 3211.68,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "of mind. They develop an understanding",
      "offset": 3212.8,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "that there are other agents that like",
      "offset": 3214.96,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "can take actions and and have motives",
      "offset": 3216.96,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "and all this stuff. And these models",
      "offset": 3218.96,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "just develop that implicitly with scale",
      "offset": 3220.96,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "and and more capable behavior broadly.",
      "offset": 3223.92,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "So that's the perspective I take these",
      "offset": 3226.16,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "days. So like what I just said was an",
      "offset": 3227.76,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "example of a heristic that is not bitter",
      "offset": 3229.28,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "lesson filled and you just it just goes",
      "offset": 3230.88,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "away. Yeah. It's really all come back to",
      "offset": 3232.8,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "the bitter lesson. got to cite them",
      "offset": 3234.96,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "every every podcast. So, one of the",
      "offset": 3237.2,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "interesting findings and most consistent",
      "offset": 3239.44,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "findings, you know, I think you were at",
      "offset": 3241.2,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "ICLR and uh one of the hit talks there",
      "offset": 3242.4,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "was about open-endedness and this guy",
      "offset": 3244.72,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "Tim who gave that talk has been doing a",
      "offset": 3246.72,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "lot bunch of research about multi- aent",
      "offset": 3249.359,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "systems too. One of the most consistent",
      "offset": 3251.44,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "findings is always that um it's better",
      "offset": 3253.2,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "for AIS to selfplay and improve",
      "offset": 3255.28,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "competitively as opposed to sort of",
      "offset": 3258.079,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "humans training and guiding them. And",
      "offset": 3259.839,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "you find that with like you know alpha",
      "offset": 3262.24,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "zero and R10 whatever that was. Do you",
      "offset": 3264.4,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "think this will hold for multi- aents",
      "offset": 3268.48,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "like selfplay to improve better than",
      "offset": 3270.48,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "humans? Yeah. So okay so this is a great",
      "offset": 3273.04,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "question and I I think this is like",
      "offset": 3274.88,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "worth expanding on. So I think a lot of",
      "offset": 3277.119,
      "duration": 5.361
    },
    {
      "lang": "en",
      "text": "people today see selfplay as like the",
      "offset": 3279.68,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "next step and maybe the last step that",
      "offset": 3282.48,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "we need for super intelligence. And I",
      "offset": 3284.96,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "think if you're following, you know, you",
      "offset": 3286.64,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "look at something like Alpha Alpha Go",
      "offset": 3288.559,
      "duration": 3.121
    },
    {
      "lang": "en",
      "text": "and Alpha Zero, we seem to be following",
      "offset": 3289.839,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "a very similar trend, right? Like the",
      "offset": 3291.68,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "first step in Alpha Go was you do large",
      "offset": 3293.599,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "scale pre-training. In that case, it was",
      "offset": 3295.119,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "on human Go games. With LMS, it's",
      "offset": 3296.96,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "pre-training on, you know, tons of like",
      "offset": 3299.44,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "internet data and that gets you a strong",
      "offset": 3301.76,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "model, but it doesn't get you uh you",
      "offset": 3304,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "know an extremely strong model, you",
      "offset": 3306.16,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "know, it doesn't get you superhuman",
      "offset": 3307.92,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "model. And then the next step in the",
      "offset": 3309.2,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "alpha go paradigm is you do large scale",
      "offset": 3310.72,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "test time compute or like large scale",
      "offset": 3313.28,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "inference compute and in that case with",
      "offset": 3314.96,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "um MCTS and now we have like reasoning",
      "offset": 3317.04,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "models that also do like this large",
      "offset": 3319.68,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "scale inference compute and again that",
      "offset": 3320.96,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "like boosts the capabilities a ton.",
      "offset": 3322.8,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "Finally with Alph Go and Alpha Zero you",
      "offset": 3324.88,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "have selfplay where the model plays",
      "offset": 3327.28,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "against itself learns from those games",
      "offset": 3329.119,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "gets better and better and better and",
      "offset": 3331.599,
      "duration": 2.801
    },
    {
      "lang": "en",
      "text": "just like goes from something that's",
      "offset": 3332.88,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "like around human level performance to",
      "offset": 3334.4,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "like way beyond human capability. It's",
      "offset": 3336.24,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "like these go policies now are so strong",
      "offset": 3338.64,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "that it's just like incomprehensible.",
      "offset": 3340.88,
      "duration": 2.32
    },
    {
      "lang": "en",
      "text": "Like what they're doing is",
      "offset": 3342.48,
      "duration": 2.48
    },
    {
      "lang": "en",
      "text": "incomprehensible to humans. Same thing",
      "offset": 3343.2,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "with chess. And we don't have that right",
      "offset": 3344.96,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "now with language models. And so it's",
      "offset": 3346.88,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "like it's really tempting to look at",
      "offset": 3349.04,
      "duration": 2.16
    },
    {
      "lang": "en",
      "text": "that and say like oh well we just need",
      "offset": 3350,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "these like AI models to now interact",
      "offset": 3351.2,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "with each other and learn from each",
      "offset": 3352.88,
      "duration": 2.479
    },
    {
      "lang": "en",
      "text": "other and they're just going to like get",
      "offset": 3354.4,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "to super intelligence. The challenge and",
      "offset": 3355.359,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "I kind of mentioned this like a little",
      "offset": 3357.92,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "bit when I was talking about diplomacy.",
      "offset": 3359.28,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "The challenge is that go is this",
      "offset": 3361.599,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "two-player zero sum game. And two-player",
      "offset": 3363.76,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "zerosome games have this very nice",
      "offset": 3366,
      "duration": 6.4
    },
    {
      "lang": "en",
      "text": "property where when you do selfplay, you",
      "offset": 3368.559,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "are converging to a minimax equilibrium.",
      "offset": 3372.4,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "And I I guess I should take a step back",
      "offset": 3374.799,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "and say like in two-player zero some",
      "offset": 3376.4,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "games, two player zero games are are",
      "offset": 3379.2,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "chess, go even two-player poker, all two",
      "offset": 3380.72,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "player zero sum. What you typically want",
      "offset": 3382.88,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "is what's called a minax equilibrium.",
      "offset": 3385.92,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "This is that that GTO policy. this",
      "offset": 3387.92,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "policy that you play where you're",
      "offset": 3390.16,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "guaranteeing that you're not going to",
      "offset": 3391.599,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "lose to any opponent in expectation. I",
      "offset": 3393.359,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "think in chess and go that's like pretty",
      "offset": 3395.76,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "clearly what you want. Interestingly, in",
      "offset": 3397.599,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "when you look at poker, it's not as",
      "offset": 3399.28,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "obvious. In a two-player zero some",
      "offset": 3401.119,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "version of poker, you could play the GTO",
      "offset": 3402.72,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "miniax policy and that guarantees that",
      "offset": 3404.319,
      "duration": 5.121
    },
    {
      "lang": "en",
      "text": "you won't lose to any opponent on Earth.",
      "offset": 3406.72,
      "duration": 5.119
    },
    {
      "lang": "en",
      "text": "But again, I mentioned there's you're",
      "offset": 3409.44,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "not going to to beat a weak player.",
      "offset": 3411.839,
      "duration": 2.641
    },
    {
      "lang": "en",
      "text": "You're not going to make as much money",
      "offset": 3413.52,
      "duration": 3.039
    },
    {
      "lang": "en",
      "text": "off of them as you could if you instead",
      "offset": 3414.48,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "played an exploitative policy. So,",
      "offset": 3416.559,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "there's this question of like, what do",
      "offset": 3418.88,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "you want? Do you want to make as much",
      "offset": 3420.24,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "money as possible, or do you want to",
      "offset": 3421.68,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "guarantee that you're not going to lose",
      "offset": 3423.839,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "to any human alive? What all the bots",
      "offset": 3425.2,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "have decided is like, well, what all the",
      "offset": 3427.44,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "like AI developers in these games have",
      "offset": 3429.28,
      "duration": 2.319
    },
    {
      "lang": "en",
      "text": "decided is like, well, we're going to",
      "offset": 3430.72,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "choose the miniax policy. And",
      "offset": 3431.599,
      "duration": 3.281
    },
    {
      "lang": "en",
      "text": "conveniently, that's exactly what",
      "offset": 3433.68,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "selfplay converges to. If you have these",
      "offset": 3434.88,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "AIs play against each other, learn from",
      "offset": 3436.4,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "their mistakes, they converge over time",
      "offset": 3438.16,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "to this miniax policy, guaranteed. But",
      "offset": 3440.079,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "once you go outside a two-player zero",
      "offset": 3442.24,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "some games, like in the case of",
      "offset": 3443.839,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "diplomacy, that's actually not a useful",
      "offset": 3445.52,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "policy anymore. You don't want to just",
      "offset": 3448.24,
      "duration": 5.599
    },
    {
      "lang": "en",
      "text": "like have this very defensive policy and",
      "offset": 3450.319,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "you're going to end up with really weird",
      "offset": 3453.839,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "behavior if you start doing the same",
      "offset": 3455.28,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "kind of self-play in things like math.",
      "offset": 3458.079,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "So for example, what does it mean to do",
      "offset": 3460.48,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "self-play in math? you could fall into",
      "offset": 3463.04,
      "duration": 5.279
    },
    {
      "lang": "en",
      "text": "this trap of like, well, I just want one",
      "offset": 3465.28,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "model to pose really difficult questions",
      "offset": 3468.319,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "and the other model to solve those",
      "offset": 3470.16,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "questions. You know, that's like a",
      "offset": 3471.76,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "two-player zero sum game. The problem is",
      "offset": 3473.2,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "that like, well, you could just like",
      "offset": 3475.119,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "pose really difficult questions that are",
      "offset": 3476.24,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "not interesting. You know, you just like",
      "offset": 3478.72,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "get ask it to do like 30digit",
      "offset": 3480.24,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "multiplication. It's a very difficult",
      "offset": 3482.88,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "problem for the AI models. Is that",
      "offset": 3484.88,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "really making progress in the dimension",
      "offset": 3487.28,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "that we want? Like not really. So",
      "offset": 3488.88,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "selfplay outside of these two players",
      "offset": 3491.44,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "here some games becomes like a much more",
      "offset": 3493.04,
      "duration": 5.279
    },
    {
      "lang": "en",
      "text": "difficult nuanced question. So I think",
      "offset": 3495.28,
      "duration": 5.279
    },
    {
      "lang": "en",
      "text": "and then Tim Tim kind of like basically",
      "offset": 3498.319,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "said something similar in his talk that",
      "offset": 3500.559,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "there's a lot of challenges in really",
      "offset": 3502.559,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "deciding what you're optimizing for when",
      "offset": 3504.079,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "you start to talk about selfplay outside",
      "offset": 3506.16,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "of two players here some games. My point",
      "offset": 3507.599,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "is that like this is where the AlphaGo",
      "offset": 3509.52,
      "duration": 5.599
    },
    {
      "lang": "en",
      "text": "go analogy breaks down and not",
      "offset": 3512.4,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "necessarily breaks down but like it's",
      "offset": 3515.119,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "not going to be as easy as selfplay was",
      "offset": 3516.48,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "in AlphaGo. What is the objective",
      "offset": 3518.319,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "function then for that? What is the new",
      "offset": 3520.4,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "objective function? Yeah, it's a it's a",
      "offset": 3523.359,
      "duration": 2.561
    },
    {
      "lang": "en",
      "text": "good it's a good question. Yeah. And I",
      "offset": 3524.64,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "think that that's something that um you",
      "offset": 3525.92,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "know a lot of people are thinking about.",
      "offset": 3527.92,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "Yeah. Um I'm sure you are. One of the",
      "offset": 3529.28,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "last podcasts that you did, you",
      "offset": 3532.72,
      "duration": 2.16
    },
    {
      "lang": "en",
      "text": "mentioned that you were very impressed",
      "offset": 3533.92,
      "duration": 2.32
    },
    {
      "lang": "en",
      "text": "by Sora. You don't you don't work",
      "offset": 3534.88,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "directly on Sora, but obviously it's",
      "offset": 3536.24,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "part of OpenAI. I think the the most",
      "offset": 3537.92,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "recent uh new updates or in that sort of",
      "offset": 3539.839,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "generative media space is auto",
      "offset": 3542.88,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "reggressive image gen. Is that",
      "offset": 3544.16,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "interesting or surprising in any way",
      "offset": 3546.64,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "that you want to comment about? I don't",
      "offset": 3548.319,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "work on image gen, so I my ability to",
      "offset": 3550.799,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "comment on this is kind of limited, but",
      "offset": 3552.72,
      "duration": 2.879
    },
    {
      "lang": "en",
      "text": "I will say like I I love it. Like I",
      "offset": 3554,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "think it's super impressive. It's like",
      "offset": 3555.599,
      "duration": 3.281
    },
    {
      "lang": "en",
      "text": "one of those things where, you know, you",
      "offset": 3557.28,
      "duration": 3.039
    },
    {
      "lang": "en",
      "text": "work on these reasoning models and you",
      "offset": 3558.88,
      "duration": 2.959
    },
    {
      "lang": "en",
      "text": "think like, wow, we're going to like be",
      "offset": 3560.319,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "able to do all sorts of crazy stuff like",
      "offset": 3561.839,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "advanced science and um, you know, solve",
      "offset": 3564,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "agentic tasks and and software",
      "offset": 3566.799,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "engineering. And then there's like this",
      "offset": 3568.559,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "whole other like dimension of progress",
      "offset": 3570.319,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "where you're like, oh, you're able to",
      "offset": 3572.559,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "like make images and videos now and it's",
      "offset": 3573.599,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "like so much fun. And that's getting a",
      "offset": 3575.599,
      "duration": 2.881
    },
    {
      "lang": "en",
      "text": "lot more the attention to be honest,",
      "offset": 3577.2,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "especially in the general public. And",
      "offset": 3578.48,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "it's probably driving a lot more of the",
      "offset": 3579.92,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "like, you know, subscription plans for",
      "offset": 3581.2,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "CHBT, which is is great, but I think",
      "offset": 3582.88,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "it's just kind of funny that like yeah,",
      "offset": 3585.2,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "we're also I promise we're also working",
      "offset": 3586.64,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "on super intelligence,",
      "offset": 3588.4,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "but you can make everything gibbly. Uh I",
      "offset": 3590.799,
      "duration": 5.441
    },
    {
      "lang": "en",
      "text": "think the the delta for me was um I was",
      "offset": 3593.92,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "actually harboring this thesis that",
      "offset": 3596.24,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "diffusion was over because of auto",
      "offset": 3598,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "reggressive imaging. Like there were",
      "offset": 3600.559,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "rumors about this end of last year and",
      "offset": 3601.68,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "obviously now it's come now it's come",
      "offset": 3603.599,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "out. Then Gemini comes out with text",
      "offset": 3604.96,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "diffusion and like diffusion is so bad",
      "offset": 3607.2,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "and like this is two directions and it's",
      "offset": 3609.04,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "very relevant for inference of auto",
      "offset": 3611.92,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "reggressive versus um diffusion. Do we",
      "offset": 3613.839,
      "duration": 6.081
    },
    {
      "lang": "en",
      "text": "have both? Does one win? The beauty of",
      "offset": 3617.44,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "research is like you know you got to",
      "offset": 3619.92,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "pursue different different directions",
      "offset": 3621.76,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "and it's not it's not always going to be",
      "offset": 3623.359,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "clear like what is um you know the",
      "offset": 3625.52,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "promising path like and I think it's",
      "offset": 3627.92,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "great that people are looking into",
      "offset": 3629.68,
      "duration": 2
    },
    {
      "lang": "en",
      "text": "different directions and trying",
      "offset": 3630.72,
      "duration": 2.639
    },
    {
      "lang": "en",
      "text": "different things. I I think that there's",
      "offset": 3631.68,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "a lot of value in that exploration and I",
      "offset": 3633.359,
      "duration": 5.121
    },
    {
      "lang": "en",
      "text": "think we all benefit from seeing what",
      "offset": 3636.319,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "works. Any potential in diffusion",
      "offset": 3638.48,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "reasoning let's say your channel I can",
      "offset": 3640.48,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "answer that. Okay. So you did a masters",
      "offset": 3642.96,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "in robotics too. Would love to get your",
      "offset": 3645.04,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "thoughts on one you know open kind of",
      "offset": 3647.359,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "started with the pen spinning trick and",
      "offset": 3649.44,
      "duration": 3.119
    },
    {
      "lang": "en",
      "text": "like the robotic arm they wanted to",
      "offset": 3651.2,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "build. Is it right to work on the",
      "offset": 3652.559,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "humanoid likes? Do you think that's kind",
      "offset": 3654.559,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "of like the wrong embodiment of AI",
      "offset": 3656.16,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "outside of the usual, you know, how long",
      "offset": 3659.04,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "until we get robots, blah blah blah. Is",
      "offset": 3660.88,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "there something that you think is like",
      "offset": 3663.119,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "fundamentally not being explored right",
      "offset": 3664.64,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "now that people should really be doing",
      "offset": 3666.48,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "in robotics? I did a masters in robotics",
      "offset": 3668,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "years ago and my takeaway from that",
      "offset": 3671.04,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "experience, first of all, I didn't",
      "offset": 3674.16,
      "duration": 2.399
    },
    {
      "lang": "en",
      "text": "actually work with robots that much. I",
      "offset": 3675.2,
      "duration": 2.639
    },
    {
      "lang": "en",
      "text": "was like technically in a robotics",
      "offset": 3676.559,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "program. I played around with some Lego",
      "offset": 3677.839,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "robots my my first week of the program,",
      "offset": 3680.4,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "but then honestly I just like pretty",
      "offset": 3682.4,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "quickly shifted just working on AI for",
      "offset": 3683.92,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "poker and um was kind of nominally in",
      "offset": 3685.44,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "the robotics masters. But my takeaway",
      "offset": 3688.4,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "from like interacting with all these",
      "offset": 3691.2,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "roboticists and seeing their research",
      "offset": 3692.64,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "was that I did not want to work on",
      "offset": 3695.599,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "robots because the research cycle is so",
      "offset": 3697.359,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "much slower and so much more painful",
      "offset": 3700.319,
      "duration": 2.641
    },
    {
      "lang": "en",
      "text": "when you're dealing with like physical",
      "offset": 3701.839,
      "duration": 2.561
    },
    {
      "lang": "en",
      "text": "hardware. like software goes so much",
      "offset": 3702.96,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "more quickly and I think that's why",
      "offset": 3704.4,
      "duration": 2.959
    },
    {
      "lang": "en",
      "text": "we're seeing so much progress with",
      "offset": 3705.92,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "language models and like all these like",
      "offset": 3707.359,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "virtual co-orker kind of tasks but",
      "offset": 3709.359,
      "duration": 2.881
    },
    {
      "lang": "en",
      "text": "haven't seen as much progress in",
      "offset": 3711.04,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "robotics that like physical hardware",
      "offset": 3712.24,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "just is much more painful to iterate on",
      "offset": 3714.48,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "the question of humanoids I don't have",
      "offset": 3717.44,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "very strong opinions here because this",
      "offset": 3720,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "isn't what I'm working on but I think",
      "offset": 3721.92,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "there's a lot of value in nonhumanoid",
      "offset": 3725.04,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "robotics as well I think drones are a",
      "offset": 3727.92,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "perfect example where like there's",
      "offset": 3730.16,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "clearly a lot of value in That is that a",
      "offset": 3731.68,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "humanoid? No. But in many ways that's",
      "offset": 3733.52,
      "duration": 3.039
    },
    {
      "lang": "en",
      "text": "great, you know, like you don't want a",
      "offset": 3735.04,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "humanoid for for that kind of",
      "offset": 3736.559,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "technology. I think weekly um I think",
      "offset": 3737.76,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "that nonhumanoids provide a lot of",
      "offset": 3740.72,
      "duration": 5.119
    },
    {
      "lang": "en",
      "text": "value. I was reading u Richard Hammings",
      "offset": 3743.28,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "the art of doing science and",
      "offset": 3745.839,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "engineering. And he talks about how when",
      "offset": 3747.119,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "you have a new technological shift,",
      "offset": 3749.119,
      "duration": 3.521
    },
    {
      "lang": "en",
      "text": "people try and take the old workloads",
      "offset": 3750.799,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "and like replicate them just in the new",
      "offset": 3752.64,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "technology versus you actually have to",
      "offset": 3754.64,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "change the way you do it. And you know",
      "offset": 3756.16,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "when I see this video of like you know",
      "offset": 3757.76,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "your humanoid in the house it's like",
      "offset": 3759.359,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "well the human shape is kind of has a",
      "offset": 3761.04,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "lot of limitations that could actually",
      "offset": 3763.44,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "be improved that I think people what's",
      "offset": 3765.28,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "familiar you know it's like would you",
      "offset": 3767.599,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "put a robot with like 10 arms and like",
      "offset": 3769.2,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "you know five legs in your house or",
      "offset": 3772.16,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "would that be yuri at night when you get",
      "offset": 3773.92,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "up and you see that thing walking around",
      "offset": 3775.599,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "and is that why we use humanoids. So I I",
      "offset": 3777.359,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "think to me there's almost like this",
      "offset": 3779.359,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "local maximum of like you know we got to",
      "offset": 3780.559,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "make it look like a human but I think",
      "offset": 3782.319,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "like what's like the the best shape uh",
      "offset": 3784.64,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "in house would be I'm terrible at",
      "offset": 3787.599,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "product design so I I am not the person",
      "offset": 3789.28,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "to ask on this. I think there is a",
      "offset": 3791.599,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "question of like is it better to make",
      "offset": 3793.92,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "humanoids because they're more familiar",
      "offset": 3796,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "to us or is it worse to make humanoids",
      "offset": 3797.599,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "because they're more similar to us but",
      "offset": 3799.599,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "not quite identical like I I don't know",
      "offset": 3801.44,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "which one I would actually find",
      "offset": 3803.76,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "creepier. Yeah. Yeah. The thing that got",
      "offset": 3804.72,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "me humanoid pilled a little bit was just",
      "offset": 3807.52,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "the argument that most of the world is",
      "offset": 3809.92,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "made for humans anyway. So if you want",
      "offset": 3811.52,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "to replace human labor, you have to make",
      "offset": 3814,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "a humanoid. I don't know if that's",
      "offset": 3815.599,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "convincing. Again, I don't have very",
      "offset": 3818.16,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "strong opinions in this field because",
      "offset": 3819.839,
      "duration": 2.801
    },
    {
      "lang": "en",
      "text": "like I don't work in it. Um I was like",
      "offset": 3820.96,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "weekly in favor of humanoids. And I",
      "offset": 3822.64,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "think what really persuaded me to be",
      "offset": 3824.16,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "weekly in favor of like non-humanoids",
      "offset": 3825.76,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "was listening to um the physical",
      "offset": 3828.24,
      "duration": 5.359
    },
    {
      "lang": "en",
      "text": "intelligence CEO and like some of his",
      "offset": 3831.44,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "pitches about like why they're not",
      "offset": 3833.599,
      "duration": 2.161
    },
    {
      "lang": "en",
      "text": "pursuing why they're pursuing like",
      "offset": 3834.72,
      "duration": 2.639
    },
    {
      "lang": "en",
      "text": "non-humanoid robotics. Okay. And",
      "offset": 3835.76,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "conveniently their office is actually",
      "offset": 3837.359,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "like very close to here. So if you",
      "offset": 3838.72,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "wanted to they're speaking at the the",
      "offset": 3840.079,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "conference I'm running. Okay. You know",
      "offset": 3841.68,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "I'd say like listen to his pitch and",
      "offset": 3844,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "maybe he can convince you that is the",
      "offset": 3845.28,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "way to go. Awesome. The other one I",
      "offset": 3847.359,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "would refer people to is Jim Fan",
      "offset": 3849.76,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "recently did a talk on the physical",
      "offset": 3851.44,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "tearing test which uh which he did at",
      "offset": 3852.64,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "the Sequoia conference which um was very",
      "offset": 3854.4,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "very good. Um he's such a great educator",
      "offset": 3856.559,
      "duration": 5.361
    },
    {
      "lang": "en",
      "text": "and explainer of things. Um it's very",
      "offset": 3859.76,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "hard especially in that field. Um cool",
      "offset": 3861.92,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "we're done asking you about things that",
      "offset": 3864.48,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "you don't work on.",
      "offset": 3865.76,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "So these are just more rapid fires to to",
      "offset": 3867.839,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "sort of explore some of your boundaries",
      "offset": 3869.839,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "and get get some quick hits. How do you",
      "offset": 3871.52,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "or top industry labs keep on top of",
      "offset": 3873.839,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "research? like what are your tools and",
      "offset": 3876.079,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "practices? Uh it's it's really hard. I",
      "offset": 3878.48,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "think that a lot of people have this",
      "offset": 3881.039,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "perception that like academic research",
      "offset": 3882.319,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "is irrelevant and that's actually not",
      "offset": 3884.48,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "the case. I think that we do we look at",
      "offset": 3885.599,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "academic research I I think one of the",
      "offset": 3887.76,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "um challenges is like a lot of academic",
      "offset": 3889.599,
      "duration": 6.161
    },
    {
      "lang": "en",
      "text": "research shows promise in their papers",
      "offset": 3892.96,
      "duration": 5.599
    },
    {
      "lang": "en",
      "text": "but then actually doesn't work at scale",
      "offset": 3895.76,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "or even doesn't replicate. I think if we",
      "offset": 3898.559,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "find interesting papers like we're going",
      "offset": 3900.88,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "to try to reproduce that in-house and",
      "offset": 3902.4,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "see if it like still holds up and then",
      "offset": 3903.76,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "also does it scale well. But that is",
      "offset": 3905.839,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "like a big source of inspiration for us.",
      "offset": 3907.599,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "Whatever hits archive literally you do",
      "offset": 3910.24,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "the same as the rest of us or do you",
      "offset": 3911.76,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "have like a special process? Especially",
      "offset": 3913.599,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "if I get recommendations like we have an",
      "offset": 3915.76,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "internal channel where people will post",
      "offset": 3917.119,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "interesting papers and like I think",
      "offset": 3919.52,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "that's a good source of like okay well",
      "offset": 3920.88,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "this person that is more familiar with",
      "offset": 3922.24,
      "duration": 3.119
    },
    {
      "lang": "en",
      "text": "this area thinks that this paper is",
      "offset": 3924.24,
      "duration": 2.24
    },
    {
      "lang": "en",
      "text": "interesting so therefore I should read",
      "offset": 3925.359,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "it. Yeah. Um and similarly like I'll",
      "offset": 3926.48,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "keep track of things that are happening",
      "offset": 3929.2,
      "duration": 2.879
    },
    {
      "lang": "en",
      "text": "in my space that I think are interesting",
      "offset": 3930.4,
      "duration": 2.959
    },
    {
      "lang": "en",
      "text": "and like if I think it's really",
      "offset": 3932.079,
      "duration": 2.561
    },
    {
      "lang": "en",
      "text": "interesting maybe I'll share it. For me",
      "offset": 3933.359,
      "duration": 2.801
    },
    {
      "lang": "en",
      "text": "it's like WhatsApp and signal group",
      "offset": 3934.64,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "chats with researchers and that's it.",
      "offset": 3936.16,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "Yeah. I think it is like I mean a lot of",
      "offset": 3937.839,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "people look at things like Twitter and I",
      "offset": 3940.799,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "think it's really unfortunate that we've",
      "offset": 3942.64,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "reached this point where things need to",
      "offset": 3944.319,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "get a lot of attention on social media",
      "offset": 3946.72,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "for it to be paid attention to. Um",
      "offset": 3948.319,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "that's what the grad students are",
      "offset": 3950.4,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "trained. They're taking classes to do",
      "offset": 3951.76,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "this. I I do recommend to like you know",
      "offset": 3953.599,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "I've worked with grad students work with",
      "offset": 3955.44,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "fewer now because we don't publish as",
      "offset": 3957.28,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "much but when I was at fair publishing",
      "offset": 3958.88,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "papers like I would tell the grad",
      "offset": 3961.28,
      "duration": 2.799
    },
    {
      "lang": "en",
      "text": "students I was working with that like",
      "offset": 3962.72,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "you need to post it on Twitter and you",
      "offset": 3964.079,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "need to and we go over like the Twitter",
      "offset": 3966.72,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "thread about like how to present the",
      "offset": 3968.72,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "work and everything and um there's a",
      "offset": 3969.92,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "real art to it and it does matter and",
      "offset": 3972.079,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "it's kind of the sad truth. I know when",
      "offset": 3973.839,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "you were doing the ACPC like the AI",
      "offset": 3975.839,
      "duration": 5.041
    },
    {
      "lang": "en",
      "text": "poker competition, you mentioned that",
      "offset": 3978.24,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "people were not doing search because",
      "offset": 3980.88,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "they were limited to like two CPUs at",
      "offset": 3982.64,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "inference. Do you see similar things",
      "offset": 3984.559,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "today that are like keeping interesting",
      "offset": 3986.88,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "research from being done? That might be",
      "offset": 3988.88,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "it's not as popular. It doesn't get you",
      "offset": 3990.64,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "into the top conferences like uh are",
      "offset": 3992.4,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "there some environmental limiters?",
      "offset": 3994.64,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "Absolutely. And I I think one example is",
      "offset": 3997.28,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "for benchmarks that you look at things",
      "offset": 3999.28,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "like humanity's last exam like you have",
      "offset": 4002.4,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "these incredibly difficult problems but",
      "offset": 4004,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "then are still very easily gradable and",
      "offset": 4006,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "I think that actually limits the scope",
      "offset": 4008.24,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "of what you can evaluate these models",
      "offset": 4009.599,
      "duration": 2.561
    },
    {
      "lang": "en",
      "text": "on. If you if you stick to that",
      "offset": 4010.96,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "paradigm, it's very convenient because",
      "offset": 4012.16,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "you know it's very easy to like then",
      "offset": 4014.16,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "score the models. But actually a lot of",
      "offset": 4015.52,
      "duration": 3.039
    },
    {
      "lang": "en",
      "text": "the things that we want to you know",
      "offset": 4017.2,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "evaluate these models on are kind of",
      "offset": 4018.559,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "like more fuzzy tasks that are not",
      "offset": 4020,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "multiple choice questions and making",
      "offset": 4021.92,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "benchmarks for that for those kinds of",
      "offset": 4024.72,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "things is so much harder and probably",
      "offset": 4026.16,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "also like a lot more expensive to",
      "offset": 4028.559,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "evaluate. But I think that those are",
      "offset": 4030.319,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "really valuable things to work on and",
      "offset": 4032.319,
      "duration": 5.681
    },
    {
      "lang": "en",
      "text": "that would fit the semment GBD 4.5 is",
      "offset": 4034.4,
      "duration": 5.679
    },
    {
      "lang": "en",
      "text": "like a high taste model in a way.",
      "offset": 4038,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "there's kind of like all these like",
      "offset": 4040.079,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "nonmeasurable",
      "offset": 4041.68,
      "duration": 2.879
    },
    {
      "lang": "en",
      "text": "things about a model that are really",
      "offset": 4043.28,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "good that maybe people are not well I",
      "offset": 4044.559,
      "duration": 2.56
    },
    {
      "lang": "en",
      "text": "think there are things that are",
      "offset": 4046.4,
      "duration": 2.08
    },
    {
      "lang": "en",
      "text": "measurable but they're just like much",
      "offset": 4047.119,
      "duration": 3.121
    },
    {
      "lang": "en",
      "text": "more difficult to measure and I think",
      "offset": 4048.48,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "that a lot of benchmarks have kind of",
      "offset": 4050.24,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "stuck to this paradigm of posing really",
      "offset": 4051.68,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "difficult problems that are really easy",
      "offset": 4054.4,
      "duration": 4.959
    },
    {
      "lang": "en",
      "text": "to measure so let's say the pre-training",
      "offset": 4056.4,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "scaling paradigm took about 5 years from",
      "offset": 4059.359,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "like discovery of GPT to scaling it up",
      "offset": 4061.76,
      "duration": 5.599
    },
    {
      "lang": "en",
      "text": "to GPT4 and then we give you we give",
      "offset": 4064.16,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "test time compute 5 years as well So,",
      "offset": 4067.359,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "um, if test time comput a wall by 2030,",
      "offset": 4070.4,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "what would be the probable cause? It's",
      "offset": 4072.799,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "very similar to pre-training. We're",
      "offset": 4075.039,
      "duration": 2.32
    },
    {
      "lang": "en",
      "text": "like, you can push pre-training a lot",
      "offset": 4076.079,
      "duration": 2.321
    },
    {
      "lang": "en",
      "text": "further and it just becomes more",
      "offset": 4077.359,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "expensive with each iteration. I think",
      "offset": 4078.4,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "we're going to see something similar",
      "offset": 4080.559,
      "duration": 2.401
    },
    {
      "lang": "en",
      "text": "with test time compute. We're like,",
      "offset": 4081.44,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "okay, we're going to get them thinking",
      "offset": 4082.96,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "instead of 3 minutes, they're for 3",
      "offset": 4085.119,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "hours and then 3 days and then 3 weeks.",
      "offset": 4087.2,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "Um, you run out of human life. Well, so",
      "offset": 4089.44,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "there's two there's two there's two",
      "offset": 4092,
      "duration": 3.039
    },
    {
      "lang": "en",
      "text": "concerns. One is that it becomes much",
      "offset": 4093.28,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "more expensive to get the models to like",
      "offset": 4095.039,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "think for that long or like scale up",
      "offset": 4097.359,
      "duration": 2.4
    },
    {
      "lang": "en",
      "text": "test time compute. Like as you scale up",
      "offset": 4098.64,
      "duration": 2.48
    },
    {
      "lang": "en",
      "text": "test time compute, you're spending more",
      "offset": 4099.759,
      "duration": 2.56
    },
    {
      "lang": "en",
      "text": "on test time compute which means that",
      "offset": 4101.12,
      "duration": 2.32
    },
    {
      "lang": "en",
      "text": "like there's a limit to how much you",
      "offset": 4102.319,
      "duration": 3.121
    },
    {
      "lang": "en",
      "text": "could spend. That's one potential",
      "offset": 4103.44,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "ceiling. Now obviously, well, not",
      "offset": 4105.44,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "obviously, but like I should say that",
      "offset": 4107.04,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "we're also becoming more efficient.",
      "offset": 4109.199,
      "duration": 2.401
    },
    {
      "lang": "en",
      "text": "These models are becoming more efficient",
      "offset": 4110.4,
      "duration": 2.319
    },
    {
      "lang": "en",
      "text": "in the way they're thinking is they're",
      "offset": 4111.6,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "able to do more with the same amount of",
      "offset": 4112.719,
      "duration": 2.801
    },
    {
      "lang": "en",
      "text": "test time comput. And I think that's a",
      "offset": 4114.4,
      "duration": 2.64
    },
    {
      "lang": "en",
      "text": "very underappreciated point that it's",
      "offset": 4115.52,
      "duration": 2.48
    },
    {
      "lang": "en",
      "text": "not just that we're getting these models",
      "offset": 4117.04,
      "duration": 2.159
    },
    {
      "lang": "en",
      "text": "to think for longer. In fact, if you",
      "offset": 4118,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "look at 03, it's thinking for longer",
      "offset": 4119.199,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "than 01 preview for some questions, but",
      "offset": 4121.12,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "it's not like a radical difference, but",
      "offset": 4122.719,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "it's way better. Why? Because it's just",
      "offset": 4124.319,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "like becoming better at thinking.",
      "offset": 4126.319,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "Anyway, yeah, these models um you're",
      "offset": 4128.56,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "going to scale up test on comput, you",
      "offset": 4130.719,
      "duration": 2.64
    },
    {
      "lang": "en",
      "text": "can only scale it up so much. Like that",
      "offset": 4131.759,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "becomes a soft barrier in the same way",
      "offset": 4133.359,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "that pre-training it's becoming more and",
      "offset": 4135.199,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "more expensive to train better and",
      "offset": 4136.719,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "better pre-trained models or bigger",
      "offset": 4138.159,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "pre-trained models. The second point is",
      "offset": 4139.839,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "that like as you have these models think",
      "offset": 4141.759,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "for longer, you kind of get bottlenecked",
      "offset": 4143.279,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "by walk time. Like if you want to",
      "offset": 4145.44,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "iterate on experiments, it is really",
      "offset": 4147.279,
      "duration": 3.121
    },
    {
      "lang": "en",
      "text": "easy to iterate on experiments when",
      "offset": 4149.279,
      "duration": 2.321
    },
    {
      "lang": "en",
      "text": "these models would respond instantly.",
      "offset": 4150.4,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "It's actually much harder when they take",
      "offset": 4151.6,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "three hours to respond and what happens",
      "offset": 4154.08,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "when they have three weeks. It takes you",
      "offset": 4156,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "at least 3 weeks to do those evaluations",
      "offset": 4157.92,
      "duration": 5.279
    },
    {
      "lang": "en",
      "text": "and to then iterate on that and and a",
      "offset": 4160.48,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "lot of this you can paralyze experiments",
      "offset": 4163.199,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "to some extent, but a lot of it you have",
      "offset": 4165.279,
      "duration": 3.121
    },
    {
      "lang": "en",
      "text": "to run the experiment, complete it and",
      "offset": 4166.88,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "then see the results in order to decide",
      "offset": 4168.4,
      "duration": 3.279
    },
    {
      "lang": "en",
      "text": "on the next set of experiments. I think",
      "offset": 4169.92,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "this is actually the strongest case for",
      "offset": 4171.679,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "for long timelines that the models",
      "offset": 4173.04,
      "duration": 5.279
    },
    {
      "lang": "en",
      "text": "because they just have to like do so",
      "offset": 4176,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "much in serial time, we can only iterate",
      "offset": 4178.319,
      "duration": 5.121
    },
    {
      "lang": "en",
      "text": "so quickly. How would you overcome that?",
      "offset": 4181.199,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "Well, it's it's a challenge and I think",
      "offset": 4183.44,
      "duration": 3.279
    },
    {
      "lang": "en",
      "text": "it depends on the domain. So drug",
      "offset": 4184.719,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "discovery I think is one domain where",
      "offset": 4186.719,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "this could be a real bottleneck. I mean",
      "offset": 4188.64,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "if you want to see if something like",
      "offset": 4190.08,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "extends human life, it's going to take",
      "offset": 4191.44,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "you a long time to figure out if like",
      "offset": 4193.12,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "this new drug that you developed like",
      "offset": 4195.28,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "actually extends human life and doesn't",
      "offset": 4197.28,
      "duration": 2.879
    },
    {
      "lang": "en",
      "text": "have like terrible side effects along",
      "offset": 4199.04,
      "duration": 2.48
    },
    {
      "lang": "en",
      "text": "the way. Side note, do we not have",
      "offset": 4200.159,
      "duration": 3.121
    },
    {
      "lang": "en",
      "text": "perfect models of human chemistry and",
      "offset": 4201.52,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "biology by now? Well, so this this is I",
      "offset": 4203.28,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "think the thing and again I want to be",
      "offset": 4204.88,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "cautious here because I'm not actually a",
      "offset": 4206.56,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "biologist or chemist. Like I don't I",
      "offset": 4208.48,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "know very little about about these",
      "offset": 4210,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "fields. I last time I took a biology",
      "offset": 4211.36,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "class was 10th grade in high school. I",
      "offset": 4212.8,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "don't think that there's a perfect uh",
      "offset": 4214.96,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "simulator of human biology right now.",
      "offset": 4216.48,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "And I think that that's something that",
      "offset": 4218.96,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "could potentially help address this",
      "offset": 4220.4,
      "duration": 2.64
    },
    {
      "lang": "en",
      "text": "problem. That's like the number one",
      "offset": 4221.76,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "thing that we should all work on. Well,",
      "offset": 4223.04,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "that's one of the things that we're",
      "offset": 4225.44,
      "duration": 2
    },
    {
      "lang": "en",
      "text": "hoping that these racing models will",
      "offset": 4226.239,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "help us with. Yeah. How would you",
      "offset": 4227.44,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "classify mid-training versus",
      "offset": 4229.199,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "post-training today? It's it's such the",
      "offset": 4231.04,
      "duration": 5.679
    },
    {
      "lang": "en",
      "text": "all these definitions are so fuzzy. So I",
      "offset": 4233.44,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "I don't have I don't have a great answer",
      "offset": 4236.719,
      "duration": 3.281
    },
    {
      "lang": "en",
      "text": "there. It's a question people have and",
      "offset": 4238.239,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "you're and like open eyes like now",
      "offset": 4240,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "explicitly hiring for mid-training and",
      "offset": 4241.84,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "everyone is like what the hell is",
      "offset": 4243.92,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "mid-training? I think mid-training is",
      "offset": 4245.199,
      "duration": 5.441
    },
    {
      "lang": "en",
      "text": "between pre-training and post- training.",
      "offset": 4247.92,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "It's like it's like uh it's not it's not",
      "offset": 4250.64,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "post- training. It's not pre-training.",
      "offset": 4253.12,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "It's like adding more to the models but",
      "offset": 4255.52,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "like after pre-training like I don't",
      "offset": 4258.159,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "know interesting ways. Yeah. Okay. All",
      "offset": 4259.679,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "right. Well, you know I trying to get",
      "offset": 4261.679,
      "duration": 5.121
    },
    {
      "lang": "en",
      "text": "some clarity.",
      "offset": 4264.48,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "Is the pre-trained model now basically",
      "offset": 4266.8,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "like a just an artifact that then spawns",
      "offset": 4268.96,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "other models and it's almost like the",
      "offset": 4271.84,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "core pre-training model is never really",
      "offset": 4273.92,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "exposed anymore and it's the",
      "offset": 4275.92,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "mid-training the new pre-training and",
      "offset": 4277.52,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "then there's the post-training once you",
      "offset": 4279.76,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "have the models branched out. you never",
      "offset": 4281.36,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "interact with an actual just like raw",
      "offset": 4283.84,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "pre-trained model. Like if you're going",
      "offset": 4285.6,
      "duration": 2.24
    },
    {
      "lang": "en",
      "text": "to interact with the model, it's going",
      "offset": 4286.8,
      "duration": 2.32
    },
    {
      "lang": "en",
      "text": "to go through mid-training and post-",
      "offset": 4287.84,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "training. So, um, so you're seeing the",
      "offset": 4289.12,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "final product. Well, you don't let us do",
      "offset": 4291.199,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "it, but you know, we used to. Well,",
      "offset": 4292.48,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "yeah. I mean, I guess if you, you know,",
      "offset": 4294.239,
      "duration": 2.401
    },
    {
      "lang": "en",
      "text": "there's open source models where you can",
      "offset": 4295.6,
      "duration": 1.84
    },
    {
      "lang": "en",
      "text": "just like interact with the raw",
      "offset": 4296.64,
      "duration": 3.039
    },
    {
      "lang": "en",
      "text": "pre-train model. Um, but for for OpenAI",
      "offset": 4297.44,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "models, like they go through a",
      "offset": 4299.679,
      "duration": 1.921
    },
    {
      "lang": "en",
      "text": "mid-training step, then they go through",
      "offset": 4300.64,
      "duration": 2.079
    },
    {
      "lang": "en",
      "text": "a post- training step and then, and then",
      "offset": 4301.6,
      "duration": 2.4
    },
    {
      "lang": "en",
      "text": "they're released. And they're a lot more",
      "offset": 4302.719,
      "duration": 2.721
    },
    {
      "lang": "en",
      "text": "useful. Like, frankly, if you interacted",
      "offset": 4304,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "with a only pre-trained model, it would",
      "offset": 4305.44,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "be super difficult to work with and it",
      "offset": 4307.76,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "would Yeah. It would seem kind of dumb.",
      "offset": 4309.199,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "Yeah. But it' be it'd be useful in weird",
      "offset": 4311.28,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "ways, you know, because there's a mode",
      "offset": 4313.76,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "collapse when you when you post",
      "offset": 4315.92,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "straightforward for like chat. Yeah. In",
      "offset": 4317.199,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "some ways, you want that mode collapse",
      "offset": 4319.76,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "like you want you want that collapse of",
      "offset": 4321.44,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "like to be useful. I I get it. We're",
      "offset": 4322.8,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "interviewing Greg Brockman next. Uh",
      "offset": 4325.28,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "you've talked to him a lot. What would",
      "offset": 4327.76,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "you ask him? What would I ask Greg? I",
      "offset": 4330.08,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "mean I mean I get to ask Greg all the",
      "offset": 4332.48,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "time. What What should you ask Greg?",
      "offset": 4333.76,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "like to to evoke an interesting response",
      "offset": 4335.36,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "that like uh not he doesn't get asked",
      "offset": 4337.28,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "enough about but you know like this is",
      "offset": 4339.52,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "something that he's passionate about or",
      "offset": 4341.04,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "you just want his thoughts. I think in",
      "offset": 4344.239,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "general it's worth asking where this",
      "offset": 4345.92,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "goes, you know, like what does the world",
      "offset": 4348.96,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "actually look like in five years? What",
      "offset": 4350.719,
      "duration": 2.881
    },
    {
      "lang": "en",
      "text": "does the world look like in 10 years?",
      "offset": 4352,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "What does that distribution of outcomes",
      "offset": 4353.6,
      "duration": 6.4
    },
    {
      "lang": "en",
      "text": "look like? And what could the world or",
      "offset": 4355.28,
      "duration": 7.919
    },
    {
      "lang": "en",
      "text": "individuals do to help steer things",
      "offset": 4360,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "towards like the good outcomes instead",
      "offset": 4363.199,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "of the negative outcomes? Okay, like an",
      "offset": 4364.64,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "alignment question. I think people get",
      "offset": 4366.88,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "very focused on what's going to happen",
      "offset": 4369.199,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "in like one or two years. And I think",
      "offset": 4371.6,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "it's also worth spending some time",
      "offset": 4373.76,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "thinking about like well what happens in",
      "offset": 4374.96,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "five or 10 years and what what does that",
      "offset": 4376.48,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "world look like? Um I mean he doesn't",
      "offset": 4378.56,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "have a crystal ball like but he he",
      "offset": 4380.56,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "certainly has he certainly has thoughts.",
      "offset": 4383.199,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "Yeah. So I think that's worth exploring.",
      "offset": 4384.96,
      "duration": 5.279
    },
    {
      "lang": "en",
      "text": "Yeah. Okay. What are games that you",
      "offset": 4388.239,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "recommend to people? Uh especially",
      "offset": 4390.239,
      "duration": 3.521
    },
    {
      "lang": "en",
      "text": "socially. Uh what are games that I",
      "offset": 4392.159,
      "duration": 2.881
    },
    {
      "lang": "en",
      "text": "recommend to people? Uh I've been",
      "offset": 4393.76,
      "duration": 2.959
    },
    {
      "lang": "en",
      "text": "playing a lot of this game called Blood",
      "offset": 4395.04,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "on the Clock Tower lately. Um what is",
      "offset": 4396.719,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "it? It's kind of like Mafia or Werewolf.",
      "offset": 4398.96,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "It's become very popular in San",
      "offset": 4401.199,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "Francisco is uh Oh, that's the one who",
      "offset": 4403.92,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "played in your house. Yeah. Okay. Got",
      "offset": 4405.52,
      "duration": 2.56
    },
    {
      "lang": "en",
      "text": "it. Got it. It's kind of funny because",
      "offset": 4406.96,
      "duration": 2.56
    },
    {
      "lang": "en",
      "text": "like I was talking to a couple people",
      "offset": 4408.08,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "now that have told me that it used to be",
      "offset": 4409.52,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "that poker was the like way that like",
      "offset": 4411.52,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "the VCs and tech founders and stuff",
      "offset": 4414.88,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "would socialize with each other. And",
      "offset": 4416.4,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "actually now it's shifting more towards",
      "offset": 4418.32,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "blood on the clock tower. Like that's",
      "offset": 4419.76,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "the that the thing that people use to",
      "offset": 4421.04,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "like um you know connect in the Bay",
      "offset": 4423.04,
      "duration": 7.44
    },
    {
      "lang": "en",
      "text": "Area. And I was actually told that a a",
      "offset": 4425.76,
      "duration": 7.36
    },
    {
      "lang": "en",
      "text": "startup held a recruiting event that was",
      "offset": 4430.48,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "a blood on the clock tower game. Wow.",
      "offset": 4433.12,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "Yeah. So, uh I guess it's like it's",
      "offset": 4435.28,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "really catching on, but it's a fun game",
      "offset": 4437.44,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "and I guess you lose less money playing",
      "offset": 4439.52,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "it than you do playing poker. So, it's",
      "offset": 4441.199,
      "duration": 2.641
    },
    {
      "lang": "en",
      "text": "like better for people that are not very",
      "offset": 4442.719,
      "duration": 2.801
    },
    {
      "lang": "en",
      "text": "good at these things. U I I think it's",
      "offset": 4443.84,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "kind of like a weird recruiting event,",
      "offset": 4445.52,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "but it's certainly a fun game. What",
      "offset": 4447.12,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "qualities make a winner here that is",
      "offset": 4449.04,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "interesting to hire for? That's the",
      "offset": 4452.08,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "thing is like okay I guess you get",
      "offset": 4453.76,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "ability to lie deception and like",
      "offset": 4456.32,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "picking up on deception like is that the",
      "offset": 4458.08,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "best employee I don't know.",
      "offset": 4459.84,
      "duration": 6.399
    },
    {
      "lang": "en",
      "text": "So my slight final pet topic is Magic",
      "offset": 4463.6,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "the Gathering. So you have we talked",
      "offset": 4466.239,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "about some of these games Chesco and",
      "offset": 4468.64,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "they have perfect information. Then you",
      "offset": 4470.239,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "have Poker which is imperfect",
      "offset": 4471.6,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "information in a pretty limited",
      "offset": 4473.44,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "universe. You only have a 52 card deck.",
      "offset": 4475.12,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "And then you have these other games that",
      "offset": 4477.52,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "have imperfect information like a huge",
      "offset": 4478.88,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "pool of possible options. Do you have",
      "offset": 4481.04,
      "duration": 5.119
    },
    {
      "lang": "en",
      "text": "any idea of like how much harder that",
      "offset": 4483.6,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "is? Like how does the difficulty of this",
      "offset": 4486.159,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "problem scale? I love that you asked",
      "offset": 4488.239,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "that because I have this like huge store",
      "offset": 4489.76,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "of knowledge on AI frame information",
      "offset": 4492.239,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "games like this is my my area of",
      "offset": 4494.8,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "research for so long and I know all",
      "offset": 4496.239,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "these things but I don't get to talk",
      "offset": 4498.08,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "about it very often. We've made",
      "offset": 4499.6,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "superhuman poker AIs for no limit Texas",
      "offset": 4501.28,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "holdem. One of the interesting things",
      "offset": 4504.239,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "about that is that like the amount of",
      "offset": 4505.92,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "hidden information is actually pretty",
      "offset": 4508.239,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "limited because you have two hidden",
      "offset": 4510.32,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "cards when you're playing Texas Holdem.",
      "offset": 4511.92,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "And so the number of possible states",
      "offset": 4513.76,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "that you could be in is 1,326",
      "offset": 4516.48,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "when you're playing heads up at least.",
      "offset": 4519.36,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "And you know that's multiplied by the",
      "offset": 4521.12,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "number of other players that there are",
      "offset": 4523.04,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "at the table, but it's still like not a",
      "offset": 4524.08,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "massive number. And so the way these AI",
      "offset": 4525.76,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "models work is they enumerate all the",
      "offset": 4528,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "different states that you could be in.",
      "offset": 4530.56,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "So, if you're playing like six-handed",
      "offset": 4532.56,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "poker, there's five other players. Five",
      "offset": 4534.32,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "times 1,326. That's the number of states",
      "offset": 4535.6,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "that you be. And then you assign a",
      "offset": 4537.92,
      "duration": 2.64
    },
    {
      "lang": "en",
      "text": "probability to each one. And then you",
      "offset": 4538.96,
      "duration": 2.64
    },
    {
      "lang": "en",
      "text": "feed those probabilities into your",
      "offset": 4540.56,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "neural net. And you get actions back for",
      "offset": 4541.6,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "each of those states. The problem is",
      "offset": 4544,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "that as you scale the number of hidden",
      "offset": 4545.76,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "possibilities, like the number of state",
      "offset": 4548.64,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "of of possible states you could be in,",
      "offset": 4550,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "that approach breaks down and there's",
      "offset": 4552.08,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "still this very interesting unanswered",
      "offset": 4554.239,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "question of what do you do when the",
      "offset": 4556.8,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "number of hidden states becomes",
      "offset": 4559.04,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "extremely large. Mhm. You know, so if",
      "offset": 4560.719,
      "duration": 3.121
    },
    {
      "lang": "en",
      "text": "you go to Omaha Poker where you have",
      "offset": 4562.48,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "four hidden cards, there are things you",
      "offset": 4563.84,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "could do that's kind of like that are",
      "offset": 4565.679,
      "duration": 2.56
    },
    {
      "lang": "en",
      "text": "kind of heristic that you could do to",
      "offset": 4566.88,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "reduce the number of states, but",
      "offset": 4568.239,
      "duration": 2.721
    },
    {
      "lang": "en",
      "text": "actually it's still a very difficult",
      "offset": 4569.76,
      "duration": 2.64
    },
    {
      "lang": "en",
      "text": "question. And then if you go to a game",
      "offset": 4570.96,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "like Strateggo where you have 40 pieces,",
      "offset": 4572.4,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "so there's like close to 40 factorial",
      "offset": 4575.28,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "different states you could be in, then",
      "offset": 4577.6,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "all these like existing approaches that",
      "offset": 4579.92,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "we used for poker kind of break down and",
      "offset": 4581.52,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "you need different approaches and",
      "offset": 4583.52,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "there's a lot of active research going",
      "offset": 4585.12,
      "duration": 2.559
    },
    {
      "lang": "en",
      "text": "on about like how do how do you cope",
      "offset": 4586.32,
      "duration": 2.879
    },
    {
      "lang": "en",
      "text": "with that? So for something like Magic",
      "offset": 4587.679,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "the Gathering, the techniques that we",
      "offset": 4589.199,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "used in poker would not out of the box",
      "offset": 4591.04,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "work. And it's still an interesting",
      "offset": 4593.36,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "research question of like what do you",
      "offset": 4596,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "do? Now I should say this becomes a",
      "offset": 4597.44,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "problem when you're doing the kinds of",
      "offset": 4599.679,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "search techniques that we used in poker.",
      "offset": 4601.84,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "If you're just doing model free RL, it's",
      "offset": 4603.36,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "not a problem. And my guess is that if",
      "offset": 4606.239,
      "duration": 3.521
    },
    {
      "lang": "en",
      "text": "somebody put in the effort, they could",
      "offset": 4608.56,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "probably make a superhuman bot for Magic",
      "offset": 4609.76,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "the Gathering. Now, yeah, there's still",
      "offset": 4611.28,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "some unanswered research questions in",
      "offset": 4613.12,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "that space. Now, are they the most",
      "offset": 4615.199,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "important unanswered research questions?",
      "offset": 4616.8,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "Like, I'm inclined to say no. I think",
      "offset": 4618.159,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "there's like the problem is that like",
      "offset": 4620,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "the techniques that we used in poker to",
      "offset": 4621.679,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "do this kind of search stuff were pretty",
      "offset": 4623.36,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "limited. And like if you expand if you",
      "offset": 4625.12,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "expand those techniques, maybe you get",
      "offset": 4627.199,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "them to work on things like strategic",
      "offset": 4629.04,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "the gathering, but they're still going",
      "offset": 4630.719,
      "duration": 2.241
    },
    {
      "lang": "en",
      "text": "to be limited. They're not going to get",
      "offset": 4631.92,
      "duration": 3.279
    },
    {
      "lang": "en",
      "text": "you like superhuman and code forces with",
      "offset": 4632.96,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "language models. So, I think it's more",
      "offset": 4635.199,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "valuable to just focus on the very",
      "offset": 4637.679,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "general reasoning techniques. And one",
      "offset": 4639.44,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "day as we improve those, I think we'll",
      "offset": 4641.28,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "have a model that just out of the box",
      "offset": 4643.44,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "one day plays Magic the Gathering at a",
      "offset": 4645.199,
      "duration": 3.121
    },
    {
      "lang": "en",
      "text": "superhuman level. And I think that's the",
      "offset": 4646.64,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "more important and more impressive",
      "offset": 4648.32,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "research direction. Cool. Amazing. Yeah.",
      "offset": 4649.6,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "Thanks very much for coming on, N. Yeah.",
      "offset": 4652.32,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "Thanks for your time. Yeah. Thanks.",
      "offset": 4654.48,
      "duration": 3.03
    },
    {
      "lang": "en",
      "text": "Thanks for having me.",
      "offset": 4656,
      "duration": 10.29
    },
    {
      "lang": "en",
      "text": "[Music]",
      "offset": 4657.51,
      "duration": 8.78
    }
  ],
  "cleanText": "Hey everyone, welcome to the L and Space podcast. This is Allesio, partner and CTO of Deible, and I'm joined by my co-host Spooks, founder of Small AI. Hello. And we are here recording on a holiday Monday with Noam Brown from OpenAI. Welcome. Thank you. So glad to have you finally join us. Uh, a lot of people have heard you. You've been rather generous of your time on podcasts, like Lex Friedman, and you've done a TED talk recently just talking about the thinking paradigm, but I think maybe perhaps your most interesting recent achievement is winning the World Diplomacy Championship. Yeah. In 2022, you built like sort of Cicero, which was top 10% of human players. I guess my opening question is how has your Diplomacy playing changed since working on Cicero and then now personally playing it? When you work on these games, you kind of have to understand the game well enough to like be able to debug your bot because if the bot does something that's like really radical and that humans typically wouldn't do, you're not sure if that's like a mistake or if that's just uh like if it's a bug in the system or it's actually just like the bot being brilliant. When we were working on Diplomacy, I kind of did this deep dive like trying to understand the game better. I played in tournaments. I watched a lot of tutorial videos and commentary videos on games, and over that process I got better. And then also seeing the bot, like the way it would behave in these games, like sometimes it would do things that humans typically wouldn't do, and that taught me about the game as well. When we released Cicero, we announced it in late 2022. I still found the game really fascinating, and so I kept up with it. I continue to play, and that led to me winning the championship in the World Championship in 2025, so just a couple months ago. There's always a question of like centaur systems where humans and machines work together. Like was there an equivalent of what happened in Go where you updated your play style because you're asking if I used Cicero when I played in the tournament. The answer is no. Seeing the way the bot played and like taking inspiration from that, I think did help me in the tournament. Yeah. Do people now ask Turing questions every single time when they're playing Diplomacy? Ask to try to figure out if uh if the person they're playing with is a bot or? Yeah, like that's the one thing you're worried about when you started. It was really interesting when we were working on Cisero because like, you know, we didn't have the best language models. We were really bottlenecked on the quality of the language models, and sometimes the bot would do would say bizarre things. Like, you know, 90, 99% of the time it was fine, but then like every once in a while it would say this like really bizarre thing, like it would just hallucinate about something. Somebody would reference something that they said earlier in a conversation with the bot, and the bot would be like, \"I have no idea we're talking about. I never said that.\" And then the person would be like, \"Look, you could just scroll up in the chat.\" It's like literally right there. And the bot would be like, \"No, you're Windows.\" And when it does these kinds of things, like people just kind of shrugged it off as like, \"Oh, that's just, you know, the person's tired or they're drunk or whatever or they're just like trolling me.\" But I think like that's because people weren't looking for a bot. They weren't expecting a bot to be in the games. We're actually really scared because we were afraid that people would figure out at one point that there's a bot in these games, and then they would just like always be on the lookout for it, and they would always be, and if you're if you're looking for it, you're able to spot it. That's the thing. So, I think now that it's announced and that people know to look for it, I think they would have an easier time spotting it. Now, that said, the language models have also gotten a lot better since 2022. It's adversarial. Yeah. So, at this point, like, you know, GP4 and like 03, these models are like passing the Turing Test. So I don't think they can really ask that many Turing complete questions that would actually make a difference. And Cesar was very small, like 2.7b, right? It was a very small uh language model. Yeah. This is one of the things we realized over the course of the project that like, oh yeah, you really benefit a lot from just having like larger language models, right? Yep. How do you think about today's perception of AI and a lot of like maybe the safety discourse of like, you know, you're kind of built a bot that is really good at persuading people into like helping them win a game, and I think maybe today labs want to say they don't work on that type of problem. How do you think about that dichotomy so to speak between the two? No, honestly, like after we released Cicero, a lot of the AI safety community was really happy with the research and like the way it worked because it was a very controllable system. Like we conditioned Cicero on certain concrete actions, and that gave it a lot of steerability to say like, okay, well, it's going to pursue a behavior that we can like very clearly interpret and and very clearly define. It's not just like, oh, it's a language model like running loose and doing whatever it feels like. It's actually like pretty steerable, and there's this whole reasoning system that steers the way the language model interacts with the human. Actually, a lot of researchers reached out to me and said like, we think this is like potentially a really good way to achieve safety with these systems. I guess the last Diplomacy related questions that we might have is have you updated or tested like O-Series models on Diplomacy and would you would you expect a lot more difference? I have not. I think I said this on Twitter at one point that I think this would be a great benchmark. I would love to see all the leading bots play a game of Diplomacy with each other and see like who does best, and I think a couple people have like taken inspiration from that and are actually like building out these benchmarks and like evaling the models. My understanding is that they don't do very well right now. Um, but I think I think it really is a fascinating benchmark, and I think it would be um yeah, I think it'd be a really cool thing to to try out. Well, we're going to go a little bit into O-Series now. I think the last time you did a lot of publicity, you were just launching 01. You did your TED talk and everything. How has the vibe, how have the vibes changed just in general? You said you were very excited to learn from domain experts like in chemistry, like how they review the O-Series models, like how have you updated since, let's say, end of last year? I think the trajectory was pretty clear pretty early on in the development cycle. And I think that everything that's unfolded since then has been pretty on track for what I expected. So, I wouldn't say that my perception of where where things are going or has honestly changed that much. Um, I think that we're going to continue to see, I said before that we're going to see this paradigm continue to progress rapidly, and I think that that's true even today, that we we saw that with like going from 01 preview to 01 to 03, consistent progress, and we're going to continue to see that going forward, and I think that we're going to see a broadening of what these models can do as well. You know, like we're going to start seeing agentic behavior. We're already starting to see agentic behavior. Like honestly, for me, 03, I've been using it a ton in my day-to-day life. I just find it so useful, this, especially the fact that I can now browse the web and like, you know, do meaningful research on my behalf, like it's kind of like a mini deep research that you can just get a response in three minutes. So yeah, I think it's just going to continue to become more and more useful and uh more powerful as time goes on and pretty quickly. Yeah. And talking about deep research, you tweeted about if you need proof that we can do this in a number of viable domains, deep research is kind of like a great example. Can you maybe talk about if there's something that people are missing? You know, I feel like I hear that repeated a lot. It's like, you know, it's easier to do encoding and math, but like not in these other domains. I frequently get this question, including from pretty established AI researchers, that okay, we're seeing these like reasoning models succeed in math and coding and these these easily verifiable domains, but are they ever going to succeed in domains where success is less well defined? I'm surprised that this is such a common perception because we've released deep research and people can try it out. People do use it. It's very popular, and that is very clearly a domain where you don't have an easily verifiable metric for success. It's very like what what is the best research report that you could generate, and yet these models are doing extremely well at this at this domain. So I think that's like an existence proof that these models can succeed in tasks that don't have as easily verifiable rewards. Is it because there's also not necessarily like a wrong answer, like there's a spectrum of deep research quality, right? You can have like a report that like looks good, but the information is kind of so and so, and then you have a great report. Do you think people have a hard time understanding the difference when they get the result? My impression is that people do understand the difference when they get a result, and and I think that they're surprised at how good the deep research results are. There's certainly it's not not 100%. It could be better, and we're going to make it better, but I think people can tell the difference between a good report and a bad report and and certainly and a good report and a mediocre report, and that's enough to kind of feed the the loop later to build the product and improve the model performance. I mean, I think if you're in a situation where people can't tell the difference between the outputs, then it doesn't really matter if you're like, you know, hill climbing on on progress. Uh, these models are going to get better at domains where there is a measure of success. Now I think this idea that it has to be like easily verifiable or something like that. I don't think that's true. I think that you can have you can have these models do well even in domains where success is a very difficult to define thing, could sometimes even be subjective. People lean on a lot, you've you've done as well, is the thinking fast as slow analogy for just uh thinking models, and I think it's reasonably well diffused now, the idea of uh that that this is kind of the next scaling paradigm. All analogies are imperfect. What is one way in which thinking fast and slow or System 1, System 2 kind of doesn't transfer to how we actually scale these things? One thing that I think is underappreciated is that the models, the pre-trained models, need a certain level of capability in order to really benefit from this like extra thinking. This is kind of why you you've seen the reasoning paradigm emerge around the time that it did. I think it could have happened earlier, but if you try to do the reasoning paradigm on top of GBD2, I don't think it would have gotten you almost anything. Is this emergence? Hard to say. I if it's emergence necessarily, but like I haven't done the um, you know, the measurements to really define that clearly. Um, but I think it's pretty clear, you know, people tried chain of thought with GBD, like really small models, and they saw that it just didn't really do anything. Then you go to bigger models and it starts to to give a lift. I think there's a lot of debate about like the extent to which this kind of behavior is emergent, but clearly there is a difference. So, it's not like there are these two independent paradigms. I think that they are related in the sense that you need a certain level of System 1 capability in your models in order to have System 2 is to be able to benefit from System 2. Yeah, I have play tried to play amateur neuroscientists before and try to compare it to the evolution of the brain and how you have to evolve the cortex first before you evolve the other parts of the brain. And perhaps that is what we're doing here. Yeah. And you could argue that actually this is not that different from like I guess the um the System 1, System 2 paradigm because, you know, if you ask like a pigeon to think really hard about playing chess, you know, it's not going to get that far. You know, it doesn't matter if it like thinks for a thousand years, it's like not going to be able to be better at playing chess. So maybe you do still also also in like with animals and humans that you need a certain level of intellectual ability just in terms of System 1 in order to benefit from System 2 as well. Yeah. Just this side tangent, does this also apply to visual reasoning? So let's say we have now we have the 40 like natively omnimodel type of thing, then that also makes 03 really good at GeoGuessr. Does that apply to other modalities too? I I think the evidence is yes. It depends on exactly the kinds of questions that you're asking. Like there are some questions that I think don't really benefit from System 2. I think GeoGuessr is certainly one uh where where you do benefit. I think image recognition, if if I had to guess, it's like one of those things where you probably benefit less from System 2 thinking, cuz you know it or you don't. Yeah. Exactly. There's no way. Yeah. And the the thing the thing I typically point to is just like information like retrieval. If somebody asks you like when was this person born and you don't have access to the web, then you either know it or you don't, and you can sit there and you can think about it for a long time. Maybe you can make an educated guess and you can say like, well, this person was like probably lived around this time, and so this is like a rough date. But you're not going to be able to like get the date unless you actually just just know it. But like spatial reasoning, like Tic-Tac-Toe might be better because you have all the information there. Yeah. And I think it's true that like with Tic-Tac-Toe, we see that like GPD 4.5 falls over. You know, it plays decently well. I shouldn't say it falls over. It does reasonably well. You can draw the board. It can make legal moves, but it will make mistakes sometimes. And if you really need that\n\n\nSystem two to enable it\nto play perfectly.\nNow, it's possible that if you got to GBD6 and you just did\nsystem one, it would also play perfectly.\nYou know, I guess we'll know one day, but I think right now you would need the system two to really like do well.\nWhat do you think are like the things that you need in system one?\nSo obviously general understanding of like game rules.\nDo you also need to understand some sort of like metagame of like you know usually this is like how you value pieces in different games even though it's a you know how do you generalize in system one so that then in system two you can kind of get to the gameplay so to speak.\nI think the more that you have in your in the system one, like this is the same thing with humans, you know, like humans are when they're playing for the first time, uh, a game like Chess, they can apply a lot of system two thinking to it.\nAnd if you if you apply a ton of system two thinking to it, like if if you just present a really smart person with a completely novel game, and you tell them like, okay, you're going to play this game against like an AI or like a human that's like mastered this game, and you tell them to like sit there and and think about it for like 3 weeks about how to play this game.\nMy guess is they could actually do pretty well, but it certainly helps to build up that system one thinking like build up intuition about about the game because it will just make you so much Yeah.\nSo much faster.\nI think the Pokemon example is a good one of like the system one kind of has maybe all this information about games and then once you put it in the game it still needs a lot of harnesses to work and I'm trying to figure out how much of can we take things from the harness and have them in system one to the system two as harness free as possible but I guess that's like the question about generalizing games and AI.\nYeah, I guess I view that as a different question.\nI think the question about like harnesses in my view is that the ideal harness is no harness.\nYeah, right.\nI think harnesses are like a crutch that eventually we're going to be able to move beyond.\nSo, only two costs and you could ask, you know, you could just ask O3.\nAnd actually, you know, it's interesting cuz like when this uh playing Pokemon thing kind of like emerged as as this like, you know, benchmark.\nI was actually like pretty opposed to evaling this with our with our like OpenAI models because my feeling is like, okay, if we're going to do this eval, let's just do it with O3.\nYou know, how far does O3 get without any harness?\nHow far does it get playing PokÃ©mon?\nAnd the answer is like not very far, you know?\nUm, and that's fine.\nI think it's fine to have an eval where the models do terribly.\nAnd I don't think the answer to that should be like, well, let's build a really good harness so that now it can do well on this eval.\nI think the answer is like, okay, well, let's just like improve the capabilities of our models so they can do well at everything and then they also happen to make progress on this eval.\nWould you consider things like checking for a valid move a harness or is this in the in the model?\nYou know, like Chess.\nIt's like you can either have the model learn in system one what moves are valid and what it can and cannot do versus in system two figuring out.\nI think I think there's like a lot of this is design questions.\nLike for me, I think you should give the model the ability to check if a move is legal if you want.\nLike that that could be an option in the environment of like okay here's a you know an action that you can like a tool call that you can make to see if an action is legal if it wants to use that it it can and then there's like design question of like well what do you do if the model makes an illegal move and I think it's totally reasonable to say like well if they make an illegal move then they lose the game like I don't know what what happens when a human makes an illegal move in a game of Chess.\nI actually don't know they're just not allowed to yeah like do you just lose the game I don't know so if that's if that's the case then I think it's totally reasonable to say like yeah we're going to have an eval where that's also the criteria for for the AI models.\nYeah.\nBut I think like maybe one way to interpret that in sort of researcher terms is are you allowed to do search?\nAnd one of the famous findings from DeepMind is that MCTS wasn't that useful to them.\nBut I think like there are a lot of engineers trying out search and spending a lot of tokens doing that and maybe it's not worth it.\nWell, I'm making a distinction here between like a tool call to check whether a move is legal or illegal is different from actually making that move and then seeing whether it ended up being legal or illegal.\nRight?\nSo, if that tool call is available, I think it's totally fine to make that tool call and check whether a move is legal or illegal.\nI think it's different to have the model say, \"Oh, I'm making this move.\"\nYeah.\nAnd then, you know, it gets feedback that like, \"Oh, you made an illegal move.\"\nAnd so, then it's like, \"Oh, just kidding.\nLike, I'm going to do something else now.\"\nSo, so that's that's the distinction I'm I'm drawing.\nSome people have tried to classify that second type of laying things out as test time compute.\nYou would not classify that as test time compute.\nThere's a lot of reasons why you would not want to rely on that paradigm when you're going to the imagine you have a robot, you know, and your robot like takes some action in the world and it like breaks something and you're just like, oh, you can't say like, oh, just kidding.\nI didn't mean to do that.\nI'm going to undo that action.\nLike the thing is broken.\nSo if you want to simulate what would happen if I move the robot in this way and then in your simulation you saw that this thing broke and then you decide not to do that action that's totally fine but you can't just like undo actions that you've taken in the world.\nThere's a couple more things I wanted to cover in this rough area.\nI actually had an answer on the on the thinking fast and slow side which maybe I I'm curious what you think about like a lot of people are trying to put in effectively model router layers let's say between like the the fast response model and the the long thinking model.\nAnthropic is explicitly doing that and I think there's a question about always do you need a smart judge to route or do you need a dumb judge judge to route because it's fast so when you have a model router let's say let's say you're passing requests between system one side and system two\nDoes the router need to be as smart as the smart model or dumb to be fast?\nI think it's possible for a dumb model to recognize that a problem is really hard and that it won't be able to solve it and then route it to a a more capable model, but it's also possible for a dumb model to be fooled or to be overconfident.\nI don't know.\nI think there's a real trade-off there.\nBut I will say like I think I think there are a lot of things that people are building right now that will eventually be washed away by scale.\nSo I think harnesses are a good example where I think eventually the models are going to be and I think this actually happened with the reasoning models like before the reasoning models emerged there was like all of this work that went into engineering these like agentic systems that like made a lot of calls to GBD40 or like the these non-reasoning models to get reasoning behavior and then it turns out like oh we just like created reasoning models and they you don't need this like complex behavior.\nIn fact, in many ways it makes it worse like you just give the reasoning model the same question without any sort of scaffolding and it just does it now that you can still and so people are building scaffolding on top of the reasoning models right now but I think in many ways like those scaffolds will also just be replaced by the reasoning models and models in general becoming more capable and similarly I think things like model uh like these routers you know we've said pretty openly that we want to move to a world where there is a single unified model and in that world you shouldn't need a router on top of the model.\nSo I think that the router issue will eventually be solved.\nAlso like you're building the router into the model kind of weights itself.\nI don't think there will be a a a benefit for like I I shouldn't say because it's I could be wrong about this like you know and certainly maybe there's um you know reasons to route to different model providers or whatever but I think that routers are going to um eventually go away and I can understand why it's worth doing it in the short term because like the fact is it is beneficial right now and if you're building a product and you you're getting a lift from it then it's it's worth doing right now.\nOne of the tricky things I'd imagine that a lot of developers are facing is that like you kind of have to plan for where these models are going to be in six months and 12 months and that's like very hard to do because things are progressing very quickly.\nYou know, you don't want to spend 6 months building something and then just have it be totally washed away by scale.\nBut I I think I would encourage developers like when when they're, you know, building these kinds of things like scaffolds and and routers, keep in mind that the field is evolving very rapidly.\nYou know, things are going to change in 3 months, uh, let alone 6 months.\nAnd that might require radically changing these things around or or tossing them out completely.\nSo don't spend 6 months building something that might get tossed out in 6 months.\nIt's so hard though.\nEveryone says this and then like no one has concrete suggestions on how.\nWhat about reinforcement fine-tuning?\nIs this something that obviously you just released it a month ago at OpenAI.\nIs that something people should spend time on right now or maybe wait until the next jump?\nI think reinforcement fine is is pretty cool and I I think it's like worth looking into because it's really about specializing the models for the data that you have and I think that um something that's like worth worth looking into for for developers like we're not we're not suddenly going to like have that data baked into the raw model a lot of times.\nSo I I think that's kind of like a separate question.\nYeah.\nSo creating the environment and the reward model is the best thing people can do right now.\nI think the question that people have is like should I rush to fine-tune the the model using RFT or should I build the harness to then RFT the models as they get better.\nI think the difference is that like for reinforcement fine-tuning you're collecting data that's going to be useful as the models improve as well.\nSo if we come out with like future models that are even more capable, you could still fine-tune them on your data.\nThat's I think actually a good example where you're building something that's going to complement the model scaling and becoming more capable rather than necessarily getting washed away by the scale.\nY one last question on Ilya.\nYou mentioned on I think the Sarah and Elad podcast where you had this conversation with Ilya a few years ago about more RL and reasoning and language models.\nJust any speculation or thoughts on why his attempt when he tried it, it didn't work or the timing wasn't right and why the time is right now.\nI don't think I I would frame it that way that like this his attempt didn't work in many ways it did.\nUm so Ilya for me I saw that in all of these domains that I had worked on in Poker and Hannabi and Diplomacy having the models think before acting made a huge difference in performance like orders of magnitude difference like 10,000 times or something.\nYeah.\nLike you know a thousand to 100,000 times like it's the the equivalent of a model that's like a thousand to 100,000 times bigger.\nAnd in language models you weren't really seeing that that the model the models would just respond instantly.\nSome people in the field in the LLM field were like convinced that like, okay, we just keep scaling pre-training, we're going to get to super intelligence.\nAnd I was kind of skeptical of that perspective.\nIn late 2021, I was having a meal with Ilya.\nHe asked me what my AGI timelines are.\nVery standard SF question.\nAnd I told him like, look, I think it's actually quite far away because we're going to need to figure out this reasoning paradigm in a very general way.\nAnd with things like LLMs, LLMs are very general, but they don't have a reasoning paradigm that's very general.\nAnd until they do, they're going to be limited in what they can do.\nLike, we're going to scale it.\nSure, we're going to scale these things up by a few more orders of magnitude.\nThey're going to become more capable, but we're not going to see super intelligence from just that.\nAnd like, yes, if we had a quadrillion dollars to train these models, then maybe we would, but like you're going to hit the limits of what's economically feasible before you get to super intelligence, unless you have a reasoning paradigm.\nAnd I was convinced incorrectly that the reasoning paradigm would take a long time to figure out because it's like this big unanswered research question and you know Ilya agreed with me and he said like yeah you know I think we need this like additional paradigm but his take was that like maybe it's not that hard.\nI I didn't know it at the time but like he and others at OpenAI had also been thinking about this.\nThey had also been thinking about RL.\nThey had been working on it and I think they had some success but like you know with most research like it does you have to iterate on things.\nYou have to try out different ideas.\nYou have to yeah try different things and then also as the models become more capable as they become faster it becomes easier to iterate on experiments and I think that the work that they did even though it didn't like result in a reasoning paradigm it all builds on top of previous work right so they built a lot of things that over over time led to this reasoning paradigm for listeners Noam can talk about this but the rumor is that that thing was cenamed GPT0 if you want to search for that that line of work.\nI think there was a time where like basically RL kind of went through a dark age when everyone like went all in on it and then nothing happens and they gave up and like now it's like kind of the golden age again.\nSo that's what I'm like trying to identify like why what is it and it\n\n\nIt could just be that we have smarter base models and better data.\nI don't think it's just that we have smarter base models.\nI think it's that, yeah, so we did end up getting a big success with reasoning, and I think it was in many ways a gradual thing.\nTo some extent, it was gradual.\nYou know, like there were signs, there were signs of life, and then we, like, you know, iterated and tried out some more things.\nWe got like better signs of life.\nI think it was around like November 2023 or October 2023 when I think I was convinced that we had like very conclusive signs of life that, like, oh, this, this was going to be, this is the paradigm, and it's going to be a big...\nThat was in many ways a gradual thing.\nI think what OpenAI did well is, like, when we got those signs of life, they recognized it for what it was and invested heavily in scaling it up.\nAnd I think that's ultimately what led to reasoning models arriving when they did.\nWas there any disagreement internally, especially because, like, you know, OpenAI kind of pioneered pre-training scaling, you know, and kind of like \"computers, all you need,\" and then you're kind of saying maybe that's not how we get there?\nWas it clear to everybody that, like, okay, this is going to work, or was it controversial?\nThere's always different opinions about this stuff.\nI think there were some people that felt that pre-training was all we need, and we scaled it up to infinity, and we were there.\nI think a lot of the leadership actually at OpenAI recognized that there was another paradigm that was needed, and that was why they were investing all this, like, research effort into this, like, RL stuff.\nAnd I think that's also to the credit of OpenAI that, like, okay, yes, they figured out the pre-training paradigm, and they were very focused on scaling that up.\nIn fact, the vast majority of resources were focused on scaling that up.\nBut they also recognized the value that something else was going to be needed, and it was worth researching, putting researcher effort into other directions to figure out what that extra paradigm was going to be.\nThere was a lot of debate about, first of all, like, what is that extra paradigm?\nSo I think a lot of the researchers looked at reasoning, and RL was not really about scaling test time compute.\nIt was more about data efficiency because, you know, the feeling was that, well, we have tons and tons of compute, but we actually are more limited by data.\nSo there's the data wall, and we're going to hit that before we hit limits on the compute.\nSo how do we make these algorithms more data efficient?\nThey are more data efficient, but I think that also, like, um, they are also just like the equivalent of scaling up compute also by a ton.\nThat was interesting.\nThere's like a lot of debate around, like, okay, what, what exactly are we doing here?\nAnd then I think also, even when we got the signs of life, I think there was a lot of debate about the significance of it, that, like, okay, how much should we invest in scaling up this paradigm?\nI think, especially when you're, when you're in a small company, like, you know, OpenAI, like, in 2023 was not as big as it is today, and compute was more constrained than it is today.\nAnd if you're investing resources in a direction that's coming at the expense of something else, and so if you look at these signs of life on reasoning and you're saying, \"Okay, well, this looks promising.\nWe're going to scale this up by a ton and invest a lot more resources into it, where are those resources coming from?\"\nYou have to make that tough call about where to, where to draw the resources from.\nAnd that is a very controversial, very difficult call to make, um, that makes some people unhappy.\nAnd I think there was debate about whether we're focusing too much on this paradigm, whether it's really a big deal, whether we would see it generalize and do various things.\nAnd I remember it was interesting that I talked to somebody who left OpenAI after we had discovered the reasoning paradigm, but before we announced O-Series, and they ended up going to a computing lab.\nI saw them afterwards after we announced, um, O-Series, and they told me that, like, at the time, they really didn't think this, like, reasoning thing, like, this, these O-Series, the strawberry models, were like that, that big of a deal.\nIt was like they thought we were making a bigger deal of it than it really deserved to be.\nAnd then when we announced O-Series and they saw the reaction of their co-workers at this competing lab about how everybody was like, \"Oh crap, like, this is a big deal,\" and they, like, pivoted the whole research agenda.\nOh my god.\nTo focus on this, that then they realized, like, \"Oh, actually, like, this maybe is a big deal.\"\nYou know, a lot of this seems obvious in retrospect, but at the time, it's actually not so obvious and be quite difficult to recognize something for what it is.\nI mean, OpenAI has, like, a great history of just making the right bet.\nI feel GBD models are kind of similar, right?\nWhere, like, it started with games and RL, and then it's like, maybe we can just scale these language models instead.\nAnd, um, I'm just impressed by the leadership and obviously the research team that keeps coming out with these insights.\nLooking back on it today, it might seem obvious that, like, oh, of course, like these models get better with scaled, so you should just scale them up a ton, and it'll get better.\nBut it really is the best research is obvious in retrospect, and at the time, it's not as obvious as it might seem today.\nFollow questions on data efficiency.\nUh, this is, this is a pet topic of mine.\nIt seems that our current methods of learning are so inefficient still, right?\nLike, compared to the existence proof of humans.\nWe take five samples and we learn, learn something.\nMachines, 200, maybe, you know, per, per, like, whatever data point you might need.\nAnyone doing anything interesting in data efficiency, or do you think, like, there's just a fundamental inefficiency that machine learning has that will just always be there compared to humans?\nI think it's a good point that if you look at the amount of data these models are trained on and you compare it to, like, the amount of data that a human observes to get the same performance.\nI guess pre-training, it's a little hard to make an apples-to-apples comparison because, like, I don't know how much, how many tokens does a baby actually absorb when they're developing.\nBut I think it's a fair statement to say that these models are less data efficient than humans.\nAnd I think that that's an unsolved research question and probably one of the most important unsolved research questions.\nMaybe more important than algorithmic improvements, cuz you can just, you can, we can increase the supply of data out of the existing set of the worlds and humans.\nI guess, okay.\nSo a couple thoughts on that.\nLike, one is that the answer might be an algorithmic improvement.\nLike, maybe algorithmic improvements do lead to greater data efficiency.\nAnd the second thing is that, like, it's not like humans learn from just reading the internet.\nSo, um, I think it's certainly easiest to learn from just, like, data that's on the internet.\nUm, but I don't think that's, like, the limit of what data you could collect.\nThe last follow-up before we change topics to coding.\nAny other just anecdotes or insights from Ilya just in general, cuz, like, you've worked with him, so there's not that many people that we can talk to that have worked with him.\nI think I've just been very, very impressed with his vision that I think, like, especially when I joined and and I saw, you know, the internal documents at OpenAI of, like, what he had been thinking about back in, like, 2021, 2022, even earlier, I, I was very impressed that he had a clear vision of, like, where this was all going and what was needed.\nSome of his emails from 2016-17 when they were founding OpenAI was published, and even then he was talking about how he thinks, like, one big experiment is much more valuable than 100 small ones.\nThat was, like, a core insight that differentiated them from Brain, for example.\nIt just seems very insightful that he just sees things much more clearly than others, and I, I just wonder what his production function is like.\nHow do you make a human like that, and how do you improve your own thinking to better model it?\nI mean, I think it is true that, I mean, one of OpenAI's big successes was betting on the scaling paradigm.\nIt is just kind of odd because, you know, they were not the biggest lab, you know, it was like difficult for them to scale back then.\nIt was much more common to do, like, a lot of small experiments.\nMore academic-style people were trying to figure out these, um, various, like, algorithmic improvements, and OpenAI bet pretty early on, like, large scale.\nWe had David Wan on, who I think was VP at the time of GPT1 and 2, and he talked about how the differences between Brain and OpenAI was basically the cause of Google's inability to come out with a scaled model.\nLike, just structurally, everyone had allocated compute, and you had to pull resources together to make bets, and you just couldn't.\nI think that's true that OpenAI was structured differently, and I think that really helped them.\nLike, OpenAI functions a lot like a startup, and other places tended to function more like universities or, or, you know, research labs as they traditionally existed.\nThe way that OpenAI operates more like as a startup with this mission of building AGI and super intelligence that helped them organize, collaborate, pull resources together, make hard choices about, like, how to allocate resources, and I think a lot of the other labs, like, have now been trying to adopt paradigms more like that, like, setups more like that.\nLet's talk about maybe the killer use case, at least in my mind, of these models, which is coding.\nYeah, you released Codex recently, but I would love to talk through the Noam Brown coding stack.\nWhat models you use, how you interact with them, Cursor, Windsurf.\nUh, lately, I've been using Windsurf and Codex, like, actually a lot of Codex.\nI've been having a lot of fun.\nLike, you just give it a task, and it just goes off and does it and comes back five minutes later with, like, a, you know, pull request.\nAnd is it core research task or, like, side stuff that you don't super care about?\nI wouldn't say it's like side stuff.\nI would say basically anything that I would normally try to code up, I try to do it with Codex first.\nWell, for you, it's free, but yeah, for everybody, it's free right now.\nAnd I think that's partly because it's the, it's the most effective way for me to do it, and also it's good for me to get experience working with this technology and then also, like, seeing the shortcomings of it.\nIt just helps me, like, better understand, like, okay, this is the limits of these models and, like, what we need to push on next.\nHave you felt the AGI?\nI felt the AGI multiple times.\nYes.\nLike, like, um, how should people push Codex in ways that you've, you've done, and you know, I think you, you see it before others, cuz obviously you were closer to it.\nI think anybody can use Codex and feel the AGI.\nIt's kind of funny how, like, you feel the AGI, and then you get used to it very quickly, you know?\nSo...\nSo it's really like dissatisfied with, like, where it's lacking.\nYeah, I know.\nAnd you know, it's, it's magical one day.\nI was actually looking back at the old uh Sora videos when they were announced, cuz, like, you remember when Sora came out, it was just like the biggest news ever.\nIt was just magical.\nYou look at that, and you're like, it's like, it's really here, like, this is AGI.\nBut you look at it now, and it's kind of like, oh, you know, the people, like, don't move, like, very organically, and it's like, there's, like, a lack of consistency in some ways, and you see all these flaws in it now that you just didn't really notice when it was first came out, and yeah, you get used to this technology very quickly, and but I think what's cool about it is that because it's developing so quickly, you get those feely AGI moments, like, every few months.\nSo something else is going to come out, and just, like, it's magical to you, and uh, and then you get used to it very quickly.\nYeah.\nWhat are your Windsurf pro tips now that you've immersed in it?\nI think one thing I'm surprised by is how few people, I mean, maybe your audience is going to be more comfortable with reasoning models and, like, use reasoning models more.\nBut I'm surprised at how many people don't even know that GPT-3 exists.\nLike, I've been using it day to day.\nIt's basically replaced Google search for me.\nLike, I just use it all the time.\nLike, and also for things like coding, like, I, I tend to just use the reasoning models.\nMy suggestion is, like, if people are not, have not tried the reasoning models yet, because, like, honestly, like, we do, like, people love them, people that use it love them.\nObviously, a lot more people use GPT-4 and just like the default and what on Chatbot and that kind of stuff.\nI think it's worth trying the reasoning models.\nLike, I think people would be surprised at what they can do.\nI use Windsurf daily, and they still haven't actually enabled it as, like, a default in Windsurf.\nLike, I always have to dig up, like, type in GPT-3, and then it, then it's like, oh yeah, that, that exists.\nIt's, it's uh, it's weird.\nI would say, like, my struggle with it has been that it takes so long to reason and actually break out of flow.\nI think that is true.\nYes.\nAnd and I think this, this is one of the advantages of Codex, that, like, okay, you can give it a task that's kind of self-contained, and, like, it can go off and do its thing and come back 10 minutes later.\nAnd I can see that if you're doing, if you're using this thing as, like, more like a, like a pair programmer kind of thing, then yeah, you want to use GPT-4.1 or something like that.\nWhat do you think are the most broken part of the development cycle with AI?\nLike, in my mind, it's like, um, pull request review.\nLike, for me, like, I use Codex all the time, and then I got all these pull requests, and it's kind of hard to, like, go through all of them.\nWhat other thing would you like people to build to make this CVN more scalable?\nI think it's really on us to build a lot more stuff.\nThese models are very limited in in some ways.\nI think I find it frustrating that, you know, you ask them to do something, and they spend 10 minutes doing it, and then you ask them to do something pretty similar, and then they go spend 10 minutes doing it, and, like, you know, it's, I, I think I describe them as, like, they're geniuses, but it's their first day on the job, you know, and that's, like, kind of annoying.\nLike, even the smartest person on earth, when they're, when it's their first day on the job, you know, they're not going to be, like, as useful as you would like them.\n\n\nto be.\nSo I think being able to get more experience and like act like somebody that's actually been on the job for like six months instead of what one day, I think would make them a lot more useful, but that's really on us to build that capability.\nDo you think a lot of it is like GPU constraint for you?\nLike if I think about Codex, why is it asking me to set up the environment myself when like the model, if I ask code three to like create an environment setup script for a repo, I'm sure it'll be able to do it, but today in the product I have to do it.\nSo I'm wondering in your mind, could these be a lot more if we just again put more test time compute on them, or do you think there's like a fundamental model capability limitation today that we still need a lot of like human harnesses around it?\nI think that we're in an awkward state right now where like progress is very fast and there's things that are like clearly we could do this and the models would be better.\nWe're going to get to it.\nIt's um you're just limited by how many hours there are in the day, you know?\nSo progress can only proceed so quickly.\nWe're trying to get to everything as fast as we can and and I think that the 03 is not where the technology will be in six months.\nI like that question overall in like there's a software development life cycle, not just generation of the code, like from issue to PR, basically is is like the the typical commentary of that, and then there's the windsurf side, which is insider your ID, like what else, right?\nPull request review is like something that people don't really, there are startups that built around it, it's not something that Codex does, and it could.\nAnd so like then there's like what else is there, you know, that is sort of rate limiting the amount of software you could be iterating on, it's an open question.\nI don't I don't I don't know if there's an answer.\nAnything else on on AI in general?\nLike where do you think this goes just in form factors or what will we be looking at this time next year in terms of how things are, how what models were able to do that they're not able to today?\nI don't think it's going to be limited to ASU, you know.\nI think I don't think it's going to be limited to software engineering.\nI think it's going to be able to do a lot of remote work kind of tasks.\nYeah.\nLike freelancer type Upwork.\nYeah.\nYeah.\nOr just like even things that are not necessarily software engineering.\nOkay.\nSo, the way that I think about it is like anybody that's doing a remote work kind of job, I think it's valuable to become familiar with the technology and like kind of get a sense of like what it can do, what it can't do, what it's good at, what it's not good at because I think the the breadth of things that it's going to be able to do is going to expand over time as well.\nI feel like virtual assistants might be the next thing after as then because they're the most easily like, you know, virtual assist, like hire someone in the Philippines, someone uh who who just look through your email and all that because that is entirely you can intercept all the inputs and all the outputs and train on that and maybe OpenAI just buys a virtual assistant company.\nYeah, I think what I'm looking forward to is that um for things like virtual assistants, the the models, like if they're aligned well, they could end up being like really preferable for that for that kind of work.\nYou know, if there's always this like principal agent problem where if you delegate a task to somebody, then like are they really aligned with like doing it as you would want it to be done and just as cheaply as quickly as they can.\nYeah.\nYeah.\nAnd so if you have an AI model that's like actually really aligned to you and your preferences, then that could end up doing a way better job than a human could.\nWell, not not it's doing a better job than a human could, but like it's doing a better job than a human would.\nThat word alignment, by the way, I think there's like an interesting overwriting or uh homorphism between safety alignment and instruction following alignment.\nAnd I wonder where they diverge.\nOkay, so I think where it diverges is like what do you want to align the models to like that?\nThat's I think a difficult question, you know, like you could say like you wanted to align it to the user.\nOkay.\nWell, what happens if the user wants to build a novel virus that's going to wipe out half of humanity?\nSafety alignment.\nSo there's a question of like I think alignment I think they're related, you know, and I think that the big question is like what are you aligning towards?\nYeah.\nThere's like humanity goals and then there's your personal goals and everything in between.\nMhm.\nSo that's kind of I guess the individual agent and you announced the you're leading the multi-agent team at OpenAI.\nI haven't really seen many announcements.\nMaybe I missed them on what you've been working on, but what can you share about interesting research directions or um anything from there?\nYeah, there's uh hasn't really been announcements on this.\nI think we're working on cool stuff and I think we'll get to announce some cool stuff uh at some point.\nI think the team in many ways is actually a misnomer because we're working on more more than just multi-agent.\nMulti-agent is one of the things we're working on.\nUm some other things we're working on is just like being able to scale up test time compute by by a ton.\nSo how you know we get these models thinking for 15 minutes now, how do we get them to think for hours, how do we get them to think for days even longer and be able to solve incredibly difficult problems, so that's one direction that we're pursuing.\nUh multi-agent is another direction and here I think there's a few different motivations.\nUh we're interested in like both the collaborative and the competitive aspect of multi-agent.\nI think the way that I describe it is people often say in AI circles that humans occupy this very narrow band of intelligence and AI are just going to like quickly catch up and then surpass like this band of intelligence.\nAnd I actually don't think that the band of int of human intelligence is that narrow.\nI think it's actually quite broad because if you compare anatomically identical humans from, you know, caveman times, they didn't get that far in terms of like, you know, what we would consider intelligence today, right?\nThey're not putting a man on the moon, you know, they're not like building semiconductors or nuclear reactors or anything like that.\nAnd and we have those today even though we as humans are not anatomically different.\nAnd so what's the difference?\nWell, I think the difference is that you have thousands of years, a lot of humans, billions of humans cooperating and competing with each other, building up civilization over time.\nAnd the technology that we're seeing is the product of this civilization.\nAnd I think similarly, the AIs that we have today are kind of like the cavemen of AI.\nAnd and I think that if you're able to um have them cooperate and compete with billions of AIs over a long period of time and build up a civilization, essentially the things that they would be able to produce and answer would be far beyond what is possible today with with the AIs that we have today.\nDo you see that being similar to maybe like Jim Fan's Voyager skill library idea re-saving these things or is it just the models then being retrained on this new knowledge because the humans then have it a lot of it in the brain as they grow?\nI think I'm going to be evasive here and say that like we're we're not going to yeah we're not going to we're until we have something to announce which I think that I think that we will in the not too distant future.\nI think I'm going to uh be a bit vague about like exactly what we're doing.\nBut I will say that the way that we are approaching multi-agent in the details and the way we're actually going about it is I think very different from how it's been done historically and how it's being done today by by other places.\nUm I've been in the multi-agent field for a long time.\nI've kind of felt like the multi-agent field has been a bit misguided in some ways and the things that the approaches that the field has taken and like the way it's been approached.\nAnd so I think we're trying to take a very principled approach to multi-agent.\nSorry, I got to ask like so you you can't talk about what you're doing, but you can say what's misguided.\nWhat's misguided?\nI think that a lot of the approaches that have been taken have been very heristic and haven't really been following like the bitter lesson approach to scaling and research.\nOkay, I think maybe this might be a good a good spot.\nSo obviously you've done a lot of amazing work in in poker and I think as the recent model got better I was talking to one of my friends who used to be a a hardcore poker grinder and I told them I was going to interview you and uh their question was at the table you can get a lot of information from a small sample size about how a person plays, but today GTO is like so prevalent that sometimes people forget that you can play exploitatively, what do you think is the state as you think about multi-agent and kind of like competition, is it always going to be trying to find the optimal thing or is a lot of it trying to think more in the moment like how to exploit somebody?\nI'm guessing your audience is probably not super familiar with poker terminology.\nSo, I'll just like explain this a bit.\nUh, a lot of people think that poker is just like a luck game and that's not true.\nIt's actually there's a lot of strategy in poker.\nSo, you can win consistently in poker if you're playing the right strategy.\nSo, there's different approaches to poker.\nOne is game theory optimal.\nThis is like you're playing an unbeatable strategy and expectation.\nLike you're just unexploitable.\nIt's kind of like in rock paper scissors.\nYou can be unbeatable in rock paper scissors if you just randomly choose between rock paper and scissors with equal probability because no matter what the other guy does, you know, they're not going to be able to exploit you or you're going to win.\nYou're going to like not lose an expectation.\nNow, a lot of people hear that and they think like, well, that also means that you're not going to win an expectation because you're just playing totally randomly.\nBut in poker, if you play the equilibrium strategy, it's actually really difficult for the opponents to figure out how to tie you, and they're going to end up making mistakes that will lead you to win over the long run.\nIt might not be a massive win, but it is going to be a win.\nIf you play enough hands for a long enough period of time, you're you're going to win in expectation.\nNow, there's also exploitative poker, and the idea here is that you're trying to spot weaknesses in how the opponent plays.\nYou know, maybe they're maybe they're not bluffing enough or maybe they fold too easily to a bluff.\nAnd so you start adapting from the game theory optimal balance strategy of like you bluff sometimes, you you don't bluff sometimes to then playing a very unbalanced strategy that's like, oh, I'm just going to like bluff a ton against this person because they always fold whenever I bluff.\nNow, the key is that there's a trade-off here because if you're taking this exploitative approach, then you're opening yourself up to exploitation as well.\nAnd so you have to choose this balance between playing a defensive game theory optimal policy that guarantees you're not going to lose but might not make you as much money as you potentially could versus playing an exploitative strategy that could be much more profitable but also it creates weaknesses that the opponents could take advantage of and trick you.\nAnd there's no way to perfectly balance the two.\nIt's kind of like in rock paper scissors if you notice somebody is like playing paper for five times in a row, you might think like oh they're they they have a weakness in their strategy.\nI should just be throwing scissors and I'm going to take advantage of them.\nAnd so on the sixth time you throw scissors, but actually that's the time when they throw rock, you know, so and you never really know.\nSo you always have this trade-off.\nThe poker AIs that have been extremely successful and like my background is like I worked on AI for poker for several years during grad school and made the first superhuman no limit poker AIs, the approach that we took was this game theory optimal approach where the AIs would play this unbeatable strategy and they would play against the world's best and beat them.\nNow that also means they they beat the world's worst.\nLike they would just beat anybody.\nBut if they were up against a weak opponent, they might not beat them as severely as a human expert might because the human expert would know how to adapt from the game optimal policy to be able to exploit these weak players.\nAnd so there's this kind of unanswered question of like how do you make an exploitative poker AI?\nAnd a lot of people had pursued this research direction.\nI like dabbled in it a little bit during grad school.\nAnd I think fundamentally it just comes down to AI not being as sample efficient as humans.\nYou know, we discussed earlier, if a human's playing poker, they're able to get a really good sense of of the strengths and weaknesses of a player within a dozen hands.\nIt's like honestly really impressive.\nAnd back when we were working on AI for poker in like the, you know, mid2010s, you'd have to these AIs would have to play like 10,000 hands of poker to like get a good profile of like who this player is, like how they're playing, where their weaknesses are.\nNow, I think with more recent technology, that has come down.\nUm, but still the sample efficiency has been a big challenge.\nNow, what's interesting is that after working on poker, I worked on diplomacy.\nI think we talked about this earlier.\nAnd diplomacy is this, you know, it's a seven player negotiation game.\nAnd when we started working on it, I took a very game theory approach to the problem.\nI I felt like, okay, we're it's kind of like poker.\nYou have to compute this game theory optimal policy, and you just play this, you're going to not lose an expectation.\nYou're going to win in practice.\nBut that actually doesn't work in diplomacy.\nAnd it doesn't work.\nAgain, for question of like how how much of a rabbit hole do we want to go down on this, but like basically when you're playing like the zero sum games like like poker, game theory optimal works really well.\nWhen you're playing a game like Diplomacy where there's like you need to collaborate and compete and you need there's there's room for collaboration, then game theory optimal actually doesn't work that well and you have to understand the players.\n\n\nand adapt to them much better.\nSo, this ends up being very similar to the problem in poker of how do you adapt to your opponents.\nIn poker, it's about adapting to their weaknesses and take advantage of that.\nIn diplomacy, it's about adapting to their play styles.\nIt's kind of like if you're at a table and everybody's speaking French, you don't want to just keep talking in English.\nYou want to adapt to them and speak in French as well.\nThat's the realization that I have with diplomacy that we need to shift away from this game theory optimal paradigm towards modeling the other players, understanding who they are, and then responding accordingly.\nAnd so in many ways the techniques that we developed in diplomacy are exploitative, like they're not exploitative.\nThey're really, you know, just adapting to the opponents, to the other players at the table.\nUm, but I think the same techniques could be used in AI for poker to make exploitative poker AIs.\nIf I didn't get, you know, AGI pill by the incredible progress that we were seeing with language models and like shifting my whole research agenda to focusing on like general reasoning, probably what I would have worked on next was making these like exploitative poker AI.\nIt would be a really fun research direction to go down.\nI think it's still there for anybody that wants to do it.\nAnd I think the key would be taking the techniques that we used in diplomacy and applying them to things like poker.\nI think to me the core piece is when you play online you have a HUD which tells you, you know, all these stats about the other player, like you know how much they participate pre flop, blah blah blah, and to me it's like a lot of these models from my understanding are not really leveraging the behavior of the other players at the table, they're just kind of looking at the board state and kind of working from there.\nThat's correct.\nThe way the poker AIs work today, they're just kind of like sticking to their precomputed GTO strategy and they're not adapting to the other players at the table.\nAnd like you can do various like kind of hacky things to get them to adapt, but you know, they're not very principled, they're not, they don't work super well.\nOkay, any grad students listening, uh, if you want to work on that, I, I think that is a very, very reasonable research direction that'll at least uh, get in front of you and you know, get some attention at least.\nThe other thing that this conversation brings up for me is, yeah, well, one of the hypothesis for like what is the next step after testime compute is world models is world modeling.\nImportance or worthwhile research direction, like Yan Lakun has been talking about this non-stop, but like basically no LLMs have like they have internal world models, but like not explicitly a world model.\nI think it's pretty clear that as these models get bigger, they have a world model and that world model becomes better uh with scale, so they are implicitly developing a world model and I don't think it's something that you need to explicitly model.\nUm, I could be wrong about that.\nYou know, there's when when dealing with people or multi-agents, it might be because you have entities that are not the world and you're resolving hypotheses of what which of the many types of entities you could be dealing with.\nYou know, there was this like long debate in the multi-agent AI community for a long time about, and it's still going on, about whether you need to explicitly model other agents, like other people, or if they can be implicitly modeled as part of the environment.\nFor a long time, I was like on the, on the took the perspective of like, of course you have to like explicitly model these other agents because they're, they're behaving differently from the environment.\nLike, they, they take actions, they're unpredictable, you know, they, they have agency.\nBut I think I've actually shifted over time to thinking that like, actually if these models become smart enough, they develop things like theory of mind.\nThey develop an understanding that there are other agents that like can take actions and and have motives and all this stuff.\nAnd these models just develop that implicitly with scale and and more capable behavior broadly.\nSo that's the perspective I take these days.\nSo like what I just said was an example of a heuristic that is not bitter lesson filled and you just it just goes away.\nYeah.\nIt's really all come back to the bitter lesson.\nGot to cite them every every podcast.\nSo, one of the interesting findings and most consistent findings, you know, I think you were at ICLR and uh one of the hit talks there was about open-endedness and this guy Tim who gave that talk has been doing a lot bunch of research about multi-agent systems too.\nOne of the most consistent findings is always that um it's better for AIs to selfplay and improve competitively as opposed to sort of humans training and guiding them.\nAnd you find that with like you know Alpha Zero and R10 whatever that was.\nDo you think this will hold for multi-agents, like selfplay to improve better than humans?\nYeah.\nSo okay, so this is a great question and I, I think this is like worth expanding on.\nSo I think a lot of people today see selfplay as like the next step and maybe the last step that we need for super intelligence.\nAnd I think if you're following, you know, you look at something like Alpha Go and Alpha Zero, we seem to be following a very similar trend, right?\nLike the first step in Alpha Go was you do large scale pre-training.\nIn that case, it was on human Go games.\nWith LLMs, it's pre-training on, you know, tons of like internet data and that gets you a strong model, but it doesn't get you uh you know an extremely strong model, you know, it doesn't get you superhuman model.\nAnd then the next step in the Alpha Go paradigm is you do large scale test time compute or like large scale inference compute and in that case with um MCTS and now we have like reasoning models that also do like this large scale inference compute and again that like boosts the capabilities a ton.\nFinally with Alpha Go and Alpha Zero you have selfplay where the model plays against itself, learns from those games, gets better and better and better and just like goes from something that's like around human level performance to like way beyond human capability.\nIt's like these Go policies now are so strong that it's just like incomprehensible.\nLike what they're doing is incomprehensible to humans.\nSame thing with Chess.\nAnd we don't have that right now with language models.\nAnd so it's like it's really tempting to look at that and say like, oh well, we just need these like AI models to now interact with each other and learn from each other and they're just going to like get to super intelligence.\nThe challenge, and I kind of mentioned this like a little bit when I was talking about diplomacy.\nThe challenge is that Go is this two-player zero sum game.\nAnd two-player zerosome games have this very nice property where when you do selfplay, you are converging to a minimax equilibrium.\nAnd I, I guess I should take a step back and say like in two-player zero sum games, two player zero games are are chess, Go, even two-player poker, all two player zero sum.\nWhat you typically want is what's called a minmax equilibrium.\nThis is that that GTO policy, this policy that you play where you're guaranteeing that you're not going to lose to any opponent in expectation.\nI think in chess and Go that's like pretty clearly what you want.\nInterestingly, in when you look at poker, it's not as obvious.\nIn a two-player zero sum version of poker, you could play the GTO minmax policy and that guarantees that you won't lose to any opponent on Earth.\nBut again, I mentioned there's you're not going to to beat a weak player.\nYou're not going to make as much money off of them as you could if you instead played an exploitative policy.\nSo, there's this question of like, what do you want?\nDo you want to make as much money as possible, or do you want to guarantee that you're not going to lose to any human alive?\nWhat all the bots have decided is like, well, what all the like AI developers in these games have decided is like, well, we're going to choose the minmax policy.\nAnd conveniently, that's exactly what selfplay converges to.\nIf you have these AIs play against each other, learn from their mistakes, they converge over time to this minmax policy, guaranteed.\nBut once you go outside a two-player zero sum games, like in the case of diplomacy, that's actually not a useful policy anymore.\nYou don't want to just like have this very defensive policy and you're going to end up with really weird behavior if you start doing the same kind of self-play in things like math.\nSo for example, what does it mean to do self-play in math?\nYou could fall into this trap of like, well, I just want one model to pose really difficult questions and the other model to solve those questions.\nYou know, that's like a two-player zero sum game.\nThe problem is that like, well, you could just like pose really difficult questions that are not interesting.\nYou know, you just like get ask it to do like 30-digit multiplication.\nIt's a very difficult problem for the AI models.\nIs that really making progress in the dimension that we want?\nLike not really.\nSo selfplay outside of these two players here some games becomes like a much more difficult nuanced question.\nSo I think and then Tim, Tim kind of like basically said something similar in his talk that there's a lot of challenges in really deciding what you're optimizing for when you start to talk about selfplay outside of two players here some games.\nMy point is that like this is where the AlphaGo go analogy breaks down and not necessarily breaks down, but like it's not going to be as easy as selfplay was in AlphaGo.\nWhat is the objective function then for that?\nWhat is the new objective function?\nYeah, it's a, it's a good, it's a good question.\nYeah.\nAnd I think that that's something that um, you know, a lot of people are thinking about.\nYeah.\nUm, I'm sure you are.\nOne of the last podcasts that you did, you mentioned that you were very impressed by Sora.\nYou don't, you don't work directly on Sora, but obviously it's part of OpenAI.\nI think the the most recent uh new updates or in that sort of generative media space is auto regressive image gen.\nIs that interesting or surprising in any way that you want to comment about?\nI don't work on image gen, so I my ability to comment on this is kind of limited, but I will say like I, I love it.\nLike I think it's super impressive.\nIt's like one of those things where, you know, you work on these reasoning models and you think like, wow, we're going to like be able to do all sorts of crazy stuff like advanced science and um, you know, solve agentic tasks and and software engineering.\nAnd then there's like this whole other like dimension of progress where you're like, oh, you're able to like make images and videos now and it's like so much fun.\nAnd that's getting a lot more the attention to be honest, especially in the general public.\nAnd it's probably driving a lot more of the like, you know, subscription plans for CHBT, which is is great, but I think it's just kind of funny that like yeah, we're also I promise we're also working on super intelligence, but you can make everything gibbly.\nUh, I think the the delta for me was um, I was actually harboring this thesis that diffusion was over because of auto regressive imaging.\nLike there were rumors about this end of last year and obviously now it's come now it's come out.\nThen Gemini comes out with text diffusion and like diffusion is so bad and like this is two directions and it's very relevant for inference of auto regressive versus um diffusion.\nDo we have both?\nDoes one win?\nThe beauty of research is like you know you got to pursue different different directions and it's not it's not always going to be clear like what is um you know the promising path like and I think it's great that people are looking into different directions and trying different things.\nI, I think that there's a lot of value in that exploration and I think we all benefit from seeing what works.\nAny potential in diffusion reasoning, let's say your channel, I can answer that.\nOkay.\nSo you did a masters in robotics too.\nWould love to get your thoughts on one, you know, Open AI kind of started with the pen spinning trick and like the robotic arm they wanted to build.\nIs it right to work on the humanoid likes?\nDo you think that's kind of like the wrong embodiment of AI outside of the usual, you know, how long until we get robots, blah blah blah.\nIs there something that you think is like fundamentally not being explored right now that people should really be doing in robotics?\nI did a masters in robotics years ago and my takeaway from that experience, first of all, I didn't actually work with robots that much.\nI was like technically in a robotics program.\nI played around with some Lego robots my my first week of the program, but then honestly I just like pretty quickly shifted just working on AI for poker and um was kind of nominally in the robotics masters.\nBut my takeaway from like interacting with all these roboticists and seeing their research was that I did not want to work on robots because the research cycle is so much slower and so much more painful when you're dealing with like physical hardware.\nLike software goes so much more quickly and I think that's why we're seeing so much progress with language models and like all these like virtual co-worker kind of tasks, but haven't seen as much progress in robotics that like physical hardware just is much more painful to iterate on.\nThe question of humanoids, I don't have very strong opinions here because this isn't what I'm working on, but I think there's a lot of value in nonhumanoid robotics as well.\nI think drones are a perfect example where like there's clearly a lot of value in.\nThat is that a humanoid?\nNo.\nBut in many ways that's great, you know, like you don't want a humanoid for for that kind of technology.\nI think weekly um I think that nonhumanoids provide a lot of value.\nI was reading Richard Hamming's The Art of Doing Science and Engineering.\nAnd he talks about how when you have a new technological shift, people try and take the old workloads and like replicate them just in the new technology versus you actually have to change the way you do it.\nAnd you know, when I see this video of like you know your humanoid in the house, it's like, well, the human shape is kind of has a lot of limitations that could actually be improved that I think people what's familiar, you know, it's like would you put a robot with like 10 arms and like you know five legs in your house or would that be yuri at night when you get up and you see that thing walking around and is that why we use humanoids.\nSo I, I think to me there's almost like this local maximum of like\n\n\nYou know, we got to make it look like a human, but I think, like, what's like the, the best shape, uh, in house would be? I'm terrible at product design, so I, I am not the person to ask on this. I think there is a question of, like, is it better to make humanoids because they're more familiar to us, or is it worse to make humanoids because they're more similar to us but not quite identical? Like, I, I don't know which one I would actually find creepier. Yeah. Yeah. The thing that got me humanoid pilled a little bit was just the argument that most of the world is made for humans anyway. So if you want to replace human labor, you have to make a humanoid. I don't know if that's convincing. Again, I don't have very strong opinions in this field because, like, I don't work in it. Um, I was like weekly in favor of humanoids. And I think what really persuaded me to be weekly in favor of, like, non-humanoids was listening to, um, the physical intelligence CEO and, like, some of his pitches about, like, why they're not pursuing, why they're pursuing, like, non-humanoid robotics. Okay. And conveniently, their office is actually, like, very close to here. So if you wanted to, they're speaking at the conference I'm running. Okay. You know, I'd say, like, listen to his pitch and maybe he can convince you that is the way to go. Awesome. The other one I would refer people to is Jim Fan. Recently did a talk on the physical tearing test, which, uh, which he did at the Sequoia conference, which, um, was very, very good. Um, he's such a great educator and explainer of things. Um, it's very hard, especially in that field. Um, cool, we're done asking you about things that you don't work on.\n\nSo these are just more rapid fires to, to sort of explore some of your boundaries and get, get some quick hits. How do you or top industry labs keep on top of research? Like, what are your tools and practices? Uh, it's, it's really hard. I think that a lot of people have this perception that, like, academic research is irrelevant, and that's actually not the case. I think that we do, we look at academic research. I, I think one of the, um, challenges is, like, a lot of academic research shows promise in their papers, but then actually doesn't work at scale or even doesn't replicate. I think if we find interesting papers, like, we're going to try to reproduce that in-house and see if it, like, still holds up and then also does it scale well. But that is, like, a big source of inspiration for us. Whatever hits archive, literally, you do the same as the rest of us, or do you have, like, a special process? Especially if I get recommendations, like, we have an internal channel where people will post interesting papers, and, like, I think that's a good source of, like, okay, well, this person that is more familiar with this area thinks that this paper is interesting, so therefore I should read it. Yeah. Um, and similarly, like, I'll keep track of things that are happening in my space that I think are interesting, and, like, if I think it's really interesting, maybe I'll share it. For me, it's like WhatsApp and Signal group chats with researchers, and that's it. Yeah. I think it is, like, I mean, a lot of people look at things like Twitter, and I think it's really unfortunate that we've reached this point where things need to get a lot of attention on social media for it to be paid attention to. Um, that's what the grad students are trained. They're taking classes to do this. I, I do recommend to, like, you know, I've worked with grad students, work with fewer now because we don't publish as much, but when I was at Fair publishing papers, like, I would tell the grad students I was working with that, like, you need to post it on Twitter and you need to, and we go over, like, the Twitter thread about, like, how to present the work and everything, and, um, there's a real art to it, and it does matter, and it's kind of the sad truth. I know when you were doing the ACPC, like, the AI poker competition, you mentioned that people were not doing search because they were limited to, like, two CPUs at inference. Do you see similar things today that are, like, keeping interesting research from being done? That might be, it's not as popular. It doesn't get you into the top conferences, like, uh, are there some environmental limiters? Absolutely. And I, I think one example is for benchmarks that you look at things like humanity's last exam, like, you have these incredibly difficult problems, but then are still very easily gradable, and I think that actually limits the scope of what you can evaluate these models on. If you, if you stick to that paradigm, it's very convenient because, you know, it's very easy to, like, then score the models. But actually, a lot of the things that we want to, you know, evaluate these models on are kind of, like, more fuzzy tasks that are not multiple choice questions, and making benchmarks for that, for those kinds of things is so much harder and probably also, like, a lot more expensive to evaluate. But I think that those are really valuable things to work on, and that would fit the sentiment. GBD 4.5 is, like, a high taste model in a way.\n\nThere's kind of, like, all these, like, nonmeasurable things about a model that are really good that maybe people are not, well, I think there are things that are measurable, but they're just, like, much more difficult to measure, and I think that a lot of benchmarks have kind of stuck to this paradigm of posing really difficult problems that are really easy to measure. So let's say the pre-training scaling paradigm took about five years from, like, discovery of GPT to scaling it up to GPT4, and then we give you, we give test time compute five years as well. So, um, if test time compute a wall by 2030, what would be the probable cause? It's very similar to pre-training. We're like, you can push pre-training a lot further, and it just becomes more expensive with each iteration. I think we're going to see something similar with test time compute. We're like, okay, we're going to get them thinking instead of three minutes, they're for three hours, and then three days, and then three weeks. Um, you run out of human life. Well, so there's two, there's two, there's two concerns. One is that it becomes much more expensive to get the models to, like, think for that long, or, like, scale up test time compute. Like, as you scale up test time compute, you're spending more on test time compute, which means that, like, there's a limit to how much you could spend. That's one potential ceiling. Now, obviously, well, not obviously, but, like, I should say that we're also becoming more efficient. These models are becoming more efficient in the way they're thinking is they're able to do more with the same amount of test time compute. And I think that's a very underappreciated point that it's not just that we're getting these models to think for longer. In fact, if you look at 03, it's thinking for longer than 01 preview for some questions, but it's not, like, a radical difference, but it's way better. Why? Because it's just, like, becoming better at thinking. Anyway, yeah, these models, um, you're going to scale up test on comput, you can only scale it up so much. Like, that becomes a soft barrier in the same way that pre-training, it's becoming more and more expensive to train better and better pre-trained models or bigger pre-trained models. The second point is that, like, as you have these models think for longer, you kind of get bottlenecked by walk time. Like, if you want to iterate on experiments, it is really easy to iterate on experiments when these models would respond instantly. It's actually much harder when they take three hours to respond, and what happens when they have three weeks? It takes you at least three weeks to do those evaluations and to then iterate on that, and, and a lot of this, you can paralyze experiments to some extent, but a lot of it, you have to run the experiment, complete it, and then see the results in order to decide on the next set of experiments. I think this is actually the strongest case for, for long timelines that the models, because they just have to, like, do so much in serial time, we can only iterate so quickly. How would you overcome that? Well, it's, it's a challenge, and I think it depends on the domain. So drug discovery, I think, is one domain where this could be a real bottleneck. I mean, if you want to see if something, like, extends human life, it's going to take you a long time to figure out if, like, this new drug that you developed, like, actually extends human life and doesn't have, like, terrible side effects along the way. Side note, do we not have perfect models of human chemistry and biology by now? Well, so this, this is, I think, the thing, and again, I want to be cautious here because I'm not actually a biologist or chemist. Like, I don't, I know very little about about these fields. I last time I took a biology class was 10th grade in high school. I don't think that there's a perfect, uh, simulator of human biology right now. And I think that that's something that could potentially help address this problem. That's, like, the number one thing that we should all work on. Well, that's one of the things that we're hoping that these racing models will help us with. Yeah. How would you classify mid-training versus post-training today? It's, it's such, the all these definitions are so fuzzy. So I, I don't have, I don't have a great answer there. It's a question people have, and you're, and, like, OpenAI is, like, now explicitly hiring for mid-training, and everyone is, like, what the hell is mid-training? I think mid-training is between pre-training and post- training. It's, like, it's, like, uh, it's not, it's not post- training. It's not pre-training. It's, like, adding more to the models, but, like, after pre-training, like, I don't know, interesting ways. Yeah. Okay. All right. Well, you know, I'm trying to get some clarity.\n\nIs the pre-trained model now basically, like, a just an artifact that then spawns other models, and it's almost like the core pre-training model is never really exposed anymore, and it's the mid-training, the new pre-training, and then there's the post-training once you have the models branched out? You never interact with an actual just, like, raw pre-trained model. Like, if you're going to interact with the model, it's going to go through mid-training and post-training. So, um, so you're seeing the final product. Well, you don't let us do it, but, you know, we used to. Well, yeah. I mean, I guess if you, you know, there's open source models where you can just, like, interact with the raw pre-train model. Um, but for OpenAI models, like, they go through a mid-training step, then they go through a post- training step, and then, and then they're released. And they're a lot more useful. Like, frankly, if you interacted with a only pre-trained model, it would be super difficult to work with, and it would, Yeah. It would seem kind of dumb. Yeah. But it'd be, it'd be useful in weird ways, you know, because there's a mode collapse when you, when you post straightforward for, like, chat. Yeah. In some ways, you want that mode collapse, like, you want, you want that collapse of, like, to be useful. I, I get it. We're interviewing Greg Brockman next. Uh, you've talked to him a lot. What would you ask him? What would I ask Greg? I mean, I mean, I get to ask Greg all the time. What, what should you ask Greg? Like, to, to evoke an interesting response that, like, uh, not he doesn't get asked enough about, but, you know, like, this is something that he's passionate about, or you just want his thoughts. I think in general, it's worth asking where this goes, you know, like, what does the world actually look like in five years? What does the world look like in 10 years? What does that distribution of outcomes look like? And what could the world or individuals do to help steer things towards, like, the good outcomes instead of the negative outcomes? Okay, like an alignment question. I think people get very focused on what's going to happen in, like, one or two years. And I think it's also worth spending some time thinking about, like, well, what happens in five or 10 years, and what, what does that world look like? Um, I mean, he doesn't have a crystal ball, like, but he, he certainly has, he certainly has thoughts. Yeah. So I think that's worth exploring. Yeah. Okay. What are games that you recommend to people? Uh, especially socially. Uh, what are games that I recommend to people? Uh, I've been playing a lot of this game called Blood on the Clock Tower lately. Um, what is it? It's kind of like Mafia or Werewolf. It's become very popular in San Francisco is, uh, Oh, that's the one who played in your house. Yeah. Okay. Got it. Got it. It's kind of funny because, like, I was talking to a couple people now that have told me that it used to be that poker was the, like, way that, like, the VCs and tech founders and stuff would socialize with each other. And actually now it's shifting more towards Blood on the Clock Tower. Like, that's the, that the thing that people use to, like, um, you know, connect in the Bay Area. And I was actually told that a startup held a recruiting event that was a Blood on the Clock Tower game. Wow. Yeah. So, uh, I guess it's, like, it's really catching on, but it's a fun game, and I guess you lose less money playing it than you do playing poker. So, it's, like, better for people that are not very good at these things. Uh, I, I think it's kind of, like, a weird recruiting event, but it's certainly a fun game. What qualities make a winner here that is interesting to hire for? That's the thing is, like, okay, I guess you get ability to lie, deception, and, like, picking up on deception, like, is that the best employee? I don't know.\n\nSo my slight final pet topic is Magic the Gathering. So you have, we talked about some of these games, Chess, and they have perfect information. Then you have Poker, which is imperfect information in a pretty limited universe. You only have a 52 card deck. And then you have these other games that have imperfect information, like a huge pool of possible options. Do you have any idea of, like, how much harder that is? Like, how does the difficulty of this problem scale? I love that you asked that because I have this, like, huge store of knowledge on AI frame information games, like, this is my, my area of research for so long, and I know all these things, but I don't get to talk about it very often. We've made superhuman poker AIs for no limit Texas Hold'em. One of the interesting things about that is that, like, the amount of hidden information is actually pretty limited because you have two hidden cards when you're playing Texas Hold'em. And so the number of possible states that you could be in is 1,326 when you're playing heads up at least. And you know that's multiplied by the number of other players that there are at the table, but it's still, like, not\n\n\nA massive number.\n\nAnd so the way these AI models work is they enumerate all the different states that you could be in.\nSo, if you're playing like six-handed Poker, there's five other players.\nFive times 1,326.\nThat's the number of states that you be.\nAnd then you assign a probability to each one.\nAnd then you feed those probabilities into your neural net.\nAnd you get actions back for each of those states.\nThe problem is that as you scale the number of hidden possibilities, like the number of state of possible states you could be in, that approach breaks down and there's still this very interesting unanswered question of what do you do when the number of hidden states becomes extremely large.\nMhm.\nYou know, so if you go to Omaha Poker where you have four hidden cards, there are things you could do that's kind of like that are kind of heristic that you could do to reduce the number of states, but actually it's still a very difficult question.\nAnd then if you go to a game like Stratego where you have 40 pieces, so there's like close to 40 factorial different states you could be in, then all these like existing approaches that we used for Poker kind of break down and you need different approaches and there's a lot of active research going on about like how do how do you cope with that?\nSo for something like MTG, the techniques that we used in Poker would not out of the box work.\nAnd it's still an interesting research question of like what do you do?\nNow I should say this becomes a problem when you're doing the kinds of search techniques that we used in Poker.\nIf you're just doing model free RL, it's not a problem.\nAnd my guess is that if somebody put in the effort, they could probably make a superhuman bot for MTG.\nNow, yeah, there's still some unanswered research questions in that space.\nNow, are they the most important unanswered research questions?\nLike, I'm inclined to say no.\nI think there's like the problem is that like the techniques that we used in Poker to do this kind of search stuff were pretty limited.\nAnd like if you expand if you expand those techniques, maybe you get them to work on things like strategic the gathering, but they're still going to be limited.\nThey're not going to get you like superhuman and code forces with language models.\nSo, I think it's more valuable to just focus on the very general reasoning techniques.\nAnd one day as we improve those, I think we'll have a model that just out of the box one day plays Magic the Gathering at a superhuman level.\nAnd I think that's the more important and more impressive research direction.\nCool.\nAmazing.\nYeah.\nThanks very much for coming on, N.\nYeah.\nThanks for your time.\nYeah.\nThanks.\nThanks for having me.\n[Music]\n",
  "dumpedAt": "2025-07-21T18:43:26.249Z"
}