{
  "episodeId": "Mp0gxUunnWI",
  "channelSlug": "@latentspacepod",
  "title": "⚡️Ranking Agentic LLMs — Pratik Bhavsar, Galileo",
  "publishedAt": "2025-07-14T19:07:37.000Z",
  "rawLines": [
    {
      "lang": "en",
      "text": "Hey everyone, welcome back to another",
      "offset": 3.12,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "laten space learning pod. This is",
      "offset": 4.88,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "Allesio, partner and CTO at Desible and",
      "offset": 6.56,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "I'm joined by my co-host Wix, founder of",
      "offset": 8.72,
      "duration": 2.56
    },
    {
      "lang": "en",
      "text": "Small AI.",
      "offset": 10.48,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "Hello. Hello. And uh today we are",
      "offset": 11.28,
      "duration": 5.919
    },
    {
      "lang": "en",
      "text": "excited to talk about agent evals and",
      "offset": 14.32,
      "duration": 7.119
    },
    {
      "lang": "en",
      "text": "leaderboards. Uh Pratik uh",
      "offset": 17.199,
      "duration": 5.681
    },
    {
      "lang": "en",
      "text": "I I think I'm missing out the intro, but",
      "offset": 21.439,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "Pratik Bravsar uh you're uh from Galileo",
      "offset": 22.88,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "Labs and you worked on agent",
      "offset": 25.68,
      "duration": 3.999
    },
    {
      "lang": "en",
      "text": "leaderboards this year.",
      "offset": 27.279,
      "duration": 5.921
    },
    {
      "lang": "en",
      "text": "Hey folks, uh it's so cool to be here",
      "offset": 29.679,
      "duration": 5.841
    },
    {
      "lang": "en",
      "text": "and I'm excited to chat with you on",
      "offset": 33.2,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "everything about agents today.",
      "offset": 35.52,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "Yeah. Awesome. Um yeah, so I think you",
      "offset": 38.079,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "know I I first got uh notice of this",
      "offset": 40.239,
      "duration": 7.121
    },
    {
      "lang": "en",
      "text": "thing in our discord when Erin Staples",
      "offset": 44.079,
      "duration": 5.441
    },
    {
      "lang": "en",
      "text": "uh who I think who works with you guys",
      "offset": 47.36,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "uh posted it and said that she was also",
      "offset": 49.52,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "contributing to it. I think the broad",
      "offset": 51.6,
      "duration": 7.56
    },
    {
      "lang": "en",
      "text": "thing is there's a big shift in eval",
      "offset": 53.68,
      "duration": 5.48
    },
    {
      "lang": "en",
      "text": "maybe which quickly gets saturated. I",
      "offset": 60.8,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "think Ella Marina also had some",
      "offset": 63.84,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "controversies",
      "offset": 66,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "and I think more generally people want",
      "offset": 67.52,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "agent evals anyway where they are",
      "offset": 69.92,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "evaluating the ability of uh you know",
      "offset": 72.159,
      "duration": 6.481
    },
    {
      "lang": "en",
      "text": "models to do real tasks and uh instead",
      "offset": 75.92,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "of like testing knowledge they're",
      "offset": 78.64,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "actually you know doing doing tool",
      "offset": 80.159,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "calling and um utilizing context very",
      "offset": 81.84,
      "duration": 5.279
    },
    {
      "lang": "en",
      "text": "well that kind of stuff. So um you know",
      "offset": 85.04,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "I think one of those things where this",
      "offset": 87.119,
      "duration": 2.401
    },
    {
      "lang": "en",
      "text": "is one of those things where I think",
      "offset": 88.56,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "there's a lot of fresh uh area and",
      "offset": 89.52,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "potential for progress if you can do a",
      "offset": 92.88,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "credible eval. And the other thing I",
      "offset": 95.52,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "like to see is an EVEL that has a cost",
      "offset": 97.36,
      "duration": 5.439
    },
    {
      "lang": "en",
      "text": "awareness component. Uh because I think",
      "offset": 100,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "uh you know people don't really uh stay",
      "offset": 102.799,
      "duration": 6.401
    },
    {
      "lang": "en",
      "text": "very honest with like oh yeah like we",
      "offset": 105.6,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "spent a lot more tokens or we spent a",
      "offset": 109.2,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "lot more money on this agents and we did",
      "offset": 111.36,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "better but it's not really clear like if",
      "offset": 113.52,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "the other comparison could also just do",
      "offset": 116.079,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "the same thing like just think harder",
      "offset": 118.32,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "and also have the same performance.",
      "offset": 120.399,
      "duration": 6.32
    },
    {
      "lang": "en",
      "text": "100%. Yeah. So we have been hearing from",
      "offset": 123.68,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "our customers that they are now moving",
      "offset": 126.719,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "towards agents and it become very clear",
      "offset": 128.879,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "to us that last year we did rag",
      "offset": 130.959,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "evaluation because that's what we're",
      "offset": 133.36,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "discussing more with the customers but",
      "offset": 135.28,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "uh we quickly understood that this is",
      "offset": 138,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "going to be a year of agents and what we",
      "offset": 140.319,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "have understood from our previous",
      "offset": 143.84,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "experiments is that it's not necessary",
      "offset": 145.2,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "that what you see as top models in let's",
      "offset": 147.36,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "say al marina or other specific",
      "offset": 150.08,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "evaluation or general evaluations",
      "offset": 152.48,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "they might not be also the same ranking",
      "offset": 154.64,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "for other task like agentic task. So",
      "offset": 157.76,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "that was our goal uh very fundamentally",
      "offset": 160.56,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "speaking that we want to get very",
      "offset": 163.2,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "specific insights apart from just",
      "offset": 165.2,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "performance and cost was the other thing",
      "offset": 166.879,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "that comes up a lot with enterprise",
      "offset": 168.959,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "customers and that was the second focus",
      "offset": 170.8,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "essentially and then there's SLM versus",
      "offset": 173.76,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "LLM kind of debate are small models also",
      "offset": 175.92,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "catching up and there is the big one",
      "offset": 178.959,
      "duration": 3.041
    },
    {
      "lang": "en",
      "text": "open source versus private so those were",
      "offset": 180.4,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "the dimensions we were interested in",
      "offset": 182,
      "duration": 5.599
    },
    {
      "lang": "en",
      "text": "um and in the email itself um the big",
      "offset": 184.959,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "thing that you highlight is obviously",
      "offset": 187.599,
      "duration": 2.881
    },
    {
      "lang": "en",
      "text": "tool selection",
      "offset": 189.2,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "and tool use and then single versus",
      "offset": 190.48,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "multi-turn interaction maybe for for",
      "offset": 192.72,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "people that are just learning about the",
      "offset": 195.04,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "leaderboard now just give a high level",
      "offset": 196.64,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "overview of what are like the key things",
      "offset": 198.08,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "that you test for.",
      "offset": 200.08,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "Sure.",
      "offset": 201.92,
      "duration": 6.88
    },
    {
      "lang": "en",
      "text": "So um agent leaderboard is kind of uh",
      "offset": 203.76,
      "duration": 8.559
    },
    {
      "lang": "en",
      "text": "eval harness specifically created to",
      "offset": 208.8,
      "duration": 8.079
    },
    {
      "lang": "en",
      "text": "test LLM on hundreds of domains. uh we",
      "offset": 212.319,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "know that there are certain research",
      "offset": 216.879,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "papers from top companies and research",
      "offset": 218.159,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "institutes uh from Berkeley from",
      "offset": 221.04,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "Salesforce from other companies which",
      "offset": 224.159,
      "duration": 5.281
    },
    {
      "lang": "en",
      "text": "are targeting certain domains for",
      "offset": 226.56,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "evaluation but what our goal was that",
      "offset": 229.44,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "let's take all of these benchmarks",
      "offset": 232.08,
      "duration": 3.519
    },
    {
      "lang": "en",
      "text": "because it's possible that certain",
      "offset": 234.159,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "models are good at certain data sets",
      "offset": 235.599,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "there might be issues of leakage also so",
      "offset": 237.92,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "we wanted to create a as much possible",
      "offset": 240.4,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "holistic benchmark by taking on data",
      "offset": 243.12,
      "duration": 7.36
    },
    {
      "lang": "en",
      "text": "sets, slices on single turn, multi-turn",
      "offset": 246.64,
      "duration": 6.159
    },
    {
      "lang": "en",
      "text": "and also on like different kind of edge",
      "offset": 250.48,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "cases like there is some information",
      "offset": 252.799,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "missing for the parameter the tool",
      "offset": 255.2,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "itself is missing and long context and",
      "offset": 257.199,
      "duration": 5.361
    },
    {
      "lang": "en",
      "text": "all those problems all at once called",
      "offset": 260.88,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "composite scenario all those things we",
      "offset": 262.56,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "wanted to take from these different data",
      "offset": 264.4,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "sets. We did filtering on it for as hard",
      "offset": 266.639,
      "duration": 6.081
    },
    {
      "lang": "en",
      "text": "as possible. Um and then we created 14",
      "offset": 269.12,
      "duration": 6.56
    },
    {
      "lang": "en",
      "text": "small data sets. Uh and then evaluated",
      "offset": 272.72,
      "duration": 6.56
    },
    {
      "lang": "en",
      "text": "LM specifically for tool calling. uh and",
      "offset": 275.68,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "we use it used a Galileo metric called",
      "offset": 279.28,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "tool selection quality which is an LLM",
      "offset": 281.68,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "judge based metric powered by GPT4 and",
      "offset": 283.759,
      "duration": 5.361
    },
    {
      "lang": "en",
      "text": "it has multiple judges so that we get",
      "offset": 287.12,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "higher performance and that's how we",
      "offset": 289.12,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "kind of get the score for each sample",
      "offset": 291.759,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "and we aggregate for a data set and then",
      "offset": 294.72,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "we do average over the data set and",
      "offset": 296.72,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "that's how we get the overall score for",
      "offset": 299.199,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "the ranking but yes customers uh sorry",
      "offset": 301.12,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "uh readers can also go through",
      "offset": 304.08,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "particular performances of all certain",
      "offset": 305.84,
      "duration": 8.079
    },
    {
      "lang": "en",
      "text": "data sets as well. Um apart from that uh",
      "offset": 308.639,
      "duration": 8.881
    },
    {
      "lang": "en",
      "text": "we were pragmatic about uh selection in",
      "offset": 313.919,
      "duration": 5.441
    },
    {
      "lang": "en",
      "text": "a way that we were very interested in",
      "offset": 317.52,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "how the small models perform as well as",
      "offset": 319.36,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "opensource model performs because we",
      "offset": 322.24,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "know that there can be interesting",
      "offset": 324.16,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "insights. Historically what we have seen",
      "offset": 325.52,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "from our previous initiatives is that",
      "offset": 327.84,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "okay maybe the best GPT or best claude",
      "offset": 330.24,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "is the top model but there might be very",
      "offset": 333.28,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "small gap with the model just below it",
      "offset": 335.6,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "and then it becomes a cost performance",
      "offset": 338.16,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "tradeoff so that users can kind of",
      "offset": 340,
      "duration": 5.759
    },
    {
      "lang": "en",
      "text": "select a model which is a slightly worse",
      "offset": 343.36,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "but also gives them a similar",
      "offset": 345.759,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "performance and there can be some hidden",
      "offset": 347.52,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "models that nobody's talking about",
      "offset": 349.68,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "because it's all about the latest top",
      "offset": 351.12,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "releases and not about like the mid tier",
      "offset": 353.12,
      "duration": 3.519
    },
    {
      "lang": "en",
      "text": "or slightly lower tier.",
      "offset": 354.96,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "uh models uh from other providers and",
      "offset": 356.639,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "sometimes it's just they haven't got",
      "offset": 359.28,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "enough traction on social so they might",
      "offset": 361.919,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "be just missing from the discussions. So",
      "offset": 364.16,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "we tried to find this kind of models uh",
      "offset": 366.319,
      "duration": 6.481
    },
    {
      "lang": "en",
      "text": "and get insights as much as possible. So",
      "offset": 369.52,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "that that was the whole initiative. Uh",
      "offset": 372.8,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "we launched in February this year. Uh",
      "offset": 375.12,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "previously we had done rack specific u",
      "offset": 378,
      "duration": 7.36
    },
    {
      "lang": "en",
      "text": "evaluations. Now a small update on the",
      "offset": 381.6,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "agent leaderboard also like we are going",
      "offset": 385.36,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "to do a v2 of it because we find certain",
      "offset": 386.96,
      "duration": 7.28
    },
    {
      "lang": "en",
      "text": "issues with this evaluation and let's",
      "offset": 390.08,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "talk about this more at the end of the",
      "offset": 394.24,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "podcast like what's next coming uh we",
      "offset": 396.56,
      "duration": 5.759
    },
    {
      "lang": "en",
      "text": "can dive more into that",
      "offset": 398.96,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "um what are some of the surprising",
      "offset": 402.319,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "results for me it was m small being the",
      "offset": 404.16,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "best open source model above 40 and",
      "offset": 407.039,
      "duration": 6.641
    },
    {
      "lang": "en",
      "text": "deepseek v3 um Anything else that that",
      "offset": 410,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "jumped out to you?",
      "offset": 413.68,
      "duration": 5.359
    },
    {
      "lang": "en",
      "text": "Oh yes. So first and foremost I would",
      "offset": 415.199,
      "duration": 7.28
    },
    {
      "lang": "en",
      "text": "say that I was expecting GPTs to perform",
      "offset": 419.039,
      "duration": 6.801
    },
    {
      "lang": "en",
      "text": "the best because they kind of pioneered",
      "offset": 422.479,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "the tool calling function calling",
      "offset": 425.84,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "support and they have been working on",
      "offset": 427.599,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "that for so long. Um there was some",
      "offset": 429.36,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "criticism around Germany models although",
      "offset": 432.4,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "they were catching up and I was very",
      "offset": 434.72,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "surprised to see when we released this",
      "offset": 436.72,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "leaderboard. Of course, there has been",
      "offset": 439.68,
      "duration": 2.639
    },
    {
      "lang": "en",
      "text": "some shaking of the leaderboard also",
      "offset": 440.96,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "later on, but when we released, we found",
      "offset": 442.319,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "that Germany models are performing",
      "offset": 444,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "really good and they were extremely",
      "offset": 445.759,
      "duration": 5.601
    },
    {
      "lang": "en",
      "text": "cost efficient at the time compared to",
      "offset": 448.88,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "the second and second model like GP4 at",
      "offset": 451.36,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "the time. And so it became kind of a",
      "offset": 454.08,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "no-brainer that if anybody wants top",
      "offset": 456.24,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "performance, they can just directly go",
      "offset": 458.4,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "and go with the Gemini models. And they",
      "offset": 460.4,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "just not only released like the Pro",
      "offset": 462.72,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "model, they also released the flash",
      "offset": 464.16,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "version later on and flashlight. of the",
      "offset": 465.68,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "flashlight is a very interesting model.",
      "offset": 468.24,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "Uh in in the blog we have the models",
      "offset": 469.919,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "when we released this leaderboard but in",
      "offset": 472.16,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "the on the original leaderboard. We also",
      "offset": 473.919,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "have flashlight which is nobody talks",
      "offset": 476.72,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "about I think even now but I think it's",
      "offset": 478.879,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "got a very decent score which is like in",
      "offset": 480.8,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "the top five models. It's extremely",
      "offset": 482.8,
      "duration": 5.519
    },
    {
      "lang": "en",
      "text": "cheap model like it's so dirt cheap that",
      "offset": 484.96,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "it makes a lot of sense. Only model that",
      "offset": 488.319,
      "duration": 5.521
    },
    {
      "lang": "en",
      "text": "is cheaper than it is and performant is",
      "offset": 490.72,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "from Amazon uh which also nobody talks",
      "offset": 493.84,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "about. So we found this interesting",
      "offset": 496.08,
      "duration": 5.519
    },
    {
      "lang": "en",
      "text": "models uh that worked surprisingly well.",
      "offset": 497.919,
      "duration": 6.481
    },
    {
      "lang": "en",
      "text": "Then another surprise for me was that",
      "offset": 501.599,
      "duration": 5.521
    },
    {
      "lang": "en",
      "text": "the reasoning models were not performing",
      "offset": 504.4,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "well enough. uh they had certain kind of",
      "offset": 507.12,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "limitation when we probed into it like",
      "offset": 510.16,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "why are they scoring less overall they",
      "offset": 512.08,
      "duration": 7.68
    },
    {
      "lang": "en",
      "text": "were like the 01 the O4 03 they",
      "offset": 515.2,
      "duration": 6.56
    },
    {
      "lang": "en",
      "text": "were not performing well specifically",
      "offset": 519.76,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "for multiple tool call outputs that you",
      "offset": 521.76,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "have this request and it requires either",
      "offset": 524.8,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "the same tool being called multiple time",
      "offset": 527.76,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "or different tools being called all at",
      "offset": 530.72,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "once and they were outputting only",
      "offset": 532.56,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "single tool call um and because our",
      "offset": 534.24,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "evaluation require required that okay",
      "offset": 537.6,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "all the tools should be outputed at once",
      "offset": 539.279,
      "duration": 3.281
    },
    {
      "lang": "en",
      "text": "that's the instruction as well in the",
      "offset": 541.04,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "prompt uh it was not following and it",
      "offset": 542.56,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "was getting lower score so because of",
      "offset": 544.8,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "certain inherent characteristic of",
      "offset": 547.279,
      "duration": 5.921
    },
    {
      "lang": "en",
      "text": "specifically O series model uh we did",
      "offset": 550.08,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "not find them well to be performing on",
      "offset": 553.2,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "this kind of data set we had like",
      "offset": 555.68,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "multiple data set on the multiple tool",
      "offset": 557.04,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "output uh and that brought down the",
      "offset": 559.519,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "score for this model and of course they",
      "offset": 561.68,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "are extremely costly right so then it",
      "offset": 563.36,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "became uh it becomes straight",
      "offset": 565.44,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "straightforward that you should not be",
      "offset": 568,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "using this for tool calling and instead",
      "offset": 569.279,
      "duration": 7.601
    },
    {
      "lang": "en",
      "text": "go for the regular models probably. Um",
      "offset": 571.76,
      "duration": 6.96
    },
    {
      "lang": "en",
      "text": "apart from that on the open source side",
      "offset": 576.88,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "I would say that I was very impressed by",
      "offset": 578.72,
      "duration": 6.799
    },
    {
      "lang": "en",
      "text": "Mistral's entry into the ecosystem. Um",
      "offset": 580.8,
      "duration": 8.32
    },
    {
      "lang": "en",
      "text": "3.1 just straight away went up and it",
      "offset": 585.519,
      "duration": 6.401
    },
    {
      "lang": "en",
      "text": "also very costefficient. So I think",
      "offset": 589.12,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "people not people might have known at",
      "offset": 591.92,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "that time and in the launch blog also",
      "offset": 593.68,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "there was not much discussion on tool",
      "offset": 595.12,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "calling but the model performed very",
      "offset": 596.48,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "well on our benchmark.",
      "offset": 598.08,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "You mean meta right? You you said",
      "offset": 599.68,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "mistral but you meant meta.",
      "offset": 601.2,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "No I meant mistrol only. Yes, Mr. 3.1",
      "offset": 603.04,
      "duration": 5.359
    },
    {
      "lang": "en",
      "text": "performing really well on our benchmark.",
      "offset": 606,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "And the of course the other the surprise",
      "offset": 608.399,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "was that all the Llama models were not",
      "offset": 610.24,
      "duration": 5.599
    },
    {
      "lang": "en",
      "text": "performing well on our benchmark. Uh 3.3",
      "offset": 612.24,
      "duration": 7.36
    },
    {
      "lang": "en",
      "text": "and even the Llama 4 all were really",
      "offset": 615.839,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "performing extremely poor. So they are",
      "offset": 619.6,
      "duration": 2.799
    },
    {
      "lang": "en",
      "text": "like",
      "offset": 622.079,
      "duration": 2.241
    },
    {
      "lang": "en",
      "text": "oh no",
      "offset": 622.399,
      "duration": 6.081
    },
    {
      "lang": "en",
      "text": "and that was unbelievable like it felt",
      "offset": 624.32,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "like they were not tuned for tool",
      "offset": 628.48,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "calling at all in some way. Um so I mean",
      "offset": 630.16,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "historically it's all historical right",
      "offset": 634.64,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "there were certain biases and we like",
      "offset": 636.32,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "meta 1 the llama 1 was great llama 2 was",
      "offset": 638.079,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "great three also was great and then",
      "offset": 641.04,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "suddenly something happened and it went",
      "offset": 642.56,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "downhill so these are like surprising",
      "offset": 644.16,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "results for us I would say when we",
      "offset": 646.8,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "started the leaderboard plot 3.5 was",
      "offset": 649.04,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "somewhere in the middle um it was really",
      "offset": 652,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "good at certain other things but for",
      "offset": 654.64,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "tool calling it was not awesome but then",
      "offset": 656.16,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "the when we released the leaderboard and",
      "offset": 658.88,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "just in a league that launched 3.7 and",
      "offset": 661.2,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "that went straight up and nobody has",
      "offset": 663.76,
      "duration": 5.519
    },
    {
      "lang": "en",
      "text": "beaten it so far. We have not added the",
      "offset": 666.64,
      "duration": 7.68
    },
    {
      "lang": "en",
      "text": "latest one um like a quen models u",
      "offset": 669.279,
      "duration": 7.041
    },
    {
      "lang": "en",
      "text": "also the latest mistral models and not",
      "offset": 674.32,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "the 03 pro models which just came out in",
      "offset": 676.32,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "last few days we not evaluated but it it",
      "offset": 678.48,
      "duration": 6.88
    },
    {
      "lang": "en",
      "text": "held the for many many weeks. So uh that",
      "offset": 682.16,
      "duration": 7.6
    },
    {
      "lang": "en",
      "text": "was very impressive release I would say.",
      "offset": 685.36,
      "duration": 7.2
    },
    {
      "lang": "en",
      "text": "Awesome. Um, you know, I think, uh,",
      "offset": 689.76,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "that's a very good overview, uh, very",
      "offset": 692.56,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "good findings. I think people can use",
      "offset": 694.56,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "this leaderboard to pick out what base",
      "offset": 696.24,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "model that they want to use for their",
      "offset": 698.24,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "agents. Um, and I think that's a really",
      "offset": 699.839,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "good, uh, starting point. Uh, I wanted",
      "offset": 702.56,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "to dive in a little bit on a little bit",
      "offset": 704.72,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "a few of the metrics that you chose. Uh",
      "offset": 706.48,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "so first of all, did you just did you",
      "offset": 709.44,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "just rerun um all these evals basically",
      "offset": 711.2,
      "duration": 6.879
    },
    {
      "lang": "en",
      "text": "the BFCL, the XLAM",
      "offset": 714.56,
      "duration": 6.399
    },
    {
      "lang": "en",
      "text": "or do you um do you take it off of um",
      "offset": 718.079,
      "duration": 5.601
    },
    {
      "lang": "en",
      "text": "whatever they publicly report?",
      "offset": 720.959,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "No, we actually took the data slices",
      "offset": 723.68,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "from the original data set",
      "offset": 726.16,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "and then ran it ourselves. Uh we got the",
      "offset": 728,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "outputs uh and then we evaluated with",
      "offset": 731.12,
      "duration": 5.519
    },
    {
      "lang": "en",
      "text": "our metrics. So it's fresh. Everything",
      "offset": 733.92,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "is fresh.",
      "offset": 736.639,
      "duration": 2.64
    },
    {
      "lang": "en",
      "text": "Okay. Everything's fresh. Yeah, as long",
      "offset": 737.519,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "as it's like you know comparable and um",
      "offset": 739.279,
      "duration": 5.841
    },
    {
      "lang": "en",
      "text": "ba basically like do you think that",
      "offset": 741.76,
      "duration": 6.319
    },
    {
      "lang": "en",
      "text": "these are do you have any comments on",
      "offset": 745.12,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "just like BFCL versus XLAM versus you",
      "offset": 748.079,
      "duration": 7.44
    },
    {
      "lang": "en",
      "text": "know your uh your own TSQ thing? Um how",
      "offset": 750.88,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "do they how do they differ in",
      "offset": 755.519,
      "duration": 2.961
    },
    {
      "lang": "en",
      "text": "methodology? Like what should people",
      "offset": 756.88,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "know about their pros and cons?",
      "offset": 758.48,
      "duration": 7.2
    },
    {
      "lang": "en",
      "text": "Uh so I feel like they are very similar",
      "offset": 762.32,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "in nature. It's just that uh the data",
      "offset": 765.68,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "set is from different domains and",
      "offset": 768.8,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "certain data sets have multi-turn",
      "offset": 771.68,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "conversions available while certain do",
      "offset": 774.88,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "not have it. I do not remember exactly",
      "offset": 777.2,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "which one has it and which one doesn't",
      "offset": 779.44,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "but there were this discrepancies and we",
      "offset": 780.959,
      "duration": 5.521
    },
    {
      "lang": "en",
      "text": "really enjoyed like I really loved to",
      "offset": 783.92,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "benchmark also because it had",
      "offset": 786.48,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "particularly hard",
      "offset": 788.32,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "evaluation data sets and a lot of models",
      "offset": 790.16,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "had some struggle over there and",
      "offset": 792.48,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "I think Clay uh Clay from Sierra",
      "offset": 794.56,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "actually just released an update to Talb",
      "offset": 796.8,
      "duration": 3.279
    },
    {
      "lang": "en",
      "text": "for some reason. Um,",
      "offset": 798.8,
      "duration": 3.039
    },
    {
      "lang": "en",
      "text": "oh,",
      "offset": 800.079,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "yeah. Uh, Allesio's best friend now.",
      "offset": 801.839,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "Um,",
      "offset": 804.959,
      "duration": 2.56
    },
    {
      "lang": "en",
      "text": "yeah, super.",
      "offset": 805.76,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "So, he he he released towel squared.",
      "offset": 807.519,
      "duration": 5.361
    },
    {
      "lang": "en",
      "text": "Wow. I was not",
      "offset": 811.6,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "motion of dual. It was yesterday. I just",
      "offset": 812.88,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "I happened to see it.",
      "offset": 815.279,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "Um,",
      "offset": 817.04,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "to coordinate, guide, and assist the",
      "offset": 819.68,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "user.",
      "offset": 821.92,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "Yeah.",
      "offset": 824.399,
      "duration": 3.281
    },
    {
      "lang": "en",
      "text": "Wonderful. So, so Talbench, uh, I",
      "offset": 824.8,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "actually didn't know it was from Sierra,",
      "offset": 827.68,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "uh, but it's it's actually very very uh,",
      "offset": 829.519,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "influential.",
      "offset": 831.44,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "Yeah. Can you talk more about Tbench?",
      "offset": 834.32,
      "duration": 5.519
    },
    {
      "lang": "en",
      "text": "Oh, yes. So, essentially Towerbench when",
      "offset": 837.04,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "it was released, I mean, yes, the top",
      "offset": 839.839,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "bench is for two two domains. It test",
      "offset": 842,
      "duration": 6.399
    },
    {
      "lang": "en",
      "text": "agentic capabilities for two domains,",
      "offset": 846.079,
      "duration": 5.361
    },
    {
      "lang": "en",
      "text": "which is airline and retail. And their",
      "offset": 848.399,
      "duration": 6.081
    },
    {
      "lang": "en",
      "text": "setup is that they have very long",
      "offset": 851.44,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "prompts in the beginning with lot of",
      "offset": 854.48,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "request",
      "offset": 856.8,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "and then you have these tools defined",
      "offset": 857.92,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "and the tools can interact with certain",
      "offset": 860.48,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "databases for read and write operations",
      "offset": 863.44,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "and it is predefined that once the whole",
      "offset": 866.24,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "agentic task is done then they will",
      "offset": 868.56,
      "duration": 6.8
    },
    {
      "lang": "en",
      "text": "validate on the final uh edits made on",
      "offset": 871.36,
      "duration": 6.479
    },
    {
      "lang": "en",
      "text": "and they'll check for the expected value",
      "offset": 875.36,
      "duration": 5.599
    },
    {
      "lang": "en",
      "text": "in the data sets uh in the data database",
      "offset": 877.839,
      "duration": 5.841
    },
    {
      "lang": "en",
      "text": "which are basically CSV style of files.",
      "offset": 880.959,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "Uh and in that fashion they will",
      "offset": 883.68,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "validate whether the task was done or",
      "offset": 885.36,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "not. So it's it's it's pretty well",
      "offset": 887.12,
      "duration": 6.719
    },
    {
      "lang": "en",
      "text": "defined such that you get a",
      "offset": 891.36,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "deterministic evaluation all the times.",
      "offset": 893.839,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "That's the beauty of this and it's it's",
      "offset": 896.32,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "domain specific which I think is missing",
      "offset": 898.639,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "from other evaluation data sets. Uh and",
      "offset": 900.959,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "that is also one of the themes for our",
      "offset": 903.839,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "V2. Um because I think people don't just",
      "offset": 905.519,
      "duration": 5.841
    },
    {
      "lang": "en",
      "text": "come from a general mindset right people",
      "offset": 908.8,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "in the industry come from let's say I'm",
      "offset": 911.36,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "from healthcare I want to see if my if",
      "offset": 913.199,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "this models work for healthcare or not",
      "offset": 914.959,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "people are coming from investment",
      "offset": 916.88,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "finance insurance industry and they want",
      "offset": 918.56,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "to know that is are the models ready for",
      "offset": 920.72,
      "duration": 5.679
    },
    {
      "lang": "en",
      "text": "my domain or not because we all know",
      "offset": 924.56,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "that things work on the general domain",
      "offset": 926.399,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "side and when you just put it on the",
      "offset": 928.959,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "internal data things start to blow up",
      "offset": 931.199,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "with internal terminologies and the",
      "offset": 933.04,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "cases that happen. So I think that was",
      "offset": 934.88,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "the beauty that they specifically",
      "offset": 937.199,
      "duration": 3.521
    },
    {
      "lang": "en",
      "text": "targeted domains. I don't know if they",
      "offset": 938.72,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "might have expanded domains in this",
      "offset": 940.72,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "because two was I felt like less but it",
      "offset": 942.16,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "was a decent start. It was more about",
      "offset": 944.079,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "the methodology rather than the domains",
      "offset": 945.839,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "I feel with the towel",
      "offset": 947.519,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "it was very hard as well.",
      "offset": 950.56,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "Yeah. So squared you know just kind of",
      "offset": 952.399,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "looking at it now squared doesn't seem",
      "offset": 954.88,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "like it expanded domains. Uh it just",
      "offset": 956.639,
      "duration": 5.041
    },
    {
      "lang": "en",
      "text": "they they expanded the way that they",
      "offset": 959.279,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "interact with the user.",
      "offset": 961.68,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "Mhm. So, some kind of uh Oh, they they",
      "offset": 963.68,
      "duration": 5.279
    },
    {
      "lang": "en",
      "text": "added a telecom domain. Okay, there we",
      "offset": 967.36,
      "duration": 2.479
    },
    {
      "lang": "en",
      "text": "go.",
      "offset": 968.959,
      "duration": 2.161
    },
    {
      "lang": "en",
      "text": "I see.",
      "offset": 969.839,
      "duration": 3.521
    },
    {
      "lang": "en",
      "text": "But I I I if it seems like the emphasis",
      "offset": 971.12,
      "duration": 5.279
    },
    {
      "lang": "en",
      "text": "was on more sort of um co-piloting",
      "offset": 973.36,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "uh rather than uh two call just, you",
      "offset": 976.399,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "know, they call it dual control. Yeah,",
      "offset": 979.6,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "we'll we'll see. We'll see what this is.",
      "offset": 981.839,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "I It's It's obviously very new, so I",
      "offset": 983.44,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "just saw I just happened to see it the",
      "offset": 986.24,
      "duration": 1.839
    },
    {
      "lang": "en",
      "text": "other day.",
      "offset": 987.519,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "Um solid.",
      "offset": 988.079,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "Yeah. Yeah, definitely put that in your",
      "offset": 990.639,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "uh V2. Um and then yeah and I also",
      "offset": 992.399,
      "duration": 6.88
    },
    {
      "lang": "en",
      "text": "wanted to go back to um TSQ. I think",
      "offset": 995.6,
      "duration": 6.32
    },
    {
      "lang": "en",
      "text": "that's the other sort of maybe uh novel",
      "offset": 999.279,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "part. Um I think a lot of people have",
      "offset": 1001.92,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "queries about LM as judge",
      "offset": 1004.399,
      "duration": 6.401
    },
    {
      "lang": "en",
      "text": "and um yeah just tell us more like you",
      "offset": 1006.88,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "know do you have any calibration errors?",
      "offset": 1010.8,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "Um how how uh do should more people",
      "offset": 1013.04,
      "duration": 6.4
    },
    {
      "lang": "en",
      "text": "adopt this kind of approach?",
      "offset": 1016.24,
      "duration": 7.92
    },
    {
      "lang": "en",
      "text": "Indeed I agree. LLMs have been notorious",
      "offset": 1019.44,
      "duration": 7.999
    },
    {
      "lang": "en",
      "text": "with their own biases, right? We have",
      "offset": 1024.16,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "seen historically",
      "offset": 1027.439,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "they have issues with long context and",
      "offset": 1029.28,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "they have issues with certain kind of",
      "offset": 1031.439,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "outputs. They prefer let's say lengthy",
      "offset": 1034.319,
      "duration": 7.12
    },
    {
      "lang": "en",
      "text": "outputs and stuff. Um what we so this",
      "offset": 1036.959,
      "duration": 6.801
    },
    {
      "lang": "en",
      "text": "our philosophy in the company is that",
      "offset": 1041.439,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "any metric that comes out which is a",
      "offset": 1043.76,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "Galileo metric we do extensive research",
      "offset": 1045.679,
      "duration": 5.441
    },
    {
      "lang": "en",
      "text": "on it. uh we create very hard data sets",
      "offset": 1048.079,
      "duration": 6.241
    },
    {
      "lang": "en",
      "text": "internally um and we evaluate it",
      "offset": 1051.12,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "evaluate them we annotate them and check",
      "offset": 1054.32,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "for correlations and only once we feel",
      "offset": 1057.12,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "comfortable then only we release it and",
      "offset": 1059.76,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "which is usually",
      "offset": 1061.919,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "based on the correlation scores like AU",
      "offset": 1064.08,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "ROC scores that we find if they are high",
      "offset": 1066.72,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "enough only then we release it otherwise",
      "offset": 1069.2,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "it goes through a rigorous process of",
      "offset": 1070.96,
      "duration": 6.36
    },
    {
      "lang": "en",
      "text": "prompt optimization so we",
      "offset": 1072.559,
      "duration": 4.761
    },
    {
      "lang": "en",
      "text": "allow people to use any LLM with the",
      "offset": 1077.76,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "metric. Uh and so because LLMs keep on",
      "offset": 1080.32,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "improving, the metric also keeps on",
      "offset": 1083.12,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "improving and then the prompts is what",
      "offset": 1084.48,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "we kind of",
      "offset": 1086.72,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "uh the prompt is where all the research",
      "offset": 1088.72,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "goes through and we have seen tremendous",
      "offset": 1090.96,
      "duration": 4.959
    },
    {
      "lang": "en",
      "text": "value in that. And yes, people would",
      "offset": 1092.64,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "definitely",
      "offset": 1095.919,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "have skepticism. A lot of people, a lot",
      "offset": 1097.6,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "of our customers also have that",
      "offset": 1099.919,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "skepticism. Some are still stuck on blue",
      "offset": 1101.44,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "and root score. So which is surprising",
      "offset": 1103.84,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "because they think that LLMs are not",
      "offset": 1106.16,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "there yet. But we what we try to do from",
      "offset": 1108.08,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "our side is essentially we try to follow",
      "offset": 1110.559,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "the best practice of prompting. Uh we",
      "offset": 1112.72,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "try to exploit the reasoning as much as",
      "offset": 1115.36,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "possible from this LLMs and then we use",
      "offset": 1118.16,
      "duration": 5.519
    },
    {
      "lang": "en",
      "text": "basically self-consistency by using",
      "offset": 1121.44,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "multiple outputs and then taking a",
      "offset": 1123.679,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "majority voting over there. People can",
      "offset": 1126.32,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "people have flexibility on these things.",
      "offset": 1128.48,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "They can select the LLMs they feel they",
      "offset": 1130.4,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "want to use. If they want to use much",
      "offset": 1132.559,
      "duration": 3.521
    },
    {
      "lang": "en",
      "text": "more powerful LM, go ahead. You can",
      "offset": 1134,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "customize it. If you want to change the",
      "offset": 1136.08,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "prompts, go ahead, customize it. If you",
      "offset": 1138.96,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "want to increase the number of judges,",
      "offset": 1141.44,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "go ahead and customize it. So, we allow",
      "offset": 1143.36,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "full flexibility to people to modify",
      "offset": 1145.36,
      "duration": 7.28
    },
    {
      "lang": "en",
      "text": "this LLMs judges so that they can get",
      "offset": 1149.12,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "the best performance possible.",
      "offset": 1152.64,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "uh of course people should always",
      "offset": 1155.12,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "validate with some small annotated data",
      "offset": 1158.08,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "set before they go ahead and just make",
      "offset": 1160.4,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "it the default evaluation. So that is",
      "offset": 1162.24,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "always a part of our PC's and we they",
      "offset": 1164.559,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "get clarity and confidence only then",
      "offset": 1167.84,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "they will be they are using it. So I",
      "offset": 1169.52,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "feel I have basically I have been a",
      "offset": 1172.64,
      "duration": 5.279
    },
    {
      "lang": "en",
      "text": "fullstack ML engineer previously um and",
      "offset": 1174.559,
      "duration": 6.081
    },
    {
      "lang": "en",
      "text": "I've worked a lot with annotators",
      "offset": 1177.919,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "and I have seen a lot of biases and",
      "offset": 1180.64,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "issues with annotators as well. Um it's",
      "offset": 1183.039,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "just so time consuming that you can't",
      "offset": 1186.08,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "afford to kind of wait for annotation to",
      "offset": 1187.919,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "get completed. So it's just much much",
      "offset": 1190.16,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "better to add more instructions do some",
      "offset": 1192.64,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "initial validation and get the clarity",
      "offset": 1194.96,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "that okay this is where it works. If it",
      "offset": 1197.039,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "doesn't work yes go back to annotation",
      "offset": 1199.84,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "but I feel like it has started working a",
      "offset": 1201.6,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "lot one year back because we I I was",
      "offset": 1203.679,
      "duration": 5.761
    },
    {
      "lang": "en",
      "text": "running this experiments like we were",
      "offset": 1207.2,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "working with hall simulation index 2",
      "offset": 1209.44,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "years back uh where we using our own",
      "offset": 1211.52,
      "duration": 5.279
    },
    {
      "lang": "en",
      "text": "similar LMS as judge methodology and we",
      "offset": 1214.32,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "had certain issues and there were some",
      "offset": 1216.799,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "correlation issues at the time but last",
      "offset": 1218.88,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "year it got pretty strong and this year",
      "offset": 1221.919,
      "duration": 5.361
    },
    {
      "lang": "en",
      "text": "I feel it's so strong LLMs judges is so",
      "offset": 1224.24,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "strong that you can use it. I've gone",
      "offset": 1227.28,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "through like hundreds of explanations",
      "offset": 1229.28,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "like our LLMs also generate explanations",
      "offset": 1230.88,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "that users can read to validate whether",
      "offset": 1233.2,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "it actually went through this correct",
      "offset": 1235.919,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "reasoning or not and I have gone through",
      "offset": 1238,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "many explanations in the during this V2",
      "offset": 1240.48,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "experiment that I'm doing right now and",
      "offset": 1243.6,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "also the V1 and I did not find any",
      "offset": 1245.039,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "issues. So",
      "offset": 1247.2,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "uh I feel like when we so that's why we",
      "offset": 1249.36,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "released all the data set people can go",
      "offset": 1252.4,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "and run the experiment with the v2 also",
      "offset": 1254,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "we're going to release the data sets",
      "offset": 1256.32,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "people can just take them and run it on",
      "offset": 1258.159,
      "duration": 5.281
    },
    {
      "lang": "en",
      "text": "our uh cluster it's free our our product",
      "offset": 1260.24,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "is free so anybody can just sign up and",
      "offset": 1263.44,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "start using it they can just put the",
      "offset": 1265.12,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "open a key and they will get a clarity",
      "offset": 1266.48,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "okay it's working or not we want to be",
      "offset": 1268.559,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "as open as possible while still being a",
      "offset": 1270.159,
      "duration": 6.241
    },
    {
      "lang": "en",
      "text": "closed source company so I feel like",
      "offset": 1273.28,
      "duration": 5.279
    },
    {
      "lang": "en",
      "text": "it's working and people should",
      "offset": 1276.4,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "definitely pay attention to it.",
      "offset": 1278.559,
      "duration": 5.921
    },
    {
      "lang": "en",
      "text": "What What's the gap between the best",
      "offset": 1281.44,
      "duration": 5.119
    },
    {
      "lang": "en",
      "text": "prompts and like the worst prompts? Like",
      "offset": 1284.48,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "when you run as as you iterated on on",
      "offset": 1286.559,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "the prompting of the LMS judge, did you",
      "offset": 1288.72,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "see models change in like position in",
      "offset": 1290.88,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "the leaderboard or were the results",
      "offset": 1293.76,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "mostly the same just like difference",
      "offset": 1296,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "scores?",
      "offset": 1297.6,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "Uh so I was not working on fine-tuning",
      "offset": 1299.28,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "the LM as a judge. it is done by a",
      "offset": 1301.679,
      "duration": 5.281
    },
    {
      "lang": "en",
      "text": "separate team member and once the metric",
      "offset": 1304.159,
      "duration": 6.321
    },
    {
      "lang": "en",
      "text": "is ready only then I start using it but",
      "offset": 1306.96,
      "duration": 6.079
    },
    {
      "lang": "en",
      "text": "um that was they went through like a",
      "offset": 1310.48,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "months of rigorous process of prompt",
      "offset": 1313.039,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "tuning and uh it's a very structured",
      "offset": 1315.36,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "prompt it's like an art in itself as you",
      "offset": 1318.799,
      "duration": 5.441
    },
    {
      "lang": "en",
      "text": "know where we have certain sections for",
      "offset": 1321.12,
      "duration": 5.439
    },
    {
      "lang": "en",
      "text": "certain things on certain instruction",
      "offset": 1324.24,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "and what we do is like it's a very",
      "offset": 1326.559,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "simple philosophy we start with a base",
      "offset": 1328.88,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "prompt with what we think is correct and",
      "offset": 1331.36,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "then we see where it's failing and then",
      "offset": 1333.84,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "we try to add those instructions again",
      "offset": 1336.32,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "uh defining the failure cases in a",
      "offset": 1339.039,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "general way to the prompt of the LMS",
      "offset": 1341.84,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "just and see if it's following it or not",
      "offset": 1344.159,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "and that iteration goes on for multiple",
      "offset": 1346.559,
      "duration": 5.281
    },
    {
      "lang": "en",
      "text": "times till we are sure that we are",
      "offset": 1348.88,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "getting the performance that we need. So",
      "offset": 1351.84,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "it's it's very common senseoriented",
      "offset": 1353.919,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "approach I would say and it's just plain",
      "offset": 1356.32,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "English writing with just following the",
      "offset": 1358.4,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "right way of writing prompts. Um and of",
      "offset": 1360,
      "duration": 6.32
    },
    {
      "lang": "en",
      "text": "course the leaderboard if you are using",
      "offset": 1363.52,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "different judges with different models",
      "offset": 1366.32,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "it can there can be heavy shakeup of the",
      "offset": 1368.24,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "leaderboard also. So that's why we use",
      "offset": 1369.919,
      "duration": 6.401
    },
    {
      "lang": "en",
      "text": "the best LLM available at that when we",
      "offset": 1373.12,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "are run when we are starting or near",
      "offset": 1376.32,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "completion of the experiments. We try to",
      "offset": 1378.32,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "use the best LLM available so that it",
      "offset": 1380.159,
      "duration": 5.361
    },
    {
      "lang": "en",
      "text": "should not be a situation where you're",
      "offset": 1383.6,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "trying to evaluate uh 3.7 with GPT4 mini",
      "offset": 1385.52,
      "duration": 6.32
    },
    {
      "lang": "en",
      "text": "right it will be really",
      "offset": 1389.52,
      "duration": 6.72
    },
    {
      "lang": "en",
      "text": "uh risky proposition because GPD4 mini",
      "offset": 1391.84,
      "duration": 6.319
    },
    {
      "lang": "en",
      "text": "can't find",
      "offset": 1396.24,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "uh cannot evaluate the hard outputs that",
      "offset": 1398.159,
      "duration": 6.801
    },
    {
      "lang": "en",
      "text": "3.7 might be doing correctly right um so",
      "offset": 1401.44,
      "duration": 7.52
    },
    {
      "lang": "en",
      "text": "and so for this V2 I'm using the 3.7",
      "offset": 1404.96,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "because it is one of the top training",
      "offset": 1408.96,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "models right now and I've seen really",
      "offset": 1411.44,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "good performance from it. So we",
      "offset": 1413.039,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "essentially swap these models as much as",
      "offset": 1415.44,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "possible when we are running the",
      "offset": 1417.6,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "experiment otherwise it looks way we",
      "offset": 1419.039,
      "duration": 5.681
    },
    {
      "lang": "en",
      "text": "have seen situation previously where we",
      "offset": 1421.919,
      "duration": 5.681
    },
    {
      "lang": "en",
      "text": "were using probably GP4 turbo or",
      "offset": 1424.72,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "something or even GP 3.5 long back I'm",
      "offset": 1427.6,
      "duration": 6.959
    },
    {
      "lang": "en",
      "text": "talking like 2 years back um and we had",
      "offset": 1430.64,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "to use GPT4 Turbo at the time because",
      "offset": 1434.559,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "with the 3.5 it was something else",
      "offset": 1436.48,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "entirely because it can't find the",
      "offset": 1439.44,
      "duration": 5.239
    },
    {
      "lang": "en",
      "text": "errors. Mhm.",
      "offset": 1441.2,
      "duration": 3.479
    },
    {
      "lang": "en",
      "text": "Okay. Um, awesome. Uh, maybe give a",
      "offset": 1445.52,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "little preview of the thinking for V2",
      "offset": 1448.159,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "and uh, maybe that that will be uh,",
      "offset": 1451.76,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "where we end up. But, uh, yeah, I'm just",
      "offset": 1453.919,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "kind of curious.",
      "offset": 1456,
      "duration": 7.6
    },
    {
      "lang": "en",
      "text": "Sure. Uh what we found issues with V1 is",
      "offset": 1457.84,
      "duration": 7.92
    },
    {
      "lang": "en",
      "text": "uh it has a lot of domains but it does",
      "offset": 1463.6,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "not have a domain vertical in itself",
      "offset": 1465.76,
      "duration": 5.519
    },
    {
      "lang": "en",
      "text": "that okay if somebody is interested on",
      "offset": 1468.64,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "how this model performance let's say",
      "offset": 1471.279,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "telecommunication they can just select",
      "offset": 1472.88,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "filter and then see which",
      "offset": 1475.279,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "what's the performance from each model",
      "offset": 1477.919,
      "duration": 3.281
    },
    {
      "lang": "en",
      "text": "what are the gaps what's the cost",
      "offset": 1479.6,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "related to them um so first thing first",
      "offset": 1481.2,
      "duration": 7.28
    },
    {
      "lang": "en",
      "text": "we are doing domain specific um which",
      "offset": 1485.84,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "means that these are like new data sets",
      "offset": 1488.48,
      "duration": 5.679
    },
    {
      "lang": "en",
      "text": "from scratch and why are we doing this",
      "offset": 1490.88,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "uh is of course the first problem is",
      "offset": 1494.159,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "like they are not available easily",
      "offset": 1495.76,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "secondly it's because which is I would",
      "offset": 1497.279,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "say the main reason is that we don't",
      "offset": 1499.52,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "want leakage at all to be possible in",
      "offset": 1500.88,
      "duration": 7.2
    },
    {
      "lang": "en",
      "text": "any of the LLM so far so we want that",
      "offset": 1505.12,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "these are new data sets so we guarantee",
      "offset": 1508.08,
      "duration": 5.599
    },
    {
      "lang": "en",
      "text": "that it's going to be performing",
      "offset": 1510.72,
      "duration": 7.52
    },
    {
      "lang": "en",
      "text": "as unbiased as possible Then we saw with",
      "offset": 1513.679,
      "duration": 7.201
    },
    {
      "lang": "en",
      "text": "V1 that the scores are saturating. We",
      "offset": 1518.24,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "see that the best score has already",
      "offset": 1520.88,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "reached 0.95. And so if we are going to",
      "offset": 1522.88,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "evalate even better models that are",
      "offset": 1525.279,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "coming out, they'll be definitely in the",
      "offset": 1526.88,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "range of 0.9 to1 which I feel is not",
      "offset": 1528.48,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "very informative for anybody to take a",
      "offset": 1531.84,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "call because there can be some noise",
      "offset": 1534.24,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "also in this evaluation. So we want the",
      "offset": 1536,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "benchmarks to be harder uh and this tool",
      "offset": 1538.48,
      "duration": 6.88
    },
    {
      "lang": "en",
      "text": "calling to be more complicated and",
      "offset": 1542.08,
      "duration": 7.12
    },
    {
      "lang": "en",
      "text": "realistic at the same time. uh this BFCL",
      "offset": 1545.36,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "if you just open and check it uh you",
      "offset": 1549.2,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "would be like okay this is okay but you",
      "offset": 1551.6,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "don't want to use it for realistic",
      "offset": 1554.08,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "evaluation because it's like some math",
      "offset": 1556.88,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "problem statement is given some simple",
      "offset": 1560.32,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "tool is given just like let's say some",
      "offset": 1562.08,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "exponential tool logarithmic tool",
      "offset": 1563.76,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "multiplication and stuff and you are",
      "offset": 1565.6,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "just like evaluating on that so I was",
      "offset": 1567.52,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "not particularly happy with that but",
      "offset": 1569.84,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "that's the best we could got get at the",
      "offset": 1571.6,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "time um and that's why we mixed up all",
      "offset": 1573.679,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "the data sets that's available at the",
      "offset": 1576.4,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "time to kind of average out the things",
      "offset": 1577.84,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "uh because it's very specific right if",
      "offset": 1580,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "some model is good at math then it might",
      "offset": 1581.679,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "perform really great for DFCL but if",
      "offset": 1583.919,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "some model doesn't understand the",
      "offset": 1586.559,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "terminologies of industry uh then it",
      "offset": 1588.24,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "will not perform well for tow so we kind",
      "offset": 1591.2,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "of mixed up the data set during that",
      "offset": 1593.279,
      "duration": 7.121
    },
    {
      "lang": "en",
      "text": "time um so two things uh domain and one",
      "offset": 1595.36,
      "duration": 7.36
    },
    {
      "lang": "en",
      "text": "is increasing the hardness third thing",
      "offset": 1600.4,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "is multi-turn evaluation so this whole",
      "offset": 1602.72,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "evaluation is going to be let's say a",
      "offset": 1606,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "support bots. The whole theme is",
      "offset": 1608.08,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "evaluating support agents with lot of",
      "offset": 1610.96,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "tools like we are having 20 tools. We're",
      "offset": 1613.52,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "going to have five domains uh some like",
      "offset": 1616,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "banking, finance, telecommunication,",
      "offset": 1619.2,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "insurance, healthcare. Um and we'll be",
      "offset": 1621.6,
      "duration": 7.04
    },
    {
      "lang": "en",
      "text": "having certain categories also like",
      "offset": 1624.96,
      "duration": 6.719
    },
    {
      "lang": "en",
      "text": "coordinate the tools and then kind of",
      "offset": 1628.64,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "adversarial scenarios customers",
      "offset": 1631.679,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "manipulating",
      "offset": 1633.76,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "uh the bot to kind of do certain things",
      "offset": 1635.36,
      "duration": 8.72
    },
    {
      "lang": "en",
      "text": "and uh uh yeah multi-turn coming to",
      "offset": 1639.76,
      "duration": 6.159
    },
    {
      "lang": "en",
      "text": "multi-turn the in the multi-turn what we",
      "offset": 1644.08,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "are doing is we this time last time it",
      "offset": 1645.919,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "was more like there's an input with one",
      "offset": 1648.24,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "message or multiple messages and then",
      "offset": 1650.48,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "give one output. That was how we",
      "offset": 1652.799,
      "duration": 3.281
    },
    {
      "lang": "en",
      "text": "evaluating. This time it's like a",
      "offset": 1654.559,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "realistic scenario that we start with a",
      "offset": 1656.08,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "scenario. Uh scenario has a first",
      "offset": 1659.679,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "message and there's like goals to be",
      "offset": 1662,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "accomplished. This scenario is given to",
      "offset": 1664.88,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "a user simulator. Now user simulator is",
      "offset": 1666.96,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "basically a prompt uh which gets the",
      "offset": 1670.24,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "whole history of the chat and it's",
      "offset": 1672.48,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "supposed to generate a new message and",
      "offset": 1674.159,
      "duration": 6.721
    },
    {
      "lang": "en",
      "text": "that simulation is ran by GPT 4.1 mini",
      "offset": 1676.64,
      "duration": 6.8
    },
    {
      "lang": "en",
      "text": "model. Now there is another simulator",
      "offset": 1680.88,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "which I called the tool simulator. Why",
      "offset": 1683.44,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "we selected tool simulator? Essentially",
      "offset": 1685.919,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "because of ease of running this",
      "offset": 1688.24,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "simulation. Uh as we saw with toao toao",
      "offset": 1690.88,
      "duration": 5.519
    },
    {
      "lang": "en",
      "text": "has like tool definitions and uh",
      "offset": 1693.76,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "databases with CSV file kind of thing",
      "offset": 1696.399,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "with red operations going on. Now I",
      "offset": 1698.559,
      "duration": 6.881
    },
    {
      "lang": "en",
      "text": "wanted to have like many tools created.",
      "offset": 1701.2,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "We have like 20 tool per domain and we",
      "offset": 1705.44,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "have like five domain. So it's like 100",
      "offset": 1707.36,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "tools. I did not have do not have the",
      "offset": 1709.36,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "time to kind of write this uh tool with",
      "offset": 1711.039,
      "duration": 5.921
    },
    {
      "lang": "en",
      "text": "all these databases. So I kind of",
      "offset": 1714.399,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "created a tool simulator again with the",
      "offset": 1716.96,
      "duration": 7.36
    },
    {
      "lang": "en",
      "text": "GP4.1 mini model uh which takes into the",
      "offset": 1719.2,
      "duration": 7.92
    },
    {
      "lang": "en",
      "text": "input as a tool call. So it gets the",
      "offset": 1724.32,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "tool call, it gets the response of the",
      "offset": 1727.12,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "tool to tool call schema and then it is",
      "offset": 1729.44,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "supposed to give an output. So rather",
      "offset": 1732.159,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "than like interacting with an API or",
      "offset": 1734.24,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "calling certain databases, it is going",
      "offset": 1736.799,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "to generate an output based on the",
      "offset": 1739.2,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "history of the conversation and the tool",
      "offset": 1741.12,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "call information that is given. And the",
      "offset": 1743.6,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "main one that we are evaluating is",
      "offset": 1745.919,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "basically the agent. Uh the agent we are",
      "offset": 1747.84,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "swap swapping all the LLMs to evaluate",
      "offset": 1750.399,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "and this agent basically is",
      "offset": 1752.88,
      "duration": 5.279
    },
    {
      "lang": "en",
      "text": "accomplishing the task given by the user",
      "offset": 1754.96,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "and then user gives a task. The agent",
      "offset": 1758.159,
      "duration": 5.681
    },
    {
      "lang": "en",
      "text": "either gives a response or gives the",
      "offset": 1761.12,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "tool call. If it's a tool call, it's",
      "offset": 1763.84,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "given to the tool simulator and the tool",
      "offset": 1766,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "simulator will give an output and then",
      "offset": 1768.48,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "that output will be used by the agent to",
      "offset": 1770.559,
      "duration": 7.281
    },
    {
      "lang": "en",
      "text": "give a response to the user. So what we",
      "offset": 1773.6,
      "duration": 7.28
    },
    {
      "lang": "en",
      "text": "do in this is we for every scenario we",
      "offset": 1777.84,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "have like at least like five to seven",
      "offset": 1780.88,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "goals to be accomplished. uh they are",
      "offset": 1783.12,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "pretty complex like toao and they are",
      "offset": 1785.2,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "all they all need to be accomplished",
      "offset": 1788.64,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "otherwise it's the task is not",
      "offset": 1790.96,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "accomplished. So this time you have to",
      "offset": 1793.279,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "get done many things at once which makes",
      "offset": 1795.039,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "the thing hard and it's possible that",
      "offset": 1797.679,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "you might miss out on certain things and",
      "offset": 1799.52,
      "duration": 6.32
    },
    {
      "lang": "en",
      "text": "we see that the best models are good at",
      "offset": 1802.88,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "it but certain models are really",
      "offset": 1805.84,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "struggling on it and they can go into",
      "offset": 1807.919,
      "duration": 6.161
    },
    {
      "lang": "en",
      "text": "iterations for no reason. Um and so the",
      "offset": 1809.6,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "scores are coming out to be lower much",
      "offset": 1814.08,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "lower like the pro the main problem 0.95",
      "offset": 1816.08,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "is like solved that okay the scores are",
      "offset": 1819.279,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "lower we are getting domain specific",
      "offset": 1821.2,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "insights uh and then it's a very",
      "offset": 1823.12,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "realistic scenario where you are having",
      "offset": 1825.44,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "multiple turn and then only finally when",
      "offset": 1827.039,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "everything is done then the task is",
      "offset": 1829.44,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "accomplished. So what's how do we",
      "offset": 1831.36,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "evaluate the performance in this case",
      "offset": 1833.919,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "and we use two metrics",
      "offset": 1835.919,
      "duration": 5.441
    },
    {
      "lang": "en",
      "text": "we use the older one the tool selection",
      "offset": 1839.12,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "quality which is just telling us whether",
      "offset": 1841.36,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "the tool selected was correct or not but",
      "offset": 1843.039,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "other thing which is super important",
      "offset": 1845.76,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "from a business point of view and",
      "offset": 1847.44,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "practical point of view is that whether",
      "offset": 1849.36,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "the agent completed all the task or not",
      "offset": 1851.039,
      "duration": 6.081
    },
    {
      "lang": "en",
      "text": "right uh so for that we have the action",
      "offset": 1854.399,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "completion metric uh which will check",
      "offset": 1857.12,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "that whether all the request that were",
      "offset": 1859.679,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "given by the user where all of them",
      "offset": 1862.24,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "accomplished and were they accomplished",
      "offset": 1864.159,
      "duration": 5.041
    },
    {
      "lang": "en",
      "text": "correctly or not. So for every sample",
      "offset": 1865.679,
      "duration": 5.841
    },
    {
      "lang": "en",
      "text": "we'll get okay this is how good it is at",
      "offset": 1869.2,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "tool selection this is how great the",
      "offset": 1871.52,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "model was at accomplishing the task. So",
      "offset": 1873.919,
      "duration": 5.281
    },
    {
      "lang": "en",
      "text": "in that fashion we'll get the scores for",
      "offset": 1876.88,
      "duration": 5.679
    },
    {
      "lang": "en",
      "text": "every scenario and then we'll aggregate",
      "offset": 1879.2,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "the scores for all the data sets to get",
      "offset": 1882.559,
      "duration": 5.521
    },
    {
      "lang": "en",
      "text": "the final score. So this time we are",
      "offset": 1884.96,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "kind of focusing more on action",
      "offset": 1888.08,
      "duration": 4.959
    },
    {
      "lang": "en",
      "text": "completion metric I would say which is",
      "offset": 1890.159,
      "duration": 6.161
    },
    {
      "lang": "en",
      "text": "telling us that the user asked for this",
      "offset": 1893.039,
      "duration": 5.921
    },
    {
      "lang": "en",
      "text": "five things and whether the agent",
      "offset": 1896.32,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "completed all the five things or not. So",
      "offset": 1898.96,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "that's how we are going to evaluate it.",
      "offset": 1901.12,
      "duration": 3.039
    },
    {
      "lang": "en",
      "text": "We'll also have the tool selection",
      "offset": 1902.88,
      "duration": 3.279
    },
    {
      "lang": "en",
      "text": "quality metric but that doesn't tell us",
      "offset": 1904.159,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "the whole story. It just tells a very",
      "offset": 1906.159,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "granular story. It's a debugging metric",
      "offset": 1908.48,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "but this is like an overall performance",
      "offset": 1910.64,
      "duration": 5.919
    },
    {
      "lang": "en",
      "text": "metric for the user. And then so",
      "offset": 1912.559,
      "duration": 5.761
    },
    {
      "lang": "en",
      "text": "essentially a user workflow will be like",
      "offset": 1916.559,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "in the realistic production workflow is",
      "offset": 1918.32,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "that uh they have this agent working and",
      "offset": 1920.32,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "in certain scenarios the action",
      "offset": 1922.559,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "completion is method is lower so they",
      "offset": 1924.32,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "will go there and then check what went",
      "offset": 1926.399,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "wrong and then also they might see some",
      "offset": 1928.159,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "tool selection issues and they will",
      "offset": 1930.32,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "debug them.",
      "offset": 1932.08,
      "duration": 5.839
    },
    {
      "lang": "en",
      "text": "Got it. Um perfect. Uh I think that's a",
      "offset": 1934.799,
      "duration": 6.081
    },
    {
      "lang": "en",
      "text": "that's a very comprehensive recap. Um I",
      "offset": 1937.919,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "like the I like the new direction for",
      "offset": 1940.88,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "for V2. It sounds like a lot of work.",
      "offset": 1942.559,
      "duration": 5.041
    },
    {
      "lang": "en",
      "text": "So good luck with that.",
      "offset": 1945.919,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "It's difficult to tune the promps uh to",
      "offset": 1947.6,
      "duration": 5.439
    },
    {
      "lang": "en",
      "text": "get the right direction of what you want",
      "offset": 1950.48,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "to achieve because",
      "offset": 1953.039,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "you want to be realistic and we the the",
      "offset": 1955.519,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "hard part for me that I have spent a lot",
      "offset": 1958.559,
      "duration": 5.441
    },
    {
      "lang": "en",
      "text": "of time was generating hard scenarios",
      "offset": 1961.039,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "um because",
      "offset": 1964,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "you take a hard you take the best model",
      "offset": 1965.76,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "to generate hard scenarios and you then",
      "offset": 1967.84,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "see if the best model can accomplish",
      "offset": 1970.24,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "those task or not like let's say I",
      "offset": 1972.159,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "generate by 3.7 and then check on 4.1",
      "offset": 1974,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "one and then it's able to accomplish. So",
      "offset": 1976.399,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "I had to kind of do a lot of iterations",
      "offset": 1978.96,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "on like finding where the model fails",
      "offset": 1981.6,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "and how to trigger this kind of",
      "offset": 1984.24,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "scenarios. Uh and that took a lot of",
      "offset": 1985.679,
      "duration": 6.081
    },
    {
      "lang": "en",
      "text": "time. So but I think we have got we have",
      "offset": 1988.08,
      "duration": 6.079
    },
    {
      "lang": "en",
      "text": "reached a certain stage where we feel",
      "offset": 1991.76,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "confident that yes they're realistic and",
      "offset": 1994.159,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "hard scenarios and the model is not able",
      "offset": 1996.399,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "to accomplish all the time. So this is",
      "offset": 1998.48,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "going to like reset this is going to",
      "offset": 2000.159,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "calibrate the expectation of the users",
      "offset": 2002.88,
      "duration": 5.519
    },
    {
      "lang": "en",
      "text": "uh the of the LLMs because our last",
      "offset": 2005.519,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "leaderboard was saying that models are",
      "offset": 2008.399,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "really great at tool calling. So it's",
      "offset": 2010.24,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "like safe but they're actually not",
      "offset": 2012.559,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "right. They're making mistakes and this",
      "offset": 2013.84,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "is going to recalibrate the expectation",
      "offset": 2015.76,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "of the users that be careful because",
      "offset": 2017.679,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "they're still not perfect in real",
      "offset": 2020.72,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "scenarios.",
      "offset": 2022.48,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "Awesome. Okay, I think that's about uh",
      "offset": 2024.72,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "all we have time for. Uh, anything else,",
      "offset": 2027.039,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "Allesio? Any anything else,",
      "offset": 2029.279,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "thanks for joining us, Prateik.",
      "offset": 2031.2,
      "duration": 6.199
    },
    {
      "lang": "en",
      "text": "My pleasure. This was awesome.",
      "offset": 2033.279,
      "duration": 4.12
    },
    {
      "lang": "en",
      "text": "[Music]",
      "offset": 2038.47,
      "duration": 11.959
    }
  ],
  "cleanText": "Hey everyone, welcome back to another\nLaten Space Learning Pod.\nThis is Allesio, partner and CTO at Desible, and\nI'm joined by my co-host Wix, founder of\nSmall AI.\nHello.\nHello.\nAnd uh today we are excited to talk about agent evals and leaderboards.\nUh Pratik uh\nI I think I'm missing out the intro, but\nPratik Bhavsar, uh you're uh from Galileo Labs and you worked on Agent Leaderboards this year.\nHey folks, uh it's so cool to be here\nand I'm excited to chat with you on\neverything about agents today.\nYeah.\nAwesome.\nUm yeah, so I think you know I I first got uh notice of this thing in our discord when Erin Staples, uh who I think who works with you guys, uh posted it and said that she was also contributing to it.\nI think the broad thing is there's a big shift in eval maybe which quickly gets saturated.\nI think Ella Marina also had some controversies\nand I think more generally people want agent evals anyway where they are evaluating the ability of uh you know models to do real tasks and uh instead of like testing knowledge they're actually you know doing tool calling and um utilizing context very well, that kind of stuff.\nSo um you know I think one of those things where this is one of those things where I think there's a lot of fresh uh area and potential for progress if you can do a credible eval.\nAnd the other thing I like to see is an eval that has a cost awareness component.\nUh because I think uh you know people don't really uh stay very honest with like oh yeah like we spent a lot more tokens or we spent a lot more money on this agents and we did better but it's not really clear like if the other comparison could also just do the same thing like just think harder and also have the same performance.\n100%.\nYeah.\nSo we have been hearing from our customers that they are now moving towards agents and it become very clear to us that last year we did rag evaluation because that's what we're discussing more with the customers but uh we quickly understood that this is going to be a year of agents and what we have understood from our previous experiments is that it's not necessary that what you see as top models in let's say al marina or other specific evaluation or general evaluations they might not be also the same ranking for other task like agentic task.\nSo that was our goal uh very fundamentally speaking that we want to get very specific insights apart from just performance and cost was the other thing that comes up a lot with enterprise customers and that was the second focus essentially and then there's SLM versus LLM kind of debate are small models also catching up and there is the big one open source versus private so those were the dimensions we were interested in\num and in the email itself um the big thing that you highlight is obviously tool selection\nand tool use and then single versus multi-turn interaction maybe for for people that are just learning about the Agent Leaderboard now just give a high level overview of what are like the key things that you test for.\nSure.\nSo um Agent Leaderboard is kind of uh eval harness specifically created to test LLMs on hundreds of domains.\nUh we know that there are certain research papers from top companies and research institutes uh from Berkeley from Salesforce from other companies which are targeting certain domains for evaluation but what our goal was that let's take all of these benchmarks because it's possible that certain models are good at certain data sets there might be issues of leakage also so we wanted to create a as much possible holistic benchmark by taking on data sets, slices on single turn, multi-turn and also on like different kind of edge cases like there is some information missing for the parameter the tool itself is missing and long context and all those problems all at once called composite scenario all those things we wanted to take from these different data sets.\nWe did filtering on it for as hard as possible.\nUm and then we created 14 small data sets.\nUh and then evaluated LLM specifically for tool calling.\nUh and we use it used a Galileo metric called tool selection quality which is an LLM judge based metric powered by GPT4 and it has multiple judges so that we get higher performance and that's how we kind of get the score for each sample and we aggregate for a data set and then we do average over the data set and that's how we get the overall score for the ranking but yes customers uh sorry uh readers can also go through particular performances of all certain data sets as well.\nUm apart from that uh we were pragmatic about uh selection in a way that we were very interested in how the small models perform as well as opensource model performs because we know that there can be interesting insights.\nHistorically what we have seen from our previous initiatives is that okay maybe the best GPT or best claude is the top model but there might be very small gap with the model just below it and then it becomes a cost performance tradeoff so that users can kind of select a model which is a slightly worse but also gives them a similar performance and there can be some hidden models that nobody's talking about because it's all about the latest top releases and not about like the mid tier or slightly lower tier.\nuh models uh from other providers and sometimes it's just they haven't got enough traction on social so they might be just missing from the discussions.\nSo we tried to find this kind of models uh and get insights as much as possible.\nSo that that was the whole initiative.\nUh we launched in February this year.\nUh previously we had done rack specific evaluations.\nNow a small update on the Agent Leaderboard also like we are going to do a V2 of it because we find certain issues with this evaluation and let's talk about this more at the end of the podcast like what's next coming uh we can dive more into that\num what are some of the surprising results for me it was Mistral small being the best open source model above 40 and deepseek v3 um Anything else that that jumped out to you?\nOh yes.\nSo first and foremost I would say that I was expecting GPTs to perform the best because they kind of pioneered the tool calling function calling support and they have been working on that for so long.\nUm there was some criticism around Germany models although they were catching up and I was very surprised to see when we released this leaderboard.\nOf course, there has been some shaking of the leaderboard also later on, but when we released, we found that Germany models are performing really good and they were extremely cost efficient at the time compared to the second and second model like GP4 at the time.\nAnd so it became kind of a no-brainer that if anybody wants top performance, they can just directly go and go with the Gemini models.\nAnd they just not only released like the Pro model, they also released the flash version later on and flashlight.\nof the flashlight is a very interesting model.\nUh in in the blog we have the models when we released this leaderboard but in the on the original leaderboard.\nWe also have flashlight which is nobody talks about I think even now but I think it's got a very decent score which is like in the top five models.\nIt's extremely cheap model like it's so dirt cheap that it makes a lot of sense.\nOnly model that is cheaper than it is and performant is from Amazon uh which also nobody talks about.\nSo we found this interesting models uh that worked surprisingly well.\nThen another surprise for me was that the reasoning models were not performing well enough.\nuh they had certain kind of limitation when we probed into it like why are they scoring less overall they were like the 01 the O4 03 they were not performing well specifically for multiple tool call outputs that you have this request and it requires either the same tool being called multiple time or different tools being called all at once and they were outputting only single tool call um and because our evaluation require required that okay all the tools should be outputed at once that's the instruction as well in the prompt uh it was not following and it was getting lower score so because of certain inherent characteristic of specifically O series model uh we did not find them well to be performing on this kind of data set we had like multiple data set on the multiple tool output uh and that brought down the score for this model and of course they are extremely costly right so then it became uh it becomes straightforward that you should not be using this for tool calling and instead go for the regular models probably.\nUm apart from that on the open source side I would say that I was very impressed by Mistral's entry into the ecosystem.\nUm 3.1 just straight away went up and it also very costefficient.\nSo I think people not people might have known at that time and in the launch blog also there was not much discussion on tool calling but the model performed very well on our benchmark.\nYou mean meta right?\nYou you said mistral but you meant meta.\nNo I meant mistral only.\nYes, Mr. 3.1 performing really well on our benchmark.\nAnd the of course the other the surprise was that all the Llama models were not performing well on our benchmark.\nUh 3.3 and even the Llama 4 all were really performing extremely poor.\nSo they are like\noh no\nand that was unbelievable like it felt like they were not tuned for tool calling at all in some way.\nUm so I mean historically it's all historical right there were certain biases and we like meta 1 the llama 1 was great llama 2 was great three also was great and then suddenly something happened and it went downhill so these are like surprising results for us I would say when we started the leaderboard plot 3.5 was somewhere in the middle um it was really good at certain other things but for tool calling it was not awesome but then the when we released the leaderboard and just in a league that launched 3.7 and that went straight up and nobody has beaten it so far.\nWe have not added the latest one um like a quen models u also the latest mistral models and not the 03 pro models which just came out in last few days we not evaluated but it it held the for many many weeks.\nSo uh that was very impressive release I would say.\nAwesome.\nUm, you know, I think, uh, that's a very good overview, uh, very good findings.\nI think people can use this leaderboard to pick out what base model that they want to use for their agents.\nUm, and I think that's a really good, uh, starting point.\nUh, I wanted to dive in a little bit on a little bit a few of the metrics that you chose.\nUh so first of all, did you just did you just rerun um all these evals basically the BFCO, the XLAN\nor do you um do you take it off of um whatever they publicly report?\nNo, we actually took the data slices from the original data set\nand then ran it ourselves.\nUh we got the outputs uh and then we evaluated with our metrics.\nSo it's fresh.\nEverything is fresh.\nOkay.\nEverything's fresh.\nYeah, as long as it's like you know comparable and um ba basically like do you think that these are do you have any comments on just like BFCO versus XLAN versus you know your uh your own TSQ thing?\nUm how do they how do they differ in methodology?\nLike what should people know about their pros and cons?\nUh so I feel like they are very similar in nature.\nIt's just that uh the data set is from different domains and certain data sets have multi-turn conversions available while certain do not have it.\nI do not remember exactly which one has it and which one doesn't but there were this discrepancies and we really enjoyed like I really loved to benchmark also because it had particularly hard evaluation data sets and a lot of models had some struggle over there and\nI think Clay uh Clay from Sierra actually just released an update to Tau Bench for some reason.\nUm,\noh,\nyeah.\nUh, Allesio's best friend now.\nUm,\nyeah, super.\nSo, he he he released Tau Squared.\nWow.\nI was not\nmotion of dual.\nIt was yesterday.\nI just I happened to see it.\nUm,\nto coordinate, guide, and assist the user.\nYeah.\nWonderful.\nSo, so Tau Bench, uh, I actually didn't know it was from Sierra, uh, but it's it's actually very very uh, influential.\nYeah.\nCan you talk more about Tau Bench?\nOh, yes.\nSo, essentially Tau Bench when it was released, I mean, yes, the top bench is for two two domains.\nIt test agentic capabilities for two domains, which is airline and retail.\nAnd their setup is that they have very long prompts in the beginning with lot of request\nand then you have these tools defined and the tools can interact with certain databases for read and write operations and it is predefined that once the whole agentic task is done then they will validate on the final uh edits made on and they'll check for the expected value in the data sets uh in the data database which are basically CSV style of files.\nUh and in that fashion they will validate whether the task was done or not.\nSo it's it's it's pretty well defined such that you get a deterministic evaluation all the times.\nThat's the beauty of this and it's it's domain specific which I think is missing from other evaluation data sets.\nUh and that is also one of the themes for our V2.\nUm because I think people don't just come from a general mindset right people in the industry come from let's say I'm from healthcare I want to see if my if this models work for healthcare or not people are coming from investment finance insurance industry and they want to know that is are the models ready for my domain or not because we all know that things work on the general domain side and when you just put it on the internal data things start to blow up with internal terminologies and the cases that happen.\nSo I think that was the beauty that they specifically targeted domains.\nI don't know if they might have expanded domains in this because two was I felt like less but it was a decent start.\nIt was more about the methodology rather than the domains I feel with the towel\nit was very hard as well.\nYeah.\nSo squared you know just kind of looking at it now squared doesn't seem like it expanded domains.\nUh it just they they expanded the way that they interact with the user.\nMhm.\nSo, some kind of uh Oh, they they added a telecom domain.\nOkay, there we go.\nI see.\nBut I I I if it seems like the emphasis was on more sort of um co-piloting\nuh rather than uh two call just, you know, they call it dual control.\nYeah, we'll we'll see.\nWe'll see what this is.\nI It's It's obviously very new, so I just saw I just happened to see it the other day.\nUm solid.\nYeah.\nYeah, definitely put that in your uh V2.\nUm and then yeah and I also wanted to go back to um TSQ.\nI think that's the other sort of maybe uh novel part.\nUm I think a lot of people have queries about LLM as judge\nand um yeah just tell us more like you know do you have any calibration errors?\nUm how how uh do should more people adopt this kind of approach?\nIndeed I agree.\nLLMs have been notorious with their own biases, right?\nWe have seen historically\nthey have issues with long context and they have issues with certain kind of outputs.\n\n\nThey prefer, let's say, lengthy outputs and stuff.\nUm, what we, so this, our philosophy in the company is that any metric that comes out, which is a Galileo metric, we do extensive research on it.\nUh, we create very hard data sets internally, um, and we evaluate it, evaluate them, we annotate them and check for correlations, and only once we feel comfortable, then only we release it, and which is usually based on the correlation scores, like AU ROC scores that we find.\nIf they are high enough, only then we release it, otherwise it goes through a rigorous process of prompt optimization.\nSo we allow people to use any LLM with the metric.\nUh, and so because LLMs keep on improving, the metric also keeps on improving, and then the prompts is what we kind of, uh, the prompt is where all the research goes through, and we have seen tremendous value in that.\nAnd yes, people would definitely have skepticism.\nA lot of people, a lot of our customers also have that skepticism.\nSome are still stuck on blue and root score, so which is surprising because they think that LLMs are not there yet.\nBut we, what we try to do from our side is essentially, we try to follow the best practice of prompting.\nUh, we try to exploit the reasoning as much as possible from these LLMs, and then we use basically self-consistency by using multiple outputs and then taking a majority voting over there.\nPeople can, people have flexibility on these things.\nThey can select the LLMs they feel they want to use.\nIf they want to use a much more powerful LM, go ahead.\nYou can customize it.\nIf you want to change the prompts, go ahead, customize it.\nIf you want to increase the number of judges, go ahead and customize it.\nSo, we allow full flexibility to people to modify these LLMs judges so that they can get the best performance possible.\nUh, of course, people should always validate with some small annotated data set before they go ahead and just make it the default evaluation.\nSo that is always a part of our PCs, and we, they get clarity and confidence, only then they will be, they are using it.\nSo I feel, I have basically, I have been a full-stack ML engineer previously, um, and I've worked a lot with annotators, and I have seen a lot of biases and issues with annotators as well.\nUm, it's just so time-consuming that you can't afford to kind of wait for annotation to get completed.\nSo it's just much, much better to add more instructions, do some initial validation, and get the clarity that, okay, this is where it works.\nIf it doesn't work, yes, go back to annotation, but I feel like it has started working a lot one year back because we, I, I was running these experiments, like we were working with hall simulation index two years back, uh, where we using our own similar LMS as judge methodology, and we had certain issues, and there were some correlation issues at the time, but last year it got pretty strong, and this year I feel it's so strong, LLMs judges is so strong that you can use it.\nI've gone through like hundreds of explanations, like our LLMs also generate explanations that users can read to validate whether it actually went through this correct reasoning or not, and I have gone through many explanations in the during this V2 experiment that I'm doing right now, and also the V1, and I did not find any issues.\nSo, uh, I feel like when we, so that's why we released all the data set.\nPeople can go and run the experiment with the V2 also.\nWe're going to release the data sets.\nPeople can just take them and run it on our, uh, cluster.\nIt's free.\nOur, our product is free, so anybody can just sign up and start using it.\nThey can just put the open a key, and they will get a clarity, okay, it's working or not.\nWe want to be as open as possible while still being a closed-source company, so I feel like it's working, and people should definitely pay attention to it.\nWhat, what's the gap between the best prompts and like the worst prompts?\nLike, when you run, as, as you iterated on on the prompting of the LMS judge, did you see models change in like position in the leaderboard, or were the results mostly the same, just like difference scores?\nUh, so I was not working on fine-tuning the LM as a judge.\nIt is done by a separate team member, and once the metric is ready, only then I start using it, but, um, that was, they went through like a months of rigorous process of prompt tuning, and, uh, it's a very structured prompt.\nIt's like an art in itself, as you know, where we have certain sections for certain things on certain instruction, and what we do is like, it's a very simple philosophy.\nWe start with a base prompt with what we think is correct, and then we see where it's failing, and then we try to add those instructions again, uh, defining the failure cases in a general way to the prompt of the LMS just and see if it's following it or not, and that iteration goes on for multiple times till we are sure that we are getting the performance that we need.\nSo it's, it's very common sense-oriented approach, I would say, and it's just plain English writing with just following the right way of writing prompts.\nUm, and of course, the leaderboard, if you are using different judges with different models, it can, there can be heavy shakeup of the leaderboard also.\nSo that's why we use the best LLM available at that when we are run, when we are starting or near completion of the experiments.\nWe try to use the best LLM available so that it should not be a situation where you're trying to evaluate, uh, 3.7 with GPT4 mini, right?\nIt will be really, uh, risky proposition because GPD4 mini can't find, uh, cannot evaluate the hard outputs that 3.7 might be doing correctly, right?\nUm, so, and so for this V2, I'm using the 3.7 because it is one of the top training models right now, and I've seen really good performance from it.\nSo we essentially swap these models as much as possible when we are running the experiment, otherwise it looks way, we have seen situation previously where we were using probably GP4 turbo or something, or even GP 3.5 long back, I'm talking like two years back, um, and we had to use GPT4 Turbo at the time because with the 3.5, it was something else entirely because it can't find the errors.\nMhm.\nOkay.\nUm, awesome.\nUh, maybe give a little preview of the thinking for V2, and, uh, maybe that, that will be, uh, where we end up.\nBut, uh, yeah, I'm just kind of curious.\nSure.\nUh, what we found issues with V1 is, uh, it has a lot of domains, but it does not have a domain vertical in itself, that, okay, if somebody is interested on how this model performance, let's say, telecommunication, they can just select, filter, and then see which, what's the performance from each model, what are the gaps, what's the cost related to them.\nUm, so first thing first, we are doing domain-specific, um, which means that these are like new data sets from scratch, and why are we doing this?\nUh, is, of course, the first problem is like they are not available easily.\nSecondly, it's because, which is I would say the main reason, is that we don't want leakage at all to be possible in any of the LLM so far, so we want that these are new data sets, so we guarantee that it's going to be performing as unbiased as possible.\nThen we saw with V1 that the scores are saturating.\nWe see that the best score has already reached 0.95.\nAnd so if we are going to evaluate even better models that are coming out, they'll be definitely in the range of 0.9 to 1, which I feel is not very informative for anybody to take a call because there can be some noise also in this evaluation.\nSo we want the benchmarks to be harder, uh, and this tool calling to be more complicated and realistic at the same time.\nUh, this BFCL, if you just open and check it, uh, you would be like, okay, this is okay, but you don't want to use it for realistic evaluation because it's like some math problem statement is given, some simple tool is given, just like, let's say, some exponential tool, logarithmic tool, multiplication and stuff, and you are just like evaluating on that.\nSo I was not particularly happy with that, but that's the best we could got get at the time, um, and that's why we mixed up all the data sets that's available at the time to kind of average out the things, uh, because it's very specific, right?\nIf some model is good at math, then it might perform really great for DFCL, but if some model doesn't understand the terminologies of industry, uh, then it will not perform well for tow, so we kind of mixed up the data set during that time.\nUm, so two things, uh, domain, and one is increasing the hardness.\nThird thing is multi-turn evaluation.\nSo this whole evaluation is going to be, let's say, a support bots.\nThe whole theme is evaluating support agents with a lot of tools, like we are having 20 tools.\nWe're going to have five domains, uh, some like banking, finance, telecommunication, insurance, healthcare.\nUm, and we'll be having certain categories also, like coordinate the tools, and then kind of adversarial scenarios, customers manipulating, uh, the bot to kind of do certain things.\nAnd, uh, uh, yeah, multi-turn, coming to multi-turn, the in the multi-turn, what we are doing is we, this time, last time it was more like there's an input with one message or multiple messages and then give one output.\nThat was how we evaluating.\nThis time it's like a realistic scenario that we start with a scenario.\nUh, scenario has a first message, and there's like goals to be accomplished.\nThis scenario is given to a user simulator.\nNow user simulator is basically a prompt, uh, which gets the whole history of the chat, and it's supposed to generate a new message, and that simulation is ran by GPT 4.1 mini model.\nNow there is another simulator, which I called the tool simulator.\nWhy we selected tool simulator?\nEssentially because of ease of running this simulation.\nUh, as we saw with toao, toao has like tool definitions and, uh, databases with CSV file kind of thing with red operations going on.\nNow I wanted to have like many tools created.\nWe have like 20 tool per domain, and we have like five domain.\nSo it's like 100 tools.\nI did not have, do not have the time to kind of write this, uh, tool with all these databases.\nSo I kind of created a tool simulator again with the GP4.1 mini model, uh, which takes into the input as a tool call.\nSo it gets the tool call, it gets the response of the tool to tool call schema, and then it is supposed to give an output.\nSo rather than like interacting with an API or calling certain databases, it is going to generate an output based on the history of the conversation and the tool call information that is given.\nAnd the main one that we are evaluating is basically the agent.\nUh, the agent we are swap swapping all the LLMs to evaluate, and this agent basically is accomplishing the task given by the user, and then user gives a task.\nThe agent either gives a response or gives the tool call.\nIf it's a tool call, it's given to the tool simulator, and the tool simulator will give an output, and then that output will be used by the agent to give a response to the user.\nSo what we do in this is we, for every scenario, we have like at least like five to seven goals to be accomplished.\nUh, they are pretty complex, like toao, and they are all, they all need to be accomplished, otherwise it's the task is not accomplished.\nSo this time you have to get done many things at once, which makes the thing hard, and it's possible that you might miss out on certain things, and we see that the best models are good at it, but certain models are really struggling on it, and they can go into iterations for no reason.\nUm, and so the scores are coming out to be lower, much lower, like the pro, the main problem, 0.95 is like solved, that, okay, the scores are lower, we are getting domain-specific insights, uh, and then it's a very realistic scenario where you are having multiple turn, and then only finally when everything is done, then the task is accomplished.\nSo what's, how do we evaluate the performance in this case?\nAnd we use two metrics.\nWe use the older one, the tool selection quality, which is just telling us whether the tool selected was correct or not, but other thing which is super important from a business point of view and practical point of view is that whether the agent completed all the task or not, right?\nUh, so for that we have the action completion metric, uh, which will check that whether all the requests that were given by the user, where all of them accomplished and were they accomplished correctly or not.\nSo for every sample, we'll get, okay, this is how good it is at tool selection, this is how great the model was at accomplishing the task.\nSo in that fashion, we'll get the scores for every scenario, and then we'll aggregate the scores for all the data sets to get the final score.\nSo this time we are kind of focusing more on action completion metric, I would say, which is telling us that the user asked for this five things, and whether the agent completed all the five things or not.\nSo that's how we are going to evaluate it.\nWe'll also have the tool selection quality metric, but that doesn't tell us the whole story.\nIt just tells a very granular story.\nIt's a debugging metric, but this is like an overall performance metric for the user.\nAnd then, so essentially a user workflow will be like in the realistic production workflow is that, uh, they have this agent working, and in certain scenarios the action completion is method is lower, so they will go there and then check what went wrong, and then also they might see some tool selection issues, and they will debug them.\nGot it.\nUm, perfect.\nUh, I think that's a, that's a very comprehensive recap.\nUm, I like the, I like the new direction for, for V2.\nIt sounds like a lot of work.\nSo good luck with that.\nIt's difficult to tune the promps, uh, to get the right direction of what you want to achieve because you want to be realistic, and we, the, the hard part for me that I have spent a lot of time was generating hard scenarios, um, because you take a hard, you take the best model to generate hard scenarios, and you then see if the best model can accomplish those task or not, like, let's say, I generate by 3.7 and then check on 4.1 one, and then it's able to accomplish.\nSo I had to kind of do a lot of iterations on like finding where the model fails and how to trigger this kind of scenarios.\nUh, and that took a lot of time.\nSo, but I think we have got, we have reached a certain stage where we feel confident that yes, they're realistic and hard scenarios, and the model is not able to accomplish all the time.\nSo this is going to like reset, this is going to calibrate the expectation of the users, uh, the of the LLMs, because our last leaderboard was saying that models are really great at tool calling.\nSo it's like safe, but they're actually not, right?\nThey're making mistakes, and this is going to recalibrate the expectation of the users that be careful because they're still not perfect in real scenarios.\nAwesome.\nOkay, I think that's about, uh, all we have time for.\nUh, anything else, Allesio?\nAny, anything else, thanks for joining us, Pratik.\nMy pleasure.\nThis was awesome.\n[Music]\n",
  "dumpedAt": "2025-07-21T18:43:26.011Z"
}