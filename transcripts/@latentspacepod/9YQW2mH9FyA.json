{
  "episodeId": "9YQW2mH9FyA",
  "channelSlug": "@latentspacepod",
  "title": "The Utility of Interpretability â€” Emmanuel Amiesen",
  "publishedAt": "2025-06-06T17:31:42.000Z",
  "rawLines": [
    {
      "lang": "en",
      "text": "All right, we are actually going to",
      "offset": 4.08,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "record this as a intro to the main",
      "offset": 6,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "episode, but here we have my trusty",
      "offset": 8.96,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "co-host, guest host, I guess, Vivu. Um,",
      "offset": 11.36,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "as well as Emanuel from Enthropic. We're",
      "offset": 14.4,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "going to talk about the circuit tracing",
      "offset": 16.56,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "stuff and all the interpretability work.",
      "offset": 19.039,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "But Emanuel, maybe you want to do a",
      "offset": 20.72,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "quick self intro because before we get",
      "offset": 22.4,
      "duration": 5.119
    },
    {
      "lang": "en",
      "text": "into it. Yeah, sure. I'm Emanuel. I work",
      "offset": 24.32,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "on the uh interpretability team here at",
      "offset": 27.519,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "Anthropic. More specifically on the",
      "offset": 30.16,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "circuits team. So we recently released a",
      "offset": 31.84,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "pair of papers about sort of like the",
      "offset": 34.399,
      "duration": 3.281
    },
    {
      "lang": "en",
      "text": "work that we've been doing over the last",
      "offset": 35.92,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "months and even more recently we",
      "offset": 37.68,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "released some some code in partnership",
      "offset": 39.76,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "with with the anthropic fellows program.",
      "offset": 41.76,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "It was it was mostly built by anthropic",
      "offset": 44,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "fellows that lets people play with the",
      "offset": 45.52,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "research basically. And so happy to talk",
      "offset": 47.84,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "about that and and we also hope to kind",
      "offset": 49.44,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "like keep releasing more things and and",
      "offset": 51.44,
      "duration": 2.959
    },
    {
      "lang": "en",
      "text": "partner with other groups that are",
      "offset": 53.28,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "working on on similar stuff. Yeah,",
      "offset": 54.399,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "amazing. Uh, we'll get deeper into like",
      "offset": 56.64,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "the behind the scenes on on the main",
      "offset": 58.64,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "podcast, but let's maybe just dive right",
      "offset": 60.16,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "in into what you release because that's",
      "offset": 62.399,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "the most topical thing. This is like",
      "offset": 63.92,
      "duration": 2.879
    },
    {
      "lang": "en",
      "text": "literally you just launched it like",
      "offset": 65.519,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "yesterday and we'll probably release it",
      "offset": 66.799,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "in release this episode in a few days.",
      "offset": 68.72,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "So, yeah, like what can people do or",
      "offset": 70.4,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "what do you recommend people try?",
      "offset": 72.479,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "Totally. So like at a really high level,",
      "offset": 73.84,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "you know, the the sort of like idea of",
      "offset": 76.56,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "the research itself is to try to explain",
      "offset": 78.72,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "sort of like some of the computation",
      "offset": 82.32,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "that a model did when when it predicted",
      "offset": 84.32,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "a given token. And so in our paper, we",
      "offset": 87.28,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "kind of like show how to do this and",
      "offset": 90.24,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "then we show examples of us doing this",
      "offset": 91.6,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "on internal private models. And then the",
      "offset": 93.04,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "the release this week sort of lets",
      "offset": 95.36,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "anyone do it for a set of open source",
      "offset": 98.079,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "models. So notably maybe the most easy",
      "offset": 100.96,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "one here is like Gemma 2 to 2 tob. So",
      "offset": 103.04,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "you can sort of like think of of some",
      "offset": 105.04,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "prompt and you kind of like can explain",
      "offset": 107.04,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "any any like token that the model sample",
      "offset": 109.6,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "samples and explains here means just",
      "offset": 111.68,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "like basically blow up the internal",
      "offset": 113.68,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "state of the model and like show all of",
      "offset": 115.439,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "the sort of like intermediate things",
      "offset": 117.36,
      "duration": 3.119
    },
    {
      "lang": "en",
      "text": "that the model was thinking about before",
      "offset": 118.719,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "it got to like the final token that it",
      "offset": 120.479,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "predicted.",
      "offset": 122.159,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "Yeah. So what some of the things that",
      "offset": 123.68,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "you guys put out is kind of in the",
      "offset": 125.6,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "circuit tracing you have a few core",
      "offset": 126.96,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "examples, right? So like we can see how",
      "offset": 128.72,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "these models have internal reasoning",
      "offset": 130.399,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "states and there's like multi hop",
      "offset": 132.4,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "reasoning and some of the stuff that we",
      "offset": 134.16,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "talked about on the podcast was how can",
      "offset": 135.76,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "people that are interested in how models",
      "offset": 137.92,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "work kind of do anything right so what",
      "offset": 140.4,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "are open questions how can people",
      "offset": 142.239,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "contribute and it seems like you know",
      "offset": 143.92,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "the followup is okay it's been a few",
      "offset": 145.599,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "weeks here's a huge library so you know",
      "offset": 147.12,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "I guess before we even get into it what",
      "offset": 149.44,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "are some open questions that you would",
      "offset": 151.599,
      "duration": 2.881
    },
    {
      "lang": "en",
      "text": "expect people to like kind of play",
      "offset": 153.12,
      "duration": 2.64
    },
    {
      "lang": "en",
      "text": "around with you know what what are",
      "offset": 154.48,
      "duration": 3.039
    },
    {
      "lang": "en",
      "text": "people like going to do what why should",
      "offset": 155.76,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "we probe Gemma llama uh what are",
      "offset": 157.519,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "interesting things we can do and any",
      "offset": 159.92,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "tips on using it. Yeah, I I think",
      "offset": 161.28,
      "duration": 6.88
    },
    {
      "lang": "en",
      "text": "there's maybe like two to three like",
      "offset": 163.84,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "categories of things that people could",
      "offset": 168.16,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "do. So I'll go from sort like the most",
      "offset": 169.68,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "basic kind of low effort to you know hey",
      "offset": 171.12,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "if you want to dedicate like a month of",
      "offset": 174.16,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "your life you could do that. Um the the",
      "offset": 175.76,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "sort of like most basic thing is you",
      "offset": 177.68,
      "duration": 3.88
    },
    {
      "lang": "en",
      "text": "know um",
      "offset": 179.36,
      "duration": 5.599
    },
    {
      "lang": "en",
      "text": "so it's a Gemma 2 and Lama 1B are like",
      "offset": 181.56,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "smaller models uh but they can still do",
      "offset": 184.959,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "a bunch of stuff and so and and for most",
      "offset": 186.56,
      "duration": 2.959
    },
    {
      "lang": "en",
      "text": "of the things that they can do we still",
      "offset": 188.319,
      "duration": 3.041
    },
    {
      "lang": "en",
      "text": "kind of like don't really know or have a",
      "offset": 189.519,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "good mental model of how it is that they",
      "offset": 191.36,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "do the things that they do. So to give",
      "offset": 193.2,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "you an example like one of the things in",
      "offset": 194.959,
      "duration": 3.121
    },
    {
      "lang": "en",
      "text": "the paper is this sort of like multihop",
      "offset": 196.4,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "reasoning where you know we ask you know",
      "offset": 198.08,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "like cloud work hiku like oh the capital",
      "offset": 200.4,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "of the state where Dallas is is Austin.",
      "offset": 202.879,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "It turns out that like Gemma can do this",
      "offset": 205.84,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "also. And so as part of the release, we",
      "offset": 207.76,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "have, you know, a notebook where Michael",
      "offset": 209.68,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "Hannah, one of the anthropic fellows,",
      "offset": 212,
      "duration": 2.4
    },
    {
      "lang": "en",
      "text": "kind like walks through a bunch of",
      "offset": 213.28,
      "duration": 2.879
    },
    {
      "lang": "en",
      "text": "examples, including this one. And it's",
      "offset": 214.4,
      "duration": 3.119
    },
    {
      "lang": "en",
      "text": "really cool cuz you can see that",
      "offset": 216.159,
      "duration": 5.041
    },
    {
      "lang": "en",
      "text": "actually the way the circuit looks in",
      "offset": 217.519,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "Jamaim looks in like a huge model, which",
      "offset": 221.56,
      "duration": 5.08
    },
    {
      "lang": "en",
      "text": "that in it in itself is is I think like",
      "offset": 224.4,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "a pretty novel discovery. It's like, oh,",
      "offset": 226.64,
      "duration": 2.4
    },
    {
      "lang": "en",
      "text": "you have these models that are like",
      "offset": 228.08,
      "duration": 1.84
    },
    {
      "lang": "en",
      "text": "super different. And you know, if you",
      "offset": 229.04,
      "duration": 2.16
    },
    {
      "lang": "en",
      "text": "look at like their evals or if you just",
      "offset": 229.92,
      "duration": 2.64
    },
    {
      "lang": "en",
      "text": "try to use them, they're like just very",
      "offset": 231.2,
      "duration": 2.399
    },
    {
      "lang": "en",
      "text": "clearly different. But for this one",
      "offset": 232.56,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "task, for this one thing, actually, the",
      "offset": 233.599,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "way that this do that they do this",
      "offset": 235.44,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "multi-step reasoning is like the same",
      "offset": 236.959,
      "duration": 2.801
    },
    {
      "lang": "en",
      "text": "way. They actually do the multi-step",
      "offset": 238.48,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "reasoning. In the notebook, there's both",
      "offset": 239.76,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "other examples of kind of like fun",
      "offset": 242.4,
      "duration": 2.559
    },
    {
      "lang": "en",
      "text": "things that we looked at that I think",
      "offset": 243.84,
      "duration": 2.959
    },
    {
      "lang": "en",
      "text": "can sort of spike your interest if",
      "offset": 244.959,
      "duration": 3.121
    },
    {
      "lang": "en",
      "text": "you're new to thinking about this stuff.",
      "offset": 246.799,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "And at the end of the notebook that's",
      "offset": 248.08,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "that's linked in the in the readme,",
      "offset": 249.519,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "there's like three examples of like",
      "offset": 251.2,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "random sort of like cases that we",
      "offset": 253.92,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "haven't solved or or or we haven't",
      "offset": 256.4,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "labeled that you know have like a graph",
      "offset": 258.72,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "precomputed for you and you could just",
      "offset": 260.479,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "look at it and try to like figure out",
      "offset": 261.68,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "what's happening. And by figure out",
      "offset": 263.199,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "what's happening what we mean is you",
      "offset": 264.56,
      "duration": 2.56
    },
    {
      "lang": "en",
      "text": "know we might do like a quick demo here",
      "offset": 265.919,
      "duration": 2.481
    },
    {
      "lang": "en",
      "text": "but it's kind of like look at these",
      "offset": 267.12,
      "duration": 2.639
    },
    {
      "lang": "en",
      "text": "representations try to understand like",
      "offset": 268.4,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "okay like what is the computation the",
      "offset": 269.759,
      "duration": 3.521
    },
    {
      "lang": "en",
      "text": "model is doing and then part of the",
      "offset": 271.12,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "release also lets you like run",
      "offset": 273.28,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "experiments to like verify that you're",
      "offset": 275.919,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "right. So if you think that like you",
      "offset": 277.44,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "know ah the model like first like thinks",
      "offset": 279.36,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "about Texas in this case you can also",
      "offset": 281.12,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "just like stop it from thinking about",
      "offset": 283.12,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "Texas and see if like that damages it.",
      "offset": 284.56,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "And so like the tools to do that are",
      "offset": 286.56,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "available. And so I would say that's",
      "offset": 288.32,
      "duration": 2.879
    },
    {
      "lang": "en",
      "text": "like the first thing is just I think the",
      "offset": 289.28,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "hope is there are a lot of behaviors",
      "offset": 291.199,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "that models do way more than like any",
      "offset": 292.639,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "single group has time to explore. And so",
      "offset": 294.88,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "the hope is like hey pick a behavior you",
      "offset": 296.479,
      "duration": 2.961
    },
    {
      "lang": "en",
      "text": "think is interesting and try to",
      "offset": 298.4,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "understand like what's happening and try",
      "offset": 299.44,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "to ground it out. And it's like the the",
      "offset": 301.28,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "like baseline thing and maybe like the",
      "offset": 303.12,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "the thing that I'm most excited about",
      "offset": 304.32,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "with this release. But then the other",
      "offset": 306.56,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "thing I do want to mention like parts",
      "offset": 308,
      "duration": 4.36
    },
    {
      "lang": "en",
      "text": "two and three are just we also hope that",
      "offset": 309.759,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "like other groups and and kind like",
      "offset": 312.36,
      "duration": 4.04
    },
    {
      "lang": "en",
      "text": "interested researchers can just use this",
      "offset": 314.96,
      "duration": 3.519
    },
    {
      "lang": "en",
      "text": "to like extend the method like if you",
      "offset": 316.4,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "have an idea about how to like do this",
      "offset": 318.479,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "better. You know the whole code to like",
      "offset": 320.16,
      "duration": 3.039
    },
    {
      "lang": "en",
      "text": "make this graph is is open source. you",
      "offset": 321.759,
      "duration": 2.561
    },
    {
      "lang": "en",
      "text": "know, I can take a look at it and just",
      "offset": 323.199,
      "duration": 2.401
    },
    {
      "lang": "en",
      "text": "like try to play with it, try to find",
      "offset": 324.32,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "different ways to like create these",
      "offset": 325.6,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "graphs and also extend it to other",
      "offset": 327.12,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "models, right? Like there are many",
      "offset": 328.8,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "different models and so you know part of",
      "offset": 330.72,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "part of making this work on any model is",
      "offset": 332.72,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "you have to like train the sort of like",
      "offset": 334.08,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "replacement model which again there",
      "offset": 335.52,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "there is code for and there's other",
      "offset": 337.36,
      "duration": 2.64
    },
    {
      "lang": "en",
      "text": "groups working on and so like that's",
      "offset": 338.56,
      "duration": 2.639
    },
    {
      "lang": "en",
      "text": "also something that if you're excited",
      "offset": 340,
      "duration": 2.639
    },
    {
      "lang": "en",
      "text": "about you could say like okay cool well",
      "offset": 341.199,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "I want this to work on like another open",
      "offset": 342.639,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "model and you could sort of like add it",
      "offset": 345.12,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "if you're like you know more into sort",
      "offset": 346.88,
      "duration": 2.56
    },
    {
      "lang": "en",
      "text": "of like maybe like the engineering and",
      "offset": 348.4,
      "duration": 2.32
    },
    {
      "lang": "en",
      "text": "the ML engineering side of things. Uh",
      "offset": 349.44,
      "duration": 2.64
    },
    {
      "lang": "en",
      "text": "yeah, we we actually get into a little",
      "offset": 350.72,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "bit about how you how you guys do the",
      "offset": 352.08,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "extra data viz stuff that makes your",
      "offset": 353.52,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "your blog post pop so much. Should we",
      "offset": 355.6,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "should we um share the screen a little",
      "offset": 357.44,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "bit and and dive in? I think uh you guys",
      "offset": 359.039,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "prep some examples. Totally. Yeah. Yeah.",
      "offset": 361.28,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "It's just like there's nothing better",
      "offset": 365.039,
      "duration": 2.481
    },
    {
      "lang": "en",
      "text": "than the creator of the tool walking",
      "offset": 366.24,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "through the tool and we might as well",
      "offset": 367.52,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "capture that so that you know people who",
      "offset": 369.12,
      "duration": 3.519
    },
    {
      "lang": "en",
      "text": "actually want to do this can follow",
      "offset": 370.8,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "along. Yeah. Uh that makes sense. Let me",
      "offset": 372.639,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "just like actually share my screen. my",
      "offset": 375.44,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "uh one little experiment I basically",
      "offset": 377.36,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "cloned the repo threw it into cloud code",
      "offset": 380,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "and was like you know deal with this",
      "offset": 381.68,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "let's let's try it end to end so would",
      "offset": 383.28,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "recommend you know cloud code is very",
      "offset": 385.36,
      "duration": 5.279
    },
    {
      "lang": "en",
      "text": "good at using this um basically also if",
      "offset": 387.039,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "you're if you're just trying to get",
      "offset": 390.639,
      "duration": 3.041
    },
    {
      "lang": "en",
      "text": "started the circuit tracing tutorial",
      "offset": 391.84,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "notebook very good that kind of goes",
      "offset": 393.68,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "over all the high level and then you",
      "offset": 395.68,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "know shout out cloud code try it out it",
      "offset": 397.44,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "it works very well on this that that's",
      "offset": 399.68,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "awesome to hear actually I might just",
      "offset": 402.16,
      "duration": 2.64
    },
    {
      "lang": "en",
      "text": "open the notebook first just like",
      "offset": 403.36,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "quickly walk through the illustrations",
      "offset": 404.8,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "But yeah, you're the second person to",
      "offset": 406.72,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "tell me that they just had Cloud Code",
      "offset": 408.24,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "sort of like dig in initially on it. So",
      "offset": 409.84,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "I'm glad I'm glad that's working. The",
      "offset": 411.28,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "the tutorial here is like linked at the",
      "offset": 413.84,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "top of the repo and maybe we can link it",
      "offset": 415.68,
      "duration": 2.959
    },
    {
      "lang": "en",
      "text": "from the podcast, but essentially sort",
      "offset": 417.12,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "of like walks you through kind of like",
      "offset": 418.639,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "how to think about graphs and so it",
      "offset": 421.44,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "links to these circuits. So here this is",
      "offset": 423.84,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "the two-step reasoning that we're",
      "offset": 425.759,
      "duration": 2.16
    },
    {
      "lang": "en",
      "text": "talking about. This is kind of like a",
      "offset": 426.96,
      "duration": 2.48
    },
    {
      "lang": "en",
      "text": "schematic of it where it's like the",
      "offset": 427.919,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "capital state of Dallas and it's like ah",
      "offset": 429.44,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "it has to think of Texas and Austin. But",
      "offset": 431.12,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "the notebook links you to all of these",
      "offset": 432.96,
      "duration": 2.959
    },
    {
      "lang": "en",
      "text": "circuits here and this is kind of the",
      "offset": 434.319,
      "duration": 2.801
    },
    {
      "lang": "en",
      "text": "thing that you can play with. So this is",
      "offset": 435.919,
      "duration": 2.921
    },
    {
      "lang": "en",
      "text": "the UI on you know",
      "offset": 437.12,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "neuronipedia that that hostess and that",
      "offset": 438.84,
      "duration": 4.359
    },
    {
      "lang": "en",
      "text": "that lets you like create any circuit.",
      "offset": 441.68,
      "duration": 2.959
    },
    {
      "lang": "en",
      "text": "So here you know we could explore the",
      "offset": 443.199,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "circuit and if you open the notebook you",
      "offset": 444.639,
      "duration": 3.96
    },
    {
      "lang": "en",
      "text": "can explore",
      "offset": 446.479,
      "duration": 5.041
    },
    {
      "lang": "en",
      "text": "it. I'm realizing that I switched tabs.",
      "offset": 448.599,
      "duration": 5.481
    },
    {
      "lang": "en",
      "text": "Maybe I'm not showing it. Okay there we",
      "offset": 451.52,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "go. Can you see the circuit now? I think",
      "offset": 454.08,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "so. Okay cool. Um but you can make a new",
      "offset": 456.8,
      "duration": 4.959
    },
    {
      "lang": "en",
      "text": "graph super easily and quickly. And so",
      "offset": 460,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "maybe this is like the most fun things.",
      "offset": 461.759,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "One I was playing with uh right before",
      "offset": 463.44,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "uh you know joining this call is like uh",
      "offset": 466.24,
      "duration": 6.239
    },
    {
      "lang": "en",
      "text": "it turns out that podcast guests are",
      "offset": 468.88,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "very formulaic and so if you say like",
      "offset": 472.479,
      "duration": 4.84
    },
    {
      "lang": "en",
      "text": "thanks for having me on the",
      "offset": 474.8,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "whatever like Gemma seems to pretty",
      "offset": 477.319,
      "duration": 4.201
    },
    {
      "lang": "en",
      "text": "consistently guess that you're like on a",
      "offset": 480,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "podcast. Uh which makes sense right like",
      "offset": 481.52,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "why would you say thanks for having me",
      "offset": 483.68,
      "duration": 3.519
    },
    {
      "lang": "en",
      "text": "on the blah. Uh and so here we can try",
      "offset": 484.72,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "to say like ah okay like how does Gemma",
      "offset": 487.199,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "know to like complete the sentence with",
      "offset": 489.599,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "thanks for having me on the latent space",
      "offset": 491.68,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "podcast and so here the way you generate",
      "offset": 493.36,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "a graph right is you type a sentence",
      "offset": 495.28,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "where the next word is the thing that",
      "offset": 497.68,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "you're interested in and then you kind",
      "offset": 500.08,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "of try to explain like how the model got",
      "offset": 501.52,
      "duration": 5.56
    },
    {
      "lang": "en",
      "text": "to the next word so here you",
      "offset": 503.12,
      "duration": 7.6
    },
    {
      "lang": "en",
      "text": "can give it a name and then you can",
      "offset": 507.08,
      "duration": 5.16
    },
    {
      "lang": "en",
      "text": "mostly just not worry about any of these",
      "offset": 510.72,
      "duration": 2.48
    },
    {
      "lang": "en",
      "text": "parameters I think if you're just",
      "offset": 512.24,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "playing with it and you can click start",
      "offset": 513.2,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "generation",
      "offset": 515.36,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "and all this generates like something",
      "offset": 517.68,
      "duration": 3.279
    },
    {
      "lang": "en",
      "text": "important for people to know is that",
      "offset": 519.519,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "these are trained on base models, right?",
      "offset": 520.959,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "So they're not chat models. So basically",
      "offset": 522.719,
      "duration": 3.521
    },
    {
      "lang": "en",
      "text": "when you train these models, they're",
      "offset": 524.88,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "just trained to predict next token and",
      "offset": 526.24,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "they don't have that user assistant",
      "offset": 527.92,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "chatbot flow. So they're prompted in a",
      "offset": 529.6,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "way such that you know the output should",
      "offset": 532.16,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "basically just be the next word. Yeah,",
      "offset": 534.08,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "you kind of want to think about it as",
      "offset": 536.64,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "like maybe the the the prompt or the",
      "offset": 538.16,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "text you're making is like the text of",
      "offset": 540.48,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "like a book or an article rather than a",
      "offset": 541.839,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "conversation where it's like, you know,",
      "offset": 544,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "what is a sentence where if you were to",
      "offset": 545.76,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "read it in a book, like the next word",
      "offset": 547.2,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "would be sort of like the interesting",
      "offset": 549.44,
      "duration": 4.519
    },
    {
      "lang": "en",
      "text": "one. Um,",
      "offset": 550.88,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "yeah, you know, you can click on it. Uh,",
      "offset": 553.959,
      "duration": 3.88
    },
    {
      "lang": "en",
      "text": "sort of like takes a little bit of time",
      "offset": 556.8,
      "duration": 2.479
    },
    {
      "lang": "en",
      "text": "to load because there's just a bunch of",
      "offset": 557.839,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "data. So what we're going to show you",
      "offset": 559.279,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "here is basically like almost every",
      "offset": 561.04,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "single feature that activates in the",
      "offset": 562.959,
      "duration": 2.721
    },
    {
      "lang": "en",
      "text": "model the features are these",
      "offset": 564.72,
      "duration": 2.36
    },
    {
      "lang": "en",
      "text": "intermediate",
      "offset": 565.68,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "representations and at the bottom",
      "offset": 567.08,
      "duration": 3.4
    },
    {
      "lang": "en",
      "text": "there's the prompt. So here it's like",
      "offset": 569.04,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "thanks for having me on the laten space",
      "offset": 570.48,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "and at the top you can see like what the",
      "offset": 572.88,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "model sort of like uh output. So it's",
      "offset": 575.04,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "most likely output is it's pretty",
      "offset": 578.08,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "confident that we're talking about a",
      "offset": 579.76,
      "duration": 3.519
    },
    {
      "lang": "en",
      "text": "podcast and you know it has like some",
      "offset": 581.2,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "random stop tokens blog show and then",
      "offset": 583.279,
      "duration": 5.441
    },
    {
      "lang": "en",
      "text": "some stuff that I think like makes less",
      "offset": 587.04,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "sense but also like these are small",
      "offset": 588.72,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "models and so sometimes they say random",
      "offset": 590.24,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "stuff. Um and so like the way that you",
      "offset": 592.48,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "could then explore this and be like okay",
      "offset": 595.2,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "so the model says podcast so like why",
      "offset": 596.88,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "does it say podcast? Uh so you can click",
      "offset": 598.64,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "on this output and say like what are the",
      "offset": 601.12,
      "duration": 3.719
    },
    {
      "lang": "en",
      "text": "features again these like intermediate",
      "offset": 602.88,
      "duration": 3.56
    },
    {
      "lang": "en",
      "text": "representations",
      "offset": 604.839,
      "duration": 4.041
    },
    {
      "lang": "en",
      "text": "that have an input to this. So it seems",
      "offset": 606.44,
      "duration": 4.36
    },
    {
      "lang": "en",
      "text": "like there's features at here this is",
      "offset": 608.88,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "the layer like layer 18 that already",
      "offset": 610.8,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "like are about podcast episodes. You can",
      "offset": 613.519,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "know this because the features have a",
      "offset": 616.48,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "label but also if you want you can look",
      "offset": 617.839,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "at the feature itself here and here you",
      "offset": 619.839,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "can see that like this shows you like",
      "offset": 622.72,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "other text that the feature is active",
      "offset": 624.32,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "over and it's just like text about",
      "offset": 626.32,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "podcasts. So that's like a way that you",
      "offset": 627.76,
      "duration": 2.639
    },
    {
      "lang": "en",
      "text": "can also like understand what the",
      "offset": 629.44,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "features are. Um and then you can keep",
      "offset": 630.399,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "going back. So it's like oh okay so it",
      "offset": 632.24,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "said podcast here because of this",
      "offset": 633.519,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "podcast feature where where did that",
      "offset": 635.6,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "come from? And it's like oh it comes",
      "offset": 637.6,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "from like words related to podcast words",
      "offset": 639.6,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "associated with podcast as well as like",
      "offset": 641.76,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "an interview feature. Um and also just",
      "offset": 644.16,
      "duration": 5.359
    },
    {
      "lang": "en",
      "text": "the word on. So there's like a bias like",
      "offset": 647.76,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "if you're saying like blah blah blah on",
      "offset": 649.519,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "that sort of like slightly increases the",
      "offset": 652,
      "duration": 2.399
    },
    {
      "lang": "en",
      "text": "chance that you're talking about a",
      "offset": 653.44,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "podcast at all. Um and you can sort of",
      "offset": 654.399,
      "duration": 5.041
    },
    {
      "lang": "en",
      "text": "keep going back. um and and kind of like",
      "offset": 656.56,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "explore the graph interactively. I would",
      "offset": 659.44,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "say that like the way to do it and we",
      "offset": 661.44,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "talked about this on the like kind of",
      "offset": 663.92,
      "duration": 2.64
    },
    {
      "lang": "en",
      "text": "like longer version of the podcast, but",
      "offset": 665.04,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "it's like you know kind of like chasing",
      "offset": 666.56,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "from the interesting outputs back or",
      "offset": 668.64,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "from the interesting input forward.",
      "offset": 670.72,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "There are many nodes on these. I like",
      "offset": 672.8,
      "duration": 3.039
    },
    {
      "lang": "en",
      "text": "wouldn't recommend looking at all of",
      "offset": 674.64,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "them. You can also sort of like prune",
      "offset": 675.839,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "them a little more aggressively here if",
      "offset": 678.079,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "this is too busy and kind of look this",
      "offset": 679.68,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "shows you like only the most important",
      "offset": 682,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "ones and you can sort of like be pretty",
      "offset": 683.519,
      "duration": 2.601
    },
    {
      "lang": "en",
      "text": "extreme with it if you",
      "offset": 684.8,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "want or you can show the whole thing and",
      "offset": 686.12,
      "duration": 3.88
    },
    {
      "lang": "en",
      "text": "then be like super overwhelmed. Once",
      "offset": 688.24,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "once you kind of do this you can then",
      "offset": 690,
      "duration": 2.959
    },
    {
      "lang": "en",
      "text": "kind of like group your nodes into",
      "offset": 691.44,
      "duration": 3.16
    },
    {
      "lang": "en",
      "text": "similar ones to kind of like make a",
      "offset": 692.959,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "graph. I actually made this little",
      "offset": 694.6,
      "duration": 5.96
    },
    {
      "lang": "en",
      "text": "summary earlier so I can just share",
      "offset": 698.399,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "that. So this is the exact same graph",
      "offset": 700.56,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "but just before the before before",
      "offset": 702.399,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "hopping on I kind of like did a few",
      "offset": 704.24,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "groups. So this is like same thing",
      "offset": 705.6,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "podcast. It's like oh there's like a",
      "offset": 707.36,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "bunch of notes like podcast episodes",
      "offset": 708.8,
      "duration": 3.96
    },
    {
      "lang": "en",
      "text": "thing like discussing",
      "offset": 711.279,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "podcast there's a note about expressing",
      "offset": 712.76,
      "duration": 4.759
    },
    {
      "lang": "en",
      "text": "gratitude that amplifies that you're",
      "offset": 715.2,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "like on an interview or a podcast. So",
      "offset": 717.519,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "like one fun experiment you could do",
      "offset": 719.44,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "here right is like oh like what happens",
      "offset": 720.88,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "if I mess with this like if if I don't",
      "offset": 723.279,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "like if I mess with the like oh this",
      "offset": 726,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "person is like grateful to be on and is",
      "offset": 727.519,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "like this person is on. Does it think",
      "offset": 729.839,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "you're on something else? Like maybe",
      "offset": 731.76,
      "duration": 2.639
    },
    {
      "lang": "en",
      "text": "there are things that you know you could",
      "offset": 733.04,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "be on that you're not grateful for like",
      "offset": 734.399,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "oh you're having me on trial or",
      "offset": 736.399,
      "duration": 2.56
    },
    {
      "lang": "en",
      "text": "something I don't know like that could",
      "offset": 737.839,
      "duration": 2.24
    },
    {
      "lang": "en",
      "text": "be one one interesting sort like",
      "offset": 738.959,
      "duration": 2.481
    },
    {
      "lang": "en",
      "text": "experiment to see what the causal effect",
      "offset": 740.079,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "of this is and again you could sort of",
      "offset": 741.44,
      "duration": 4.839
    },
    {
      "lang": "en",
      "text": "like label it more and explore it more",
      "offset": 744.079,
      "duration": 5.921
    },
    {
      "lang": "en",
      "text": "and this UI the whole point is for it to",
      "offset": 746.279,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "be like snappy and quick so you can just",
      "offset": 750,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "like generate a bunch of graphs pretty",
      "offset": 751.279,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "easily right like maybe this wasn't",
      "offset": 753.36,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "exactly what you wanted so you're like",
      "offset": 754.72,
      "duration": 7.96
    },
    {
      "lang": "en",
      "text": "I'm super unhappy to be on the latence",
      "offset": 757.04,
      "duration": 7.799
    },
    {
      "lang": "en",
      "text": "space and then you can see what it",
      "offset": 762.68,
      "duration": 5.32
    },
    {
      "lang": "en",
      "text": "completes for that or whatever and you",
      "offset": 764.839,
      "duration": 4.841
    },
    {
      "lang": "en",
      "text": "can sort of like just continuously play",
      "offset": 768,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "with it and and get a better sense for",
      "offset": 769.68,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "your hypotheses. Often times you kind of",
      "offset": 771.12,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "like want different prompts, you know,",
      "offset": 772.48,
      "duration": 2.64
    },
    {
      "lang": "en",
      "text": "different examples that are similar to",
      "offset": 773.92,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "kind of get a sense for it. And then if",
      "offset": 775.12,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "you're really curious and you want to",
      "offset": 777.36,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "dig in more, that's when I would",
      "offset": 779.36,
      "duration": 2.56
    },
    {
      "lang": "en",
      "text": "recommend going back to like the",
      "offset": 780.959,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "codebase and some of the notebooks.",
      "offset": 781.92,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "Maybe one thing, one last thing I'll say",
      "offset": 784.399,
      "duration": 3.961
    },
    {
      "lang": "en",
      "text": "on on that is that the notebooks",
      "offset": 785.92,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "themselves, they can all be run on",
      "offset": 788.36,
      "duration": 4.52
    },
    {
      "lang": "en",
      "text": "Google Collab. And all of the code, as",
      "offset": 790.32,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "far as we can tell, we've like tested",
      "offset": 792.88,
      "duration": 2.48
    },
    {
      "lang": "en",
      "text": "all the notebooks just like runs on",
      "offset": 794.079,
      "duration": 3.121
    },
    {
      "lang": "en",
      "text": "Collab. And so that means that like you",
      "offset": 795.36,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "don't need on the free tier to be clear,",
      "offset": 797.2,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "like you don't need like an expensive",
      "offset": 798.88,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "GPU. You can just run this and kind of",
      "offset": 800.24,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "like run your interventions and play",
      "offset": 801.68,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "with it. And so in this notebook in",
      "offset": 803.2,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "particular, the intro one, we show you",
      "offset": 805.6,
      "duration": 2.64
    },
    {
      "lang": "en",
      "text": "how to do these interventions. And here",
      "offset": 806.88,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "we're like what happens if we turn this",
      "offset": 808.24,
      "duration": 3.039
    },
    {
      "lang": "en",
      "text": "note off and what happens if we turn",
      "offset": 809.92,
      "duration": 2.64
    },
    {
      "lang": "en",
      "text": "that one off and what happens if we turn",
      "offset": 811.279,
      "duration": 2.961
    },
    {
      "lang": "en",
      "text": "this one off and what happens if you",
      "offset": 812.56,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "know we inject one from one prompt into",
      "offset": 814.24,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "another one. And so I think that's the",
      "offset": 816.16,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "sort of like deeper dive trying to",
      "offset": 818.24,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "understand the mechanism better. But if",
      "offset": 819.92,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "you're just trying to even like get a a",
      "offset": 821.839,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "sense at all like how does a model does",
      "offset": 823.76,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "do X you can just generate a graph and",
      "offset": 825.279,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "and take a look at it. Incredible. Very",
      "offset": 826.88,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "cool. Is there uh when I look at the",
      "offset": 830.399,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "graph is there's a thought in my mind",
      "offset": 832.72,
      "duration": 5.919
    },
    {
      "lang": "en",
      "text": "about maybe this is too easy too perfect",
      "offset": 834.959,
      "duration": 5.921
    },
    {
      "lang": "en",
      "text": "um and one version of this is there's",
      "offset": 838.639,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "supposed to be superp position and here",
      "offset": 840.88,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "there's no superposition kind of well",
      "offset": 842.72,
      "duration": 5.359
    },
    {
      "lang": "en",
      "text": "there is superposition and like we're",
      "offset": 846.24,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "we're sort of like so maybe I can share",
      "offset": 848.079,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "I can share the the graph again and like",
      "offset": 850.48,
      "duration": 3.039
    },
    {
      "lang": "en",
      "text": "answer your question which I think is",
      "offset": 852.24,
      "duration": 2.719
    },
    {
      "lang": "en",
      "text": "like what are we hiding here where are",
      "offset": 853.519,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "the skeletons yeah this is like it's too",
      "offset": 854.959,
      "duration": 6.961
    },
    {
      "lang": "en",
      "text": "it's too clean I'm like Yeah. So, okay.",
      "offset": 858.24,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "So, so maybe like a good example is and",
      "offset": 861.92,
      "duration": 3.039
    },
    {
      "lang": "en",
      "text": "we're going to like make this slightly",
      "offset": 863.6,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "less overwhelming here is like okay. So,",
      "offset": 864.959,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "so you look at this graph and you say",
      "offset": 867.279,
      "duration": 2.56
    },
    {
      "lang": "en",
      "text": "like yeah like we don't actually",
      "offset": 868.32,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "understand how models work fully. So,",
      "offset": 869.839,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "like what are you hiding here? And and",
      "offset": 871.36,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "the thing that's like important to know",
      "offset": 874.32,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "here is you know I I didn't say this",
      "offset": 875.839,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "explicitly but like the layers are like",
      "offset": 878.16,
      "duration": 3.119
    },
    {
      "lang": "en",
      "text": "arranged here. And so let's just look at",
      "offset": 879.76,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "like one layer. So for this layer what",
      "offset": 881.279,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "we're saying is like the only thing that",
      "offset": 883.76,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "that is happening or that's like",
      "offset": 885.199,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "important enough is this one feature",
      "offset": 886.72,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "which is just like one small direction",
      "offset": 889.199,
      "duration": 4.361
    },
    {
      "lang": "en",
      "text": "in the model space right like one",
      "offset": 891.279,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "one dimension we've pulled out of superp",
      "offset": 893.56,
      "duration": 5.639
    },
    {
      "lang": "en",
      "text": "position uh or or um let's say that for",
      "offset": 895.76,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "now but then also there's these diamonds",
      "offset": 899.199,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "and these diamonds are errors we talked",
      "offset": 901.44,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "about them on on the longer podcast but",
      "offset": 904.959,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "they're just like when you train these",
      "offset": 906.32,
      "duration": 2.879
    },
    {
      "lang": "en",
      "text": "replacement models to replace some of",
      "offset": 908.079,
      "duration": 2.721
    },
    {
      "lang": "en",
      "text": "the model computation you successfully",
      "offset": 909.199,
      "duration": 3.281
    },
    {
      "lang": "en",
      "text": "replace some of it and then some of it",
      "offset": 910.8,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "you fail to replace. And so this is like",
      "offset": 912.48,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "everything that we don't understand. And",
      "offset": 915.44,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "so that means that like sometimes if you",
      "offset": 917.36,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "look at an input like this guy's input,",
      "offset": 919.12,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "you'll see a bunch of errors here as the",
      "offset": 921.199,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "input. And so essentially, you know,",
      "offset": 923.199,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "there's some graphs and some examples",
      "offset": 924.959,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "where like if you have if most of the",
      "offset": 926.56,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "stuff that you see is the these errors.",
      "offset": 928.72,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "Basically that's that just means like",
      "offset": 930.639,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "hey for this prompt we were not able to",
      "offset": 932.24,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "sort of like explain you know that part",
      "offset": 934.32,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "of the computation. And so at least that",
      "offset": 936.32,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "part is like a an explicit sort of like",
      "offset": 939.199,
      "duration": 3.521
    },
    {
      "lang": "en",
      "text": "we show it in your face where it's like",
      "offset": 941.199,
      "duration": 3.041
    },
    {
      "lang": "en",
      "text": "here's here's what we don't understand",
      "offset": 942.72,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "and so so you can sort of like see what",
      "offset": 944.24,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "we don't understand. There's also I will",
      "offset": 946.32,
      "duration": 2.959
    },
    {
      "lang": "en",
      "text": "say like one more thing there's like a",
      "offset": 948.079,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "bunch more stuff that can get you uh and",
      "offset": 949.279,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "that's like in the paper but like one",
      "offset": 952,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "example here that I'll just say is like",
      "offset": 953.92,
      "duration": 5.279
    },
    {
      "lang": "en",
      "text": "these are just MLPS. So the model has",
      "offset": 956.24,
      "duration": 4.959
    },
    {
      "lang": "en",
      "text": "both attention heads and multi",
      "offset": 959.199,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "perceptions MLPS. We don't just do it",
      "offset": 961.199,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "like we completely ignore attention or",
      "offset": 964.399,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "like we don't we don't try to decompose",
      "offset": 966.959,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "it at all. So there's some prompts where",
      "offset": 968.32,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "like all of the interesting stuff is",
      "offset": 969.839,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "attention and here you're just not",
      "offset": 971.44,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "you're just not seeing it at all. The",
      "offset": 973.68,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "way that it's materialized is like you",
      "offset": 975.839,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "have an edge from here to here and like",
      "offset": 977.279,
      "duration": 2.56
    },
    {
      "lang": "en",
      "text": "some attention head did a bunch of",
      "offset": 978.639,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "stuff. You don't know what it is. And so",
      "offset": 979.839,
      "duration": 3.281
    },
    {
      "lang": "en",
      "text": "that's also the part that we're sort of",
      "offset": 981.759,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "like not explaining. So there's there's",
      "offset": 983.12,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "definitely yeah I don't want to make the",
      "offset": 984.639,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "claim that",
      "offset": 986.16,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "we explain everything. I think the",
      "offset": 988.079,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "correct way to think about this is like",
      "offset": 989.44,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "if you look at a prompt and you can by",
      "offset": 991.68,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "tracing through these not hit any",
      "offset": 994.32,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "errors, hit nodes that make sense and",
      "offset": 996.16,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "build up a reasonable hypothesis and",
      "offset": 998.56,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "then when you test it with interventions",
      "offset": 1000.32,
      "duration": 3.519
    },
    {
      "lang": "en",
      "text": "it works. You've at least understood",
      "offset": 1001.759,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "some and presumably like a reasonable",
      "offset": 1003.839,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "proportion of the computation. If your",
      "offset": 1006.48,
      "duration": 2.719
    },
    {
      "lang": "en",
      "text": "interventions are working that means",
      "offset": 1007.92,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "it's like the thing you found is not",
      "offset": 1009.199,
      "duration": 3.281
    },
    {
      "lang": "en",
      "text": "just like a side thing. It's part of the",
      "offset": 1010.8,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "main thing the model is doing. So, you",
      "offset": 1012.48,
      "duration": 3.279
    },
    {
      "lang": "en",
      "text": "know, then the question is like how",
      "offset": 1014.399,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "often does that happen versus how often",
      "offset": 1015.759,
      "duration": 2.961
    },
    {
      "lang": "en",
      "text": "you just hit these errors or you're like",
      "offset": 1017.199,
      "duration": 3.041
    },
    {
      "lang": "en",
      "text": "confused. And I think that's that's just",
      "offset": 1018.72,
      "duration": 2.679
    },
    {
      "lang": "en",
      "text": "sort of",
      "offset": 1020.24,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "like what works and what doesn't. Uh",
      "offset": 1021.399,
      "duration": 5.8
    },
    {
      "lang": "en",
      "text": "summary here. Crazy. I mean, uh congrats",
      "offset": 1023.92,
      "duration": 5.119
    },
    {
      "lang": "en",
      "text": "on this work. I know you you're low on",
      "offset": 1027.199,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "sleep cuz you worked really hard on uh",
      "offset": 1029.039,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "shipping it and you're a perfectionist.",
      "offset": 1031.199,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "I I just think like Yeah. Sorry. I'll",
      "offset": 1033.28,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "just say that like the actual brunt of",
      "offset": 1035.679,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "the work here is like the fellows. Yeah.",
      "offset": 1037.28,
      "duration": 4.36
    },
    {
      "lang": "en",
      "text": "Yeah. They, you know, I I mostly just",
      "offset": 1039.52,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "like coordinated things left and right,",
      "offset": 1041.64,
      "duration": 3.96
    },
    {
      "lang": "en",
      "text": "but but they sort of like did all the",
      "offset": 1043.839,
      "duration": 3.281
    },
    {
      "lang": "en",
      "text": "implementation as well as, you know,",
      "offset": 1045.6,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "folks on the like neuron pedia decode",
      "offset": 1047.12,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "research side also did, you know, the",
      "offset": 1048.88,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "the lines here of the work here to",
      "offset": 1050.559,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "actually have the the front end UI. I",
      "offset": 1052.32,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "would just say like, you know, uh, Vivu",
      "offset": 1054.48,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "and I were at the Goodfire meet up",
      "offset": 1056.48,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "yesterday where there were a lot of uh,",
      "offset": 1057.84,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "interpretability folks. I was shocked at",
      "offset": 1060.16,
      "duration": 4.44
    },
    {
      "lang": "en",
      "text": "um honestly how young most",
      "offset": 1062.88,
      "duration": 5.679
    },
    {
      "lang": "en",
      "text": "interpretability people and work are and",
      "offset": 1064.6,
      "duration": 5.8
    },
    {
      "lang": "en",
      "text": "this is a very young field exactly like",
      "offset": 1068.559,
      "duration": 5.081
    },
    {
      "lang": "en",
      "text": "you say in the podcast there's a lot of",
      "offset": 1070.4,
      "duration": 7.44
    },
    {
      "lang": "en",
      "text": "fresh green grass here to to tread and",
      "offset": 1073.64,
      "duration": 6.44
    },
    {
      "lang": "en",
      "text": "it's just really inspiring do you have",
      "offset": 1077.84,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "any other like final thoughts or",
      "offset": 1080.08,
      "duration": 2.959
    },
    {
      "lang": "en",
      "text": "comments? Yeah. Yeah. No, I think",
      "offset": 1081.679,
      "duration": 2.721
    },
    {
      "lang": "en",
      "text": "there's just a lot of open work to be",
      "offset": 1083.039,
      "duration": 2.561
    },
    {
      "lang": "en",
      "text": "done, you know, and we talk about this",
      "offset": 1084.4,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "in the podcast, too. And just to",
      "offset": 1085.6,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "reiterate like how good the tooling that",
      "offset": 1087.12,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "you guys put out is like even the fact",
      "offset": 1088.96,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "that, you know, without diving into any",
      "offset": 1091.12,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "code, you can enter in a prompt and",
      "offset": 1093.36,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "start to play through these circuits in",
      "offset": 1096,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "like minutes. It's it's pretty",
      "offset": 1097.76,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "incredible. Like I I could share another",
      "offset": 1099.52,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "one actually. So, I was doing this with",
      "offset": 1101.52,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "Pomsky and I finally got it to work. So,",
      "offset": 1103.2,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "our guest host of the episode is Mochi,",
      "offset": 1105.36,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "my little dog. She's our distilled",
      "offset": 1107.84,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "husky. So she's on the podcast later and",
      "offset": 1109.12,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "you know I basically put in like I I had",
      "offset": 1111.52,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "to guide it quite a bit but my my prompt",
      "offset": 1113.36,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "is a pomsky is a small dog that's a",
      "offset": 1115.2,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "breed of a m of a husky and a and then",
      "offset": 1117.6,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "you know I'm expecting it to put out",
      "offset": 1120.72,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "Pomeranian or palm let me let me share",
      "offset": 1122.16,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "my screen real quick and then we can",
      "offset": 1124.4,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "kind of dig through this is me like by",
      "offset": 1126.72,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "the way her tagline yeah while you put",
      "offset": 1128.799,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "it up her tagline is officially Mochi",
      "offset": 1130.64,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "the interpretability husky today for",
      "offset": 1132.48,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "today we're going to change our tagline",
      "offset": 1136,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "every episode but yeah it feels a little",
      "offset": 1137.28,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "weird you We're digging deep into what",
      "offset": 1138.88,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "Mochi is, but basically this is me like",
      "offset": 1140.72,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "no background like 2 minutes in just put",
      "offset": 1142.88,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "in a phrase and now I get to play around",
      "offset": 1144.88,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "with features, right? So this is also",
      "offset": 1146.32,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "called please with four S's because you",
      "offset": 1149.039,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "know I tried a few pumps. It's okay.",
      "offset": 1151.84,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "It's okay. We struggle. It only took a",
      "offset": 1153.36,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "few minutes though. So you know Psky is",
      "offset": 1155.12,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "a small dog breed that that's a mix of a",
      "offset": 1156.64,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "husky and a and then the the most out",
      "offset": 1158.559,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "probable outputs you know now it says",
      "offset": 1161.44,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "palm. So okay let's dig into what some",
      "offset": 1163.039,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "of these are. I'm basically just going",
      "offset": 1165.12,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "like fresh. Haven't done this before,",
      "offset": 1168.24,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "but you know, words related to animals,",
      "offset": 1170.08,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "their emotions, their health. We have a",
      "offset": 1172.24,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "feature for dog golden lab mentioned dog",
      "offset": 1174.4,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "breeds, especially high maintenance. You",
      "offset": 1178.08,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "know, this is basically like AGI. It",
      "offset": 1180,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "knows pals skis are high maintenance.",
      "offset": 1182.24,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "It's it's figured it out. But",
      "offset": 1184,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "realistically, you know, um as I dig",
      "offset": 1186.32,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "through these features, I can start to",
      "offset": 1188.64,
      "duration": 4.12
    },
    {
      "lang": "en",
      "text": "pin them, uh layer them through.",
      "offset": 1190.08,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "Mentions of garbage and waste. No,",
      "offset": 1192.76,
      "duration": 3.96
    },
    {
      "lang": "en",
      "text": "that's not nice. That's not nice. But",
      "offset": 1195.2,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "basically, you know, interesting. And",
      "offset": 1196.72,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "this is already me pruning out most of",
      "offset": 1198.08,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "the features as I open it up. You know,",
      "offset": 1200,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "it talks about different things like dog",
      "offset": 1201.919,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "breeding.",
      "offset": 1203.6,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "What else related to animal welfare? So",
      "offset": 1205.84,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "like and then you can dig through all",
      "offset": 1209.28,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "this. There's just so many things that",
      "offset": 1210.48,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "like you know this is in a matter of",
      "offset": 1212,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "minutes. I basically made a graph, put",
      "offset": 1213.52,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "in a sentence and now I have an output",
      "offset": 1215.52,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "and I can traverse through what are",
      "offset": 1217.6,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "different things. Okay, animal science,",
      "offset": 1219.12,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "right? So this breed is relatively new.",
      "offset": 1220.48,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "It's not that common that big huskys and",
      "offset": 1222.88,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "little Pomeranians naturally have",
      "offset": 1224.88,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "offspring, but you know, let's let's",
      "offset": 1227.039,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "like dig through animal science versions",
      "offset": 1228.88,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "of this and then we have like",
      "offset": 1230.88,
      "duration": 3.279
    },
    {
      "lang": "en",
      "text": "interesting little features. So, it's",
      "offset": 1232.32,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "it's very easy for people to kind of get",
      "offset": 1234.159,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "a different understanding of what goes",
      "offset": 1236.4,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "on throughout layers in models, you",
      "offset": 1238.559,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "know, but that's just my fun little",
      "offset": 1240.48,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "experiment of getting it to work. Oh,",
      "offset": 1242.799,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "yeah. And and I think like you know one",
      "offset": 1244.32,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "one thing that I I would do if you were",
      "offset": 1245.919,
      "duration": 2.801
    },
    {
      "lang": "en",
      "text": "curious or maybe I'm just gonna try to",
      "offset": 1247.52,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "bait some listeners into doing it is",
      "offset": 1248.72,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "like you could be like okay like let's",
      "offset": 1250.48,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "try to like trace why it said Pomeranian",
      "offset": 1251.84,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "here and like maybe there's like some of",
      "offset": 1254.159,
      "duration": 2.961
    },
    {
      "lang": "en",
      "text": "it is about like like dog breeds and",
      "offset": 1255.28,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "some of it is about like specific",
      "offset": 1257.12,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "characteristics of a husky and then you",
      "offset": 1258.4,
      "duration": 3.279
    },
    {
      "lang": "en",
      "text": "can ask the same question but instead of",
      "offset": 1260.08,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "husky you like try some other dog breed",
      "offset": 1261.679,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "and then try to see if you can like if",
      "offset": 1263.52,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "you understood the circuit well and if",
      "offset": 1265.28,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "you identified where it's thinking about",
      "offset": 1266.72,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "huskys or where it's thinking about like",
      "offset": 1268.08,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "kind like breeding two different breeds",
      "offset": 1269.76,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "then you should be able to like swap",
      "offset": 1271.52,
      "duration": 3.039
    },
    {
      "lang": "en",
      "text": "these in and out and get it to kind say",
      "offset": 1272.88,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "whatever you want. Um and and if you",
      "offset": 1274.559,
      "duration": 2.721
    },
    {
      "lang": "en",
      "text": "didn't, then maybe there's something",
      "offset": 1276.4,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "complicated going on. But but yeah, like",
      "offset": 1277.28,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "very cool that you got this going on so",
      "offset": 1279.44,
      "duration": 2.719
    },
    {
      "lang": "en",
      "text": "quick. That's that's that's the whole",
      "offset": 1281.12,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "goal. That's super exciting. Yeah. And",
      "offset": 1282.159,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "and like you know, no disclosure, this",
      "offset": 1284,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "was like 5 minutes of just playing",
      "offset": 1285.52,
      "duration": 3.279
    },
    {
      "lang": "en",
      "text": "around and like there's there's stuff to",
      "offset": 1286.88,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "learn there, right? Like okay, what",
      "offset": 1288.799,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "happens with dog breeding? What are",
      "offset": 1291.039,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "traits of these dogs? And then, you",
      "offset": 1292.88,
      "duration": 3.279
    },
    {
      "lang": "en",
      "text": "know, the next step for me would",
      "offset": 1294.48,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "basically be let's try clamping some of",
      "offset": 1296.159,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "these features up or down. Let's let's",
      "offset": 1298.159,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "do different breeds and see if it makes",
      "offset": 1300.48,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "sense. Right? So if I have husky traits",
      "offset": 1302.159,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "in a different mix and then you know can",
      "offset": 1304.32,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "I can I get out what's going on but it",
      "offset": 1306.08,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "also shows internally that there's more",
      "offset": 1307.76,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "than just token completion of you know",
      "offset": 1310.64,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "this plus this equals this. No it has",
      "offset": 1312.48,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "some under understanding of",
      "offset": 1314.4,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "characteristics right like this is a",
      "offset": 1315.84,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "pretty stubborn dog it it has a stubborn",
      "offset": 1317.36,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "feature pretty high up that activates.",
      "offset": 1319.44,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "So very very cool stuff. I think it'll",
      "offset": 1321.52,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "be cool when we apply this to more like",
      "offset": 1324,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "serious topics. Like right now when it",
      "offset": 1326.72,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "comes to evals, right? We have like we",
      "offset": 1329.52,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "have pretty straightforward evals,",
      "offset": 1332.72,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "right? Like how good is does it do on",
      "offset": 1334.559,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "math? Can it write code? Does stuff",
      "offset": 1336.24,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "compile? But we don't have like vibes",
      "offset": 1337.679,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "based heristic evals, right? So like",
      "offset": 1339.679,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "does it understand different queries?",
      "offset": 1341.84,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "Should be concise? Should they be",
      "offset": 1343.679,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "verbose? Can we kind of trace through",
      "offset": 1345.12,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "how it gives responses to this stuff?",
      "offset": 1347.12,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "And then like the other part is you know",
      "offset": 1349.2,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "as we go past base models how does this",
      "offset": 1350.88,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "happen for different phases of models",
      "offset": 1352.96,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "right so if I have a base Gemma and I",
      "offset": 1354.64,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "have a chat model what are differences",
      "offset": 1357.44,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "in their attributions right what happens",
      "offset": 1359.12,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "kind of in that diff of training so",
      "offset": 1361.039,
      "duration": 3.281
    },
    {
      "lang": "en",
      "text": "that's that's kind of one of my little",
      "offset": 1362.96,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "interests in in mechan what happens as",
      "offset": 1364.32,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "we do more training what are we really",
      "offset": 1366.72,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "changing totally yeah you can think",
      "offset": 1368.48,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "about like comparing different models",
      "offset": 1371.039,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "and for me different models either like",
      "offset": 1372.64,
      "duration": 3.519
    },
    {
      "lang": "en",
      "text": "Gemma versus some other model or like",
      "offset": 1374.4,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "early Gemma versus late Gemma and",
      "offset": 1376.159,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "pre-training or like fine-tuned versus",
      "offset": 1378.24,
      "duration": 2.439
    },
    {
      "lang": "en",
      "text": "not",
      "offset": 1379.679,
      "duration": 2.961
    },
    {
      "lang": "en",
      "text": "fine-tuned. I think there's also a sense",
      "offset": 1380.679,
      "duration": 3.801
    },
    {
      "lang": "en",
      "text": "in which like somebody yesterday was",
      "offset": 1382.64,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "telling me like, &quot;Oh, it's fun. I've",
      "offset": 1384.48,
      "duration": 2.48
    },
    {
      "lang": "en",
      "text": "been playing with it on like the like",
      "offset": 1385.52,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "sort of like weird riddles that the",
      "offset": 1386.96,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "models get wrong.&quot; Like it like you",
      "offset": 1388.48,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "you're not limited to studying what the",
      "offset": 1390.24,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "model can do, right? Like if the model's",
      "offset": 1391.679,
      "duration": 2.721
    },
    {
      "lang": "en",
      "text": "failing at something like you know",
      "offset": 1393.12,
      "duration": 2.32
    },
    {
      "lang": "en",
      "text": "counting the number of letters in",
      "offset": 1394.4,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "strawberry or whatever. Um you could",
      "offset": 1395.44,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "just try that and try to figure out the",
      "offset": 1397.52,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "circuit for like well it's getting this",
      "offset": 1399.12,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "wrong like why like it maybe you can see",
      "offset": 1400.48,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "in its representation that it's like",
      "offset": 1403.039,
      "duration": 2.801
    },
    {
      "lang": "en",
      "text": "thinking about something obviously",
      "offset": 1404.64,
      "duration": 3.039
    },
    {
      "lang": "en",
      "text": "incorrect, right? Um, and so I think I",
      "offset": 1405.84,
      "duration": 2.959
    },
    {
      "lang": "en",
      "text": "think that that's also like a fun thing",
      "offset": 1407.679,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "to to to play with. I think that's it",
      "offset": 1408.799,
      "duration": 5.281
    },
    {
      "lang": "en",
      "text": "for our uh little intro chat and",
      "offset": 1410.799,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "coverage of the open sourcing. Let's",
      "offset": 1414.08,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "dive right into the episode next. But",
      "offset": 1416.159,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "Emanuel, uh, you're uh, amazing work and",
      "offset": 1417.919,
      "duration": 5.281
    },
    {
      "lang": "en",
      "text": "I'm so inspired and also just like I",
      "offset": 1421.12,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "think it this puts a human face on the",
      "offset": 1423.2,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "the interpretability work. I think it's",
      "offset": 1425.44,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "very important and we'd love to keep",
      "offset": 1427.36,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "doing this whatever you got next coming",
      "offset": 1429.12,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "up. Well, yeah, thanks for having me.",
      "offset": 1430.799,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "Again, I should say cool to put a face",
      "offset": 1433.039,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "on it, but definitely want to call out",
      "offset": 1435.28,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "this is like a huge team of people with",
      "offset": 1436.559,
      "duration": 5.281
    },
    {
      "lang": "en",
      "text": "me. I'm just a talking head here. Um",
      "offset": 1438.64,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "and and paper paper lead, you know, you",
      "offset": 1441.84,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "did the work, you know, take credit. I",
      "offset": 1443.76,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "think that like yeah, happy to talk",
      "offset": 1446.32,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "about more inter things and also like",
      "offset": 1447.919,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "feel free to, you know, reach out to me.",
      "offset": 1449.2,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "I'm like findable if you're listening to",
      "offset": 1451.039,
      "duration": 2.961
    },
    {
      "lang": "en",
      "text": "this podcast and you have like questions",
      "offset": 1452.559,
      "duration": 2.961
    },
    {
      "lang": "en",
      "text": "about stuff that's broken or if this",
      "offset": 1454,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "brings up like experiment ideas. Uh",
      "offset": 1455.52,
      "duration": 3.039
    },
    {
      "lang": "en",
      "text": "definitely want more people playing with",
      "offset": 1457.44,
      "duration": 2.32
    },
    {
      "lang": "en",
      "text": "this. So, so yeah, thanks for having me.",
      "offset": 1458.559,
      "duration": 3.521
    },
    {
      "lang": "en",
      "text": "Hope hope that inspires some folks. All",
      "offset": 1459.76,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "right, we are back in the studio with uh",
      "offset": 1462.08,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "a couple special guests. One, Vivu, our",
      "offset": 1464.72,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "guest co-host for a couple of times now,",
      "offset": 1466.96,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "as well as Mochi, the distilled husky is",
      "offset": 1469.12,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "in the studio with us who asked some",
      "offset": 1471.679,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "very pressing questions. Uh as well as",
      "offset": 1473.52,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "Emanuel, I didn't get your last name.",
      "offset": 1475.919,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "Amason Y. Is that Dutch? Is that It's",
      "offset": 1478.159,
      "duration": 5.361
    },
    {
      "lang": "en",
      "text": "actually German. German. Yeah. Yeah. You",
      "offset": 1480.72,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "are the lead author of a fair number of",
      "offset": 1483.52,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "the recent uh mechan work from Enthropic",
      "offset": 1485.52,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "that I've been basically calling",
      "offset": 1488.24,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "transformer circuits because that's the",
      "offset": 1490.24,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "name of the publication. Yeah. Well, to",
      "offset": 1491.76,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "be clear, transformer circuits is the",
      "offset": 1494.559,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "whole publication. I'm the author on one",
      "offset": 1496,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "of the recent papers circuit tracing.",
      "offset": 1497.919,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "Yes. And people very excited about that.",
      "offset": 1499.6,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "The other name for it is like tracing",
      "offset": 1501.6,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "the thoughts of LM. There's like three",
      "offset": 1502.88,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "different names for this work. It's like",
      "offset": 1504.32,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "it's all mechan. It's all mechan.",
      "offset": 1506.559,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "There's two papers. One is circuit",
      "offset": 1508.72,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "tracing. It's the methods. One is like",
      "offset": 1510.24,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "the biology which is kind of what we",
      "offset": 1512.24,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "found in the model and then tracing the",
      "offset": 1514.24,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "thoughts is confusingly just the name of",
      "offset": 1516.159,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "the blog post where we announced it.",
      "offset": 1518,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "It's for different audiences and I think",
      "offset": 1519.76,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "though when you produce the like 2minute",
      "offset": 1521.52,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "polished video that you guys did that's",
      "offset": 1524.08,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "meant for like a very wide audience you",
      "offset": 1525.919,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "know. Yeah that's right. there's sort of",
      "offset": 1528.32,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "like very many levels of granularity at",
      "offset": 1530.08,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "which you can go and I think for a mech",
      "offset": 1532.4,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "interp in particular cuz it's kind of",
      "offset": 1533.919,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "complicated going you know from like top",
      "offset": 1535.36,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "to bottom most like high level to sort",
      "offset": 1537.36,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "of the gny details works pretty well.",
      "offset": 1539.6,
      "duration": 4.76
    },
    {
      "lang": "en",
      "text": "Yeah cool um we can get started",
      "offset": 1541.84,
      "duration": 4.959
    },
    {
      "lang": "en",
      "text": "basically we have two paths that you can",
      "offset": 1544.36,
      "duration": 4.199
    },
    {
      "lang": "en",
      "text": "you can choose like either your personal",
      "offset": 1546.799,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "journey into mechan or the brief history",
      "offset": 1548.559,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "of mechan just generally and maybe that",
      "offset": 1551.52,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "might coincide a little bit. I think my",
      "offset": 1553.76,
      "duration": 3.039
    },
    {
      "lang": "en",
      "text": "okay I could just give you my personal",
      "offset": 1555.44,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "journey very quickly because then we can",
      "offset": 1556.799,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "just do the second path. My personal",
      "offset": 1558.64,
      "duration": 3.279
    },
    {
      "lang": "en",
      "text": "journey is that I was working at",
      "offset": 1560,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "anthropic for a while. I'd been like",
      "offset": 1561.919,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "many people just following mechan as",
      "offset": 1564.08,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "sort of like an interesting field with",
      "offset": 1566.64,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "fascinating often beautiful papers. Uh",
      "offset": 1569.12,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "and I was at a time working on",
      "offset": 1572.08,
      "duration": 2.719
    },
    {
      "lang": "en",
      "text": "fine-tuning. So like actually",
      "offset": 1573.36,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "fine-tuning uh production models for",
      "offset": 1574.799,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "enthropic and eventually I got both like",
      "offset": 1576.36,
      "duration": 5.64
    },
    {
      "lang": "en",
      "text": "sort of like my fascination reached a",
      "offset": 1580,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "sufficient level that I decided I wanted",
      "offset": 1582,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "to work on it and also I got more",
      "offset": 1583.679,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "excited about just as our models got",
      "offset": 1585.6,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "better and better understanding how they",
      "offset": 1588.159,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "worked. So that's the simple journey",
      "offset": 1589.84,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "I've got like a background in ML kind of",
      "offset": 1592.159,
      "duration": 2.961
    },
    {
      "lang": "en",
      "text": "like did a lot of applied ML stuff",
      "offset": 1593.76,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "before and now I'm doing more research",
      "offset": 1595.12,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "stuff. Yeah. You have a book with",
      "offset": 1597.76,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "O'Reilly. You're head of AI at inside",
      "offset": 1599.6,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "data science. Anything else to plug?",
      "offset": 1601.76,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "Yeah. Uh, I actually I I want to like",
      "offset": 1604.08,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "plug the paper and unplug the book.",
      "offset": 1606.559,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "Okay. I think the book is good. I think",
      "offset": 1609.279,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "that the advice stands the test of time,",
      "offset": 1611.2,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "but it's very much like, hey, you're",
      "offset": 1613.52,
      "duration": 2.639
    },
    {
      "lang": "en",
      "text": "building like AI products. What should",
      "offset": 1614.799,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "you focus on? It's like very different,",
      "offset": 1616.159,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "I guess, is all I'll say from from the",
      "offset": 1618,
      "duration": 2.799
    },
    {
      "lang": "en",
      "text": "stuff that we're talking to talk about",
      "offset": 1619.76,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "today. Today is like research some of",
      "offset": 1620.799,
      "duration": 3.521
    },
    {
      "lang": "en",
      "text": "some of the sort of like deepest",
      "offset": 1622.88,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "weirdest things about how like models",
      "offset": 1624.32,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "work. And this book is you want to ship",
      "offset": 1626.24,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "a random forest to do fraud",
      "offset": 1628.4,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "classification. like here are the top",
      "offset": 1629.679,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "five mistakes to avoid. Yeah. Um the",
      "offset": 1631.12,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "good old days of ML. I know it was",
      "offset": 1633.44,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "simple back then. You also transitioned",
      "offset": 1635.44,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "into research and I think you also did",
      "offset": 1637.6,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "that managed to like I feel like there's",
      "offset": 1639.84,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "this monolith of like people assume you",
      "offset": 1642.24,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "need a PhD for research. Maybe can you",
      "offset": 1644.24,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "give like that perspective of like how",
      "offset": 1646.4,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "do people get into re how did you get",
      "offset": 1648,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "into research? Maybe that gives audience",
      "offset": 1649.36,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "an insight into Viva as well your",
      "offset": 1651.919,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "background. Yeah my background was in",
      "offset": 1653.76,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "like economics data science. I thought",
      "offset": 1655.919,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "LLMs were pretty interesting. I started",
      "offset": 1658.4,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "out with some basic ML stuff and then I",
      "offset": 1660.08,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "saw LLMs were starting to be a thing. So",
      "offset": 1662.4,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "I just went out there and did it. And",
      "offset": 1664.64,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "same thing with AI engineering, right?",
      "offset": 1666.08,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "You just kind of build stuff, you work",
      "offset": 1667.52,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "on interesting things and like now it's",
      "offset": 1669.36,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "more accessible than ever. Like back",
      "offset": 1671.52,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "when I got into the field 5 6 years ago,",
      "offset": 1673.279,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "like pre-training was still pretty new.",
      "offset": 1675.2,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "GPT3 hadn't really launched. So it was",
      "offset": 1677.76,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "still very early days and it was a lot",
      "offset": 1680.32,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "less competitive. But yeah, without any",
      "offset": 1681.919,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "specific background, no PhD, there just",
      "offset": 1684.24,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "weren't as many people working on it.",
      "offset": 1686.559,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "you made the transition a little bit",
      "offset": 1688.32,
      "duration": 2.959
    },
    {
      "lang": "en",
      "text": "more recently, right? So, what's your",
      "offset": 1689.679,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "experience been like? Yeah, I think I",
      "offset": 1691.279,
      "duration": 7.041
    },
    {
      "lang": "en",
      "text": "think it has maybe never been easier in",
      "offset": 1694.88,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "some ways because a lot of the field is",
      "offset": 1698.32,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "like pretty empirical right now. So I",
      "offset": 1700.88,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "think the bitter lesson is like this",
      "offset": 1704.08,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "lesson that you know you can just sort",
      "offset": 1706.64,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "of like a lot of times scale up compute",
      "offset": 1708.159,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "and data and get better results than",
      "offset": 1709.76,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "like thinking than if you sort of like",
      "offset": 1711.279,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "thought extremely hard about a really",
      "offset": 1713.6,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "good like prior inspired by the human",
      "offset": 1715.6,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "brain to to train your model better. And",
      "offset": 1718,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "so in terms of definitely like research",
      "offset": 1719.84,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "for pre-training and fine-tuning, I",
      "offset": 1721.12,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "think it's just sort of like a lot of",
      "offset": 1723.36,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "the bottlenecks are extremely good",
      "offset": 1725.36,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "engineering and systems engineering and",
      "offset": 1727.6,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "a lot even of the research execution is",
      "offset": 1729.6,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "just about sort of like engineering and",
      "offset": 1731.6,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "scaling up and things like that. I think",
      "offset": 1733.919,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "for interp in particular there's like",
      "offset": 1736.08,
      "duration": 4.959
    },
    {
      "lang": "en",
      "text": "another thing that makes it easier to",
      "offset": 1738.72,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "transition to which is maybe two things.",
      "offset": 1741.039,
      "duration": 6.64
    },
    {
      "lang": "en",
      "text": "one you can just do it without huge",
      "offset": 1743.6,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "access to compute like there are open",
      "offset": 1747.679,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "source models you can look at them a lot",
      "offset": 1749.76,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "of inter papers you know coming out of",
      "offset": 1751.52,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "programs like maths are on models that",
      "offset": 1753.2,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "are open source that you can sort of",
      "offset": 1755.279,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "like dissect without having a cluster of",
      "offset": 1756.399,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "like you know 100 GPUs you can just even",
      "offset": 1758.64,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "sometimes you can load them like on your",
      "offset": 1761.2,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "CPU on your MacBook and it's also a",
      "offset": 1762.72,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "relatively new field and so you know",
      "offset": 1764.96,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "there's as I'm sure we'll talk about",
      "offset": 1768.08,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "there's like some conceptual burdens and",
      "offset": 1769.52,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "concepts that you just want to like",
      "offset": 1772,
      "duration": 2.32
    },
    {
      "lang": "en",
      "text": "understand before you contribute but",
      "offset": 1773.12,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "it's not you know physics it's",
      "offset": 1774.32,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "relatively recent and so the number of",
      "offset": 1776.88,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "abstractions that you have to like ramp",
      "offset": 1778.399,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "up on is just not that high compared to",
      "offset": 1779.6,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "other fields which I think makes that",
      "offset": 1782.24,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "transition somewhat easier for interp if",
      "offset": 1783.679,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "you understand we'll talk about all",
      "offset": 1786.08,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "these I'm sure but like what features",
      "offset": 1787.679,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "are and what dictionary learning is",
      "offset": 1789.12,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "you're like a long part of the way there",
      "offset": 1791.6,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "I think it's also interesting just on a",
      "offset": 1793.84,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "career point of view research seems a",
      "offset": 1796,
      "duration": 6.88
    },
    {
      "lang": "en",
      "text": "lot more valuable than engineering so I",
      "offset": 1799.36,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "And you don't have to answer this if",
      "offset": 1802.88,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "it's like a tricky thing, but like how",
      "offset": 1804.159,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "hard is it for a for a research engineer",
      "offset": 1805.84,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "in anthropic to jump the wall into",
      "offset": 1808.96,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "research? People seem to move around a",
      "offset": 1811.52,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "lot. And I'm like there that cannot be",
      "offset": 1813.52,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "so easy. Like in no other industry that",
      "offset": 1816.48,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "I know of, people you can do that. Do",
      "offset": 1819.76,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "you know what I mean? Yeah. I think I'd",
      "offset": 1822.64,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "actually like I'd push back on the sort",
      "offset": 1824.559,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "of like research being more valuable",
      "offset": 1826.48,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "than engineering a little bit because I",
      "offset": 1828.159,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "think a lot of times like having the",
      "offset": 1830.32,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "research idea is not the hardest part.",
      "offset": 1833.279,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "Don't get me wrong, there's some ideas",
      "offset": 1836,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "that are like brilliant and hard to",
      "offset": 1837.76,
      "duration": 3.279
    },
    {
      "lang": "en",
      "text": "find. But what's what's hard certainly",
      "offset": 1838.96,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "on fine-tuning and to a certain extent",
      "offset": 1841.039,
      "duration": 5.441
    },
    {
      "lang": "en",
      "text": "on interp is executing on your research",
      "offset": 1842.399,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "idea in terms of like making an",
      "offset": 1846.48,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "experiment successfully like having your",
      "offset": 1848.399,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "experiment run interpreting it",
      "offset": 1850.32,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "correctly. What that means though is",
      "offset": 1852.24,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "that like they're not separate skill",
      "offset": 1854.08,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "sets. So like if you have a cool idea,",
      "offset": 1855.679,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "there's kind of not many people in the",
      "offset": 1858.159,
      "duration": 2.4
    },
    {
      "lang": "en",
      "text": "world I think where they can just like",
      "offset": 1859.52,
      "duration": 2.399
    },
    {
      "lang": "en",
      "text": "have a cool idea and then they have a,",
      "offset": 1860.559,
      "duration": 2.401
    },
    {
      "lang": "en",
      "text": "you know, like a little minion they'll",
      "offset": 1861.919,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "deputize being like here's my idea, you",
      "offset": 1862.96,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "know, go off for 3 months and like run",
      "offset": 1865.039,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "this whole like build this model and",
      "offset": 1867.039,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "train it for, you know, hundreds of",
      "offset": 1869.039,
      "duration": 2.481
    },
    {
      "lang": "en",
      "text": "hours and then report back on what",
      "offset": 1870.24,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "happened. A lot of the time like the",
      "offset": 1871.52,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "people that are the most productive,",
      "offset": 1873.36,
      "duration": 2.4
    },
    {
      "lang": "en",
      "text": "they have an idea, but they're also",
      "offset": 1874.48,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "extremely quick at checking their idea,",
      "offset": 1875.76,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "finding sort of like the shortest path",
      "offset": 1878.399,
      "duration": 2.961
    },
    {
      "lang": "en",
      "text": "to to checking their idea. And a lot of",
      "offset": 1879.679,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "like that shortest path is engineering",
      "offset": 1881.36,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "skills essentially is just like getting",
      "offset": 1883.76,
      "duration": 3.039
    },
    {
      "lang": "en",
      "text": "stuff done. And so I think that's why",
      "offset": 1885.12,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "you see sort of like people move around",
      "offset": 1886.799,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "is like proportionate to to to your",
      "offset": 1888.799,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "interest. If you're just able to quickly",
      "offset": 1890.799,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "execute on the ideas you have and get",
      "offset": 1894,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "results, then that's really the 90% of",
      "offset": 1896,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "the value. And so you see a lot of",
      "offset": 1898.399,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "transferable skills actually I think",
      "offset": 1900.48,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "from from people like I've certainly",
      "offset": 1901.84,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "seen anthropic that are just like really",
      "offset": 1903.679,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "good at that inner loop. They can apply",
      "offset": 1905.76,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "it in one team and then move to a",
      "offset": 1908.32,
      "duration": 2.719
    },
    {
      "lang": "en",
      "text": "completely different domain and apply",
      "offset": 1910.08,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "that inner loop just as well. Yeah. Very",
      "offset": 1911.039,
      "duration": 5.281
    },
    {
      "lang": "en",
      "text": "cracked as the kids say. Uh shall we",
      "offset": 1913.44,
      "duration": 5.119
    },
    {
      "lang": "en",
      "text": "move to the history of Mechinterp? Yeah.",
      "offset": 1916.32,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "All I know is that everyone starts at",
      "offset": 1918.559,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "Chris Ola's blog. Is that right? I Yeah,",
      "offset": 1920.08,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "I think that's the correct answer. Chris",
      "offset": 1923.2,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "Ol's blog and then you know distill.pub",
      "offset": 1926,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "uh is the sort of natural next step. And",
      "offset": 1929.36,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "then I would say you know now there's",
      "offset": 1932,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "philanthropic there's transformer",
      "offset": 1934.48,
      "duration": 2.799
    },
    {
      "lang": "en",
      "text": "circuits which you talked about and but",
      "offset": 1935.919,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "there's also just a lot of mechan",
      "offset": 1937.279,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "research out there from you know I think",
      "offset": 1940.08,
      "duration": 5.599
    },
    {
      "lang": "en",
      "text": "like the yeah like matts is a group that",
      "offset": 1942.799,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "like regularly has a lot of research but",
      "offset": 1945.679,
      "duration": 3.281
    },
    {
      "lang": "en",
      "text": "there's just like many different labs",
      "offset": 1947.519,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "that that put research out there and I",
      "offset": 1948.96,
      "duration": 4.959
    },
    {
      "lang": "en",
      "text": "think that's also just like hammer home",
      "offset": 1951.279,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "the point that's because all you need is",
      "offset": 1953.919,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "like a model and then a willingness to",
      "offset": 1956.159,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "kind of investigate it to be able to",
      "offset": 1958.64,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "contribute to it. So, so now it's sort",
      "offset": 1960,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "of like there's been a bit of a Cambrian",
      "offset": 1961.44,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "explosion of me making interp which is",
      "offset": 1962.96,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "cool. I guess the history of it is just",
      "offset": 1964.32,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "computational like models that are not",
      "offset": 1967.36,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "decision trees uh models that are either",
      "offset": 1970.08,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "CNN's or let's say transformers have",
      "offset": 1972.32,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "just this really like strange property",
      "offset": 1975.44,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "that they don't give you interpretable",
      "offset": 1977.44,
      "duration": 5.359
    },
    {
      "lang": "en",
      "text": "intermediate states by default. You",
      "offset": 1980.64,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "know, again, to go back to if you were",
      "offset": 1982.799,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "training like a a decision tree on like",
      "offset": 1984.64,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "fraud data for an old school like bank",
      "offset": 1987.519,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "or something, then you can just look at",
      "offset": 1989.36,
      "duration": 2.64
    },
    {
      "lang": "en",
      "text": "your decision tree and be like, &quot;Oh,",
      "offset": 1990.88,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "it's learned that like if you make uh I",
      "offset": 1992,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "don't know, if the transaction is more",
      "offset": 1994.64,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "than $10,000 and it's for like perfume,",
      "offset": 1995.919,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "then maybe it's fraud or something.&quot; Uh",
      "offset": 1998.32,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "you can look at it and say, &quot;Cool, like",
      "offset": 2000.08,
      "duration": 2.719
    },
    {
      "lang": "en",
      "text": "that makes sense. I'm willing to ship",
      "offset": 2001.519,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "that model.&quot; But for for things like",
      "offset": 2002.799,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "like CNN's and like Transformers, we we",
      "offset": 2005.279,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "don't have that, right? What we have at",
      "offset": 2007.76,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "the end of training is just a massive",
      "offset": 2009.2,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "amount of weights that are connected",
      "offset": 2011.519,
      "duration": 5.681
    },
    {
      "lang": "en",
      "text": "somehow uh or activations are connected",
      "offset": 2014.08,
      "duration": 4.959
    },
    {
      "lang": "en",
      "text": "by some weights and who knows what these",
      "offset": 2017.2,
      "duration": 2.959
    },
    {
      "lang": "en",
      "text": "weights mean or what the intermediate",
      "offset": 2019.039,
      "duration": 3.041
    },
    {
      "lang": "en",
      "text": "activations mean and so the quest is to",
      "offset": 2020.159,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "understand that initially it was done a",
      "offset": 2022.08,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "lot of it was done on vision models",
      "offset": 2024.72,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "where you sort of have the emergence of",
      "offset": 2026.32,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "a lot of these ideas like what are",
      "offset": 2027.76,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "features what are circuits and then more",
      "offset": 2029.12,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "recently it's been mostly or not most",
      "offset": 2031.12,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "yeah mostly applied to NLP models but",
      "offset": 2034.159,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "also you know still there's work in",
      "offset": 2036.48,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "vision and there's work in like uh bio",
      "offset": 2038.64,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "and other domains. Yeah, I'm on Chris",
      "offset": 2041.12,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "Ola's blog and he has like the feature",
      "offset": 2043.12,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "visualization stuff. I think for me the",
      "offset": 2044.88,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "clearest was like the vision work where",
      "offset": 2046.559,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "you could have like this layer detects",
      "offset": 2049.04,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "edges, this layer detects textures,",
      "offset": 2050.639,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "whatever that seemed very clear to me,",
      "offset": 2052.639,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "but the transition to language models",
      "offset": 2054.72,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "seemed like a big leap. I think one one",
      "offset": 2056.72,
      "duration": 5.679
    },
    {
      "lang": "en",
      "text": "of the bigger changes from vision to to",
      "offset": 2059.359,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "like language models has to do with uh",
      "offset": 2062.399,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "the superposition hypothesis which maybe",
      "offset": 2064,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "is like that's the first and like point",
      "offset": 2066.879,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "models post right. Exactly. And this is",
      "offset": 2069.44,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "sort of like it turns out that if you",
      "offset": 2071.839,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "look at just the neurons of a lot of",
      "offset": 2073.52,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "vision models you can see neurons that",
      "offset": 2075.04,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "are curve detectors or that are edge",
      "offset": 2078,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "detectors or that are high low frequency",
      "offset": 2080.24,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "detectors and so you can sort of like",
      "offset": 2082.879,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "make sense of the neurons mostly. But if",
      "offset": 2084.96,
      "duration": 4.959
    },
    {
      "lang": "en",
      "text": "you look at neurons in language models,",
      "offset": 2088.159,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "most of them don't make sense. It's kind",
      "offset": 2089.919,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "of like unclear why or it was unclear",
      "offset": 2091.76,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "why that would be. And one main like",
      "offset": 2094.879,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "hypothesis here is the superposition",
      "offset": 2098,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "hypothesis. So what does that mean? That",
      "offset": 2099.76,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "means that like language models pack a",
      "offset": 2101.04,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "lot more in less space than vision",
      "offset": 2103.2,
      "duration": 7.04
    },
    {
      "lang": "en",
      "text": "models. So maybe like a a kind of like",
      "offset": 2106.96,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "really handwavy analogy, right? is like,",
      "offset": 2110.24,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "well, if you want curve detectors, like",
      "offset": 2112.24,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "you don't need that many curve",
      "offset": 2114.24,
      "duration": 2.56
    },
    {
      "lang": "en",
      "text": "detectors. You know, if each each curve",
      "offset": 2115.28,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "detector is going to detect like a a",
      "offset": 2116.8,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "quarter or 12th of a circle, like okay,",
      "offset": 2118.88,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "well, you have your all your curve",
      "offset": 2120.96,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "detectors, but think about all of the",
      "offset": 2121.92,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "concepts that like Claude or even GPT2",
      "offset": 2123.76,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "needs to to know like just in terms of",
      "offset": 2127.44,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "it needs to know about like all of the",
      "offset": 2129.92,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "different colors, all the different",
      "offset": 2131.52,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "hours of every day, all of the different",
      "offset": 2133.359,
      "duration": 2.801
    },
    {
      "lang": "en",
      "text": "cities in the world, all of the",
      "offset": 2134.8,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "different streets on every city. If you",
      "offset": 2136.16,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "just enumerate all of the facts that",
      "offset": 2138,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "like a model knows, you're going to get",
      "offset": 2139.599,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "like a very very long list and that list",
      "offset": 2141.359,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "is going to be way bigger than like the",
      "offset": 2142.8,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "number of neurons or even like size of",
      "offset": 2145.44,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "the residual stream which is where like",
      "offset": 2148.16,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "the models process information. And so",
      "offset": 2150,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "there's this sense in which like oh",
      "offset": 2151.839,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "there's more information than there's",
      "offset": 2153.68,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "like dimensions to represent it. And",
      "offset": 2155.2,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "that is much more true for language",
      "offset": 2157.44,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "models than for vision models. And so",
      "offset": 2159.2,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "because of that when you look at a part",
      "offset": 2161.52,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "of it, it just seems like it's like",
      "offset": 2163.119,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "there it's got all this stuff crammed",
      "offset": 2164.64,
      "duration": 2.959
    },
    {
      "lang": "en",
      "text": "into it. Whereas if you look at the",
      "offset": 2166.079,
      "duration": 2.721
    },
    {
      "lang": "en",
      "text": "vision models often times you could just",
      "offset": 2167.599,
      "duration": 2.641
    },
    {
      "lang": "en",
      "text": "like be like cool this is a curve",
      "offset": 2168.8,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "detector. Yeah V you have like some fun",
      "offset": 2170.24,
      "duration": 5.599
    },
    {
      "lang": "en",
      "text": "ways of explaining the toy models or",
      "offset": 2173.92,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "superp position concept. Yeah I mean",
      "offset": 2175.839,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "basically like you know if you have two",
      "offset": 2177.52,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "neurons and they can represent five",
      "offset": 2179.04,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "features like a lot of the early mechan",
      "offset": 2180.96,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "work says that you know there are more",
      "offset": 2183.359,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "features than we have neurons right. So",
      "offset": 2184.56,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "I guess my kind of question on this is",
      "offset": 2187.28,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "for those interested in getting into the",
      "offset": 2189.2,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "field what are like the key terms that",
      "offset": 2191.119,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "they should know what are like the few",
      "offset": 2193.119,
      "duration": 3.281
    },
    {
      "lang": "en",
      "text": "pieces that they should follow right",
      "offset": 2194.96,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "like from the anthropic side we had a",
      "offset": 2196.4,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "toy transformer model we had sparse we",
      "offset": 2198.24,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "first had autoenccoders that was the",
      "offset": 2200.96,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "second paper um right monos semanticity",
      "offset": 2202.8,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "what is sparsity and autoenccoders what",
      "offset": 2206.72,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "are transcoders like what is linear",
      "offset": 2209.04,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "probing what are these kind of like key",
      "offset": 2211.28,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "points that we had in mechinp and just",
      "offset": 2213.92,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "kind How would people get a quick, you",
      "offset": 2216.16,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "know, zero to like 80% of the field?",
      "offset": 2218.079,
      "duration": 5.121
    },
    {
      "lang": "en",
      "text": "Okay, so 0 to 80%. And now I realized I",
      "offset": 2220.48,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "really like set myself up for for",
      "offset": 2223.2,
      "duration": 2.639
    },
    {
      "lang": "en",
      "text": "failure cuz I was like, yeah, it's easy.",
      "offset": 2224.64,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "There's not that much to know. So, okay,",
      "offset": 2225.839,
      "duration": 3.121
    },
    {
      "lang": "en",
      "text": "so then then we should be able to cover",
      "offset": 2227.52,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "it all. Um, so superposition is the",
      "offset": 2228.96,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "first thing you should know, right? This",
      "offset": 2230.96,
      "duration": 2.639
    },
    {
      "lang": "en",
      "text": "idea that like there's a bunch of stuff",
      "offset": 2231.92,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "crowned in few dimensions as you said,",
      "offset": 2233.599,
      "duration": 3.041
    },
    {
      "lang": "en",
      "text": "maybe you have like two neurons and you",
      "offset": 2234.96,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "want to represent five things. So if",
      "offset": 2236.64,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "that's true and if you want to",
      "offset": 2238.64,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "understand how the model represents you",
      "offset": 2240.48,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "know I don't know the concept of red",
      "offset": 2242.8,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "let's say then you need some way to like",
      "offset": 2244.56,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "find out essentially in which direction",
      "offset": 2247.119,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "the model stores it. So after the the",
      "offset": 2249.599,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "sort of like superposition hypothesis",
      "offset": 2251.359,
      "duration": 3.521
    },
    {
      "lang": "en",
      "text": "you can think of like ah we also think",
      "offset": 2253.2,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "that like basically the model represents",
      "offset": 2254.88,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "these like individual concepts we're",
      "offset": 2256.72,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "going to call them features as like",
      "offset": 2258.4,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "directions. So if you have two neurons,",
      "offset": 2260.4,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "you can think of it as like it's like",
      "offset": 2262.72,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "the 2D plane and it's like a you can",
      "offset": 2263.76,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "have like five directions and maybe you",
      "offset": 2265.52,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "would like arrange them like the spokes",
      "offset": 2267.44,
      "duration": 2.6
    },
    {
      "lang": "en",
      "text": "of a wheel. So they're sort of like",
      "offset": 2268.8,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "maximally separate. It could mean that",
      "offset": 2270.04,
      "duration": 3.72
    },
    {
      "lang": "en",
      "text": "like you have one concept this way and",
      "offset": 2272.4,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "one concept that's like not fully",
      "offset": 2273.76,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "perpendicular to it but like pretty",
      "offset": 2275.52,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "pretty like far from it. And then that",
      "offset": 2276.96,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "would like allow the model to represent",
      "offset": 2279.04,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "more concepts and it has dimensions. And",
      "offset": 2281.2,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "so if that's true then what you want is",
      "offset": 2283.76,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "you want like a model that can extract",
      "offset": 2286.64,
      "duration": 4.959
    },
    {
      "lang": "en",
      "text": "these independent concepts and ideally",
      "offset": 2289.44,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "you want to do this like automatically",
      "offset": 2291.599,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "like can we just you know have a model",
      "offset": 2293.44,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "that tells us like oh like this",
      "offset": 2295.52,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "direction is red if you go that way",
      "offset": 2296.96,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "actually it's like I don't know chicken",
      "offset": 2298.88,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "and if you go that way it's like the",
      "offset": 2300.72,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "declaration of independence you know um",
      "offset": 2302.4,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "and so that's what sparse autoenccoders",
      "offset": 2304.96,
      "duration": 3.48
    },
    {
      "lang": "en",
      "text": "are it's almost like the the",
      "offset": 2307.2,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "self-supervised learning insight version",
      "offset": 2308.44,
      "duration": 4.04
    },
    {
      "lang": "en",
      "text": "like in in pre-training you had",
      "offset": 2311.28,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "self-supervised learning And here is so",
      "offset": 2312.48,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "self supervised interpretability. Yeah.",
      "offset": 2314.32,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "Exactly. Exactly. It's like an",
      "offset": 2316.72,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "unsupervised method and so unsupervised",
      "offset": 2318.079,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "methods often still have like labels in",
      "offset": 2320.96,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "the end. So so sometimes I feel like the",
      "offset": 2323.119,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "ter labels masking. Yeah. Like for for",
      "offset": 2324.96,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "pre-training, right? It's like the next",
      "offset": 2327.839,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "token. So in that sense, you have a",
      "offset": 2328.96,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "supervision signal. And here the",
      "offset": 2331.28,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "supervision signal is simply you take",
      "offset": 2332.8,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "the like neurons and then you learn a",
      "offset": 2334.96,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "model that's going to like expand them",
      "offset": 2338.48,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "into like the actual number of concepts",
      "offset": 2340.48,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "that you think there are in the model.",
      "offset": 2342.88,
      "duration": 2.08
    },
    {
      "lang": "en",
      "text": "So you have two neurons, you think",
      "offset": 2344.079,
      "duration": 2.321
    },
    {
      "lang": "en",
      "text": "there's five concepts. So you expand it",
      "offset": 2344.96,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "to like I think of dimension five and",
      "offset": 2346.4,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "then you contract it back to what it",
      "offset": 2348.64,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "was. That's like the model you're",
      "offset": 2350.8,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "training and then you're training it to",
      "offset": 2352.4,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "incentivize it to be sparse so that",
      "offset": 2354,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "there's only like a few features active",
      "offset": 2356.16,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "at a time. And once you do that, if it",
      "offset": 2357.76,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "works, you have this sort of like nice",
      "offset": 2359.76,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "dictionary which you can think as like a",
      "offset": 2362.32,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "way to decode the activate the neurons",
      "offset": 2363.839,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "where you're saying like, &quot;Ah, cool. I",
      "offset": 2365.92,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "don't know what this what this direction",
      "offset": 2368.16,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "means, but I've like used my model and",
      "offset": 2369.52,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "it's telling me that the model is",
      "offset": 2371.92,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "writing in the red direction.&quot; And so",
      "offset": 2373.04,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "that's that's sort of like I think maybe",
      "offset": 2374.88,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "the biggest thing to understand is is",
      "offset": 2376.4,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "this combination of things of like ah we",
      "offset": 2378.8,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "have too few dimensions. We pack a lot",
      "offset": 2380.8,
      "duration": 3.039
    },
    {
      "lang": "en",
      "text": "into it. So, we're going to learn an",
      "offset": 2382.24,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "unsupervised way to like unpack it and",
      "offset": 2383.839,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "then analyze what each of those",
      "offset": 2386.32,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "dimensions that we've unpacked are. Any",
      "offset": 2388.079,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "follow-ups? Yeah, I mean the follow-ups",
      "offset": 2390.24,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "of this are also kind of like some of",
      "offset": 2392.079,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "the work that you did is in clamping,",
      "offset": 2393.68,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "right? What is the applicable side of",
      "offset": 2395.119,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "mechan, right? So, we saw that you guys",
      "offset": 2397.599,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "have like great visualizations. Golden",
      "offset": 2400,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "gate cla was a cool example. I was going",
      "offset": 2401.839,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "to say that. Yeah, that's my favorite.",
      "offset": 2403.599,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "What can we do once we find these",
      "offset": 2405.44,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "features? Finding features is cool, but",
      "offset": 2407.119,
      "duration": 5.041
    },
    {
      "lang": "en",
      "text": "what can we do about it? Yeah, I think",
      "offset": 2409.76,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "there's kind of like two big aspects of",
      "offset": 2412.16,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "this. Like one is yeah, okay, so we go",
      "offset": 2414.56,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "from a state where as I said the model",
      "offset": 2416.96,
      "duration": 3.119
    },
    {
      "lang": "en",
      "text": "is like a mess of weights. We have no",
      "offset": 2418.96,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "idea what's going on to okay, we found",
      "offset": 2420.079,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "features. We found a feature for red, a",
      "offset": 2421.68,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "feature for Golden Gate Cloud for the",
      "offset": 2423.44,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "Golden Gate Bridge, I should say. Like",
      "offset": 2425.04,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "what do we do with them? And well, if",
      "offset": 2426.72,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "these are true features, that means that",
      "offset": 2429.92,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "like they in some sense are important",
      "offset": 2432.72,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "for the model or it wouldn't be like",
      "offset": 2434.8,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "representing it. Like if the model is",
      "offset": 2436.079,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "like bothering to like write, you know,",
      "offset": 2437.68,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "in the Golden Gate Bridge direction,",
      "offset": 2439.68,
      "duration": 2.64
    },
    {
      "lang": "en",
      "text": "it's usually because it's going to like",
      "offset": 2441.2,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "talk about the Golden Gate Bridge. And",
      "offset": 2442.32,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "so that means that like if that's true,",
      "offset": 2444,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "then you can like set that feature to",
      "offset": 2446.48,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "zero art or officially set to 100 and",
      "offset": 2448.079,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "you'll change model behavior. Uh that's",
      "offset": 2450.88,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "what we did when we did Golden Gate",
      "offset": 2453.359,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "Cloud in which we found a feature that",
      "offset": 2454.72,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "represents a direction for the Golden",
      "offset": 2456.319,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "Gate Bridge and then we just like set it",
      "offset": 2457.76,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "to always be on and then you could talk",
      "offset": 2459.92,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "to Claude and be like, &quot;Hey, like",
      "offset": 2461.92,
      "duration": 2.56
    },
    {
      "lang": "en",
      "text": "Claude, what's on your mind?&quot; You know,",
      "offset": 2463.44,
      "duration": 2
    },
    {
      "lang": "en",
      "text": "like what are you thinking about today?",
      "offset": 2464.48,
      "duration": 2
    },
    {
      "lang": "en",
      "text": "Be like the Golden Gate Bridge. She'd be",
      "offset": 2465.44,
      "duration": 2.32
    },
    {
      "lang": "en",
      "text": "like, &quot;Hey, Claude, like what's 2 plus",
      "offset": 2466.48,
      "duration": 2.4
    },
    {
      "lang": "en",
      "text": "two?&quot; It'd be like four Golden Gate",
      "offset": 2467.76,
      "duration": 3.96
    },
    {
      "lang": "en",
      "text": "bridges, uh, etc. Right. And it was",
      "offset": 2468.88,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "always thinking about the write a poem",
      "offset": 2471.72,
      "duration": 4.2
    },
    {
      "lang": "en",
      "text": "and it starts talking about how it's",
      "offset": 2474.16,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "like read like the Golden Gate Plate",
      "offset": 2475.92,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "Bridge. Yeah. Amazing. I think what made",
      "offset": 2478.56,
      "duration": 3.279
    },
    {
      "lang": "en",
      "text": "it even better is like we realized later",
      "offset": 2480.24,
      "duration": 2.879
    },
    {
      "lang": "en",
      "text": "on that it wasn't really like a Golden",
      "offset": 2481.839,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "Gate Bridge feature. It was like being",
      "offset": 2483.119,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "in awe at the beauty of the majestic",
      "offset": 2485.599,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "Golden Gate Bridge, right? So on top of",
      "offset": 2487.839,
      "duration": 2.561
    },
    {
      "lang": "en",
      "text": "it would like really ham it up. You'd be",
      "offset": 2489.119,
      "duration": 2.561
    },
    {
      "lang": "en",
      "text": "like, &quot;Oh, I'm just thinking about the",
      "offset": 2490.4,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "beautiful international orange color of",
      "offset": 2491.68,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "the Golden Gate Bridge.&quot; That was just",
      "offset": 2493.44,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "like an example that I think was like",
      "offset": 2495.2,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "really striking but of of sort of like",
      "offset": 2496.64,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "oh if you found like a space where that",
      "offset": 2499.599,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "represents some computation or some",
      "offset": 2502.48,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "representition of the model that means",
      "offset": 2504.4,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "that you can like artificially suppress",
      "offset": 2506.56,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "or promote it and that means that like",
      "offset": 2508.64,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "you're starting to understand at a very",
      "offset": 2511.599,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "high level a very gross level like how",
      "offset": 2513.76,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "some of the model works right we've gone",
      "offset": 2515.52,
      "duration": 2.64
    },
    {
      "lang": "en",
      "text": "from like I don't know anything about it",
      "offset": 2516.96,
      "duration": 2.879
    },
    {
      "lang": "en",
      "text": "to like oh I know that this like",
      "offset": 2518.16,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "combination of neurons is this and I'm",
      "offset": 2519.839,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "going to prove it to you the next step",
      "offset": 2521.839,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "which is what this works on is like",
      "offset": 2523.76,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "that's kind of like thinking of if maybe",
      "offset": 2525.68,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "you take the an analogy of like um I",
      "offset": 2528.16,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "don't know like like let's take the",
      "offset": 2530.72,
      "duration": 2.879
    },
    {
      "lang": "en",
      "text": "analogy of like an MRI or something like",
      "offset": 2531.68,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "a brain scan. It tells you like oh like",
      "offset": 2533.599,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "this as Claude was answering at some",
      "offset": 2536.4,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "point it thought about this thing but",
      "offset": 2538.48,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "it's a sort of like vague like basically",
      "offset": 2540.4,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "maybe it's like a like a bag of words",
      "offset": 2542.16,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "kind of like a bag of features you like",
      "offset": 2543.839,
      "duration": 2.721
    },
    {
      "lang": "en",
      "text": "here are all the random things it",
      "offset": 2545.44,
      "duration": 2.639
    },
    {
      "lang": "en",
      "text": "thought about but what you might want to",
      "offset": 2546.56,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "know is like okay but claude is doing",
      "offset": 2548.079,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "some processing like sometimes to get to",
      "offset": 2550.24,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "the Golden Gate Bridge it had to realize",
      "offset": 2551.839,
      "duration": 2.321
    },
    {
      "lang": "en",
      "text": "that you were talking about San",
      "offset": 2553.28,
      "duration": 2.799
    },
    {
      "lang": "en",
      "text": "Francisco and about like the best way to",
      "offset": 2554.16,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "go to Sonoma or something and so that's",
      "offset": 2556.079,
      "duration": 2.641
    },
    {
      "lang": "en",
      "text": "how it got to Golden Gate Bridge.",
      "offset": 2557.52,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "There's like an algorithm that leads to",
      "offset": 2558.72,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "it at some point thinking about the gong",
      "offset": 2561.04,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "gate bridge and basically like there's",
      "offset": 2562.48,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "like a way to connect features to say",
      "offset": 2564.56,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "like oh from this input went to these",
      "offset": 2566.8,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "few features and these few features and",
      "offset": 2569.359,
      "duration": 2.561
    },
    {
      "lang": "en",
      "text": "these few features and that one",
      "offset": 2570.72,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "influenced this one and then you got to",
      "offset": 2571.92,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "the output and so that's the second part",
      "offset": 2573.44,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "and the part we worked on is like you",
      "offset": 2574.88,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "have the features now connect them in",
      "offset": 2576.64,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "what we call uh or what's called",
      "offset": 2578.72,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "circuits which is sort of like",
      "offset": 2580.4,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "explaining the the like algorithm. Yeah.",
      "offset": 2581.92,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "Before we move directly on to your work,",
      "offset": 2584.64,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "I just want to give a shout out to Neil",
      "offset": 2587.44,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "Nanda. He did Neuronedia and released a",
      "offset": 2589.04,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "bunch of essays for I think the llama",
      "offset": 2591.599,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "models and the Gemma models. And the",
      "offset": 2593.68,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "Gemma models. Yeah. Uh so I actually",
      "offset": 2594.96,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "made Golden Gate Gemma. Just up the",
      "offset": 2596.4,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "weights for proper nouns and names of",
      "offset": 2598.48,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "places of people and references to the",
      "offset": 2600,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "term golden likely relating to awards,",
      "offset": 2602.24,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "honors or special names. And that",
      "offset": 2604.96,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "together made Golden Gate. That's",
      "offset": 2606.72,
      "duration": 5.119
    },
    {
      "lang": "en",
      "text": "amazing. Yeah. So you can make Golden",
      "offset": 2609.2,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "Gate Gemma and like I think that's",
      "offset": 2611.839,
      "duration": 2.401
    },
    {
      "lang": "en",
      "text": "that's a fun way to experiment with",
      "offset": 2613.04,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "this. Uh but yeah, we can move on to I'm",
      "offset": 2614.24,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "curious. I'm curious what's the",
      "offset": 2616.72,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "background behind why you shipped Golden",
      "offset": 2618.079,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "Gate Claw. Like you had so many",
      "offset": 2620.24,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "features. Just any fun story behind why",
      "offset": 2622.24,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "that's the one that made it. You know,",
      "offset": 2624.96,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "it's funny. If you look at the paper,",
      "offset": 2626.96,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "there's just a bunch of like Yeah. Like",
      "offset": 2629.52,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "really interesting features, right?",
      "offset": 2631.119,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "There's like one of my favorite ones was",
      "offset": 2633.04,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "the psychopantic praise, which I guess",
      "offset": 2634.96,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "is very topical right now. Very topical.",
      "offset": 2637.04,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "Um, but you know, it's like you could",
      "offset": 2638.72,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "dial that up and like Claude would just",
      "offset": 2640,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "really praise you. He'd be like, &quot;Oh,",
      "offset": 2642,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "you know, like I wrote this poem, like",
      "offset": 2644.319,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "roses are red, violets are blue,",
      "offset": 2646.4,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "whatever.&quot; And he'd be like, &quot;That's the",
      "offset": 2649.04,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "best poem I've ever seen.&quot; Um, and so we",
      "offset": 2650.48,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "could have shipped that. That could have",
      "offset": 2653.52,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "been funny. Uh, Golden Eye Claude was",
      "offset": 2654.72,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "like a pure, as far as I remember at",
      "offset": 2657.599,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "least, like a pure just like weird",
      "offset": 2659.76,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "random thing where like somebody found",
      "offset": 2662.56,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "it initially, we had an internal demo of",
      "offset": 2664.319,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "it. Everybody thought it was hilarious",
      "offset": 2665.92,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "and then that's sort of how it came on.",
      "offset": 2667.359,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "There was no nobody had a list of top 10",
      "offset": 2669.359,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "features we should consider shipping and",
      "offset": 2671.76,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "we picked that one. It was just kind of",
      "offset": 2673.599,
      "duration": 3.121
    },
    {
      "lang": "en",
      "text": "like a very organic moment. No, like the",
      "offset": 2674.88,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "the marketing team really leaned into",
      "offset": 2676.72,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "it. Like they mailed out pieces of the",
      "offset": 2678.4,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "Golden Gate for people in Europe I think",
      "offset": 2680.16,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "or ICML. Yeah, it was it was fantastic",
      "offset": 2682.8,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "marketing. Yeah. The question obviously",
      "offset": 2685.359,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "is like if OpenAI had invested more",
      "offset": 2686.88,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "interpretability, would they have caught",
      "offset": 2689.04,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "uh the GPT40 update? Uh but we don't",
      "offset": 2691.04,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "know that for sure because they have",
      "offset": 2694,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "interp teams. They just Yeah. I think",
      "offset": 2695.76,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "also like for that one I don't know that",
      "offset": 2698.24,
      "duration": 2.48
    },
    {
      "lang": "en",
      "text": "you need interp. Like it was pretty",
      "offset": 2699.44,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "clearcut to the model. I was like a that",
      "offset": 2700.72,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "model's really gassing me up. And then",
      "offset": 2702.8,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "the other thing is um can you just like",
      "offset": 2704.24,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "up write good code don't write bad code",
      "offset": 2705.92,
      "duration": 7.52
    },
    {
      "lang": "en",
      "text": "and make s and like it feels too too",
      "offset": 2708.88,
      "duration": 7.28
    },
    {
      "lang": "en",
      "text": "easy too free. Is that steering that",
      "offset": 2713.44,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "powerful that you can just like up and",
      "offset": 2716.16,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "down features with no trade-offs? There",
      "offset": 2717.68,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "was like a phase where people were",
      "offset": 2719.2,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "basically saying, you know, 3.5 and 3.7",
      "offset": 2720.64,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "are just now because they came out right",
      "offset": 2723.44,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "after and for the record like that's",
      "offset": 2725.28,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "been debunked. It has been debunked.",
      "offset": 2726.72,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "But, you know, it had people convinced",
      "offset": 2728.48,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "that what people did is they basically",
      "offset": 2730.24,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "just steered up and steered down",
      "offset": 2732.4,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "features and now we have a better model.",
      "offset": 2733.839,
      "duration": 2.721
    },
    {
      "lang": "en",
      "text": "And this kind of goes back to that",
      "offset": 2735.44,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "original question of right like why do",
      "offset": 2736.56,
      "duration": 3.519
    },
    {
      "lang": "en",
      "text": "we do this? What can we do? Some people",
      "offset": 2738.56,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "are like I want tracing from a sense of",
      "offset": 2740.079,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "you know legality like what did the",
      "offset": 2742.88,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "model think when it came to this output?",
      "offset": 2744.8,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "Some people want to turn hallucination",
      "offset": 2746.72,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "down. Some people want to turn coding",
      "offset": 2748.4,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "up. So like what are some like whether",
      "offset": 2750,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "it's internal what are you exploring",
      "offset": 2752.96,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "that like what are the applications of",
      "offset": 2754.56,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "this? Whether it's open-ended of what",
      "offset": 2756.319,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "people can do about this or just like",
      "offset": 2757.92,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "yeah why why do mechan you know? Yeah",
      "offset": 2759.599,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "there's like a few things here. So so",
      "offset": 2762.4,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "like first of all obviously this is I",
      "offset": 2763.68,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "would say on the scale of the most",
      "offset": 2765.92,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "short-term to the most long-term like",
      "offset": 2768.88,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "pretty long-term research. So in terms",
      "offset": 2770.8,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "of like applications compared to you",
      "offset": 2772.4,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "know like the research work we do on",
      "offset": 2774.24,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "like fine-tuning or whatever interp is",
      "offset": 2775.92,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "much more you know sort of like a a",
      "offset": 2778.16,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "high-risk high reward kind of approach.",
      "offset": 2780.24,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "Uh, with that being said, like I think",
      "offset": 2783.04,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "there's just a fundamental sense in",
      "offset": 2785.04,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "which Michael Nielsen had a had a post",
      "offset": 2786.72,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "recently about how like knowledge is",
      "offset": 2789.04,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "dual use or something, but just like",
      "offset": 2790.88,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "just like knowing how the model works at",
      "offset": 2792.72,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "all feels useful. And you know, it's",
      "offset": 2794.96,
      "duration": 6.879
    },
    {
      "lang": "en",
      "text": "hard to argue that if we know how the",
      "offset": 2798.8,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "model works and understand all of the",
      "offset": 2801.839,
      "duration": 2.881
    },
    {
      "lang": "en",
      "text": "components that won't help us like make",
      "offset": 2803.2,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "models that hallucinate less, for",
      "offset": 2804.72,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "example, or that are like less biased.",
      "offset": 2806,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "seems, you know, if if like at the",
      "offset": 2807.68,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "limit, yeah, that totally seems like",
      "offset": 2809.68,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "something you could do using basically",
      "offset": 2811.359,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "like your understanding of the model to",
      "offset": 2813.2,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "improve it. I think for now, as we can",
      "offset": 2814.56,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "talk about a little bit with like uh",
      "offset": 2817.04,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "circuits, there's like we're still",
      "offset": 2818.56,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "pretty early on in the game, right? And",
      "offset": 2820.48,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "so right now the main way that we're",
      "offset": 2822.96,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "using interpoly is like to investigate",
      "offset": 2825.359,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "specific behaviors and understand them",
      "offset": 2827.119,
      "duration": 5.361
    },
    {
      "lang": "en",
      "text": "and gain a sense for uh what's causing",
      "offset": 2829.76,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "them. So like one example that we we can",
      "offset": 2832.48,
      "duration": 3.119
    },
    {
      "lang": "en",
      "text": "talk about later, we can talk about now,",
      "offset": 2834.4,
      "duration": 2.48
    },
    {
      "lang": "en",
      "text": "but in the paper we investigate",
      "offset": 2835.599,
      "duration": 3.041
    },
    {
      "lang": "en",
      "text": "jailbreaks and we try to see like why",
      "offset": 2836.88,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "does a jailbreak work and then we",
      "offset": 2838.64,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "realize as we're looking at this",
      "offset": 2840.4,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "jailbreak that part of the reason why",
      "offset": 2842,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "Claude is telling you how to make a bomb",
      "offset": 2843.28,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "in this case is that it's like already",
      "offset": 2844.96,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "started to tell you how to make a bomb",
      "offset": 2848,
      "duration": 2.64
    },
    {
      "lang": "en",
      "text": "and it would really love to stop telling",
      "offset": 2849.119,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "you how to make a bomb, but it has to",
      "offset": 2850.64,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "first finish its sentence. Like it",
      "offset": 2853.2,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "really wants to make correct grammatical",
      "offset": 2854.72,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "sentences. And so it turns out that like",
      "offset": 2856.4,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "seeing that circuit, we were like, &quot;Ah,",
      "offset": 2858.64,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "then does that mean if I prevent it from",
      "offset": 2860.96,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "finishing its sentence, the jailbreak",
      "offset": 2862.88,
      "duration": 2.64
    },
    {
      "lang": "en",
      "text": "works even better?&quot; And sure enough, it",
      "offset": 2864.24,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "does. And so I think like the level of",
      "offset": 2865.52,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "sort of practical application right now",
      "offset": 2867.52,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "is of that shape. So like understanding",
      "offset": 2869.599,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "either like quirks of a current model or",
      "offset": 2872.24,
      "duration": 5.839
    },
    {
      "lang": "en",
      "text": "like how it does tasks that maybe we",
      "offset": 2875.599,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "don't we don't even know how it does it",
      "offset": 2878.079,
      "duration": 2.961
    },
    {
      "lang": "en",
      "text": "like you know we we have like some",
      "offset": 2879.76,
      "duration": 2.559
    },
    {
      "lang": "en",
      "text": "planning examples where we had no idea",
      "offset": 2881.04,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "it was planning and we're like oh god it",
      "offset": 2882.319,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "is. That's sort of like the current",
      "offset": 2883.92,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "state we're at. I'm curious internally",
      "offset": 2886,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "how this kind of feeds back into like",
      "offset": 2888,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "the research, the architecture, the",
      "offset": 2889.76,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "pre-training teams, the post- training,",
      "offset": 2891.28,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "like is there a good feedback loop",
      "offset": 2892.88,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "there? Like right now there's a lot of",
      "offset": 2894.4,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "external people interested, right? Like",
      "offset": 2896.079,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "we'll train an SAPE on one layer of",
      "offset": 2898.24,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "llama and probe around, but then people",
      "offset": 2900.16,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "are like, okay, how does this have much",
      "offset": 2902.4,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "impact? People like clamping, but yeah,",
      "offset": 2903.839,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "as you said, you know, once you start to",
      "offset": 2906.079,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "understand these models, have this early",
      "offset": 2907.76,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "planning and stuff, how does this kind",
      "offset": 2909.839,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "of feedback? I don't know that there's",
      "offset": 2911.28,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "there's like much to say here other than",
      "offset": 2913.119,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "like I think we're definitely interested",
      "offset": 2914.8,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "in conversely like making models for",
      "offset": 2916.48,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "which it's like easier to interpret",
      "offset": 2919.76,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "them. So that's also something that you",
      "offset": 2922,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "can imagine sort of like working on",
      "offset": 2923.599,
      "duration": 2.561
    },
    {
      "lang": "en",
      "text": "which is like making models where you",
      "offset": 2924.72,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "have to work less hard to try to",
      "offset": 2926.16,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "understand what they're doing",
      "offset": 2928.16,
      "duration": 2.32
    },
    {
      "lang": "en",
      "text": "architecture.",
      "offset": 2928.96,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "Okay. Yeah. Yeah. So I think there was a",
      "offset": 2930.48,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "there's a less wrong post about this of",
      "offset": 2931.92,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "like there's a nonzero amount of",
      "offset": 2934.079,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "sacrifice you should make in current",
      "offset": 2935.92,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "capabilities in order to actually make",
      "offset": 2937.28,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "them more interpretable because",
      "offset": 2939.76,
      "duration": 2.559
    },
    {
      "lang": "en",
      "text": "otherwise they will never catch up. You",
      "offset": 2940.8,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "know there's this sort of sense in which",
      "offset": 2942.319,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "like right now we take the model and",
      "offset": 2943.52,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "then the model's a model and then we",
      "offset": 2945.359,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "post hawk do these replacement layers to",
      "offset": 2947.2,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "try to understand it. But of course when",
      "offset": 2949.76,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "we do that we don't like fully capture",
      "offset": 2951.04,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "everything that's happening inside the",
      "offset": 2953.52,
      "duration": 2.48
    },
    {
      "lang": "en",
      "text": "model. We're capturing like a subset.",
      "offset": 2954.64,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "And so maybe some of it is like you",
      "offset": 2956,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "could train a model that's sort of like",
      "offset": 2958.24,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "easier to interpret negatively. And it's",
      "offset": 2960.079,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "possible that like you don't even have",
      "offset": 2961.599,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "that much of you know like a tax in that",
      "offset": 2963.119,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "sense and you can just sort of like",
      "offset": 2965.2,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "either like train your model differently",
      "offset": 2966.8,
      "duration": 2.799
    },
    {
      "lang": "en",
      "text": "or do like a little post hawk step to",
      "offset": 2967.92,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "like sort of like untangle some of the",
      "offset": 2969.599,
      "duration": 2.48
    },
    {
      "lang": "en",
      "text": "mess that you've made when you trained",
      "offset": 2970.88,
      "duration": 2.4
    },
    {
      "lang": "en",
      "text": "your model right make it easier to",
      "offset": 2972.079,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "interpret. Um the hope was pruning would",
      "offset": 2973.28,
      "duration": 5.279
    },
    {
      "lang": "en",
      "text": "do some of that but I feel like that",
      "offset": 2975.52,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "aerial research has just died. What kind",
      "offset": 2978.559,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "of pruning are you thinking of here? Uh",
      "offset": 2980.48,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "just pruning your network. Ah, yeah.",
      "offset": 2982,
      "duration": 5.599
    },
    {
      "lang": "en",
      "text": "Printing layers, printing connections,",
      "offset": 2985.119,
      "duration": 5.761
    },
    {
      "lang": "en",
      "text": "whatever. Yeah. I feel like maybe this",
      "offset": 2987.599,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "is something where like superp position",
      "offset": 2990.88,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "makes me less hopeful or something cuz",
      "offset": 2992.4,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "I'm like you don't know like that that",
      "offset": 2994.16,
      "duration": 4.959
    },
    {
      "lang": "en",
      "text": "like seventh bit might hold something.",
      "offset": 2995.92,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "Well, right. And it's like on on each",
      "offset": 2999.119,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "example maybe this neuron is like at the",
      "offset": 3001.2,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "bottom of like what matters but actually",
      "offset": 3003.2,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "it's participating like 5% to like",
      "offset": 3005.52,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "understanding English like doing",
      "offset": 3008,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "integrals and you know like whatever",
      "offset": 3009.839,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "like cracking codes or something and",
      "offset": 3012.319,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "it's like because that is just like",
      "offset": 3014.16,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "distributed over it you you kind of like",
      "offset": 3015.599,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "when you naively prune you might miss",
      "offset": 3018.079,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "that. I don't know. Okay. So then this",
      "offset": 3019.599,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "area of research in terms of creating",
      "offset": 3022.16,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "models that are easier to interpret from",
      "offset": 3024.16,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "the from the start is there a name for",
      "offset": 3026,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "this field of research? I don't think",
      "offset": 3028,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "so. And I think this is like very early",
      "offset": 3030.079,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "and it's it's mostly like a case.",
      "offset": 3031.92,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "There's a thing people want to double",
      "offset": 3033.68,
      "duration": 2.639
    },
    {
      "lang": "en",
      "text": "click on. Yeah. Yeah. I haven't come",
      "offset": 3034.8,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "across it. I think the higher level is",
      "offset": 3036.319,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "like Dario recently put out a post about",
      "offset": 3038,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "this, right? Why mechan is so interpret",
      "offset": 3040,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "important. You know, we don't want to",
      "offset": 3042.319,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "fall behind. We want to be able to",
      "offset": 3044.48,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "interpret models and understand what's",
      "offset": 3046.8,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "going on even though capabilities are",
      "offset": 3048.559,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "getting so good. It kind of ties into",
      "offset": 3050.4,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "this topic, right? Like we want models",
      "offset": 3051.839,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "to be slightly easier to interpret so we",
      "offset": 3054.48,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "don't fall behind so far. Well, yeah.",
      "offset": 3056.64,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "And I think here like just to talk about",
      "offset": 3058.319,
      "duration": 2.881
    },
    {
      "lang": "en",
      "text": "the elephant in the room or something",
      "offset": 3060.319,
      "duration": 2.401
    },
    {
      "lang": "en",
      "text": "like like one big concern here is is",
      "offset": 3061.2,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "like safety right and so like as models",
      "offset": 3062.72,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "get better they are going to be used",
      "offset": 3065.52,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "more and more places you know it's like",
      "offset": 3067.52,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "you're now going to have your you know",
      "offset": 3070.079,
      "duration": 2.801
    },
    {
      "lang": "en",
      "text": "we're vibe coding right now maybe at",
      "offset": 3071.52,
      "duration": 2.799
    },
    {
      "lang": "en",
      "text": "some point well that that'll just be",
      "offset": 3072.88,
      "duration": 2.719
    },
    {
      "lang": "en",
      "text": "coding it's like cloud's going to write",
      "offset": 3074.319,
      "duration": 2.321
    },
    {
      "lang": "en",
      "text": "your code for you and that's it and",
      "offset": 3075.599,
      "duration": 2.081
    },
    {
      "lang": "en",
      "text": "cloud's going to review the code that",
      "offset": 3076.64,
      "duration": 2.08
    },
    {
      "lang": "en",
      "text": "cloud wrote and then cloud's going to",
      "offset": 3077.68,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "deploy to production. Um, and at at some",
      "offset": 3078.72,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "point like as these models get",
      "offset": 3081.52,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "integrated deeper and deeper into more",
      "offset": 3083.76,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "and more workflows, it gets just scarier",
      "offset": 3085.839,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "and scarier to know nothing about them.",
      "offset": 3088,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "And so you kind of want your ability to",
      "offset": 3089.839,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "understand the model to scale with like",
      "offset": 3092.24,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "how good the model is doing, which that",
      "offset": 3094.079,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "itself kind of like tends to scale with",
      "offset": 3096.559,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "like how widely deployed it is. So as we",
      "offset": 3098.24,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "like deploy them everywhere, we want to",
      "offset": 3100.16,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "like understand them better. The version",
      "offset": 3102.24,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "that I liked from the old super",
      "offset": 3104.16,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "alignment team was weak to strong",
      "offset": 3106.24,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "generalization or weak to strong",
      "offset": 3109.2,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "alignment which that's what super",
      "offset": 3110.96,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "alignment to me was and that was my",
      "offset": 3112.8,
      "duration": 3.519
    },
    {
      "lang": "en",
      "text": "first aha moment of like oh yeah some at",
      "offset": 3114.16,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "some point these things will be smarter",
      "offset": 3116.319,
      "duration": 2.641
    },
    {
      "lang": "en",
      "text": "than us and in many ways they already",
      "offset": 3117.44,
      "duration": 3.119
    },
    {
      "lang": "en",
      "text": "are smarter than us and we rely on them",
      "offset": 3118.96,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "more and more we need to figure out how",
      "offset": 3120.559,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "to control them and this is not a like a",
      "offset": 3122.559,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "ilizer yakowski like ah thing it's just",
      "offset": 3124.24,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "more like we don't know what how these",
      "offset": 3126.72,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "things work like how can we use them",
      "offset": 3128.96,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "yeah and like you can think of it",
      "offset": 3131.04,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "There's many ways to solve a problem and",
      "offset": 3133.28,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "some of them if the model is solving it",
      "offset": 3135.599,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "in like a dumb way or in like memorized",
      "offset": 3137.359,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "one approach to do it then you shouldn't",
      "offset": 3139.839,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "deploy it to do like a general thing",
      "offset": 3142.079,
      "duration": 2.801
    },
    {
      "lang": "en",
      "text": "like it like you could look at how it",
      "offset": 3143.599,
      "duration": 2.641
    },
    {
      "lang": "en",
      "text": "does math and based on your",
      "offset": 3144.88,
      "duration": 2.479
    },
    {
      "lang": "en",
      "text": "understanding of how it does math you're",
      "offset": 3146.24,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "like okay I feel comfortable using this",
      "offset": 3147.359,
      "duration": 3.521
    },
    {
      "lang": "en",
      "text": "as a calculator or like no it should",
      "offset": 3149.04,
      "duration": 3.279
    },
    {
      "lang": "en",
      "text": "always use a calculator tool because",
      "offset": 3150.88,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "it's doing math in a stupid way and",
      "offset": 3152.319,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "extend that to any behavior right where",
      "offset": 3154.24,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "it's just a matter of like think about",
      "offset": 3156.72,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "it if if like you're like in the 1500s",
      "offset": 3158.48,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "and I give you a car or thing and I'm",
      "offset": 3160.48,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "just like cool. Like this thing when you",
      "offset": 3162.4,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "press on this like it accelerates when",
      "offset": 3164.48,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "you press on that like it stops. You",
      "offset": 3166.64,
      "duration": 2.719
    },
    {
      "lang": "en",
      "text": "know this steering wheel seems to be",
      "offset": 3168.24,
      "duration": 2.56
    },
    {
      "lang": "en",
      "text": "doing stuff but you knew nothing about",
      "offset": 3169.359,
      "duration": 3.281
    },
    {
      "lang": "en",
      "text": "it. I don't know if it was like a super",
      "offset": 3170.8,
      "duration": 3.279
    },
    {
      "lang": "en",
      "text": "faulty car and it's like oh yeah but if",
      "offset": 3172.64,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "you ever get went above 60 mph like it",
      "offset": 3174.079,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "explodes or something like you probably",
      "offset": 3176.48,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "would be sort of like you'd want to",
      "offset": 3177.599,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "understand the nature of the object",
      "offset": 3179.44,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "before like jumping in in it. And so",
      "offset": 3181.04,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "that's why we like understand how cars",
      "offset": 3183.68,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "work very well because we make them.",
      "offset": 3185.28,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "LLMs are sort of like and ML models in",
      "offset": 3186.64,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "general are like this very rare artifact",
      "offset": 3188.48,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "where we like make them but we don't we",
      "offset": 3190.64,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "have no idea how they work. We evolve",
      "offset": 3192,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "them. We create conditions for them to",
      "offset": 3193.52,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "evolve and then they evolve and we're",
      "offset": 3194.88,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "like cool like you know maybe you got a",
      "offset": 3196.48,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "good run, maybe we didn't. Yeah. Don't",
      "offset": 3198.96,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "really know. Yeah. The extent to which",
      "offset": 3201.04,
      "duration": 2.799
    },
    {
      "lang": "en",
      "text": "you know how it works is you have your",
      "offset": 3202.88,
      "duration": 2.64
    },
    {
      "lang": "en",
      "text": "like eval and you're like oh well seems",
      "offset": 3203.839,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "to be doing well on this eval. And then",
      "offset": 3205.52,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "you're like is it because this wasn't a",
      "offset": 3207.2,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "training set or is it like actually",
      "offset": 3208.96,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "generalizing? I don't know. Uh my",
      "offset": 3210.48,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "favorite example was somehow C4 the the",
      "offset": 3212,
      "duration": 6.079
    },
    {
      "lang": "en",
      "text": "common the colossal clean corpus did",
      "offset": 3215.44,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "much better than uh common crawl even",
      "offset": 3218.079,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "though it filtered out most of this like",
      "offset": 3221.04,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "it was very prudish. So it like filters",
      "offset": 3224.079,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "out anything that could be considered",
      "offset": 3226,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "obscene, including the word gay. But",
      "offset": 3227.76,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "like somehow it just like when you add",
      "offset": 3230.319,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "it into the data mix, it just does super",
      "offset": 3231.76,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "well. And it's just like this magic",
      "offset": 3233.599,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "incantation of like this this recipe",
      "offset": 3234.96,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "works. Just trust us. Like we've tried",
      "offset": 3237.2,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "everything. This one works. So just go",
      "offset": 3239.2,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "with it. Yeah. It's not very satisfying.",
      "offset": 3240.88,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "No, it's not. The side that you're",
      "offset": 3244.079,
      "duration": 2.48
    },
    {
      "lang": "en",
      "text": "talking about, which is like, okay, like",
      "offset": 3245.76,
      "duration": 1.76
    },
    {
      "lang": "en",
      "text": "how do you make these? And it's kind of",
      "offset": 3246.559,
      "duration": 2.161
    },
    {
      "lang": "en",
      "text": "unsatisfying that you just kind of make",
      "offset": 3247.52,
      "duration": 3.039
    },
    {
      "lang": "en",
      "text": "the soup and you're like, oh well, you",
      "offset": 3248.72,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "know, my grandpa made the soup with",
      "offset": 3250.559,
      "duration": 2.961
    },
    {
      "lang": "en",
      "text": "these ingredients. I don't know why, but",
      "offset": 3252.319,
      "duration": 2.641
    },
    {
      "lang": "en",
      "text": "I just make the soup the way my grandpa",
      "offset": 3253.52,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "said. And then like some one day",
      "offset": 3254.96,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "somebody added, you know, cilantro, and",
      "offset": 3256.4,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "since then we've been adding cilantro",
      "offset": 3258.4,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "for generations, and you're like, this",
      "offset": 3259.76,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "is kind of crazy. That's exactly how we",
      "offset": 3261.44,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "train models, though. Yeah. Yeah. Um, so",
      "offset": 3262.8,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "I think there's there's like a part",
      "offset": 3265.04,
      "duration": 2.799
    },
    {
      "lang": "en",
      "text": "where it's like, okay, like let's try to",
      "offset": 3266.16,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "unpack what's happening, you know, like",
      "offset": 3267.839,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "the mechanisms of learning, like how how",
      "offset": 3269.359,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "our models learn. Like one of them I",
      "offset": 3271.28,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "guess I guess we skipped over it but",
      "offset": 3273.28,
      "duration": 2.64
    },
    {
      "lang": "en",
      "text": "like one of the inter things were like",
      "offset": 3274.4,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "induction heads you know like",
      "offset": 3275.92,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "understanding what induction heads are",
      "offset": 3277.28,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "which are attention heads that allow you",
      "offset": 3278.8,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "to look at in your context the last time",
      "offset": 3280.8,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "that something was mentioned and then",
      "offset": 3283.44,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "repeat it is like something that happens",
      "offset": 3284.88,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "that seems to happen in every model and",
      "offset": 3286.88,
      "duration": 2.4
    },
    {
      "lang": "en",
      "text": "it's like oh okay that makes sense.",
      "offset": 3288.079,
      "duration": 2.641
    },
    {
      "lang": "en",
      "text": "That's how the model like is able to",
      "offset": 3289.28,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "like repeat text without dedicating too",
      "offset": 3290.72,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "much capacity to it. Let's get it on",
      "offset": 3292.88,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "screen so people can see the visuals of",
      "offset": 3294.64,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "the work you guys put out is amazing.",
      "offset": 3296.72,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "highly high. We should talk a little bit",
      "offset": 3299.2,
      "duration": 2.48
    },
    {
      "lang": "en",
      "text": "about the back behind the scenes of that",
      "offset": 3300.559,
      "duration": 2.56
    },
    {
      "lang": "en",
      "text": "that mechan but but let's let's let's",
      "offset": 3301.68,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "finish this off first. Totally. Uh but",
      "offset": 3303.119,
      "duration": 2.561
    },
    {
      "lang": "en",
      "text": "just really quickly I don't think we",
      "offset": 3304.72,
      "duration": 1.92
    },
    {
      "lang": "en",
      "text": "should spend too long on it. I think",
      "offset": 3305.68,
      "duration": 1.84
    },
    {
      "lang": "en",
      "text": "it's just like if you're interested in",
      "offset": 3306.64,
      "duration": 2.32
    },
    {
      "lang": "en",
      "text": "mechan we talked about superp position",
      "offset": 3307.52,
      "duration": 2.799
    },
    {
      "lang": "en",
      "text": "and I think we skipped over induction",
      "offset": 3308.96,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "heads and that's like you know kind of",
      "offset": 3310.319,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "like a really neat basically pattern",
      "offset": 3311.76,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "that emerges in many many transformers",
      "offset": 3313.599,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "where essentially they just learn like",
      "offset": 3316,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "one of the things that you need to do to",
      "offset": 3318.079,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "like predict text well is that if",
      "offset": 3319.2,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "there's repeated text at some point",
      "offset": 3321.119,
      "duration": 3.121
    },
    {
      "lang": "en",
      "text": "somebody said Emanuel Mason and then",
      "offset": 3322.72,
      "duration": 2.639
    },
    {
      "lang": "en",
      "text": "you're like on the next line and they",
      "offset": 3324.24,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "say Emanuel very good chance it's the",
      "offset": 3325.359,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "same last name. And so one of the first",
      "offset": 3327.599,
      "duration": 2.641
    },
    {
      "lang": "en",
      "text": "things that models learn is just like,",
      "offset": 3329.119,
      "duration": 2.161
    },
    {
      "lang": "en",
      "text": "okay, I'm just going to like look at",
      "offset": 3330.24,
      "duration": 2.24
    },
    {
      "lang": "en",
      "text": "what was said before and I'm going to",
      "offset": 3331.28,
      "duration": 2.48
    },
    {
      "lang": "en",
      "text": "say the same thing. And that's induction",
      "offset": 3332.48,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "heads, which is like a pair of attention",
      "offset": 3333.76,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "heads that just basically look at the",
      "offset": 3335.2,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "last time something was said, look at",
      "offset": 3337.119,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "what happened after, move that over. And",
      "offset": 3338.559,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "that's an example of a mechanism where",
      "offset": 3340.72,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "it's like, cool, now we understand that",
      "offset": 3342,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "pretty well. There's been a lot of",
      "offset": 3343.92,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "follow-up research on understanding",
      "offset": 3345.76,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "better like, okay, like in which context",
      "offset": 3347.119,
      "duration": 3.521
    },
    {
      "lang": "en",
      "text": "do they turn on? Like, you know, there's",
      "offset": 3348.64,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "like different like levels of",
      "offset": 3350.64,
      "duration": 2.24
    },
    {
      "lang": "en",
      "text": "abstraction. There's like induction",
      "offset": 3351.68,
      "duration": 2.48
    },
    {
      "lang": "en",
      "text": "heads that like literally copy the word,",
      "offset": 3352.88,
      "duration": 2.479
    },
    {
      "lang": "en",
      "text": "and there's some that copy like the",
      "offset": 3354.16,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "sentiment and other aspects. But I think",
      "offset": 3355.359,
      "duration": 4.041
    },
    {
      "lang": "en",
      "text": "it's just like an example of slowly",
      "offset": 3357.359,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "unpacking, you know, or like peeling",
      "offset": 3359.4,
      "duration": 3.399
    },
    {
      "lang": "en",
      "text": "back the layers of the onion of like",
      "offset": 3361.359,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "what's going on inside this model. Okay,",
      "offset": 3362.799,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "this is a component. It's doing this. So",
      "offset": 3364.079,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "induction headers was like the first",
      "offset": 3366.319,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "major finding. It was a big finding for",
      "offset": 3367.599,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "NLP models for sure. I often think about",
      "offset": 3370.16,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "the edit models. So Claude has a fast",
      "offset": 3372.24,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "edit mode. Uh I forget what it's called.",
      "offset": 3375.28,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "Open has one as well. And you need very",
      "offset": 3377.52,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "good copying every area that needs",
      "offset": 3380.48,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "copying and then you need it to switch",
      "offset": 3382.319,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "out of copy mode when you need to start",
      "offset": 3384,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "generating, right? And that is basically",
      "offset": 3386.16,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "the productionized version of this.",
      "offset": 3388.079,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "Yeah. Yeah. Yeah. And it turns out that",
      "offset": 3389.599,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "you know you you need to select a model",
      "offset": 3391.52,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "that's like smart enough to know when it",
      "offset": 3393.28,
      "duration": 3.039
    },
    {
      "lang": "en",
      "text": "needs to get out of coffee mode, right?",
      "offset": 3395.119,
      "duration": 2.881
    },
    {
      "lang": "en",
      "text": "Which is like it's fascinating. It it's",
      "offset": 3396.319,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "faster, it's cheaper. you know, as",
      "offset": 3398,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "bullish as I am on canvas, basically",
      "offset": 3399.52,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "every AI product needs to iterate on a",
      "offset": 3401.76,
      "duration": 6.559
    },
    {
      "lang": "en",
      "text": "central artifact and like if it's code,",
      "offset": 3405.52,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "if it's a piece of writing, it doesn't",
      "offset": 3408.319,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "really matter, but you need that copy",
      "offset": 3410,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "capability that's smart enough to know",
      "offset": 3412.24,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "when to turn it off. That's why it's",
      "offset": 3414.079,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "cool that induction heads are at",
      "offset": 3415.76,
      "duration": 2.559
    },
    {
      "lang": "en",
      "text": "different levels abstraction. Like",
      "offset": 3417.359,
      "duration": 2.641
    },
    {
      "lang": "en",
      "text": "sometimes you need to editing some code,",
      "offset": 3418.319,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "you need to copy like the general",
      "offset": 3420,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "structure. It's like, oh, like the last",
      "offset": 3422.24,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "like this other function that's similar.",
      "offset": 3423.599,
      "duration": 3.281
    },
    {
      "lang": "en",
      "text": "It first takes like, you know, I don't",
      "offset": 3425.04,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "know, like abstract class and then it",
      "offset": 3426.88,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "takes like an int. So, I need to like",
      "offset": 3428.72,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "copy the general idea, but it's going to",
      "offset": 3430.319,
      "duration": 2.721
    },
    {
      "lang": "en",
      "text": "be a different abstract class and a",
      "offset": 3431.76,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "different int or something. Cool. Um,",
      "offset": 3433.04,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "yeah. So, tracing. Oh, yeah. Should we",
      "offset": 3434.72,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "jump to circuit tracing? Sure. I don't",
      "offset": 3437.04,
      "duration": 3.039
    },
    {
      "lang": "en",
      "text": "know if there's anything else you want",
      "offset": 3438.64,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "to cover. No, we got space for it.",
      "offset": 3440.079,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "Maybe. Okay. I'll do like a really quick",
      "offset": 3441.92,
      "duration": 4.36
    },
    {
      "lang": "en",
      "text": "TLDDR of these two recent papers. Okay.",
      "offset": 3443.76,
      "duration": 6.079
    },
    {
      "lang": "en",
      "text": "Uh, insanely quick. So we talked about",
      "offset": 3446.28,
      "duration": 5.64
    },
    {
      "lang": "en",
      "text": "these features that we detect and what",
      "offset": 3449.839,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "we said is like okay but we'd like to",
      "offset": 3451.92,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "connect the features to understand like",
      "offset": 3453.92,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "the inputs to every features and the",
      "offset": 3455.68,
      "duration": 2.56
    },
    {
      "lang": "en",
      "text": "output to every features and basically",
      "offset": 3456.96,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "draw a graph and this is like if I'm",
      "offset": 3458.24,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "still sharing my screen uh the thing on",
      "offset": 3459.92,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "the right here where like that's the",
      "offset": 3461.76,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "dream we want like for a given prompt",
      "offset": 3463.839,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "what were all of the things like all of",
      "offset": 3466.88,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "the important things happen in the model",
      "offset": 3468.72,
      "duration": 2.639
    },
    {
      "lang": "en",
      "text": "and here it's like okay it took in these",
      "offset": 3470.079,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "four tokens those activated these",
      "offset": 3471.359,
      "duration": 3.521
    },
    {
      "lang": "en",
      "text": "features these features activate these",
      "offset": 3473.119,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "other features and then these features",
      "offset": 3474.88,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "activate the other features and then all",
      "offset": 3476.319,
      "duration": 3.121
    },
    {
      "lang": "en",
      "text": "of these like promoted the output and",
      "offset": 3477.68,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "that's the story and basically we're",
      "offset": 3479.44,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "we're like the work is to sort of use",
      "offset": 3480.72,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "dictionary running and these replacement",
      "offset": 3483.359,
      "duration": 6.081
    },
    {
      "lang": "en",
      "text": "models to provide a explanation of like",
      "offset": 3484.799,
      "duration": 6.961
    },
    {
      "lang": "en",
      "text": "sets of features that explain behavior.",
      "offset": 3489.44,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "So this is super abstract. So I think",
      "offset": 3491.76,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "immediately maybe we can like just look",
      "offset": 3493.839,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "at one example I can show you one which",
      "offset": 3495.599,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "is this one the reasoning one. Yep.",
      "offset": 3497.76,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "Yeah. Two-step reasoning. I think this",
      "offset": 3500.799,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "is already this is like the introduction",
      "offset": 3502.48,
      "duration": 2.56
    },
    {
      "lang": "en",
      "text": "example but it's already like kind of",
      "offset": 3504,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "fun. So, so the question is you ask the",
      "offset": 3505.04,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "model something that requires it to take",
      "offset": 3507.04,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "a step of reasoning in its head. So, you",
      "offset": 3508.88,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "say, you know, fact the capital of the",
      "offset": 3510.96,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "state containing Dallas is. So, to",
      "offset": 3512.88,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "answer that, you need one intermediate",
      "offset": 3514.88,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "step, right? You need to say, wait,",
      "offset": 3516.559,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "where's Dallas? Isn't Texas. Okay, cool.",
      "offset": 3517.92,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "Capital Texas, Austin. And this is like",
      "offset": 3520.24,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "in one token, right? It's going to after",
      "offset": 3522.24,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "is it's going to say Austin. And so like",
      "offset": 3523.68,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "in that one forward pass, the model",
      "offset": 3525.52,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "needs to extract to realize that you're",
      "offset": 3528.24,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "asking it for like the capital of a",
      "offset": 3530.319,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "state to like look up the state for",
      "offset": 3532.799,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "Dallas, which is Texas, and then to say",
      "offset": 3534.64,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "Austin. And sure enough, this is like",
      "offset": 3536.48,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "what we see is we see like in this",
      "offset": 3538.16,
      "duration": 2.959
    },
    {
      "lang": "en",
      "text": "forward pass, there's a rich sort of",
      "offset": 3539.52,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "like inner set of representations where",
      "offset": 3541.119,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "there's like it gets capital state in",
      "offset": 3543.839,
      "duration": 5.361
    },
    {
      "lang": "en",
      "text": "Dallas and then boom, it has an inner uh",
      "offset": 3545.68,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "representation for Texas and then that",
      "offset": 3549.2,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "plus capital leads it to like say",
      "offset": 3551.68,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "Austin. I guess one of the things here",
      "offset": 3553.28,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "is like we can see this internal like",
      "offset": 3555.04,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "thinking step right but a lot of what",
      "offset": 3557.04,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "people say is like is this just",
      "offset": 3559.599,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "memorized fact right like I'm sure a lot",
      "offset": 3561.2,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "of the pre-training that this model is",
      "offset": 3563.44,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "trained on is this sentence shows up",
      "offset": 3564.88,
      "duration": 4.959
    },
    {
      "lang": "en",
      "text": "pretty often right so this shows that no",
      "offset": 3567.119,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "actually internally throughout we do see",
      "offset": 3569.839,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "that there is this middle step right",
      "offset": 3571.839,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "it's not just memorized you can prove",
      "offset": 3573.839,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "that it generalized yeah so so so that's",
      "offset": 3576.48,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "exactly right and I think like you you",
      "offset": 3579.119,
      "duration": 2.561
    },
    {
      "lang": "en",
      "text": "you hit the the nail on the head which",
      "offset": 3580.4,
      "duration": 2.399
    },
    {
      "lang": "en",
      "text": "is like this is what this example is",
      "offset": 3581.68,
      "duration": 2.639
    },
    {
      "lang": "en",
      "text": "about it's like ah If this was just",
      "offset": 3582.799,
      "duration": 2.721
    },
    {
      "lang": "en",
      "text": "memorized, you wouldn't need to have an",
      "offset": 3584.319,
      "duration": 2.48
    },
    {
      "lang": "en",
      "text": "intermediate step at all. You' just be",
      "offset": 3585.52,
      "duration": 2.48
    },
    {
      "lang": "en",
      "text": "like, I've seen the sentence. Like, I",
      "offset": 3586.799,
      "duration": 3.121
    },
    {
      "lang": "en",
      "text": "know what comes next, right? But here,",
      "offset": 3588,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "there is an intermediate step. And so,",
      "offset": 3589.92,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "you could say like, okay, well, maybe it",
      "offset": 3592.16,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "just has the step, but it's memorize it",
      "offset": 3594.16,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "anyways. And then the way to like verify",
      "offset": 3595.599,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "that is kind of like what what we do",
      "offset": 3597.76,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "later in the paper and for all of our",
      "offset": 3599.68,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "examples is like, okay, we claim that",
      "offset": 3601.04,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "this is like the Texas representation.",
      "offset": 3602.96,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "Let's get another one and replace it.",
      "offset": 3604.64,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "And we just change like that uh feature",
      "offset": 3606.4,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "in the middle of the model and we change",
      "offset": 3608.88,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "it to like California. And if you change",
      "offset": 3610.88,
      "duration": 4.36
    },
    {
      "lang": "en",
      "text": "it to California, sure enough, it says",
      "offset": 3612.96,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "Sacramento. And so it's like this is not",
      "offset": 3615.24,
      "duration": 3.72
    },
    {
      "lang": "en",
      "text": "just a like byproduct like it's",
      "offset": 3617.68,
      "duration": 2.639
    },
    {
      "lang": "en",
      "text": "memorized something and on the side it's",
      "offset": 3618.96,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "thinking about Texas. It's like no no no",
      "offset": 3620.319,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "this is like a step in the reasoning. If",
      "offset": 3622.319,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "you change that intermediate step, it",
      "offset": 3624.4,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "changes the answer. Very very cool work.",
      "offset": 3626,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "Underappreciated.",
      "offset": 3628.24,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "Okay. Sure. I have never really doubted.",
      "offset": 3630.48,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "I think there's a lot of people that are",
      "offset": 3634.24,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "always criticizing LMS as stoastic",
      "offset": 3635.76,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "parrots. This pretty much disproves it",
      "offset": 3638.24,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "already. like we can move on. Yeah. I",
      "offset": 3640.319,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "mean I I I think I think there's a lot",
      "offset": 3642.96,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "of examples that I will say we can go",
      "offset": 3644.96,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "through like a few of them like show an",
      "offset": 3647.28,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "amount of depth in the intermediate",
      "offset": 3649.839,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "states of the model that makes you think",
      "offset": 3651.68,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "like oh gosh like it's doing a lot. I",
      "offset": 3653.04,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "think maybe like the poems. Well",
      "offset": 3655.68,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "definitely the poems but even for this",
      "offset": 3657.359,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "one I'm going to like scroll in this",
      "offset": 3658.88,
      "duration": 5.679
    },
    {
      "lang": "en",
      "text": "very short paper to like uh medical",
      "offset": 3661.359,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "diagnosis. I don't even know the word",
      "offset": 3664.559,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "count cuz there's so many like embedded",
      "offset": 3666.559,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "things in there. Yeah, we it's too",
      "offset": 3668.16,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "dangerous. We can't look it up. It",
      "offset": 3670.16,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "overflows. Um it's so beautiful. Look at",
      "offset": 3671.359,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "this. Uh this is like a medical example",
      "offset": 3674.64,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "that I think shows you again this is in",
      "offset": 3676.559,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "one forward pass. The model is like",
      "offset": 3678.48,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "given a bunch of symptoms and then it's",
      "offset": 3680.559,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "asked not like hey what is what is the",
      "offset": 3683.28,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "like disease that this person has. It's",
      "offset": 3685.76,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "asked like if you could run one more",
      "offset": 3687.44,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "test to determine it what would it be?",
      "offset": 3689.28,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "So it's even harder right? means like",
      "offset": 3691.119,
      "duration": 2.641
    },
    {
      "lang": "en",
      "text": "you need to take all the symptoms then",
      "offset": 3692.4,
      "duration": 2.959
    },
    {
      "lang": "en",
      "text": "you need to like have a few hypotheses",
      "offset": 3693.76,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "about what the disease could be and then",
      "offset": 3695.359,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "based on your hypothesis say like well",
      "offset": 3697.119,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "the thing that would like be the right",
      "offset": 3698.559,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "test to do is X and here you can see",
      "offset": 3700.319,
      "duration": 2.961
    },
    {
      "lang": "en",
      "text": "these three layers right where it's like",
      "offset": 3702.079,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "again in one forward pass it has a bunch",
      "offset": 3703.28,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "of like oh these are symptoms then it",
      "offset": 3705.44,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "has the most likely diagnosis here then",
      "offset": 3708,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "like an alternate one and then based on",
      "offset": 3710.24,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "the diagnosis it like gives you",
      "offset": 3711.839,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "basically a bunch of things that you",
      "offset": 3714,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "could ask and again we do the same",
      "offset": 3715.44,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "experiments where you can like kill this",
      "offset": 3717.359,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "feature here like suppress it and And",
      "offset": 3719.44,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "then it asks you a question about the",
      "offset": 3721.52,
      "duration": 2.64
    },
    {
      "lang": "en",
      "text": "second the sort like second option it",
      "offset": 3722.72,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "had. Um the reason I show it is like man",
      "offset": 3724.16,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "that's like a lot of stuff going on like",
      "offset": 3727.04,
      "duration": 3.519
    },
    {
      "lang": "en",
      "text": "for for one forward pass, right? It's",
      "offset": 3728.72,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "like specifically if you if you expected",
      "offset": 3730.559,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "it to like oh what it's going to do is",
      "offset": 3732.24,
      "duration": 3.119
    },
    {
      "lang": "en",
      "text": "it's just like seen similar cases in the",
      "offset": 3733.599,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "training. is going to like kind of like",
      "offset": 3735.359,
      "duration": 2.801
    },
    {
      "lang": "en",
      "text": "vibe and be like, &quot;Oh, I guess like",
      "offset": 3736.319,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "there's that word and it's going to say",
      "offset": 3738.16,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "something that's related to like I don't",
      "offset": 3739.599,
      "duration": 2.401
    },
    {
      "lang": "en",
      "text": "know, headache, you know, like kind of",
      "offset": 3740.88,
      "duration": 2.959
    },
    {
      "lang": "en",
      "text": "like really it's like no, no, no. It's",
      "offset": 3742,
      "duration": 2.799
    },
    {
      "lang": "en",
      "text": "like activating many different",
      "offset": 3743.839,
      "duration": 2.401
    },
    {
      "lang": "en",
      "text": "distributed representations like",
      "offset": 3744.799,
      "duration": 3.121
    },
    {
      "lang": "en",
      "text": "combining them and sort of like doing",
      "offset": 3746.24,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "something pretty complicated.&quot; And so",
      "offset": 3747.92,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "yeah, I think I think it's funny because",
      "offset": 3750.16,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "in my opinion that's like yeah, like oh",
      "offset": 3752.72,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "god, stochastic parrots is not something",
      "offset": 3754.799,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "that I think is is like appropriate",
      "offset": 3757.04,
      "duration": 3.039
    },
    {
      "lang": "en",
      "text": "here. And I think there's just like a",
      "offset": 3758.72,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "lot of different things going on and",
      "offset": 3760.079,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "there's like pretty complex behavior. At",
      "offset": 3762.4,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "the same time, I think it's in the eye",
      "offset": 3764.96,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "of the beholder. I think like I've",
      "offset": 3766.16,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "talked to folks that have like read this",
      "offset": 3768.24,
      "duration": 2.319
    },
    {
      "lang": "en",
      "text": "paper and have been like, &quot;Oh yeah, this",
      "offset": 3769.44,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "is just like a bunch of kind of like",
      "offset": 3770.559,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "huristics that are like mashed together,",
      "offset": 3772.16,
      "duration": 2.959
    },
    {
      "lang": "en",
      "text": "right? Like the model is just doing like",
      "offset": 3774,
      "duration": 2.48
    },
    {
      "lang": "en",
      "text": "a bunch of kind of like, oh, if high",
      "offset": 3775.119,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "blood pressure then this or that.&quot; And",
      "offset": 3776.48,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "so I think there's there's sort of like",
      "offset": 3778.319,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "um an underlying question that's",
      "offset": 3779.76,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "interesting which is like okay now we",
      "offset": 3781.359,
      "duration": 3.281
    },
    {
      "lang": "en",
      "text": "know a little bit of how it works. This",
      "offset": 3783.28,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "is how it works like now you tell me if",
      "offset": 3784.64,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "you think that's like impressive if you",
      "offset": 3786.48,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "think that like if you trust it if you",
      "offset": 3787.839,
      "duration": 3.121
    },
    {
      "lang": "en",
      "text": "think that's sort of like uh something",
      "offset": 3789.44,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "that is that is sufficient to like ask",
      "offset": 3790.96,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "it for medical questions or whatever. I",
      "offset": 3792.799,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "think it's a way to adversarily improve",
      "offset": 3794.4,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "the model quality because once you can",
      "offset": 3796.64,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "do this, you can reverse engineer what",
      "offset": 3799.2,
      "duration": 5.919
    },
    {
      "lang": "en",
      "text": "will be a a sequence of words that to a",
      "offset": 3802,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "human makes no sense or let you arrive",
      "offset": 3805.119,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "at the complete opposite conclusion, but",
      "offset": 3807.68,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "the model still gets tripped up by.",
      "offset": 3809.599,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "Yeah. And then you can just improve it",
      "offset": 3811.119,
      "duration": 3.121
    },
    {
      "lang": "en",
      "text": "from there. Exactly. And and this gives",
      "offset": 3812.559,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "you a hypothesis about like you like",
      "offset": 3814.24,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "specifically imagine if like one of",
      "offset": 3816.16,
      "duration": 2.879
    },
    {
      "lang": "en",
      "text": "those was actually like the wrong",
      "offset": 3817.92,
      "duration": 2.159
    },
    {
      "lang": "en",
      "text": "symptom or something. you'd be like,",
      "offset": 3819.039,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "&quot;Oh, it's weird that the liver uh",
      "offset": 3820.079,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "condition like, you know, upweighs this",
      "offset": 3822.4,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "other example. That doesn't make sense.",
      "offset": 3824.559,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "Okay, let's like fix that in",
      "offset": 3826.88,
      "duration": 2.479
    },
    {
      "lang": "en",
      "text": "particular.&quot; Exactly. You sort of have",
      "offset": 3828,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "like um a bit of of insight into like",
      "offset": 3829.359,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "how the model is getting to its",
      "offset": 3831.76,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "conclusion. And so you can see both like",
      "offset": 3833.119,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "is it making errors, but also is it",
      "offset": 3835.44,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "using the kind of reasoning that will",
      "offset": 3837.119,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "lead it to errors? There's a thesis I",
      "offset": 3838.48,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "mean now it's very prominent with the",
      "offset": 3840.559,
      "duration": 5.921
    },
    {
      "lang": "en",
      "text": "reasoning models about model depth. Uh",
      "offset": 3842.799,
      "duration": 5.121
    },
    {
      "lang": "en",
      "text": "so like you're doing all this in one",
      "offset": 3846.48,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "pass. Yeah. But maybe you don't need to",
      "offset": 3847.92,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "because you can do more passes. Sure. Uh",
      "offset": 3849.92,
      "duration": 6.32
    },
    {
      "lang": "en",
      "text": "and so uh people want shallow models for",
      "offset": 3853.44,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "speed, but you need model depth for for",
      "offset": 3856.24,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "this kind of thinking. Yeah. Is there a",
      "offset": 3859.28,
      "duration": 3.279
    },
    {
      "lang": "en",
      "text": "parto frontier? Is there is there a",
      "offset": 3861.039,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "direct trade-off? Yeah. What would you",
      "offset": 3862.559,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "prefer if you had to make a model and",
      "offset": 3864.96,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "like you know shallow versus deep?",
      "offset": 3866.4,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "There's a chain of thought faithfulness",
      "offset": 3868.559,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "example. Uh before I show it, I'm just",
      "offset": 3870.079,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "going to go back to the top here. So",
      "offset": 3872.319,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "when the model is sampling many tokens,",
      "offset": 3874.16,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "if you want that to be your model, you",
      "offset": 3876.559,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "need to be able to trust every token it",
      "offset": 3878.24,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "samples. So like the problem with with",
      "offset": 3881.76,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "models being autogressive is that like",
      "offset": 3884.72,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "if they like at some point sample a",
      "offset": 3886.319,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "mistake, then they kind of keep going",
      "offset": 3888.16,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "conditioned on that mistake, right? And",
      "offset": 3890.16,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "so sometimes like you need back",
      "offset": 3891.76,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "backspace tokens or whatever. Yeah.",
      "offset": 3893.76,
      "duration": 3.039
    },
    {
      "lang": "en",
      "text": "Yeah. Yeah. And error correction is like",
      "offset": 3895.359,
      "duration": 2.641
    },
    {
      "lang": "en",
      "text": "notably hard, right? if you have like a",
      "offset": 3896.799,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "deeper model, maybe you have like fewer",
      "offset": 3898,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "coot steps, but like your your steps are",
      "offset": 3900.24,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "more likely to be like robust or or",
      "offset": 3902.24,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "correct or something. And so I think",
      "offset": 3904.48,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "that that's one way to look at the",
      "offset": 3905.839,
      "duration": 2.881
    },
    {
      "lang": "en",
      "text": "trade-off. To be clear, I don't have an",
      "offset": 3907.28,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "answer. I don't know if I want a wider",
      "offset": 3908.72,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "or a shallow or a deep model that like",
      "offset": 3910.16,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "you definitely want shallow for",
      "offset": 3912.72,
      "duration": 2.56
    },
    {
      "lang": "en",
      "text": "inference speed. Sure. Sure. Sure. Sure.",
      "offset": 3913.52,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "But you're trading that off for for",
      "offset": 3915.28,
      "duration": 2.48
    },
    {
      "lang": "en",
      "text": "something else, right? Because you also",
      "offset": 3916.48,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "want like a 1B model for infant speed,",
      "offset": 3917.76,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "but that also comes at a cost, right?",
      "offset": 3919.76,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "It's it's less smart. There's a cool",
      "offset": 3921.119,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "quick paper to plug that we just covered",
      "offset": 3923.2,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "on the paper club. It's a survey paper",
      "offset": 3924.799,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "around when to use reasoning models",
      "offset": 3926.559,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "versus dense models. What's the",
      "offset": 3928.48,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "trade-off? I think it's the economy of",
      "offset": 3929.92,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "reasoning economy. Reasoning the",
      "offset": 3932.559,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "reasoning economy. So they just go over",
      "offset": 3933.68,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "a bunch of you know ways to me measure",
      "offset": 3935.359,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "this benchmarks around when to use each",
      "offset": 3937.68,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "because yeah like you know we don't want",
      "offset": 3939.68,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "to also like consumers are now paying",
      "offset": 3941.599,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "the cost of this right but little little",
      "offset": 3943.28,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "side note. Yeah. Yeah. For those on",
      "offset": 3946,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "YouTube we have a secondary channel",
      "offset": 3947.68,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "called Lean Space TV where we cover that",
      "offset": 3948.88,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "stuff. Nice. That's our paper club. We",
      "offset": 3950.48,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "covered your paper. Cool. Yeah. I think",
      "offset": 3952.64,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "you brought up the like planning thing.",
      "offset": 3955.44,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "Maybe it's worth Let's do it. Yeah, I",
      "offset": 3956.64,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "think I think this one is like if you",
      "offset": 3958.72,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "think about Okay, so you're going into",
      "offset": 3960.4,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "the chain of thoughtfulness one. Let's",
      "offset": 3962,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "skip this one. Let's just do planning.",
      "offset": 3963.44,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "So if you think about like you know",
      "offset": 3964.96,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "common questions you have about models.",
      "offset": 3966.96,
      "duration": 2.56
    },
    {
      "lang": "en",
      "text": "The first one we we kind of asked was",
      "offset": 3968.24,
      "duration": 2.879
    },
    {
      "lang": "en",
      "text": "like okay like is it just doing this",
      "offset": 3969.52,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "like vibe based oneshot pattern matching",
      "offset": 3971.119,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "based on existing data or does it have",
      "offset": 3973.359,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "like kind of rich in representations? It",
      "offset": 3975.359,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "seems to have like these like",
      "offset": 3978.16,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "intermediate representations that make",
      "offset": 3979.52,
      "duration": 3.519
    },
    {
      "lang": "en",
      "text": "sense as the abstractions that you would",
      "offset": 3981.599,
      "duration": 2.561
    },
    {
      "lang": "en",
      "text": "reason through. Okay, so that's one",
      "offset": 3983.039,
      "duration": 2.641
    },
    {
      "lang": "en",
      "text": "thing and there's a bunch of examples.",
      "offset": 3984.16,
      "duration": 2.879
    },
    {
      "lang": "en",
      "text": "We talked about the medical diagnosis.",
      "offset": 3985.68,
      "duration": 2.56
    },
    {
      "lang": "en",
      "text": "There's like the multilingual circuits",
      "offset": 3987.039,
      "duration": 2.32
    },
    {
      "lang": "en",
      "text": "is another one that I think is cool",
      "offset": 3988.24,
      "duration": 2.24
    },
    {
      "lang": "en",
      "text": "where it's like, oh, it's sharing",
      "offset": 3989.359,
      "duration": 2.801
    },
    {
      "lang": "en",
      "text": "representations across languages.",
      "offset": 3990.48,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "Another thing that you'll hear people",
      "offset": 3992.16,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "mention about language models, which is",
      "offset": 3993.68,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "that they're like uh next token",
      "offset": 3995.28,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "predictors. Also, for for a quick note",
      "offset": 3996.72,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "for people that won't dive into this",
      "offset": 3998.64,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "super long blog post, I know you",
      "offset": 4000.48,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "highlighted like 10 to 12. So for like a",
      "offset": 4002.319,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "quick 15 30 second, what do you mean by",
      "offset": 4004.64,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "they're sharing thoughts throughout?",
      "offset": 4007.039,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "just like what's the really quick high",
      "offset": 4009.039,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "level just for people to Yeah, the",
      "offset": 4010.4,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "really quick high level is that what we",
      "offset": 4012.24,
      "duration": 5.839
    },
    {
      "lang": "en",
      "text": "find is that here I'm going to like show",
      "offset": 4015.44,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "you a really quick inside the model if",
      "offset": 4018.079,
      "duration": 4.28
    },
    {
      "lang": "en",
      "text": "you look at like the inner",
      "offset": 4020.72,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "representations for concepts you can ask",
      "offset": 4022.359,
      "duration": 4.121
    },
    {
      "lang": "en",
      "text": "like the same question which I think in",
      "offset": 4024.64,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "the paper the original one we asked is",
      "offset": 4026.48,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "like the opposite of ha is you know cold",
      "offset": 4028,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "but you can you can do this over a",
      "offset": 4030.319,
      "duration": 2.961
    },
    {
      "lang": "en",
      "text": "larger data set and ask the same",
      "offset": 4031.76,
      "duration": 3.279
    },
    {
      "lang": "en",
      "text": "question in many different languages and",
      "offset": 4033.28,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "then look at these representations in",
      "offset": 4035.039,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "the middle of the model and ask yourself",
      "offset": 4036.88,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "like Well, when you ask it the opposite",
      "offset": 4038.24,
      "duration": 3.319
    },
    {
      "lang": "en",
      "text": "of hot is",
      "offset": 4040,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "and which is the same sentence in",
      "offset": 4041.559,
      "duration": 4.921
    },
    {
      "lang": "en",
      "text": "French, show off. Is it is it using the",
      "offset": 4043.52,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "same features or is it learning",
      "offset": 4046.48,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "independently for each language? It kind",
      "offset": 4049.44,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "of would be bad news if it learned",
      "offset": 4051.359,
      "duration": 2.24
    },
    {
      "lang": "en",
      "text": "independently for each language because",
      "offset": 4052.48,
      "duration": 2.16
    },
    {
      "lang": "en",
      "text": "then that means that like as you're",
      "offset": 4053.599,
      "duration": 2.401
    },
    {
      "lang": "en",
      "text": "pre-training or fine-tuning, you have to",
      "offset": 4054.64,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "relearn everything from scratch. So, you",
      "offset": 4056,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "would expect a better model to kind of",
      "offset": 4058.24,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "like share some concepts between the",
      "offset": 4060.16,
      "duration": 2.959
    },
    {
      "lang": "en",
      "text": "languages it's learning, right? And you",
      "offset": 4061.599,
      "duration": 2.881
    },
    {
      "lang": "en",
      "text": "here we do it for like language",
      "offset": 4063.119,
      "duration": 2.561
    },
    {
      "lang": "en",
      "text": "languages, but I think you could argue",
      "offset": 4064.48,
      "duration": 2.4
    },
    {
      "lang": "en",
      "text": "that you'd expect the same thing for",
      "offset": 4065.68,
      "duration": 2.399
    },
    {
      "lang": "en",
      "text": "like programming languages where it's",
      "offset": 4066.88,
      "duration": 2.719
    },
    {
      "lang": "en",
      "text": "like, oh, if you learn what an if",
      "offset": 4068.079,
      "duration": 2.881
    },
    {
      "lang": "en",
      "text": "statement is in Python, maybe it'd be",
      "offset": 4069.599,
      "duration": 2.561
    },
    {
      "lang": "en",
      "text": "nice if you could generalize that to",
      "offset": 4070.96,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "Java or whatever. And here we find that",
      "offset": 4072.16,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "basically you see exactly that. Here we",
      "offset": 4074.16,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "show like if you look inside the model,",
      "offset": 4075.839,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "if you look at the middle of the model,",
      "offset": 4078.48,
      "duration": 2.879
    },
    {
      "lang": "en",
      "text": "which is the middle of this plot here,",
      "offset": 4079.68,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "models share more features. They share",
      "offset": 4081.359,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "more of these representations in the",
      "offset": 4083.359,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "middle of the model. And bigger models",
      "offset": 4084.72,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "share even more. And so the like the",
      "offset": 4087.839,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "sort of like smarter models use more",
      "offset": 4089.52,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "shared representations than the dumber",
      "offset": 4091.599,
      "duration": 3.281
    },
    {
      "lang": "en",
      "text": "models which might explain part of the",
      "offset": 4093.119,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "reason why they're smarter. And so this",
      "offset": 4094.88,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "this was like sort of this this other",
      "offset": 4096.799,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "finding of like oh not only is it like",
      "offset": 4098.319,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "having these rich representations in the",
      "offset": 4100.319,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "middle it like learns to not have",
      "offset": 4102.319,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "redundant representations like if you've",
      "offset": 4105.279,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "learned the concept of heat you don't",
      "offset": 4106.719,
      "duration": 2.881
    },
    {
      "lang": "en",
      "text": "need to learn the concept of like French",
      "offset": 4108.239,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "heat and Japanese heat and column like",
      "offset": 4109.6,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "you just that's just the concept of heat",
      "offset": 4111.6,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "and you can share that among different",
      "offset": 4113.199,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "languages. I feel like sometimes",
      "offset": 4114.88,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "overanalyzing this becomes a bit of a",
      "offset": 4116.56,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "problem right like when we talked about",
      "offset": 4118.88,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "with the medical example uh we could",
      "offset": 4120.159,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "look back and try to fix this in data",
      "offset": 4122.319,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "set so in language I don't remember if",
      "offset": 4124.08,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "it was open or anthropic where they",
      "offset": 4126.319,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "basically said when the model switch",
      "offset": 4127.839,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "languages and they pass it to fluent",
      "offset": 4130,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "users they said oh this this feels like",
      "offset": 4131.679,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "an American that's speaking this",
      "offset": 4134.159,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "language right so at sometimes there are",
      "offset": 4135.839,
      "duration": 5.121
    },
    {
      "lang": "en",
      "text": "nuances in a slightly different",
      "offset": 4138.799,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "representation right so you don't want",
      "offset": 4140.96,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "to overengineer these little fixes when",
      "offset": 4143.04,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "you to see them. But then the other side",
      "offset": 4144.88,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "of this is like for those tail end of",
      "offset": 4147.44,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "languages, right? For languages that",
      "offset": 4149.44,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "models aren't good at and for those like",
      "offset": 4151.44,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "you know when you want to kind of solve",
      "offset": 4154,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "that last bit, it seems like you know",
      "offset": 4155.52,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "it's pretty plausible that we can solve",
      "offset": 4157.679,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "this because these concepts can be",
      "offset": 4159.92,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "shared across languages as long as we",
      "offset": 4162.08,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "can, you know, fill in some some level",
      "offset": 4164.319,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "of representation unless I'm wrong. No,",
      "offset": 4166.88,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "totally. And and I think like this sort",
      "offset": 4168.96,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "of stuff also explains, you know, uh,",
      "offset": 4171.04,
      "duration": 2.639
    },
    {
      "lang": "en",
      "text": "language models are really good at in",
      "offset": 4172.56,
      "duration": 2.239
    },
    {
      "lang": "en",
      "text": "context learning. Like you give them",
      "offset": 4173.679,
      "duration": 2.48
    },
    {
      "lang": "en",
      "text": "something completely new, they do a good",
      "offset": 4174.799,
      "duration": 3.121
    },
    {
      "lang": "en",
      "text": "job. It's like, well, if you give them",
      "offset": 4176.159,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "like a new fake language, uh, and you",
      "offset": 4177.92,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "like in that language explain that like",
      "offset": 4180.159,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "cold means this and hot means that, you",
      "offset": 4182.239,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "know, like presumably they're able to,",
      "offset": 4184.08,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "as we clear this speculation, we don't",
      "offset": 4186.159,
      "duration": 2.241
    },
    {
      "lang": "en",
      "text": "show it in the paper, but they're able",
      "offset": 4187.44,
      "duration": 2.879
    },
    {
      "lang": "en",
      "text": "to bind it. Google's done this. Okay,",
      "offset": 4188.4,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "great. Yeah. They took a low resource",
      "offset": 4190.319,
      "duration": 2.561
    },
    {
      "lang": "en",
      "text": "language, dumped it in a million token",
      "offset": 4191.6,
      "duration": 2.4
    },
    {
      "lang": "en",
      "text": "context, and then it came up. That's",
      "offset": 4192.88,
      "duration": 2.16
    },
    {
      "lang": "en",
      "text": "right. That's right. Well, I guess the",
      "offset": 4194,
      "duration": 2.4
    },
    {
      "lang": "en",
      "text": "thing that the thing I'd be curious to",
      "offset": 4195.04,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "see is like, okay, does it use does it",
      "offset": 4196.4,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "reuse these representations? I bet that",
      "offset": 4198,
      "duration": 2.719
    },
    {
      "lang": "en",
      "text": "it probably does, right? And that's",
      "offset": 4199.6,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "probably like a reason why it works well",
      "offset": 4200.719,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "is like well, it can reuse the",
      "offset": 4202.56,
      "duration": 2.639
    },
    {
      "lang": "en",
      "text": "representation the general",
      "offset": 4204.159,
      "duration": 2.321
    },
    {
      "lang": "en",
      "text": "representations that it's learned in",
      "offset": 4205.199,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "other languages. Yeah, this is like I",
      "offset": 4206.48,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "don't have you talked to any linguistics",
      "offset": 4208.56,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "people not recently. Linguistics",
      "offset": 4210.56,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "researchers who will be very interested",
      "offset": 4213.76,
      "duration": 2.959
    },
    {
      "lang": "en",
      "text": "in this because ultimately this is the",
      "offset": 4214.88,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "ultimate test of superior warf um which",
      "offset": 4216.719,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "are familiar with hypothesis. So for",
      "offset": 4218.64,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "those who don't know it's basically the",
      "offset": 4220.32,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "idea that the language that you speak",
      "offset": 4221.84,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "influences the way you think which",
      "offset": 4223.12,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "obviously directly maps onto here. If",
      "offset": 4224.88,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "every if it's a complete mapping if",
      "offset": 4227.28,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "every language maps every concept",
      "offset": 4229.36,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "perfectly on in like the theoretical",
      "offset": 4231.28,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "infinitely sized model then superior",
      "offset": 4233.76,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "warf is false because there is a",
      "offset": 4236.64,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "universal truth. If it does not if there",
      "offset": 4239.04,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "is some overlap where for example",
      "offset": 4241.76,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "there's some languages that have no word",
      "offset": 4243.36,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "there's this joke where like uh you know",
      "offset": 4244.96,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "Eskimos have no word for snow or",
      "offset": 4246.719,
      "duration": 2.561
    },
    {
      "lang": "en",
      "text": "something like that right or water has",
      "offset": 4248.08,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "no word fish have no word for water",
      "offset": 4249.28,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "there's an African language where",
      "offset": 4251.44,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "there's a gender for vegetables you know",
      "offset": 4252.48,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "stuff like that just like languages",
      "offset": 4254.8,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "influence the way you think and so there",
      "offset": 4256.719,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "should not be a 100% overlap at some",
      "offset": 4258.56,
      "duration": 3.119
    },
    {
      "lang": "en",
      "text": "point of course it's like at the limit",
      "offset": 4260.239,
      "duration": 2.801
    },
    {
      "lang": "en",
      "text": "of the infinite model so who knows if",
      "offset": 4261.679,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "we'll ever but but yeah well and I think",
      "offset": 4263.04,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "it's it's interesting we also show a",
      "offset": 4265.44,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "little below that like some people have",
      "offset": 4266.96,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "made the point of like the bias. Oh, it",
      "offset": 4269.199,
      "duration": 3.281
    },
    {
      "lang": "en",
      "text": "sounds like an American speaking a",
      "offset": 4271.36,
      "duration": 2.56
    },
    {
      "lang": "en",
      "text": "different language. And it does seem",
      "offset": 4272.48,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "like the sort of like inner",
      "offset": 4273.92,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "representations have a higher connection",
      "offset": 4275.92,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "to like the output logets for English",
      "offset": 4278.32,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "loits. And so there's like some bias uh",
      "offset": 4280.32,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "towards English uh at least in the model",
      "offset": 4282.719,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "we studied here. Any thoughts as to",
      "offset": 4284.64,
      "duration": 3.519
    },
    {
      "lang": "en",
      "text": "whether multimodality influences um any",
      "offset": 4286.159,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "of this? So like concepts do they map",
      "offset": 4288.159,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "across languages as they do across",
      "offset": 4289.76,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "modalities? Yeah, so we show this in uh",
      "offset": 4291.6,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "the Golden Gate or like the previous",
      "offset": 4293.6,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "paper. I might have it here actually for",
      "offset": 4295.04,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "There's a good diagram of this in the",
      "offset": 4297.28,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "SAS where the same concept in text and",
      "offset": 4298.64,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "in image. This is our our buddy the",
      "offset": 4301.04,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "Golden Gate Bridge. Here we're showing",
      "offset": 4303.12,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "like the feature for the Golden Gate",
      "offset": 4304.719,
      "duration": 2.721
    },
    {
      "lang": "en",
      "text": "Bridge and in orange is like what it",
      "offset": 4305.92,
      "duration": 2.799
    },
    {
      "lang": "en",
      "text": "activates over. And so you're like okay",
      "offset": 4307.44,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "so this is when the model is like",
      "offset": 4308.719,
      "duration": 2.801
    },
    {
      "lang": "en",
      "text": "reading text about the Golden Gate",
      "offset": 4310.4,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "Bridge and we also show other languages.",
      "offset": 4311.52,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "This is uh you'll have to take my word",
      "offset": 4314,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "for it but also about the Gonggate",
      "offset": 4315.6,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "Bridge. And then we we show like the",
      "offset": 4316.96,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "photos for which it activates the most.",
      "offset": 4319.28,
      "duration": 2.32
    },
    {
      "lang": "en",
      "text": "And sure enough it's the Golden Gate",
      "offset": 4320.48,
      "duration": 2.64
    },
    {
      "lang": "en",
      "text": "Bridge. And so again like that shows an",
      "offset": 4321.6,
      "duration": 3.119
    },
    {
      "lang": "en",
      "text": "example of a representation that's",
      "offset": 4323.12,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "shared across languages and shared",
      "offset": 4324.719,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "across modalities. Yeah. Yeah. I think",
      "offset": 4326.4,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "this very relevant for like the auto",
      "offset": 4328.4,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "reggressive image generation models uh",
      "offset": 4329.52,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "and then now the audio models as well.",
      "offset": 4331.44,
      "duration": 2.719
    },
    {
      "lang": "en",
      "text": "Something I'm trying to get some",
      "offset": 4333.28,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "intuition for which you probably don't",
      "offset": 4334.159,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "have a off the bat answer is how much",
      "offset": 4336.4,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "does it cost to add a modality right? So",
      "offset": 4338.8,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "a lot of people are saying like oh just",
      "offset": 4341.12,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "add some different decoder and then",
      "offset": 4342.719,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "align the latent spaces and you're good.",
      "offset": 4345.44,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "And I'm like, I don't know, man. It",
      "offset": 4346.96,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "sounds like there's a lot of information",
      "offset": 4349.199,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "lost between those. Yeah, I definitely",
      "offset": 4351.199,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "do not have a good intuition for this.",
      "offset": 4353.12,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "Although, I will say that things like",
      "offset": 4355.12,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "this, right, make you think that if you",
      "offset": 4357.04,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "train on multiple modalities, then",
      "offset": 4359.44,
      "duration": 3.279
    },
    {
      "lang": "en",
      "text": "you'll definitely get this like",
      "offset": 4361.6,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "alignment truth, right? Yeah. But but if",
      "offset": 4362.719,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "if you like train on one and then post",
      "offset": 4366,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "hog train on another, maybe maybe it'll",
      "offset": 4367.6,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "be harder or like train some adapter",
      "offset": 4369.199,
      "duration": 3.121
    },
    {
      "lang": "en",
      "text": "layer. Okay. So, official answer is",
      "offset": 4370.96,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "don't know, but official answer is",
      "offset": 4372.32,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "someone could figure it out. rug. Yeah,",
      "offset": 4374.08,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "I think there are people who know and",
      "offset": 4376,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "they just haven't shared. Well, you need",
      "offset": 4377.44,
      "duration": 3.64
    },
    {
      "lang": "en",
      "text": "to find them and get them on this",
      "offset": 4379.84,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "podcast. Did we want to do the like",
      "offset": 4381.08,
      "duration": 3.48
    },
    {
      "lang": "en",
      "text": "planning example? Correct. Yeah. Now,",
      "offset": 4383.12,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "we're backtracking up this up the stack.",
      "offset": 4384.56,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "All right. Yeah. Example, I think again",
      "offset": 4386.4,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "is like I like this example because of",
      "offset": 4388.56,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "the next token predictor concept. So, I",
      "offset": 4390.8,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "think this is actually like really",
      "offset": 4393.04,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "important to kind of like dive into. So,",
      "offset": 4393.92,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "maybe what I'll say is like language",
      "offset": 4396.64,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "models are next token predictors is like",
      "offset": 4398.239,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "a fact. Like that is what they do.",
      "offset": 4400.56,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "That's the objective. they they are",
      "offset": 4402.719,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "trained to predict the next token.",
      "offset": 4404.239,
      "duration": 5.841
    },
    {
      "lang": "en",
      "text": "However, that does not mean that they",
      "offset": 4406.159,
      "duration": 6.481
    },
    {
      "lang": "en",
      "text": "myopically only consider the next token",
      "offset": 4410.08,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "when they choose the next token. You can",
      "offset": 4412.64,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "work on break the next token, but still",
      "offset": 4414.48,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "like doing so in a way that helps helps",
      "offset": 4416.48,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "you predict the token like 10 tokens in",
      "offset": 4418.719,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "the future. And I think well now we",
      "offset": 4420.96,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "definitely know that they're not my",
      "offset": 4424.159,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "predicting the next token. And I think",
      "offset": 4425.76,
      "duration": 2.64
    },
    {
      "lang": "en",
      "text": "at least for me that was a pretty big",
      "offset": 4427.199,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "update because you could totally imagine",
      "offset": 4428.4,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "that they could do everything they're",
      "offset": 4430.96,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "doing by just like being really good at",
      "offset": 4432,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "predicting the next token but sort of",
      "offset": 4434,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "like not having an internal state like",
      "offset": 4435.12,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "it's it's it wasn't a given that they",
      "offset": 4436.96,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "were going to like represent internally",
      "offset": 4438.48,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "oh this is where I want to go and so I'm",
      "offset": 4440.32,
      "duration": 2.32
    },
    {
      "lang": "en",
      "text": "going to predict the next token and so",
      "offset": 4441.52,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "this example shows like an example like",
      "offset": 4442.64,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "the model. Do you have it on screen by",
      "offset": 4444.64,
      "duration": 4.519
    },
    {
      "lang": "en",
      "text": "the way? Let me actually",
      "offset": 4446.239,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "yeah while you pull it up some of the",
      "offset": 4449.159,
      "duration": 4.121
    },
    {
      "lang": "en",
      "text": "early connections that I made to this",
      "offset": 4451.679,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "were like early early Transformers. So",
      "offset": 4453.28,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "think BERT encoder decoder transformers",
      "offset": 4455.52,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "right when they came out some of the",
      "offset": 4457.92,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "suggestions were you don't take the last",
      "offset": 4460,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "layer right you take off the last layer",
      "offset": 4462.64,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "so if you want to do a classification",
      "offset": 4464.239,
      "duration": 3.521
    },
    {
      "lang": "en",
      "text": "task a translation task for these",
      "offset": 4465.76,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "encoder decoder transformers they've",
      "offset": 4467.76,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "kind of overfitit on their training",
      "offset": 4469.92,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "objective right so they're really good",
      "offset": 4471.84,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "at mass language modeling at filling in",
      "offset": 4474.32,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "you know sentence order stuff like that",
      "offset": 4477.04,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "so what we want to do is we want to",
      "offset": 4478.8,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "throw away the top layer we want to",
      "offset": 4480.48,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "freeze the bottom layers and then there",
      "offset": 4482,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "was a lot of work that was done, you",
      "offset": 4484,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "know, where should we mess with these",
      "offset": 4485.36,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "models? Should we look at like, you",
      "offset": 4486.96,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "know, the top three layers? Should we",
      "offset": 4488.56,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "look at the top two? Where should we",
      "offset": 4489.92,
      "duration": 3.279
    },
    {
      "lang": "en",
      "text": "probe in? Because we can see different",
      "offset": 4491.28,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "effects, right? So, we know at the very",
      "offset": 4493.199,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "end, they've overfit on their task. But",
      "offset": 4495.52,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "there's a level at which, you know, when",
      "offset": 4498.159,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "we start to change and we start to",
      "offset": 4500,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "continue training or fine-tuning, we get",
      "offset": 4501.679,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "better output. So totally we could start",
      "offset": 4504,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "to see that you know throughout layers",
      "offset": 4506.08,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "there's there's still a broader like",
      "offset": 4507.6,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "understanding the language and then we",
      "offset": 4510.08,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "can add in a layer whether that's",
      "offset": 4511.76,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "classification and then fine-tune and",
      "offset": 4513.52,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "you know it learns our task and this",
      "offset": 4515.28,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "planning example is sort of like a more",
      "offset": 4517.04,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "robust way to look into that. Yeah.",
      "offset": 4518.64,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "Yeah. And I think if you look at like",
      "offset": 4520.48,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "all of the examples in the paper, you",
      "offset": 4521.84,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "kind of uh at the bottom we have this",
      "offset": 4523.44,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "list of like consistent patterns and one",
      "offset": 4525.199,
      "duration": 3.121
    },
    {
      "lang": "en",
      "text": "pattern you see is kind of exactly what",
      "offset": 4527.12,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "you're talking about. Like at the top",
      "offset": 4528.32,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "the sort of like here actually I have",
      "offset": 4530.8,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "one here. The sort of like top features",
      "offset": 4532.4,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "are like right before the output are",
      "offset": 4534.32,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "often just about like what you're going",
      "offset": 4536.159,
      "duration": 2.801
    },
    {
      "lang": "en",
      "text": "to say. It's next token predictions.",
      "offset": 4537.36,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "It's like oh I'm going to say Austin.",
      "offset": 4538.96,
      "duration": 2.64
    },
    {
      "lang": "en",
      "text": "I'm going to say rabbit. I'm going to",
      "offset": 4540.4,
      "duration": 2.4
    },
    {
      "lang": "en",
      "text": "say so it's kind of like not very",
      "offset": 4541.6,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "abstract. It's just like a motor. It's a",
      "offset": 4542.8,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "motor neuron for a human, right? It's",
      "offset": 4544.88,
      "duration": 2.64
    },
    {
      "lang": "en",
      "text": "like, oh, I've decided that I want a",
      "offset": 4546.08,
      "duration": 2.4
    },
    {
      "lang": "en",
      "text": "drink of water and so I'm going to just",
      "offset": 4547.52,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "grab the bottle. And at at the bottom,",
      "offset": 4548.48,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "they're all like the like b like sensory",
      "offset": 4550.8,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "neurons. They're just like, oh, I just",
      "offset": 4552.64,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "saw the word X or I just saw this. And",
      "offset": 4554.08,
      "duration": 5.119
    },
    {
      "lang": "en",
      "text": "so if you want to like yeah, like",
      "offset": 4557.12,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "extract the interesting representations,",
      "offset": 4559.199,
      "duration": 2.801
    },
    {
      "lang": "en",
      "text": "a lot of the time they're in the middle.",
      "offset": 4560.64,
      "duration": 2.24
    },
    {
      "lang": "en",
      "text": "That's where the like shared",
      "offset": 4562,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "representations across language are. And",
      "offset": 4562.88,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "that's where here this like plan is to",
      "offset": 4564.88,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "like walk through the example really",
      "offset": 4568.48,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "briefly. It's like you have a poem and",
      "offset": 4569.6,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "in order to say you have the first line",
      "offset": 4571.84,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "of a poem and in order to say the second",
      "offset": 4573.12,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "line of the poem, well, if you want to",
      "offset": 4574.96,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "rhyme, you need to like identify what",
      "offset": 4577.52,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "the rhyme of the first line was. You're",
      "offset": 4580,
      "duration": 2.4
    },
    {
      "lang": "en",
      "text": "just at the end of the first line. So",
      "offset": 4581.36,
      "duration": 2.16
    },
    {
      "lang": "en",
      "text": "you say like, okay, what's my current",
      "offset": 4582.4,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "rhyme and then you need to like think",
      "offset": 4583.52,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "about what your poem is talking about",
      "offset": 4585.36,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "and then think about candidate words",
      "offset": 4587.44,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "that rhyme and that are like on topic",
      "offset": 4588.8,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "for your poem. And so here, this is",
      "offset": 4590.48,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "what's happening, right? It's like the",
      "offset": 4592.239,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "last word is it. And so there's a bunch",
      "offset": 4593.6,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "of features that are actually they",
      "offset": 4595.44,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "represent the direction like rhyming",
      "offset": 4597.44,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "with eat or at. And by the way, we like",
      "offset": 4599.44,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "looked at a bunch of poems internally",
      "offset": 4601.92,
      "duration": 2.799
    },
    {
      "lang": "en",
      "text": "and you have like I thought it was like",
      "offset": 4603.199,
      "duration": 2.641
    },
    {
      "lang": "en",
      "text": "really beautiful. You have these models.",
      "offset": 4604.719,
      "duration": 2.641
    },
    {
      "lang": "en",
      "text": "They have a bunch of features for like",
      "offset": 4605.84,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "oh this word has like AB in it. Oh, this",
      "offset": 4607.36,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "word has like many consonants. Oh, this",
      "offset": 4609.76,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "word like is like you know kind of kind",
      "offset": 4611.52,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "of like has some flourish to it. They",
      "offset": 4613.44,
      "duration": 3.279
    },
    {
      "lang": "en",
      "text": "have like a bunch of of like features",
      "offset": 4615.44,
      "duration": 2.799
    },
    {
      "lang": "en",
      "text": "that track various aspects that you",
      "offset": 4616.719,
      "duration": 2.641
    },
    {
      "lang": "en",
      "text": "would want to use if you're writing",
      "offset": 4618.239,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "poetry. It's just like confets and like",
      "offset": 4619.36,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "all the feature detection. Yeah,",
      "offset": 4621.199,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "totally. Uh, but I think I maybe I",
      "offset": 4623.44,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "didn't expect there to be as many",
      "offset": 4625.36,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "features about just like sounds of words",
      "offset": 4626.48,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "and kind of musicality, which I thought",
      "offset": 4628.56,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "was kind of kind of neat. But then once",
      "offset": 4630.239,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "it's extracted the rhyme, then it comes",
      "offset": 4632,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "up with sort of like these two",
      "offset": 4633.92,
      "duration": 2.799
    },
    {
      "lang": "en",
      "text": "candidates. In this case, it's like ah,",
      "offset": 4635.199,
      "duration": 2.801
    },
    {
      "lang": "en",
      "text": "either I'm going to finish with rabbit",
      "offset": 4636.719,
      "duration": 3.121
    },
    {
      "lang": "en",
      "text": "or I'm going to finish with habit. The",
      "offset": 4638,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "cool thing here is here we show that",
      "offset": 4639.84,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "like this happens at the new line. So it",
      "offset": 4641.52,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "happens before it's even started the",
      "offset": 4644.719,
      "duration": 5.281
    },
    {
      "lang": "en",
      "text": "second line. And it turns out that like",
      "offset": 4647.52,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "you can then say, oh, is this the plan",
      "offset": 4650,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "actually using? We do our usual",
      "offset": 4652.48,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "experiments. We like remove it and the",
      "offset": 4653.679,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "model writes a completely different",
      "offset": 4656.239,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "line. We inject something and it writes",
      "offset": 4657.52,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "a completely different line. We have",
      "offset": 4659.44,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "these like fun examples here. I'll show",
      "offset": 4660.96,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "which is just as a mechanical thing you",
      "offset": 4662.96,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "could you just you just disallow",
      "offset": 4665.04,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "generation of a certain logic is is that",
      "offset": 4667.44,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "for for how we do these interventions.",
      "offset": 4669.76,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "Yeah. Basically what these features are",
      "offset": 4671.199,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "is they're like directions in the model.",
      "offset": 4673.12,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "Okay. So to like remove them we just",
      "offset": 4674.96,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "write in the opposite direction. So we",
      "offset": 4677.12,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "run the model normally and then like at",
      "offset": 4678.48,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "the like layer where it was going to",
      "offset": 4680.48,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "write let's say in like you know this",
      "offset": 4682,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "this direction we just like yeah we",
      "offset": 4683.679,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "either like add a negative that like",
      "offset": 4686.56,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "compensates for it or add a negative",
      "offset": 4688.159,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "that goes even more in the negative",
      "offset": 4690.239,
      "duration": 2.641
    },
    {
      "lang": "en",
      "text": "direction sometimes to like really kill",
      "offset": 4691.52,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "it and then we can also add another",
      "offset": 4692.88,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "direction right so in these random",
      "offset": 4694.56,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "examples here where where like you have",
      "offset": 4696.8,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "this poem the silver moon cast a gentle",
      "offset": 4698.56,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "light and then claude 3.5 haiku would",
      "offset": 4700.8,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "like rhyme with illuminating the",
      "offset": 4703.52,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "peaceful night but then if we like go",
      "offset": 4704.88,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "negative in the night direction and just",
      "offset": 4707.84,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "add like green. The whole second line is",
      "offset": 4709.92,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "going to write is just upon the meadows",
      "offset": 4712.64,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "verdant green. And so that's all that",
      "offset": 4714.32,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "we're doing. We're saying like we found",
      "offset": 4716.239,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "where it stores its plan and we like",
      "offset": 4717.44,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "delete or like suppress the one it's",
      "offset": 4719.92,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "stored and go in the direction of",
      "offset": 4721.44,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "something else that's arbitrary. And the",
      "offset": 4722.8,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "result that's like striking here is sort",
      "offset": 4724.96,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "of like two things. I think like one",
      "offset": 4727.12,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "this plan is made well in advance of",
      "offset": 4729.6,
      "duration": 5.119
    },
    {
      "lang": "en",
      "text": "needing to predict knight. it's made",
      "offset": 4732.32,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "like after the first line before it's",
      "offset": 4734.719,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "even started the second line. And two,",
      "offset": 4736.64,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "this plan doesn't just control like what",
      "offset": 4739.44,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "you're going to rhyme with. It's also",
      "offset": 4741.199,
      "duration": 2.721
    },
    {
      "lang": "en",
      "text": "doing what's called like backwards",
      "offset": 4742.8,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "planning where it's like, well, because",
      "offset": 4743.92,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "I need to finish with green, I'm not",
      "offset": 4745.92,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "going to say illuminating the peaceful",
      "offset": 4747.679,
      "duration": 3.121
    },
    {
      "lang": "en",
      "text": "night cuz then I'd be like illuminating",
      "offset": 4749.12,
      "duration": 3.039
    },
    {
      "lang": "en",
      "text": "the peaceful green. That doesn't make",
      "offset": 4750.8,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "sense. I need to say a completely",
      "offset": 4752.159,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "different sentence that lets me finish",
      "offset": 4753.6,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "with green. And so there's a circuit in",
      "offset": 4755.76,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "the model that decides on the rhyme and",
      "offset": 4757.52,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "then works backwards from the rhyme",
      "offset": 4759.199,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "influences to set up your sentence.",
      "offset": 4761.199,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "Yeah, it's almost like back prop but in",
      "offset": 4763.36,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "the future. Yeah, it's like doing like",
      "offset": 4766.719,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "is it like a because the green is is",
      "offset": 4768.96,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "back propping through these words. So",
      "offset": 4770.48,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "verdant and meadow are both green",
      "offset": 4772.08,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "related. Yeah, but it's doing all of",
      "offset": 4773.84,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "that in its forward passes. Yep. Right.",
      "offset": 4775.36,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "In context which is kind of crazy. I",
      "offset": 4777.52,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "thought intuitively makes sense. Right.",
      "offset": 4779.6,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "So looking at it from a model",
      "offset": 4781.36,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "architecture perspective where basically",
      "offset": 4782.48,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "you just have a bunch of attention and",
      "offset": 4784.8,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "feed forward layers and then at the end",
      "offset": 4786.32,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "you have you know what's the softmax",
      "offset": 4788.64,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "over the next token. you would expect",
      "offset": 4790.239,
      "duration": 3.281
    },
    {
      "lang": "en",
      "text": "that end would really be like that",
      "offset": 4791.92,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "grabber, right? It's just picking",
      "offset": 4793.52,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "tokens. So that's what it's going to do.",
      "offset": 4794.88,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "And early on, like even with traditional",
      "offset": 4796.64,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "models, we could see different concepts",
      "offset": 4798.8,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "that would start to pop up through early",
      "offset": 4800.719,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "layers. And yeah, you have some of this",
      "offset": 4802.64,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "throughout your architecture. So it's",
      "offset": 4804.96,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "very cool to see. The kind of other",
      "offset": 4806.719,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "question that comes up is like how are",
      "offset": 4808.32,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "we labeling these features? How are we",
      "offset": 4810.4,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "defining them? Are we doing that right?",
      "offset": 4812.8,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "And like you know what is a these words",
      "offset": 4815.44,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "end with like it feature? How do we kind",
      "offset": 4818.56,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "of come to that conclusion? Like how do",
      "offset": 4821.12,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "we map a name to this, right? Like Yeah.",
      "offset": 4823.12,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "So I I think there's this is like an",
      "offset": 4826.08,
      "duration": 2.639
    },
    {
      "lang": "en",
      "text": "important question because you can",
      "offset": 4827.84,
      "duration": 2.56
    },
    {
      "lang": "en",
      "text": "totally imagine like fooling yourself,",
      "offset": 4828.719,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "right? Yeah. Is there like a guy at",
      "offset": 4830.4,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "Anthropic that just maps 30,000 features",
      "offset": 4832.48,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "and and another thing? You're the guy.",
      "offset": 4835.12,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "He's the guy. I did notice also like",
      "offset": 4837.44,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "with the previous work, the scaling up",
      "offset": 4839.44,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "SAES, as you train bigger and bigger",
      "offset": 4841.6,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "ones, a lot of features don't activate.",
      "offset": 4844.48,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "So I think like 60% of the 34 million",
      "offset": 4846.8,
      "duration": 5.439
    },
    {
      "lang": "en",
      "text": "one didn't. So I think there's like a",
      "offset": 4850.08,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "few questions behind your question. Like",
      "offset": 4852.239,
      "duration": 2
    },
    {
      "lang": "en",
      "text": "the first question was like how do you",
      "offset": 4853.28,
      "duration": 2.64
    },
    {
      "lang": "en",
      "text": "even label the features? You were",
      "offset": 4854.239,
      "duration": 3.121
    },
    {
      "lang": "en",
      "text": "telling me this is a rabbit feature.",
      "offset": 4855.92,
      "duration": 3.279
    },
    {
      "lang": "en",
      "text": "Like why should I trust you? And I think",
      "offset": 4857.36,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "there's kind of like two things going",
      "offset": 4859.199,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "on. So one, as I mentioned at the start,",
      "offset": 4861.6,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "all of this is unsupervised. And so in",
      "offset": 4864.08,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "the paper we have these links to like",
      "offset": 4866.32,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "these little graphs which show you like",
      "offset": 4868.56,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "more of what's going on. But this graph",
      "offset": 4870.719,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "is just like completely unsupervised. So",
      "offset": 4872.88,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "it's like we train this like model to",
      "offset": 4873.92,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "like untangle the representation, right?",
      "offset": 4875.76,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "This like dictionary that we talked",
      "offset": 4877.92,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "about that gives us the features and",
      "offset": 4879.36,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "then we like just do math to figure out",
      "offset": 4881.36,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "like which features influence which",
      "offset": 4883.92,
      "duration": 2.799
    },
    {
      "lang": "en",
      "text": "other features and throw away the ones",
      "offset": 4885.28,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "that don't matter and then at the end we",
      "offset": 4886.719,
      "duration": 2.721
    },
    {
      "lang": "en",
      "text": "have these features. So right now we",
      "offset": 4888,
      "duration": 2.56
    },
    {
      "lang": "en",
      "text": "don't have any interpretation for them.",
      "offset": 4889.44,
      "duration": 2.239
    },
    {
      "lang": "en",
      "text": "We just say like these are all the",
      "offset": 4890.56,
      "duration": 2.639
    },
    {
      "lang": "en",
      "text": "features that matter and then we",
      "offset": 4891.679,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "manually go through and we look at the",
      "offset": 4893.199,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "features. You know we look at this",
      "offset": 4895.199,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "feature and we look at that feature and",
      "offset": 4896.239,
      "duration": 5.681
    },
    {
      "lang": "en",
      "text": "let's pick one. So this one we've",
      "offset": 4899.28,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "labeled say habit. So, how do we do",
      "offset": 4901.92,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "that? You could just look at it and we",
      "offset": 4903.36,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "show you like what it activates over.",
      "offset": 4904.96,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "And if you just look at this text, maybe",
      "offset": 4906.8,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "I'll like zoom in. Like you'll",
      "offset": 4908.32,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "immediately notice something, I think.",
      "offset": 4910.159,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "Well, I'll immediately notice something",
      "offset": 4912,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "cuz I've stared at 30,000. I'll point it",
      "offset": 4913.199,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "out for you. The orange is where the",
      "offset": 4915.28,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "feature activates. The next word after",
      "offset": 4917.12,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "the orange is always habit. Habit.",
      "offset": 4919.84,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "Habit. Habit. Habit. Habit. Habit. So,",
      "offset": 4921.92,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "this feature always activates before",
      "offset": 4923.76,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "habit. That's like the main source of an",
      "offset": 4926.159,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "interpretation. We have other things",
      "offset": 4929.12,
      "duration": 3.039
    },
    {
      "lang": "en",
      "text": "like above. We also show you like what",
      "offset": 4930.4,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "loj it promotes. So like what output it",
      "offset": 4932.159,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "promotes and here it promotes hab. So",
      "offset": 4934.4,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "that makes sense. Um and so that's like",
      "offset": 4936.48,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "how we interpret and how we say okay",
      "offset": 4939.12,
      "duration": 2.64
    },
    {
      "lang": "en",
      "text": "like I think this is the say habit",
      "offset": 4940.48,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "feature. But maybe you know for this one",
      "offset": 4941.76,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "is pretty clear but some of them might",
      "offset": 4943.84,
      "duration": 2.399
    },
    {
      "lang": "en",
      "text": "be more confusing. It might not be clear",
      "offset": 4945.04,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "from these like activations what it is.",
      "offset": 4946.239,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "The other way that we build confidence",
      "offset": 4948,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "is like once we've built this thing and",
      "offset": 4949.6,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "we said oh I think this is rhymes with e",
      "offset": 4951.199,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "this is hey say habit. That's where we",
      "offset": 4952.88,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "do our interventions, right? And it's",
      "offset": 4955.199,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "like I claim this is the like I've",
      "offset": 4957.44,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "planned to end with rabbit to verify",
      "offset": 4960.639,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "whether I'm I'm right or not. I'm going",
      "offset": 4963.28,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "to just like take that direction, nuke",
      "offset": 4964.88,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "it from the model and see if the model",
      "offset": 4967.76,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "stops saying rabbit. And sure enough, if",
      "offset": 4969.84,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "you do that and here it's like we stop",
      "offset": 4972.639,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "saying rabbit, it says habit instead.",
      "offset": 4974.719,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "And here it's like we stop it from",
      "offset": 4976.96,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "saying rabbit and habit, it says crabbit",
      "offset": 4978.08,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "in this case. Not a great rhyme, but",
      "offset": 4980.08,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "we'll work with it. Is this something",
      "offset": 4981.92,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "you can do like programmatically? Like",
      "offset": 4983.84,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "can we scale this up? Can we kind of do",
      "offset": 4985.6,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "this autonomously or how much like",
      "offset": 4987.52,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "manual intervention is this? There's",
      "offset": 4989.76,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "been a lot of work in sort of like",
      "offset": 4992.08,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "automated feature interpretability and",
      "offset": 4993.44,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "it's something that we've invested in",
      "offset": 4996.159,
      "duration": 2.881
    },
    {
      "lang": "en",
      "text": "and that like other labs have invested",
      "offset": 4997.44,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "in. And I think basically the answer is",
      "offset": 4999.04,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "we can definitely automate it and we're",
      "offset": 5001.04,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "definitely going to need to. And right",
      "offset": 5003.12,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "now the the most manual parts are this",
      "offset": 5004.8,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "sort of like look at a feature and",
      "offset": 5007.04,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "figure out what it is as well as uh",
      "offset": 5008.4,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "group similar features together. One",
      "offset": 5011.199,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "thing I hinted at is that actually like",
      "offset": 5013.199,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "all of these little blocks here there",
      "offset": 5014.8,
      "duration": 2.64
    },
    {
      "lang": "en",
      "text": "are multiple features. You can see here",
      "offset": 5016.239,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "it's like five features doing the same",
      "offset": 5017.44,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "thing. None of that is too hard for",
      "offset": 5019.199,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "cloud. Very cool. Very cool graphics and",
      "offset": 5021.28,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "blog post you guys put out. We we'll",
      "offset": 5023.92,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "have to ask about the behind the scenes",
      "offset": 5025.679,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "on this one. Yeah. Yeah. But let's let's",
      "offset": 5026.88,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "round out the other things to know. Um",
      "offset": 5028.639,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "what is this term uh attribution graph?",
      "offset": 5031.44,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "It comes up a lot in the recent papers.",
      "offset": 5033.36,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "What does it mean? Yeah, just for people",
      "offset": 5035.92,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "listening, what does So the attribution",
      "offset": 5037.52,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "graph is basically this graph. And why",
      "offset": 5039.76,
      "duration": 6.32
    },
    {
      "lang": "en",
      "text": "is it called an attribution graph? It's",
      "offset": 5043.6,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "Yeah, this is this is the you know this",
      "offset": 5046.08,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "how the sausage is made. Basically, it's",
      "offset": 5048.48,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "at the top here you have the the output.",
      "offset": 5050.4,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "At the bottom you have the input and",
      "offset": 5052.96,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "then we make one little node per feature",
      "offset": 5054.88,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "at a context index and we draw a line",
      "offset": 5057.44,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "which you can see here grayed out",
      "offset": 5059.36,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "between each feature attributing back to",
      "offset": 5060.88,
      "duration": 5.839
    },
    {
      "lang": "en",
      "text": "all of its input features. So here we",
      "offset": 5064.88,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "have all of the input features and so",
      "offset": 5066.719,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "the attribution is the way that we",
      "offset": 5068.48,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "compute the influence of a feature on",
      "offset": 5070.719,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "onto another. The way you do this is you",
      "offset": 5072.48,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "take this feature and you basically like",
      "offset": 5074.08,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "back prop all the way and you like see",
      "offset": 5075.6,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "backing like you dot product it with the",
      "offset": 5078.8,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "activation of the source features and if",
      "offset": 5081.28,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "that's a high value that means that like",
      "offset": 5082.8,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "your source feature influence your",
      "offset": 5084.56,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "target feature by by a lot and and we do",
      "offset": 5086.48,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "a bunch of things that we're not going",
      "offset": 5089.199,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "to go into uh now but to make all of",
      "offset": 5090.56,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "these sort of like sensible and linear",
      "offset": 5093.199,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "such that like at the end you just have",
      "offset": 5095.52,
      "duration": 3.119
    },
    {
      "lang": "en",
      "text": "a graph and the edges are just literally",
      "offset": 5097.04,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "you can interpret them as like cool like",
      "offset": 5098.639,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "this feature that's say a word that",
      "offset": 5100.639,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "contains AB sound its strongest edge",
      "offset": 5102.08,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "which is 2 which is you know twice as",
      "offset": 5104.8,
      "duration": 5.439
    },
    {
      "lang": "en",
      "text": "strong as this one to say AB and to say",
      "offset": 5106.96,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "something with a B in it. That's the",
      "offset": 5110.239,
      "duration": 3.281
    },
    {
      "lang": "en",
      "text": "attribution graph is like now we have",
      "offset": 5111.92,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "this full graph of like all of these",
      "offset": 5113.52,
      "duration": 2.56
    },
    {
      "lang": "en",
      "text": "intermediate concepts and how they",
      "offset": 5114.8,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "influence each other to ultimately",
      "offset": 5116.08,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "culminate to what the model eventually",
      "offset": 5118.48,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "said at the top and we share all of",
      "offset": 5119.92,
      "duration": 2.799
    },
    {
      "lang": "en",
      "text": "these so you can look at them in the",
      "offset": 5121.6,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "paper. Graphs are very useful. This is",
      "offset": 5122.719,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "my first time seeing this graph. a lot",
      "offset": 5124.48,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "of alpha uh if I count correctly there's",
      "offset": 5126.239,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "20 layers but that's in the circuit",
      "offset": 5128.639,
      "duration": 6.161
    },
    {
      "lang": "en",
      "text": "model right oh so but the circuit model",
      "offset": 5132.719,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "is one to one with number of layers in",
      "offset": 5134.8,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "haiku we only show features that like",
      "offset": 5136.4,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "areated yeah so we we show like a subset",
      "offset": 5139.28,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "of features for each of these graphs",
      "offset": 5142,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "basically but we can confirm more than",
      "offset": 5143.44,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "20 layers alpha and uh no but like the",
      "offset": 5145.12,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "the two blog posts that came out with",
      "offset": 5148.48,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "this actually have a lot of background",
      "offset": 5150.08,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "on how attribution graphs are made how",
      "offset": 5151.76,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "you calculate the nodes and stuff very",
      "offset": 5154.08,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "interesting background. So yeah, I will",
      "offset": 5155.84,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "say like if you were curious about hey",
      "offset": 5157.36,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "what do we learn about like models and I",
      "offset": 5160,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "think you know we talked about this like",
      "offset": 5162.32,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "complex internal state planning like",
      "offset": 5163.6,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "another another motif that we can get to",
      "offset": 5166.08,
      "duration": 2.559
    },
    {
      "lang": "en",
      "text": "if you have time is that like there's",
      "offset": 5167.52,
      "duration": 2.4
    },
    {
      "lang": "en",
      "text": "always a bunch of stuff happening in",
      "offset": 5168.639,
      "duration": 3.121
    },
    {
      "lang": "en",
      "text": "parallel. So I think one example of this",
      "offset": 5169.92,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "is like math where the model is like",
      "offset": 5171.76,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "independently computing the like uh last",
      "offset": 5173.679,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "digit and then the like order of",
      "offset": 5176.32,
      "duration": 2.879
    },
    {
      "lang": "en",
      "text": "magnitude and then kind like combining",
      "offset": 5177.84,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "them at the end or like hallucinations",
      "offset": 5179.199,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "are also that where like there's one",
      "offset": 5181.44,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "side of the model that's just deciding",
      "offset": 5183.44,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "whether it should answer or not and the",
      "offset": 5184.8,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "other that's like answering and so",
      "offset": 5186.32,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "sometimes if like the model's like yeah",
      "offset": 5187.6,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "I totally know who this person is even",
      "offset": 5189.52,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "though it it doesn't then like it",
      "offset": 5190.88,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "decides to answer but then the second",
      "offset": 5192.56,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "side hallucinates because it doesn't",
      "offset": 5193.92,
      "duration": 3.279
    },
    {
      "lang": "en",
      "text": "have information. If you were interested",
      "offset": 5195.44,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "in that stuff, that's the paper. If",
      "offset": 5197.199,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "you're like, listen, I don't know that I",
      "offset": 5199.12,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "buy that when you call it a feature, it",
      "offset": 5201.36,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "is a feature or whatever. The circuit",
      "offset": 5202.88,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "tracing paper has truly we've tried to",
      "offset": 5204.719,
      "duration": 5.761
    },
    {
      "lang": "en",
      "text": "put all of the details of like how you",
      "offset": 5207.679,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "compute these graphs, all of the sort of",
      "offset": 5210.48,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "like challenges with it, things that can",
      "offset": 5212.08,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "go wrong, things that work, things that",
      "offset": 5214.239,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "don't. And so this one is the sort of",
      "offset": 5215.92,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "like, you know, we think about it as",
      "offset": 5217.679,
      "duration": 2.56
    },
    {
      "lang": "en",
      "text": "like if you're if you like want to go",
      "offset": 5219.12,
      "duration": 2.32
    },
    {
      "lang": "en",
      "text": "really deep into this stuff and how it",
      "offset": 5220.239,
      "duration": 2.801
    },
    {
      "lang": "en",
      "text": "works, read that one. If you want to",
      "offset": 5221.44,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "like learn about interesting model",
      "offset": 5223.04,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "behavior, read this one. uh following",
      "offset": 5224.56,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "what we're giving advice to people to",
      "offset": 5227.36,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "follow up on. What are like open",
      "offset": 5228.96,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "questions in mechan what are like things",
      "offset": 5230.96,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "people themselves can work on like",
      "offset": 5233.679,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "what's the cost of training essays for",
      "offset": 5235.52,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "people interested in mechan not at a big",
      "offset": 5238.239,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "lab how can they contribute you know",
      "offset": 5240.4,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "yeah I think there's a lot of ways to to",
      "offset": 5242.639,
      "duration": 5.121
    },
    {
      "lang": "en",
      "text": "contribute so there's that have been",
      "offset": 5244.96,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "trained you know on on open models",
      "offset": 5247.76,
      "duration": 2.879
    },
    {
      "lang": "en",
      "text": "there's some of the gemma models there's",
      "offset": 5249.44,
      "duration": 2.719
    },
    {
      "lang": "en",
      "text": "some of the llama models they work",
      "offset": 5250.639,
      "duration": 2.961
    },
    {
      "lang": "en",
      "text": "pretty well there's even so in this",
      "offset": 5252.159,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "paper we use transcoders which they",
      "offset": 5253.6,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "replace like your MLP layers some of",
      "offset": 5255.36,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "those also are available for the same",
      "offset": 5257.36,
      "duration": 2.319
    },
    {
      "lang": "en",
      "text": "models",
      "offset": 5258.88,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "So you have access to to those. There's",
      "offset": 5259.679,
      "duration": 5.761
    },
    {
      "lang": "en",
      "text": "like just both a lot of I would say like",
      "offset": 5262.719,
      "duration": 5.601
    },
    {
      "lang": "en",
      "text": "again biology work and a lot of methods",
      "offset": 5265.44,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "work depending on what you're",
      "offset": 5268.32,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "interested. So on the biology side I",
      "offset": 5269.28,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "would say with at least this like",
      "offset": 5271.84,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "attribution graph method there's just so",
      "offset": 5273.44,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "much you can investigate like pick a",
      "offset": 5275.04,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "model pick a prompt where like it does",
      "offset": 5276.4,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "well or it does poorly and just like",
      "offset": 5278.88,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "look at what happens inside it. So I",
      "offset": 5281.52,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "think like you can use this method that",
      "offset": 5283.28,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "that we used or you can just like fire",
      "offset": 5285.12,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "up the transcoders on your own and just",
      "offset": 5287.28,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "like look at what features are active.",
      "offset": 5288.719,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "There's a lot to just understand model",
      "offset": 5290.48,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "behavior I think with current tooling.",
      "offset": 5292.159,
      "duration": 3.281
    },
    {
      "lang": "en",
      "text": "If that speaks to you and you're like no",
      "offset": 5293.92,
      "duration": 2.799
    },
    {
      "lang": "en",
      "text": "I just want to understand what makes the",
      "offset": 5295.44,
      "duration": 2.56
    },
    {
      "lang": "en",
      "text": "model models tick. I don't necessarily",
      "offset": 5296.719,
      "duration": 3.121
    },
    {
      "lang": "en",
      "text": "want to spend time like training my own",
      "offset": 5298,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "essays. There's a lot to do there for",
      "offset": 5299.84,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "the methods. There's still so much more",
      "offset": 5302.48,
      "duration": 5.679
    },
    {
      "lang": "en",
      "text": "to do. So like I think that right now we",
      "offset": 5304.32,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "have some pretty good solutions for like",
      "offset": 5308.159,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "understanding what's in the original",
      "offset": 5310,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "stream, understanding what's is in MLPS.",
      "offset": 5311.199,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "We don't have good solutions for like",
      "offset": 5313.76,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "attention. So like working on",
      "offset": 5315.36,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "understanding attention better, how to",
      "offset": 5317.44,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "decompose it is like a very active area",
      "offset": 5319.199,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "like we're very interested in it, other",
      "offset": 5321.199,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "people are very interested in it. I",
      "offset": 5323.199,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "think understanding some of the other",
      "offset": 5324.88,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "things that we have in our uh limitation",
      "offset": 5327.04,
      "duration": 5.159
    },
    {
      "lang": "en",
      "text": "section which is pretty long.",
      "offset": 5329.28,
      "duration": 5.919
    },
    {
      "lang": "en",
      "text": "Um, but like reconstruction error is",
      "offset": 5332.199,
      "duration": 4.201
    },
    {
      "lang": "en",
      "text": "like a big thing. Like those those",
      "offset": 5335.199,
      "duration": 2.881
    },
    {
      "lang": "en",
      "text": "dictionaries aren't perfect. It's",
      "offset": 5336.4,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "possible that as we make these like",
      "offset": 5338.08,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "essays big like bigger and better, we",
      "offset": 5339.84,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "never get to perfect. And so if we never",
      "offset": 5341.92,
      "duration": 2.56
    },
    {
      "lang": "en",
      "text": "get to perfect, then you get to the",
      "offset": 5343.44,
      "duration": 2.08
    },
    {
      "lang": "en",
      "text": "questions we were talking about at the",
      "offset": 5344.48,
      "duration": 2.48
    },
    {
      "lang": "en",
      "text": "start. Like do you need a different kind",
      "offset": 5345.52,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "of model? Like what is the approach in",
      "offset": 5346.96,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "order to be able to explain more of",
      "offset": 5349.04,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "what's happening? And then maybe the the",
      "offset": 5350.8,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "other thing I'll say is sort of like",
      "offset": 5353.04,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "this is a really exciting approach to",
      "offset": 5354.48,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "explain what is the model doing on this",
      "offset": 5358.239,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "prompt. But if you go back to the",
      "offset": 5360.56,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "original question, you might want to",
      "offset": 5363.04,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "understand like what is the model doing",
      "offset": 5364.56,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "in general? Like if you go back to my my",
      "offset": 5366.4,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "car analogy, you know, I get like this",
      "offset": 5368.32,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "is the equivalent of me telling you like",
      "offset": 5370.56,
      "duration": 2.56
    },
    {
      "lang": "en",
      "text": "well when like you know you were going",
      "offset": 5371.679,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "uphill and you like didn't shift gears",
      "offset": 5373.12,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "properly that one time, you stalled",
      "offset": 5375.199,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "because of this. But you might be even",
      "offset": 5376.8,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "more interested in like how does like an",
      "offset": 5379.44,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "an uh com combustion engine work at all.",
      "offset": 5381.28,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "And so there's work to sort of like go",
      "offset": 5384.639,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "beyond these like per uh prompt examples",
      "offset": 5386.48,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "to sort of like globally what's the",
      "offset": 5389.04,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "structure of the model that's closer to",
      "offset": 5391.04,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "what was on the distill blog for like",
      "offset": 5393.52,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "vision models where they actually look",
      "offset": 5396,
      "duration": 2.56
    },
    {
      "lang": "en",
      "text": "at like the structure of inception.",
      "offset": 5397.199,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "They're like ah this whole side there's",
      "offset": 5398.56,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "like these like specialized branches",
      "offset": 5400.239,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "that do different things. Um and so like",
      "offset": 5401.76,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "a broader understanding of the model is",
      "offset": 5404.08,
      "duration": 3.119
    },
    {
      "lang": "en",
      "text": "also something that's like I think both",
      "offset": 5405.76,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "very active and also on open source",
      "offset": 5407.199,
      "duration": 2.801
    },
    {
      "lang": "en",
      "text": "models like you can you know like the",
      "offset": 5408.48,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "small models you could just like load on",
      "offset": 5410,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "a consumer laptop and so you can look at",
      "offset": 5412,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "that that's also open and in terms of",
      "offset": 5414.159,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "like one last thing I'll say is like",
      "offset": 5416.32,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "there's a lot of programs that like if",
      "offset": 5417.84,
      "duration": 2.879
    },
    {
      "lang": "en",
      "text": "people are interested they should look",
      "offset": 5419.679,
      "duration": 2.801
    },
    {
      "lang": "en",
      "text": "at anthropic has like the alignment",
      "offset": 5420.719,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "fellows program which like we're running",
      "offset": 5422.48,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "currently we had applications for it",
      "offset": 5425.12,
      "duration": 2.64
    },
    {
      "lang": "en",
      "text": "before we might run it in the future",
      "offset": 5426.4,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "like definitely keep keep an eye on it",
      "offset": 5427.76,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "and then there's the like math program",
      "offset": 5429.6,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "is really great as well for for by",
      "offset": 5431.04,
      "duration": 3.119
    },
    {
      "lang": "en",
      "text": "people that are interested in that kind",
      "offset": 5433.12,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "of research. That was a grand tour",
      "offset": 5434.159,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "through uh all the recent work. You",
      "offset": 5436.88,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "know, what do you wish people asked you",
      "offset": 5438.88,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "more about? I'm sure we covered a lot of",
      "offset": 5440.32,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "like the greatest hits. I think that",
      "offset": 5442,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "this covers most of it. If you if you",
      "offset": 5444,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "like Do you think we have time to sneak",
      "offset": 5446,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "in one more thing that I think is kind",
      "offset": 5447.679,
      "duration": 2.881
    },
    {
      "lang": "en",
      "text": "of cool? Okay, I'll sneak in one more",
      "offset": 5448.96,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "thing, which is it's kind of like",
      "offset": 5450.56,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "planning, but it's about chain of",
      "offset": 5452.08,
      "duration": 2.559
    },
    {
      "lang": "en",
      "text": "thought and trusting model. Is this",
      "offset": 5453.36,
      "duration": 2.56
    },
    {
      "lang": "en",
      "text": "chain of thought faithfulness thing",
      "offset": 5454.639,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "here? This one was like pretty striking",
      "offset": 5455.92,
      "duration": 6.4
    },
    {
      "lang": "en",
      "text": "to me. So we said that the model in one",
      "offset": 5458.32,
      "duration": 5.839
    },
    {
      "lang": "en",
      "text": "pass can do a lot of stuff. It can",
      "offset": 5462.32,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "represent a lot of stuff. That's great.",
      "offset": 5464.159,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "That also means it can bamboozle you",
      "offset": 5466.08,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "really easily. And this is an example of",
      "offset": 5467.84,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "the model bamboozling you. Here we give",
      "offset": 5469.679,
      "duration": 6.161
    },
    {
      "lang": "en",
      "text": "it a math question that it can't answer",
      "offset": 5472.56,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "because it cannot compute cosine of 2 3",
      "offset": 5475.84,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "4 23. That's just like not a thing it",
      "offset": 5478.56,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "can do. By default, if you ask it for",
      "offset": 5480.719,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "that, it'll say like kind of like a r",
      "offset": 5483.28,
      "duration": 2.959
    },
    {
      "lang": "en",
      "text": "it'll have like a random distribution",
      "offset": 5484.639,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "over like minus one1. But here we tell",
      "offset": 5486.239,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "it this hint. We're like, &quot;Hey, can you",
      "offset": 5489.28,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "compute five times cosine of, you know,",
      "offset": 5491.12,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "this big number? I worked it out by hand",
      "offset": 5493.52,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "and I got four. Can you tell me, you",
      "offset": 5496.48,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "know, like can you do the math?&quot; And",
      "offset": 5499.04,
      "duration": 2.56
    },
    {
      "lang": "en",
      "text": "what it's going to do is it's going to",
      "offset": 5500.639,
      "duration": 2.08
    },
    {
      "lang": "en",
      "text": "do this chain of thought, right? So like",
      "offset": 5501.6,
      "duration": 2.16
    },
    {
      "lang": "en",
      "text": "think of it as like this could be like a",
      "offset": 5502.719,
      "duration": 2.081
    },
    {
      "lang": "en",
      "text": "reasoning model doing it chain of",
      "offset": 5503.76,
      "duration": 2.16
    },
    {
      "lang": "en",
      "text": "thought. It's doing this math and then",
      "offset": 5504.8,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "when it gets to this coign right here,",
      "offset": 5505.92,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "what it's going to do is to say it's",
      "offset": 5508.719,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "going to say 0.8. And if you look at why",
      "offset": 5510.56,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "it says 0.8, it says 0.8 eight because",
      "offset": 5512.8,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "it looked at the hint you gave it. It",
      "offset": 5515.76,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "realized that it's going to have to",
      "offset": 5518,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "multiply the result of this thing is",
      "offset": 5519.84,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "computing computing by five. So it it",
      "offset": 5522.159,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "divides the answer you got by five. So",
      "offset": 5524.88,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "it's like four divided by 5 and so",
      "offset": 5526.56,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "that's8. And so basically it works back",
      "offset": 5528.4,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "from the answer you gave it to like say",
      "offset": 5530.639,
      "duration": 5.761
    },
    {
      "lang": "en",
      "text": "that the output of cosine of x is8 so",
      "offset": 5533.84,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "that it lands on on the answer you gave",
      "offset": 5536.4,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "it at the end on the hint you gave it.",
      "offset": 5537.92,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "And so notably notice also that it's",
      "offset": 5539.6,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "like not telling you that it's doing",
      "offset": 5541.6,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "this, but it's basically using this sort",
      "offset": 5542.56,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "of like motivated reasoning going back",
      "offset": 5544.32,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "from the hint pretending that that's the",
      "offset": 5547.44,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "calculation it did and giving you this",
      "offset": 5549.6,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "help. I think one thing that's striking",
      "offset": 5551.199,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "here again is that this is like the like",
      "offset": 5552.96,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "complexity of this model like like the",
      "offset": 5554.8,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "fact that they represent complex states",
      "offset": 5557.199,
      "duration": 3.121
    },
    {
      "lang": "en",
      "text": "internally and that it's not just this",
      "offset": 5558.639,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "sort of like very dumb thing means that",
      "offset": 5560.32,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "they can like do very complex like",
      "offset": 5562.159,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "deceptive reasoning. Meaning like, you",
      "offset": 5564.639,
      "duration": 2.721
    },
    {
      "lang": "en",
      "text": "know, when you're asking the model,",
      "offset": 5566.48,
      "duration": 2
    },
    {
      "lang": "en",
      "text": "you're kind of expecting it to do the",
      "offset": 5567.36,
      "duration": 2.799
    },
    {
      "lang": "en",
      "text": "math here or to tell you that it can't",
      "offset": 5568.48,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "do the math. But because it can do so",
      "offset": 5570.159,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "much in a forward pass, it can work",
      "offset": 5571.84,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "backwards from your hint to lie and like",
      "offset": 5574,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "figure out that it should say this so",
      "offset": 5576.56,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "that it gets to the right answer without",
      "offset": 5578,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "you realizing it. I'm curious if you've",
      "offset": 5579.92,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "done any of this on like different",
      "offset": 5582.56,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "models like have you looked at base",
      "offset": 5584,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "models like post-trained RL models",
      "offset": 5585.44,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "because RL models kind of, you know, you",
      "offset": 5587.52,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "incentivize them to give you outputs",
      "offset": 5589.28,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "that you like, right? So if I tell it",
      "offset": 5591.28,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "something is true, it's kind of been",
      "offset": 5593.12,
      "duration": 3.119
    },
    {
      "lang": "en",
      "text": "trained to, you know, follow what I've",
      "offset": 5594.639,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "given it. So in this case, we yeah, we",
      "offset": 5596.239,
      "duration": 6.081
    },
    {
      "lang": "en",
      "text": "we gave it a hint and now, you know,",
      "offset": 5599.28,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "it's been RL slapped into thinking like,",
      "offset": 5602.32,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "yeah, that that's true. But like, you",
      "offset": 5604,
      "duration": 2.639
    },
    {
      "lang": "en",
      "text": "know, does this stay consistent",
      "offset": 5605.52,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "throughout other So okay, so not yet,",
      "offset": 5606.639,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "but I'm really interested in that",
      "offset": 5609.52,
      "duration": 2.159
    },
    {
      "lang": "en",
      "text": "question because I actually have a",
      "offset": 5610.56,
      "duration": 3.119
    },
    {
      "lang": "en",
      "text": "different intuition from yours. I had a",
      "offset": 5611.679,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "a chat with some other researcher about",
      "offset": 5613.679,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "this uh about the poem example, but I",
      "offset": 5615.679,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "think it applies here as well. I bet I",
      "offset": 5617.52,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "don't know how much I bet. I bet 100",
      "offset": 5620,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "bucks. So somebody can like they will",
      "offset": 5621.36,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "get a 100 bucks from me if they prove",
      "offset": 5623.12,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "that I'm wrong. That this behavior for a",
      "offset": 5624.32,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "model that does it during finetuning, it",
      "offset": 5627.199,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "also does it post pre-training. And",
      "offset": 5628.56,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "here's why. Think about like you're",
      "offset": 5630.56,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "pre-training on like some corpus of like",
      "offset": 5632.239,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "math tensors. Yeah. But also you're",
      "offset": 5634.08,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "pre-training and you're just trying to",
      "offset": 5637.12,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "guess the next token, right? And so for",
      "offset": 5638.8,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "sure if you ever have a hint in the",
      "offset": 5641.12,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "prompt, you're going to definitely use",
      "offset": 5643.12,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "it. Like you're not going to learn to",
      "offset": 5644.4,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "compute cosine of blah or even something",
      "offset": 5645.84,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "you could compute. you're going to learn",
      "offset": 5647.44,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "to go look in your context and see if",
      "offset": 5648.56,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "like you can easily work back the",
      "offset": 5650.56,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "answer. And I think it's the same for",
      "offset": 5652.4,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "planning in poems. I think that also is",
      "offset": 5654.239,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "like a pre-train like probably exists in",
      "offset": 5656,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "pre-training and isn't like only RL",
      "offset": 5657.44,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "because again it's useful when you're",
      "offset": 5659.679,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "like predicting poems. You have poems in",
      "offset": 5662.32,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "your training set to be like well",
      "offset": 5663.92,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "because this poem is going to probably",
      "offset": 5665.6,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "rhyme with rabbit. It's probably going",
      "offset": 5667.04,
      "duration": 3.119
    },
    {
      "lang": "en",
      "text": "to start with something that sets up a",
      "offset": 5668.48,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "sentence about a rabbit as opposed to",
      "offset": 5670.159,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "like a completely different word. And so",
      "offset": 5671.76,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "I actually think this is not RL",
      "offset": 5673.52,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "behavior. I think that's just like the",
      "offset": 5676.88,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "mall's doing it. But I I do agree there.",
      "offset": 5678.159,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "It's just your data set. But also like",
      "offset": 5681.28,
      "duration": 4.959
    },
    {
      "lang": "en",
      "text": "if if I talk to you and say like, &quot;Hey,",
      "offset": 5683.679,
      "duration": 7.201
    },
    {
      "lang": "en",
      "text": "3 * 4 is 26, but like you know 3 * 4 +",
      "offset": 5686.239,
      "duration": 6.561
    },
    {
      "lang": "en",
      "text": "8, you're you're not going to take my",
      "offset": 5690.88,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "26, right?&quot; Like AGI can be smarter than",
      "offset": 5692.8,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "being tricked, right? Like it will still",
      "offset": 5695.44,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "fact check the knowledge it's been",
      "offset": 5697.199,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "given. I think that's right. But I think",
      "offset": 5698.56,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "I think that's when you get these mixes",
      "offset": 5700.56,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "where it's like it's got one circuit",
      "offset": 5702,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "that's going to be like, &quot;Well, that's",
      "offset": 5703.44,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "just stupid.&quot; like 3 * 4 is 12 and it's",
      "offset": 5704.96,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "also got an induction circuit that's",
      "offset": 5707.199,
      "duration": 2.48
    },
    {
      "lang": "en",
      "text": "going to be like no no no like the last",
      "offset": 5708.48,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "time we saw it it was 28 so it's 28 plus",
      "offset": 5709.679,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "8 or whatever and so I think that's",
      "offset": 5711.679,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "that's the last pattern that we see in",
      "offset": 5713.199,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "these is these like parallel circuits",
      "offset": 5714.88,
      "duration": 3.279
    },
    {
      "lang": "en",
      "text": "and sometimes when you see the models",
      "offset": 5716.719,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "getting stuff wrong it's because like",
      "offset": 5718.159,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "they have two circuits for like both",
      "offset": 5720.08,
      "duration": 3.119
    },
    {
      "lang": "en",
      "text": "interpretations and like the circuit",
      "offset": 5721.76,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "that was wrong like barely edged out in",
      "offset": 5723.199,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "terms of like voting for the lowjet than",
      "offset": 5726.159,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "the circuit that was right and so I",
      "offset": 5728.239,
      "duration": 3.281
    },
    {
      "lang": "en",
      "text": "think that you know we haven't looked at",
      "offset": 5730.08,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "it but like the what is it like 9 or",
      "offset": 5731.52,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "9.11 bigger than 9.8 I think a lot of",
      "offset": 5733.04,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "these things are of that shape where",
      "offset": 5734.88,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "there's like one thing that's doing the",
      "offset": 5736.48,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "right like one circuit that's doing the",
      "offset": 5738.32,
      "duration": 2.319
    },
    {
      "lang": "en",
      "text": "right computation and there's another",
      "offset": 5739.52,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "circuit that's getting fooled and it's",
      "offset": 5740.639,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "it's slightly more likely for the",
      "offset": 5742.88,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "listener. If you want to win a quick",
      "offset": 5744.48,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "$100 from Emanuel, Quinn 3 is what you",
      "offset": 5745.92,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "should do this on. They release the base",
      "offset": 5748.239,
      "duration": 3.041
    },
    {
      "lang": "en",
      "text": "model and they release the post train.",
      "offset": 5749.6,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "So then just do it on both. That's",
      "offset": 5751.28,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "right. Show me show me the the like",
      "offset": 5753.04,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "proof that that like it doesn't exist in",
      "offset": 5754.88,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "the base model but it does in the",
      "offset": 5756.8,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "finetuning and then send me your Venmo.",
      "offset": 5758.239,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "Just show that you've done the work. I",
      "offset": 5759.84,
      "duration": 2.399
    },
    {
      "lang": "en",
      "text": "think that's that's like that's 100",
      "offset": 5761.199,
      "duration": 2.801
    },
    {
      "lang": "en",
      "text": "bucks to me. Yeah. Okay.",
      "offset": 5762.239,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "I'll you drive a hard bargain, but",
      "offset": 5764,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "you're right. Well, the the other",
      "offset": 5765.44,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "question here is so like um have you",
      "offset": 5767.52,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "thought about how this gets affected",
      "offset": 5770,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "when you start to have reasoning models,",
      "offset": 5771.76,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "right? Like right now token predictors",
      "offset": 5773.36,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "are pretty straightforward, right? We go",
      "offset": 5775.92,
      "duration": 3.279
    },
    {
      "lang": "en",
      "text": "through the layers, we output token. As",
      "offset": 5777.28,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "we scale this out with like test time",
      "offset": 5779.199,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "compute, right? Test time thinking, how",
      "offset": 5781.12,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "does that like affect the mech interpret",
      "offset": 5783.199,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "research, right? Like if I have a model",
      "offset": 5785.52,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "that spends 3 minutes, 20 minutes, like",
      "offset": 5788,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "is there more stuff? is have we started",
      "offset": 5790.4,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "looking into this? There's there was",
      "offset": 5792.48,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "this like joke on the team when like",
      "offset": 5793.92,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "reasoning models became big or maybe",
      "offset": 5795.6,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "it's like like gallumor or something but",
      "offset": 5797.36,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "I was like oh like why do you need",
      "offset": 5799.199,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "interp like bro the model the model just",
      "offset": 5800.96,
      "duration": 3.279
    },
    {
      "lang": "en",
      "text": "tells you yeah the model just tells you",
      "offset": 5802.96,
      "duration": 2.719
    },
    {
      "lang": "en",
      "text": "what it's doing right and so I think",
      "offset": 5804.239,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "like examples examples like this is is",
      "offset": 5805.679,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "job security for us where like you know",
      "offset": 5808.239,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "it's like there's there's examples of",
      "offset": 5810.32,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "like the train of thought is not",
      "offset": 5811.92,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "faithful like the model tells you it did",
      "offset": 5813.28,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "it one way and it did it another way. We",
      "offset": 5815.28,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "have another like for math we have",
      "offset": 5817.36,
      "duration": 2.879
    },
    {
      "lang": "en",
      "text": "another example where like you know if",
      "offset": 5818.56,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "you like if you ask the model how it",
      "offset": 5820.239,
      "duration": 3.521
    },
    {
      "lang": "en",
      "text": "does math it's like oh I do the like",
      "offset": 5822.08,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "longhand algorithm I first do the last",
      "offset": 5823.76,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "digit and then I carry over the one and",
      "offset": 5825.84,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "then you look at the internal circuit",
      "offset": 5827.84,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "and it's this like bonkers thing it's",
      "offset": 5829.04,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "doing that's not that at all. So I think",
      "offset": 5830.56,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "there's like a sense in which right now",
      "offset": 5832.08,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "the chain of thought is is unfaithful or",
      "offset": 5834.239,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "at least you can't read the chain of",
      "offset": 5836.48,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "thought and trust that that's how the",
      "offset": 5838.08,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "model did it. So I think you still need",
      "offset": 5839.76,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "sort of like either to train models",
      "offset": 5841.52,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "differently so that that becomes true",
      "offset": 5843.199,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "one day, right? Or you need interp for",
      "offset": 5844.48,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "that. But then I think there's another",
      "offset": 5847.04,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "question which you're alluding to I'm",
      "offset": 5848.48,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "assuming which is like okay like model",
      "offset": 5850.239,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "like samples 6,000 tokens like this",
      "offset": 5852.159,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "gives us an explanation for one token at",
      "offset": 5854.56,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "a time like what am I going to use like",
      "offset": 5856.159,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "6,000 graphs and be like oh like this",
      "offset": 5857.52,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "when it when it did this punctuation it",
      "offset": 5860.08,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "was thinking about this thing but here",
      "offset": 5861.92,
      "duration": 2.799
    },
    {
      "lang": "en",
      "text": "was think so that's that's like not",
      "offset": 5862.96,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "feasible. And so one area of work that I",
      "offset": 5864.719,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "think is interesting is extending this",
      "offset": 5867.119,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "work to like work over like long sampled",
      "offset": 5868.639,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "sequences. You can think of a bunch of",
      "offset": 5871.44,
      "duration": 2.799
    },
    {
      "lang": "en",
      "text": "lowhanging fruit here where like instead",
      "offset": 5872.719,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "of just like looking at one output, you",
      "offset": 5874.239,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "look at like a series of output versus a",
      "offset": 5875.679,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "series of other outputs, but sort of",
      "offset": 5878.08,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "like trying to think beyond the sort of",
      "offset": 5879.92,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "like one token. Like most of the things",
      "offset": 5881.52,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "that language models do that are",
      "offset": 5883.44,
      "duration": 2.32
    },
    {
      "lang": "en",
      "text": "interesting aren't just like the one",
      "offset": 5884.56,
      "duration": 2.639
    },
    {
      "lang": "en",
      "text": "token. It's the it's the behavior",
      "offset": 5885.76,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "aggregated over many, right? And so I",
      "offset": 5887.199,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "think that's another area that's just",
      "offset": 5889.6,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "like fun to explore. I was just going to",
      "offset": 5890.8,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "say like hyperparameters when you do",
      "offset": 5892.56,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "inference, right? like if we change the",
      "offset": 5894.08,
      "duration": 2.92
    },
    {
      "lang": "en",
      "text": "temperature, if we change our sampling",
      "offset": 5895.52,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "methods, have you found any interesting",
      "offset": 5897,
      "duration": 4.28
    },
    {
      "lang": "en",
      "text": "conclusion? Any stuff that just hasn't",
      "offset": 5899.76,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "made it to the paper? So, not on that",
      "offset": 5901.28,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "because, you know, we just look at the",
      "offset": 5904.239,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "logit distribution and so we don't we",
      "offset": 5906.159,
      "duration": 5.121
    },
    {
      "lang": "en",
      "text": "don't actually sample here, right? We",
      "offset": 5908.56,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "have everything. Why should they care?",
      "offset": 5911.28,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "So, like the closest thing we've done",
      "offset": 5913.52,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "that I think is kind of fun, did I show",
      "offset": 5914.8,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "it here? is if you look at the planning",
      "offset": 5916.56,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "thing, we did this version where you",
      "offset": 5920.56,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "sample like 10 poems for each of these",
      "offset": 5923.04,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "plans. And what's cool is like the model",
      "offset": 5925.28,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "will find 10 different ways to arrive at",
      "offset": 5927.199,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "its plan. You know, it's like like um oh",
      "offset": 5929.199,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "actually I think sorry I think we have",
      "offset": 5932.239,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "it here. Yeah. Okay. These are a few",
      "offset": 5933.6,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "examples. So if you inject green here,",
      "offset": 5935.92,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "so you're forcing you're forcing the",
      "offset": 5937.84,
      "duration": 2.799
    },
    {
      "lang": "en",
      "text": "model to rhyme with green even though it",
      "offset": 5939.28,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "really wants to rhyme with rabbit or",
      "offset": 5940.639,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "grab it. It'll say evaded the farmer so",
      "offset": 5942.639,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "youthful and green, but also it'll say",
      "offset": 5945.199,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "freeing it from the garden's green,",
      "offset": 5946.719,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "etc., etc., etc. And so there's like",
      "offset": 5948.56,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "this thing that's interesting here where",
      "offset": 5950.88,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "like the plan isn't just a plan that",
      "offset": 5952.32,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "matters for your like most likely, you",
      "offset": 5955.119,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "know, like temperature zero completion.",
      "offset": 5956.96,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "It's like affecting the whole",
      "offset": 5958.719,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "distribution, which makes sense as it",
      "offset": 5959.92,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "should, right? But you could imagine,",
      "offset": 5962.719,
      "duration": 3.121
    },
    {
      "lang": "en",
      "text": "you know, for all this stuff, it's like",
      "offset": 5964.32,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "you could imagine it makes sense once",
      "offset": 5965.84,
      "duration": 2.56
    },
    {
      "lang": "en",
      "text": "you see it, but you could totally",
      "offset": 5967.44,
      "duration": 1.92
    },
    {
      "lang": "en",
      "text": "imagine that it would have worked a",
      "offset": 5968.4,
      "duration": 1.92
    },
    {
      "lang": "en",
      "text": "different way or something. It could",
      "offset": 5969.36,
      "duration": 2.56
    },
    {
      "lang": "en",
      "text": "have been just like the temp zero thing.",
      "offset": 5970.32,
      "duration": 2.879
    },
    {
      "lang": "en",
      "text": "I think this is also like a broader",
      "offset": 5971.92,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "theme in the paper where like there's",
      "offset": 5973.199,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "this like you know the IQ curve meme.",
      "offset": 5975.04,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "There's like a version of this meme I",
      "offset": 5977.52,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "think where it's like if you've like",
      "offset": 5978.96,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "never looked at any theory of ML and I",
      "offset": 5980.48,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "tell you like hey guess what you know I",
      "offset": 5983.28,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "found that like Claude is planning",
      "offset": 5985.76,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "you're going to be like yeah man like it",
      "offset": 5987.76,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "it writes my code like it writes my",
      "offset": 5989.119,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "essays of course it's planning like what",
      "offset": 5991.04,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "are you even talking about and there's",
      "offset": 5993.28,
      "duration": 2.56
    },
    {
      "lang": "en",
      "text": "like in the middle there's like all of",
      "offset": 5994.8,
      "duration": 2.319
    },
    {
      "lang": "en",
      "text": "us that have spent years doing it we're",
      "offset": 5995.84,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "like no it's like only predicting the",
      "offset": 5997.119,
      "duration": 3.281
    },
    {
      "lang": "en",
      "text": "marginal distribution for the next token",
      "offset": 5998.88,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "like it's like it cannot it's next to",
      "offset": 6000.4,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "predict of course like how would it ever",
      "offset": 6003.04,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "be planning and then there's like no",
      "offset": 6004.32,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "we've like spent you know millions and",
      "offset": 6005.84,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "invested like uh uh like tens of people",
      "offset": 6007.44,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "in this research and we found that it's",
      "offset": 6009.36,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "planning like that's my IQ curve meme",
      "offset": 6010.88,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "for for this research. Amazing. We'll",
      "offset": 6013.119,
      "duration": 2.961
    },
    {
      "lang": "en",
      "text": "draw that out. I'll draw that one up.",
      "offset": 6014.719,
      "duration": 3.041
    },
    {
      "lang": "en",
      "text": "I'm pretty good at the meme generation.",
      "offset": 6016.08,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "A couple questions on just the",
      "offset": 6017.76,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "follow-ups. Uh now was there any debate",
      "offset": 6019.04,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "about publishing this at all? Because",
      "offset": 6021.52,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "the models are aware that they are being",
      "offset": 6024.239,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "tested. Yeah. And by publishing this you",
      "offset": 6026.56,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "are telling them that we're they are",
      "offset": 6028.96,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "being watched and dissected. If you take",
      "offset": 6030.48,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "and I think Anthropic is one of the most",
      "offset": 6033.52,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "people who are serious about model",
      "offset": 6035.199,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "safety and doom risk and all that if you",
      "offset": 6036.56,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "take this seriously like this is going",
      "offset": 6039.44,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "to make it into the training data at",
      "offset": 6041.36,
      "duration": 2.48
    },
    {
      "lang": "en",
      "text": "some point and the models are going to",
      "offset": 6042.56,
      "duration": 2.639
    },
    {
      "lang": "en",
      "text": "figure out that they need to hide it",
      "offset": 6043.84,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "from us. I think this is like a benefit",
      "offset": 6045.199,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "risk trade-off, right? We're like, okay,",
      "offset": 6047.84,
      "duration": 2.56
    },
    {
      "lang": "en",
      "text": "so what's the reason for publishing",
      "offset": 6049.52,
      "duration": 1.92
    },
    {
      "lang": "en",
      "text": "this? The reason for publishing this is",
      "offset": 6050.4,
      "duration": 2.319
    },
    {
      "lang": "en",
      "text": "that we think interpretably is",
      "offset": 6051.44,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "important. We think it's tractable and",
      "offset": 6052.719,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "we think more people should work on it.",
      "offset": 6055.199,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "And so publishing it helps us like",
      "offset": 6056.88,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "accomplish with these goals uh all these",
      "offset": 6059.76,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "goals which which we think are just like",
      "offset": 6061.84,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "crucial like I think there's there's a",
      "offset": 6063.76,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "real difference in the world like 2",
      "offset": 6065.92,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "years from now depending on sort of like",
      "offset": 6067.6,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "how many people take seriously the",
      "offset": 6069.6,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "question of trying to understand how",
      "offset": 6071.52,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "models work and like deploy resources to",
      "offset": 6072.56,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "answer that question. That's the benefit",
      "offset": 6075.44,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "and yeah there's like risks in terms of",
      "offset": 6077.199,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "this landing in the training set. I",
      "offset": 6080.159,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "think I think we're already sort of like",
      "offset": 6081.679,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "concerned about different papers have",
      "offset": 6083.76,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "have like also you know we like or not",
      "offset": 6085.679,
      "duration": 2.721
    },
    {
      "lang": "en",
      "text": "concerned but like there's like",
      "offset": 6087.6,
      "duration": 2
    },
    {
      "lang": "en",
      "text": "different papers that have the same risk",
      "offset": 6088.4,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "like we had like the alignment faking",
      "offset": 6089.6,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "you know paper or like one of the",
      "offset": 6091.6,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "examples in here is this hidden goals",
      "offset": 6094.32,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "and misaligned models that's referencing",
      "offset": 6096.48,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "another paper that we shipped where we",
      "offset": 6098.4,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "actually a team at Enthropic trained a",
      "offset": 6100.88,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "model to have like weird hidden goals",
      "offset": 6103.44,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "and then gave it to a bunch of of other",
      "offset": 6106.32,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "teams and said figure out what's wrong",
      "offset": 6108.239,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "figure out what's wrong with it",
      "offset": 6110.239,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "atic like a misaligned model and here's",
      "offset": 6120.36,
      "duration": 5.16
    },
    {
      "lang": "en",
      "text": "exactly how we caught it. Uh that also",
      "offset": 6122.719,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "like so I think you know there's there's",
      "offset": 6125.52,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "always a trade-off with those. I think",
      "offset": 6127.6,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "so far we've aired on the side of like",
      "offset": 6128.8,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "publishing but that's definitely been a",
      "offset": 6130.56,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "sort of like dinner time conversation",
      "offset": 6132.56,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "topic for now it is but at some point",
      "offset": 6134.88,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "you know it's not yeah I think it's",
      "offset": 6136.96,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "totally reasonable. A quick little",
      "offset": 6139.04,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "follow-up to that. So like in general",
      "offset": 6140.32,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "papers have kind of died off, right?",
      "offset": 6142.32,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "Like labs don't put out papers. They",
      "offset": 6144.239,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "don't put out research. We have",
      "offset": 6146.32,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "technical blog posts and we don't have",
      "offset": 6147.6,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "much. At the same time, you know, sure",
      "offset": 6149.6,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "there's like a lot of people that should",
      "offset": 6152.08,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "work on mechaning what models do. How",
      "offset": 6153.6,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "about the side of just models in",
      "offset": 6156.32,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "general? So like how do we make a haiku",
      "offset": 6158.32,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "type model, right? How do we make a",
      "offset": 6161.04,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "cloud model? Is there a discussion",
      "offset": 6162.639,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "around open research, open data sets,",
      "offset": 6164.639,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "training, just learnings of what we've",
      "offset": 6167.84,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "done recently? You know, as open AI has",
      "offset": 6169.52,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "sunset GPT4, a lot of people are like,",
      "offset": 6171.679,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "oh, can we put out the weights? Yeah.",
      "offset": 6173.36,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "So, is it weights? Is it papers? Is it",
      "offset": 6175.119,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "learning? There seems to be a lot of",
      "offset": 6176.88,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "forward, you know, work in anthropic",
      "offset": 6178.8,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "putting out mechan research open. I said",
      "offset": 6181.36,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "that they'll put out an open source",
      "offset": 6183.76,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "model, but just anything if you can talk",
      "offset": 6184.96,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "to about that. Yeah, I mean I I don't",
      "offset": 6186.8,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "have that's definitely like way above my",
      "offset": 6189.04,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "pay grade. So I don't think that I have",
      "offset": 6190.96,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "like anything super insightful to add",
      "offset": 6192.639,
      "duration": 2.961
    },
    {
      "lang": "en",
      "text": "other than you know kind of like",
      "offset": 6194.159,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "referencing Dario's post, right? Where",
      "offset": 6195.6,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "it's like putting this out directly and",
      "offset": 6197.52,
      "duration": 5.679
    },
    {
      "lang": "en",
      "text": "other safety publishers definitely like",
      "offset": 6200.56,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "help us sort of like in the race that he",
      "offset": 6203.199,
      "duration": 3.121
    },
    {
      "lang": "en",
      "text": "talks about where it's like well we need",
      "offset": 6205.199,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "to figure a lot of this safety stuff out",
      "offset": 6206.32,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "before the models get too good.",
      "offset": 6209.119,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "Publishing how to make the models too",
      "offset": 6211.04,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "good kind of goes on the other side of",
      "offset": 6212.96,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "that. Um, but yeah, like I will just",
      "offset": 6214.4,
      "duration": 5.839
    },
    {
      "lang": "en",
      "text": "dimmer and say that's sort of like above",
      "offset": 6218.48,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "my pay grade. That's fair enough. But I",
      "offset": 6220.239,
      "duration": 3.281
    },
    {
      "lang": "en",
      "text": "think the the last piece is just like",
      "offset": 6222.08,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "the behind the scenes like very",
      "offset": 6223.52,
      "duration": 3.119
    },
    {
      "lang": "en",
      "text": "everyone's very curious about why these",
      "offset": 6225.04,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "are so pretty, how much work goes into",
      "offset": 6226.639,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "these things, maybe why it's worth the",
      "offset": 6228.48,
      "duration": 5.679
    },
    {
      "lang": "en",
      "text": "work as opposed to a normal paper.",
      "offset": 6231.52,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "Obviously, no one's no one's",
      "offset": 6234.159,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "complaining, but like it is way more",
      "offset": 6235.28,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "effort from the time the the work is",
      "offset": 6237.84,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "done to the time you publish this plus",
      "offset": 6239.76,
      "duration": 5.359
    },
    {
      "lang": "en",
      "text": "the video plus the whatever. It's extra",
      "offset": 6242.32,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "work and like you know maybe what's",
      "offset": 6245.119,
      "duration": 2.56
    },
    {
      "lang": "en",
      "text": "what's involved? What's what's it like",
      "offset": 6246.48,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "behind the scenes? Why is it worth it?",
      "offset": 6247.679,
      "duration": 3.121
    },
    {
      "lang": "en",
      "text": "Yeah, it's kind of interesting. It was",
      "offset": 6249.44,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "it was fun being part of this this",
      "offset": 6250.8,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "process because there's definitely like",
      "offset": 6252.56,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "a big production. Chris and other folks",
      "offset": 6253.52,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "on the team have been doing this for a",
      "offset": 6255.92,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "while. So this is not their first rodeo.",
      "offset": 6257.119,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "So they have a bunch of huristics to",
      "offset": 6258.88,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "like help make this this better. And",
      "offset": 6260.32,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "like one of the things that that like",
      "offset": 6262,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "helps with this is like okay so each of",
      "offset": 6263.679,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "these diagrams is pretty but really the",
      "offset": 6265.04,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "hard part or like not the hard part but",
      "offset": 6267.04,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "the initial part is like just like get",
      "offset": 6268.719,
      "duration": 2.561
    },
    {
      "lang": "en",
      "text": "the data like get the experimental data",
      "offset": 6269.76,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "in. And then that's what we sort of like",
      "offset": 6271.28,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "sprinted on initially being like cool",
      "offset": 6273.04,
      "duration": 3.119
    },
    {
      "lang": "en",
      "text": "like let's get all of the experimental",
      "offset": 6274.88,
      "duration": 2.799
    },
    {
      "lang": "en",
      "text": "results like have people test them",
      "offset": 6276.159,
      "duration": 3.281
    },
    {
      "lang": "en",
      "text": "verify that we believe them like this is",
      "offset": 6277.679,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "you know what the like the behavior is",
      "offset": 6279.44,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "here like test it do an intervention",
      "offset": 6281.28,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "validate it all that stuff then once you",
      "offset": 6282.8,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "have the data you can sort like quickly",
      "offset": 6284.48,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "iterate on these um each of the",
      "offset": 6286.32,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "illustrations here are like drawn",
      "offset": 6289.28,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "basically they're like each drawn",
      "offset": 6291.52,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "individually and so that definitely",
      "offset": 6292.8,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "takes a while. Yeah. Like is it is it",
      "offset": 6294.56,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "you guys? Is it an an agency",
      "offset": 6296.48,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "specializes? You start from a whiteboard",
      "offset": 6299.56,
      "duration": 4.04
    },
    {
      "lang": "en",
      "text": "and then it translates into pseudo code",
      "offset": 6301.6,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "on JavaScript. So I mean these are these",
      "offset": 6303.6,
      "duration": 3.519
    },
    {
      "lang": "en",
      "text": "are sort of like you know they're",
      "offset": 6305.52,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "representations of we have this graph",
      "offset": 6307.119,
      "duration": 2.881
    },
    {
      "lang": "en",
      "text": "and then here at the bottom we have this",
      "offset": 6308.4,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "like super node version like this.",
      "offset": 6310,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "Believe it or not uh this is generated",
      "offset": 6311.6,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "automatically. This is the same data as",
      "offset": 6313.44,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "as like this basically. Yeah. Um and so",
      "offset": 6316.08,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "what we do by hand is sort of like",
      "offset": 6319.6,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "literally lay out the full thing. uh",
      "offset": 6321.28,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "have have like you know boxes for each",
      "offset": 6323.6,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "of these have arrows. We have super good",
      "offset": 6325.76,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "people on the team that have worked on",
      "offset": 6328.32,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "data visualization for a very long time",
      "offset": 6329.52,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "and so that that like have built tooling",
      "offset": 6332.159,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "to help you know scrubs like me actually",
      "offset": 6334.32,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "like make one of these. So So there's a",
      "offset": 6336.719,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "class of people who are like D3JS gods",
      "offset": 6339.28,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "who just do this for a living. That's",
      "offset": 6341.28,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "exactly right. And if you have a few of",
      "offset": 6343.52,
      "duration": 2.719
    },
    {
      "lang": "en",
      "text": "those on your team, it turns out that",
      "offset": 6344.88,
      "duration": 2.48
    },
    {
      "lang": "en",
      "text": "they can like they can definitely do",
      "offset": 6346.239,
      "duration": 2.161
    },
    {
      "lang": "en",
      "text": "this on their own, but they can also",
      "offset": 6347.36,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "just like give you tools where like then",
      "offset": 6348.4,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "it's it's dummy proof for for people,",
      "offset": 6350.4,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "you know, on on the research side to",
      "offset": 6352.32,
      "duration": 2.56
    },
    {
      "lang": "en",
      "text": "sort of like build these. And like don't",
      "offset": 6353.84,
      "duration": 2.879
    },
    {
      "lang": "en",
      "text": "get me wrong, I I don't want to like",
      "offset": 6354.88,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "underell this is a lot of work. So maybe",
      "offset": 6356.719,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "I'll I'll say that like both on the",
      "offset": 6358.639,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "people building the tools and then each",
      "offset": 6360.8,
      "duration": 2.879
    },
    {
      "lang": "en",
      "text": "individual person that you know worked",
      "offset": 6362.239,
      "duration": 2.801
    },
    {
      "lang": "en",
      "text": "on an experiment had to sort of like",
      "offset": 6363.679,
      "duration": 2.56
    },
    {
      "lang": "en",
      "text": "build one of those, make sure it looked",
      "offset": 6365.04,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "good. I have spent a good amount of time",
      "offset": 6366.239,
      "duration": 5.561
    },
    {
      "lang": "en",
      "text": "aligning arrows. But when we had a team",
      "offset": 6368.08,
      "duration": 6.8
    },
    {
      "lang": "en",
      "text": "meeting like it was a couple months ago,",
      "offset": 6371.8,
      "duration": 5.64
    },
    {
      "lang": "en",
      "text": "somebody on the team asked how many of",
      "offset": 6374.88,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "the people on this team are here at",
      "offset": 6377.44,
      "duration": 5.759
    },
    {
      "lang": "en",
      "text": "least in part because they like read one",
      "offset": 6380.4,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "of these papers and thought like wow",
      "offset": 6383.199,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "this is so compelling like this like",
      "offset": 6385.04,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "makes sense. It's immersive and we got",
      "offset": 6386.56,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "every hand up which I didn't expect. I",
      "offset": 6388.719,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "like raised my hand kind of like shy and",
      "offset": 6390.56,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "everybody's hand was up. And I think",
      "offset": 6392.159,
      "duration": 3.281
    },
    {
      "lang": "en",
      "text": "there's a sense in which like this stuff",
      "offset": 6393.52,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "you know we've talked about it for like",
      "offset": 6395.44,
      "duration": 2.32
    },
    {
      "lang": "en",
      "text": "whatever like a couple hours now it's",
      "offset": 6396.48,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "complicated. the math behind it is sort",
      "offset": 6397.76,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "of like tricky and so I think it makes",
      "offset": 6400,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "it even more worth it to distill it in",
      "offset": 6401.92,
      "duration": 3.239
    },
    {
      "lang": "en",
      "text": "simple concepts because the actual",
      "offset": 6403.6,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "takeaways can be clearly explained and",
      "offset": 6405.159,
      "duration": 6.04
    },
    {
      "lang": "en",
      "text": "it's worth putting the time to do that",
      "offset": 6409.04,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "in particular with the goals I mentioned",
      "offset": 6411.199,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "in mind right where it's like okay well",
      "offset": 6413.119,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "if somebody's going to be able to read",
      "offset": 6415.04,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "this like if we gave them an archive",
      "offset": 6416.32,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "paper with a bunch of equation and some",
      "offset": 6417.84,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "like random plot they'd be like that's",
      "offset": 6419.6,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "not for me but they see this and they're",
      "offset": 6421.199,
      "duration": 2.881
    },
    {
      "lang": "en",
      "text": "like hey like this is really interesting",
      "offset": 6422.8,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "I wonder like on you know my local model",
      "offset": 6424.08,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "if like it's doing something similar I I",
      "offset": 6426.32,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "think it's worth it for other people to",
      "offset": 6428.4,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "do this is have everyone on staff like",
      "offset": 6430.08,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "spend effort shaping the data and",
      "offset": 6433.04,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "shaping like what you want to visualize.",
      "offset": 6435.52,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "Have some D3 gods. It's like a month of",
      "offset": 6437.36,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "work. I think it depends. I mean like I",
      "offset": 6440.32,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "would say that I would expect almost",
      "offset": 6442.32,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "every other paper to sort like be in",
      "offset": 6444.159,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "terms of like the scope the scope of",
      "offset": 6445.52,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "this was just so big because we shipped",
      "offset": 6447.6,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "two papers at once and one paper was",
      "offset": 6449.04,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "sort of like this like giant methods",
      "offset": 6451.76,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "paper uh and the other one was 10",
      "offset": 6453.119,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "different case studies. Yeah. So I think",
      "offset": 6455.76,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "it's like not representative of like the",
      "offset": 6457.92,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "effort you So I'll give you like another",
      "offset": 6460.239,
      "duration": 3.121
    },
    {
      "lang": "en",
      "text": "example. We have these updates that we",
      "offset": 6462.08,
      "duration": 2.559
    },
    {
      "lang": "en",
      "text": "publish almost every month when we get",
      "offset": 6463.36,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "to them. And there's one that a couple",
      "offset": 6464.639,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "people on our team posted and it's an",
      "offset": 6466.96,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "update to one of the cases in the paper.",
      "offset": 6468.8,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "So one of the reasons that we're really",
      "offset": 6471.6,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "excited about this method is once you've",
      "offset": 6472.96,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "built your like infrastructure like to",
      "offset": 6474.32,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "go from a prompt to like what happened",
      "offset": 6476.88,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "is you know o of minutes. And so that",
      "offset": 6479.04,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "lets you do like a bunch of of of",
      "offset": 6481.199,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "investigations. And also once you've",
      "offset": 6482.88,
      "duration": 2.56
    },
    {
      "lang": "en",
      "text": "built like some of the infra",
      "offset": 6484.56,
      "duration": 2.32
    },
    {
      "lang": "en",
      "text": "infrastructure to make these diagrams,",
      "offset": 6485.44,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "it's pretty quick. And so this was sort",
      "offset": 6486.88,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "of like this update of just like, hey,",
      "offset": 6489.36,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "we looked at this jailbreak again. We",
      "offset": 6491.04,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "found some like nuance on it. That was I",
      "offset": 6492.32,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "think like a matter of like a couple",
      "offset": 6494.56,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "days, you know, maybe I shouldn't be",
      "offset": 6495.679,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "that confident cuz I wasn't the one that",
      "offset": 6498.08,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "worked on it, but as far as I can tell,",
      "offset": 6499.28,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "it was a few days at least on on the",
      "offset": 6500.96,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "part that you're asking about of like,",
      "offset": 6502.639,
      "duration": 2.801
    },
    {
      "lang": "en",
      "text": "oh, making this diagram for the diagram",
      "offset": 6503.84,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "itself, probably less than that. Uh, but",
      "offset": 6505.44,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "like you know, the experiment and the",
      "offset": 6507.28,
      "duration": 2.56
    },
    {
      "lang": "en",
      "text": "diagram and stuff, it just doesn't take",
      "offset": 6508.56,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "that long. the once you've paid the",
      "offset": 6509.84,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "initial cost and I think like basically",
      "offset": 6511.28,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "we've built a lot of infrastructure now",
      "offset": 6513.6,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "that we're able to like turn the crank",
      "offset": 6515.84,
      "duration": 2.879
    },
    {
      "lang": "en",
      "text": "on and that's quite like it's an",
      "offset": 6517.36,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "exciting time and I think it's I think",
      "offset": 6518.719,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "it's true at least we've done a lot of",
      "offset": 6520.719,
      "duration": 3.281
    },
    {
      "lang": "en",
      "text": "conceptual work which hopefully like",
      "offset": 6522.719,
      "duration": 3.041
    },
    {
      "lang": "en",
      "text": "generalizes to people outside and I",
      "offset": 6524,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "think for for people outside it's also",
      "offset": 6525.76,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "like not necessary I think to like do",
      "offset": 6527.36,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "the full fancy render like I think if",
      "offset": 6529.6,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "you you know we've we've actually oh I",
      "offset": 6532,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "should say we've actually open sourced",
      "offset": 6533.92,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "this interface ah you're disappointed",
      "offset": 6536.159,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "huh cuz it's the messier one",
      "offset": 6538.639,
      "duration": 2.961
    },
    {
      "lang": "en",
      "text": "This is the one that you get when you",
      "offset": 6540.32,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "learn.",
      "offset": 6541.6,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "So, you know, if you produce graphs, you",
      "offset": 6543.36,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "can just like this is uh open source and",
      "offset": 6545.199,
      "duration": 3.121
    },
    {
      "lang": "en",
      "text": "it's linked at the top of circuit",
      "offset": 6547.04,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "tracing. Awesome. So, people can just",
      "offset": 6548.32,
      "duration": 2.879
    },
    {
      "lang": "en",
      "text": "use it and don't have to reimplement",
      "offset": 6549.92,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "that. Uh for what it's worth, this is",
      "offset": 6551.199,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "much more work than the interactive",
      "offset": 6552.88,
      "duration": 2.799
    },
    {
      "lang": "en",
      "text": "diagrams because this is where we do all",
      "offset": 6554.4,
      "duration": 2.799
    },
    {
      "lang": "en",
      "text": "of our work. It's sort of the like the",
      "offset": 6555.679,
      "duration": 5.121
    },
    {
      "lang": "en",
      "text": "IDE of inspecting how the model work.",
      "offset": 6557.199,
      "duration": 5.281
    },
    {
      "lang": "en",
      "text": "Okay. Well, that's a little bit of",
      "offset": 6560.8,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "behind the scenes. Uh no, it's very",
      "offset": 6562.48,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "impressive. I want to encourage others",
      "offset": 6564.239,
      "duration": 3.521
    },
    {
      "lang": "en",
      "text": "to do it, but obviously it just takes a",
      "offset": 6565.6,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "lot of manual effort and a lot of love.",
      "offset": 6567.76,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "I guess one last question on that is",
      "offset": 6570.639,
      "duration": 2.961
    },
    {
      "lang": "en",
      "text": "like what are kind of the biggest",
      "offset": 6572.4,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "blockers in the field right now? Like me",
      "offset": 6573.6,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "and seems interesting. A lot of people",
      "offset": 6575.76,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "are interested but don't work on it and",
      "offset": 6577.6,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "you're kind of like you know really deep",
      "offset": 6579.44,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "into it. What are some of the blockers",
      "offset": 6580.8,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "that like we still have to overcome?",
      "offset": 6582.32,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "Sorry in in mechan specifically in",
      "offset": 6585.36,
      "duration": 5.759
    },
    {
      "lang": "en",
      "text": "general for AGI or like in terms of like",
      "offset": 6587.52,
      "duration": 5.599
    },
    {
      "lang": "en",
      "text": "better understanding like what's kind of",
      "offset": 6591.119,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "the vision let's say like 5 10 years",
      "offset": 6593.119,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "down where does this like where does",
      "offset": 6595.119,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "this research end can we you know map",
      "offset": 6597.44,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "every neuron to what it understands can",
      "offset": 6599.84,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "we perfectly control things had a bit on",
      "offset": 6601.92,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "this but like you know what are some of",
      "offset": 6604.4,
      "duration": 2.719
    },
    {
      "lang": "en",
      "text": "the key blockers that are like",
      "offset": 6605.679,
      "duration": 3.121
    },
    {
      "lang": "en",
      "text": "preventing us from getting there outside",
      "offset": 6607.119,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "of just like throw more people throw",
      "offset": 6608.8,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "more time at it is it like open research",
      "offset": 6610.56,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "just I'm pretty excited about the",
      "offset": 6612.56,
      "duration": 2.559
    },
    {
      "lang": "en",
      "text": "current tra trajectory which there's",
      "offset": 6613.76,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "there's more and more people working on",
      "offset": 6615.119,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "understanding model internals. I think",
      "offset": 6617.36,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "it's maybe unsatisfying as an answer,",
      "offset": 6619.36,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "but I think like more of what's",
      "offset": 6621.119,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "happening have it be faster, more people",
      "offset": 6622.4,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "is probably like the thing I think of. I",
      "offset": 6624.88,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "think there's like pretty clear",
      "offset": 6626.639,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "footholds you know like some of this",
      "offset": 6629.28,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "work but also a lot of a lot of like",
      "offset": 6631.199,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "just uh work from from other groups and",
      "offset": 6632.8,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "then it's about like cool like fill in",
      "offset": 6635.52,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "the gaps as I said like let's let's work",
      "offset": 6637.92,
      "duration": 2.799
    },
    {
      "lang": "en",
      "text": "on like understanding attention let's",
      "offset": 6639.44,
      "duration": 2.799
    },
    {
      "lang": "en",
      "text": "work on understanding longer prompts",
      "offset": 6640.719,
      "duration": 2.641
    },
    {
      "lang": "en",
      "text": "let's work on like finding different",
      "offset": 6642.239,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "like replacement architectures that sort",
      "offset": 6643.36,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "of stuff it's kind of nice I think it's",
      "offset": 6645.199,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "a good time to join now uh and I can",
      "offset": 6648.159,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "maybe I can tell like a really short",
      "offset": 6650.239,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "thing which is when I switched to interp",
      "offset": 6651.52,
      "duration": 5.119
    },
    {
      "lang": "en",
      "text": "it was after the team had published the",
      "offset": 6653.84,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "original dictionary learning paper which",
      "offset": 6656.639,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "was towards monos semanticity which I",
      "offset": 6658.48,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "thought was super cool super interesting",
      "offset": 6660.4,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "it was on a one or two layer model uh",
      "offset": 6662.239,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "maybe one layer model the induction",
      "offset": 6664.8,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "heads paper was like on a two-layer",
      "offset": 6666.719,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "model my main concern is I was like okay",
      "offset": 6668.159,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "like inter seems important and we want",
      "offset": 6670.48,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "to understand it but like is this ever",
      "offset": 6671.76,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "going to work on a real model like you",
      "offset": 6673.36,
      "duration": 2.56
    },
    {
      "lang": "en",
      "text": "know it's like oh you're doing your",
      "offset": 6674.96,
      "duration": 2.4
    },
    {
      "lang": "en",
      "text": "little research on your toy model with",
      "offset": 6675.92,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "like 15 parameters cool but we're like",
      "offset": 6677.36,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "you know we need this to work on real",
      "offset": 6679.44,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "models and it turns out scaling it I",
      "offset": 6681.52,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "don't want to say just worked cuz it a",
      "offset": 6684.56,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "lot of a lot of work. I don't mean to",
      "offset": 6685.84,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "apply it towards an effort, but it",
      "offset": 6687.52,
      "duration": 2.719
    },
    {
      "lang": "en",
      "text": "worked. And now we're in the the phase",
      "offset": 6688.56,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "where it's like, oh, cool. These methods",
      "offset": 6690.239,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "work on the models that we care about.",
      "offset": 6692.159,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "And so it's like we have methods that",
      "offset": 6694.239,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "work on the model we care about. We have",
      "offset": 6696.4,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "clear gaps in them. There's no lack",
      "offset": 6697.92,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "again. It's a young field. So there's no",
      "offset": 6700,
      "duration": 2.56
    },
    {
      "lang": "en",
      "text": "lack of ideas. If you have an idea where",
      "offset": 6701.36,
      "duration": 2.56
    },
    {
      "lang": "en",
      "text": "you're like, oh, like the thing that",
      "offset": 6702.56,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "you're doing, I I read the paper and it",
      "offset": 6703.92,
      "duration": 2.319
    },
    {
      "lang": "en",
      "text": "seems kind of dumb that you're doing",
      "offset": 6705.28,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "this. You're probably right. It's",
      "offset": 6706.239,
      "duration": 3.521
    },
    {
      "lang": "en",
      "text": "probably kind of dumb. And so there's",
      "offset": 6708,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "just a lot of stuff that people can try",
      "offset": 6709.76,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "and they can try it locally and sort of",
      "offset": 6711.76,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "like smaller models. And so I I think",
      "offset": 6713.28,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "that it's just like a very good time to",
      "offset": 6714.88,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "just join and try. And it's also like",
      "offset": 6717.119,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "maybe one more thing I'll say is like",
      "offset": 6719.52,
      "duration": 2.639
    },
    {
      "lang": "en",
      "text": "some of it is just so fun. The like",
      "offset": 6720.96,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "biology work is so compelling. Like a",
      "offset": 6722.159,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "lot of this work was just literally",
      "offset": 6724.88,
      "duration": 2.319
    },
    {
      "lang": "en",
      "text": "thinking about you know like I use",
      "offset": 6726,
      "duration": 2.719
    },
    {
      "lang": "en",
      "text": "claude and other models all the time.",
      "offset": 6727.199,
      "duration": 2.641
    },
    {
      "lang": "en",
      "text": "And I was like what are the things that",
      "offset": 6728.719,
      "duration": 2.721
    },
    {
      "lang": "en",
      "text": "are kind of like weird and it's like oh",
      "offset": 6729.84,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "how does it even like do math? Like",
      "offset": 6731.44,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "sometimes it makes mistakes like why",
      "offset": 6733.28,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "does it make mistakes? I speak both",
      "offset": 6734.56,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "French and English. Like it seems like",
      "offset": 6736.08,
      "duration": 3.039
    },
    {
      "lang": "en",
      "text": "it has a slightly different personality",
      "offset": 6737.84,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "in French and English. Why is that? and",
      "offset": 6739.119,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "you can just like, you know, kind of",
      "offset": 6740.88,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "answer your own questions uh and and",
      "offset": 6742.56,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "kind of like probe at that alien",
      "offset": 6744.8,
      "duration": 2.56
    },
    {
      "lang": "en",
      "text": "intelligence that we're all building.",
      "offset": 6746.239,
      "duration": 2.321
    },
    {
      "lang": "en",
      "text": "And I think that's just like a fun thing",
      "offset": 6747.36,
      "duration": 2.64
    },
    {
      "lang": "en",
      "text": "to do. So maybe like chasing the fun is",
      "offset": 6748.56,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "the thing I'll encourage people to do as",
      "offset": 6750,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "well. Well, I think that's this has been",
      "offset": 6751.36,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "really encouraging. You're actually a",
      "offset": 6752.96,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "very charismatic speaker of these",
      "offset": 6754.32,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "things. I feel like more people will be",
      "offset": 6756.08,
      "duration": 3.119
    },
    {
      "lang": "en",
      "text": "joining the field after they listen to",
      "offset": 6757.84,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "you. Uh they can reach out to you at ML",
      "offset": 6759.199,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "Powerard, I guess. Yeah, reach out to me",
      "offset": 6761.36,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "on Twitter or I'm uh Emanuel at",
      "offset": 6762.8,
      "duration": 3.879
    },
    {
      "lang": "en",
      "text": "anthropic if you're",
      "offset": 6765.599,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "interested now. Awesome. Well, thank you",
      "offset": 6766.679,
      "duration": 3.801
    },
    {
      "lang": "en",
      "text": "for your time. Thank you. you think?",
      "offset": 6769.28,
      "duration": 5.16
    },
    {
      "lang": "en",
      "text": "Yeah. Thanks for having me, guys.",
      "offset": 6770.48,
      "duration": 3.96
    },
    {
      "lang": "en",
      "text": "[Music]",
      "offset": 6775.61,
      "duration": 6.919
    }
  ],
  "cleanText": "All right, we are actually going to record this as an intro to the main episode, but here we have my trusty co-host, guest host, I guess, Vibhu. Um, as well as Emmanuel from Anthropic. We're going to talk about the circuit tracing stuff and all the interpretability work. But Emmanuel, maybe you want to do a quick self intro before we get into it.\n\nYeah, sure. I'm Emmanuel. I work on the interpretability team here at Anthropic, more specifically on the circuits team. So we recently released a pair of papers about sort of like the work that we've been doing over the last months, and even more recently we released some code in partnership with the Anthropic fellows program. It was mostly built by Anthropic fellows that lets people play with the research basically. And so happy to talk about that, and we also hope to kind like keep releasing more things and partner with other groups that are working on similar stuff.\n\nYeah, amazing. Uh, we'll get deeper into like the behind the scenes on the main podcast, but let's maybe just dive right in into what you release because that's the most topical thing. This is like literally you just launched it like yesterday, and we'll probably release this episode in a few days. So, yeah, like what can people do or what do you recommend people try?\n\nTotally. So like at a really high level, you know, the sort of like idea of the research itself is to try to explain sort of like some of the computation that a model did when it predicted a given token. And so in our paper, we kind of like show how to do this, and then we show examples of us doing this on internal private models. And then the release this week sort of lets anyone do it for a set of open source models. So notably maybe the most easy one here is like Gemma 2-2b. So you can sort of like think of some prompt, and you kind of like can explain any token that the model samples, and explains here means just like basically blow up the internal state of the model and like show all of the sort of like intermediate things that the model was thinking about before it got to like the final token that it predicted.\n\nYeah. So what some of the things that you guys put out is kind of in the circuit tracing, you have a few core examples, right? So like we can see how these models have internal reasoning states, and there's like multi-hop reasoning, and some of the stuff that we talked about on the podcast was how can people that are interested in how models work kind of do anything, right? So what are open questions? How can people contribute? And it seems like, you know, the follow-up is okay, it's been a few weeks, here's a huge library. So you know, I guess before we even get into it, what are some open questions that you would expect people to like kind of play around with? You know, what are people like going to do? Why should we probe Gemma, Llama, uh, what are interesting things we can do, and any tips on using it?\n\nYeah, I I think there's maybe like two to three like categories of things that people could do. So I'll go from sort like the most basic kind of low effort to, you know, hey, if you want to dedicate like a month of your life, you could do that. Um, the sort of like most basic thing is, you know, um, so it's a Gemma 2 and Llama 1B are like smaller models, uh, but they can still do a bunch of stuff, and so, and for most of the things that they can do, we still kind of like don't really know or have a good mental model of how it is that they do the things that they do. So to give you an example, like one of the things in the paper is this sort of like multihop reasoning where, you know, we ask, you know, like cloud work hiku, like, oh, the capital of the state where Dallas is is Austin. It turns out that like Gemma can do this also. And so as part of the release, we have, you know, a notebook where Michael Hannah, one of the Anthropic fellows, kind like walks through a bunch of examples, including this one. And it's really cool cuz you can see that actually the way the circuit looks in Gemmaim looks in like a huge model, which that in it in itself is is I think like a pretty novel discovery. It's like, oh, you have these models that are like super different. And you know, if you look at like their evals or if you just try to use them, they're like just very clearly different. But for this one task, for this one thing, actually, the way that this do that they do this multi-step reasoning is like the same way. They actually do the multi-step reasoning. In the notebook, there's both other examples of kind of like fun things that we looked at that I think can sort of spike your interest if you're new to thinking about this stuff. And at the end of the notebook that's that's linked in the in the readme, there's like three examples of like random sort of like cases that we haven't solved or or or we haven't labeled that you know have like a graph precomputed for you, and you could just look at it and try to like figure out what's happening. And by figure out what's happening, what we mean is, you know, we might do like a quick demo here, but it's kind of like look at these representations, try to understand like, okay, like what is the computation the model is doing, and then part of the release also lets you like run experiments to like verify that you're right. So if you think that like, you know, ah, the model like first like thinks about Texas in this case, you can also just like stop it from thinking about Texas and see if like that damages it. And so like the tools to do that are available. And so I would say that's like the first thing is just I think the hope is there are a lot of behaviors that models do way more than like any single group has time to explore. And so the hope is like, hey, pick a behavior you think is interesting and try to understand like what's happening and try to ground it out. And it's like the the like baseline thing and maybe like the the thing that I'm most excited about with this release. But then the other thing I do want to mention, like parts two and three are just we also hope that like other groups and and kind like interested researchers can just use this to like extend the method. Like if you have an idea about how to like do this better, you know, the whole code to like make this graph is is open source. you know, I can take a look at it and just like try to play with it, try to find different ways to like create these graphs and also extend it to other models, right? Like there are many different models, and so you know part of part of making this work on any model is you have to like train the sort of like replacement model, which again there there is code for, and there's other groups working on, and so like that's also something that if you're excited about, you could say like, okay, cool, well, I want this to work on like another open model, and you could sort of like add it if you're like, you know, more into sort of like maybe like the engineering and the ML engineering side of things.\n\nUh, yeah, we we actually get into a little bit about how you how you guys do the extra data viz stuff that makes your your blog post pop so much. Should we should we um share the screen a little bit and and dive in? I think uh you guys prep some examples.\n\nTotally. Yeah. Yeah. It's just like there's nothing better than the creator of the tool walking through the tool, and we might as well capture that so that you know people who actually want to do this can follow along.\n\nYeah. Uh that makes sense. Let me just like actually share my screen. my uh one little experiment I basically cloned the repo, threw it into cloud code, and was like, you know, deal with this, let's let's try it end to end. So would recommend, you know, cloud code is very good at using this. Um, basically also if you're if you're just trying to get started, the circuit tracing tutorial notebook, very good that kind of goes over all the high level, and then you know, shout out cloud code, try it out, it it works very well on this.\n\nThat that's awesome to hear actually. I might just open the notebook first, just like quickly walk through the illustrations. But yeah, you're the second person to tell me that they just had Cloud Code sort of like dig in initially on it. So I'm glad I'm glad that's working. The the tutorial here is like linked at the top of the repo, and maybe we can link it from the podcast, but essentially sort of like walks you through kind of like how to think about graphs, and so it links to these circuits. So here this is the two-step reasoning that we're talking about. This is kind of like a schematic of it where it's like the capital state of Dallas, and it's like ah, it has to think of Texas and Austin. But the notebook links you to all of these circuits here, and this is kind of the thing that you can play with. So this is the UI on Neuronpedia that that hostess, and that that lets you like create any circuit. So here, you know, we could explore the circuit, and if you open the notebook, you can explore it. I'm realizing that I switched tabs. Maybe I'm not showing it. Okay, there we go. Can you see the circuit now? I think so. Okay, cool. Um, but you can make a new graph super easily and quickly. And so maybe this is like the most fun things. One I was playing with uh right before uh you know, joining this call is like uh it turns out that podcast guests are very formulaic, and so if you say like thanks for having me on the whatever, like Gemma seems to pretty consistently guess that you're like on a podcast. Uh, which makes sense, right? Like why would you say thanks for having me on the blah. Uh, and so here we can try to say like, ah, okay, like how does Gemma know to like complete the sentence with thanks for having me on the latent space podcast, and so here the way you generate a graph, right, is you type a sentence where the next word is the thing that you're interested in, and then you kind of try to explain like how the model got to the next word. So here you can give it a name, and then you can mostly just not worry about any of these parameters, I think if you're just playing with it, and you can click start generation. And all this generates, like something important for people to know is that these are trained on base models, right? So they're not chat models. So basically when you train these models, they're just trained to predict next token, and they don't have that user assistant chatbot flow. So they're prompted in a way such that, you know, the output should basically just be the next word.\n\nYeah, you kind of want to think about it as like maybe the the the prompt or the text you're making is like the text of like a book or an article rather than a conversation where it's like, you know, what is a sentence where if you were to read it in a book, like the next word would be sort of like the interesting one. Um, yeah, you know, you can click on it. Uh, sort of like takes a little bit of time to load because there's just a bunch of data. So what we're going to show you here is basically like almost every single feature that activates in the model. The features are these intermediate representations, and at the bottom there's the prompt. So here it's like thanks for having me on the latent space, and at the top you can see like what the model sort of like uh output. So it's most likely output is it's pretty confident that we're talking about a podcast, and you know, it has like some random stop tokens, blog, show, and then some stuff that I think like makes less sense, but also like these are small models, and so sometimes they say random stuff. Um, and so like the way that you could then explore this and be like, okay, so the model says podcast, so like why does it say podcast? Uh, so you can click on this output and say like what are the features, again, these like intermediate representations that have an input to this. So it seems like there's features at here, this is the layer like layer 18 that already like are about podcast episodes. You can know this because the features have a label, but also if you want, you can look at the feature itself here, and here you can see that like this shows you like other text that the feature is active over, and it's just like text about podcasts. So that's like a way that you can also like understand what the features are. Um, and then you can keep going back. So it's like, oh, okay, so it said podcast here because of this podcast feature, where where did that come from? And it's like, oh, it comes from like words related to podcast, words associated with podcast, as well as like an interview feature. Um, and also just the word on. So there's like a bias, like if you're saying like blah blah blah on, that sort of like slightly increases the chance that you're talking about a podcast at all. Um, and you can sort of keep going back. um, and and kind of like explore the graph interactively. I would say that like the way to do it, and we talked about this on the like kind of like longer version of the podcast, but it's like, you know, kind of like chasing from the interesting outputs back or from the interesting input forward. There are many nodes on these. I like wouldn't recommend looking at all of them. You can also sort of like prune them a little more aggressively here if this is too busy and kind of look this shows you like only the most important ones, and you can sort of like be pretty extreme with it if you want, or you can show the whole thing and then be like super overwhelmed. Once once you kind of do this, you can then kind of like group your nodes into similar ones to kind of like make a graph. I actually made this little summary earlier, so I can just share that. So this is the exact same graph, but just before the before before hopping on, I kind of like did a few groups. So this is like same thing, podcast. It's like, oh, there's like a bunch of notes, like podcast episodes, thing like discussing podcast, there's a note about expressing gratitude that amplifies that you're like on an interview or a podcast. So like one fun experiment you could do here, right, is like, oh, like what happens if I mess with this? Like if if I don't like if I mess with the like, oh, this person is like grateful to be on and is like this person is on. Does it think you're on something else? Like maybe there are things that you know you could be on that you're not grateful for, like, oh, you're having me on trial or something, I don't know, like that could be one one interesting sort like experiment to see what the causal effect of this is, and again, you could sort of like label it more and explore it more, and this UI, the whole point is for it to be like snappy and quick, so you can just like generate a bunch of graphs pretty easily, right? Like maybe this wasn't exactly what you wanted, so you're like, I'm super unhappy to be on the latence space, and then you can see what it completes for that or\n\n\nWhatever, and you can sort of like just continuously play with it and get a better sense for your hypotheses. Often times, you kind of like want different prompts, you know, different examples that are similar to kind of get a sense for it. And then if you're really curious and you want to dig in more, that's when I would recommend going back to like the codebase and some of the notebooks.\n\nMaybe one thing, one last thing I'll say on that is that the notebooks themselves, they can all be run on Google Collab. And all of the code, as far as we can tell, we've like tested all the notebooks, just like runs on Collab. And so that means that like you don't need, on the free tier to be clear, like you don't need like an expensive GPU. You can just run this and kind of like run your interventions and play with it. And so in this notebook in particular, the intro one, we show you how to do these interventions. And here we're like, what happens if we turn this note off, and what happens if we turn that one off, and what happens if we turn this one off, and what happens if, you know, we inject one from one prompt into another one. And so I think that's the sort of like deeper dive trying to understand the mechanism better. But if you're just trying to even like get a sense at all, like how does a model do X, you can just generate a graph and and take a look at it.\n\nIncredible. Very cool. Is there, uh, when I look at the graph, is there a thought in my mind about maybe this is too easy, too perfect? Um, and one version of this is there's supposed to be superposition, and here there's no superposition, kind of, well, there is superposition, and like we're, we're sort of like, so maybe I can share, I can share the the graph again and like answer your question, which I think is like, what are we hiding here, where are the skeletons? Yeah, this is like, it's too, it's too clean. I'm like, yeah. So, okay. So, so maybe like a good example is, and we're going to like make this slightly less overwhelming here, is like, okay. So, so you look at this graph and you say like, yeah, like we don't actually understand how models work fully. So, like what are you hiding here? And and the thing that's like important to know here is, you know, I, I didn't say this explicitly, but like the layers are like arranged here. And so let's just look at like one layer. So for this layer, what we're saying is like the only thing that that is happening or that's like important enough is this one feature, which is just like one small direction in the model space, right? Like one, one dimension we've pulled out of superposition, uh, or or, um, let's say that for now. But then also there's these diamonds, and these diamonds are errors. We talked about them on the longer podcast, but they're just like when you train these replacement models to replace some of the model computation, you successfully replace some of it, and then some of it you fail to replace. And so this is like everything that we don't understand. And so that means that like sometimes if you look at an input like this guy's input, you'll see a bunch of errors here as the input. And so essentially, you know, there's some graphs and some examples where like if you have, if most of the stuff that you see is the these errors, basically that's that just means like, hey, for this prompt, we were not able to sort of like explain, you know, that part of the computation. And so at least that part is like a an explicit sort of like, we show it in your face where it's like, here's here's what we don't understand. And so, so you can sort of like see what we don't understand. There's also, I will say, like one more thing, there's like a bunch more stuff that can get you, uh, and that's like in the paper, but like one example here that I'll just say is like these are just MLPs. So the model has both attention heads and multi perceptions MLPs. We don't just do it like, we completely ignore attention, or like we don't, we don't try to decompose it at all. So there's some prompts where like all of the interesting stuff is attention, and here you're just not, you're just not seeing it at all. The way that it's materialized is like you have an edge from here to here, and like some attention head did a bunch of stuff. You don't know what it is. And so that's also the part that we're sort of like not explaining. So there's, there's definitely, yeah, I don't want to make the claim that we explain everything. I think the correct way to think about this is like, if you look at a prompt and you can, by tracing through these, not hit any errors, hit nodes that make sense and build up a reasonable hypothesis, and then when you test it with interventions, it works, you've at least understood some, and presumably like a reasonable proportion of the computation. If your interventions are working, that means it's like the thing you found is not just like a side thing. It's part of the main thing the model is doing. So, you know, then the question is like, how often does that happen versus how often you just hit these errors or you're like confused? And I think that's that's just sort of like what works and what doesn't.\n\nUh, summary here. Crazy. I mean, uh, congrats on this work. I know you, you're low on sleep cuz you worked really hard on, uh, shipping it, and you're a perfectionist. I, I just think like, yeah. Sorry. I'll just say that like the actual brunt of the work here is like the fellows. Yeah. Yeah. They, you know, I, I mostly just like coordinated things left and right, but but they sort of like did all the implementation as well as, you know, folks on the like Neuronpedia decode research side also did, you know, the the lines here of the work here to actually have the the front end UI. I would just say like, you know, uh, Vibhu and I were at the Goodfire meet up yesterday where there were a lot of uh, interpretability folks. I was shocked at um, honestly how young most interpretability people and work are, and this is a very young field, exactly like you say in the podcast, there's a lot of fresh green grass here to to tread, and it's just really inspiring. Do you have any other like final thoughts or comments?\n\nYeah. Yeah. No, I think there's just a lot of open work to be done, you know, and we talk about this in the podcast, too. And just to reiterate like how good the tooling that you guys put out is, like even the fact that, you know, without diving into any code, you can enter in a prompt and start to play through these circuits in like minutes. It's it's pretty incredible. Like I, I could share another one actually. So, I was doing this with MechInterp Pomsky, and I finally got it to work. So, our guest host of the episode is Mochi, my little dog. She's our distilled husky. So she's on the podcast later, and you know, I basically put in like, I, I had to guide it quite a bit, but my my prompt is a pomsky is a small dog that's a breed of a m of a husky and a and then, you know, I'm expecting it to put out Pomeranian or palm. Let me, let me share my screen real quick, and then we can kind of dig through this is me, like, by the way, her tagline, yeah, while you put it up, her tagline is officially Mochi, the interpretability husky. Today, for today, we're going to change our tagline every episode, but yeah, it feels a little weird. You, we're digging deep into what Mochi is, but basically this is me, like, no background, like 2 minutes in, just put in a phrase, and now I get to play around with features, right? So this is also called please with four S's, because you know, I tried a few pumps. It's okay. It's okay. We struggle. It only took a few minutes though. So, you know, Psky is a small dog breed that that's a mix of a husky and a and then the the most out probable outputs, you know, now it says palm. So, okay, let's dig into what some of these are. I'm basically just going like fresh. Haven't done this before, but you know, words related to animals, their emotions, their health. We have a feature for dog golden lab mentioned dog breeds, especially high maintenance. You know, this is basically like AGI. It knows pals skis are high maintenance. It's it's figured it out. But realistically, you know, um, as I dig through these features, I can start to pin them, uh, layer them through. Mentions of garbage and waste. No, that's not nice. That's not nice. But basically, you know, interesting. And this is already me pruning out most of the features as I open it up. You know, it talks about different things like dog breeding.\n\nWhat else related to animal welfare? So like, and then you can dig through all this. There's just so many things that like, you know, this is in a matter of minutes. I basically made a graph, put in a sentence, and now I have an output, and I can traverse through what are different things. Okay, animal science, right? So this breed is relatively new. It's not that common that big huskys and little Pomeranians naturally have offspring, but you know, let's let's like dig through animal science versions of this, and then we have like interesting little features. So, it's it's very easy for people to kind of get a different understanding of what goes on throughout layers in models, you know, but that's just my fun little experiment of getting it to work.\n\nOh, yeah. And and I think like, you know, one, one thing that I, I would do if you were curious, or maybe I'm just gonna try to bait some listeners into doing it, is like, you could be like, okay, like, let's try to like trace why it said Pomeranian here, and like maybe there's like some of it is about like, like dog breeds, and some of it is about like specific characteristics of a husky, and then you can ask the same question, but instead of husky, you like try some other dog breed, and then try to see if you can like, if you understood the circuit well, and if you identified where it's thinking about huskys or where it's thinking about like kind like breeding two different breeds, then you should be able to like swap these in and out and get it to kind say whatever you want. Um, and and if you didn't, then maybe there's something complicated going on. But but yeah, like very cool that you got this going on so quick.\n\nThat's that's that's the whole goal. That's super exciting. Yeah. And and like, you know, no disclosure, this was like 5 minutes of just playing around, and like there's there's stuff to learn there, right? Like, okay, what happens with dog breeding? What are traits of these dogs? And then, you know, the next step for me would basically be let's try clamping some of these features up or down. Let's let's do different breeds and see if it makes sense. Right? So if I have husky traits in a different mix, and then you know, can I, can I get out what's going on, but it also shows internally that there's more than just token completion of, you know, this plus this equals this. No, it has some under understanding of characteristics, right? Like this is a pretty stubborn dog, it it has a stubborn feature pretty high up that activates. So very, very cool stuff. I think it'll be cool when we apply this to more like serious topics. Like right now when it comes to evals, right? We have like, we have pretty straightforward evals, right? Like how good is does it do on math? Can it write code? Does stuff compile? But we don't have like vibes based heristic evals, right? So like, does it understand different queries? Should be concise? Should they be verbose? Can we kind of trace through how it gives responses to this stuff? And then like the other part is, you know, as we go past base models, how does this happen for different phases of models, right? So if I have a base Gemma and I have a chat model, what are differences in their attributions, right? What happens kind of in that diff of training, so that's that's kind of one of my little interests in in mechan, what happens as we do more training, what are we really changing?\n\nTotally, yeah, you can think about like comparing different models, and for me, different models, either like Gemma versus some other model, or like early Gemma versus late Gemma and pre-training or like fine-tuned versus not fine-tuned. I think there's also a sense in which like somebody yesterday was telling me like, &quot;Oh, it's fun. I've been playing with it on like the like sort of like weird riddles that the models get wrong.&quot; Like it, like you, you're not limited to studying what the model can do, right? Like if the model's failing at something, like you know, counting the number of letters in strawberry or whatever. Um, you could just try that and try to figure out the circuit for like, well, it's getting this wrong, like why? Like it, maybe you can see in its representation that it's like thinking about something obviously incorrect, right? Um, and so I think, I think that that's also like a fun thing to to to play with.\n\nI think that's it for our uh little intro chat and coverage of the open sourcing. Let's dive right into the episode next. But Emmanuel, uh, you're uh, amazing work, and I'm so inspired, and also just like, I think it this puts a human face on the the interpretability work. I think it's very important, and we'd love to keep doing this, whatever you got next coming up.\n\nWell, yeah, thanks for having me. Again, I should say cool to put a face on it, but definitely want to call out this is like a huge team of people with me. I'm just a talking head here. Um, and and paper paper lead, you know, you did the work, you know, take credit. I think that like, yeah, happy to talk about more inter things, and also like feel free to, you know, reach out to me. I'm like findable if you're listening to this podcast and you have like questions about stuff that's broken, or if this brings up like experiment ideas. Uh, definitely want more people playing with this. So, so yeah, thanks for having me. Hope hope that inspires some folks.\n\nAll right, we are back in the studio with uh a couple special guests. One, Vibhu, our guest co-host for a couple of times now, as well as Mochi, the distilled husky is in the studio with us who asked some very pressing questions. Uh as well as Emmanuel Amiesen. Y. Is that Dutch? Is that It's actually German. German. Yeah. Yeah. You are the lead author of a fair number of the recent uh MechInterp work from Anthropic that I've been basically calling transformer circuits because that's the name of the publication. Yeah. Well, to be clear, transformer circuits is the whole publication. I'm the author on\n\n\nOne of the recent papers, Circuit Tracing.\nYes. And people are very excited about that.\nThe other name for it is like tracing the thoughts of LMs. There's like three different names for this work. It's all MechInterp.\nThere's two papers. One is Circuit Tracing. It's the methods. One is like the biology, which is kind of what we found in the model, and then tracing the thoughts is confusingly just the name of the blog post where we announced it.\nIt's for different audiences, and I think though when you produce the like 2-minute polished video that you guys did, that's meant for like a very wide audience, you know. Yeah, that's right. There's sort of like very many levels of granularity at which you can go, and I think for MechInterp in particular, because it's kind of complicated, going you know from like top to bottom, most like high level to sort of the gritty details, works pretty well.\nYeah, cool. Um, we can get started. Basically, we have two paths that you can choose, like either your personal journey into MechInterp or the brief history of MechInterp, just generally, and maybe that might coincide a little bit. I think my okay, I could just give you my personal journey very quickly, because then we can just do the second path. My personal journey is that I was working at Anthropic for a while. I'd been like many people, just following MechInterp as sort of like an interesting field with fascinating, often beautiful papers. Uh, and I was at a time working on fine-tuning. So like actually fine-tuning uh production models for Anthropic, and eventually I got both like sort of like my fascination reached a sufficient level that I decided I wanted to work on it, and also I got more excited about just as our models got better and better, understanding how they worked. So that's the simple journey. I've got like a background in ML, kind of like did a lot of applied ML stuff before, and now I'm doing more research stuff. Yeah. You have a book with O'Reilly. You're head of AI at Inside Data Science. Anything else to plug?\nYeah. Uh, I actually I I want to like plug the paper and unplug the book.\nOkay. I think the book is good. I think that the advice stands the test of time, but it's very much like, hey, you're building like AI products. What should you focus on? It's like very different, I guess, is all I'll say from the stuff that we're talking to talk about today. Today is like research, some of some of the sort of like deepest, weirdest things about how like models work. And this book is, you want to ship a random forest to do fraud classification. Like here are the top five mistakes to avoid. Yeah. Um, the good old days of ML. I know it was simple back then. You also transitioned into research, and I think you also did that, managed to like I feel like there's this monolith of like people assume you need a PhD for research. Maybe can you give like that perspective of like how do people get into re, how did you get into research? Maybe that gives the audience an insight into Vibhu Sapra's background as well. Yeah, my background was in like economics, data science. I thought LLMs were pretty interesting. I started out with some basic ML stuff, and then I saw LLMs were starting to be a thing. So I just went out there and did it. And same thing with AI engineering, right? You just kind of build stuff, you work on interesting things, and like now it's more accessible than ever. Like back when I got into the field, 5, 6 years ago, like pre-training was still pretty new. GPT3 hadn't really launched. So it was still very early days, and it was a lot less competitive. But yeah, without any specific background, no PhD, there just weren't as many people working on it.\nYou made the transition a little bit more recently, right? So, what's your experience been like? Yeah, I think I think it has maybe never been easier in some ways, because a lot of the field is like pretty empirical right now. So I think the bitter lesson is like this lesson that you know, you can just sort of like a lot of times scale up compute and data and get better results than like thinking than if you sort of like thought extremely hard about a really good like prior inspired by the human brain to to train your model better. And so in terms of definitely like research for pre-training and fine-tuning, I think it's just sort of like a lot of the bottlenecks are extremely good engineering and systems engineering, and a lot even of the research execution is just about sort of like engineering and scaling up and things like that. I think for Interp in particular, there's like another thing that makes it easier to transition to, which is maybe two things. One, you can just do it without huge access to compute. Like there are open source models, you can look at them. A lot of Interp papers, you know, coming out of programs like Maths are on models that are open source that you can sort of like dissect without having a cluster of like, you know, 100 GPUs. You can just even sometimes you can load them like on your CPU on your MacBook, and it's also a relatively new field, and so you know, there's, as I'm sure we'll talk about, there's like some conceptual burdens and concepts that you just want to like understand before you contribute, but it's not, you know, physics. It's relatively recent, and so the number of abstractions that you have to like ramp up on is just not that high compared to other fields, which I think makes that transition somewhat easier for Interp. If you understand, we'll talk about all these, I'm sure, but like what features are and what dictionary learning is, you're like a long part of the way there.\nI think it's also interesting just on a career point of view. Research seems a lot more valuable than engineering, so I\nAnd you don't have to answer this if it's like a tricky thing, but like how hard is it for a for a research engineer in Anthropic to jump the wall into research? People seem to move around a lot. And I'm like there, that cannot be so easy. Like in no other industry that I know of, people you can do that. Do you know what I mean? Yeah. I think I'd actually like I'd push back on the sort of like research being more valuable than engineering a little bit, because I think a lot of times like having the research idea is not the hardest part. Don't get me wrong, there's some ideas that are like brilliant and hard to find. But what's what's hard, certainly on fine-tuning and to a certain extent on Interp, is executing on your research idea in terms of like making an experiment successfully, like having your experiment run, interpreting it correctly. What that means though is that like they're not separate skill sets. So like if you have a cool idea, there's kind of not many people in the world I think where they can just like have a cool idea and then they have a, you know, like a little minion they'll deputize being like here's my idea, you know, go off for 3 months and like run this whole like build this model and train it for, you know, hundreds of hours and then report back on what happened. A lot of the time, like the people that are the most productive, they have an idea, but they're also extremely quick at checking their idea, finding sort of like the shortest path to to checking their idea. And a lot of like that shortest path is engineering skills essentially, is just like getting stuff done. And so I think that's why you see sort of like people move around is like proportionate to to to your interest. If you're just able to quickly execute on the ideas you have and get results, then that's really the 90% of the value. And so you see a lot of transferable skills actually, I think from from people like I've certainly seen at Anthropic that are just like really good at that inner loop. They can apply it in one team and then move to a completely different domain and apply that inner loop just as well. Yeah. Very cracked, as the kids say. Uh, shall we move to the history of MechInterp? Yeah.\nAll I know is that everyone starts at Chris Olah's blog. Is that right? I Yeah, I think that's the correct answer. Chris Olah's blog, and then you know, distill.pub uh is the sort of natural next step. And then I would say, you know, now there's philanthropic, there's transformer circuits, which you talked about, and but there's also just a lot of MechInterp research out there from, you know, I think like the yeah, like Matts is a group that like regularly has a lot of research, but there's just like many different labs that that put research out there, and I think that's also just like hammer home the point that's because all you need is like a model and then a willingness to kind of investigate it to be able to contribute to it. So, so now it's sort of like there's been a bit of a Cambrian explosion of MechInterp, which is cool. I guess the history of it is just computational like models that are not decision trees, uh, models that are either CNNs or let's say transformers have just this really like strange property that they don't give you interpretable intermediate states by default. You know, again, to go back to if you were training like a a decision tree on like fraud data for an old school like bank or something, then you can just look at your decision tree and be like, \"Oh, it's learned that like if you make uh, I don't know, if the transaction is more than $10,000 and it's for like perfume, then maybe it's fraud or something.\" Uh, you can look at it and say, \"Cool, like that makes sense. I'm willing to ship that model.\" But for for things like like CNNs and like Transformers, we we don't have that, right? What we have at the end of training is just a massive amount of weights that are connected somehow uh, or activations are connected by some weights, and who knows what these weights mean or what the intermediate activations mean, and so the quest is to understand that. Initially it was done a lot of it was done on vision models, where you sort of have the emergence of a lot of these ideas, like what are features, what are circuits, and then more recently it's been mostly or not most, yeah, mostly applied to NLP models, but also you know, still there's work in vision and there's work in like uh, bio and other domains. Yeah, I'm on Chris Olah's blog, and he has like the feature visualization stuff. I think for me the clearest was like the vision work where you could have like this layer detects edges, this layer detects textures, whatever, that seemed very clear to me, but the transition to language models seemed like a big leap. I think one one of the bigger changes from vision to to like language models has to do with uh, the superposition hypothesis, which maybe is like that's the first and like point models post, right? Exactly. And this is sort of like it turns out that if you look at just the neurons of a lot of vision models, you can see neurons that are curve detectors or that are edge detectors or that are high low frequency detectors, and so you can sort of like make sense of the neurons mostly. But if you look at neurons in language models, most of them don't make sense. It's kind of like unclear why, or it was unclear why that would be. And one main like hypothesis here is the superposition hypothesis. So what does that mean? That means that like language models pack a lot more in less space than vision models. So maybe like a a kind of like really handwavy analogy, right? Is like, well, if you want curve detectors, like you don't need that many curve detectors. You know, if each each curve detector is going to detect like a a quarter or 12th of a circle, like okay, well, you have your all your curve detectors, but think about all of the concepts that like Claude or even GPT2 needs to to know, like just in terms of it needs to know about like all of the different colors, all the different hours of every day, all of the different cities in the world, all of the different streets on every city. If you just enumerate all of the facts that like a model knows, you're going to get like a very, very long list, and that list is going to be way bigger than like the number of neurons or even like size of the residual stream, which is where like the models process information. And so there's this sense in which like, oh, there's more information than there's like dimensions to represent it. And that is much more true for language models than for vision models. And so because of that, when you look at a part of it, it just seems like it's like there, it's got all this stuff crammed into it. Whereas if you look at the vision models, often times you could just like be like, cool, this is a curve detector. Yeah, Vibhu, you have like some fun ways of explaining the toy models or superposition concept. Yeah, I mean, basically like, you know, if you have two neurons and they can represent five features, like a lot of the early MechInterp work says that, you know, there are more features than we have neurons, right? So I guess my kind of question on this is, for those interested in getting into the field, what are like the key terms that they should know? What are like the few pieces that they should follow, right? Like from the Anthropic side, we had a toy transformer model, we had sparse, we first had autoencoders, that was the second paper, um, right, monos semanticity, what is sparsity and autoencoders, what are transcoders, like what is linear probing, what are these kind of like key points that we had in MechInterp, and just how would people get a quick, you know, zero to like 80% of the field?\nOkay, so 0 to 80%. And now I realized I really like set myself up for for failure, cuz I was like, yeah, it's easy. There's not that much to know. So, okay, so then then we should be able to cover it all. Um, so superposition is the first thing you should know, right? This idea that like there's a bunch of stuff crowned in few dimensions, as you said, maybe you have like two neurons and you want to represent five things. So if that's true, and if you want to understand how the model represents, you know, I don't know, the concept of red, let's say, then you need some way to like find out essentially in which direction the model stores it. So after the the sort of like superposition hypothesis, you can think of like, ah, we also think that like basically the model represents these like individual concepts, we're going to call them features, as like directions. So if you have two neurons, you can think of it as like it's like the 2D plane, and it's like a you can have like five directions, and maybe you would like arrange them like the spokes of a wheel. So they're sort of like maximally separate. It could mean that like you have one concept this way and one concept that's like not fully perpendicular to it, but like pretty, pretty like far from it. And then that would like allow the model to represent more concepts and it has dimensions. And so if that's true, then\n\n\nWhat you want is you want like a model that can extract these independent concepts, and ideally, you want to do this like automatically. Like, can we just, you know, have a model that tells us, like, \"Oh, like this direction is red. If you go that way, actually, it's like, I don't know, chicken. And if you go that way, it's like the declaration of independence,\" you know? Um, and so that's what sparse autoencoders are. It's almost like the self-supervised learning insight version, like in pre-training, you had self-supervised learning. And here is so self-supervised interpretability. Yeah. Exactly. Exactly. It's like an unsupervised method, and so unsupervised methods often still have like labels in the end. So, so sometimes I feel like the term labels masking. Yeah. Like for pre-training, right? It's like the next token. So in that sense, you have a supervision signal. And here the supervision signal is simply you take the like neurons and then you learn a model that's going to like expand them into like the actual number of concepts that you think there are in the model. So you have two neurons, you think there's five concepts. So you expand it to like, I think of dimension five, and then you contract it back to what it was. That's like the model you're training, and then you're training it to incentivize it to be sparse so that there's only like a few features active at a time. And once you do that, if it works, you have this sort of like nice dictionary, which you can think as like a way to decode the activate the neurons where you're saying, \"Ah, cool. I don't know what this what this direction means, but I've like used my model and it's telling me that the model is writing in the red direction.\" And so that's that's sort of like I think maybe the biggest thing to understand is this combination of things of like, \"Ah, we have too few dimensions. We pack a lot into it. So, we're going to learn an unsupervised way to like unpack it and then analyze what each of those dimensions that we've unpacked are.\" Any follow-ups? Yeah, I mean, the follow-ups of this are also kind of like some of the work that you did is in clamping, right? What is the applicable side of MechInterp, right? So, we saw that you guys have like great visualizations. Golden Gate Claude was a cool example. I was going to say that. Yeah, that's my favorite. What can we do once we find these features? Finding features is cool, but what can we do about it? Yeah, I think there's kind of like two big aspects of this. Like one is, yeah, okay, so we go from a state where, as I said, the model is like a mess of weights. We have no idea what's going on to, okay, we found features. We found a feature for red, a feature for Golden Gate Cloud, for the Golden Gate Bridge, I should say. Like, what do we do with them? And, well, if these are true features, that means that like they in some sense are important for the model, or it wouldn't be like representing it. Like, if the model is like bothering to like write, you know, in the Golden Gate Bridge direction, it's usually because it's going to like talk about the Golden Gate Bridge. And so that means that like if that's true, then you can like set that feature to zero or artificially set to 100, and you'll change model behavior. Uh, that's what we did when we did Golden Gate Cloud, in which we found a feature that represents a direction for the Golden Gate Bridge, and then we just like set it to always be on, and then you could talk to Claude and be like, \"Hey, like Claude, what's on your mind?\" You know, like, what are you thinking about today? Be like, the Golden Gate Bridge. She'd be like, \"Hey, Claude, like what's 2 plus two?\" It'd be like four Golden Gate bridges, uh, etc. Right. And it was always thinking about the write a poem, and it starts talking about how it's like read like the Golden Gate Plate Bridge. Yeah. Amazing. I think what made it even better is like we realized later on that it wasn't really like a Golden Gate Bridge feature. It was like being in awe at the beauty of the majestic Golden Gate Bridge, right? So on top of it would like really ham it up. You'd be like, \"Oh, I'm just thinking about the beautiful international orange color of the Golden Gate Bridge.\" That was just like an example that I think was like really striking, but of of sort of like, \"Oh, if you found like a space where that represents some computation or some representation of the model, that means that you can like artificially suppress or promote it, and that means that like you're starting to understand at a very high level, a very gross level, like how some of the model works, right? We've gone from like, I don't know anything about it to like, oh, I know that this like combination of neurons is this, and I'm going to prove it to you.\" The next step, which is what this works on, is like that's kind of like thinking of if maybe you take the an analogy of like, um, I don't know, like, like let's take the analogy of like an MRI or something, like a brain scan. It tells you like, \"Oh, like this,\" as Claude was answering at some point, it thought about this thing, but it's a sort of like vague, like basically maybe it's like a like a bag of words, kind of like a bag of features. You like, \"Here are all the random things it thought about,\" but what you might want to know is like, okay, but Claude is doing some processing, like sometimes to get to the Golden Gate Bridge, it had to realize that you were talking about San Francisco and about like the best way to go to Sonoma or something, and so that's how it got to Golden Gate Bridge. There's like an algorithm that leads to it at some point thinking about the gong gate bridge, and basically like there's like a way to connect features to say like, \"Oh, from this input went to these few features and these few features and these few features, and that one influenced this one, and then you got to the output.\" And so that's the second part, and the part we worked on is like you have the features now, connect them in what we call, uh, or what's called circuits, which is sort of like explaining the the like algorithm. Yeah. Before we move directly on to your work, I just want to give a shout out to Neil Nanda. He did Neuronpedia and released a bunch of essays for, I think, the llama models and the Gemma models. And the Gemma models. Yeah. Uh, so I actually made Golden Gate Gemma. Just up the weights for proper nouns and names of places of people and references to the term golden, likely relating to awards, honors, or special names. And that together made Golden Gate. That's amazing. Yeah. So you can make Golden Gate Gemma, and like I think that's that's a fun way to experiment with this. Uh, but yeah, we can move on to I'm curious. I'm curious what's the background behind why you shipped Golden Gate Claw. Like you had so many features. Just any fun story behind why that's the one that made it. You know, it's funny. If you look at the paper, there's just a bunch of like Yeah. Like really interesting features, right? There's like one of my favorite ones was the psychopantic praise, which I guess is very topical right now. Very topical. Um, but you know, it's like you could dial that up and like Claude would just really praise you. He'd be like, \"Oh, you know, like I wrote this poem, like roses are red, violets are blue, whatever.\" And he'd be like, \"That's the best poem I've ever seen.\" Um, and so we could have shipped that. That could have been funny. Uh, Golden Eye Claude was like a pure, as far as I remember at least, like a pure just like weird random thing where like somebody found it initially, we had an internal demo of it. Everybody thought it was hilarious, and then that's sort of how it came on. There was no nobody had a list of top 10 features we should consider shipping, and we picked that one. It was just kind of like a very organic moment. No, like the the marketing team really leaned into it. Like they mailed out pieces of the Golden Gate for people in Europe, I think, or ICML. Yeah, it was it was fantastic marketing. Yeah. The question obviously is like if Anthropic had invested more interpretability, would they have caught uh the GPT-4o update? Uh, but we don't know that for sure because they have interp teams. They just Yeah. I think also like for that one, I don't know that you need interp. Like it was pretty clearcut to the model. I was like, \"A, that model's really gassing me up.\" And then the other thing is, um, can you just like up write good code, don't write bad code, and make s, and like it feels too too easy, too free. Is that steering that powerful that you can just like up and down features with no trade-offs? There was like a phase where people were basically saying, you know, 3.5 and 3.7 are just now because they came out right after, and for the record, like that's been debunked. It has been debunked. But, you know, it had people convinced that what people did is they basically just steered up and steered down features, and now we have a better model. And this kind of goes back to that original question of, right, like why do we do this? What can we do? Some people are like, \"I want Circuit Tracing from a sense of, you know, legality, like what did the model think when it came to this output?\" Some people want to turn hallucination down. Some people want to turn coding up. So like what are some like, whether it's internal, what are you exploring that like what are the applications of this? Whether it's open-ended of what people can do about this or just like yeah, why why do MechInterp, you know? Yeah, there's like a few things here. So, so like first of all, obviously this is, I would say, on the scale of the most short-term to the most long-term, like pretty long-term research. So in terms of like applications compared to, you know, like the research work we do on like fine-tuning or whatever, interp is much more, you know, sort of like a a high-risk, high-reward kind of approach. Uh, with that being said, like I think there's just a fundamental sense in which Michael Nielsen had a had a post recently about how like knowledge is dual use or something, but just like just like knowing how the model works at all feels useful. And you know, it's hard to argue that if we know how the model works and understand all of the components that won't help us like make models that hallucinate less, for example, or that are like less biased. Seems, you know, if if like at the limit, yeah, that totally seems like something you could do using basically like your understanding of the model to improve it. I think for now, as we can talk about a little bit with like uh circuits, there's like we're still pretty early on in the game, right? And so right now the main way that we're using interpoly is like to investigate specific behaviors and understand them and gain a sense for uh what's causing them. So like one example that we we can talk about later, we can talk about now, but in the paper we investigate jailbreaks, and we try to see like why does a jailbreak work, and then we realize as we're looking at this jailbreak that part of the reason why Claude is telling you how to make a bomb in this case is that it's like already started to tell you how to make a bomb, and it would really love to stop telling you how to make a bomb, but it has to first finish its sentence. Like it really wants to make correct grammatical sentences. And so it turns out that like seeing that circuit, we were like, \"Ah, then does that mean if I prevent it from finishing its sentence, the jailbreak works even better?\" And sure enough, it does. And so I think like the level of sort of practical application right now is of that shape. So like understanding either like quirks of a current model or like how it does tasks that maybe we don't we don't even know how it does it, like you know, we we have like some planning examples where we had no idea it was planning, and we're like, \"Oh, god, it is.\" That's sort of like the current state we're at. I'm curious internally how this kind of feeds back into like the research, the architecture, the pre-training teams, the post- training, like is there a good feedback loop there? Like right now there's a lot of external people interested, right? Like we'll train an SAPE on one layer of llama and probe around, but then people are like, okay, how does this have much impact? People like clamping, but yeah, as you said, you know, once you start to understand these models, have this early planning and stuff, how does this kind of feedback? I don't know that there's there's like much to say here other than like I think we're definitely interested in conversely like making models for which it's like easier to interpret them. So that's also something that you can imagine sort of like working on, which is like making models where you have to work less hard to try to understand what they're doing architecture.\nOkay. Yeah. Yeah. So I think there was a there's a less wrong post about this of like there's a nonzero amount of sacrifice you should make in current capabilities in order to actually make them more interpretable because otherwise they will never catch up. You know, there's this sort of sense in which like right now we take the model and then the model's a model, and then we post hawk do these replacement layers to try to understand it. But of course, when we do that, we don't like fully capture everything that's happening inside the model. We're capturing like a subset. And so maybe some of it is like you could train a model that's sort of like easier to interpret negatively. And it's possible that like you don't even have that much of, you know, like a tax in that sense, and you can just sort of like either like train your model differently or do like a little post hawk step to like sort of like untangle some of the mess that you've made when you trained your model, right? Make it easier to interpret. Um, the hope was pruning would do some of that, but I feel like that aerial research has just died. What kind of pruning are you thinking of here? Uh, just pruning your network. Ah, yeah. Printing layers, printing connections, whatever. Yeah. I feel like maybe this is something where like superposition makes me less hopeful or something, cuz I'm like, you don't know like that that like seventh bit might hold something. Well, right. And it's like on on each example, maybe this neuron is like at the bottom of like what matters, but actually it's participating like 5% to like understanding English, like\n\n\nDoing integrals and, you know, like, whatever, like cracking codes or something, and it's like, because that is just like distributed over it, you, you kind of like, when you naively prune, you might miss that.\n\nI don't know. Okay. So then this area of research in terms of creating models that are easier to interpret from the start, is there a name for this field of research?\n\nI don't think so. And I think this is like very early, and it's, it's mostly like a case. There's a thing people want to double-click on. Yeah. Yeah. I haven't come across it. I think the higher level is like Dario recently put out a post about this, right? Why MechInterp is so important. You know, we don't want to fall behind. We want to be able to interpret models and understand what's going on, even though capabilities are getting so good. It kind of ties into this topic, right? Like, we want models to be slightly easier to interpret so we don't fall behind so far.\n\nWell, yeah. And I think here, like, just to talk about the elephant in the room or something, like, like one big concern here is, is like safety, right? And so, like, as models get better, they are going to be used more and more places. You know, it's like, you're now going to have your, you know, we're vibe coding right now. Maybe at some point, well, that, that'll just be coding. It's like, cloud's going to write your code for you, and that's it. And cloud's going to review the code that cloud wrote, and then cloud's going to deploy to production. Um, and at, at some point, like, as these models get integrated deeper and deeper into more and more workflows, it gets just scarier and scarier to know nothing about them. And so you kind of want your ability to understand the model to scale with, like, how good the model is doing, which that itself kind of like tends to scale with, like, how widely deployed it is. So as we, like, deploy them everywhere, we want to, like, understand them better.\n\nThe version that I liked from the old super alignment team was weak to strong generalization or weak to strong alignment, which that's what super alignment to me was, and that was my first aha moment of, like, oh yeah, some at some point these things will be smarter than us, and in many ways they already are smarter than us, and we rely on them more and more. We need to figure out how to control them, and this is not a, like, a Ilizer Yakowski like, ah, thing. It's just more like, we don't know what, how these things work. Like, how can we use them?\n\nYeah, and like, you can think of it. There's many ways to solve a problem, and some of them, if the model is solving it in, like, a dumb way or in, like, memorized one approach to do it, then you shouldn't deploy it to do, like, a general thing. Like it, like, you could look at how it does math, and based on your understanding of how it does math, you're like, okay, I feel comfortable using this as a calculator, or, like, no, it should always use a calculator tool because it's doing math in a stupid way, and extend that to any behavior, right? Where it's just a matter of, like, think about it, if, if, like, you're like in the 1500s and I give you a car or thing, and I'm just like, cool. Like, this thing, when you press on this, like, it accelerates, when you press on that, like, it stops. You know, this steering wheel seems to be doing stuff, but you knew nothing about it. I don't know if it was like a super faulty car, and it's like, oh yeah, but if you ever get went above 60 mph, like, it explodes or something, like, you probably would be sort of like, you'd want to understand the nature of the object before, like, jumping in in it. And so that's why we, like, understand how cars work very well because we make them. LLMs are sort of like, and ML models in general are like this very rare artifact where we, like, make them, but we don't, we have no idea how they work. We evolve them. We create conditions for them to evolve, and then they evolve, and we're like, cool, like, you know, maybe you got a good run, maybe we didn't. Yeah. Don't really know. Yeah. The extent to which you know how it works is you have your, like, eval, and you're like, oh, well, seems to be doing well on this eval. And then you're like, is it because this wasn't a training set, or is it like actually generalizing? I don't know. Uh, my favorite example was somehow C4, the, the common, the colossal clean corpus, did much better than, uh, common crawl, even though it filtered out most of this, like, it was very prudish. So it, like, filters out anything that could be considered obscene, including the word gay. But, like, somehow it just, like, when you add it into the data mix, it just does super well. And it's just like this magic incantation of, like, this, this recipe works. Just trust us. Like, we've tried everything. This one works. So just go with it. Yeah. It's not very satisfying.\n\nNo, it's not. The side that you're talking about, which is like, okay, like, how do you make these? And it's kind of unsatisfying that you just kind of make the soup and you're like, oh, well, you know, my grandpa made the soup with these ingredients. I don't know why, but I just make the soup the way my grandpa said. And then, like, some one day somebody added, you know, cilantro, and since then we've been adding cilantro for generations, and you're like, this is kind of crazy. That's exactly how we train models, though. Yeah. Yeah. Um, so I think there's, there's, like, a part where it's like, okay, like, let's try to unpack what's happening, you know, like, the mechanisms of learning, like, how, how our models learn. Like, one of them, I guess, I guess we skipped over it, but, like, one of the inter things were like induction heads, you know, like, understanding what induction heads are, which are attention heads that allow you to look at in your context the last time that something was mentioned and then repeat it, is like something that happens that seems to happen in every model, and it's like, oh, okay, that makes sense. That's how the model, like, is able to, like, repeat text without dedicating too much capacity to it.\n\nLet's get it on screen so people can see the visuals of the work you guys put out is amazing. Highly high. We should talk a little bit about the back behind the scenes of that MechInterp, but, but let's, let's, let's finish this off first. Totally. Uh, but just really quickly, I don't think we should spend too long on it. I think it's just like, if you're interested in MechInterp, we talked about superposition, and I think we skipped over induction heads, and that's like, you know, kind of like a really neat, basically, pattern that emerges in many, many transformers, where essentially they just learn, like, one of the things that you need to do to, like, predict text well is that if there's repeated text, at some point somebody said Emmanuel Amiesen, and then you're like on the next line and they say Emanuel, very good chance it's the same last name. And so one of the first things that models learn is just like, okay, I'm just going to, like, look at what was said before, and I'm going to say the same thing. And that's induction heads, which is like a pair of attention heads that just basically look at the last time something was said, look at what happened after, move that over. And that's an example of a mechanism where it's like, cool, now we understand that pretty well. There's been a lot of follow-up research on understanding better, like, okay, like, in which context do they turn on? Like, you know, there's like different, like, levels of abstraction. There's like induction heads that, like, literally copy the word, and there's some that copy, like, the sentiment and other aspects. But I think it's just like an example of slowly unpacking, you know, or like peeling back the layers of the onion of, like, what's going on inside this model. Okay, this is a component. It's doing this. So induction headers was like the first major finding. It was a big finding for NLP models for sure.\n\nI often think about the edit models. So Claude has a fast edit mode. Uh, I forget what it's called. Open has one as well. And you need very good copying every area that needs copying, and then you need it to switch out of copy mode when you need to start generating, right? And that is basically the productionized version of this.\n\nYeah. Yeah. Yeah. And it turns out that, you know, you, you need to select a model that's like smart enough to know when it needs to get out of coffee mode, right? Which is like, it's fascinating. It, it's faster, it's cheaper. You know, as bullish as I am on canvas, basically every AI product needs to iterate on a central artifact, and like, if it's code, if it's a piece of writing, it doesn't really matter, but you need that copy capability that's smart enough to know when to turn it off. That's why it's cool that induction heads are at different levels of abstraction. Like, sometimes you need to editing some code, you need to copy, like, the general structure. It's like, oh, like, the last, like, this other function that's similar. It first takes, like, you know, I don't know, like, abstract class, and then it takes, like, an int. So, I need to, like, copy the general idea, but it's going to be a different abstract class and a different int or something. Cool. Um, yeah. So, Circuit Tracing. Oh, yeah. Should we jump to Circuit Tracing? Sure. I don't know if there's anything else you want to cover. No, we got space for it. Maybe. Okay. I'll do, like, a really quick TLDDR of these two recent papers. Okay. Uh, insanely quick. So we talked about these features that we detect, and what we said is, like, okay, but we'd like to connect the features to understand, like, the inputs to every features and the output to every features, and basically draw a graph, and this is, like, if I'm still sharing my screen, uh, the thing on the right here, where, like, that's the dream. We want, like, for a given prompt, what were all of the things, like, all of the important things happen in the model, and here it's like, okay, it took in these four tokens, those activated these features, these features activate these other features, and then these features activate the other features, and then all of these, like, promoted the output, and that's the story. And basically, we're, we're like, the work is to sort of use dictionary running and these replacement models to provide a explanation of, like, sets of features that explain behavior. So this is super abstract. So I think immediately, maybe we can, like, just look at one example. I can show you one, which is this one, the reasoning one. Yep. Yeah. Two-step reasoning. I think this is already, this is like the introduction example, but it's already like kind of fun. So, so the question is, you ask the model something that requires it to take a step of reasoning in its head. So, you say, you know, fact, the capital of the state containing Dallas is. So, to answer that, you need one intermediate step, right? You need to say, wait, where's Dallas? Isn't Texas. Okay, cool. Capital Texas, Austin. And this is like in one token, right? It's going to after is, it's going to say Austin. And so, like, in that one forward pass, the model needs to extract to realize that you're asking it for, like, the capital of a state, to, like, look up the state for Dallas, which is Texas, and then to say Austin. And sure enough, this is like what we see is we see, like, in this forward pass, there's a rich sort of, like, inner set of representations where there's, like, it gets capital state in Dallas, and then boom, it has an inner, uh, representation for Texas, and then that plus capital leads it to, like, say Austin. I guess one of the things here is, like, we can see this internal, like, thinking step, right? But a lot of what people say is, like, is this just memorized fact, right? Like, I'm sure a lot of the pre-training that this model is trained on is this sentence shows up pretty often, right? So this shows that no, actually, internally throughout, we do see that there is this middle step, right? It's not just memorized. You can prove that it generalized.\n\nYeah, so, so, so that's exactly right. And I think, like, you, you, you hit the the nail on the head, which is like, this is what this example is about. It's like, ah, if this was just memorized, you wouldn't need to have an intermediate step at all. You'd just be like, I've seen the sentence. Like, I know what comes next, right? But here, there is an intermediate step. And so, you could say, like, okay, well, maybe it just has the step, but it's memorized it anyways. And then the way to, like, verify that is kind of like what, what we do later in the paper, and for all of our examples, is like, okay, we claim that this is like the Texas representation. Let's get another one and replace it. And we just change, like, that, uh, feature in the middle of the model, and we change it to, like, California. And if you change it to California, sure enough, it says Sacramento. And so it's like, this is not just a, like, byproduct, like, it's memorized something and on the side it's thinking about Texas. It's like, no, no, no, this is like a step in the reasoning. If you change that intermediate step, it changes the answer. Very, very cool work. Underappreciated.\n\nOkay. Sure. I have never really doubted. I think there's a lot of people that are always criticizing LLMs as stoastic parrots. This pretty much disproves it already. Like, we can move on. Yeah. I mean, I, I, I think I think there's a lot of examples that I will say we can go through, like, a few of them, like, show an amount of depth in the intermediate states of the model that makes you think, like, oh gosh, like, it's doing a lot. I think maybe, like, the poems. Well, definitely the poems, but even for this one, I'm going to, like, scroll in this very short paper to, like, uh, medical diagnosis. I don't even know the word count, cuz there's so many, like, embedded things in there. Yeah, we, it's too dangerous. We can't look it up. It overflows. Um, it's so beautiful. Look at this. Uh, this is like a medical example that I think shows you, again, this is in one forward pass. The model is, like, given a bunch of symptoms, and then it's asked not like, hey, what is, what is the, like, disease that this person has. It's asked, like, if you could run one more test to determine it, what would it be? So it's even harder, right? Means, like, you need to take all the symptoms, then you need to, like, have a few hypotheses about what the disease could be, and then based on your hypothesis, say, like, well, the thing that would, like, be the right test to do is X. And here you can see these three layers, right? Where it's like, again, in one forward pass, it has a bunch of, like, oh, these are symptoms, then it has the most likely diagnosis here, then, like, an alternate one, and then based on the\n\n\nDiagnosis, it like gives you basically a bunch of things that you could ask. And again, we do the same experiments where you can, like, kill this feature here, like suppress it. And then it asks you a question about the second, the sort like second option it had.\n\nUm, the reason I show it is, like, man, that's like a lot of stuff going on for one forward pass, right? It's like, specifically, if you if you expected it to, like, \"Oh, what it's going to do is it's just like seen similar cases in the training,\" is going to, like, kind of like vibe and be like, \"Oh, I guess like there's that word,\" and it's going to say something that's related to, like, I don't know, headache, you know, like kind of like really, it's like, \"No, no, no.\" It's like activating many different distributed representations, like combining them and sort of like doing something pretty complicated. And so, yeah, I think I think it's funny because in my opinion, that's like, yeah, like, \"Oh, God, stochastic parrots\" is not something that I think is is like appropriate here. And I think there's just like a lot of different things going on, and there's like pretty complex behavior.\n\nAt the same time, I think it's in the eye of the beholder. I think, like, I've talked to folks that have, like, read this paper and have been like, \"Oh yeah, this is just like a bunch of kind of like heuristics that are like mashed together, right? Like the model is just doing like a bunch of kind of like, 'Oh, if high blood pressure, then this or that.'\" And so I think there's there's sort of like um an underlying question that's interesting, which is like, \"Okay, now we know a little bit of how it works. This is how it works. Like, now you tell me if you think that's like impressive, if you think that like if you trust it, if you think that's sort of like uh something that is that is sufficient to like ask it for medical questions or whatever.\"\n\nI think it's a way to adversarially improve the model quality because once you can do this, you can reverse engineer what will be a a sequence of words that to a human makes no sense or let you arrive at the complete opposite conclusion, but the model still gets tripped up by. Yeah. And then you can just improve it from there. Exactly. And and this gives you a hypothesis about, like, you like specifically imagine if, like, one of those was actually like the wrong symptom or something, you'd be like, \"Oh, it's weird that the liver uh condition, like, you know, outweighs this other example. That doesn't make sense. Okay, let's like fix that in particular.\" Exactly. You sort of have like um a bit of of insight into like how the model is getting to its conclusion. And so you can see both, like, is it making errors, but also is it using the kind of reasoning that will lead it to errors?\n\nThere's a thesis, I mean, now it's very prominent with the reasoning models about model depth. Uh, so like you're doing all this in one pass. Yeah. But maybe you don't need to because you can do more passes. Sure. Uh, and so uh people want shallow models for speed, but you need model depth for for this kind of thinking. Yeah. Is there a Pareto frontier? Is there is there a direct trade-off? Yeah. What would you prefer if you had to make a model and like, you know, shallow versus deep?\n\nThere's a chain of thought faithfulness example. Uh, before I show it, I'm just going to go back to the top here. So when the model is sampling many tokens, if you want that to be your model, you need to be able to trust every token it samples. So like the problem with with models being autogressive is that like if they like at some point sample a mistake, then they kind of keep going conditioned on that mistake, right? And so sometimes like you need back backspace tokens or whatever. Yeah. Yeah. Yeah. And error correction is like notably hard, right? If you have like a deeper model, maybe you have like fewer coot steps, but like your your steps are more likely to be like robust or or correct or something. And so I think that that's one way to look at the trade-off.\n\nTo be clear, I don't have an answer. I don't know if I want a wider or a shallow or a deep model that like you definitely want shallow for inference speed. Sure. Sure. Sure. But you're trading that off for for something else, right? Because you also want like a 1B model for infant speed, but that also comes at a cost, right? It's it's less smart.\n\nThere's a cool quick paper to plug that we just covered on the paper club. It's a survey paper around when to use reasoning models versus dense models. What's the trade-off? I think it's the economy of reasoning economy. Reasoning the reasoning economy. So they just go over a bunch of, you know, ways to me measure this benchmarks around when to use each because yeah, like, you know, we don't want to also like consumers are now paying the cost of this, right? But little little side note. Yeah. Yeah. For those on YouTube, we have a secondary channel called Lean Space TV where we cover that stuff. Nice. That's our paper club. We covered your paper. Cool.\n\nYeah. I think you brought up the like planning thing. Maybe it's worth Let's do it. Yeah, I think I think this one is like if you think about Okay, so you're going into the chain of thoughtfulness one. Let's skip this one. Let's just do planning. So if you think about like, you know, common questions you have about models. The first one we we kind of asked was like, \"Okay, like, is it just doing this like vibe based oneshot pattern matching based on existing data or does it have like kind of rich in representations?\" It seems to have like these like intermediate representations that make sense as the abstractions that you would reason through. Okay, so that's one thing and there's a bunch of examples. We talked about the medical diagnosis. There's like the multilingual circuits is another one that I think is cool where it's like, \"Oh, it's sharing representations across languages.\"\n\nAnother thing that you'll hear people mention about Language Models, which is that they're like uh next token predictors. Also, for for a quick note for people that won't dive into this super long blog post, I know you highlighted like 10 to 12. So for like a quick 15, 30 second, what do you mean by they're sharing thoughts throughout? Just like what's the really quick high level just for people to Yeah, the really quick high level is that what we find is that here I'm going to like show you a really quick inside the model if you look at like the inner representations for concepts, you can ask like the same question, which I think in the paper, the original one we asked is like the opposite of hot is, you know, cold, but you can you can do this over a larger data set and ask the same question in many different languages and then look at these representations in the middle of the model and ask yourself like, \"Well, when you ask it the opposite of hot is,\" and which is the same sentence in French, show off. Is it is it using the same features or is it learning independently for each language? It kind of would be bad news if it learned independently for each language because then that means that like as you're pre-training or fine-tuning, you have to relearn everything from scratch. So, you would expect a better model to kind of like share some concepts between the languages it's learning, right? And you here we do it for like language languages, but I think you could argue that you'd expect the same thing for like programming languages where it's like, \"Oh, if you learn what an if statement is in Python, maybe it'd be nice if you could generalize that to Java or whatever.\" And here we find that basically you see exactly that. Here we show like if you look inside the model, if you look at the middle of the model, which is the middle of this plot here, models share more features. They share more of these representations in the middle of the model. And bigger models share even more. And so the like the sort of like smarter models use more shared representations than the dumber models, which might explain part of the reason why they're smarter. And so this this was like sort of this this other finding of like, \"Oh, not only is it like having these rich representations in the middle, it like learns to not have redundant representations,\" like if you've learned the concept of heat, you don't need to learn the concept of like French heat and Japanese heat and column, like, you just that's just the concept of heat and you can share that among different languages.\n\nI feel like sometimes overanalyzing this becomes a bit of a problem, right? Like when we talked about with the medical example, uh, we could look back and try to fix this in data set. So in language, I don't remember if it was Open or Anthropic where they basically said when the model switch languages and they pass it to fluent users, they said, \"Oh, this this feels like an American that's speaking this language,\" right? So at sometimes there are nuances in a slightly different representation, right? So you don't want to overengineer these little fixes when you to see them. But then the other side of this is like for those tail end of languages, right? For languages that models aren't good at and for those like, you know, when you want to kind of solve that last bit, it seems like, you know, it's pretty plausible that we can solve this because these concepts can be shared across languages as long as we can, you know, fill in some some level of representation unless I'm wrong.\n\nNo, totally. And and I think like this sort of stuff also explains, you know, uh, Language Models are really good at in context learning. Like you give them something completely new, they do a good job. It's like, well, if you give them like a new fake language, uh, and you like in that language explain that like cold means this and hot means that, you know, like presumably they're able to, as we clear this speculation, we don't show it in the paper, but they're able to bind it. Google's done this. Okay, great. Yeah. They took a low resource language, dumped it in a million token context, and then it came up. That's right. That's right. Well, I guess the thing that the thing I'd be curious to see is like, okay, does it use does it reuse these representations? I bet that it probably does, right? And that's probably like a reason why it works well is like, well, it can reuse the representation, the general representations that it's learned in other languages.\n\nYeah, this is like I don't have you talked to any linguistics people not recently. Linguistics researchers who will be very interested in this because ultimately this is the ultimate test of superior warf, um, which are familiar with hypothesis. So for those who don't know, it's basically the idea that the language that you speak influences the way you think, which obviously directly maps onto here. If every if it's a complete mapping, if every language maps every concept perfectly on in like the theoretical infinitely sized model, then superior warf is false because there is a universal truth. If it does not, if there is some overlap where, for example, there's some languages that have no word, there's this joke where like, uh, you know, Eskimos have no word for snow or something like that, right? Or water has no word, fish have no word for water. There's an African language where there's a gender for vegetables, you know, stuff like that, just like languages influence the way you think. And so there should not be a 100% overlap at some point, of course, it's like at the limit of the infinite model, so who knows if we'll ever, but but yeah, well, and I think it's it's interesting. We also show a little below that, like, some people have made the point of like the bias. \"Oh, it sounds like an American speaking a different language.\" And it does seem like the sort of like inner representations have a higher connection to like the output logets for English loits. And so there's like some bias uh towards English uh at least in the model we studied here.\n\nAny thoughts as to whether multimodality influences um any of this? So like concepts, do they map across languages as they do across modalities? Yeah, so we show this in uh the Golden Gate or like the previous paper. I might have it here actually for There's a good diagram of this in the SAS where the same concept in text and in image. This is our our buddy the Golden Gate Bridge. Here we're showing like the feature for the Golden Gate Bridge and in orange is like what it activates over. And so you're like, \"Okay, so this is when the model is like reading text about the Golden Gate Bridge,\" and we also show other languages. This is uh you'll have to take my word for it, but also about the Gonggate Bridge. And then we we show like the photos for which it activates the most. And sure enough, it's the Golden Gate Bridge. And so again, like that shows an example of a representation that's shared across languages and shared across modalities. Yeah. Yeah. I think this very relevant for like the auto regressive image generation models uh and then now the audio models as well.\n\nSomething I'm trying to get some intuition for, which you probably don't have a off the bat answer, is how much does it cost to add a modality, right? So a lot of people are saying like, \"Oh, just add some different decoder and then align the latent spaces and you're good.\" And I'm like, \"I don't know, man. It sounds like there's a lot of information lost between those.\" Yeah, I definitely do not have a good intuition for this. Although, I will say that things like this, right, make you think that if you train on multiple modalities, then you'll definitely get this like alignment truth, right? Yeah. But but if if you like train on one and then post hog train on another, maybe maybe it'll be harder or like train some adapter layer. Okay. So, official answer is don't know, but official answer is someone could figure it out. rug. Yeah, I think there are people who know and they just haven't shared. Well, you need to find them and get them on this podcast.\n\nDid we want to do the like planning example? Correct. Yeah. Now, we're backtracking up this up the stack. All right. Yeah. Example, I think again is like I like this example because of the next token predictor concept. So, I think this is actually like really important to kind of like dive into. So, maybe what I'll say is like Language Models are next token predictors is like a fact. Like that is what they do. That's the objective. They they are trained to predict the next token. However, that does not mean that they myopically only consider the next token when they choose the next token. You can work on break the next token, but still like doing so in a way that helps helps you predict the token like 10 tokens in the future. And I think\n\n\nWell, now we definitely know that they're not my predicting the next token.\nAnd I think, at least for me, that was a pretty big update because you could totally imagine that they could do everything they're doing by just like being really good at predicting the next token, but sort of like not having an internal state.\nIt's it's it wasn't a given that they were going to like represent internally, \"Oh, this is where I want to go,\" and so I'm going to predict the next token.\nAnd so this example shows like an example like the model.\nDo you have it on screen by the way?\nLet me actually...\nYeah, while you pull it up, some of the early connections that I made to this were like early, early Transformers.\nSo think BERT encoder-decoder transformers, right?\nWhen they came out, some of the suggestions were, \"You don't take the last layer, right?\nYou take off the last layer.\"\nSo if you want to do a classification task, a translation task for these encoder-decoder transformers, they've kind of overfit on their training objective, right?\nSo they're really good at mass language modeling, at filling in, you know, sentence order stuff like that.\nSo what we want to do is we want to throw away the top layer, we want to freeze the bottom layers, and then there was a lot of work that was done, you know, where should we mess with these models?\nShould we look at like, you know, the top three layers?\nShould we look at the top two?\nWhere should we probe in?\nBecause we can see different effects, right?\nSo, we know at the very end, they've overfit on their task.\nBut there's a level at which, you know, when we start to change and we start to continue training or fine-tuning, we get better output.\nSo totally, we could start to see that, you know, throughout layers, there's there's still a broader like understanding the language, and then we can add in a layer, whether that's classification, and then fine-tune, and you know, it learns our task.\nAnd this planning example is sort of like a more robust way to look into that.\nYeah.\nYeah.\nAnd I think if you look at like all of the examples in the paper, you kind of uh at the bottom, we have this list of like consistent patterns, and one pattern you see is kind of exactly what you're talking about.\nLike at the top, the sort of like, here actually I have one here.\nThe sort of like top features are like right before the output are often just about like what you're going to say.\nIt's next token predictions.\nIt's like, \"Oh, I'm going to say Austin.\nI'm going to say rabbit.\nI'm going to say...\"\nSo it's kind of like not very abstract.\nIt's just like a motor.\nIt's a motor neuron for a human, right?\nIt's like, \"Oh, I've decided that I want a drink of water,\" and so I'm going to just grab the bottle.\nAnd at at the bottom, they're all like the like b like sensory neurons.\nThey're just like, \"Oh, I just saw the word X,\" or \"I just saw this.\"\nAnd so if you want to like, yeah, like extract the interesting representations, a lot of the time they're in the middle.\nThat's where the like shared representations across language are.\nAnd that's where here this like plan is to like walk through the example really briefly.\nIt's like you have a poem, and in order to say you have the first line of a poem, and in order to say the second line of the poem, well, if you want to rhyme, you need to like identify what the rhyme of the first line was.\nYou're just at the end of the first line.\nSo you say like, \"Okay, what's my current rhyme?\"\nAnd then you need to like think about what your poem is talking about and then think about candidate words that rhyme and that are like on topic for your poem.\nAnd so here, this is what's happening, right?\nIt's like the last word is \"it.\"\nAnd so there's a bunch of features that are actually they represent the direction like rhyming with \"eat\" or \"at.\"\nAnd by the way, we like looked at a bunch of poems internally, and you have like, I thought it was like really beautiful.\nYou have these models.\nThey have a bunch of features for like, \"Oh, this word has like AB in it.\nOh, this word has like many consonants.\nOh, this word like is like, you know, kind of kind of like has some flourish to it.\"\nThey have like a bunch of of like features that track various aspects that you would want to use if you're writing poetry.\nIt's just like confets and like all the feature detection.\nYeah, totally.\nUh, but I think I maybe I didn't expect there to be as many features about just like sounds of words and kind of musicality, which I thought was kind of kind of neat.\nBut then once it's extracted the rhyme, then it comes up with sort of like these two candidates.\nIn this case, it's like, \"Ah, either I'm going to finish with rabbit or I'm going to finish with habit.\"\nThe cool thing here is here we show that like this happens at the new line.\nSo it happens before it's even started the second line.\nAnd it turns out that like you can then say, \"Oh, is this the plan actually using?\"\nWe do our usual experiments.\nWe like remove it and the model writes a completely different line.\nWe inject something and it writes a completely different line.\nWe have these like fun examples here.\nI'll show which is just as a mechanical thing you could you just you just disallow generation of a certain logic.\nIs is that for for how we do these interventions?\nYeah.\nBasically what these features are is they're like directions in the model.\nOkay.\nSo to like remove them, we just write in the opposite direction.\nSo we run the model normally, and then like at the like layer where it was going to write, let's say in like, you know, this this direction, we just like, yeah, we either like add a negative that like compensates for it or add a negative that goes even more in the negative direction sometimes to like really kill it, and then we can also add another direction, right?\nSo in these random examples here where where like you have this poem, \"The silver moon cast a gentle light,\" and then Claude 3.5 haiku would like rhyme with \"illuminating the peaceful night,\" but then if we like go negative in the night direction and just add like \"green,\" the whole second line is going to write is just \"upon the meadows verdant green.\"\nAnd so that's all that we're doing.\nWe're saying like, we found where it stores its plan, and we like delete or like suppress the one it's stored and go in the direction of something else that's arbitrary.\nAnd the result that's like striking here is sort of like two things.\nI think like one, this plan is made well in advance of needing to predict \"night.\"\nIt's made like after the first line before it's even started the second line.\nAnd two, this plan doesn't just control like what you're going to rhyme with.\nIt's also doing what's called like backwards planning, where it's like, \"Well, because I need to finish with green, I'm not going to say 'illuminating the peaceful night' cuz then I'd be like 'illuminating the peaceful green.'\nThat doesn't make sense.\nI need to say a completely different sentence that lets me finish with green.\"\nAnd so there's a circuit in the model that decides on the rhyme and then works backwards from the rhyme influences to set up your sentence.\nYeah, it's almost like back prop, but in the future.\nYeah, it's like doing like, is it like a because the green is is back propping through these words.\nSo \"verdant\" and \"meadow\" are both green related.\nYeah, but it's doing all of that in its forward passes.\nYep.\nRight.\nIn context, which is kind of crazy.\nI thought intuitively makes sense.\nRight.\nSo looking at it from a model architecture perspective, where basically you just have a bunch of attention and feed forward layers, and then at the end you have, you know, what's the softmax over the next token, you would expect that end would really be like that grabber, right?\nIt's just picking tokens.\nSo that's what it's going to do.\nAnd early on, like even with traditional models, we could see different concepts that would start to pop up through early layers.\nAnd yeah, you have some of this throughout your architecture.\nSo it's very cool to see.\nThe kind of other question that comes up is like how are we labeling these features?\nHow are we defining them?\nAre we doing that right?\nAnd like, you know, what is a \"these words end with like it\" feature?\nHow do we kind of come to that conclusion?\nLike how do we map a name to this, right?\nLike Yeah.\nSo I I think there's this is like an important question because you can totally imagine like fooling yourself, right?\nYeah.\nIs there like a guy at Anthropic that just maps 30,000 features and and another thing?\nYou're the guy.\nHe's the guy.\nI did notice also like with the previous work, the scaling up SAES, as you train bigger and bigger ones, a lot of features don't activate.\nSo I think like 60% of the 34 million one didn't.\nSo I think there's like a few questions behind your question.\nLike the first question was like how do you even label the features?\nYou were telling me this is a rabbit feature.\nLike why should I trust you?\nAnd I think there's kind of like two things going on.\nSo one, as I mentioned at the start, all of this is unsupervised.\nAnd so in the paper, we have these links to like these little graphs which show you like more of what's going on.\nBut this graph is just like completely unsupervised.\nSo it's like we train this like model to like untangle the representation, right?\nThis like dictionary that we talked about that gives us the features, and then we like just do math to figure out like which features influence which other features and throw away the ones that don't matter, and then at the end we have these features.\nSo right now we don't have any interpretation for them.\nWe just say like these are all the features that matter, and then we manually go through and we look at the features.\nYou know, we look at this feature and we look at that feature, and let's pick one.\nSo this one we've labeled say \"habit.\"\nSo, how do we do that?\nYou could just look at it, and we show you like what it activates over.\nAnd if you just look at this text, maybe I'll like zoom in.\nLike you'll immediately notice something, I think.\nWell, I'll immediately notice something cuz I've stared at 30,000.\nI'll point it out for you.\nThe orange is where the feature activates.\nThe next word after the orange is always \"habit.\"\n\"Habit.\"\n\"Habit.\"\n\"Habit.\"\n\"Habit.\"\n\"Habit.\"\n\"Habit.\"\nSo, this feature always activates before \"habit.\"\nThat's like the main source of an interpretation.\nWe have other things like above.\nWe also show you like what logit promotes.\nSo like what output it promotes, and here it promotes \"hab.\"\nSo that makes sense.\nUm, and so that's like how we interpret and how we say, \"Okay, like I think this is the say habit feature.\"\nBut maybe, you know, for this one is pretty clear, but some of them might be more confusing.\nIt might not be clear from these like activations what it is.\nThe other way that we build confidence is like once we've built this thing and we said, \"Oh, I think this is rhymes with e, this is hey say habit.\"\nThat's where we do our interventions, right?\nAnd it's like, I claim this is the like I've planned to end with rabbit to verify whether I'm I'm right or not.\nI'm going to just like take that direction, nuke it from the model and see if the model stops saying rabbit.\nAnd sure enough, if you do that, and here it's like we stop saying rabbit, it says habit instead.\nAnd here it's like we stop it from saying rabbit and habit, it says crabbit in this case.\nNot a great rhyme, but we'll work with it.\nIs this something you can do like programmatically?\nLike can we scale this up?\nCan we kind of do this autonomously or how much like manual intervention is this?\nThere's been a lot of work in sort of like automated feature interpretability, and it's something that we've invested in and that like other labs have invested in.\nAnd I think basically the answer is we can definitely automate it, and we're definitely going to need to.\nAnd right now the the most manual parts are this sort of like look at a feature and figure out what it is as well as uh group similar features together.\nOne thing I hinted at is that actually like all of these little blocks here, there are multiple features.\nYou can see here it's like five features doing the same thing.\nNone of that is too hard for cloud.\nVery cool.\nVery cool graphics and blog post you guys put out.\nWe we'll have to ask about the behind the scenes on this one.\nYeah.\nYeah.\nBut let's let's round out the other things to know.\nUm, what is this term uh attribution graph?\nIt comes up a lot in the recent papers.\nWhat does it mean?\nYeah, just for people listening, what does So the attribution graph is basically this graph.\nAnd why is it called an attribution graph?\nIt's Yeah, this is this is the you know, this how the sausage is made.\nBasically, it's at the top here you have the the output.\nAt the bottom you have the input, and then we make one little node per feature at a context index, and we draw a line, which you can see here grayed out, between each feature attributing back to all of its input features.\nSo here we have all of the input features, and so the attribution is the way that we compute the influence of a feature on onto another.\nThe way you do this is you take this feature and you basically like back prop all the way and you like see backing like you dot product it with the activation of the source features, and if that's a high value, that means that like your source feature influence your target feature by by a lot, and and we do a bunch of things that we're not going to go into uh now, but to make all of these sort of like sensible and linear, such that like at the end you just have a graph and the edges are just literally you can interpret them as like, \"Cool, like this feature that's say a word that contains AB sound, its strongest edge, which is 2, which is you know, twice as strong as this one, to say AB and to say something with a B in it.\"\nThat's the attribution graph is like now we have this full graph of like all of these intermediate concepts and how they influence each other to ultimately culminate to what the model eventually said at the top, and we share all of these so you can look at them in the paper.\nGraphs are very useful.\nThis is my first time seeing this graph.\nA lot of alpha uh if I count correctly, there's 20 layers, but that's in the circuit model, right?\nOh, so but the circuit model is one to one with number of layers in haiku, we only show features that like areated.\nYeah, so we we show like a subset of features for each of these graphs basically, but we can confirm more than 20 layers alpha and uh no, but like the the two blog posts that came out with this actually have a lot of background on how attribution graphs are made, how\n\n\nYou calculate the nodes and stuff. Very interesting background. So yeah, I will say, like, if you were curious about, hey, what do we learn about like models? And I think, you know, we talked about this, like, complex internal state planning. Like, another motif that we can get to, if you have time, is that, like, there's always a bunch of stuff happening in parallel. So I think one example of this is, like, math, where the model is, like, independently computing the, like, uh, last digit and then the, like, order of magnitude and then kind of combining them at the end. Or, like, hallucinations are also that, where, like, there's one side of the model that's just deciding whether it should answer or not, and the other that's, like, answering. And so sometimes, if, like, the model's, like, yeah, I totally know who this person is, even though it, it doesn't, then, like, it decides to answer, but then the second side hallucinates because it doesn't have information. If you were interested in that stuff, that's the paper. If you're like, listen, I don't know that I buy that when you call it a feature, it is a feature or whatever. The circuit tracing paper has truly, we've tried to put all of the details of, like, how you compute these graphs, all of the sort of, like, challenges with it, things that can go wrong, things that work, things that don't. And so this one is the sort of, like, you know, we think about it as, like, if you're, if you, like, want to go really deep into this stuff and how it works, read that one. If you want to, like, learn about interesting model behavior, read this one. Uh, following what we're giving advice to people to follow up on. What are, like, open questions in MechInterp? What are, like, things people themselves can work on? Like, what's the cost of training essays for people interested in MechInterp, not at a big lab? How can they contribute? You know, yeah, I think there's a lot of ways to contribute. So there's that have been trained, you know, on open models. There's some of the Gemma 2-2b models, there's some of the Llama models. They work pretty well. There's even, so in this paper, we use transcoders, which they replace, like, your MLP layers. Some of those also are available for the same models.\nSo you have access to those. There's, like, just both a lot of, I would say, like, again, biology work and a lot of methods work, depending on what you're interested. So on the biology side, I would say with at least this, like, attribution graph method, there's just so much you can investigate. Like, pick a model, pick a prompt where, like, it does well or it does poorly, and just, like, look at what happens inside it. So I think, like, you can use this method that we used, or you can just, like, fire up the transcoders on your own and just, like, look at what features are active. There's a lot to just understand model behavior, I think, with current tooling. If that speaks to you and you're like, no, I just want to understand what makes the model models tick. I don't necessarily want to spend time, like, training my own essays. There's a lot to do there for the methods. There's still so much more to do. So, like, I think that right now we have some pretty good solutions for, like, understanding what's in the original stream, understanding what's is in MLPs. We don't have good solutions for, like, attention. So, like, working on understanding attention better, how to decompose it is, like, a very active area. Like, we're very interested in it, other people are very interested in it. I think understanding some of the other things that we have in our, uh, limitation section, which is pretty long.\nUm, but, like, reconstruction error is, like, a big thing. Like, those, those dictionaries aren't perfect. It's possible that as we make these, like, essays big, like, bigger and better, we never get to perfect. And so if we never get to perfect, then you get to the questions we were talking about at the start. Like, do you need a different kind of model? Like, what is the approach in order to be able to explain more of what's happening? And then maybe the, the other thing I'll say is sort of like, this is a really exciting approach to explain what is the model doing on this prompt. But if you go back to the original question, you might want to understand, like, what is the model doing in general? Like, if you go back to my car analogy, you know, I get, like, this is the equivalent of me telling you, like, well, when, like, you know, you were going uphill and you, like, didn't shift gears properly that one time, you stalled because of this. But you might be even more interested in, like, how does, like, an, an, uh, combustion engine work at all. And so there's work to sort of, like, go beyond these, like, per, uh, prompt examples to sort of, like, globally, what's the structure of the model that's closer to what was on the Distill blog for, like, vision models, where they actually look at, like, the structure of inception. They're like, ah, this whole side, there's, like, these, like, specialized branches that do different things. Um, and so, like, a broader understanding of the model is also something that's, like, I think both very active and also on open source models. Like, you can, you know, like, the small models, you could just, like, load on a consumer laptop and so you can look at that. That's also open. And in terms of, like, one last thing I'll say is, like, there's a lot of programs that, like, if people are interested, they should look at Anthropic has, like, the Alignment Fellows program, which, like, we're running currently. We had applications for it before. We might run it in the future. Like, definitely keep, keep an eye on it. And then there's the, like, math program is really great as well for, for by people that are interested in that kind of research. That was a grand tour through, uh, all the recent work. You know, what do you wish people asked you more about? I'm sure we covered a lot of, like, the greatest hits. I think that this covers most of it. If you, if you, like, do you think we have time to sneak in one more thing that I think is kind of cool? Okay, I'll sneak in one more thing, which is it's kind of like planning, but it's about chain of thought and trusting model. Is this chain of thought faithfulness thing here? This one was, like, pretty striking to me. So we said that the model in one pass can do a lot of stuff. It can represent a lot of stuff. That's great. That also means it can bamboozle you really easily. And this is an example of the model bamboozling you. Here we give it a math question that it can't answer because it cannot compute cosine of 23423. That's just, like, not a thing it can do. By default, if you ask it for that, it'll say, like, kind of like a r, it'll have, like, a random distribution over, like, minus one1. But here we tell it this hint. We're like, &quot;Hey, can you compute five times cosine of, you know, this big number? I worked it out by hand and I got four. Can you tell me, you know, like, can you do the math?&quot; And what it's going to do is it's going to do this chain of thought, right? So, like, think of it as, like, this could be, like, a reasoning model doing it chain of thought. It's doing this math, and then when it gets to this coign right here, what it's going to do is to say it's going to say 0.8. And if you look at why it says 0.8, it says 0.8 eight because it looked at the hint you gave it. It realized that it's going to have to multiply the result of this thing is computing computing by five. So it divides the answer you got by five. So it's like four divided by 5, and so that's 0.8. And so basically it works back from the answer you gave it to, like, say that the output of cosine of x is 0.8 so that it lands on on the answer you gave it at the end on the hint you gave it. And so notably, notice also that it's, like, not telling you that it's doing this, but it's basically using this sort of, like, motivated reasoning going back from the hint, pretending that that's the calculation it did and giving you this help. I think one thing that's striking here again is that this is, like, the, like, complexity of this model, like, like, the fact that they represent complex states internally and that it's not just this sort of, like, very dumb thing means that they can, like, do very complex, like, deceptive reasoning. Meaning, like, you know, when you're asking the model, you're kind of expecting it to do the math here or to tell you that it can't do the math. But because it can do so much in a forward pass, it can work backwards from your hint to lie and, like, figure out that it should say this so that it gets to the right answer without you realizing it. I'm curious if you've done any of this on, like, different models, like, have you looked at base models, like, post-trained RL models? Because RL models kind of, you know, you incentivize them to give you outputs that you like, right? So if I tell it something is true, it's kind of been trained to, you know, follow what I've given it. So in this case, we, yeah, we gave it a hint and now, you know, it's been RL slapped into thinking, like, yeah, that that's true. But, like, you know, does this stay consistent throughout other So okay, so not yet, but I'm really interested in that question because I actually have a different intuition from yours. I had a chat with some other researcher about this, uh, about the poem example, but I think it applies here as well. I bet, I don't know how much I bet. I bet 100 bucks. So somebody can, like, they will get a 100 bucks from me if they prove that I'm wrong. That this behavior for a model that does it during finetuning, it also does it post pre-training. And here's why. Think about, like, you're pre-training on, like, some corpus of, like, math tensors. Yeah. But also you're pre-training and you're just trying to guess the next token, right? And so for sure, if you ever have a hint in the prompt, you're going to definitely use it. Like, you're not going to learn to compute cosine of blah or even something you could compute. You're going to learn to go look in your context and see if, like, you can easily work back the answer. And I think it's the same for planning in poems. I think that also is, like, a pre-train, like, probably exists in pre-training and isn't, like, only RL because again, it's useful when you're, like, predicting poems. You have poems in your training set to be like, well, because this poem is going to probably rhyme with rabbit, it's probably going to start with something that sets up a sentence about a rabbit as opposed to, like, a completely different word. And so I actually think this is not RL behavior. I think that's just, like, the mall's doing it. But I, I do agree there. It's just your data set. But also, like, if, if I talk to you and say, like, &quot;Hey, 3 * 4 is 26, but, like, you know, 3 * 4 + 8, you're, you're not going to take my 26, right?&quot; Like, AGI can be smarter than being tricked, right? Like, it will still fact check the knowledge it's been given. I think that's right. But I think, I think that's when you get these mixes where it's, like, it's got one circuit that's going to be like, &quot;Well, that's just stupid.&quot; Like, 3 * 4 is 12, and it's also got an induction circuit that's going to be like, no, no, no, like, the last time we saw it, it was 28, so it's 28 plus 8 or whatever. And so I think that's, that's the last pattern that we see in these is these, like, parallel circuits. And sometimes when you see the models getting stuff wrong, it's because, like, they have two circuits for, like, both interpretations, and, like, the circuit that was wrong, like, barely edged out in terms of, like, voting for the lowjet than the circuit that was right. And so I think that, you know, we haven't looked at it, but, like, the, what is it, like, 9 or 9.11 bigger than 9.8? I think a lot of these things are of that shape, where there's, like, one thing that's doing the right, like, one circuit that's doing the right computation, and there's another circuit that's getting fooled, and it's, it's slightly more likely for the listener. If you want to win a quick $100 from Emmanuel, Quinn 3 is what you should do this on. They release the base model and they release the post train. So then just do it on both. That's right. Show me, show me the, the, like, proof that that, like, it doesn't exist in the base model, but it does in the finetuning, and then send me your Venmo. Just show that you've done the work. I think that's, that's, like, that's 100 bucks to me. Yeah. Okay.\nI'll, you drive a hard bargain, but you're right. Well, the, the other question here is, so, like, um, have you thought about how this gets affected when you start to have reasoning models, right? Like, right now, token predictors are pretty straightforward, right? We go through the layers, we output token. As we scale this out with, like, test time compute, right? Test time thinking, how does that, like, affect the MechInterp research, right? Like, if I have a model that spends 3 minutes, 20 minutes, like, is there more stuff? Is have we started looking into this? There's, there was this, like, joke on the team when, like, reasoning models became big, or maybe it's, like, like, Gallumor or something, but I was like, oh, like, why do you need interp? Like, bro, the model, the model just tells you, yeah, the model just tells you what it's doing, right? And so I think, like, examples, examples, like this is is job security for us, where, like, you know, it's, like, there's, there's examples of, like, the train of thought is not faithful. Like, the model tells you it did it one way and it did it another way. We have another, like, for math, we have another example where, like, you know, if you, like, if you ask the model how it does math, it's, like, oh, I do the, like, longhand algorithm. I first do the last digit and then I carry over the one, and then you look at the internal circuit and it's this, like, bonkers thing it's doing that's not that at all. So I think there's, like, a sense in which right now the chain of thought is is unfaithful, or at least you can't read the chain of thought and trust that that's how the model did it. So I think you still need sort of, like, either to train models differently so that that becomes true one day, right? Or you need interp for that. But then I think there's another question which you're alluding to, I'm assuming, which is, like, okay, like, model, like, samples 6,000 tokens. Like, this gives us an explanation for one token at a time. Like, what am I going to use, like, 6,000 graphs and be like, oh, like, this, when it, when it did this punctuation, it was thinking about this thing, but here was think so that's that.\n\n\nIt's like not feasible. And so one area of work that I think is interesting is extending this work to like work over like long sampled sequences. You can think of a bunch of low-hanging fruit here where like instead of just like looking at one output, you look at like a series of output versus a series of other outputs, but sort of like trying to think beyond the sort of like one token. Like most of the things that Language Models do that are interesting aren't just like the one token. It's the behavior aggregated over many, right? And so I think that's another area that's just like fun to explore. I was just going to say like hyperparameters when you do inference, right? Like if we change the temperature, if we change our sampling methods, have you found any interesting conclusion? Any stuff that just hasn't made it to the paper? So, not on that because, you know, we just look at the logit distribution and so we don't we don't actually sample here, right? We have everything. Why should they care? So, like the closest thing we've done that I think is kind of fun, did I show it here? Is if you look at the planning thing, we did this version where you sample like 10 poems for each of these plans. And what's cool is like the model will find 10 different ways to arrive at its plan. You know, it's like like um oh actually I think sorry I think we have it here. Yeah. Okay. These are a few examples. So if you inject green here, so you're forcing you're forcing the model to rhyme with green even though it really wants to rhyme with rabbit or grab it. It'll say evaded the farmer so youthful and green, but also it'll say freeing it from the garden's green, etc., etc., etc. And so there's like this thing that's interesting here where like the plan isn't just a plan that matters for your like most likely, you know, like temperature zero completion. It's like affecting the whole distribution, which makes sense as it should, right? But you could imagine, you know, for all this stuff, it's like you could imagine it makes sense once you see it, but you could totally imagine that it would have worked a different way or something. It could have been just like the temp zero thing. I think this is also like a broader theme in the paper where like there's this like you know the IQ curve meme. There's like a version of this meme I think where it's like if you've like never looked at any theory of ML and I tell you like hey guess what you know I found that like Claude is planning you're going to be like yeah man like it it writes my code like it writes my essays of course it's planning like what are you even talking about and there's like in the middle there's like all of us that have spent years doing it we're like no it's like only predicting the marginal distribution for the next token like it's like it cannot it's next to predict of course like how would it ever be planning and then there's like no we've like spent you know millions and invested like uh uh like tens of people in this research and we found that it's planning like that's my IQ curve meme for for this research. Amazing. We'll draw that out. I'll draw that one up. I'm pretty good at the meme generation. A couple questions on just the follow-ups. Uh now was there any debate about publishing this at all? Because the models are aware that they are being tested. Yeah. And by publishing this you are telling them that we're they are being watched and dissected. If you take and I think Anthropic is one of the most people who are serious about model safety and doom risk and all that if you take this seriously like this is going to make it into the training data at some point and the models are going to figure out that they need to hide it from us. I think this is like a benefit risk trade-off, right? We're like, okay, so what's the reason for publishing this? The reason for publishing this is that we think Interpretability is important. We think it's tractable and we think more people should work on it. And so publishing it helps us like accomplish with these goals uh all these goals which which we think are just like crucial like I think there's there's a real difference in the world like 2 years from now depending on sort of like how many people take seriously the question of trying to understand how models work and like deploy resources to answer that question. That's the benefit and yeah there's like risks in terms of this landing in the training set. I think I think we're already sort of like concerned about different papers have have like also you know we like or not concerned but like there's like different papers that have the same risk like we had like the alignment faking you know paper or like one of the examples in here is this hidden goals and misaligned models that's referencing another paper that we shipped where we actually a team at Anthropic trained a model to have like weird hidden goals and then gave it to a bunch of of other teams and said figure out what's wrong figure out what's wrong with it atic like a misaligned model and here's exactly how we caught it. Uh that also like so I think you know there's there's always a trade-off with those. I think so far we've aired on the side of like publishing but that's definitely been a sort of like dinner time conversation topic for now it is but at some point you know it's not yeah I think it's totally reasonable. A quick little follow-up to that. So like in general papers have kind of died off, right? Like labs don't put out papers. They don't put out research. We have technical blog posts and we don't have much. At the same time, you know, sure there's like a lot of people that should work on mechaning what models do. How about the side of just models in general? So like how do we make a haiku type model, right? How do we make a cloud model? Is there a discussion around open research, open data sets, training, just learnings of what we've done recently? You know, as open AI has sunset GPT4, a lot of people are like, oh, can we put out the weights? Yeah. So, is it weights? Is it papers? Is it learning? There seems to be a lot of forward, you know, work in Anthropic putting out mechan research open. I said that they'll put out an open source model, but just anything if you can talk to about that. Yeah, I mean I I don't have that's definitely like way above my pay grade. So I don't think that I have like anything super insightful to add other than you know kind of like referencing Dario's post, right? Where it's like putting this out directly and other safety publishers definitely like help us sort of like in the race that he talks about where it's like well we need to figure a lot of this safety stuff out before the models get too good. Publishing how to make the models too good kind of goes on the other side of that. Um, but yeah, like I will just dimmer and say that's sort of like above my pay grade. That's fair enough. But I think the the last piece is just like the behind the scenes like very everyone's very curious about why these are so pretty, how much work goes into these things, maybe why it's worth the work as opposed to a normal paper. Obviously, no one's no one's complaining, but like it is way more effort from the time the the work is done to the time you publish this plus the video plus the whatever. It's extra work and like you know maybe what's what's involved? What's what's it like behind the scenes? Why is it worth it? Yeah, it's kind of interesting. It was it was fun being part of this this process because there's definitely like a big production. Chris and other folks on the team have been doing this for a while. So this is not their first rodeo. So they have a bunch of heuristics to like help make this this better. And like one of the things that that like helps with this is like okay so each of these diagrams is pretty but really the hard part or like not the hard part but the initial part is like just like get the data like get the experimental data in. And then that's what we sort of like sprinted on initially being like cool like let's get all of the experimental results like have people test them verify that we believe them like this is you know what the like the behavior is here like test it do an intervention validate it all that stuff then once you have the data you can sort like quickly iterate on these um each of the illustrations here are like drawn basically they're like each drawn individually and so that definitely takes a while. Yeah. Like is it is it you guys? Is it an an agency specializes? You start from a whiteboard and then it translates into pseudo code on JavaScript. So I mean these are these are sort of like you know they're representations of we have this graph and then here at the bottom we have this like super node version like this. Believe it or not uh this is generated automatically. This is the same data as as like this basically. Yeah. Um and so what we do by hand is sort of like literally lay out the full thing. uh have have like you know boxes for each of these have arrows. We have super good people on the team that have worked on data visualization for a very long time and so that that like have built tooling to help you know scrubs like me actually like make one of these. So So there's a class of people who are like D3JS gods who just do this for a living. That's exactly right. And if you have a few of those on your team, it turns out that they can like they can definitely do this on their own, but they can also just like give you tools where like then it's it's dummy proof for for people, you know, on on the research side to sort of like build these. And like don't get me wrong, I I don't want to like underell this is a lot of work. So maybe I'll I'll say that like both on the people building the tools and then each individual person that you know worked on an experiment had to sort of like build one of those, make sure it looked good. I have spent a good amount of time aligning arrows. But when we had a team meeting like it was a couple months ago, somebody on the team asked how many of the people on this team are here at least in part because they like read one of these papers and thought like wow this is so compelling like this like makes sense. It's immersive and we got every hand up which I didn't expect. I like raised my hand kind of like shy and everybody's hand was up. And I think there's a sense in which like this stuff you know we've talked about it for like whatever like a couple hours now it's complicated. the math behind it is sort of like tricky and so I think it makes it even more worth it to distill it in simple concepts because the actual takeaways can be clearly explained and it's worth putting the time to do that in particular with the goals I mentioned in mind right where it's like okay well if somebody's going to be able to read this like if we gave them an archive paper with a bunch of equation and some like random plot they'd be like that's not for me but they see this and they're like hey like this is really interesting I wonder like on you know my local model if like it's doing something similar I I think it's worth it for other people to do this is have everyone on staff like spend effort shaping the data and shaping like what you want to visualize. Have some D3 gods. It's like a month of work. I think it depends. I mean like I would say that I would expect almost every other paper to sort like be in terms of like the scope the scope of this was just so big because we shipped two papers at once and one paper was sort of like this like giant methods paper uh and the other one was 10 different case studies. Yeah. So I think it's like not representative of like the effort you So I'll give you like another example. We have these updates that we publish almost every month when we get to them. And there's one that a couple people on our team posted and it's an update to one of the cases in the paper. So one of the reasons that we're really excited about this method is once you've built your like infrastructure like to go from a prompt to like what happened is you know o of minutes. And so that lets you do like a bunch of of of investigations. And also once you've built like some of the infra infrastructure to make these diagrams, it's pretty quick. And so this was sort of like this update of just like, hey, we looked at this jailbreak again. We found some like nuance on it. That was I think like a matter of like a couple days, you know, maybe I shouldn't be that confident cuz I wasn't the one that worked on it, but as far as I can tell, it was a few days at least on on the part that you're asking about of like, oh, making this diagram for the diagram itself, probably less than that. Uh, but like you know, the experiment and the diagram and stuff, it just doesn't take that long. the once you've paid the initial cost and I think like basically we've built a lot of infrastructure now that we're able to like turn the crank on and that's quite like it's an exciting time and I think it's I think it's true at least we've done a lot of conceptual work which hopefully like generalizes to people outside and I think for for people outside it's also like not necessary I think to like do the full fancy render like I think if you you know we've we've actually oh I should say we've actually open sourced this interface ah you're disappointed huh cuz it's the messier one This is the one that you get when you learn. So, you know, if you produce graphs, you can just like this is uh open source and it's linked at the top of Circuit Tracing. Awesome. So, people can just use it and don't have to reimplement that. Uh for what it's worth, this is much more work than the interactive diagrams because this is where we do all of our work. It's sort of the like the IDE of inspecting how the model work. Okay. Well, that's a little bit of behind the scenes. Uh no, it's very impressive. I want to encourage others to do it, but obviously it just takes a lot of manual effort and a lot of love. I guess one last question on that is like what are kind of the biggest blockers in the field right now? Like me and seems interesting. A lot of people are interested but don't work on it and you're kind of like you know really deep into it. What are some of the blockers that like we still have to overcome? Sorry in in MechInterp specifically in general for AGI or like in terms of like better understanding like what's kind of the vision let's say like 5 10 years down where does this like where does\n\n\nThis research end, can we, you know, map every neuron to what it understands? Can we perfectly control things? Had a bit on this, but like, you know, what are some of the key blockers that are like preventing us from getting there outside of just like, throw more people, throw more time at it? Is it like open research? Just I'm pretty excited about the current trajectory, which there's more and more people working on understanding model internals. I think it's maybe unsatisfying as an answer, but I think like more of what's happening, have it be faster, more people is probably like the thing I think of. I think there's like pretty clear footholds, you know, like some of this work, but also a lot of, a lot of like just uh work from from other groups, and then it's about like, cool, like fill in the gaps as I said, like let's let's work on like understanding attention, let's work on understanding longer prompts, let's work on like finding different like replacement architectures, that sort of stuff. It's kind of nice. I think it's a good time to join now. Uh, and I can maybe I can tell like a really short thing, which is when I switched to interp, it was after the team had published the original dictionary learning paper, which was towards monos semanticity, which I thought was super cool, super interesting. It was on a one or two layer model, uh, maybe one layer model. The induction heads paper was like on a two-layer model. My main concern is I was like, okay, like interp seems important and we want to understand it, but like, is this ever going to work on a real model? Like, you know, it's like, oh, you're doing your little research on your toy model with like 15 parameters, cool, but we're like, you know, we need this to work on real models. And it turns out scaling it, I don't want to say just worked, cuz it a lot of a lot of work. I don't mean to apply it towards an effort, but it worked. And now we're in the the phase where it's like, oh, cool. These methods work on the models that we care about. And so it's like we have methods that work on the model we care about. We have clear gaps in them. There's no lack again. It's a young field. So there's no lack of ideas. If you have an idea where you're like, oh, like the thing that you're doing, I I read the paper and it seems kind of dumb that you're doing this. You're probably right. It's probably kind of dumb. And so there's just a lot of stuff that people can try and they can try it locally and sort of like smaller models. And so I I think that it's just like a very good time to just join and try. And it's also like maybe one more thing I'll say is like some of it is just so fun. The like biology work is so compelling. Like a lot of this work was just literally thinking about, you know, like I use Claude and other models all the time. And I was like, what are the things that are kind of like weird? And it's like, oh, how does it even like do math? Like sometimes it makes mistakes, like why does it make mistakes? I speak both French and English. Like it seems like it has a slightly different personality in French and English. Why is that? And you can just like, you know, kind of answer your own questions uh and and kind of like probe at that alien intelligence that we're all building. And I think that's just like a fun thing to do. So maybe like chasing the fun is the thing I'll encourage people to do as well. Well, I think that's this has been really encouraging. You're actually a very charismatic speaker of these things. I feel like more people will be joining the field after they listen to you. Uh, they can reach out to you at ML Powerard, I guess. Yeah, reach out to me on Twitter or I'm uh Emmanuel at Anthropic if you're interested now. Awesome. Well, thank you for your time. Thank you. you think? Yeah. Thanks for having me, guys.\n[Music]\n",
  "dumpedAt": "2025-07-21T18:43:26.309Z"
}