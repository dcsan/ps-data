{
  "episodeId": "IkATeZsUUko",
  "channelSlug": "@practicallyintelligent",
  "title": "OH1: Office Hours are Open!",
  "publishedAt": "2024-08-29T15:42:48.000Z",
  "rawLines": [
    {
      "lang": "en",
      "text": "[Music]",
      "offset": 0.71,
      "duration": 7.089
    },
    {
      "lang": "en",
      "text": "welcome everybody to practically",
      "offset": 6.56,
      "duration": 3.959
    },
    {
      "lang": "en",
      "text": "intelligent I'm your host sonan ozer and",
      "offset": 7.799,
      "duration": 5.041
    },
    {
      "lang": "en",
      "text": "aay is away this week so we are going to",
      "offset": 10.519,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "try something a little bit different uh",
      "offset": 12.84,
      "duration": 4.12
    },
    {
      "lang": "en",
      "text": "we are going to be hosting I am going to",
      "offset": 15.4,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "be hosting the practically intelligence",
      "offset": 16.96,
      "duration": 5.479
    },
    {
      "lang": "en",
      "text": "first ever office hours so if you have",
      "offset": 19.4,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "never um if you've never you know had a",
      "offset": 22.439,
      "duration": 4.68
    },
    {
      "lang": "en",
      "text": "chance to go to office hours in college",
      "offset": 25.24,
      "duration": 3.879
    },
    {
      "lang": "en",
      "text": "with your ta basically doors are open if",
      "offset": 27.119,
      "duration": 3.721
    },
    {
      "lang": "en",
      "text": "you have questions in and ask me",
      "offset": 29.119,
      "duration": 3.521
    },
    {
      "lang": "en",
      "text": "questions about AI about machine",
      "offset": 30.84,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "learning data science really anything",
      "offset": 32.64,
      "duration": 4.759
    },
    {
      "lang": "en",
      "text": "and I want to answer it there is a form",
      "offset": 35.079,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "on our website practically",
      "offset": 37.399,
      "duration": 3.64
    },
    {
      "lang": "en",
      "text": "intelligent.com where you can submit a",
      "offset": 39.079,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "question and we'll answer it on the show",
      "offset": 41.039,
      "duration": 4.761
    },
    {
      "lang": "en",
      "text": "uh I I'll put a link to it of course in",
      "offset": 43.879,
      "duration": 3.281
    },
    {
      "lang": "en",
      "text": "the description and I'll say it again at",
      "offset": 45.8,
      "duration": 3.8
    },
    {
      "lang": "en",
      "text": "the end of the episode but if you don't",
      "offset": 47.16,
      "duration": 5.559
    },
    {
      "lang": "en",
      "text": "know much about me outside of this show",
      "offset": 49.6,
      "duration": 6.84
    },
    {
      "lang": "en",
      "text": "I run a bunch of lectures for O'Reilly",
      "offset": 52.719,
      "duration": 7.921
    },
    {
      "lang": "en",
      "text": "on ll's generative AI Transformer uh",
      "offset": 56.44,
      "duration": 7.56
    },
    {
      "lang": "en",
      "text": "rhf all kinds of stuff and I give at",
      "offset": 60.64,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "least one lecture a week these days uh",
      "offset": 64,
      "duration": 4.6
    },
    {
      "lang": "en",
      "text": "for the O'Reilly platform and through",
      "offset": 66.4,
      "duration": 4.2
    },
    {
      "lang": "en",
      "text": "those I got a lot of questions just this",
      "offset": 68.6,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "morning before I was recording this I",
      "offset": 70.6,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "was recording a lecture in which I had",
      "offset": 72.68,
      "duration": 5.439
    },
    {
      "lang": "en",
      "text": "at least 50 questions come my way so I",
      "offset": 75.64,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "thought I thought to kick us off I would",
      "offset": 78.119,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "take a few common questions from my most",
      "offset": 79.64,
      "duration": 5.799
    },
    {
      "lang": "en",
      "text": "recent lectures to use as a jumping off",
      "offset": 82.36,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "point and then starting the next time",
      "offset": 85.439,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "that we do office hours we'll take",
      "offset": 87.56,
      "duration": 4.36
    },
    {
      "lang": "en",
      "text": "questions from um from you all from our",
      "offset": 89.2,
      "duration": 5.72
    },
    {
      "lang": "en",
      "text": "audience all right question one and I'm",
      "offset": 91.92,
      "duration": 5.559
    },
    {
      "lang": "en",
      "text": "going to say these as verbatim as I can",
      "offset": 94.92,
      "duration": 6.239
    },
    {
      "lang": "en",
      "text": "as I get them in my in my in my live",
      "offset": 97.479,
      "duration": 7.561
    },
    {
      "lang": "en",
      "text": "lessons is prompt engineering",
      "offset": 101.159,
      "duration": 8.28
    },
    {
      "lang": "en",
      "text": "dead no to put it simply uh the longer",
      "offset": 105.04,
      "duration": 7.039
    },
    {
      "lang": "en",
      "text": "answer to that is prompt engineering is",
      "offset": 109.439,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "kind of this wishy-washy term that has",
      "offset": 112.079,
      "duration": 4.441
    },
    {
      "lang": "en",
      "text": "been used uh to cover a lot of use cases",
      "offset": 114,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "over the last couple of years simply put",
      "offset": 116.52,
      "duration": 6.599
    },
    {
      "lang": "en",
      "text": "prompt engineering is just uh crafting a",
      "offset": 119.52,
      "duration": 6.559
    },
    {
      "lang": "en",
      "text": "and designing an input to usually a",
      "offset": 123.119,
      "duration": 6.76
    },
    {
      "lang": "en",
      "text": "generative AI a generative L like chat",
      "offset": 126.079,
      "duration": 7.201
    },
    {
      "lang": "en",
      "text": "GPD Claud what have you to not only make",
      "offset": 129.879,
      "duration": 6.401
    },
    {
      "lang": "en",
      "text": "the responses better I.E more accurate",
      "offset": 133.28,
      "duration": 5.48
    },
    {
      "lang": "en",
      "text": "reduce hallucinations but also to make",
      "offset": 136.28,
      "duration": 4.039
    },
    {
      "lang": "en",
      "text": "the responses a little bit more",
      "offset": 138.76,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "consistent uh consistent meaning that",
      "offset": 140.319,
      "duration": 4.601
    },
    {
      "lang": "en",
      "text": "you can expect the structure of the",
      "offset": 142.76,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "response coming out of the model and and",
      "offset": 144.92,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "more importantly for a lot of people you",
      "offset": 147,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "can actually translate that PR to",
      "offset": 148.36,
      "duration": 4.04
    },
    {
      "lang": "en",
      "text": "multiple models so if you if you're",
      "offset": 150.28,
      "duration": 3.48
    },
    {
      "lang": "en",
      "text": "familiar with things like f shot",
      "offset": 152.4,
      "duration": 4.36
    },
    {
      "lang": "en",
      "text": "learning Chain of Thought prompting um",
      "offset": 153.76,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "just those are the two main types of",
      "offset": 156.76,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "prompt engineering techniques out there",
      "offset": 159.04,
      "duration": 3.119
    },
    {
      "lang": "en",
      "text": "and they're pretty easy to master it's",
      "offset": 160.68,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "just you know including different",
      "offset": 162.159,
      "duration": 4.44
    },
    {
      "lang": "en",
      "text": "examples in your prompt or in enticing",
      "offset": 164.04,
      "duration": 5.08
    },
    {
      "lang": "en",
      "text": "the AI to actually explain its reasoning",
      "offset": 166.599,
      "duration": 4.761
    },
    {
      "lang": "en",
      "text": "before answering a question these are",
      "offset": 169.12,
      "duration": 3.8
    },
    {
      "lang": "en",
      "text": "techniques that will actually translate",
      "offset": 171.36,
      "duration": 3.48
    },
    {
      "lang": "en",
      "text": "between different llm so if you write a",
      "offset": 172.92,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "good prompt for open AI you should be",
      "offset": 174.84,
      "duration": 4.28
    },
    {
      "lang": "en",
      "text": "able to translate it to anthropic to",
      "offset": 177.08,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "cohere to you know llama to mistol what",
      "offset": 179.12,
      "duration": 7.479
    },
    {
      "lang": "en",
      "text": "have you so PRT engineering by no means",
      "offset": 182.12,
      "duration": 6.8
    },
    {
      "lang": "en",
      "text": "is dead uh and and in fact one of the",
      "offset": 186.599,
      "duration": 4.681
    },
    {
      "lang": "en",
      "text": "case studies in my most recent book a",
      "offset": 188.92,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "quick start guy to llms I I do a pretty",
      "offset": 191.28,
      "duration": 4.599
    },
    {
      "lang": "en",
      "text": "rigorous case study on prompt",
      "offset": 194.04,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "engineering because you know people",
      "offset": 195.879,
      "duration": 3.64
    },
    {
      "lang": "en",
      "text": "always tell you it's better but they",
      "offset": 198.12,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "don't really show you that it's better",
      "offset": 199.519,
      "duration": 5.121
    },
    {
      "lang": "en",
      "text": "so the the basic premise was if I take a",
      "offset": 201.56,
      "duration": 6.28
    },
    {
      "lang": "en",
      "text": "bunch of math word problems not the GSM",
      "offset": 204.64,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "Benchmark but a different one math QA",
      "offset": 207.84,
      "duration": 9.16
    },
    {
      "lang": "en",
      "text": "and I ask anthropics Opus llama 3 um GPT",
      "offset": 210.64,
      "duration": 8.44
    },
    {
      "lang": "en",
      "text": "Ford 3.5 and all of these different",
      "offset": 217,
      "duration": 5.72
    },
    {
      "lang": "en",
      "text": "models if I ask the math word problems",
      "offset": 219.08,
      "duration": 5.96
    },
    {
      "lang": "en",
      "text": "in with various ranges of prompting",
      "offset": 222.72,
      "duration": 5.799
    },
    {
      "lang": "en",
      "text": "ranging from zero shot prompting no",
      "offset": 225.04,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "Chain of Thought no F shot example just",
      "offset": 228.519,
      "duration": 5.161
    },
    {
      "lang": "en",
      "text": "ask a question and get an answer all the",
      "offset": 230.56,
      "duration": 6.599
    },
    {
      "lang": "en",
      "text": "way to Chain of Thought learning with",
      "offset": 233.68,
      "duration": 6.44
    },
    {
      "lang": "en",
      "text": "three or five shot prompting and even",
      "offset": 237.159,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "taking it one step further using",
      "offset": 240.12,
      "duration": 3.72
    },
    {
      "lang": "en",
      "text": "semantic kshop which is actually using",
      "offset": 241.48,
      "duration": 4.92
    },
    {
      "lang": "en",
      "text": "the most relevant semantically speaking",
      "offset": 243.84,
      "duration": 5.08
    },
    {
      "lang": "en",
      "text": "the most relevant examples from a data",
      "offset": 246.4,
      "duration": 6.28
    },
    {
      "lang": "en",
      "text": "set rather than just randomly taking",
      "offset": 248.92,
      "duration": 5.8
    },
    {
      "lang": "en",
      "text": "examples you can actually see a",
      "offset": 252.68,
      "duration": 4.36
    },
    {
      "lang": "en",
      "text": "performance increased um you know from",
      "offset": 254.72,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "for llama for example the performance",
      "offset": 257.04,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "went from 10% accuracy to 40% accuracy",
      "offset": 258.799,
      "duration": 5.881
    },
    {
      "lang": "en",
      "text": "just by changing the prompt for Opus it",
      "offset": 262.24,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "went from 50% to about",
      "offset": 264.68,
      "duration": 6.12
    },
    {
      "lang": "en",
      "text": "70% um accuracy and again the questions",
      "offset": 266.96,
      "duration": 5.519
    },
    {
      "lang": "en",
      "text": "didn't change the only things I was",
      "offset": 270.8,
      "duration": 4.28
    },
    {
      "lang": "en",
      "text": "changing was how many examples was it do",
      "offset": 272.479,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "I use f shot learning simple things like",
      "offset": 275.08,
      "duration": 4.2
    },
    {
      "lang": "en",
      "text": "that can actually really increase the",
      "offset": 277.68,
      "duration": 2.92
    },
    {
      "lang": "en",
      "text": "performance of your prompt so prompt",
      "offset": 279.28,
      "duration": 4.12
    },
    {
      "lang": "en",
      "text": "engineering is not dead what might be",
      "offset": 280.6,
      "duration": 5.319
    },
    {
      "lang": "en",
      "text": "dead are is the way of people kind of",
      "offset": 283.4,
      "duration": 4.44
    },
    {
      "lang": "en",
      "text": "calling themselves prompt Engineers",
      "offset": 285.919,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "saying hey I know how to use the open AI",
      "offset": 287.84,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "API I'll call myself a prompt engineer",
      "offset": 290.08,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "and that's still a useful skill to know",
      "offset": 293.28,
      "duration": 4.359
    },
    {
      "lang": "en",
      "text": "how to do that and how to do that",
      "offset": 296.32,
      "duration": 3.64
    },
    {
      "lang": "en",
      "text": "effectively but I think as AI kind of",
      "offset": 297.639,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "got easier and easier to use simply",
      "offset": 299.96,
      "duration": 6.239
    },
    {
      "lang": "en",
      "text": "knowing how to call the API or know how",
      "offset": 302.6,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "to do these prompting techniques was not",
      "offset": 306.199,
      "duration": 4.521
    },
    {
      "lang": "en",
      "text": "enough people had to start setting up",
      "offset": 308.84,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "testing harnesses testing sets iterating",
      "offset": 310.72,
      "duration": 6.6
    },
    {
      "lang": "en",
      "text": "on prompts trying different uh prompt",
      "offset": 313.96,
      "duration": 5.64
    },
    {
      "lang": "en",
      "text": "model combinations as models came out",
      "offset": 317.32,
      "duration": 5.8
    },
    {
      "lang": "en",
      "text": "tweaking prompts to to try to catch more",
      "offset": 319.6,
      "duration": 5.96
    },
    {
      "lang": "en",
      "text": "edge cases as edge cases were found so",
      "offset": 323.12,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "prompt engineering the job of a prompt",
      "offset": 325.56,
      "duration": 4.52
    },
    {
      "lang": "en",
      "text": "engineer has gotten larger it's not just",
      "offset": 327.199,
      "duration": 4.681
    },
    {
      "lang": "en",
      "text": "knowing prompting techniques and",
      "offset": 330.08,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "implementing them but it's also about",
      "offset": 331.88,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "basic kind of machine learning Tech",
      "offset": 334.72,
      "duration": 4.039
    },
    {
      "lang": "en",
      "text": "ideas like what are edge cases how do",
      "offset": 336.759,
      "duration": 3.241
    },
    {
      "lang": "en",
      "text": "you catch them and how do you account",
      "offset": 338.759,
      "duration": 2.44
    },
    {
      "lang": "en",
      "text": "for",
      "offset": 340,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "them all right question two um this is",
      "offset": 341.199,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "the one I get more often recently which",
      "offset": 344.08,
      "duration": 5.48
    },
    {
      "lang": "en",
      "text": "is what is an AI agent and what makes",
      "offset": 345.84,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "them",
      "offset": 349.56,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "useful well simply put an AI agent is",
      "offset": 350.56,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "really nothing more than a",
      "offset": 353.08,
      "duration": 3.399
    },
    {
      "lang": "en",
      "text": "well-engineered",
      "offset": 355.28,
      "duration": 3.88
    },
    {
      "lang": "en",
      "text": "prompt uh attached to some external",
      "offset": 356.479,
      "duration": 7.28
    },
    {
      "lang": "en",
      "text": "tools so chat GPT is itself an agent the",
      "offset": 359.16,
      "duration": 8.08
    },
    {
      "lang": "en",
      "text": "underlying llm is GPT let's say GPT 4 or",
      "offset": 363.759,
      "duration": 8.081
    },
    {
      "lang": "en",
      "text": "GPT 3.5 but when you ask Chad GPT to",
      "offset": 367.24,
      "duration": 8.76
    },
    {
      "lang": "en",
      "text": "draw me an image for example GPT 4",
      "offset": 371.84,
      "duration": 6.6
    },
    {
      "lang": "en",
      "text": "doesn't know how to do that on its",
      "offset": 376,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "internal prompt behind the scenes it has",
      "offset": 378.44,
      "duration": 5.759
    },
    {
      "lang": "en",
      "text": "access to different tools one of those",
      "offset": 381.44,
      "duration": 7.52
    },
    {
      "lang": "en",
      "text": "tools is do a separate image generation",
      "offset": 384.199,
      "duration": 8.321
    },
    {
      "lang": "en",
      "text": "system so when gp4 listens to me and",
      "offset": 388.96,
      "duration": 6.12
    },
    {
      "lang": "en",
      "text": "says and I say make me an image",
      "offset": 392.52,
      "duration": 5.32
    },
    {
      "lang": "en",
      "text": "underneath the hood it is deciding",
      "offset": 395.08,
      "duration": 5.48
    },
    {
      "lang": "en",
      "text": "before speaking it is deciding I should",
      "offset": 397.84,
      "duration": 6.479
    },
    {
      "lang": "en",
      "text": "call Dolly to actually make this image",
      "offset": 400.56,
      "duration": 5.56
    },
    {
      "lang": "en",
      "text": "and then that kicks off a process the",
      "offset": 404.319,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "image comes back and it shows it to me",
      "offset": 406.12,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "and then it ingests it into the",
      "offset": 408.08,
      "duration": 5.72
    },
    {
      "lang": "en",
      "text": "conversation so my point is agents are",
      "offset": 409.56,
      "duration": 7.079
    },
    {
      "lang": "en",
      "text": "just generative llms with access to",
      "offset": 413.8,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "tools and when you talk to an agent you",
      "offset": 416.639,
      "duration": 5.761
    },
    {
      "lang": "en",
      "text": "really relying on its ability to figure",
      "offset": 419.16,
      "duration": 5.96
    },
    {
      "lang": "en",
      "text": "out which tool it needs and in which",
      "offset": 422.4,
      "duration": 4.6
    },
    {
      "lang": "en",
      "text": "order and if it needs to call multiple",
      "offset": 425.12,
      "duration": 3.88
    },
    {
      "lang": "en",
      "text": "tools to answer your question and",
      "offset": 427,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "frankly hoping it gets it right and if",
      "offset": 429,
      "duration": 3.96
    },
    {
      "lang": "en",
      "text": "you're building an agent you can test",
      "offset": 430.919,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "that Chain by saying okay when I ask",
      "offset": 432.96,
      "duration": 5.679
    },
    {
      "lang": "en",
      "text": "this question I expect you to pick these",
      "offset": 435.479,
      "duration": 5.801
    },
    {
      "lang": "en",
      "text": "tools in order and if you don't I'm",
      "offset": 438.639,
      "duration": 5.641
    },
    {
      "lang": "en",
      "text": "going to change my prompt and and fix it",
      "offset": 441.28,
      "duration": 5.8
    },
    {
      "lang": "en",
      "text": "so that's what agents are today now as",
      "offset": 444.28,
      "duration": 4.759
    },
    {
      "lang": "en",
      "text": "as llms get better at better at",
      "offset": 447.08,
      "duration": 3.64
    },
    {
      "lang": "en",
      "text": "reasoning and and and better at",
      "offset": 449.039,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "processing input you might see a",
      "offset": 450.72,
      "duration": 6.12
    },
    {
      "lang": "en",
      "text": "reduction in the amount of kind of",
      "offset": 453.36,
      "duration": 5.839
    },
    {
      "lang": "en",
      "text": "logical reasoning through okay the AI is",
      "offset": 456.84,
      "duration": 3.72
    },
    {
      "lang": "en",
      "text": "going to say well first I need to call",
      "offset": 459.199,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "Dolly and then I need to do this a lot",
      "offset": 460.56,
      "duration": 4.28
    },
    {
      "lang": "en",
      "text": "of that might happen kind of at",
      "offset": 462.96,
      "duration": 3.72
    },
    {
      "lang": "en",
      "text": "processing time as you are processing",
      "offset": 464.84,
      "duration": 3.639
    },
    {
      "lang": "en",
      "text": "the input as the AI rather is processing",
      "offset": 466.68,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "the input there might be some changes in",
      "offset": 468.479,
      "duration": 5.801
    },
    {
      "lang": "en",
      "text": "the architecture that that will mitigate",
      "offset": 471.08,
      "duration": 5.559
    },
    {
      "lang": "en",
      "text": "the amount of output tokens there but at",
      "offset": 474.28,
      "duration": 4.359
    },
    {
      "lang": "en",
      "text": "the end of the day an agent is simply",
      "offset": 476.639,
      "duration": 5.321
    },
    {
      "lang": "en",
      "text": "just llm with access to external tools",
      "offset": 478.639,
      "duration": 5.56
    },
    {
      "lang": "en",
      "text": "like Google something for me draw me an",
      "offset": 481.96,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "image even memories um what we in chat",
      "offset": 484.199,
      "duration": 4.921
    },
    {
      "lang": "en",
      "text": "GPT when it when it remember something",
      "offset": 487.24,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "about you that tool is called bio I",
      "offset": 489.12,
      "duration": 4.359
    },
    {
      "lang": "en",
      "text": "believe under the hood if you do some",
      "offset": 492.039,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "prompt injection you can kind of you can",
      "offset": 493.479,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "kind of see that they call it bio it is",
      "offset": 495.159,
      "duration": 4.681
    },
    {
      "lang": "en",
      "text": "how it remembers stuff about you I",
      "offset": 497.4,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "believe that's what it was called so and",
      "offset": 499.84,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "then python uh executing python code is",
      "offset": 502.199,
      "duration": 5.321
    },
    {
      "lang": "en",
      "text": "is the fourth tool that Chad GPT has so",
      "offset": 504.56,
      "duration": 4.599
    },
    {
      "lang": "en",
      "text": "that's what an agent is it's it's a Well",
      "offset": 507.52,
      "duration": 3.799
    },
    {
      "lang": "en",
      "text": "engine prompt against a well-performing",
      "offset": 509.159,
      "duration": 4.281
    },
    {
      "lang": "en",
      "text": "model with access to well- defined",
      "offset": 511.319,
      "duration": 7.08
    },
    {
      "lang": "en",
      "text": "tools um and my favorite",
      "offset": 513.44,
      "duration": 7.88
    },
    {
      "lang": "en",
      "text": "question my favorite question that I get",
      "offset": 518.399,
      "duration": 6.201
    },
    {
      "lang": "en",
      "text": "pretty much in every lecture some form",
      "offset": 521.32,
      "duration": 5.079
    },
    {
      "lang": "en",
      "text": "of this",
      "offset": 524.6,
      "duration": 6.72
    },
    {
      "lang": "en",
      "text": "question what is the best model",
      "offset": 526.399,
      "duration": 4.921
    },
    {
      "lang": "en",
      "text": "today I usually have to take that with a",
      "offset": 531.64,
      "duration": 8.08
    },
    {
      "lang": "en",
      "text": "oh boy um because the answer is depends",
      "offset": 534.64,
      "duration": 8.319
    },
    {
      "lang": "en",
      "text": "on what are you trying to do what data",
      "offset": 539.72,
      "duration": 5.119
    },
    {
      "lang": "en",
      "text": "sets are you working with and what are",
      "offset": 542.959,
      "duration": 2.841
    },
    {
      "lang": "en",
      "text": "your",
      "offset": 544.839,
      "duration": 3.481
    },
    {
      "lang": "en",
      "text": "constraints a lot of people simply want",
      "offset": 545.8,
      "duration": 5.279
    },
    {
      "lang": "en",
      "text": "to use generative AI models like chat",
      "offset": 548.32,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "GPT to do things like classification",
      "offset": 551.079,
      "duration": 6.88
    },
    {
      "lang": "en",
      "text": "like input some text output some labels",
      "offset": 554,
      "duration": 6.64
    },
    {
      "lang": "en",
      "text": "easy peasy but that's a lot of Overkill",
      "offset": 557.959,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "in a lot of sense for for trying to do",
      "offset": 560.64,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "some basic classification especially if",
      "offset": 562.36,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "you already have some training data to",
      "offset": 564.079,
      "duration": 5.041
    },
    {
      "lang": "en",
      "text": "train a Bert model for example I do a",
      "offset": 566.12,
      "duration": 4.839
    },
    {
      "lang": "en",
      "text": "case stud in my book where I take the",
      "offset": 569.12,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "same it's a large data set it's like",
      "offset": 570.959,
      "duration": 4.361
    },
    {
      "lang": "en",
      "text": "tens of thousands of examples I take a",
      "offset": 573.279,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "large data set and I fine-tune both chat",
      "offset": 575.32,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "GPT and a Bert model so 175 billion",
      "offset": 577.36,
      "duration": 6.28
    },
    {
      "lang": "en",
      "text": "parameters versus 70 million parameters",
      "offset": 581.32,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "in a distill Bert model and they both",
      "offset": 583.64,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "perform basically the same so wh which",
      "offset": 585.88,
      "duration": 5.48
    },
    {
      "lang": "en",
      "text": "one's better is it the open source model",
      "offset": 589.16,
      "duration": 5.16
    },
    {
      "lang": "en",
      "text": "that I f- tuned or is it the Behemoth",
      "offset": 591.36,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "GPT 3.5 that also was fine-tuned with",
      "offset": 594.32,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "the same data well pricing would also be",
      "offset": 597.36,
      "duration": 5.56
    },
    {
      "lang": "en",
      "text": "a factor a Bert model can batch in",
      "offset": 600.32,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "classification inputs you can do 32 at a",
      "offset": 602.92,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "time 64 at a time GPT you know the way",
      "offset": 605.04,
      "duration": 4.44
    },
    {
      "lang": "en",
      "text": "you construct The Prompt most likely can",
      "offset": 607.959,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "do one at a time so throughput and",
      "offset": 609.48,
      "duration": 5.359
    },
    {
      "lang": "en",
      "text": "latency are an issue cost is an issue if",
      "offset": 611.72,
      "duration": 5.359
    },
    {
      "lang": "en",
      "text": "I have to host Bert myself that is some",
      "offset": 614.839,
      "duration": 5.601
    },
    {
      "lang": "en",
      "text": "cost but if I'm paying for an API call",
      "offset": 617.079,
      "duration": 5.561
    },
    {
      "lang": "en",
      "text": "every time I want to do classification",
      "offset": 620.44,
      "duration": 4.68
    },
    {
      "lang": "en",
      "text": "that's also kind of an issue isn't it so",
      "offset": 622.64,
      "duration": 4.6
    },
    {
      "lang": "en",
      "text": "best is not the right question which",
      "offset": 625.12,
      "duration": 3.719
    },
    {
      "lang": "en",
      "text": "which model is best is never the right",
      "offset": 627.24,
      "duration": 4.2
    },
    {
      "lang": "en",
      "text": "question to ask ask what model is going",
      "offset": 628.839,
      "duration": 5.641
    },
    {
      "lang": "en",
      "text": "to work best for your task given this",
      "offset": 631.44,
      "duration": 6.199
    },
    {
      "lang": "en",
      "text": "data set and your constraints it's a",
      "offset": 634.48,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "longer question but it's the right",
      "offset": 637.639,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "question that you need to ask yourself",
      "offset": 639.519,
      "duration": 3.88
    },
    {
      "lang": "en",
      "text": "are you doing a generative task versus a",
      "offset": 641.48,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "classification or even a clustering or",
      "offset": 643.399,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "embedding task are you limited by cost",
      "offset": 645.399,
      "duration": 5.401
    },
    {
      "lang": "en",
      "text": "and compute resources do you have",
      "offset": 648.519,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "sensitive data that you can't even use",
      "offset": 650.8,
      "duration": 4.68
    },
    {
      "lang": "en",
      "text": "open AI for in the first place all of",
      "offset": 652.6,
      "duration": 4.28
    },
    {
      "lang": "en",
      "text": "those things get factored into that",
      "offset": 655.48,
      "duration": 4.84
    },
    {
      "lang": "en",
      "text": "question and even if you want to say",
      "offset": 656.88,
      "duration": 6.28
    },
    {
      "lang": "en",
      "text": "fine strip all that away and say given",
      "offset": 660.32,
      "duration": 4.519
    },
    {
      "lang": "en",
      "text": "the same data given the same whatever",
      "offset": 663.16,
      "duration": 4.84
    },
    {
      "lang": "en",
      "text": "given the same everything is GPT 4",
      "offset": 664.839,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "better or worse than let's say",
      "offset": 668,
      "duration": 5.32
    },
    {
      "lang": "en",
      "text": "anthropics 3.5 Sonet that's still not a",
      "offset": 669.959,
      "duration": 5.481
    },
    {
      "lang": "en",
      "text": "fair question because the only way that",
      "offset": 673.32,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "you would begin to answer that is",
      "offset": 675.44,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "through benchmarks which are just test",
      "offset": 677.079,
      "duration": 5.041
    },
    {
      "lang": "en",
      "text": "data sets so you're still just going to",
      "offset": 679.839,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "test them against a testing data set",
      "offset": 682.12,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "it's just that we call those testing",
      "offset": 684.399,
      "duration": 2.961
    },
    {
      "lang": "en",
      "text": "data sets that we all kind of quote",
      "offset": 685.959,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "unquote agree on on",
      "offset": 687.36,
      "duration": 5.64
    },
    {
      "lang": "en",
      "text": "benchmarks and even benchmarks have",
      "offset": 690.76,
      "duration": 4.04
    },
    {
      "lang": "en",
      "text": "their own problems not every Benchmark",
      "offset": 693,
      "duration": 4.04
    },
    {
      "lang": "en",
      "text": "is going to be actually useful to the",
      "offset": 694.8,
      "duration": 5.4
    },
    {
      "lang": "en",
      "text": "lay person you know I talked about math",
      "offset": 697.04,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "Q&amp;A earlier in this in this show not",
      "offset": 700.2,
      "duration": 5.439
    },
    {
      "lang": "en",
      "text": "everybody cares about mathboard problems",
      "offset": 703.04,
      "duration": 4.76
    },
    {
      "lang": "en",
      "text": "that's fine you don't need to pick the",
      "offset": 705.639,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "best model for math word problems if",
      "offset": 707.8,
      "duration": 3.64
    },
    {
      "lang": "en",
      "text": "that's not your task it's always going",
      "offset": 709.72,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "to come down to can you effectively test",
      "offset": 711.44,
      "duration": 4.839
    },
    {
      "lang": "en",
      "text": "your model against your data in a way",
      "offset": 713.88,
      "duration": 4.759
    },
    {
      "lang": "en",
      "text": "that you can feel confident about if you",
      "offset": 716.279,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "can't do that there's not really an",
      "offset": 718.639,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "answer to the question which model is",
      "offset": 721.24,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "best because frankly you're just kind of",
      "offset": 723.519,
      "duration": 5.081
    },
    {
      "lang": "en",
      "text": "going off of Vibes and hey look I I'm",
      "offset": 725.72,
      "duration": 5.16
    },
    {
      "lang": "en",
      "text": "here for the Vibes I'm here for you know",
      "offset": 728.6,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "whatever whatever it takes to kind of",
      "offset": 730.88,
      "duration": 4.759
    },
    {
      "lang": "en",
      "text": "get you excited about using the AI but",
      "offset": 732.6,
      "duration": 4.28
    },
    {
      "lang": "en",
      "text": "at the end of the day you want to have",
      "offset": 735.639,
      "duration": 2.961
    },
    {
      "lang": "en",
      "text": "some level of confidence to be able to",
      "offset": 736.88,
      "duration": 4.44
    },
    {
      "lang": "en",
      "text": "point to and say this is better because",
      "offset": 738.6,
      "duration": 5.08
    },
    {
      "lang": "en",
      "text": "this number is better higher or lower",
      "offset": 741.32,
      "duration": 5.079
    },
    {
      "lang": "en",
      "text": "depending on what it is than this number",
      "offset": 743.68,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "end of",
      "offset": 746.399,
      "duration": 3.481
    },
    {
      "lang": "en",
      "text": "sentence okay those are the three",
      "offset": 748.16,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "questions that I had for today it's",
      "offset": 749.88,
      "duration": 3.079
    },
    {
      "lang": "en",
      "text": "pretty short episode but I we're just",
      "offset": 751.44,
      "duration": 3.079
    },
    {
      "lang": "en",
      "text": "kind of testing out this format if you",
      "offset": 752.959,
      "duration": 3.32
    },
    {
      "lang": "en",
      "text": "do like it go ahead and head over to",
      "offset": 754.519,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "practically intelligent.com send us your",
      "offset": 756.279,
      "duration": 4.68
    },
    {
      "lang": "en",
      "text": "question and I'll answer it right on",
      "offset": 758.92,
      "duration": 5.08
    },
    {
      "lang": "en",
      "text": "this show for you until then we will",
      "offset": 760.959,
      "duration": 5.401
    },
    {
      "lang": "en",
      "text": "await the Glorious return of oxay and we",
      "offset": 764,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "can get back to our regularly scheduled",
      "offset": 766.36,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "programs but I did hope you enjoyed our",
      "offset": 768.24,
      "duration": 4.12
    },
    {
      "lang": "en",
      "text": "office hours uh with me sonan alar I'll",
      "offset": 769.72,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "see you next time I'm practically",
      "offset": 772.36,
      "duration": 2.7
    },
    {
      "lang": "en",
      "text": "intelligent",
      "offset": 773.88,
      "duration": 5.519
    },
    {
      "lang": "en",
      "text": "[Music]",
      "offset": 775.06,
      "duration": 7.339
    },
    {
      "lang": "en",
      "text": "a",
      "offset": 779.399,
      "duration": 3
    }
  ],
  "cleanText": "[Music]\nWelcome everybody to Practically Intelligent. I'm your host, Sinan Ozer, and Aay is away this week, so we are going to try something a little bit different. Uh, we are going to be hosting, I am going to be hosting the Practically Intelligent first ever Office Hours. So if you have never, um, if you've never, you know, had a chance to go to Office Hours in college with your TA, basically doors are open if you have questions and ask me questions about AI, about machine learning, data science, really anything, and I want to answer it. There is a form on our website, practicallyintelligent.com, where you can submit a question, and we'll answer it on the show. Uh, I, I'll put a link to it, of course, in the description, and I'll say it again at the end of the episode. But if you don't know much about me outside of this show, I run a bunch of lectures for O'Reilly on LLMs, generative AI, Transformer, uh, RHF, all kinds of stuff, and I give at least one lecture a week these days, uh, for the O'Reilly platform. And through those, I got a lot of questions. Just this morning before I was recording this, I was recording a lecture in which I had at least 50 questions come my way. So I thought, I thought to kick us off, I would take a few common questions from my most recent lectures to use as a jumping off point, and then starting the next time that we do Office Hours, we'll take questions from, um, from you all, from our audience. All right, question one, and I'm going to say these as verbatim as I can as I get them in my, in my, in my live lessons: is prompt engineering dead?\n\nNo, to put it simply. Uh, the longer answer to that is prompt engineering is kind of this wishy-washy term that has been used, uh, to cover a lot of use cases over the last couple of years. Simply put, prompt engineering is just, uh, crafting and designing an input to usually a generative AI, a generative LLM like ChatGPT, Claude, what have you, to not only make the responses better, i.e., more accurate, reduce hallucinations, but also to make the responses a little bit more consistent, uh, consistent meaning that you can expect the structure of the response coming out of the model, and, and more importantly for a lot of people, you can actually translate that prompt to multiple models. So if you, if you're familiar with things like few-shot learning, Chain of Thought prompting, um, just those are the two main types of prompt engineering techniques out there, and they're pretty easy to master. It's just, you know, including different examples in your prompt or in enticing the AI to actually explain its reasoning before answering a question. These are techniques that will actually translate between different LLMs. So if you write a good prompt for OpenAI, you should be able to translate it to Anthropic, to Cohere, to, you know, Llama, to Mistral, what have you. So prompt engineering by no means is dead, uh, and, and in fact, one of the case studies in my most recent book, A Quick Start Guide to LLMs, I, I do a pretty rigorous case study on prompt engineering because, you know, people always tell you it's better, but they don't really show you that it's better. So the, the basic premise was, if I take a bunch of math word problems, not the GSM Benchmark, but a different one, Math QA, and I ask Anthropic's Opus, Llama 3, um, GPT-4, 3.5, and all of these different models, if I ask the math word problems in with various ranges of prompting, ranging from zero-shot prompting, no Chain of Thought, no few-shot example, just ask a question and get an answer, all the way to Chain of Thought learning with three or five shot prompting, and even taking it one step further using semantic few-shot, which is actually using the most relevant, semantically speaking, the most relevant examples from a data set rather than just randomly taking examples, you can actually see a performance increase, um, you know, from, for Llama, for example, the performance went from 10% accuracy to 40% accuracy just by changing the prompt. For Opus, it went from 50% to about 70% um, accuracy. And again, the questions didn't change. The only things I was changing was how many examples was it? Do I use few-shot learning? Simple things like that can actually really increase the performance of your prompt. So prompt engineering is not dead. What might be dead are, is the way of people kind of calling themselves prompt engineers, saying, \"Hey, I know how to use the OpenAI API, I'll call myself a prompt engineer.\" And that's still a useful skill to know how to do that and how to do that effectively, but I think as AI kind of got easier and easier to use, simply knowing how to call the API or know how to do these prompting techniques was not enough. People had to start setting up testing harnesses, testing sets, iterating on prompts, trying different, uh, prompt model combinations as models came out, tweaking prompts to, to try to catch more edge cases as edge cases were found. So prompt engineering, the job of a prompt engineer has gotten larger. It's not just knowing prompting techniques and implementing them, but it's also about basic kind of machine learning tech ideas like what are edge cases, how do you catch them, and how do you account for them?\n\nAll right, question two, um, this is the one I get more often recently, which is, what is an AI agent and what makes them useful?\n\nWell, simply put, an AI agent is really nothing more than a well-engineered prompt, uh, attached to some external tools. So ChatGPT is itself an agent. The underlying LLM is GPT, let's say GPT-4 or GPT-3.5, but when you ask Chad GPT to draw me an image, for example, GPT-4 doesn't know how to do that. On its internal prompt, behind the scenes, it has access to different tools. One of those tools is, do a separate image generation system. So when GPT-4 listens to me and says, and I say, \"Make me an image,\" underneath the hood, it is deciding, before speaking, it is deciding, \"I should call Dolly to actually make this image,\" and then that kicks off a process, the image comes back, and it shows it to me, and then it ingests it into the conversation. So my point is, agents are just generative LLMs with access to tools, and when you talk to an agent, you're really relying on its ability to figure out which tool it needs and in which order, and if it needs to call multiple tools to answer your question, and frankly, hoping it gets it right. And if you're building an agent, you can test that chain by saying, \"Okay, when I ask this question, I expect you to pick these tools in order, and if you don't, I'm going to change my prompt and and fix it.\" So that's what agents are today. Now, as, as LLMs get better at, better at reasoning and and and better at processing input, you might see a reduction in the amount of kind of logical reasoning through, \"Okay, the AI is going to say, 'Well, first I need to call Dolly, and then I need to do this.'\" A lot of that might happen kind of at processing time as you are processing the input, as the AI, rather, is processing the input. There might be some changes in the architecture that that will mitigate the amount of output tokens there, but at the end of the day, an agent is simply just an LLM with access to external tools, like Google, something for me, draw me an image, even memories, um, what we in ChatGPT, when it, when it remembers something about you, that tool is called Bio, I believe, under the hood. If you do some prompt injection, you can kind of, you can kind of see that they call it Bio. It is how it remembers stuff about you. I believe that's what it was called. So, and then Python, uh, executing Python code is, is the fourth tool that Chad GPT has. So that's what an agent is. It's it's a well-engineered prompt against a well-performing model with access to well-defined tools.\n\nUm, and my favorite question, my favorite question that I get pretty much in every lecture, some form of this question: what is the best model today?\n\nI usually have to take that with a, \"Oh boy,\" um, because the answer is, it depends on what are you trying to do, what data sets are you working with, and what are your constraints. A lot of people simply want to use generative AI models like ChatGPT to do things like classification, like input some text, output some labels, easy peasy, but that's a lot of overkill in a lot of sense for, for trying to do some basic classification, especially if you already have some training data to train a Bert model, for example. I do a case study in my book where I take the same, it's a large data set, it's like tens of thousands of examples. I take a large data set and I fine-tune both ChatGPT and a Bert model. So 175 billion parameters versus 70 million parameters in a DistilBert model, and they both perform basically the same. So which one's better? Is it the open-source model that I fine-tuned, or is it the behemoth GPT-3.5 that also was fine-tuned with the same data? Well, pricing would also be a factor. A Bert model can batch in classification inputs, you can do 32 at a time, 64 at a time. GPT, you know, the way you construct the prompt most likely can do one at a time. So throughput and latency are an issue. Cost is an issue. If I have to host Bert myself, that is some cost, but if I'm paying for an API call every time I want to do classification, that's also kind of an issue, isn't it? So best is not the right question. Which, which model is best is never the right question to ask. Ask what model is going to work best for your task, given this data set and your constraints. It's a longer question, but it's the right question that you need to ask yourself. Are you doing a generative task versus a classification or even a clustering or embedding task? Are you limited by cost and compute resources? Do you have sensitive data that you can't even use OpenAI for in the first place? All of those things get factored into that question. And even if you want to say, fine, strip all that away and say, given the same data, given the same whatever, given the same everything, is GPT-4 better or worse than, let's say, Anthropic's 3.5 Sonnet? That's still not a fair question because the only way that you would begin to answer that is through benchmarks, which are just test data sets. So you're still just going to test them against a testing data set. It's just that we call those testing data sets that we all kind of quote unquote agree on on benchmarks. And even benchmarks have their own problems. Not every benchmark is going to be actually useful to the lay person. You know, I talked about math Q&A earlier in this in this show. Not everybody cares about math word problems. That's fine. You don't need to pick the best model for math word problems if that's not your task. It's always going to come down to, can you effectively test your model against your data in a way that you can feel confident about? If you can't do that, there's not really an answer to the question, which model is best, because frankly, you're just kind of going off of vibes and, \"Hey, look, I, I'm here for the vibes. I'm here for, you know, whatever, whatever it takes to kind of get you excited about using the AI, but at the end of the day, you want to have some level of confidence to be able to point to and say, 'This is better because this number is better, higher or lower, depending on what it is, than this number.'\"\n\nEnd of sentence. Okay, those are the three questions that I had for today. It's a pretty short episode, but I, we're just kind of testing out this format. If you do like it, go ahead and head over to practicallyintelligent.com, send us your question, and I'll answer it right on this show for you. Until then, we will await the glorious return of Aay, and we can get back to our regularly scheduled programs, but I did hope you enjoyed our Office Hours, uh, with me, Sinan Ozer. I'll see you next time. I'm Practically Intelligent.\n[Music]\na\n",
  "dumpedAt": "2025-07-21T18:43:26.472Z"
}