{
  "episodeId": "VMj-3S1tku0",
  "channelSlug": "@andrejkarpathy",
  "title": "The spelled-out intro to neural networks and backpropagation: building micrograd",
  "publishedAt": "2022-08-16T22:44:26.000Z",
  "rawLines": [
    {
      "lang": "en",
      "text": "hello my name is andre",
      "offset": 0.08,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "and i've been training deep neural",
      "offset": 1.839,
      "duration": 3.041
    },
    {
      "lang": "en",
      "text": "networks for a bit more than a decade",
      "offset": 2.96,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "and in this lecture i'd like to show you",
      "offset": 4.88,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "what neural network training looks like",
      "offset": 6.799,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "under the hood so in particular we are",
      "offset": 8.639,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "going to start with a blank jupiter",
      "offset": 10.88,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "notebook and by the end of this lecture",
      "offset": 12.24,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "we will define and train in neural net",
      "offset": 14.32,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "and you'll get to see everything that",
      "offset": 16.64,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "goes on under the hood and exactly",
      "offset": 18.16,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "sort of how that works on an intuitive",
      "offset": 20.24,
      "duration": 2.4
    },
    {
      "lang": "en",
      "text": "level",
      "offset": 21.68,
      "duration": 2.48
    },
    {
      "lang": "en",
      "text": "now specifically what i would like to do",
      "offset": 22.64,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "is i would like to take you through",
      "offset": 24.16,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "building of micrograd now micrograd is",
      "offset": 26.56,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "this library that i released on github",
      "offset": 29.359,
      "duration": 3.521
    },
    {
      "lang": "en",
      "text": "about two years ago but at the time i",
      "offset": 30.88,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "only uploaded the source code and you'd",
      "offset": 32.88,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "have to go in by yourself and really",
      "offset": 34.88,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "figure out how it works",
      "offset": 37.28,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "so in this lecture i will take you",
      "offset": 39.28,
      "duration": 3.119
    },
    {
      "lang": "en",
      "text": "through it step by step and kind of",
      "offset": 40.64,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "comment on all the pieces of it so what",
      "offset": 42.399,
      "duration": 5.041
    },
    {
      "lang": "en",
      "text": "is micrograd and why is it interesting",
      "offset": 44.399,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "good",
      "offset": 47.44,
      "duration": 2.4
    },
    {
      "lang": "en",
      "text": "um",
      "offset": 48.96,
      "duration": 2.56
    },
    {
      "lang": "en",
      "text": "micrograd is basically an autograd",
      "offset": 49.84,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "engine autograd is short for automatic",
      "offset": 51.52,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "gradient and really what it does is it",
      "offset": 53.84,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "implements backpropagation now",
      "offset": 55.68,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "backpropagation is this algorithm that",
      "offset": 57.68,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "allows you to efficiently evaluate the",
      "offset": 59.359,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "gradient of",
      "offset": 61.28,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "some kind of a loss function with",
      "offset": 63.52,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "respect to the weights of a neural",
      "offset": 65.199,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "network and what that allows us to do",
      "offset": 67.2,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "then is we can iteratively tune the",
      "offset": 69.119,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "weights of that neural network to",
      "offset": 71.04,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "minimize the loss function and therefore",
      "offset": 72.56,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "improve the accuracy of the network so",
      "offset": 74.32,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "back propagation would be at the",
      "offset": 76.479,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "mathematical core of any modern deep",
      "offset": 78.32,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "neural network library like say pytorch",
      "offset": 80.479,
      "duration": 3.521
    },
    {
      "lang": "en",
      "text": "or jaxx",
      "offset": 82.56,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "so the functionality of microgrant is i",
      "offset": 84,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "think best illustrated by an example so",
      "offset": 85.6,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "if we just scroll down here",
      "offset": 87.6,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "you'll see that micrograph basically",
      "offset": 89.84,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "allows you to build out mathematical",
      "offset": 91.2,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "expressions",
      "offset": 92.96,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "and um here what we are doing is we have",
      "offset": 94.24,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "an expression that we're building out",
      "offset": 96.56,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "where you have two inputs a and b",
      "offset": 97.84,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "and you'll see that a and b are negative",
      "offset": 100.88,
      "duration": 5.279
    },
    {
      "lang": "en",
      "text": "four and two but we are wrapping those",
      "offset": 103.28,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "values into this value object that we",
      "offset": 106.159,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "are going to build out as part of",
      "offset": 108.64,
      "duration": 2.4
    },
    {
      "lang": "en",
      "text": "micrograd",
      "offset": 109.92,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "so this value object will wrap the",
      "offset": 111.04,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "numbers themselves",
      "offset": 113.52,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "and then we are going to build out a",
      "offset": 114.96,
      "duration": 3.519
    },
    {
      "lang": "en",
      "text": "mathematical expression here where a and",
      "offset": 116.24,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "b are transformed into c d and",
      "offset": 118.479,
      "duration": 5.441
    },
    {
      "lang": "en",
      "text": "eventually e f and g",
      "offset": 121.92,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "and i'm showing some of the functions",
      "offset": 123.92,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "some of the functionality of micrograph",
      "offset": 125.6,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "and the operations that it supports so",
      "offset": 127.04,
      "duration": 3.999
    },
    {
      "lang": "en",
      "text": "you can add two value objects you can",
      "offset": 128.959,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "multiply them you can raise them to a",
      "offset": 131.039,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "constant power you can offset by one",
      "offset": 133.36,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "negate squash at zero",
      "offset": 135.76,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "square divide by constant divide by it",
      "offset": 138.64,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "etc",
      "offset": 141.52,
      "duration": 2.799
    },
    {
      "lang": "en",
      "text": "and so we're building out an expression",
      "offset": 142.64,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "graph with with these two inputs a and b",
      "offset": 144.319,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "and we're creating an output value of g",
      "offset": 147.36,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "and micrograd will in the background",
      "offset": 150.319,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "build out this entire mathematical",
      "offset": 152.64,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "expression so it will for example know",
      "offset": 154.16,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "that c is also a value",
      "offset": 156.4,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "c was a result of an addition operation",
      "offset": 158.72,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "and the",
      "offset": 161.36,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "child nodes of c are a and b because the",
      "offset": 162.8,
      "duration": 6.079
    },
    {
      "lang": "en",
      "text": "and will maintain pointers to a and b",
      "offset": 166.64,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "value objects so we'll basically know",
      "offset": 168.879,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "exactly how all of this is laid out",
      "offset": 170.8,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "and then not only can we do what we call",
      "offset": 173.36,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "the forward pass where we actually look",
      "offset": 175.44,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "at the value of g of course that's",
      "offset": 177.04,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "pretty straightforward we will access",
      "offset": 178.64,
      "duration": 4.959
    },
    {
      "lang": "en",
      "text": "that using the dot data attribute and so",
      "offset": 180.56,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "the output of the forward pass the value",
      "offset": 183.599,
      "duration": 5.761
    },
    {
      "lang": "en",
      "text": "of g is 24.7 it turns out but the big",
      "offset": 186.08,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "deal is that we can also take this g",
      "offset": 189.36,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "value object and we can call that",
      "offset": 191.92,
      "duration": 2.959
    },
    {
      "lang": "en",
      "text": "backward",
      "offset": 193.84,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "and this will basically uh initialize",
      "offset": 194.879,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "back propagation at the node g",
      "offset": 196.72,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "and what backpropagation is going to do",
      "offset": 199.84,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "is it's going to start at g and it's",
      "offset": 201.36,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "going to go backwards through that",
      "offset": 203.2,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "expression graph and it's going to",
      "offset": 205.2,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "recursively apply the chain rule from",
      "offset": 206.72,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "calculus",
      "offset": 208.799,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "and what that allows us to do then is",
      "offset": 210.08,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "we're going to evaluate basically the",
      "offset": 212.239,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "derivative of g with respect to all the",
      "offset": 214.239,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "internal nodes",
      "offset": 216.56,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "like e d and c but also with respect to",
      "offset": 218.159,
      "duration": 5.041
    },
    {
      "lang": "en",
      "text": "the inputs a and b",
      "offset": 220.959,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "and then we can actually query this",
      "offset": 223.2,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "derivative of g with respect to a for",
      "offset": 225.36,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "example that's a dot grad in this case",
      "offset": 227.92,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "it happens to be 138 and the derivative",
      "offset": 230.159,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "of g with respect to b",
      "offset": 232.48,
      "duration": 4.959
    },
    {
      "lang": "en",
      "text": "which also happens to be here 645",
      "offset": 234.239,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "and this derivative we'll see soon is",
      "offset": 237.439,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "very important information because it's",
      "offset": 239.439,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "telling us how a and b are affecting g",
      "offset": 241.36,
      "duration": 5.519
    },
    {
      "lang": "en",
      "text": "through this mathematical expression so",
      "offset": 244.879,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "in particular",
      "offset": 246.879,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "a dot grad is 138 so if we slightly",
      "offset": 248.08,
      "duration": 6.719
    },
    {
      "lang": "en",
      "text": "nudge a and make it slightly larger",
      "offset": 251.84,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "138 is telling us that g will grow and",
      "offset": 254.799,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "the slope of that growth is going to be",
      "offset": 258,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "138",
      "offset": 259.44,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "and the slope of growth of b is going to",
      "offset": 260.72,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "be 645. so that's going to tell us about",
      "offset": 262.96,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "how g will respond if a and b get",
      "offset": 265.6,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "tweaked a tiny amount in a positive",
      "offset": 267.84,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "direction",
      "offset": 269.759,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "okay",
      "offset": 271.12,
      "duration": 2.24
    },
    {
      "lang": "en",
      "text": "now you might be confused about what",
      "offset": 273.44,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "this expression is that we built out",
      "offset": 274.88,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "here and this expression by the way is",
      "offset": 276.8,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "completely meaningless i just made it up",
      "offset": 278.56,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "i'm just flexing about the kinds of",
      "offset": 280.72,
      "duration": 3.039
    },
    {
      "lang": "en",
      "text": "operations that are supported by",
      "offset": 282.24,
      "duration": 2.64
    },
    {
      "lang": "en",
      "text": "micrograd",
      "offset": 283.759,
      "duration": 2.561
    },
    {
      "lang": "en",
      "text": "what we actually really care about are",
      "offset": 284.88,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "neural networks but it turns out that",
      "offset": 286.32,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "neural networks are just mathematical",
      "offset": 288.08,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "expressions just like this one but",
      "offset": 289.84,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "actually slightly bit less crazy even",
      "offset": 291.919,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "neural networks are just a mathematical",
      "offset": 294.88,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "expression they take the input data as",
      "offset": 296.32,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "an input and they take the weights of a",
      "offset": 299.04,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "neural network as an input and it's a",
      "offset": 300.72,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "mathematical expression and the output",
      "offset": 302.479,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "are your predictions of your neural net",
      "offset": 304.56,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "or the loss function we'll see this in a",
      "offset": 306.32,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "bit but basically neural networks just",
      "offset": 308.16,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "happen to be a certain class of",
      "offset": 310.56,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "mathematical expressions",
      "offset": 312.08,
      "duration": 3.119
    },
    {
      "lang": "en",
      "text": "but back propagation is actually",
      "offset": 313.759,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "significantly more general it doesn't",
      "offset": 315.199,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "actually care about neural networks at",
      "offset": 317.12,
      "duration": 3.519
    },
    {
      "lang": "en",
      "text": "all it only tells us about arbitrary",
      "offset": 318.8,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "mathematical expressions and then we",
      "offset": 320.639,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "happen to use that machinery for",
      "offset": 322.4,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "training of neural networks now one more",
      "offset": 324.479,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "note i would like to make at this stage",
      "offset": 326.8,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "is that as you see here micrograd is a",
      "offset": 328.4,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "scalar valued auto grant engine so it's",
      "offset": 330.24,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "working on the you know level of",
      "offset": 332.639,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "individual scalars like negative four",
      "offset": 334.24,
      "duration": 3.519
    },
    {
      "lang": "en",
      "text": "and two and we're taking neural nets and",
      "offset": 336,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "we're breaking them down all the way to",
      "offset": 337.759,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "these atoms of individual scalars and",
      "offset": 339.28,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "all the little pluses and times and it's",
      "offset": 341.36,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "just excessive and so obviously you",
      "offset": 343.12,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "would never be doing any of this in",
      "offset": 345.6,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "production it's really just put down for",
      "offset": 347.12,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "pedagogical reasons because it allows us",
      "offset": 348.8,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "to not have to deal with these",
      "offset": 350.8,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "n-dimensional tensors that you would use",
      "offset": 352.32,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "in modern deep neural network library so",
      "offset": 354.56,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "this is really done so that you",
      "offset": 356.96,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "understand and refactor out back",
      "offset": 358.96,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "propagation and chain rule and",
      "offset": 360.72,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "understanding of neurologic training",
      "offset": 362.72,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "and then if you actually want to train",
      "offset": 364.96,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "bigger networks you have to be using",
      "offset": 366.72,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "these tensors but none of the math",
      "offset": 368.08,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "changes this is done purely for",
      "offset": 369.84,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "efficiency we are basically taking scale",
      "offset": 371.28,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "value",
      "offset": 373.44,
      "duration": 2.56
    },
    {
      "lang": "en",
      "text": "all the scale values we're packaging",
      "offset": 374.24,
      "duration": 3.519
    },
    {
      "lang": "en",
      "text": "them up into tensors which are just",
      "offset": 376,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "arrays of these scalars and then because",
      "offset": 377.759,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "we have these large arrays we're making",
      "offset": 380.08,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "operations on those large arrays that",
      "offset": 382.4,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "allows us to take advantage of the",
      "offset": 384.4,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "parallelism in a computer and all those",
      "offset": 386.08,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "operations can be done in parallel and",
      "offset": 388.639,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "then the whole thing runs faster but",
      "offset": 390.479,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "really none of the math changes and",
      "offset": 392.4,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "that's done purely for efficiency so i",
      "offset": 393.68,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "don't think that it's pedagogically",
      "offset": 395.68,
      "duration": 2.959
    },
    {
      "lang": "en",
      "text": "useful to be dealing with tensors from",
      "offset": 396.88,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "scratch uh and i think and that's why i",
      "offset": 398.639,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "fundamentally wrote micrograd because",
      "offset": 400.72,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "you can understand how things work uh at",
      "offset": 402.56,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "the fundamental level and then you can",
      "offset": 404.479,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "speed it up later okay so here's the fun",
      "offset": 406.4,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "part my claim is that micrograd is what",
      "offset": 408.88,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "you need to train your networks and",
      "offset": 411.199,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "everything else is just efficiency so",
      "offset": 412.8,
      "duration": 3.519
    },
    {
      "lang": "en",
      "text": "you'd think that micrograd would be a",
      "offset": 414.96,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "very complex piece of code and that",
      "offset": 416.319,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "turns out to not be the case",
      "offset": 418.56,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "so if we just go to micrograd",
      "offset": 421.039,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "and you'll see that there's only two",
      "offset": 423.199,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "files here in micrograd this is the",
      "offset": 425.039,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "actual engine it doesn't know anything",
      "offset": 427.36,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "about neural nuts and this is the entire",
      "offset": 429.12,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "neural nets library",
      "offset": 430.88,
      "duration": 6.319
    },
    {
      "lang": "en",
      "text": "on top of micrograd so engine and nn.pi",
      "offset": 432.72,
      "duration": 7.12
    },
    {
      "lang": "en",
      "text": "so the actual backpropagation autograd",
      "offset": 437.199,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "engine",
      "offset": 439.84,
      "duration": 2.56
    },
    {
      "lang": "en",
      "text": "that gives you the power of neural",
      "offset": 441.199,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "networks is literally",
      "offset": 442.4,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "100 lines of code of like very simple",
      "offset": 446.08,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "python",
      "offset": 448.4,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "which we'll understand by the end of",
      "offset": 450,
      "duration": 2.08
    },
    {
      "lang": "en",
      "text": "this lecture",
      "offset": 451.12,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "and then nn.pi",
      "offset": 452.08,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "this neural network library built on top",
      "offset": 453.84,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "of the autograd engine",
      "offset": 455.68,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "um is like a joke it's like",
      "offset": 457.759,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "we have to define what is a neuron and",
      "offset": 460.56,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "then we have to define what is the layer",
      "offset": 462.72,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "of neurons and then we define what is a",
      "offset": 464,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "multi-layer perceptron which is just a",
      "offset": 466.08,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "sequence of layers of neurons and so",
      "offset": 467.84,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "it's just a total joke",
      "offset": 470.4,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "so basically",
      "offset": 472,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "there's a lot of power that comes from",
      "offset": 473.44,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "only 150 lines of code",
      "offset": 475.68,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "and that's all you need to understand to",
      "offset": 477.599,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "understand neural network training and",
      "offset": 479.12,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "everything else is just efficiency and",
      "offset": 480.879,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "of course there's a lot to efficiency",
      "offset": 482.72,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "but fundamentally that's all that's",
      "offset": 485.68,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "happening okay so now let's dive right",
      "offset": 487.12,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "in and implement micrograph step by step",
      "offset": 489.12,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "the first thing i'd like to do is i'd",
      "offset": 491.28,
      "duration": 2.479
    },
    {
      "lang": "en",
      "text": "like to make sure that you have a very",
      "offset": 492.4,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "good understanding intuitively of what a",
      "offset": 493.759,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "derivative is and exactly what",
      "offset": 496.24,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "information it gives you so let's start",
      "offset": 498.24,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "with some basic imports that i copy",
      "offset": 500.56,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "paste in every jupiter notebook always",
      "offset": 502.319,
      "duration": 5.041
    },
    {
      "lang": "en",
      "text": "and let's define a function a scalar",
      "offset": 505.28,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "valued function",
      "offset": 507.36,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "f of x",
      "offset": 508.96,
      "duration": 2.4
    },
    {
      "lang": "en",
      "text": "as follows",
      "offset": 510.24,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "so i just make this up randomly i just",
      "offset": 511.36,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "want to scale a valid function that",
      "offset": 513.2,
      "duration": 3.519
    },
    {
      "lang": "en",
      "text": "takes a single scalar x and returns a",
      "offset": 514.56,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "single scalar y",
      "offset": 516.719,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "and we can call this function of course",
      "offset": 518.56,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "so we can pass in say 3.0 and get 20",
      "offset": 520.08,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "back",
      "offset": 522.8,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "now we can also plot this function to",
      "offset": 523.919,
      "duration": 3.521
    },
    {
      "lang": "en",
      "text": "get a sense of its shape you can tell",
      "offset": 525.6,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "from the mathematical expression that",
      "offset": 527.44,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "this is probably a parabola it's a",
      "offset": 528.88,
      "duration": 2.959
    },
    {
      "lang": "en",
      "text": "quadratic",
      "offset": 530.72,
      "duration": 5.679
    },
    {
      "lang": "en",
      "text": "and so if we just uh create a set of um",
      "offset": 531.839,
      "duration": 5.841
    },
    {
      "lang": "en",
      "text": "um",
      "offset": 536.399,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "scale values that we can feed in using",
      "offset": 537.68,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "for example a range from negative five",
      "offset": 539.519,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "to five in steps of 0.25",
      "offset": 541.12,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "so this is so axis is just from negative",
      "offset": 543.92,
      "duration": 7.44
    },
    {
      "lang": "en",
      "text": "5 to 5 not including 5 in steps of 0.25",
      "offset": 546.64,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "and we can actually call this function",
      "offset": 551.36,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "on this numpy array as well so we get a",
      "offset": 552.72,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "set of y's if we call f on axis",
      "offset": 554.64,
      "duration": 5.759
    },
    {
      "lang": "en",
      "text": "and these y's are basically",
      "offset": 557.68,
      "duration": 5.839
    },
    {
      "lang": "en",
      "text": "also applying a function on every one of",
      "offset": 560.399,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "these elements independently",
      "offset": 563.519,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "and we can plot this using matplotlib so",
      "offset": 565.36,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "plt.plot x's and y's and we get a nice",
      "offset": 568,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "parabola so previously here we fed in",
      "offset": 571.2,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "3.0 somewhere here and we received 20",
      "offset": 573.76,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "back which is here the y coordinate so",
      "offset": 576.56,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "now i'd like to think through",
      "offset": 579.04,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "what is the derivative",
      "offset": 580.64,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "of this function at any single input",
      "offset": 582.399,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "point x",
      "offset": 584.32,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "right so what is the derivative at",
      "offset": 585.519,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "different points x of this function now",
      "offset": 587.04,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "if you remember back to your calculus",
      "offset": 589.839,
      "duration": 2.401
    },
    {
      "lang": "en",
      "text": "class you've probably derived",
      "offset": 591.2,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "derivatives so we take this mathematical",
      "offset": 592.24,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "expression 3x squared minus 4x plus 5",
      "offset": 594.64,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "and you would write out on a piece of",
      "offset": 597.2,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "paper and you would you know apply the",
      "offset": 598.399,
      "duration": 2.961
    },
    {
      "lang": "en",
      "text": "product rule and all the other rules and",
      "offset": 599.92,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "derive the mathematical expression of",
      "offset": 601.36,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "the great derivative of the original",
      "offset": 603.44,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "function and then you could plug in",
      "offset": 605.44,
      "duration": 2.56
    },
    {
      "lang": "en",
      "text": "different texts and see what the",
      "offset": 606.72,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "derivative is",
      "offset": 608,
      "duration": 3.519
    },
    {
      "lang": "en",
      "text": "we're not going to actually do that",
      "offset": 609.76,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "because no one in neural networks",
      "offset": 611.519,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "actually writes out the expression for",
      "offset": 613.68,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "the neural net it would be a massive",
      "offset": 615.36,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "expression um it would be you know",
      "offset": 616.64,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "thousands tens of thousands of terms no",
      "offset": 618.88,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "one actually derives the derivative of",
      "offset": 620.48,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "course and so we're not going to take",
      "offset": 622.64,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "this kind of like a symbolic approach",
      "offset": 624.399,
      "duration": 2.961
    },
    {
      "lang": "en",
      "text": "instead what i'd like to do is i'd like",
      "offset": 626.16,
      "duration": 3.119
    },
    {
      "lang": "en",
      "text": "to look at the definition of derivative",
      "offset": 627.36,
      "duration": 3.039
    },
    {
      "lang": "en",
      "text": "and just make sure that we really",
      "offset": 629.279,
      "duration": 3.041
    },
    {
      "lang": "en",
      "text": "understand what derivative is measuring",
      "offset": 630.399,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "what it's telling you about the function",
      "offset": 632.32,
      "duration": 6.32
    },
    {
      "lang": "en",
      "text": "and so if we just look up derivative",
      "offset": 634.8,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "we see that",
      "offset": 642.32,
      "duration": 2.32
    },
    {
      "lang": "en",
      "text": "okay so this is not a very good",
      "offset": 643.519,
      "duration": 2.56
    },
    {
      "lang": "en",
      "text": "definition of derivative this is a",
      "offset": 644.64,
      "duration": 2.639
    },
    {
      "lang": "en",
      "text": "definition of what it means to be",
      "offset": 646.079,
      "duration": 2.401
    },
    {
      "lang": "en",
      "text": "differentiable",
      "offset": 647.279,
      "duration": 2.881
    },
    {
      "lang": "en",
      "text": "but if you remember from your calculus",
      "offset": 648.48,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "it is the limit as h goes to zero of f",
      "offset": 650.16,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "of x plus h minus f of x over h so",
      "offset": 652.56,
      "duration": 5.839
    },
    {
      "lang": "en",
      "text": "basically what it's saying is if you",
      "offset": 655.92,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "slightly bump up you're at some point x",
      "offset": 658.399,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "that you're interested in or a and if",
      "offset": 660.959,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "you slightly bump up",
      "offset": 662.88,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "you know you slightly increase it by",
      "offset": 664.56,
      "duration": 3.519
    },
    {
      "lang": "en",
      "text": "small number h",
      "offset": 666.079,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "how does the function respond with what",
      "offset": 668.079,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "sensitivity does it respond what is the",
      "offset": 669.76,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "slope at that point does the function go",
      "offset": 671.44,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "up or does it go down and by how much",
      "offset": 673.6,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "and that's the slope of that function",
      "offset": 676.32,
      "duration": 2.4
    },
    {
      "lang": "en",
      "text": "the",
      "offset": 678,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "the slope of that response at that point",
      "offset": 678.72,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "and so we can basically evaluate",
      "offset": 681.76,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "the derivative here numerically by",
      "offset": 683.92,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "taking a very small h of course the",
      "offset": 686.32,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "definition would ask us to take h to",
      "offset": 688,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "zero we're just going to pick a very",
      "offset": 690.16,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "small h 0.001",
      "offset": 691.76,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "and let's say we're interested in point",
      "offset": 694,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "3.0 so we can look at f of x of course",
      "offset": 695.12,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "as 20",
      "offset": 697.76,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "and now f of x plus h",
      "offset": 698.959,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "so if we slightly nudge x in a positive",
      "offset": 700.959,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "direction how is the function going to",
      "offset": 702.8,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "respond",
      "offset": 704.56,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "and just looking at this do you expect",
      "offset": 705.6,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "do you expect f of x plus h to be",
      "offset": 707.68,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "slightly greater than 20 or do you",
      "offset": 709.2,
      "duration": 5.439
    },
    {
      "lang": "en",
      "text": "expect to be slightly lower than 20",
      "offset": 711.92,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "and since this 3 is here and this is 20",
      "offset": 714.639,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "if we slightly go positively the",
      "offset": 717.04,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "function will respond positively so",
      "offset": 719.04,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "you'd expect this to be slightly greater",
      "offset": 721.36,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "than 20. and now by how much it's",
      "offset": 723.04,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "telling you the",
      "offset": 725.36,
      "duration": 2.479
    },
    {
      "lang": "en",
      "text": "sort of the",
      "offset": 726.48,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "the strength of that slope right the the",
      "offset": 727.839,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "size of the slope so f of x plus h minus",
      "offset": 729.92,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "f of x this is how much the function",
      "offset": 732.8,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "responded",
      "offset": 734.8,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "in the positive direction and we have to",
      "offset": 736,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "normalize by the",
      "offset": 737.92,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "run so we have the rise over run to get",
      "offset": 739.92,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "the slope so this of course is just a",
      "offset": 742.32,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "numerical approximation of the slope",
      "offset": 744.8,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "because we have to make age very very",
      "offset": 746.8,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "small to converge to the exact amount",
      "offset": 748.88,
      "duration": 6.639
    },
    {
      "lang": "en",
      "text": "now if i'm doing too many zeros",
      "offset": 752.8,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "at some point",
      "offset": 755.519,
      "duration": 2.801
    },
    {
      "lang": "en",
      "text": "i'm gonna get an incorrect answer",
      "offset": 756.8,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "because we're using floating point",
      "offset": 758.32,
      "duration": 3.519
    },
    {
      "lang": "en",
      "text": "arithmetic and the representations of",
      "offset": 759.6,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "all these numbers in computer memory is",
      "offset": 761.839,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "finite and at some point we get into",
      "offset": 763.76,
      "duration": 2.639
    },
    {
      "lang": "en",
      "text": "trouble",
      "offset": 765.6,
      "duration": 2.16
    },
    {
      "lang": "en",
      "text": "so we can converse towards the right",
      "offset": 766.399,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "answer with this approach",
      "offset": 767.76,
      "duration": 6.8
    },
    {
      "lang": "en",
      "text": "but basically um at 3 the slope is 14.",
      "offset": 770.399,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "and you can see that by taking 3x",
      "offset": 774.56,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "squared minus 4x plus 5 and",
      "offset": 776.079,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "differentiating it in our head",
      "offset": 778.32,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "so 3x squared would be",
      "offset": 780.48,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "6 x minus 4",
      "offset": 782.639,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "and then we plug in x equals 3 so that's",
      "offset": 784.88,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "18 minus 4 is 14. so this is correct",
      "offset": 787.279,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "so that's",
      "offset": 790.88,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "at 3. now how about the slope at say",
      "offset": 792.079,
      "duration": 5.281
    },
    {
      "lang": "en",
      "text": "negative 3",
      "offset": 795.76,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "would you expect would you expect for",
      "offset": 797.36,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "the slope",
      "offset": 799.519,
      "duration": 2.801
    },
    {
      "lang": "en",
      "text": "now telling the exact value is really",
      "offset": 800.48,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "hard but what is the sign of that slope",
      "offset": 802.32,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "so at negative three",
      "offset": 804.959,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "if we slightly go in the positive",
      "offset": 806.8,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "direction at x the function would",
      "offset": 808.399,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "actually go down and so that tells you",
      "offset": 810.72,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "that the slope would be negative so",
      "offset": 812.56,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "we'll get a slight number below",
      "offset": 813.92,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "below 20. and so if we take the slope we",
      "offset": 816.72,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "expect something negative",
      "offset": 819.12,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "negative 22. okay",
      "offset": 820.88,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "and at some point here of course the",
      "offset": 823.839,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "slope would be zero now for this",
      "offset": 825.519,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "specific function i looked it up",
      "offset": 827.6,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "previously and it's at point two over",
      "offset": 828.88,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "three",
      "offset": 831.279,
      "duration": 3.041
    },
    {
      "lang": "en",
      "text": "so at roughly two over three",
      "offset": 832.16,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "uh that's somewhere here",
      "offset": 834.32,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "um",
      "offset": 835.92,
      "duration": 3.279
    },
    {
      "lang": "en",
      "text": "this derivative be zero",
      "offset": 837.12,
      "duration": 6.32
    },
    {
      "lang": "en",
      "text": "so basically at that precise point",
      "offset": 839.199,
      "duration": 5.041
    },
    {
      "lang": "en",
      "text": "yeah",
      "offset": 843.44,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "at that precise point if we nudge in a",
      "offset": 844.24,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "positive direction the function doesn't",
      "offset": 846.32,
      "duration": 3.519
    },
    {
      "lang": "en",
      "text": "respond this stays the same almost and",
      "offset": 847.76,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "so that's why the slope is zero okay now",
      "offset": 849.839,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "let's look at a bit more complex case",
      "offset": 851.839,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "so we're going to start you know",
      "offset": 854.32,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "complexifying a bit so now we have a",
      "offset": 855.839,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "function",
      "offset": 858,
      "duration": 2.959
    },
    {
      "lang": "en",
      "text": "here",
      "offset": 859.68,
      "duration": 2.959
    },
    {
      "lang": "en",
      "text": "with output variable d",
      "offset": 860.959,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "that is a function of three scalar",
      "offset": 862.639,
      "duration": 3.521
    },
    {
      "lang": "en",
      "text": "inputs a b and c",
      "offset": 864.16,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "so a b and c are some specific values",
      "offset": 866.16,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "three inputs into our expression graph",
      "offset": 868.48,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "and a single output d",
      "offset": 870.639,
      "duration": 5.681
    },
    {
      "lang": "en",
      "text": "and so if we just print d we get four",
      "offset": 872.8,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "and now what i have to do is i'd like to",
      "offset": 876.32,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "again look at the derivatives of d with",
      "offset": 878.16,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "respect to a b and c",
      "offset": 880.24,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "and uh think through uh again just the",
      "offset": 882.48,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "intuition of what this derivative is",
      "offset": 884.88,
      "duration": 2.639
    },
    {
      "lang": "en",
      "text": "telling us",
      "offset": 886.24,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "so in order to evaluate this derivative",
      "offset": 887.519,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "we're going to get a bit hacky here",
      "offset": 889.839,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "we're going to again have a very small",
      "offset": 892.079,
      "duration": 3.281
    },
    {
      "lang": "en",
      "text": "value of h",
      "offset": 893.92,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "and then we're going to fix the inputs",
      "offset": 895.36,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "at some",
      "offset": 897.36,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "values that we're interested in",
      "offset": 898.56,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "so these are the this is the point abc",
      "offset": 900.32,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "at which we're going to be evaluating",
      "offset": 902.959,
      "duration": 2.481
    },
    {
      "lang": "en",
      "text": "the the",
      "offset": 904.24,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "derivative of d with respect to all a b",
      "offset": 905.44,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "and c at that point",
      "offset": 907.76,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "so there are the inputs and now we have",
      "offset": 909.76,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "d1 is that expression",
      "offset": 911.36,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "and then we're going to for example look",
      "offset": 913.76,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "at the derivative of d with respect to a",
      "offset": 915.04,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "so we'll take a and we'll bump it by h",
      "offset": 917.12,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "and then we'll get d2 to be the exact",
      "offset": 919.92,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "same function",
      "offset": 922,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "and now we're going to print um",
      "offset": 923.839,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "you know f1",
      "offset": 926.8,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "d1 is d1",
      "offset": 928.72,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "d2 is d2",
      "offset": 931.12,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "and print slope",
      "offset": 932.88,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "so the derivative or slope",
      "offset": 935.199,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "here will be um",
      "offset": 937.68,
      "duration": 3.519
    },
    {
      "lang": "en",
      "text": "of course",
      "offset": 939.68,
      "duration": 2.399
    },
    {
      "lang": "en",
      "text": "d2",
      "offset": 941.199,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "minus d1 divide h",
      "offset": 942.079,
      "duration": 5.361
    },
    {
      "lang": "en",
      "text": "so d2 minus d1 is how much the function",
      "offset": 944.399,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "increased",
      "offset": 947.44,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "uh when we bumped",
      "offset": 948.72,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "the uh",
      "offset": 950.72,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "the specific input that we're interested",
      "offset": 951.92,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "in by a tiny amount",
      "offset": 953.44,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "and",
      "offset": 955.839,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "this is then normalized by h",
      "offset": 956.72,
      "duration": 5.359
    },
    {
      "lang": "en",
      "text": "to get the slope",
      "offset": 959.199,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "so",
      "offset": 962.8,
      "duration": 2.32
    },
    {
      "lang": "en",
      "text": "um",
      "offset": 963.6,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "yeah",
      "offset": 965.12,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "so this so if i just run this we're",
      "offset": 966.32,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "going to print",
      "offset": 968.88,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "d1",
      "offset": 970.399,
      "duration": 5.041
    },
    {
      "lang": "en",
      "text": "which we know is four",
      "offset": 972.24,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "now d2 will be bumped a will be bumped",
      "offset": 975.44,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "by h",
      "offset": 978.48,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "so let's just think through",
      "offset": 980.32,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "a little bit uh what d2 will be uh",
      "offset": 982.48,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "printed out here",
      "offset": 986.16,
      "duration": 3.119
    },
    {
      "lang": "en",
      "text": "in particular",
      "offset": 987.6,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "d1 will be four",
      "offset": 989.279,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "will d2 be a number slightly greater",
      "offset": 991.12,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "than four or slightly lower than four",
      "offset": 993.519,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "and that's going to tell us the sl the",
      "offset": 995.68,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "the sign of the derivative",
      "offset": 997.6,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "so",
      "offset": 1000.16,
      "duration": 2.239
    },
    {
      "lang": "en",
      "text": "we're bumping a by h",
      "offset": 1002.56,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "b as minus three c is ten",
      "offset": 1005.44,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "so you can just intuitively think",
      "offset": 1008.56,
      "duration": 2.56
    },
    {
      "lang": "en",
      "text": "through this derivative and what it's",
      "offset": 1009.92,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "doing a will be slightly more positive",
      "offset": 1011.12,
      "duration": 6.399
    },
    {
      "lang": "en",
      "text": "and but b is a negative number",
      "offset": 1014.8,
      "duration": 5.599
    },
    {
      "lang": "en",
      "text": "so if a is slightly more positive",
      "offset": 1017.519,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "because b is negative three",
      "offset": 1020.399,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "we're actually going to be adding less",
      "offset": 1023.199,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "to d",
      "offset": 1026.079,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "so you'd actually expect that the value",
      "offset": 1028.16,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "of the function will go down",
      "offset": 1030.64,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "so let's just see this",
      "offset": 1033.76,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "yeah and so we went from 4",
      "offset": 1036.48,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "to 3.9996",
      "offset": 1038.4,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "and that tells you that the slope will",
      "offset": 1040.799,
      "duration": 2.481
    },
    {
      "lang": "en",
      "text": "be negative",
      "offset": 1042.16,
      "duration": 2.32
    },
    {
      "lang": "en",
      "text": "and then",
      "offset": 1043.28,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "uh will be a negative number",
      "offset": 1044.48,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "because we went down",
      "offset": 1046.4,
      "duration": 2.639
    },
    {
      "lang": "en",
      "text": "and then",
      "offset": 1047.919,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "the exact number of slope will be",
      "offset": 1049.039,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "exact amount of slope is negative 3.",
      "offset": 1051.039,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "and you can also convince yourself that",
      "offset": 1053.76,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "negative 3 is the right answer",
      "offset": 1055.12,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "mathematically and analytically because",
      "offset": 1056.96,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "if you have a times b plus c and you are",
      "offset": 1059.039,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "you know you have calculus then",
      "offset": 1061.6,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "differentiating a times b plus c with",
      "offset": 1063.84,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "respect to a gives you just b",
      "offset": 1066,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "and indeed the value of b is negative 3",
      "offset": 1068.48,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "which is the derivative that we have so",
      "offset": 1070.799,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "you can tell that that's correct",
      "offset": 1072.799,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "so now if we do this with b",
      "offset": 1074.96,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "so if we bump b by a little bit in a",
      "offset": 1077.76,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "positive direction we'd get different",
      "offset": 1079.84,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "slopes so what is the influence of b on",
      "offset": 1082.24,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "the output d",
      "offset": 1084.559,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "so if we bump b by a tiny amount in a",
      "offset": 1086.32,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "positive direction then because a is",
      "offset": 1088.24,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "positive",
      "offset": 1090.16,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "we'll be adding more to d",
      "offset": 1091.6,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "right",
      "offset": 1093.76,
      "duration": 3.279
    },
    {
      "lang": "en",
      "text": "so um and now what is the what is the",
      "offset": 1094.72,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "sensitivity what is the slope of that",
      "offset": 1097.039,
      "duration": 2.801
    },
    {
      "lang": "en",
      "text": "addition",
      "offset": 1098.799,
      "duration": 2.721
    },
    {
      "lang": "en",
      "text": "and it might not surprise you that this",
      "offset": 1099.84,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "should be",
      "offset": 1101.52,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "2",
      "offset": 1102.96,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "and y is a 2 because d of d",
      "offset": 1104.24,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "by db differentiating with respect to b",
      "offset": 1107.679,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "would be would give us a",
      "offset": 1110.16,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "and the value of a is two so that's also",
      "offset": 1111.919,
      "duration": 3.521
    },
    {
      "lang": "en",
      "text": "working well",
      "offset": 1114.08,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "and then if c gets bumped a tiny amount",
      "offset": 1115.44,
      "duration": 3.119
    },
    {
      "lang": "en",
      "text": "in h",
      "offset": 1117.44,
      "duration": 2.479
    },
    {
      "lang": "en",
      "text": "by h",
      "offset": 1118.559,
      "duration": 3.281
    },
    {
      "lang": "en",
      "text": "then of course a times b is unaffected",
      "offset": 1119.919,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "and now c becomes slightly bit higher",
      "offset": 1121.84,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "what does that do to the function it",
      "offset": 1124.24,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "makes it slightly bit higher because",
      "offset": 1125.919,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "we're simply adding c",
      "offset": 1127.28,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "and it makes it slightly bit higher by",
      "offset": 1128.799,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "the exact same amount that we added to c",
      "offset": 1130.48,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "and so that tells you that the slope is",
      "offset": 1133.12,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "one",
      "offset": 1135.2,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "that will be the",
      "offset": 1136.559,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "the rate at which",
      "offset": 1139.2,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "d will increase as we scale",
      "offset": 1141.039,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "c",
      "offset": 1144.16,
      "duration": 2.639
    },
    {
      "lang": "en",
      "text": "okay so we now have some intuitive sense",
      "offset": 1145.12,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "of what this derivative is telling you",
      "offset": 1146.799,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "about the function and we'd like to move",
      "offset": 1148.24,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "to neural networks now as i mentioned",
      "offset": 1150,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "neural networks will be pretty massive",
      "offset": 1151.6,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "expressions mathematical expressions so",
      "offset": 1153.12,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "we need some data structures that",
      "offset": 1155.2,
      "duration": 2.719
    },
    {
      "lang": "en",
      "text": "maintain these expressions and that's",
      "offset": 1156.48,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "what we're going to start to build out",
      "offset": 1157.919,
      "duration": 2.561
    },
    {
      "lang": "en",
      "text": "now",
      "offset": 1159.36,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "so we're going to",
      "offset": 1160.48,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "build out this value object that i",
      "offset": 1162.24,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "showed you in the readme page of",
      "offset": 1164.08,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "micrograd",
      "offset": 1166.24,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "so let me copy paste a skeleton of the",
      "offset": 1167.52,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "first very simple value object",
      "offset": 1170.72,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "so class value takes a single",
      "offset": 1173.52,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "scalar value that it wraps and keeps",
      "offset": 1176.4,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "track of",
      "offset": 1178.32,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "and that's it so",
      "offset": 1179.28,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "we can for example do value of 2.0 and",
      "offset": 1181.44,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "then we can",
      "offset": 1183.6,
      "duration": 4.959
    },
    {
      "lang": "en",
      "text": "get we can look at its content and",
      "offset": 1185.44,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "python will internally",
      "offset": 1188.559,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "use the wrapper function",
      "offset": 1190.4,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "to uh return",
      "offset": 1192.48,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "uh this string oops",
      "offset": 1194.32,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "like that",
      "offset": 1196.88,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "so this is a value object with data",
      "offset": 1198.64,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "equals two that we're creating here",
      "offset": 1200.799,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "now we'd like to do is like we'd like to",
      "offset": 1203.2,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "be able to",
      "offset": 1204.96,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "have not just like two values",
      "offset": 1207.44,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "but we'd like to do a bluffy right we'd",
      "offset": 1210.08,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "like to add them",
      "offset": 1212,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "so currently you would get an error",
      "offset": 1213.679,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "because python doesn't know how to add",
      "offset": 1215.44,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "two value objects so we have to tell it",
      "offset": 1217.76,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "so here's",
      "offset": 1221.44,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "addition",
      "offset": 1222.48,
      "duration": 2.4
    },
    {
      "lang": "en",
      "text": "so you have to basically use these",
      "offset": 1226.32,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "special double underscore methods in",
      "offset": 1227.6,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "python to define these operators for",
      "offset": 1229.52,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "these objects so if we call um",
      "offset": 1231.84,
      "duration": 7.28
    },
    {
      "lang": "en",
      "text": "the uh if we use this plus operator",
      "offset": 1235.44,
      "duration": 7.599
    },
    {
      "lang": "en",
      "text": "python will internally call a dot add of",
      "offset": 1239.12,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "b",
      "offset": 1243.039,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "that's what will happen internally and",
      "offset": 1243.76,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "so b will be the other and",
      "offset": 1245.919,
      "duration": 5.041
    },
    {
      "lang": "en",
      "text": "self will be a",
      "offset": 1248.96,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "and so we see that what we're going to",
      "offset": 1250.96,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "return is a new value object and it's",
      "offset": 1252.159,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "just it's going to be wrapping",
      "offset": 1254.48,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "the plus of",
      "offset": 1256.72,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "their data",
      "offset": 1258.559,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "but remember now because data is the",
      "offset": 1259.76,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "actual like numbered python number so",
      "offset": 1262,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "this operator here is just the typical",
      "offset": 1264.32,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "floating point plus addition now it's",
      "offset": 1266.88,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "not an addition of value objects",
      "offset": 1269.12,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "and will return a new value so now a",
      "offset": 1271.6,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "plus b should work and it should print",
      "offset": 1274.24,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "value of",
      "offset": 1276,
      "duration": 2.4
    },
    {
      "lang": "en",
      "text": "negative one",
      "offset": 1277.2,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "because that's two plus minus three",
      "offset": 1278.4,
      "duration": 3.279
    },
    {
      "lang": "en",
      "text": "there we go",
      "offset": 1280.24,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "okay let's now implement multiply",
      "offset": 1281.679,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "just so we can recreate this expression",
      "offset": 1284.24,
      "duration": 2.559
    },
    {
      "lang": "en",
      "text": "here",
      "offset": 1285.919,
      "duration": 2.64
    },
    {
      "lang": "en",
      "text": "so multiply i think it won't surprise",
      "offset": 1286.799,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "you will be fairly similar",
      "offset": 1288.559,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "so instead of add we're going to be",
      "offset": 1291.6,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "using mul",
      "offset": 1293.2,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "and then here of course we want to do",
      "offset": 1294.32,
      "duration": 2.56
    },
    {
      "lang": "en",
      "text": "times",
      "offset": 1296,
      "duration": 2.64
    },
    {
      "lang": "en",
      "text": "and so now we can create a c value",
      "offset": 1296.88,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "object which will be 10.0 and now we",
      "offset": 1298.64,
      "duration": 5.519
    },
    {
      "lang": "en",
      "text": "should be able to do a times b well",
      "offset": 1301.039,
      "duration": 5.681
    },
    {
      "lang": "en",
      "text": "let's just do a times b first",
      "offset": 1304.159,
      "duration": 2.901
    },
    {
      "lang": "en",
      "text": "um",
      "offset": 1306.72,
      "duration": 1.76
    },
    {
      "lang": "en",
      "text": "[Music]",
      "offset": 1307.06,
      "duration": 3.66
    },
    {
      "lang": "en",
      "text": "that's value of negative six now",
      "offset": 1308.48,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "and by the way i skipped over this a",
      "offset": 1310.72,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "little bit suppose that i didn't have",
      "offset": 1312,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "the wrapper function here",
      "offset": 1313.52,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "then it's just that you'll get some kind",
      "offset": 1315.52,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "of an ugly expression so what wrapper is",
      "offset": 1317.44,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "doing is it's providing us a way to",
      "offset": 1319.679,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "print out like a nicer looking",
      "offset": 1322.08,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "expression in python",
      "offset": 1323.44,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "uh so we don't just have something",
      "offset": 1325.12,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "cryptic we actually are you know it's",
      "offset": 1327.2,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "value of",
      "offset": 1329.28,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "negative six so this gives us a times",
      "offset": 1330.48,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "and then this we should now be able to",
      "offset": 1334,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "add c to it because we've defined and",
      "offset": 1336.32,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "told the python how to do mul and add",
      "offset": 1338.08,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "and so this will call this will",
      "offset": 1340.559,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "basically be equivalent to a dot",
      "offset": 1342.4,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "small",
      "offset": 1344.88,
      "duration": 3.039
    },
    {
      "lang": "en",
      "text": "of b",
      "offset": 1346.88,
      "duration": 2.799
    },
    {
      "lang": "en",
      "text": "and then this new value object will be",
      "offset": 1347.919,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "dot add",
      "offset": 1349.679,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "of c",
      "offset": 1351.28,
      "duration": 3.519
    },
    {
      "lang": "en",
      "text": "and so let's see if that worked",
      "offset": 1352.559,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "yep so that worked well that gave us",
      "offset": 1354.799,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "four which is what we expect from before",
      "offset": 1356.4,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "and i believe we can just call them",
      "offset": 1359.2,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "manually as well there we go so",
      "offset": 1360.96,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "yeah",
      "offset": 1364.32,
      "duration": 2.56
    },
    {
      "lang": "en",
      "text": "okay so now what we are missing is the",
      "offset": 1365.12,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "connective tissue of this expression as",
      "offset": 1366.88,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "i mentioned we want to keep these",
      "offset": 1369.039,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "expression graphs so we need to know and",
      "offset": 1370.24,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "keep pointers about what values produce",
      "offset": 1372.559,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "what other values",
      "offset": 1374.96,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "so here for example we are going to",
      "offset": 1376.88,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "introduce a new variable which we'll",
      "offset": 1378.64,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "call children and by default it will be",
      "offset": 1380.08,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "an empty tuple",
      "offset": 1382.08,
      "duration": 2.719
    },
    {
      "lang": "en",
      "text": "and then we're actually going to keep a",
      "offset": 1383.52,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "slightly different variable in the class",
      "offset": 1384.799,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "which we'll call underscore prev which",
      "offset": 1386.4,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "will be the set of children",
      "offset": 1388.559,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "this is how i done i did it in the",
      "offset": 1391.52,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "original micrograd looking at my code",
      "offset": 1393.28,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "here i can't remember exactly the reason",
      "offset": 1395.12,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "i believe it was efficiency but this",
      "offset": 1397.12,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "underscore children will be a tuple for",
      "offset": 1399.12,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "convenience but then when we actually",
      "offset": 1400.96,
      "duration": 2.959
    },
    {
      "lang": "en",
      "text": "maintain it in the class it will be just",
      "offset": 1402.4,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "this set yeah i believe for efficiency",
      "offset": 1403.919,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "um",
      "offset": 1407.6,
      "duration": 2.319
    },
    {
      "lang": "en",
      "text": "so now",
      "offset": 1408.88,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "when we are creating a value like this",
      "offset": 1409.919,
      "duration": 3.521
    },
    {
      "lang": "en",
      "text": "with a constructor children will be",
      "offset": 1411.76,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "empty and prep will be the empty set but",
      "offset": 1413.44,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "when we're creating a value through",
      "offset": 1416.08,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "addition or multiplication we're going",
      "offset": 1417.36,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "to feed in the children of this value",
      "offset": 1419.36,
      "duration": 7.04
    },
    {
      "lang": "en",
      "text": "which in this case is self and other",
      "offset": 1422.64,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "so those are the children",
      "offset": 1426.4,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "here",
      "offset": 1428.08,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "so now we can do d dot prev",
      "offset": 1430.64,
      "duration": 5.279
    },
    {
      "lang": "en",
      "text": "and we'll see that the children of the",
      "offset": 1432.96,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "we now know are this value of negative 6",
      "offset": 1435.919,
      "duration": 5.041
    },
    {
      "lang": "en",
      "text": "and value of 10 and this of course is",
      "offset": 1438.88,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "the value resulting from a times b and",
      "offset": 1440.96,
      "duration": 5.599
    },
    {
      "lang": "en",
      "text": "the c value which is 10.",
      "offset": 1443.76,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "now the last piece of information we",
      "offset": 1446.559,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "don't know so we know that the children",
      "offset": 1448.559,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "of every single value but we don't know",
      "offset": 1450.64,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "what operation created this value",
      "offset": 1452.159,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "so we need one more element here let's",
      "offset": 1454.72,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "call it underscore pop",
      "offset": 1456.559,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "and by default this is the empty set for",
      "offset": 1459.12,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "leaves",
      "offset": 1461.2,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "and then we'll just maintain it here",
      "offset": 1462.32,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "and now the operation will be just a",
      "offset": 1465.6,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "simple string and in the case of",
      "offset": 1467.36,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "addition it's plus in the case of",
      "offset": 1469.52,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "multiplication is times",
      "offset": 1471.279,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "so now we",
      "offset": 1473.84,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "not just have d dot pref we also have a",
      "offset": 1475.2,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "d dot up",
      "offset": 1477.039,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "and we know that d was produced by an",
      "offset": 1478.64,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "addition of those two values and so now",
      "offset": 1480.24,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "we have the full",
      "offset": 1482.799,
      "duration": 3.281
    },
    {
      "lang": "en",
      "text": "mathematical expression uh and we're",
      "offset": 1484,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "building out this data structure and we",
      "offset": 1486.08,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "know exactly how each value came to be",
      "offset": 1487.52,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "by word expression and from what other",
      "offset": 1489.919,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "values",
      "offset": 1491.6,
      "duration": 2.4
    },
    {
      "lang": "en",
      "text": "now because these expressions are about",
      "offset": 1494.72,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "to get quite a bit larger we'd like a",
      "offset": 1496.159,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "way to nicely visualize these",
      "offset": 1498.08,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "expressions that we're building out so",
      "offset": 1500.4,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "for that i'm going to copy paste a bunch",
      "offset": 1502.24,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "of slightly scary code that's going to",
      "offset": 1503.84,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "visualize this these expression graphs",
      "offset": 1506.48,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "for us",
      "offset": 1508.64,
      "duration": 2.399
    },
    {
      "lang": "en",
      "text": "so here's the code and i'll explain it",
      "offset": 1509.6,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "in a bit but first let me just show you",
      "offset": 1511.039,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "what this code does",
      "offset": 1513.36,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "basically what it does is it creates a",
      "offset": 1514.88,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "new function drawdot that we can call on",
      "offset": 1516.72,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "some root node",
      "offset": 1519.12,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "and then it's going to visualize it so",
      "offset": 1520.72,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "if we call drawdot on d",
      "offset": 1522.72,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "which is this final value here that is a",
      "offset": 1524.72,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "times b plus c",
      "offset": 1527.039,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "it creates something like this so this",
      "offset": 1529.76,
      "duration": 2.48
    },
    {
      "lang": "en",
      "text": "is d",
      "offset": 1531.44,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "and you see that this is a times b",
      "offset": 1532.24,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "creating an integrated value plus c",
      "offset": 1534.64,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "gives us this output node d",
      "offset": 1536.96,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "so that's dried out of d",
      "offset": 1540.4,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "and i'm not going to go through this in",
      "offset": 1542.4,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "complete detail you can take a look at",
      "offset": 1544.32,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "graphless and its api uh graphis is a",
      "offset": 1546.159,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "open source graph visualization software",
      "offset": 1548.96,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "and what we're doing here is we're",
      "offset": 1551.36,
      "duration": 3.48
    },
    {
      "lang": "en",
      "text": "building out this graph and graphis",
      "offset": 1552.48,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "api and",
      "offset": 1554.84,
      "duration": 4.04
    },
    {
      "lang": "en",
      "text": "you can basically see that trace is this",
      "offset": 1556.96,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "helper function that enumerates all of",
      "offset": 1558.88,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "the nodes and edges in the graph",
      "offset": 1560.799,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "so that just builds a set of all the",
      "offset": 1562.799,
      "duration": 3.281
    },
    {
      "lang": "en",
      "text": "nodes and edges and then we iterate for",
      "offset": 1564.24,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "all the nodes and we create special node",
      "offset": 1566.08,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "objects",
      "offset": 1568,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "for them in",
      "offset": 1568.88,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "using dot node",
      "offset": 1571.36,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "and then we also create edges using dot",
      "offset": 1573.12,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "dot edge",
      "offset": 1575.679,
      "duration": 2.401
    },
    {
      "lang": "en",
      "text": "and the only thing that's like slightly",
      "offset": 1576.799,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "tricky here is you'll notice that i",
      "offset": 1578.08,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "basically add these fake nodes which are",
      "offset": 1580.4,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "these operation nodes so for example",
      "offset": 1582.4,
      "duration": 5.279
    },
    {
      "lang": "en",
      "text": "this node here is just like a plus node",
      "offset": 1584.72,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "and",
      "offset": 1587.679,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "i create these",
      "offset": 1588.799,
      "duration": 5.601
    },
    {
      "lang": "en",
      "text": "special op nodes here",
      "offset": 1591.76,
      "duration": 5.919
    },
    {
      "lang": "en",
      "text": "and i connect them accordingly so these",
      "offset": 1594.4,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "nodes of course are not actual",
      "offset": 1597.679,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "nodes in the original graph",
      "offset": 1599.52,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "they're not actually a value object the",
      "offset": 1601.679,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "only value objects here are the things",
      "offset": 1603.919,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "in squares those are actual value",
      "offset": 1606.159,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "objects or representations thereof and",
      "offset": 1608.159,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "these op nodes are just created in this",
      "offset": 1610.559,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "drawdot routine so that it looks nice",
      "offset": 1612.4,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "let's also add labels to these graphs",
      "offset": 1615.52,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "just so we know what variables are where",
      "offset": 1617.44,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "so let's create a special underscore",
      "offset": 1619.919,
      "duration": 2.961
    },
    {
      "lang": "en",
      "text": "label",
      "offset": 1621.76,
      "duration": 2.159
    },
    {
      "lang": "en",
      "text": "um",
      "offset": 1622.88,
      "duration": 2.56
    },
    {
      "lang": "en",
      "text": "or let's just do label",
      "offset": 1623.919,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "equals empty by default and save it in",
      "offset": 1625.44,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "each node",
      "offset": 1628.4,
      "duration": 2.48
    },
    {
      "lang": "en",
      "text": "and then here we're going to do label as",
      "offset": 1631.279,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "a",
      "offset": 1633.279,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "label is the",
      "offset": 1635.36,
      "duration": 6.039
    },
    {
      "lang": "en",
      "text": "label a c",
      "offset": 1637.84,
      "duration": 3.559
    },
    {
      "lang": "en",
      "text": "and then",
      "offset": 1642.799,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "let's create a special um",
      "offset": 1644.799,
      "duration": 5.921
    },
    {
      "lang": "en",
      "text": "e equals a times b",
      "offset": 1647.679,
      "duration": 3.041
    },
    {
      "lang": "en",
      "text": "and e dot label will be e",
      "offset": 1650.799,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "it's kind of naughty",
      "offset": 1654.08,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "and e will be e plus c",
      "offset": 1655.679,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "and a d dot label will be",
      "offset": 1658.159,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "d",
      "offset": 1660.799,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "okay so nothing really changes i just",
      "offset": 1662.559,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "added this new e function",
      "offset": 1664.24,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "a new e variable",
      "offset": 1666.399,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "and then here when we are",
      "offset": 1668.48,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "printing this",
      "offset": 1670.559,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "i'm going to print the label here so",
      "offset": 1671.84,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "this will be a percent s",
      "offset": 1674.24,
      "duration": 2.64
    },
    {
      "lang": "en",
      "text": "bar",
      "offset": 1676.08,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "and this will be end.label",
      "offset": 1676.88,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "and so now",
      "offset": 1681.279,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "we have the label on the left here so it",
      "offset": 1683.44,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "says a b creating e and then e plus c",
      "offset": 1685.2,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "creates d",
      "offset": 1687.679,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "just like we have it here",
      "offset": 1688.72,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "and finally let's make this expression",
      "offset": 1690.799,
      "duration": 3.521
    },
    {
      "lang": "en",
      "text": "just one layer deeper",
      "offset": 1692.32,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "so d will not be the final output node",
      "offset": 1694.32,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "instead after d we are going to create a",
      "offset": 1697.2,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "new value object",
      "offset": 1700,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "called f we're going to start running",
      "offset": 1701.76,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "out of variables soon f will be negative",
      "offset": 1703.679,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "2.0",
      "offset": 1705.919,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "and its label will of course just be f",
      "offset": 1707.36,
      "duration": 6.799
    },
    {
      "lang": "en",
      "text": "and then l capital l will be the output",
      "offset": 1710.559,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "of our graph",
      "offset": 1714.159,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "and l will be p times f",
      "offset": 1715.44,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "okay",
      "offset": 1718,
      "duration": 2.32
    },
    {
      "lang": "en",
      "text": "so l will be negative eight is the",
      "offset": 1718.799,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "output",
      "offset": 1720.32,
      "duration": 2.4
    },
    {
      "lang": "en",
      "text": "so",
      "offset": 1722.84,
      "duration": 6.68
    },
    {
      "lang": "en",
      "text": "now we don't just draw a d we draw l",
      "offset": 1724.72,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "okay",
      "offset": 1730,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "and somehow the label of",
      "offset": 1732,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "l was undefined oops all that label has",
      "offset": 1734,
      "duration": 5.679
    },
    {
      "lang": "en",
      "text": "to be explicitly sort of given to it",
      "offset": 1736.399,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "there we go so l is the output",
      "offset": 1739.679,
      "duration": 3.521
    },
    {
      "lang": "en",
      "text": "so let's quickly recap what we've done",
      "offset": 1741.679,
      "duration": 2.401
    },
    {
      "lang": "en",
      "text": "so far",
      "offset": 1743.2,
      "duration": 2.479
    },
    {
      "lang": "en",
      "text": "we are able to build out mathematical",
      "offset": 1744.08,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "expressions using only plus and times so",
      "offset": 1745.679,
      "duration": 3.521
    },
    {
      "lang": "en",
      "text": "far",
      "offset": 1748,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "they are scalar valued along the way",
      "offset": 1749.2,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "and we can do this forward pass",
      "offset": 1751.52,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "and build out a mathematical expression",
      "offset": 1754.24,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "so we have multiple inputs here a b c",
      "offset": 1756.399,
      "duration": 3.121
    },
    {
      "lang": "en",
      "text": "and f",
      "offset": 1758.64,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "going into a mathematical expression",
      "offset": 1759.52,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "that produces a single output l",
      "offset": 1761.52,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "and this here is visualizing the forward",
      "offset": 1764,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "pass so the output of the forward pass",
      "offset": 1766.399,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "is negative eight that's the value",
      "offset": 1768.72,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "now what we'd like to do next is we'd",
      "offset": 1771.6,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "like to run back propagation",
      "offset": 1773.279,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "and in back propagation we are going to",
      "offset": 1775.36,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "start here at the end and we're going to",
      "offset": 1777.2,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "reverse",
      "offset": 1779.6,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "and calculate the gradient along along",
      "offset": 1780.88,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "all these intermediate values",
      "offset": 1783.52,
      "duration": 3.279
    },
    {
      "lang": "en",
      "text": "and really what we're computing for",
      "offset": 1785.44,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "every single value here",
      "offset": 1786.799,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "um we're going to compute the derivative",
      "offset": 1788.88,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "of that node with respect to l",
      "offset": 1790.96,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "so",
      "offset": 1795.36,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "the derivative of l with respect to l is",
      "offset": 1796.159,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "just uh one",
      "offset": 1798.72,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "and then we're going to derive what is",
      "offset": 1800.48,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "the derivative of l with respect to f",
      "offset": 1801.76,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "with respect to d with respect to c with",
      "offset": 1803.84,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "respect to e",
      "offset": 1806.32,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "with respect to b and with respect to a",
      "offset": 1807.6,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "and in the neural network setting you'd",
      "offset": 1810.32,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "be very interested in the derivative of",
      "offset": 1812.08,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "basically this loss function l",
      "offset": 1813.84,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "with respect to the weights of a neural",
      "offset": 1816.72,
      "duration": 2.64
    },
    {
      "lang": "en",
      "text": "network",
      "offset": 1818.399,
      "duration": 2.321
    },
    {
      "lang": "en",
      "text": "and here of course we have just these",
      "offset": 1819.36,
      "duration": 3.039
    },
    {
      "lang": "en",
      "text": "variables a b c and f",
      "offset": 1820.72,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "but some of these will eventually",
      "offset": 1822.399,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "represent the weights of a neural net",
      "offset": 1823.84,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "and so we'll need to know how those",
      "offset": 1825.76,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "weights are impacting",
      "offset": 1827.279,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "the loss function so we'll be interested",
      "offset": 1829.12,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "basically in the derivative of the",
      "offset": 1831.039,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "output with respect to some of its leaf",
      "offset": 1832.24,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "nodes and those leaf nodes will be the",
      "offset": 1834.64,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "weights of the neural net",
      "offset": 1836.559,
      "duration": 2.961
    },
    {
      "lang": "en",
      "text": "and the other leaf nodes of course will",
      "offset": 1838.08,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "be the data itself but usually we will",
      "offset": 1839.52,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "not want or use the derivative of the",
      "offset": 1841.679,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "loss function with respect to data",
      "offset": 1844,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "because the data is fixed but the",
      "offset": 1845.76,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "weights will be iterated on",
      "offset": 1847.76,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "using the gradient information so next",
      "offset": 1850.64,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "we are going to create a variable inside",
      "offset": 1852.559,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "the value class that maintains the",
      "offset": 1854.24,
      "duration": 5.679
    },
    {
      "lang": "en",
      "text": "derivative of l with respect to that",
      "offset": 1857.039,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "value",
      "offset": 1859.919,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "and we will call this variable grad",
      "offset": 1860.96,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "so there's a data and there's a",
      "offset": 1863.76,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "self.grad",
      "offset": 1865.84,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "and initially it will be zero and",
      "offset": 1867.2,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "remember that zero is basically means no",
      "offset": 1869.6,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "effect so at initialization we're",
      "offset": 1872.24,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "assuming that every value does not",
      "offset": 1874.32,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "impact does not affect the out the",
      "offset": 1876.08,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "output",
      "offset": 1878.559,
      "duration": 2.801
    },
    {
      "lang": "en",
      "text": "right because if the gradient is zero",
      "offset": 1879.6,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "that means that changing this variable",
      "offset": 1881.36,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "is not changing the loss function",
      "offset": 1883.2,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "so by default we assume that the",
      "offset": 1885.679,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "gradient is zero",
      "offset": 1887.279,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "and then",
      "offset": 1888.88,
      "duration": 7.279
    },
    {
      "lang": "en",
      "text": "now that we have grad and it's 0.0",
      "offset": 1891.2,
      "duration": 4.959
    },
    {
      "lang": "en",
      "text": "we are going to be able to visualize it",
      "offset": 1896.559,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "here after data so here grad is 0.4 f",
      "offset": 1898.24,
      "duration": 7.439
    },
    {
      "lang": "en",
      "text": "and this will be in that graph",
      "offset": 1902.799,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "and now we are going to be showing both",
      "offset": 1905.679,
      "duration": 5.281
    },
    {
      "lang": "en",
      "text": "the data and the grad",
      "offset": 1907.6,
      "duration": 6.079
    },
    {
      "lang": "en",
      "text": "initialized at zero",
      "offset": 1910.96,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "and we are just about getting ready to",
      "offset": 1913.679,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "calculate the back propagation",
      "offset": 1915.679,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "and of course this grad again as i",
      "offset": 1917.36,
      "duration": 3.039
    },
    {
      "lang": "en",
      "text": "mentioned is representing",
      "offset": 1918.799,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "the derivative of the output in this",
      "offset": 1920.399,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "case l with respect to this value so",
      "offset": 1922.24,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "with respect to so this is the",
      "offset": 1925.039,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "derivative of l with respect to f with",
      "offset": 1926.48,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "respect to d and so on so let's now fill",
      "offset": 1928.559,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "in those gradients and actually do back",
      "offset": 1931.36,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "propagation manually so let's start",
      "offset": 1932.96,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "filling in these gradients and start all",
      "offset": 1934.72,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "the way at the end as i mentioned here",
      "offset": 1936.399,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "first we are interested to fill in this",
      "offset": 1938.48,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "gradient here so what is the derivative",
      "offset": 1940.399,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "of l with respect to l",
      "offset": 1942.88,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "in other words if i change l by a tiny",
      "offset": 1945.2,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "amount of h",
      "offset": 1947.36,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "how much does",
      "offset": 1949.039,
      "duration": 3.281
    },
    {
      "lang": "en",
      "text": "l change",
      "offset": 1950.559,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "it changes by h so it's proportional and",
      "offset": 1952.32,
      "duration": 4.959
    },
    {
      "lang": "en",
      "text": "therefore derivative will be one",
      "offset": 1955.2,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "we can of course measure these or",
      "offset": 1957.279,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "estimate these numerical gradients",
      "offset": 1959.2,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "numerically just like we've seen before",
      "offset": 1960.96,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "so if i take this expression",
      "offset": 1963.2,
      "duration": 6.079
    },
    {
      "lang": "en",
      "text": "and i create a def lol function here",
      "offset": 1965.279,
      "duration": 5.921
    },
    {
      "lang": "en",
      "text": "and put this here now the reason i'm",
      "offset": 1969.279,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "creating a gating function hello here is",
      "offset": 1971.2,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "because i don't want to pollute or mess",
      "offset": 1973.6,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "up the global scope here this is just",
      "offset": 1975.2,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "kind of like a little staging area and",
      "offset": 1977.2,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "as you know in python all of these will",
      "offset": 1978.88,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "be local variables to this function so",
      "offset": 1980.399,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "i'm not changing any of the global scope",
      "offset": 1982.72,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "here",
      "offset": 1984.72,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "so here l1 will be l",
      "offset": 1985.76,
      "duration": 7.279
    },
    {
      "lang": "en",
      "text": "and then copy pasting this expression",
      "offset": 1990,
      "duration": 6.88
    },
    {
      "lang": "en",
      "text": "we're going to add a small amount h",
      "offset": 1993.039,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "in for example a",
      "offset": 1997.36,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "right and this would be measuring the",
      "offset": 2000.559,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "derivative of l with respect to a",
      "offset": 2002.64,
      "duration": 5.519
    },
    {
      "lang": "en",
      "text": "so here this will be l2",
      "offset": 2005.44,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "and then we want to print this",
      "offset": 2008.159,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "derivative so print",
      "offset": 2009.519,
      "duration": 5.681
    },
    {
      "lang": "en",
      "text": "l2 minus l1 which is how much l changed",
      "offset": 2011.6,
      "duration": 6.079
    },
    {
      "lang": "en",
      "text": "and then normalize it by h so this is",
      "offset": 2015.2,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "the rise over run",
      "offset": 2017.679,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "and we have to be careful because l is a",
      "offset": 2019.519,
      "duration": 5.601
    },
    {
      "lang": "en",
      "text": "value node so we actually want its data",
      "offset": 2021.36,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "um",
      "offset": 2025.12,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "so that these are floats dividing by h",
      "offset": 2026.24,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "and this should print the derivative of",
      "offset": 2028.88,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "l with respect to a because a is the one",
      "offset": 2030.72,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "that we bumped a little bit by h",
      "offset": 2033.039,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "so what is the",
      "offset": 2035.6,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "derivative of l with respect to a",
      "offset": 2037.279,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "it's six",
      "offset": 2039.279,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "okay and obviously",
      "offset": 2041.039,
      "duration": 5.921
    },
    {
      "lang": "en",
      "text": "if we change l by h",
      "offset": 2043.6,
      "duration": 6.239
    },
    {
      "lang": "en",
      "text": "then that would be",
      "offset": 2046.96,
      "duration": 5.439
    },
    {
      "lang": "en",
      "text": "here effectively",
      "offset": 2049.839,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "this looks really awkward but changing l",
      "offset": 2052.399,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "by h",
      "offset": 2054.639,
      "duration": 6.161
    },
    {
      "lang": "en",
      "text": "you see the derivative here is 1. um",
      "offset": 2056.079,
      "duration": 7.121
    },
    {
      "lang": "en",
      "text": "that's kind of like the base case of",
      "offset": 2060.8,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "what we are doing here",
      "offset": 2063.2,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "so basically we cannot come up here and",
      "offset": 2064.72,
      "duration": 5.119
    },
    {
      "lang": "en",
      "text": "we can manually set l.grad to one this",
      "offset": 2066.639,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "is our manual back propagation",
      "offset": 2069.839,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "l dot grad is one and let's redraw",
      "offset": 2071.919,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "and we'll see that we filled in grad as",
      "offset": 2075.599,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "1 for l",
      "offset": 2077.679,
      "duration": 2.641
    },
    {
      "lang": "en",
      "text": "we're now going to continue the back",
      "offset": 2079.04,
      "duration": 3.119
    },
    {
      "lang": "en",
      "text": "propagation so let's here look at the",
      "offset": 2080.32,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "derivatives of l with respect to d and f",
      "offset": 2082.159,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "let's do a d first",
      "offset": 2085.52,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "so what we are interested in if i create",
      "offset": 2087.679,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "a markdown on here is we'd like to know",
      "offset": 2089.44,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "basically we have that l is d times f",
      "offset": 2091.839,
      "duration": 5.441
    },
    {
      "lang": "en",
      "text": "and we'd like to know what is uh d",
      "offset": 2094.24,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "l by d d",
      "offset": 2097.28,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "what is that",
      "offset": 2100.24,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "and if you know your calculus uh l is d",
      "offset": 2101.68,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "times f so what is d l by d d it would",
      "offset": 2103.68,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "be f",
      "offset": 2106.24,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "and if you don't believe me we can also",
      "offset": 2108.079,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "just derive it because the proof would",
      "offset": 2110.24,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "be fairly straightforward uh we go to",
      "offset": 2111.76,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "the",
      "offset": 2114.8,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "definition of the derivative which is f",
      "offset": 2115.68,
      "duration": 6.399
    },
    {
      "lang": "en",
      "text": "of x plus h minus f of x divide h",
      "offset": 2118.16,
      "duration": 6.32
    },
    {
      "lang": "en",
      "text": "as a limit limit of h goes to zero of",
      "offset": 2122.079,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "this kind of expression so when we have",
      "offset": 2124.48,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "l is d times f",
      "offset": 2126.48,
      "duration": 5.119
    },
    {
      "lang": "en",
      "text": "then increasing d by h",
      "offset": 2128.24,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "would give us the output of b plus h",
      "offset": 2131.599,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "times f",
      "offset": 2133.92,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "that's basically f of x plus h right",
      "offset": 2135.76,
      "duration": 6.56
    },
    {
      "lang": "en",
      "text": "minus d times f",
      "offset": 2138.96,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "and then divide h and symbolically",
      "offset": 2142.32,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "expanding out here we would have",
      "offset": 2144.88,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "basically d times f plus h times f minus",
      "offset": 2146.56,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "t times f divide h",
      "offset": 2150,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "and then you see how the df minus df",
      "offset": 2152.24,
      "duration": 5.119
    },
    {
      "lang": "en",
      "text": "cancels so you're left with h times f",
      "offset": 2154.4,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "divide h",
      "offset": 2157.359,
      "duration": 2.561
    },
    {
      "lang": "en",
      "text": "which is f",
      "offset": 2158.56,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "so in the limit as h goes to zero of",
      "offset": 2159.92,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "you know",
      "offset": 2163.2,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "derivative",
      "offset": 2164.48,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "definition we just get f in the case of",
      "offset": 2166.72,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "d times f",
      "offset": 2169.92,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "so",
      "offset": 2172.32,
      "duration": 2
    },
    {
      "lang": "en",
      "text": "symmetrically",
      "offset": 2173.04,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "dl by d",
      "offset": 2174.32,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "f will just be d",
      "offset": 2175.92,
      "duration": 5.439
    },
    {
      "lang": "en",
      "text": "so what we have is that f dot grad",
      "offset": 2178.48,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "we see now is just the value of d",
      "offset": 2181.359,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "which is 4.",
      "offset": 2184.48,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "and we see that",
      "offset": 2188.8,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "d dot grad",
      "offset": 2190.16,
      "duration": 5.439
    },
    {
      "lang": "en",
      "text": "is just uh the value of f",
      "offset": 2191.68,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "and so the value of f is negative two",
      "offset": 2196.88,
      "duration": 8.08
    },
    {
      "lang": "en",
      "text": "so we'll set those manually",
      "offset": 2201.2,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "let me erase this markdown node and then",
      "offset": 2205.119,
      "duration": 5.281
    },
    {
      "lang": "en",
      "text": "let's redraw what we have",
      "offset": 2207.04,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "okay",
      "offset": 2210.88,
      "duration": 2.32
    },
    {
      "lang": "en",
      "text": "and let's just make sure that these were",
      "offset": 2211.839,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "correct so we seem to think that dl by",
      "offset": 2213.2,
      "duration": 6.32
    },
    {
      "lang": "en",
      "text": "dd is negative two so let's double check",
      "offset": 2216.4,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "um let me erase this plus h from before",
      "offset": 2219.52,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "and now we want the derivative with",
      "offset": 2222.32,
      "duration": 2.799
    },
    {
      "lang": "en",
      "text": "respect to f",
      "offset": 2223.359,
      "duration": 3.521
    },
    {
      "lang": "en",
      "text": "so let's just come here when i create f",
      "offset": 2225.119,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "and let's do a plus h here and this",
      "offset": 2226.88,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "should print the derivative of l with",
      "offset": 2228.96,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "respect to f so we expect to see four",
      "offset": 2230.8,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "yeah and this is four up to floating",
      "offset": 2234.16,
      "duration": 2.959
    },
    {
      "lang": "en",
      "text": "point",
      "offset": 2236.32,
      "duration": 2.64
    },
    {
      "lang": "en",
      "text": "funkiness",
      "offset": 2237.119,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "and then dl by dd",
      "offset": 2238.96,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "should be f which is negative two",
      "offset": 2241.359,
      "duration": 5.281
    },
    {
      "lang": "en",
      "text": "grad is negative two",
      "offset": 2245.04,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "so if we again come here and we change d",
      "offset": 2246.64,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "d dot data plus equals h right here",
      "offset": 2251.839,
      "duration": 5.681
    },
    {
      "lang": "en",
      "text": "so we expect so we've added a little h",
      "offset": 2255.119,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "and then we see how l changed and we",
      "offset": 2257.52,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "expect to print",
      "offset": 2260.079,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "uh negative two",
      "offset": 2262.32,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "there we go",
      "offset": 2264.64,
      "duration": 2.479
    },
    {
      "lang": "en",
      "text": "so we've numerically verified what we're",
      "offset": 2267.359,
      "duration": 3.521
    },
    {
      "lang": "en",
      "text": "doing here is what kind of like an",
      "offset": 2269.52,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "inline gradient check gradient check is",
      "offset": 2270.88,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "when we",
      "offset": 2273.44,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "are deriving this like back propagation",
      "offset": 2274.4,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "and getting the derivative with respect",
      "offset": 2276.4,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "to all the intermediate results and then",
      "offset": 2277.599,
      "duration": 5.601
    },
    {
      "lang": "en",
      "text": "numerical gradient is just you know",
      "offset": 2280.32,
      "duration": 5.759
    },
    {
      "lang": "en",
      "text": "estimating it using small step size",
      "offset": 2283.2,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "now we're getting to the crux of",
      "offset": 2286.079,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "backpropagation so this will be the most",
      "offset": 2288,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "important node to understand because if",
      "offset": 2290.4,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "you understand the gradient for this",
      "offset": 2292.88,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "node you understand all of back",
      "offset": 2294.32,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "propagation and all of training of",
      "offset": 2296.079,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "neural nets basically",
      "offset": 2297.44,
      "duration": 5.679
    },
    {
      "lang": "en",
      "text": "so we need to derive dl by bc",
      "offset": 2299.599,
      "duration": 5.121
    },
    {
      "lang": "en",
      "text": "in other words the derivative of l with",
      "offset": 2303.119,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "respect to c",
      "offset": 2304.72,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "because we've computed all these other",
      "offset": 2306.56,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "gradients already",
      "offset": 2307.92,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "now we're coming here and we're",
      "offset": 2309.44,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "continuing the back propagation manually",
      "offset": 2310.64,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "so we want dl by dc and then we'll also",
      "offset": 2313.599,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "derive dl by de",
      "offset": 2316.16,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "now here's the problem",
      "offset": 2318.4,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "how do we derive dl",
      "offset": 2320,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "by dc",
      "offset": 2321.92,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "we actually know the derivative l with",
      "offset": 2324,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "respect to d so we know how l assessed",
      "offset": 2326.16,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "it to d",
      "offset": 2328.8,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "but how is l sensitive to c so if we",
      "offset": 2330.079,
      "duration": 5.601
    },
    {
      "lang": "en",
      "text": "wiggle c how does that impact l",
      "offset": 2333.119,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "through d",
      "offset": 2335.68,
      "duration": 6.159
    },
    {
      "lang": "en",
      "text": "so we know dl by dc",
      "offset": 2338,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "and we also here know how c impacts d",
      "offset": 2341.92,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "and so just very intuitively if you know",
      "offset": 2344.64,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "the impact that c is having on d and the",
      "offset": 2346.32,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "impact that d is having on l",
      "offset": 2349.2,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "then you should be able to somehow put",
      "offset": 2351.359,
      "duration": 3.041
    },
    {
      "lang": "en",
      "text": "that information together to figure out",
      "offset": 2352.88,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "how c impacts l",
      "offset": 2354.4,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "and indeed this is what we can actually",
      "offset": 2356.48,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "do so in particular we know just",
      "offset": 2358.48,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "concentrating on d first let's look at",
      "offset": 2360.88,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "how what is the derivative basically of",
      "offset": 2362.96,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "d with respect to c so in other words",
      "offset": 2364.88,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "what is dd by dc",
      "offset": 2367.04,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "so here we know that d is c times c plus",
      "offset": 2371.599,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "e",
      "offset": 2374.72,
      "duration": 2.639
    },
    {
      "lang": "en",
      "text": "that's what we know and now we're",
      "offset": 2375.599,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "interested in dd by dc",
      "offset": 2377.359,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "if you just know your calculus again and",
      "offset": 2379.599,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "you remember that differentiating c plus",
      "offset": 2381.359,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "e with respect to c you know that that",
      "offset": 2383.599,
      "duration": 2.881
    },
    {
      "lang": "en",
      "text": "gives you",
      "offset": 2385.52,
      "duration": 2.319
    },
    {
      "lang": "en",
      "text": "1.0",
      "offset": 2386.48,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "and we can also go back to the basics",
      "offset": 2387.839,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "and derive this because again we can go",
      "offset": 2389.76,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "to our f of x plus h minus f of x",
      "offset": 2391.76,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "divide by h",
      "offset": 2394.64,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "that's the definition of a derivative as",
      "offset": 2396.4,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "h goes to zero",
      "offset": 2398.48,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "and so here",
      "offset": 2400,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "focusing on c and its effect on d",
      "offset": 2401.44,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "we can basically do the f of x plus h",
      "offset": 2404.4,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "will be",
      "offset": 2406.319,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "c is incremented by h plus e",
      "offset": 2407.2,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "that's the first evaluation of our",
      "offset": 2410.88,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "function minus",
      "offset": 2412.16,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "c plus e",
      "offset": 2414.319,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "and then divide h",
      "offset": 2416.48,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "and so what is this",
      "offset": 2418.319,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "uh just expanding this out this will be",
      "offset": 2419.839,
      "duration": 5.601
    },
    {
      "lang": "en",
      "text": "c plus h plus e minus c minus e",
      "offset": 2421.839,
      "duration": 5.841
    },
    {
      "lang": "en",
      "text": "divide h and then you see here how c",
      "offset": 2425.44,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "minus c cancels e minus e cancels we're",
      "offset": 2427.68,
      "duration": 5.919
    },
    {
      "lang": "en",
      "text": "left with h over h which is 1.0",
      "offset": 2430.319,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "and so",
      "offset": 2433.599,
      "duration": 5.281
    },
    {
      "lang": "en",
      "text": "by symmetry also d d by d",
      "offset": 2435.28,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "e",
      "offset": 2438.88,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "will be 1.0 as well",
      "offset": 2439.76,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "so basically the derivative of a sum",
      "offset": 2442.96,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "expression is very simple and and this",
      "offset": 2444.8,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "is the local derivative so i call this",
      "offset": 2446.64,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "the local derivative because we have the",
      "offset": 2449.04,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "final output value all the way at the",
      "offset": 2451.04,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "end of this graph and we're now like a",
      "offset": 2452.48,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "small node here",
      "offset": 2454.4,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "and this is a little plus node",
      "offset": 2455.839,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "and it the little plus node doesn't know",
      "offset": 2458,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "anything about the rest of the graph",
      "offset": 2460.079,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "that it's embedded in all it knows is",
      "offset": 2462.079,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "that it did a plus it took a c and an e",
      "offset": 2464.16,
      "duration": 4.959
    },
    {
      "lang": "en",
      "text": "added them and created d",
      "offset": 2467.119,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "and this plus note also knows the local",
      "offset": 2469.119,
      "duration": 5.281
    },
    {
      "lang": "en",
      "text": "influence of c on d or rather rather the",
      "offset": 2471.2,
      "duration": 5.119
    },
    {
      "lang": "en",
      "text": "derivative of d with respect to c and it",
      "offset": 2474.4,
      "duration": 2.64
    },
    {
      "lang": "en",
      "text": "also",
      "offset": 2476.319,
      "duration": 2.401
    },
    {
      "lang": "en",
      "text": "knows the derivative of d with respect",
      "offset": 2477.04,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "to e but that's not what we want that's",
      "offset": 2478.72,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "just a local derivative what we actually",
      "offset": 2481.28,
      "duration": 6.4
    },
    {
      "lang": "en",
      "text": "want is d l by d c and l could l is here",
      "offset": 2483.359,
      "duration": 7.361
    },
    {
      "lang": "en",
      "text": "just one step away but in a general case",
      "offset": 2487.68,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "this little plus note is could be",
      "offset": 2490.72,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "embedded in like a massive graph",
      "offset": 2492.24,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "so",
      "offset": 2494.64,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "again we know how l impacts d and now we",
      "offset": 2495.76,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "know how c and e impact d how do we put",
      "offset": 2498.4,
      "duration": 4.959
    },
    {
      "lang": "en",
      "text": "that information together to write dl by",
      "offset": 2501.28,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "dc and the answer of course is the chain",
      "offset": 2503.359,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "rule in calculus",
      "offset": 2506,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "and so um",
      "offset": 2507.76,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "i pulled up a chain rule here from",
      "offset": 2510.079,
      "duration": 2.881
    },
    {
      "lang": "en",
      "text": "kapedia",
      "offset": 2511.52,
      "duration": 2.4
    },
    {
      "lang": "en",
      "text": "and",
      "offset": 2512.96,
      "duration": 1.92
    },
    {
      "lang": "en",
      "text": "i'm going to go through this very",
      "offset": 2513.92,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "briefly so chain rule",
      "offset": 2514.88,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "wikipedia sometimes can be very",
      "offset": 2517.2,
      "duration": 3.119
    },
    {
      "lang": "en",
      "text": "confusing and calculus can",
      "offset": 2518.56,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "can be very confusing like this is the",
      "offset": 2520.319,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "way i",
      "offset": 2522.56,
      "duration": 2.48
    },
    {
      "lang": "en",
      "text": "learned",
      "offset": 2523.599,
      "duration": 2.801
    },
    {
      "lang": "en",
      "text": "chain rule and it was very confusing",
      "offset": 2525.04,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "like what is happening it's just",
      "offset": 2526.4,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "complicated so i like this expression",
      "offset": 2528.56,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "much better",
      "offset": 2530.72,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "if a variable z depends on a variable y",
      "offset": 2532.96,
      "duration": 5.119
    },
    {
      "lang": "en",
      "text": "which itself depends on the variable x",
      "offset": 2535.119,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "then z depends on x as well obviously",
      "offset": 2538.079,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "through the intermediate variable y",
      "offset": 2540.319,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "in this case the chain rule is expressed",
      "offset": 2542.4,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "as",
      "offset": 2544.319,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "if you want dz by dx",
      "offset": 2545.2,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "then you take the dz by dy and you",
      "offset": 2548.079,
      "duration": 5.601
    },
    {
      "lang": "en",
      "text": "multiply it by d y by dx",
      "offset": 2550.72,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "so the chain rule fundamentally is",
      "offset": 2553.68,
      "duration": 2.48
    },
    {
      "lang": "en",
      "text": "telling you",
      "offset": 2554.96,
      "duration": 2.32
    },
    {
      "lang": "en",
      "text": "how",
      "offset": 2556.16,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "we chain these",
      "offset": 2557.28,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "uh derivatives together",
      "offset": 2559.28,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "correctly so to differentiate through a",
      "offset": 2561.52,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "function composition",
      "offset": 2564.319,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "we have to apply a multiplication",
      "offset": 2566.079,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "of",
      "offset": 2568.72,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "those derivatives",
      "offset": 2569.599,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "so that's really what chain rule is",
      "offset": 2571.68,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "telling us",
      "offset": 2573.52,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "and there's a nice little intuitive",
      "offset": 2574.64,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "explanation here which i also think is",
      "offset": 2576.48,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "kind of cute the chain rule says that",
      "offset": 2578.079,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "knowing the instantaneous rate of change",
      "offset": 2579.92,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "of z with respect to y and y relative to",
      "offset": 2581.28,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "x allows one to calculate the",
      "offset": 2583.76,
      "duration": 2.64
    },
    {
      "lang": "en",
      "text": "instantaneous rate of change of z",
      "offset": 2584.8,
      "duration": 3.039
    },
    {
      "lang": "en",
      "text": "relative to x",
      "offset": 2586.4,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "as a product of those two rates of",
      "offset": 2587.839,
      "duration": 2.561
    },
    {
      "lang": "en",
      "text": "change",
      "offset": 2589.52,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "simply the product of those two",
      "offset": 2590.4,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "so here's a good one",
      "offset": 2592.56,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "if a car travels twice as fast as",
      "offset": 2594.319,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "bicycle and the bicycle is four times as",
      "offset": 2596,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "fast as walking man",
      "offset": 2598.16,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "then the car travels two times four",
      "offset": 2599.92,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "eight times as fast as demand",
      "offset": 2602.319,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "and so this makes it very clear that the",
      "offset": 2605.119,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "correct thing to do sort of",
      "offset": 2607.2,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "is to multiply",
      "offset": 2609.2,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "so",
      "offset": 2610.96,
      "duration": 2.879
    },
    {
      "lang": "en",
      "text": "cars twice as fast as bicycle and",
      "offset": 2611.92,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "bicycle is four times as fast as man",
      "offset": 2613.839,
      "duration": 5.041
    },
    {
      "lang": "en",
      "text": "so the car will be eight times as fast",
      "offset": 2616.56,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "as the man and so we can take these",
      "offset": 2618.88,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "intermediate rates of change if you will",
      "offset": 2622.16,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "and multiply them together",
      "offset": 2624.4,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "and that justifies the",
      "offset": 2626.319,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "chain rule intuitively so have a look at",
      "offset": 2628.56,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "chain rule about here really what it",
      "offset": 2630.72,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "means for us is there's a very simple",
      "offset": 2632.64,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "recipe for deriving what we want which",
      "offset": 2634.64,
      "duration": 4.959
    },
    {
      "lang": "en",
      "text": "is dl by dc",
      "offset": 2636.64,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "and what we have so far",
      "offset": 2639.599,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "is we know",
      "offset": 2641.52,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "want",
      "offset": 2643.599,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "and we know",
      "offset": 2645.2,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "what is the",
      "offset": 2647.44,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "impact of d on l so we know d l by",
      "offset": 2648.88,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "d d the derivative of l with respect to",
      "offset": 2652.64,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "d d we know that that's negative two",
      "offset": 2654.8,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "and now because of this local",
      "offset": 2657.44,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "reasoning that we've done here we know",
      "offset": 2659.359,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "dd by d",
      "offset": 2661.44,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "c",
      "offset": 2663.04,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "so how does c impact d and in",
      "offset": 2664.56,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "particular this is a plus node so the",
      "offset": 2667.52,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "local derivative is simply 1.0 it's very",
      "offset": 2669.599,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "simple",
      "offset": 2672,
      "duration": 2.319
    },
    {
      "lang": "en",
      "text": "and so",
      "offset": 2673.28,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "the chain rule tells us that dl by dc",
      "offset": 2674.319,
      "duration": 5.841
    },
    {
      "lang": "en",
      "text": "going through this intermediate variable",
      "offset": 2677.359,
      "duration": 6.641
    },
    {
      "lang": "en",
      "text": "will just be simply d l by",
      "offset": 2680.16,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "d",
      "offset": 2684,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "times",
      "offset": 2684.8,
      "duration": 2.4
    },
    {
      "lang": "en",
      "text": "dd by dc",
      "offset": 2689.2,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "that's chain rule",
      "offset": 2691.68,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "so this is identical to what's happening",
      "offset": 2693.28,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "here",
      "offset": 2695.839,
      "duration": 2.641
    },
    {
      "lang": "en",
      "text": "except",
      "offset": 2696.72,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "z is rl",
      "offset": 2698.48,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "y is our d and x is rc",
      "offset": 2699.92,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "so we literally just have to multiply",
      "offset": 2703.76,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "these",
      "offset": 2705.2,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "and because",
      "offset": 2706.64,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "these local derivatives like dd by dc",
      "offset": 2710.319,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "are just one",
      "offset": 2712.48,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "we basically just copy over dl by dd",
      "offset": 2714.72,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "because this is just times one",
      "offset": 2717.44,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "so what does it do so because dl by dd",
      "offset": 2719.44,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "is negative two what is dl by dc",
      "offset": 2722.16,
      "duration": 6.88
    },
    {
      "lang": "en",
      "text": "well it's the local gradient 1.0 times",
      "offset": 2725.92,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "dl by dd which is negative two",
      "offset": 2729.04,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "so literally what a plus node does you",
      "offset": 2731.44,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "can look at it that way is it literally",
      "offset": 2733.76,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "just routes the gradient",
      "offset": 2735.52,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "because the plus nodes local derivatives",
      "offset": 2737.52,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "are just one and so in the chain rule",
      "offset": 2739.68,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "one times",
      "offset": 2741.92,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "dl by dd",
      "offset": 2743.52,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "is um",
      "offset": 2745.68,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "is uh is just dl by dd and so that",
      "offset": 2747.52,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "derivative just gets routed to both c",
      "offset": 2750.4,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "and to e in this case",
      "offset": 2753.28,
      "duration": 6.079
    },
    {
      "lang": "en",
      "text": "so basically um we have that that grad",
      "offset": 2755.44,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "or let's start with c since that's the",
      "offset": 2759.359,
      "duration": 3.121
    },
    {
      "lang": "en",
      "text": "one we looked at",
      "offset": 2761.04,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "is",
      "offset": 2762.48,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "negative two times one",
      "offset": 2763.92,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "negative two",
      "offset": 2766.8,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "and in the same way by symmetry e that",
      "offset": 2768.48,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "grad will be negative two that's the",
      "offset": 2771.119,
      "duration": 5.841
    },
    {
      "lang": "en",
      "text": "claim so we can set those",
      "offset": 2773.04,
      "duration": 6.319
    },
    {
      "lang": "en",
      "text": "we can redraw",
      "offset": 2776.96,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "and you see how we just assign negative",
      "offset": 2779.359,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "to negative two so this backpropagating",
      "offset": 2780.88,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "signal which is carrying the information",
      "offset": 2783.2,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "of like what is the derivative of l with",
      "offset": 2785.04,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "respect to all the intermediate nodes",
      "offset": 2786.72,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "we can imagine it almost like flowing",
      "offset": 2788.88,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "backwards through the graph and a plus",
      "offset": 2790.64,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "node will simply distribute the",
      "offset": 2792.4,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "derivative to all the leaf nodes sorry",
      "offset": 2794.319,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "to all the children nodes of it",
      "offset": 2796.4,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "so this is the claim and now let's",
      "offset": 2799.119,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "verify it so let me remove the plus h",
      "offset": 2800.48,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "here from before",
      "offset": 2803.359,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "and now instead what we're going to do",
      "offset": 2805.119,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "is we're going to increment c so c dot",
      "offset": 2806.319,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "data will be credited by h",
      "offset": 2808.48,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "and when i run this we expect to see",
      "offset": 2810.64,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "negative 2",
      "offset": 2812.4,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "negative 2. and then of course for e",
      "offset": 2814.319,
      "duration": 6.8
    },
    {
      "lang": "en",
      "text": "so e dot data plus equals h and we",
      "offset": 2818.88,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "expect to see negative 2.",
      "offset": 2821.119,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "simple",
      "offset": 2823.119,
      "duration": 2.321
    },
    {
      "lang": "en",
      "text": "so those are the derivatives of these",
      "offset": 2827.28,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "internal nodes",
      "offset": 2829.44,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "and now we're going to recurse our way",
      "offset": 2831.359,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "backwards again",
      "offset": 2833.76,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "and we're again going to apply the chain",
      "offset": 2835.52,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "rule so here we go our second",
      "offset": 2837.2,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "application of chain rule and we will",
      "offset": 2839.2,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "apply it all the way through the graph",
      "offset": 2840.96,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "we just happen to only have one more",
      "offset": 2842.559,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "node remaining",
      "offset": 2844,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "we have that d l",
      "offset": 2845.359,
      "duration": 3.121
    },
    {
      "lang": "en",
      "text": "by d e",
      "offset": 2847.2,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "as we have just calculated is negative",
      "offset": 2848.48,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "two so we know that",
      "offset": 2850.24,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "so we know the derivative of l with",
      "offset": 2852.559,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "respect to e",
      "offset": 2853.92,
      "duration": 2.56
    },
    {
      "lang": "en",
      "text": "and now we want dl",
      "offset": 2856.72,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "by",
      "offset": 2859.44,
      "duration": 2.32
    },
    {
      "lang": "en",
      "text": "da",
      "offset": 2860.559,
      "duration": 2.241
    },
    {
      "lang": "en",
      "text": "right",
      "offset": 2861.76,
      "duration": 2.559
    },
    {
      "lang": "en",
      "text": "and the chain rule is telling us that",
      "offset": 2862.8,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "that's just dl by de",
      "offset": 2864.319,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "negative 2",
      "offset": 2868.96,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "times the local gradient so what is the",
      "offset": 2870.24,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "local gradient basically d e",
      "offset": 2872.72,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "by d a",
      "offset": 2875.52,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "we have to look at that",
      "offset": 2876.96,
      "duration": 5.359
    },
    {
      "lang": "en",
      "text": "so i'm a little times node",
      "offset": 2880,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "inside a massive graph",
      "offset": 2882.319,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "and i only know that i did a times b and",
      "offset": 2884.319,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "i produced an e",
      "offset": 2886.88,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "so now what is d e by d a and d e by d b",
      "offset": 2889.04,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "that's the only thing that i sort of",
      "offset": 2892.64,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "know about that's my local gradient",
      "offset": 2894.079,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "so",
      "offset": 2897.119,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "because we have that e's a times b we're",
      "offset": 2897.839,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "asking what is d e by d a",
      "offset": 2900.559,
      "duration": 6.081
    },
    {
      "lang": "en",
      "text": "and of course we just did that here we",
      "offset": 2904.079,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "had a",
      "offset": 2906.64,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "times so i'm not going to rederive it",
      "offset": 2907.599,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "but if you want to differentiate this",
      "offset": 2910.079,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "with respect to a you'll just get b",
      "offset": 2912,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "right the value of b",
      "offset": 2914.559,
      "duration": 6.401
    },
    {
      "lang": "en",
      "text": "which in this case is negative 3.0",
      "offset": 2916.64,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "so",
      "offset": 2921.04,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "basically we have that dl by da",
      "offset": 2921.92,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "well let me just do it right here we",
      "offset": 2925.119,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "have that a dot grad and we are applying",
      "offset": 2927.2,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "chain rule here",
      "offset": 2929.44,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "is d l by d e which we see here is",
      "offset": 2930.8,
      "duration": 5.519
    },
    {
      "lang": "en",
      "text": "negative two",
      "offset": 2934.24,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "times",
      "offset": 2936.319,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "what is d e by d a",
      "offset": 2937.52,
      "duration": 7.12
    },
    {
      "lang": "en",
      "text": "it's the value of b which is negative 3.",
      "offset": 2939.76,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "that's it",
      "offset": 2944.88,
      "duration": 2.4
    },
    {
      "lang": "en",
      "text": "and then we have b grad is again dl by",
      "offset": 2947.839,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "de",
      "offset": 2950.559,
      "duration": 2.56
    },
    {
      "lang": "en",
      "text": "which is negative 2",
      "offset": 2951.44,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "just the same way",
      "offset": 2953.119,
      "duration": 2.401
    },
    {
      "lang": "en",
      "text": "times",
      "offset": 2954.4,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "what is d e by d",
      "offset": 2955.52,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "um db",
      "offset": 2958,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "is the value of a which is 2.2.0",
      "offset": 2959.599,
      "duration": 6.081
    },
    {
      "lang": "en",
      "text": "as the value of a",
      "offset": 2963.52,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "so these are our claimed derivatives",
      "offset": 2965.68,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "let's",
      "offset": 2968.8,
      "duration": 3.279
    },
    {
      "lang": "en",
      "text": "redraw",
      "offset": 2970.64,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "and we see here that",
      "offset": 2972.079,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "a dot grad turns out to be 6 because",
      "offset": 2973.76,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "that is negative 2 times negative 3",
      "offset": 2976,
      "duration": 5.119
    },
    {
      "lang": "en",
      "text": "and b dot grad is negative 4",
      "offset": 2978.4,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "times sorry is negative 2 times 2 which",
      "offset": 2981.119,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "is negative 4.",
      "offset": 2983.68,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "so those are our claims let's delete",
      "offset": 2985.44,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "this and let's verify them",
      "offset": 2987.44,
      "duration": 5.119
    },
    {
      "lang": "en",
      "text": "we have",
      "offset": 2990.48,
      "duration": 6.8
    },
    {
      "lang": "en",
      "text": "a here a dot data plus equals h",
      "offset": 2992.559,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "so the claim is that",
      "offset": 2997.52,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "a dot grad is six",
      "offset": 2999.599,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "let's verify",
      "offset": 3001.76,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "six",
      "offset": 3003.44,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "and we have beta data",
      "offset": 3004.88,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "plus equals h",
      "offset": 3007.28,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "so nudging b by h",
      "offset": 3008.8,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "and looking at what happens",
      "offset": 3011.119,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "we claim it's negative four",
      "offset": 3013.04,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "and indeed it's negative four plus minus",
      "offset": 3015.119,
      "duration": 5.361
    },
    {
      "lang": "en",
      "text": "again float oddness",
      "offset": 3017.92,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "um",
      "offset": 3020.48,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "and uh",
      "offset": 3021.839,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "that's it this",
      "offset": 3023.28,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "that was the manual",
      "offset": 3024.559,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "back propagation",
      "offset": 3026.72,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "uh all the way from here to all the leaf",
      "offset": 3028.319,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "nodes and we've done it piece by piece",
      "offset": 3030.559,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "and really all we've done is as you saw",
      "offset": 3033.28,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "we iterated through all the nodes one by",
      "offset": 3035.28,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "one and locally applied the chain rule",
      "offset": 3037.28,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "we always know what is the derivative of",
      "offset": 3039.839,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "l with respect to this little output and",
      "offset": 3041.76,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "then we look at how this output was",
      "offset": 3044.24,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "produced this output was produced",
      "offset": 3045.599,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "through some operation and we have the",
      "offset": 3047.599,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "pointers to the children nodes of this",
      "offset": 3049.52,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "operation",
      "offset": 3051.52,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "and so in this little operation we know",
      "offset": 3052.64,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "what the local derivatives are and we",
      "offset": 3054.72,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "just multiply them onto the derivative",
      "offset": 3056.64,
      "duration": 2.959
    },
    {
      "lang": "en",
      "text": "always",
      "offset": 3058.559,
      "duration": 3.121
    },
    {
      "lang": "en",
      "text": "so we just go through and recursively",
      "offset": 3059.599,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "multiply on the local derivatives and",
      "offset": 3061.68,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "that's what back propagation is is just",
      "offset": 3064.16,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "a recursive application of chain rule",
      "offset": 3065.92,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "backwards through the computation graph",
      "offset": 3068.16,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "let's see this power in action just very",
      "offset": 3070.559,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "briefly what we're going to do is we're",
      "offset": 3072.72,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "going to",
      "offset": 3074.64,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "nudge our inputs to try to make l go up",
      "offset": 3075.68,
      "duration": 5.919
    },
    {
      "lang": "en",
      "text": "so in particular what we're doing is we",
      "offset": 3079.68,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "want a.data we're going to change it",
      "offset": 3081.599,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "and if we want l to go up that means we",
      "offset": 3084.16,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "just have to go in the direction of the",
      "offset": 3086.559,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "gradient so",
      "offset": 3087.839,
      "duration": 3.121
    },
    {
      "lang": "en",
      "text": "a",
      "offset": 3089.839,
      "duration": 2.401
    },
    {
      "lang": "en",
      "text": "should increase in the direction of",
      "offset": 3090.96,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "gradient by like some small step amount",
      "offset": 3092.24,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "this is the step size",
      "offset": 3094.8,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "and we don't just want this for ba but",
      "offset": 3096.48,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "also for b",
      "offset": 3098.16,
      "duration": 2.64
    },
    {
      "lang": "en",
      "text": "also for c",
      "offset": 3101.44,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "also for f",
      "offset": 3104.48,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "those are",
      "offset": 3106.079,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "leaf nodes which we usually have control",
      "offset": 3107.2,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "over",
      "offset": 3109.68,
      "duration": 2.879
    },
    {
      "lang": "en",
      "text": "and if we nudge in direction of the",
      "offset": 3110.64,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "gradient we expect a positive influence",
      "offset": 3112.559,
      "duration": 3.121
    },
    {
      "lang": "en",
      "text": "on l",
      "offset": 3114.8,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "so we expect l to go up",
      "offset": 3115.68,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "positively",
      "offset": 3118.48,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "so it should become less negative it",
      "offset": 3119.839,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "should go up to say negative you know",
      "offset": 3121.76,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "six or something like that",
      "offset": 3123.68,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "uh it's hard to tell exactly and we'd",
      "offset": 3125.92,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "have to rewrite the forward pass so let",
      "offset": 3128.079,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "me just um",
      "offset": 3129.599,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "do that here",
      "offset": 3132.319,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "um",
      "offset": 3133.92,
      "duration": 2.24
    },
    {
      "lang": "en",
      "text": "this would be the forward pass f would",
      "offset": 3136.559,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "be unchanged this is effectively the",
      "offset": 3138.64,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "forward pass and now if we print l.data",
      "offset": 3140.8,
      "duration": 6.4
    },
    {
      "lang": "en",
      "text": "we expect because we nudged all the",
      "offset": 3144.72,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "values all the inputs in the rational",
      "offset": 3147.2,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "gradient we expected a less negative l",
      "offset": 3148.8,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "we expect it to go up",
      "offset": 3150.96,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "so maybe it's negative six or so let's",
      "offset": 3152.88,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "see what happens",
      "offset": 3154.88,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "okay negative seven",
      "offset": 3156.64,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "and uh this is basically one step of an",
      "offset": 3158.319,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "optimization that we'll end up running",
      "offset": 3161.839,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "and really does gradient just give us",
      "offset": 3163.839,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "some power because we know how to",
      "offset": 3166.16,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "influence the final outcome and this",
      "offset": 3167.68,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "will be extremely useful for training",
      "offset": 3169.52,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "knowledge as well as you'll see",
      "offset": 3170.8,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "so now i would like to do one more uh",
      "offset": 3172.88,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "example of manual backpropagation using",
      "offset": 3175.44,
      "duration": 6.879
    },
    {
      "lang": "en",
      "text": "a bit more complex and uh useful example",
      "offset": 3178.079,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "we are going to back propagate through a",
      "offset": 3182.319,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "neuron",
      "offset": 3184.079,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "so",
      "offset": 3185.359,
      "duration": 3.521
    },
    {
      "lang": "en",
      "text": "we want to eventually build up neural",
      "offset": 3187.44,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "networks and in the simplest case these",
      "offset": 3188.88,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "are multilateral perceptrons as they're",
      "offset": 3190.88,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "called so this is a two layer neural net",
      "offset": 3192.559,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "and it's got these hidden layers made up",
      "offset": 3195.599,
      "duration": 3.041
    },
    {
      "lang": "en",
      "text": "of neurons and these neurons are fully",
      "offset": 3197.04,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "connected to each other",
      "offset": 3198.64,
      "duration": 2.959
    },
    {
      "lang": "en",
      "text": "now biologically neurons are very",
      "offset": 3200,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "complicated devices but we have very",
      "offset": 3201.599,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "simple mathematical models of them",
      "offset": 3203.599,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "and so this is a very simple",
      "offset": 3206.079,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "mathematical model of a neuron you have",
      "offset": 3207.52,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "some inputs axis",
      "offset": 3209.68,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "and then you have these synapses that",
      "offset": 3211.76,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "have weights on them so",
      "offset": 3213.68,
      "duration": 5.919
    },
    {
      "lang": "en",
      "text": "the w's are weights",
      "offset": 3216.64,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "and then",
      "offset": 3219.599,
      "duration": 3.041
    },
    {
      "lang": "en",
      "text": "the synapse interacts with the input to",
      "offset": 3220.8,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "this neuron multiplicatively so what",
      "offset": 3222.64,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "flows to the cell body",
      "offset": 3224.88,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "of this neuron is w times x",
      "offset": 3227.2,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "but there's multiple inputs so there's",
      "offset": 3229.92,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "many w times x's flowing into the cell",
      "offset": 3231.359,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "body",
      "offset": 3233.92,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "the cell body then has also like some",
      "offset": 3234.96,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "bias",
      "offset": 3236.96,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "so this is kind of like the",
      "offset": 3237.92,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "inert innate sort of trigger happiness",
      "offset": 3239.68,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "of this neuron so this bias can make it",
      "offset": 3242.4,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "a bit more trigger happy or a bit less",
      "offset": 3244.64,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "trigger happy regardless of the input",
      "offset": 3246.079,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "but basically we're taking all the w",
      "offset": 3248.4,
      "duration": 2.959
    },
    {
      "lang": "en",
      "text": "times x",
      "offset": 3250.24,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "of all the inputs adding the bias and",
      "offset": 3251.359,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "then we take it through an activation",
      "offset": 3253.839,
      "duration": 3.121
    },
    {
      "lang": "en",
      "text": "function",
      "offset": 3255.599,
      "duration": 2.801
    },
    {
      "lang": "en",
      "text": "and this activation function is usually",
      "offset": 3256.96,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "some kind of a squashing function",
      "offset": 3258.4,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "like a sigmoid or 10h or something like",
      "offset": 3260.319,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "that so as an example",
      "offset": 3262.48,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "we're going to use the 10h in this",
      "offset": 3264.72,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "example",
      "offset": 3266.319,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "numpy has a",
      "offset": 3268.16,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "np.10h",
      "offset": 3269.68,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "so",
      "offset": 3271.28,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "we can call it on a range",
      "offset": 3272.48,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "and we can plot it",
      "offset": 3274.96,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "this is the 10h function and you see",
      "offset": 3276.559,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "that the inputs as they come in",
      "offset": 3278.48,
      "duration": 5.839
    },
    {
      "lang": "en",
      "text": "get squashed on the y coordinate here so",
      "offset": 3281.119,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "um",
      "offset": 3284.319,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "right at zero we're going to get exactly",
      "offset": 3285.28,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "zero and then as you go more positive in",
      "offset": 3287.359,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "the input",
      "offset": 3289.599,
      "duration": 2.801
    },
    {
      "lang": "en",
      "text": "then you'll see that the function will",
      "offset": 3290.799,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "only go up to one and then plateau out",
      "offset": 3292.4,
      "duration": 5.439
    },
    {
      "lang": "en",
      "text": "and so if you pass in very positive",
      "offset": 3295.76,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "inputs we're gonna cap it smoothly at",
      "offset": 3297.839,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "one and on the negative side we're gonna",
      "offset": 3300.16,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "cap it smoothly to negative one",
      "offset": 3302.16,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "so that's 10h",
      "offset": 3304.48,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "and that's the squashing function or an",
      "offset": 3306.4,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "activation function and what comes out",
      "offset": 3308.559,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "of this neuron is just the activation",
      "offset": 3310.72,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "function applied to the dot product of",
      "offset": 3312.64,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "the weights and the",
      "offset": 3314.96,
      "duration": 3.119
    },
    {
      "lang": "en",
      "text": "inputs",
      "offset": 3316.72,
      "duration": 2.639
    },
    {
      "lang": "en",
      "text": "so let's",
      "offset": 3318.079,
      "duration": 2.961
    },
    {
      "lang": "en",
      "text": "write one out",
      "offset": 3319.359,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "um",
      "offset": 3321.04,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "i'm going to copy paste because",
      "offset": 3322.559,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "i don't want to type too much",
      "offset": 3327.359,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "but okay so here we have the inputs",
      "offset": 3328.96,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "x1 x2 so this is a two-dimensional",
      "offset": 3331.52,
      "duration": 3.279
    },
    {
      "lang": "en",
      "text": "neuron so two inputs are going to come",
      "offset": 3333.359,
      "duration": 2.561
    },
    {
      "lang": "en",
      "text": "in",
      "offset": 3334.799,
      "duration": 2.641
    },
    {
      "lang": "en",
      "text": "these are thought out as the weights of",
      "offset": 3335.92,
      "duration": 2.879
    },
    {
      "lang": "en",
      "text": "this neuron",
      "offset": 3337.44,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "weights w1 w2 and these weights again",
      "offset": 3338.799,
      "duration": 5.121
    },
    {
      "lang": "en",
      "text": "are the synaptic strengths for each",
      "offset": 3341.599,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "input",
      "offset": 3343.92,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "and this is the bias of the neuron",
      "offset": 3345.119,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "b",
      "offset": 3347.44,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "and now we want to do is according to",
      "offset": 3349.119,
      "duration": 5.041
    },
    {
      "lang": "en",
      "text": "this model we need to multiply x1 times",
      "offset": 3351.28,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "w1",
      "offset": 3354.16,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "and x2 times w2",
      "offset": 3355.52,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "and then we need to add bias on top of",
      "offset": 3357.76,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "it",
      "offset": 3360.4,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "and it gets a little messy here but all",
      "offset": 3361.44,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "we are trying to do is x1 w1 plus x2 w2",
      "offset": 3363.28,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "plus b",
      "offset": 3366.96,
      "duration": 2.879
    },
    {
      "lang": "en",
      "text": "and these are multiply here",
      "offset": 3367.92,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "except i'm doing it in small steps so",
      "offset": 3369.839,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "that we actually have pointers to all",
      "offset": 3372.16,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "these intermediate nodes so we have x1",
      "offset": 3373.68,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "w1 variable x times x2 w2 variable and",
      "offset": 3375.76,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "i'm also labeling them",
      "offset": 3379.44,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "so n is now",
      "offset": 3381.68,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "the cell body raw",
      "offset": 3383.76,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "raw",
      "offset": 3385.44,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "activation without",
      "offset": 3386.64,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "the activation function for now",
      "offset": 3388.4,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "and this should be enough to basically",
      "offset": 3390.48,
      "duration": 6.079
    },
    {
      "lang": "en",
      "text": "plot it so draw dot of n",
      "offset": 3392.88,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "gives us x1 times w1 x2 times w2",
      "offset": 3397.839,
      "duration": 5.361
    },
    {
      "lang": "en",
      "text": "being added",
      "offset": 3401.92,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "then the bias gets added on top of this",
      "offset": 3403.2,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "and this n",
      "offset": 3405.599,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "is this sum",
      "offset": 3407.28,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "so we're now going to take it through an",
      "offset": 3409.359,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "activation function",
      "offset": 3410.88,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "and let's say we use the 10h",
      "offset": 3412.559,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "so that we produce the output",
      "offset": 3414.72,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "so what we'd like to do here is we'd",
      "offset": 3416.48,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "like to do the output and i'll call it o",
      "offset": 3418,
      "duration": 5.119
    },
    {
      "lang": "en",
      "text": "is um",
      "offset": 3421.44,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "n dot 10h",
      "offset": 3423.119,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "okay but we haven't yet written the 10h",
      "offset": 3425.2,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "now the reason that we need to implement",
      "offset": 3428.319,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "another 10h function here is that",
      "offset": 3429.839,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "tanh is a",
      "offset": 3432.88,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "hyperbolic function and we've only so",
      "offset": 3434.64,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "far implemented a plus and the times and",
      "offset": 3436.72,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "you can't make a 10h out of just pluses",
      "offset": 3438.48,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "and times",
      "offset": 3440.799,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "you also need exponentiation so 10h is",
      "offset": 3442,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "this kind of a formula here",
      "offset": 3445.119,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "you can use either one of these and you",
      "offset": 3447.2,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "see that there's exponentiation involved",
      "offset": 3448.72,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "which we have not implemented yet for",
      "offset": 3450.4,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "our low value node here so we're not",
      "offset": 3452.64,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "going to be able to produce 10h yet and",
      "offset": 3454.72,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "we have to go back up and implement",
      "offset": 3456.16,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "something like it",
      "offset": 3457.52,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "now one option here",
      "offset": 3459.04,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "is we could actually implement um",
      "offset": 3462.64,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "exponentiation",
      "offset": 3464.88,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "right and we could return the x of a",
      "offset": 3466.559,
      "duration": 5.841
    },
    {
      "lang": "en",
      "text": "value instead of a 10h of a value",
      "offset": 3469.76,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "because if we had x then we have",
      "offset": 3472.4,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "everything else that we need so um",
      "offset": 3474.319,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "because we know how to add and we know",
      "offset": 3476.96,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "how to",
      "offset": 3478.96,
      "duration": 2.32
    },
    {
      "lang": "en",
      "text": "um",
      "offset": 3480,
      "duration": 2.64
    },
    {
      "lang": "en",
      "text": "we know how to add and we know how to",
      "offset": 3481.28,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "multiply so we'd be able to create 10h",
      "offset": 3482.64,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "if we knew how to x",
      "offset": 3484.96,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "but for the purposes of this example i",
      "offset": 3486.88,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "specifically wanted to",
      "offset": 3488.319,
      "duration": 2.721
    },
    {
      "lang": "en",
      "text": "show you",
      "offset": 3490,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "that we don't necessarily need to have",
      "offset": 3491.04,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "the most atomic pieces",
      "offset": 3493.04,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "in",
      "offset": 3495.2,
      "duration": 1.68
    },
    {
      "lang": "en",
      "text": "um",
      "offset": 3496,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "in this value object we can actually",
      "offset": 3496.88,
      "duration": 6.239
    },
    {
      "lang": "en",
      "text": "like create functions at arbitrary",
      "offset": 3499.28,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "points of abstraction they can be",
      "offset": 3503.119,
      "duration": 3.281
    },
    {
      "lang": "en",
      "text": "complicated functions but they can be",
      "offset": 3504.72,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "also very very simple functions like a",
      "offset": 3506.4,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "plus and it's totally up to us the only",
      "offset": 3507.92,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "thing that matters is that we know how",
      "offset": 3510.319,
      "duration": 3.121
    },
    {
      "lang": "en",
      "text": "to differentiate through any one",
      "offset": 3511.92,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "function so we take some inputs and we",
      "offset": 3513.44,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "make an output the only thing that",
      "offset": 3515.76,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "matters it can be arbitrarily complex",
      "offset": 3517.2,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "function as long as you know how to",
      "offset": 3518.88,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "create the local derivative if you know",
      "offset": 3521.359,
      "duration": 3.521
    },
    {
      "lang": "en",
      "text": "the local derivative of how the inputs",
      "offset": 3523.28,
      "duration": 3.279
    },
    {
      "lang": "en",
      "text": "impact the output then that's all you",
      "offset": 3524.88,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "need so we're going to cluster up",
      "offset": 3526.559,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "all of this expression and we're not",
      "offset": 3529.52,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "going to break it down to its atomic",
      "offset": 3531.44,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "pieces we're just going to directly",
      "offset": 3532.64,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "implement tanh",
      "offset": 3534.16,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "so let's do that",
      "offset": 3535.52,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "depth nh",
      "offset": 3537.28,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "and then out will be a value",
      "offset": 3539.2,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "of",
      "offset": 3542.4,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "and we need this expression here so",
      "offset": 3543.599,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "um",
      "offset": 3545.92,
      "duration": 2.32
    },
    {
      "lang": "en",
      "text": "let me actually",
      "offset": 3548.48,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "copy paste",
      "offset": 3550.319,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "let's grab n which is a cell.theta",
      "offset": 3554.16,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "and then this",
      "offset": 3557.2,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "i believe is the tan h",
      "offset": 3558.72,
      "duration": 5.839
    },
    {
      "lang": "en",
      "text": "math.x of",
      "offset": 3561.44,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "two",
      "offset": 3564.559,
      "duration": 2.641
    },
    {
      "lang": "en",
      "text": "no n",
      "offset": 3565.92,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "n minus one over",
      "offset": 3567.2,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "two n plus one",
      "offset": 3568.88,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "maybe i can call this x",
      "offset": 3570.559,
      "duration": 5.121
    },
    {
      "lang": "en",
      "text": "just so that it matches exactly",
      "offset": 3573.04,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "okay and now",
      "offset": 3575.68,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "this will be t",
      "offset": 3577.52,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "and uh children of this node there's",
      "offset": 3580.319,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "just one child",
      "offset": 3582.559,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "and i'm wrapping it in a tuple so this",
      "offset": 3584,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "is a tuple of one object just self",
      "offset": 3586,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "and here the name of this operation will",
      "offset": 3588.559,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "be 10h",
      "offset": 3590.799,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "and we're going to return that",
      "offset": 3592.24,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "okay",
      "offset": 3596.319,
      "duration": 5.681
    },
    {
      "lang": "en",
      "text": "so now valley should be implementing 10h",
      "offset": 3598.559,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "and now we can scroll all the way down",
      "offset": 3602,
      "duration": 2.559
    },
    {
      "lang": "en",
      "text": "here",
      "offset": 3603.28,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "and we can actually do n.10 h and that's",
      "offset": 3604.559,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "going to return the tanhd",
      "offset": 3606.88,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "output of n",
      "offset": 3609.2,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "and now we should be able to draw it out",
      "offset": 3611.28,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "of o not of n",
      "offset": 3612.64,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "so let's see how that worked",
      "offset": 3614.72,
      "duration": 3.119
    },
    {
      "lang": "en",
      "text": "there we go",
      "offset": 3618.64,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "n went through 10 h",
      "offset": 3619.68,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "to produce this output",
      "offset": 3621.76,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "so now tan h is a",
      "offset": 3624.24,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "sort of",
      "offset": 3626.16,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "our little micro grad supported node",
      "offset": 3627.76,
      "duration": 5.359
    },
    {
      "lang": "en",
      "text": "here as an operation",
      "offset": 3630,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "and as long as we know the derivative of",
      "offset": 3633.119,
      "duration": 3.041
    },
    {
      "lang": "en",
      "text": "10h",
      "offset": 3635.2,
      "duration": 2.32
    },
    {
      "lang": "en",
      "text": "then we'll be able to back propagate",
      "offset": 3636.16,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "through it now let's see this 10h in",
      "offset": 3637.52,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "action currently it's not squashing too",
      "offset": 3639.52,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "much because the input to it is pretty",
      "offset": 3641.599,
      "duration": 5.361
    },
    {
      "lang": "en",
      "text": "low so if the bias was increased to say",
      "offset": 3643.92,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "eight",
      "offset": 3646.96,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "then we'll see that what's flowing into",
      "offset": 3649.119,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "the 10h now is",
      "offset": 3651.2,
      "duration": 3.119
    },
    {
      "lang": "en",
      "text": "two",
      "offset": 3653.04,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "and 10h is squashing it to 0.96 so we're",
      "offset": 3654.319,
      "duration": 5.601
    },
    {
      "lang": "en",
      "text": "already hitting the tail of this 10h and",
      "offset": 3657.28,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "it will sort of smoothly go up to 1 and",
      "offset": 3659.92,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "then plateau out over there",
      "offset": 3661.599,
      "duration": 3.041
    },
    {
      "lang": "en",
      "text": "okay so now i'm going to do something",
      "offset": 3663.359,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "slightly strange i'm going to change",
      "offset": 3664.64,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "this bias from 8 to this number",
      "offset": 3666.559,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "6.88 etc",
      "offset": 3669.76,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "and i'm going to do this for specific",
      "offset": 3671.839,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "reasons because we're about to start",
      "offset": 3673.359,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "back propagation",
      "offset": 3675.52,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "and i want to make sure that our numbers",
      "offset": 3676.96,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "come out nice they're not like very",
      "offset": 3679.04,
      "duration": 3.519
    },
    {
      "lang": "en",
      "text": "crazy numbers they're nice numbers that",
      "offset": 3681.04,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "we can sort of understand in our head",
      "offset": 3682.559,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "let me also add a pose label",
      "offset": 3684.72,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "o is short for output here",
      "offset": 3686.96,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "so that's zero",
      "offset": 3690,
      "duration": 2.64
    },
    {
      "lang": "en",
      "text": "okay so",
      "offset": 3691.52,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "0.88 flows into 10 h comes out 0.7 so on",
      "offset": 3692.64,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "so now we're going to do back",
      "offset": 3696.079,
      "duration": 2.641
    },
    {
      "lang": "en",
      "text": "propagation and we're going to fill in",
      "offset": 3697.119,
      "duration": 3.121
    },
    {
      "lang": "en",
      "text": "all the gradients",
      "offset": 3698.72,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "so what is the derivative o with respect",
      "offset": 3700.24,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "to",
      "offset": 3703.2,
      "duration": 1.84
    },
    {
      "lang": "en",
      "text": "all the",
      "offset": 3704,
      "duration": 3.119
    },
    {
      "lang": "en",
      "text": "inputs here and of course in the typical",
      "offset": 3705.04,
      "duration": 3.519
    },
    {
      "lang": "en",
      "text": "neural network setting what we really",
      "offset": 3707.119,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "care about the most is the derivative of",
      "offset": 3708.559,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "these neurons on the weights",
      "offset": 3711.359,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "specifically the w2 and w1 because those",
      "offset": 3713.28,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "are the weights that we're going to be",
      "offset": 3716.319,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "changing part of the optimization",
      "offset": 3717.359,
      "duration": 3.281
    },
    {
      "lang": "en",
      "text": "and the other thing that we have to",
      "offset": 3719.68,
      "duration": 2.56
    },
    {
      "lang": "en",
      "text": "remember is here we have only a single",
      "offset": 3720.64,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "neuron but in the neural natives",
      "offset": 3722.24,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "typically have many neurons and they're",
      "offset": 3723.76,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "connected",
      "offset": 3724.96,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "so this is only like a one small neuron",
      "offset": 3727.119,
      "duration": 3.521
    },
    {
      "lang": "en",
      "text": "a piece of a much bigger puzzle and",
      "offset": 3729.119,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "eventually there's a loss function that",
      "offset": 3730.64,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "sort of measures the accuracy of the",
      "offset": 3732.48,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "neural net and we're back propagating",
      "offset": 3733.839,
      "duration": 3.041
    },
    {
      "lang": "en",
      "text": "with respect to that accuracy and trying",
      "offset": 3735.28,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "to increase it",
      "offset": 3736.88,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "so let's start off by propagation here",
      "offset": 3739.2,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "in the end",
      "offset": 3741.119,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "what is the derivative of o with respect",
      "offset": 3742.48,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "to o the base case sort of we know",
      "offset": 3744.319,
      "duration": 6.161
    },
    {
      "lang": "en",
      "text": "always is that the gradient is just 1.0",
      "offset": 3746.96,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "so let me fill it in",
      "offset": 3750.48,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "and then let me",
      "offset": 3752.4,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "split out",
      "offset": 3755.119,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "the drawing function",
      "offset": 3757.119,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "here",
      "offset": 3760.079,
      "duration": 2.24
    },
    {
      "lang": "en",
      "text": "and then here cell",
      "offset": 3763.68,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "clear this output here okay",
      "offset": 3767.28,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "so now when we draw o we'll see that oh",
      "offset": 3770.079,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "that grad is one",
      "offset": 3772.48,
      "duration": 2.639
    },
    {
      "lang": "en",
      "text": "so now we're going to back propagate",
      "offset": 3773.92,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "through the tan h",
      "offset": 3775.119,
      "duration": 3.521
    },
    {
      "lang": "en",
      "text": "so to back propagate through 10h we need",
      "offset": 3776.64,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "to know the local derivative of 10h",
      "offset": 3778.64,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "so if we have that",
      "offset": 3781.2,
      "duration": 5.839
    },
    {
      "lang": "en",
      "text": "o is 10 h of",
      "offset": 3783.68,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "n",
      "offset": 3787.039,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "then what is d o by d n",
      "offset": 3788.4,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "now what you could do is you could come",
      "offset": 3792,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "here and you could take this expression",
      "offset": 3793.599,
      "duration": 2.801
    },
    {
      "lang": "en",
      "text": "and you could",
      "offset": 3795.28,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "do your calculus derivative taking",
      "offset": 3796.4,
      "duration": 4.959
    },
    {
      "lang": "en",
      "text": "um and that would work but we can also",
      "offset": 3799.119,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "just scroll down wikipedia here",
      "offset": 3801.359,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "into a section that hopefully tells us",
      "offset": 3803.76,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "that derivative uh",
      "offset": 3806.799,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "d by dx of 10 h of x is",
      "offset": 3808.96,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "any of these i like this one 1 minus 10",
      "offset": 3811.68,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "h square of x",
      "offset": 3813.76,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "so this is 1 minus 10 h",
      "offset": 3815.2,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "of x squared",
      "offset": 3817.44,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "so basically what this is saying is that",
      "offset": 3819.359,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "d o by d n",
      "offset": 3821.76,
      "duration": 2.799
    },
    {
      "lang": "en",
      "text": "is",
      "offset": 3823.68,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "1 minus 10 h",
      "offset": 3824.559,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "of n",
      "offset": 3827.44,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "squared",
      "offset": 3828.88,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "and we already have 10 h of n that's",
      "offset": 3831.039,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "just o",
      "offset": 3832.88,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "so it's one minus o squared",
      "offset": 3834.48,
      "duration": 5.359
    },
    {
      "lang": "en",
      "text": "so o is the output here so the output is",
      "offset": 3836.48,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "this number",
      "offset": 3839.839,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "data",
      "offset": 3842.16,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "is this number",
      "offset": 3844.24,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "and then",
      "offset": 3846.48,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "what this is saying is that do by dn is",
      "offset": 3848.079,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "1 minus",
      "offset": 3850.4,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "this squared so",
      "offset": 3851.68,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "one minus of that data squared",
      "offset": 3853.28,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "is 0.5 conveniently",
      "offset": 3856.48,
      "duration": 5.119
    },
    {
      "lang": "en",
      "text": "so the local derivative of this 10 h",
      "offset": 3858.96,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "operation here is 0.5",
      "offset": 3861.599,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "and",
      "offset": 3864.24,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "so that would be d o by d n",
      "offset": 3865.2,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "so",
      "offset": 3867.44,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "we can fill in that in that grad",
      "offset": 3868.559,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "is 0.5 we'll just fill in",
      "offset": 3873.28,
      "duration": 4.68
    },
    {
      "lang": "en",
      "text": "so this is exactly 0.5 one half",
      "offset": 3882.48,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "so now we're going to continue the back",
      "offset": 3885.2,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "propagation",
      "offset": 3887.119,
      "duration": 5.041
    },
    {
      "lang": "en",
      "text": "this is 0.5 and this is a plus node",
      "offset": 3889.28,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "so how is backprop going to what is that",
      "offset": 3892.16,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "going to do here",
      "offset": 3895.28,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "and if you remember our previous example",
      "offset": 3896.64,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "a plus is just a distributor of gradient",
      "offset": 3898.4,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "so this gradient will simply flow to",
      "offset": 3901.68,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "both of these equally and that's because",
      "offset": 3903.599,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "the local derivative of this operation",
      "offset": 3905.76,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "is one for every one of its nodes so 1",
      "offset": 3907.44,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "times 0.5 is 0.5",
      "offset": 3910.72,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "so therefore we know that",
      "offset": 3912.88,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "this node here which we called this",
      "offset": 3914.96,
      "duration": 6.079
    },
    {
      "lang": "en",
      "text": "its grad is just 0.5",
      "offset": 3918.64,
      "duration": 6.159
    },
    {
      "lang": "en",
      "text": "and we know that b dot grad is also 0.5",
      "offset": 3921.039,
      "duration": 7.361
    },
    {
      "lang": "en",
      "text": "so let's set those and let's draw",
      "offset": 3924.799,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "so 0.5",
      "offset": 3928.88,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "continuing we have another plus",
      "offset": 3930.48,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "0.5 again we'll just distribute it so",
      "offset": 3932.64,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "0.5 will flow to both of these",
      "offset": 3934.96,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "so we can set",
      "offset": 3937.28,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "theirs",
      "offset": 3939.2,
      "duration": 2.32
    },
    {
      "lang": "en",
      "text": "x2w2 as well that grad is 0.5",
      "offset": 3943.799,
      "duration": 6.441
    },
    {
      "lang": "en",
      "text": "and let's redraw pluses are my favorite",
      "offset": 3947.92,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "uh operations to back propagate through",
      "offset": 3950.24,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "because",
      "offset": 3951.92,
      "duration": 3.119
    },
    {
      "lang": "en",
      "text": "it's very simple",
      "offset": 3953.2,
      "duration": 3.119
    },
    {
      "lang": "en",
      "text": "so now it's flowing into these",
      "offset": 3955.039,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "expressions is 0.5 and so really again",
      "offset": 3956.319,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "keep in mind what the derivative is",
      "offset": 3958.48,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "telling us at every point in time along",
      "offset": 3959.599,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "here this is saying that",
      "offset": 3961.68,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "if we want the output of this neuron to",
      "offset": 3964.48,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "increase",
      "offset": 3966.079,
      "duration": 2.801
    },
    {
      "lang": "en",
      "text": "then",
      "offset": 3968.079,
      "duration": 2.801
    },
    {
      "lang": "en",
      "text": "the influence on these expressions is",
      "offset": 3968.88,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "positive on the output both of them are",
      "offset": 3970.88,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "positive",
      "offset": 3973.359,
      "duration": 2.401
    },
    {
      "lang": "en",
      "text": "contribution to the output",
      "offset": 3976.96,
      "duration": 3.119
    },
    {
      "lang": "en",
      "text": "so now back propagating to x2 and w2",
      "offset": 3980.48,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "first",
      "offset": 3983.2,
      "duration": 3.119
    },
    {
      "lang": "en",
      "text": "this is a times node so we know that the",
      "offset": 3984.16,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "local derivative is you know the other",
      "offset": 3986.319,
      "duration": 2.641
    },
    {
      "lang": "en",
      "text": "term",
      "offset": 3988.079,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "so if we want to calculate x2.grad",
      "offset": 3988.96,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "then",
      "offset": 3992.799,
      "duration": 2.161
    },
    {
      "lang": "en",
      "text": "can you think through what it's going to",
      "offset": 3993.599,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "be",
      "offset": 3994.96,
      "duration": 2.24
    },
    {
      "lang": "en",
      "text": "so x2.grad will be",
      "offset": 4000.88,
      "duration": 3.959
    },
    {
      "lang": "en",
      "text": "w2.data",
      "offset": 4002.799,
      "duration": 5.441
    },
    {
      "lang": "en",
      "text": "times this x2w2",
      "offset": 4004.839,
      "duration": 6.28
    },
    {
      "lang": "en",
      "text": "by grad right",
      "offset": 4008.24,
      "duration": 3.879
    },
    {
      "lang": "en",
      "text": "and",
      "offset": 4011.119,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "w2.grad will be",
      "offset": 4012.119,
      "duration": 8.48
    },
    {
      "lang": "en",
      "text": "x2 that data times x2w2.grad",
      "offset": 4015.68,
      "duration": 4.919
    },
    {
      "lang": "en",
      "text": "right so that's the local piece of chain",
      "offset": 4021.359,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "rule",
      "offset": 4023.92,
      "duration": 2.32
    },
    {
      "lang": "en",
      "text": "let's set them and let's redraw",
      "offset": 4027.039,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "so here we see that the gradient on our",
      "offset": 4029.76,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "weight 2 is 0 because x2 data was 0",
      "offset": 4031.28,
      "duration": 7.36
    },
    {
      "lang": "en",
      "text": "right but x2 will have the gradient 0.5",
      "offset": 4035.68,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "because data here was 1.",
      "offset": 4038.64,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "and so what's interesting here right is",
      "offset": 4040.559,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "because the input x2 was 0 then because",
      "offset": 4042.559,
      "duration": 5.921
    },
    {
      "lang": "en",
      "text": "of the way the times works",
      "offset": 4045.839,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "of course this gradient will be zero and",
      "offset": 4048.48,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "think about intuitively why that is",
      "offset": 4050.319,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "derivative always tells us the influence",
      "offset": 4053.2,
      "duration": 2.879
    },
    {
      "lang": "en",
      "text": "of",
      "offset": 4055.28,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "this on the final output if i wiggle w2",
      "offset": 4056.079,
      "duration": 5.441
    },
    {
      "lang": "en",
      "text": "how is the output changing",
      "offset": 4059.76,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "it's not changing because we're",
      "offset": 4061.52,
      "duration": 2.799
    },
    {
      "lang": "en",
      "text": "multiplying by zero",
      "offset": 4062.72,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "so because it's not changing there's no",
      "offset": 4064.319,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "derivative and zero is the correct",
      "offset": 4066,
      "duration": 2.799
    },
    {
      "lang": "en",
      "text": "answer",
      "offset": 4067.599,
      "duration": 2.161
    },
    {
      "lang": "en",
      "text": "because we're",
      "offset": 4068.799,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "squashing it at zero",
      "offset": 4069.76,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "and let's do it here point five should",
      "offset": 4072.16,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "come here and flow through this times",
      "offset": 4074.559,
      "duration": 7.361
    },
    {
      "lang": "en",
      "text": "and so we'll have that x1.grad is",
      "offset": 4077.68,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "can you think through a little bit what",
      "offset": 4081.92,
      "duration": 2.639
    },
    {
      "lang": "en",
      "text": "what",
      "offset": 4083.68,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "this should be",
      "offset": 4084.559,
      "duration": 2.48
    },
    {
      "lang": "en",
      "text": "the local derivative of times",
      "offset": 4087.28,
      "duration": 5.279
    },
    {
      "lang": "en",
      "text": "with respect to x1 is going to be w1",
      "offset": 4089.44,
      "duration": 5.919
    },
    {
      "lang": "en",
      "text": "so w1 is data times",
      "offset": 4092.559,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "x1 w1 dot grad",
      "offset": 4095.359,
      "duration": 8.241
    },
    {
      "lang": "en",
      "text": "and w1.grad will be x1.data times",
      "offset": 4098.799,
      "duration": 8.4
    },
    {
      "lang": "en",
      "text": "x1 w2 w1 with graph",
      "offset": 4103.6,
      "duration": 5.679
    },
    {
      "lang": "en",
      "text": "let's see what those came out to be",
      "offset": 4107.199,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "so this is 0.5 so this would be negative",
      "offset": 4109.279,
      "duration": 5.361
    },
    {
      "lang": "en",
      "text": "1.5 and this would be 1.",
      "offset": 4111.679,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "and we've back propagated through this",
      "offset": 4114.64,
      "duration": 3.519
    },
    {
      "lang": "en",
      "text": "expression these are the actual final",
      "offset": 4116.48,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "derivatives so if we want this neuron's",
      "offset": 4118.159,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "output to increase",
      "offset": 4120.88,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "we know that what's necessary is that",
      "offset": 4123.839,
      "duration": 5.761
    },
    {
      "lang": "en",
      "text": "w2 we have no gradient w2 doesn't",
      "offset": 4127.12,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "actually matter to this neuron right now",
      "offset": 4129.6,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "but this neuron this weight should uh go",
      "offset": 4131.44,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "up",
      "offset": 4134.319,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "so if this weight goes up then this",
      "offset": 4135.12,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "neuron's output would have gone up and",
      "offset": 4137.279,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "proportionally because the gradient is",
      "offset": 4139.679,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "one okay so doing the back propagation",
      "offset": 4141.199,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "manually is obviously ridiculous so we",
      "offset": 4143.44,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "are now going to put an end to this",
      "offset": 4145.359,
      "duration": 3.521
    },
    {
      "lang": "en",
      "text": "suffering and we're going to see how we",
      "offset": 4146.88,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "can implement uh the backward pass a bit",
      "offset": 4148.88,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "more automatically we're not going to be",
      "offset": 4151.04,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "doing all of it manually out here",
      "offset": 4152.719,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "it's now pretty obvious to us by example",
      "offset": 4154.88,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "how these pluses and times are back",
      "offset": 4157.04,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "property ingredients so let's go up to",
      "offset": 4158.56,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "the value",
      "offset": 4160.96,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "object and we're going to start",
      "offset": 4162.159,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "codifying what we've seen",
      "offset": 4164.48,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "in the examples below",
      "offset": 4167.12,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "so we're going to do this by storing a",
      "offset": 4169.52,
      "duration": 5.279
    },
    {
      "lang": "en",
      "text": "special cell dot backward",
      "offset": 4171.279,
      "duration": 5.841
    },
    {
      "lang": "en",
      "text": "and underscore backward and this will be",
      "offset": 4174.799,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "a function which is going to do that",
      "offset": 4177.12,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "little piece of chain rule at each",
      "offset": 4179.52,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "little node that compute that took",
      "offset": 4181.44,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "inputs and produced output uh we're",
      "offset": 4183.199,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "going to store",
      "offset": 4185.44,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "how we are going to chain the the",
      "offset": 4186.719,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "outputs gradient into the inputs",
      "offset": 4189.04,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "gradients",
      "offset": 4191.04,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "so by default",
      "offset": 4192.239,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "this will be a function",
      "offset": 4194,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "that uh doesn't do anything",
      "offset": 4195.6,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "so um",
      "offset": 4198.159,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "and you can also see that here in the",
      "offset": 4199.92,
      "duration": 3.279
    },
    {
      "lang": "en",
      "text": "value in micrograb",
      "offset": 4201.199,
      "duration": 2.801
    },
    {
      "lang": "en",
      "text": "so",
      "offset": 4203.199,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "with this backward function by default",
      "offset": 4204,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "doesn't do anything",
      "offset": 4206.56,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "this is an empty function",
      "offset": 4208.4,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "and that would be sort of the case for",
      "offset": 4210.159,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "example for a leaf node for leaf node",
      "offset": 4211.36,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "there's nothing to do",
      "offset": 4213.199,
      "duration": 5.121
    },
    {
      "lang": "en",
      "text": "but now if when we're creating these out",
      "offset": 4215.76,
      "duration": 5.919
    },
    {
      "lang": "en",
      "text": "values these out values are an addition",
      "offset": 4218.32,
      "duration": 5.839
    },
    {
      "lang": "en",
      "text": "of self and other",
      "offset": 4221.679,
      "duration": 5.441
    },
    {
      "lang": "en",
      "text": "and so we will want to sell set",
      "offset": 4224.159,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "outs backward to be",
      "offset": 4227.12,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "the function that propagates the",
      "offset": 4229.36,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "gradient",
      "offset": 4231.28,
      "duration": 2.399
    },
    {
      "lang": "en",
      "text": "so",
      "offset": 4234.159,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "let's define what should happen",
      "offset": 4235.36,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "and we're going to store it in a closure",
      "offset": 4240.48,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "let's define what should happen when we",
      "offset": 4242.159,
      "duration": 2.881
    },
    {
      "lang": "en",
      "text": "call",
      "offset": 4244.08,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "outs grad",
      "offset": 4245.04,
      "duration": 2.639
    },
    {
      "lang": "en",
      "text": "for in addition",
      "offset": 4247.76,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "our job is to take",
      "offset": 4250,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "outs grad and propagate it into self's",
      "offset": 4252,
      "duration": 5.679
    },
    {
      "lang": "en",
      "text": "grad and other grad so basically we want",
      "offset": 4255.04,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "to sell self.grad to something",
      "offset": 4257.679,
      "duration": 5.281
    },
    {
      "lang": "en",
      "text": "and we want to set others.grad to",
      "offset": 4260.56,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "something",
      "offset": 4262.96,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "okay",
      "offset": 4264.4,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "and the way we saw below how chain rule",
      "offset": 4265.76,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "works we want to take the local",
      "offset": 4268.32,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "derivative times",
      "offset": 4270,
      "duration": 2.719
    },
    {
      "lang": "en",
      "text": "the",
      "offset": 4271.679,
      "duration": 2.721
    },
    {
      "lang": "en",
      "text": "sort of global derivative i should call",
      "offset": 4272.719,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "it which is the derivative of the final",
      "offset": 4274.4,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "output of the expression with respect to",
      "offset": 4276.159,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "out's data",
      "offset": 4278.8,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "with respect to out",
      "offset": 4281.04,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "so",
      "offset": 4282.96,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "the local derivative of self in an",
      "offset": 4284.719,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "addition is 1.0",
      "offset": 4287.28,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "so it's just 1.0 times",
      "offset": 4289.6,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "outs grad",
      "offset": 4291.76,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "that's the chain rule",
      "offset": 4294.4,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "and others.grad will be 1.0 times",
      "offset": 4295.92,
      "duration": 3.279
    },
    {
      "lang": "en",
      "text": "outgrad",
      "offset": 4298.08,
      "duration": 2.4
    },
    {
      "lang": "en",
      "text": "and what you basically what you're",
      "offset": 4299.199,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "seeing here is that outscrad",
      "offset": 4300.48,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "will simply be copied onto selfs grad",
      "offset": 4302.719,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "and others grad as we saw happens for an",
      "offset": 4305.52,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "addition operation",
      "offset": 4308.159,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "so we're going to later call this",
      "offset": 4309.92,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "function to propagate the gradient",
      "offset": 4311.52,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "having done an addition",
      "offset": 4313.44,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "let's now do multiplication we're going",
      "offset": 4315.76,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "to also define that backward",
      "offset": 4317.76,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "and we're going to set its backward to",
      "offset": 4322.32,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "be backward",
      "offset": 4324.64,
      "duration": 2.64
    },
    {
      "lang": "en",
      "text": "and we want to chain outgrad into",
      "offset": 4327.679,
      "duration": 6.161
    },
    {
      "lang": "en",
      "text": "self.grad",
      "offset": 4331.12,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "and others.grad",
      "offset": 4334.32,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "and this will be a little piece of chain",
      "offset": 4337.12,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "rule for multiplication",
      "offset": 4338.48,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "so we'll have",
      "offset": 4340.32,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "so what should this be",
      "offset": 4341.679,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "can you think through",
      "offset": 4343.28,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "so what is the local derivative",
      "offset": 4348.64,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "here the local derivative was",
      "offset": 4350.8,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "others.data",
      "offset": 4352.32,
      "duration": 2.879
    },
    {
      "lang": "en",
      "text": "and then",
      "offset": 4355.44,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "oops others.data and the times of that",
      "offset": 4356.56,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "grad that's channel",
      "offset": 4359.6,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "and here we have self.data times of that",
      "offset": 4362.56,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "grad",
      "offset": 4364.8,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "that's what we've been doing",
      "offset": 4365.6,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "and finally here for 10 h",
      "offset": 4369.6,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "left backward",
      "offset": 4371.679,
      "duration": 2.721
    },
    {
      "lang": "en",
      "text": "and then we want to set out backwards to",
      "offset": 4374.8,
      "duration": 5.439
    },
    {
      "lang": "en",
      "text": "be just backward",
      "offset": 4377.36,
      "duration": 2.879
    },
    {
      "lang": "en",
      "text": "and here we need to",
      "offset": 4380.48,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "back propagate we have out that grad and",
      "offset": 4382.56,
      "duration": 6.88
    },
    {
      "lang": "en",
      "text": "we want to chain it into self.grad",
      "offset": 4384.96,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "and salt.grad will be",
      "offset": 4389.679,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "the local derivative of this operation",
      "offset": 4391.76,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "that we've done here which is 10h",
      "offset": 4393.76,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "and so we saw that the local the",
      "offset": 4396.159,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "gradient is 1 minus the tan h of x",
      "offset": 4397.52,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "squared which here is t",
      "offset": 4400.56,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "that's the local derivative because",
      "offset": 4403.6,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "that's t is the output of this 10 h so 1",
      "offset": 4405.44,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "minus t squared is the local derivative",
      "offset": 4407.679,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "and then gradient um",
      "offset": 4410,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "has to be multiplied because of the",
      "offset": 4412.48,
      "duration": 2.4
    },
    {
      "lang": "en",
      "text": "chain rule",
      "offset": 4413.84,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "so outgrad is chained through the local",
      "offset": 4414.88,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "gradient into salt.grad",
      "offset": 4416.64,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "and that should be basically it so we're",
      "offset": 4419.36,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "going to redefine our value node",
      "offset": 4421.84,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "we're going to swing all the way down",
      "offset": 4424.88,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "here",
      "offset": 4426.239,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "and we're going to",
      "offset": 4428.08,
      "duration": 3.119
    },
    {
      "lang": "en",
      "text": "redefine",
      "offset": 4429.92,
      "duration": 2.64
    },
    {
      "lang": "en",
      "text": "our expression",
      "offset": 4431.199,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "make sure that all the grads are zero",
      "offset": 4432.56,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "okay",
      "offset": 4435.28,
      "duration": 2.32
    },
    {
      "lang": "en",
      "text": "but now we don't have to do this",
      "offset": 4436.239,
      "duration": 3.521
    },
    {
      "lang": "en",
      "text": "manually anymore",
      "offset": 4437.6,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "we are going to basically be calling the",
      "offset": 4439.76,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "dot backward in the right order",
      "offset": 4441.28,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "so",
      "offset": 4444,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "first we want to call os",
      "offset": 4445.04,
      "duration": 6.6
    },
    {
      "lang": "en",
      "text": "dot backwards",
      "offset": 4447.84,
      "duration": 3.8
    },
    {
      "lang": "en",
      "text": "so o was the outcome of 10h",
      "offset": 4454,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "right so calling all that those who's",
      "offset": 4457.92,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "backward",
      "offset": 4460.48,
      "duration": 2.64
    },
    {
      "lang": "en",
      "text": "will be",
      "offset": 4462.159,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "this function this is what it will do",
      "offset": 4463.12,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "now we have to be careful because",
      "offset": 4466,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "there's a times out.grad",
      "offset": 4469.12,
      "duration": 5.119
    },
    {
      "lang": "en",
      "text": "and out.grad remember is initialized to",
      "offset": 4471.76,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "zero",
      "offset": 4474.239,
      "duration": 2.401
    },
    {
      "lang": "en",
      "text": "so here we see grad zero so as a base",
      "offset": 4478.88,
      "duration": 7.76
    },
    {
      "lang": "en",
      "text": "case we need to set both.grad to 1.0",
      "offset": 4481.36,
      "duration": 8.72
    },
    {
      "lang": "en",
      "text": "to initialize this with 1",
      "offset": 4486.64,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "and then once this is 1 we can call oda",
      "offset": 4493.52,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "backward",
      "offset": 4496.08,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "and what that should do is it should",
      "offset": 4497.199,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "propagate this grad through 10h",
      "offset": 4498.96,
      "duration": 5.279
    },
    {
      "lang": "en",
      "text": "so the local derivative times",
      "offset": 4502.159,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "the global derivative which is",
      "offset": 4504.239,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "initialized at one so",
      "offset": 4505.92,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "this should",
      "offset": 4508.239,
      "duration": 5.121
    },
    {
      "lang": "en",
      "text": "um",
      "offset": 4511.12,
      "duration": 2.24
    },
    {
      "lang": "en",
      "text": "a dope",
      "offset": 4515.679,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "so i thought about redoing it but i",
      "offset": 4517.04,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "figured i should just leave the error in",
      "offset": 4519.12,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "here because it's pretty funny why is",
      "offset": 4520.56,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "anti-object not callable",
      "offset": 4522.48,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "uh it's because",
      "offset": 4524.4,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "i screwed up we're trying to save these",
      "offset": 4527.04,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "functions so this is correct",
      "offset": 4529.28,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "this here",
      "offset": 4531.92,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "we don't want to call the function",
      "offset": 4533.28,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "because that returns none these",
      "offset": 4534.64,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "functions return none we just want to",
      "offset": 4536.08,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "store the function",
      "offset": 4538.08,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "so let me redefine the value object",
      "offset": 4539.6,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "and then we're going to come back in",
      "offset": 4542.159,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "redefine the expression draw a dot",
      "offset": 4543.76,
      "duration": 6.32
    },
    {
      "lang": "en",
      "text": "everything is great o dot grad is one",
      "offset": 4546.64,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "o dot grad is one and now",
      "offset": 4550.08,
      "duration": 5.599
    },
    {
      "lang": "en",
      "text": "now this should work of course",
      "offset": 4553.12,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "okay so all that backward should",
      "offset": 4555.679,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "this grant should now be 0.5 if we",
      "offset": 4558.56,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "redraw and if everything went correctly",
      "offset": 4560.56,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "0.5 yay",
      "offset": 4563.199,
      "duration": 6.561
    },
    {
      "lang": "en",
      "text": "okay so now we need to call ns.grad",
      "offset": 4565.28,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "and it's not awkward sorry",
      "offset": 4570.159,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "ends backward",
      "offset": 4573.04,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "so that seems to have worked",
      "offset": 4574.56,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "so instead backward routed the gradient",
      "offset": 4577.84,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "to both of these so this is looking",
      "offset": 4581.12,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "great",
      "offset": 4582.8,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "now we could of course called uh called",
      "offset": 4584.56,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "b grad",
      "offset": 4586.239,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "beat up backwards sorry",
      "offset": 4587.52,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "what's gonna happen",
      "offset": 4590.159,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "well b doesn't have it backward b is",
      "offset": 4592,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "backward",
      "offset": 4594.48,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "because b is a leaf node",
      "offset": 4595.6,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "b's backward is by initialization the",
      "offset": 4597.6,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "empty function",
      "offset": 4600.159,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "so nothing would happen but we can call",
      "offset": 4601.6,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "call it on it",
      "offset": 4604.239,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "but when we call",
      "offset": 4605.84,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "this one",
      "offset": 4608.4,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "it's backward",
      "offset": 4610.08,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "then we expect this 0.5 to get further",
      "offset": 4613.44,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "routed",
      "offset": 4616.239,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "right so there we go 0.5.5",
      "offset": 4617.52,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "and then finally",
      "offset": 4620.96,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "we want to call",
      "offset": 4622.719,
      "duration": 6.321
    },
    {
      "lang": "en",
      "text": "it here on x2 w2",
      "offset": 4625.199,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "and on x1 w1",
      "offset": 4630.32,
      "duration": 3.96
    },
    {
      "lang": "en",
      "text": "do both of those",
      "offset": 4636,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "and there we go",
      "offset": 4637.76,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "so we get 0 0.5 negative 1.5 and 1",
      "offset": 4639.679,
      "duration": 6.801
    },
    {
      "lang": "en",
      "text": "exactly as we did before but now",
      "offset": 4643.76,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "we've done it through",
      "offset": 4646.48,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "calling that backward um",
      "offset": 4648.4,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "sort of manually",
      "offset": 4650.96,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "so we have the lamp one last piece to",
      "offset": 4652.4,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "get rid of which is us calling",
      "offset": 4654.32,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "underscore backward manually so let's",
      "offset": 4656.4,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "think through what we are actually doing",
      "offset": 4658.4,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "um",
      "offset": 4660.4,
      "duration": 2.799
    },
    {
      "lang": "en",
      "text": "we've laid out a mathematical expression",
      "offset": 4661.44,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "and now we're trying to go backwards",
      "offset": 4663.199,
      "duration": 3.281
    },
    {
      "lang": "en",
      "text": "through that expression",
      "offset": 4664.88,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "um so going backwards through the",
      "offset": 4666.48,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "expression just means that we never want",
      "offset": 4668.8,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "to call a dot backward for any node",
      "offset": 4670.48,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "before",
      "offset": 4674.08,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "we've done a sort of um everything after",
      "offset": 4675.28,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "it",
      "offset": 4678.719,
      "duration": 2.561
    },
    {
      "lang": "en",
      "text": "so we have to do everything after it",
      "offset": 4679.679,
      "duration": 3.121
    },
    {
      "lang": "en",
      "text": "before we're ever going to call that",
      "offset": 4681.28,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "backward on any one node we have to get",
      "offset": 4682.8,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "all of its full dependencies everything",
      "offset": 4684.32,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "that it depends on has to",
      "offset": 4686,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "propagate to it before we can continue",
      "offset": 4688.56,
      "duration": 5.599
    },
    {
      "lang": "en",
      "text": "back propagation so this ordering of",
      "offset": 4690.48,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "graphs can be achieved using something",
      "offset": 4694.159,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "called topological sort",
      "offset": 4696,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "so topological sort",
      "offset": 4697.76,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "is basically a laying out of a graph",
      "offset": 4700.08,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "such that all the edges go only from",
      "offset": 4703.12,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "left to right basically",
      "offset": 4704.88,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "so here we have a graph it's a directory",
      "offset": 4706.64,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "a cyclic graph a dag",
      "offset": 4709.36,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "and this is two different topological",
      "offset": 4711.52,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "orders of it i believe where basically",
      "offset": 4714,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "you'll see that it's laying out of the",
      "offset": 4716.239,
      "duration": 3.121
    },
    {
      "lang": "en",
      "text": "notes such that all the edges go only",
      "offset": 4717.52,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "one way from left to right",
      "offset": 4719.36,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "and implementing topological sort you",
      "offset": 4721.92,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "can look in wikipedia and so on i'm not",
      "offset": 4724.239,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "going to go through it in detail",
      "offset": 4726.239,
      "duration": 5.281
    },
    {
      "lang": "en",
      "text": "but basically this is what builds a",
      "offset": 4728.8,
      "duration": 5.359
    },
    {
      "lang": "en",
      "text": "topological graph",
      "offset": 4731.52,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "we maintain a set of visited nodes and",
      "offset": 4734.159,
      "duration": 5.361
    },
    {
      "lang": "en",
      "text": "then we are",
      "offset": 4736.96,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "going through starting at some root node",
      "offset": 4739.52,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "which for us is o that's where we want",
      "offset": 4742,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "to start the topological sort",
      "offset": 4743.679,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "and starting at o we go through all of",
      "offset": 4745.679,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "its children and we need to lay them out",
      "offset": 4748,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "from left to right",
      "offset": 4750.48,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "and basically this starts at o",
      "offset": 4752.64,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "if it's not visited then it marks it as",
      "offset": 4754.96,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "visited and then it iterates through all",
      "offset": 4757.199,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "of its children",
      "offset": 4759.12,
      "duration": 5.119
    },
    {
      "lang": "en",
      "text": "and calls build topological on them",
      "offset": 4760.719,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "and then uh after it's gone through all",
      "offset": 4764.239,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "the children it adds itself",
      "offset": 4766.159,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "so basically",
      "offset": 4768.4,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "this node that we're going to call it on",
      "offset": 4769.92,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "like say o is only going to add itself",
      "offset": 4771.92,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "to the topo list after all of the",
      "offset": 4774.56,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "children have been processed and that's",
      "offset": 4777.6,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "how this function is guaranteeing",
      "offset": 4779.36,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "that you're only going to be in the list",
      "offset": 4781.76,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "once all your children are in the list",
      "offset": 4783.52,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "and that's the invariant that is being",
      "offset": 4785.679,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "maintained so if we built upon o and",
      "offset": 4786.96,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "then inspect this list",
      "offset": 4789.44,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "we're going to see that it ordered our",
      "offset": 4792.159,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "value objects",
      "offset": 4794.88,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "and the last one",
      "offset": 4796.48,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "is the value of 0.707 which is the",
      "offset": 4798.4,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "output",
      "offset": 4800.48,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "so this is o and then this is n",
      "offset": 4801.76,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "and then all the other nodes get laid",
      "offset": 4804.88,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "out before it",
      "offset": 4807.12,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "so that builds the topological graph and",
      "offset": 4809.76,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "really what we're doing now is we're",
      "offset": 4812,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "just calling dot underscore backward on",
      "offset": 4813.92,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "all of the nodes in a topological order",
      "offset": 4816.639,
      "duration": 5.361
    },
    {
      "lang": "en",
      "text": "so if we just reset the gradients",
      "offset": 4819.84,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "they're all zero",
      "offset": 4822,
      "duration": 2.719
    },
    {
      "lang": "en",
      "text": "what did we do",
      "offset": 4823.44,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "we started by",
      "offset": 4824.719,
      "duration": 5.121
    },
    {
      "lang": "en",
      "text": "setting o dot grad",
      "offset": 4827.679,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "to b1",
      "offset": 4829.84,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "that's the base case",
      "offset": 4831.44,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "then we built the topological order",
      "offset": 4833.6,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "and then we went for node",
      "offset": 4838.48,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "in",
      "offset": 4841.84,
      "duration": 2.16
    },
    {
      "lang": "en",
      "text": "reversed",
      "offset": 4842.639,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "of topo",
      "offset": 4844,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "now",
      "offset": 4846.4,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "in in the reverse order because this",
      "offset": 4847.6,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "list goes from",
      "offset": 4849.36,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "you know we need to go through it in",
      "offset": 4850.8,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "reversed order",
      "offset": 4852.4,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "so starting at o",
      "offset": 4853.92,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "note that backward",
      "offset": 4856.08,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "and this should be",
      "offset": 4858.639,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "it",
      "offset": 4861.36,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "there we go",
      "offset": 4863.36,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "those are the correct derivatives",
      "offset": 4865.52,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "finally we are going to hide this",
      "offset": 4867.12,
      "duration": 3.039
    },
    {
      "lang": "en",
      "text": "functionality",
      "offset": 4868.88,
      "duration": 2.799
    },
    {
      "lang": "en",
      "text": "so i'm going to",
      "offset": 4870.159,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "copy this and we're going to hide it",
      "offset": 4871.679,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "inside the valley class because we don't",
      "offset": 4873.76,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "want to have all that code lying around",
      "offset": 4875.44,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "so instead of an underscore backward",
      "offset": 4878.239,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "we're now going to define an actual",
      "offset": 4879.84,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "backward so that's backward without the",
      "offset": 4881.44,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "underscore",
      "offset": 4883.84,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "and that's going to do all the stuff",
      "offset": 4886.239,
      "duration": 2.801
    },
    {
      "lang": "en",
      "text": "that we just arrived",
      "offset": 4887.52,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "so let me just clean this up a little",
      "offset": 4889.04,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "bit so",
      "offset": 4890.4,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "we're first going to",
      "offset": 4892.48,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "build a topological graph",
      "offset": 4897.04,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "starting at self",
      "offset": 4898.96,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "so build topo of self",
      "offset": 4901.28,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "will populate the topological order into",
      "offset": 4904.159,
      "duration": 5.121
    },
    {
      "lang": "en",
      "text": "the topo list which is a local variable",
      "offset": 4906.56,
      "duration": 6.4
    },
    {
      "lang": "en",
      "text": "then we set self.grad to be one",
      "offset": 4909.28,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "and then for each node in the reversed",
      "offset": 4912.96,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "list so starting at us and going to all",
      "offset": 4915.36,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "the children",
      "offset": 4917.679,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "underscore backward",
      "offset": 4920,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "and",
      "offset": 4922.4,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "that should be it so",
      "offset": 4923.28,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "save",
      "offset": 4926.08,
      "duration": 3.119
    },
    {
      "lang": "en",
      "text": "come down here",
      "offset": 4928,
      "duration": 1.91
    },
    {
      "lang": "en",
      "text": "redefine",
      "offset": 4929.199,
      "duration": 1.921
    },
    {
      "lang": "en",
      "text": "[Music]",
      "offset": 4929.91,
      "duration": 3.69
    },
    {
      "lang": "en",
      "text": "okay all the grands are zero",
      "offset": 4931.12,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "and now what we can do is oh that",
      "offset": 4933.6,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "backward without the underscore",
      "offset": 4935.199,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "and",
      "offset": 4937.12,
      "duration": 2.24
    },
    {
      "lang": "en",
      "text": "there we go",
      "offset": 4941.44,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "and that's uh that's back propagation",
      "offset": 4942.88,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "place for one neuron",
      "offset": 4946.639,
      "duration": 3.281
    },
    {
      "lang": "en",
      "text": "now we shouldn't be too happy with",
      "offset": 4948.4,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "ourselves actually because we have a bad",
      "offset": 4949.92,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "bug um and we have not surfaced the bug",
      "offset": 4952.32,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "because of some specific conditions that",
      "offset": 4955.04,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "we are we have to think about right now",
      "offset": 4956.8,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "so here's the simplest case that shows",
      "offset": 4959.92,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "the bug",
      "offset": 4962,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "say i create a single node a",
      "offset": 4963.92,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "and then i create a b that is a plus a",
      "offset": 4968,
      "duration": 6.56
    },
    {
      "lang": "en",
      "text": "and then i called backward",
      "offset": 4971.6,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "so what's going to happen is a is 3",
      "offset": 4974.8,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "and then a b is a plus a so there's two",
      "offset": 4977.28,
      "duration": 6.32
    },
    {
      "lang": "en",
      "text": "arrows on top of each other here",
      "offset": 4980,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "then we can see that b is of course the",
      "offset": 4983.76,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "forward pass works",
      "offset": 4985.44,
      "duration": 2.64
    },
    {
      "lang": "en",
      "text": "b is just",
      "offset": 4986.88,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "a plus a which is six",
      "offset": 4988.08,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "but the gradient here is not actually",
      "offset": 4990,
      "duration": 2.639
    },
    {
      "lang": "en",
      "text": "correct",
      "offset": 4991.679,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "that we calculate it automatically",
      "offset": 4992.639,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "and that's because",
      "offset": 4995.92,
      "duration": 3.279
    },
    {
      "lang": "en",
      "text": "um",
      "offset": 4997.44,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "of course uh",
      "offset": 4999.199,
      "duration": 3.281
    },
    {
      "lang": "en",
      "text": "just doing calculus in your head the",
      "offset": 5000.48,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "derivative of b with respect to a",
      "offset": 5002.48,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "should be uh two",
      "offset": 5004.56,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "one plus one",
      "offset": 5007.6,
      "duration": 3.119
    },
    {
      "lang": "en",
      "text": "it's not one",
      "offset": 5008.8,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "intuitively what's happening here right",
      "offset": 5010.719,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "so b is the result of a plus a and then",
      "offset": 5012.239,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "we call backward on it",
      "offset": 5014.639,
      "duration": 6.401
    },
    {
      "lang": "en",
      "text": "so let's go up and see what that does",
      "offset": 5016.639,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "um",
      "offset": 5022.08,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "b is a result of addition",
      "offset": 5023.6,
      "duration": 3.24
    },
    {
      "lang": "en",
      "text": "so out as",
      "offset": 5025.52,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "b and then when we called backward what",
      "offset": 5026.84,
      "duration": 3.96
    },
    {
      "lang": "en",
      "text": "happened is",
      "offset": 5029.52,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "self.grad was set",
      "offset": 5030.8,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "to one",
      "offset": 5033.199,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "and then other that grad was set to one",
      "offset": 5034.4,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "but because we're doing a plus a",
      "offset": 5037.199,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "self and other are actually the exact",
      "offset": 5039.84,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "same object",
      "offset": 5042,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "so we are overriding the gradient we are",
      "offset": 5043.36,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "setting it to one and then we are",
      "offset": 5046,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "setting it again to one and that's why",
      "offset": 5047.6,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "it stays",
      "offset": 5050,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "at one",
      "offset": 5051.199,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "so that's a problem",
      "offset": 5053.04,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "there's another way to see this in a",
      "offset": 5054.639,
      "duration": 5.361
    },
    {
      "lang": "en",
      "text": "little bit more complicated expression",
      "offset": 5056.4,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "so here we have",
      "offset": 5061.52,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "a and b",
      "offset": 5063.52,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "and then uh d will be the multiplication",
      "offset": 5065.92,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "of the two and e will be the addition of",
      "offset": 5068.88,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "the two",
      "offset": 5070.8,
      "duration": 2.24
    },
    {
      "lang": "en",
      "text": "and",
      "offset": 5072,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "then we multiply e times d to get f and",
      "offset": 5073.04,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "then we called fda backward",
      "offset": 5075.28,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "and these gradients if you check will be",
      "offset": 5077.679,
      "duration": 3.121
    },
    {
      "lang": "en",
      "text": "incorrect",
      "offset": 5079.6,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "so fundamentally what's happening here",
      "offset": 5080.8,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "again is",
      "offset": 5082.48,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "basically we're going to see an issue",
      "offset": 5085.04,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "anytime we use a variable more than once",
      "offset": 5086.4,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "until now in these expressions above",
      "offset": 5089.199,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "every variable is used exactly once so",
      "offset": 5091.04,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "we didn't see the issue",
      "offset": 5093.199,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "but here if a variable is used more than",
      "offset": 5094.96,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "once what's going to happen during",
      "offset": 5096.639,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "backward pass we're backpropagating from",
      "offset": 5097.92,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "f to e to d so far so good but now",
      "offset": 5100.239,
      "duration": 5.601
    },
    {
      "lang": "en",
      "text": "equals it backward and it deposits its",
      "offset": 5103.28,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "gradients to a and b but then we come",
      "offset": 5105.84,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "back to d",
      "offset": 5108,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "and call backward and it overwrites",
      "offset": 5109.12,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "those gradients at a and b",
      "offset": 5111.44,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "so that's obviously a problem",
      "offset": 5114.48,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "and the solution here if you look at",
      "offset": 5117.12,
      "duration": 5.119
    },
    {
      "lang": "en",
      "text": "the multivariate case of the chain rule",
      "offset": 5119.92,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "and its generalization there",
      "offset": 5122.239,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "the solution there is basically that we",
      "offset": 5123.92,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "have to accumulate these gradients these",
      "offset": 5126.32,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "gradients add",
      "offset": 5128.08,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "and so instead of setting those",
      "offset": 5130.08,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "gradients",
      "offset": 5132.239,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "we can simply do plus equals we need to",
      "offset": 5134.639,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "accumulate those gradients",
      "offset": 5137.12,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "plus equals plus equals",
      "offset": 5139.12,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "plus equals",
      "offset": 5141.92,
      "duration": 2.64
    },
    {
      "lang": "en",
      "text": "plus equals",
      "offset": 5144.639,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "and this will be okay remember because",
      "offset": 5146.48,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "we are initializing them at zero so they",
      "offset": 5148.159,
      "duration": 3.281
    },
    {
      "lang": "en",
      "text": "start at zero",
      "offset": 5150.239,
      "duration": 3.121
    },
    {
      "lang": "en",
      "text": "and then any",
      "offset": 5151.44,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "contribution",
      "offset": 5153.36,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "that flows backwards",
      "offset": 5154.88,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "will simply add",
      "offset": 5157.199,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "so now if we redefine",
      "offset": 5158.8,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "this one",
      "offset": 5161.28,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "because the plus equals this now works",
      "offset": 5163.76,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "because a.grad started at zero and we",
      "offset": 5166,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "called beta backward we deposit one and",
      "offset": 5168.239,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "then we deposit one again and now this",
      "offset": 5171.12,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "is two which is correct",
      "offset": 5173.12,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "and here this will also work and we'll",
      "offset": 5174.96,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "get correct gradients",
      "offset": 5176.96,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "because when we call eta backward we",
      "offset": 5178.4,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "will deposit the gradients from this",
      "offset": 5180.159,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "branch and then we get to back into",
      "offset": 5181.6,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "detail backward it will deposit its own",
      "offset": 5183.76,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "gradients and then those gradients",
      "offset": 5186,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "simply add on top of each other and so",
      "offset": 5188.159,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "we just accumulate those gradients and",
      "offset": 5190.56,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "that fixes the issue okay now before we",
      "offset": 5191.92,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "move on let me actually do a bit of",
      "offset": 5194.08,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "cleanup here and delete some of these",
      "offset": 5195.679,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "some of this intermediate work so",
      "offset": 5198.8,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "we're not gonna need any of this now",
      "offset": 5201.199,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "that we've derived all of it",
      "offset": 5202.719,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "um",
      "offset": 5204.8,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "we are going to keep this because i want",
      "offset": 5205.679,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "to come back to it",
      "offset": 5208,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "delete the 10h",
      "offset": 5209.679,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "delete our morning example",
      "offset": 5211.28,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "delete the step",
      "offset": 5213.84,
      "duration": 5.839
    },
    {
      "lang": "en",
      "text": "delete this keep the code that draws",
      "offset": 5215.84,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "and then delete this example",
      "offset": 5219.679,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "and leave behind only the definition of",
      "offset": 5222.08,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "value",
      "offset": 5223.84,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "and now let's come back to this",
      "offset": 5225.28,
      "duration": 2.879
    },
    {
      "lang": "en",
      "text": "non-linearity here that we implemented",
      "offset": 5226.56,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "the tanh now i told you that we could",
      "offset": 5228.159,
      "duration": 5.601
    },
    {
      "lang": "en",
      "text": "have broken down 10h into its explicit",
      "offset": 5230.719,
      "duration": 5.281
    },
    {
      "lang": "en",
      "text": "atoms in terms of other expressions if",
      "offset": 5233.76,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "we had the x function so if you remember",
      "offset": 5236,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "tan h is defined like this and we chose",
      "offset": 5238.32,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "to develop tan h as a single function",
      "offset": 5240.56,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "and we can do that because we know its",
      "offset": 5242.88,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "derivative and we can back propagate",
      "offset": 5244.639,
      "duration": 2.321
    },
    {
      "lang": "en",
      "text": "through it",
      "offset": 5246,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "but we can also break down tan h into",
      "offset": 5246.96,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "and express it as a function of x and i",
      "offset": 5249.199,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "would like to do that now because i want",
      "offset": 5251.44,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "to prove to you that you get all the",
      "offset": 5253.12,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "same results and all those ingredients",
      "offset": 5254.32,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "but also because it forces us to",
      "offset": 5256.8,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "implement a few more expressions it",
      "offset": 5258.239,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "forces us to do exponentiation addition",
      "offset": 5260,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "subtraction division and things like",
      "offset": 5262.96,
      "duration": 3.279
    },
    {
      "lang": "en",
      "text": "that and i think it's a good exercise to",
      "offset": 5264.8,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "go through a few more of these",
      "offset": 5266.239,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "okay so let's scroll up",
      "offset": 5268.08,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "to the definition of value",
      "offset": 5270.08,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "and here one thing that we currently",
      "offset": 5272.159,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "can't do is we can do like a value of",
      "offset": 5273.84,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "say 2.0",
      "offset": 5276.639,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "but we can't do you know here for",
      "offset": 5278.4,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "example we want to add constant one and",
      "offset": 5280.4,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "we can't do something like this",
      "offset": 5282.56,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "and we can't do it because it says",
      "offset": 5285.12,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "object has no attribute data that's",
      "offset": 5286.96,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "because a plus one comes right here to",
      "offset": 5288.719,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "add",
      "offset": 5291.12,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "and then other is the integer one and",
      "offset": 5292.08,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "then here python is trying to access",
      "offset": 5294.88,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "one.data and that's not a thing and",
      "offset": 5296.56,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "that's because basically one is not a",
      "offset": 5298.719,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "value object and we only have addition",
      "offset": 5300.48,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "for value objects so as a matter of",
      "offset": 5302.239,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "convenience so that we can create",
      "offset": 5304.8,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "expressions like this and make them make",
      "offset": 5306.8,
      "duration": 2.32
    },
    {
      "lang": "en",
      "text": "sense",
      "offset": 5308.239,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "we can simply do something like this",
      "offset": 5309.12,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "basically",
      "offset": 5312.32,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "we let other alone if other is an",
      "offset": 5313.679,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "instance of value but if it's not an",
      "offset": 5315.92,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "instance of value we're going to assume",
      "offset": 5317.76,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "that it's a number like an integer float",
      "offset": 5319.04,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "and we're going to simply wrap it in in",
      "offset": 5320.88,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "value and then other will just become",
      "offset": 5323.36,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "value of other and then other will have",
      "offset": 5325.199,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "a data attribute and this should work so",
      "offset": 5326.88,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "if i just say this predefined value then",
      "offset": 5329.28,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "this should work",
      "offset": 5331.84,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "there we go okay now let's do the exact",
      "offset": 5333.199,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "same thing for multiply because we can't",
      "offset": 5335.28,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "do something like this",
      "offset": 5337.199,
      "duration": 2.561
    },
    {
      "lang": "en",
      "text": "again",
      "offset": 5338.88,
      "duration": 2.799
    },
    {
      "lang": "en",
      "text": "for the exact same reason so we just",
      "offset": 5339.76,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "have to go to mole and if other is",
      "offset": 5341.679,
      "duration": 5.841
    },
    {
      "lang": "en",
      "text": "not a value then let's wrap it in value",
      "offset": 5344.96,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "let's redefine value and now this works",
      "offset": 5347.52,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "now here's a kind of unfortunate and not",
      "offset": 5350.48,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "obvious part a times two works we saw",
      "offset": 5352.8,
      "duration": 6.8
    },
    {
      "lang": "en",
      "text": "that but two times a is that gonna work",
      "offset": 5355.28,
      "duration": 6.64
    },
    {
      "lang": "en",
      "text": "you'd expect it to right but actually it",
      "offset": 5359.6,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "will not",
      "offset": 5361.92,
      "duration": 2.319
    },
    {
      "lang": "en",
      "text": "and the reason it won't is because",
      "offset": 5362.96,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "python doesn't know",
      "offset": 5364.239,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "like when when you do a times two",
      "offset": 5366.08,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "basically um so a times two python will",
      "offset": 5368.48,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "go and it will basically do something",
      "offset": 5371.44,
      "duration": 3.279
    },
    {
      "lang": "en",
      "text": "like a dot mul",
      "offset": 5372.88,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "of two that's basically what it will",
      "offset": 5374.719,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "call but to it 2 times a is the same as",
      "offset": 5376.56,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "2 dot mol of a",
      "offset": 5379.679,
      "duration": 5.281
    },
    {
      "lang": "en",
      "text": "and it doesn't 2 can't multiply",
      "offset": 5381.84,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "value and so it's really confused about",
      "offset": 5384.96,
      "duration": 2.4
    },
    {
      "lang": "en",
      "text": "that",
      "offset": 5386.639,
      "duration": 2.56
    },
    {
      "lang": "en",
      "text": "so instead what happens is in python the",
      "offset": 5387.36,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "way this works is you are free to define",
      "offset": 5389.199,
      "duration": 5.121
    },
    {
      "lang": "en",
      "text": "something called the r mold",
      "offset": 5391.76,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "and our mole",
      "offset": 5394.32,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "is kind of like a fallback so if python",
      "offset": 5395.84,
      "duration": 6.56
    },
    {
      "lang": "en",
      "text": "can't do 2 times a it will check if um",
      "offset": 5398.32,
      "duration": 6.72
    },
    {
      "lang": "en",
      "text": "if by any chance a knows how to multiply",
      "offset": 5402.4,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "two and that will be called into our",
      "offset": 5405.04,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "mole",
      "offset": 5407.199,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "so because python can't do two times a",
      "offset": 5408.8,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "it will check is there an our mole in",
      "offset": 5411.199,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "value and because there is it will now",
      "offset": 5412.8,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "call that",
      "offset": 5415.36,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "and what we'll do here is we will swap",
      "offset": 5416.8,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "the order of the operands so basically",
      "offset": 5418.96,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "two times a will redirect to armel and",
      "offset": 5421.36,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "our mole will basically call a times two",
      "offset": 5423.52,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "and that's how that will work",
      "offset": 5426.08,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "so",
      "offset": 5428.32,
      "duration": 2.879
    },
    {
      "lang": "en",
      "text": "redefining now with armor two times a",
      "offset": 5429.04,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "becomes four okay now looking at the",
      "offset": 5431.199,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "other elements that we still need we",
      "offset": 5433.52,
      "duration": 2.719
    },
    {
      "lang": "en",
      "text": "need to know how to exponentiate and how",
      "offset": 5434.88,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "to divide so let's first the explanation",
      "offset": 5436.239,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "to the exponentiation part we're going",
      "offset": 5438.4,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "to introduce",
      "offset": 5440.48,
      "duration": 2.48
    },
    {
      "lang": "en",
      "text": "a single",
      "offset": 5441.76,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "function x here",
      "offset": 5442.96,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "and x is going to mirror 10h in the",
      "offset": 5445.04,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "sense that it's a simple single function",
      "offset": 5447.92,
      "duration": 3.279
    },
    {
      "lang": "en",
      "text": "that transforms a single scalar value",
      "offset": 5449.679,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "and outputs a single scalar value",
      "offset": 5451.199,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "so we pop out the python number we use",
      "offset": 5453.12,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "math.x to exponentiate it create a new",
      "offset": 5456,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "value object",
      "offset": 5458,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "everything that we've seen before the",
      "offset": 5459.199,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "tricky part of course is how do you",
      "offset": 5460.88,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "propagate through e to the x",
      "offset": 5462.239,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "and",
      "offset": 5464.8,
      "duration": 2.879
    },
    {
      "lang": "en",
      "text": "so here you can potentially pause the",
      "offset": 5465.92,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "video and think about what should go",
      "offset": 5467.679,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "here",
      "offset": 5469.679,
      "duration": 2.241
    },
    {
      "lang": "en",
      "text": "okay so basically we need to know what",
      "offset": 5473.28,
      "duration": 5.359
    },
    {
      "lang": "en",
      "text": "is the local derivative of e to the x so",
      "offset": 5475.84,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "d by d x of e to the x is famously just",
      "offset": 5478.639,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "e to the x and we've already just",
      "offset": 5481.28,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "calculated e to the x and it's inside",
      "offset": 5483.12,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "out that data so we can do up that data",
      "offset": 5485.52,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "times",
      "offset": 5487.76,
      "duration": 2
    },
    {
      "lang": "en",
      "text": "and",
      "offset": 5488.96,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "out that grad that's the chain rule",
      "offset": 5489.76,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "so we're just chaining on to the current",
      "offset": 5492,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "running grad",
      "offset": 5493.84,
      "duration": 2.879
    },
    {
      "lang": "en",
      "text": "and this is what the expression looks",
      "offset": 5495.28,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "like it looks a little confusing but",
      "offset": 5496.719,
      "duration": 3.281
    },
    {
      "lang": "en",
      "text": "this is what it is and that's the",
      "offset": 5498.639,
      "duration": 3.121
    },
    {
      "lang": "en",
      "text": "exponentiation",
      "offset": 5500,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "so redefining we should now be able to",
      "offset": 5501.76,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "call a.x",
      "offset": 5503.679,
      "duration": 2.48
    },
    {
      "lang": "en",
      "text": "and",
      "offset": 5505.36,
      "duration": 2.48
    },
    {
      "lang": "en",
      "text": "hopefully the backward pass works as",
      "offset": 5506.159,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "well okay and the last thing we'd like",
      "offset": 5507.84,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "to do of course is we'd like to be able",
      "offset": 5509.6,
      "duration": 2.639
    },
    {
      "lang": "en",
      "text": "to divide",
      "offset": 5510.96,
      "duration": 2.16
    },
    {
      "lang": "en",
      "text": "now",
      "offset": 5512.239,
      "duration": 2.241
    },
    {
      "lang": "en",
      "text": "i actually will implement something",
      "offset": 5513.12,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "slightly more powerful than division",
      "offset": 5514.48,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "because division is just a special case",
      "offset": 5516,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "of something a bit more powerful",
      "offset": 5517.84,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "so in particular just by rearranging",
      "offset": 5519.92,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "if we have some kind of a b equals",
      "offset": 5522.639,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "value of 4.0 here we'd like to basically",
      "offset": 5524.88,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "be able to do a divide b and we'd like",
      "offset": 5527.6,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "this to be able to give us 0.5",
      "offset": 5529.199,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "now division actually can be reshuffled",
      "offset": 5531.6,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "as follows if we have a divide b that's",
      "offset": 5534.239,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "actually the same as a multiplying one",
      "offset": 5537.04,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "over b",
      "offset": 5538.719,
      "duration": 2.881
    },
    {
      "lang": "en",
      "text": "and that's the same as a multiplying b",
      "offset": 5539.84,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "to the power of negative one",
      "offset": 5541.6,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "and so what i'd like to do instead is i",
      "offset": 5544.239,
      "duration": 3.281
    },
    {
      "lang": "en",
      "text": "basically like to implement the",
      "offset": 5545.76,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "operation of x to the k for some",
      "offset": 5547.52,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "constant uh k so it's an integer or a",
      "offset": 5549.76,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "float um and we would like to be able to",
      "offset": 5552.56,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "differentiate this and then as a special",
      "offset": 5555.28,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "case uh negative one will be division",
      "offset": 5556.96,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "and so i'm doing that just because uh",
      "offset": 5560.8,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "it's more general and um yeah you might",
      "offset": 5562.48,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "as well do it that way so basically what",
      "offset": 5565.199,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "i'm saying is we can redefine",
      "offset": 5566.96,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "uh division",
      "offset": 5569.44,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "which we will put here somewhere",
      "offset": 5571.28,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "yeah we can put it here somewhere what",
      "offset": 5574.48,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "i'm saying is that we can redefine",
      "offset": 5576.32,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "division so self-divide other",
      "offset": 5578.08,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "can actually be rewritten as self times",
      "offset": 5580.8,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "other to the power of negative one",
      "offset": 5583.12,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "and now",
      "offset": 5585.679,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "a value raised to the power of negative",
      "offset": 5587.36,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "one we have now defined that",
      "offset": 5589.199,
      "duration": 3.121
    },
    {
      "lang": "en",
      "text": "so",
      "offset": 5591.6,
      "duration": 2
    },
    {
      "lang": "en",
      "text": "here's",
      "offset": 5592.32,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "so we need to implement the pow function",
      "offset": 5593.6,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "where am i going to put the power",
      "offset": 5595.92,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "function maybe here somewhere",
      "offset": 5597.28,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "this is the skeleton for it",
      "offset": 5600,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "so this function will be called when we",
      "offset": 5602.48,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "try to raise a value to some power and",
      "offset": 5604.32,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "other will be that power",
      "offset": 5606.719,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "now i'd like to make sure that other is",
      "offset": 5608.639,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "only an int or a float usually other is",
      "offset": 5610.56,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "some kind of a different value object",
      "offset": 5613.04,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "but here other will be forced to be an",
      "offset": 5615.36,
      "duration": 5.279
    },
    {
      "lang": "en",
      "text": "end or a float otherwise the math",
      "offset": 5617.36,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "won't work for",
      "offset": 5620.639,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "for or try to achieve in the specific",
      "offset": 5622.159,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "case that would be a different",
      "offset": 5623.84,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "derivative expression if we wanted other",
      "offset": 5625.36,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "to be a value",
      "offset": 5627.84,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "so here we create the output value which",
      "offset": 5629.6,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "is just uh you know this data raised to",
      "offset": 5631.52,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "the power of other and other here could",
      "offset": 5633.679,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "be for example negative one that's what",
      "offset": 5635.44,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "we are hoping to achieve",
      "offset": 5636.88,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "and then uh this is the backwards stub",
      "offset": 5639.36,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "and this is the fun part which is what",
      "offset": 5641.84,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "is the uh chain rule expression here for",
      "offset": 5643.84,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "back for um",
      "offset": 5647.28,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "back propagating through the power",
      "offset": 5649.6,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "function where the power is to the power",
      "offset": 5651.6,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "of some kind of a constant",
      "offset": 5653.92,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "so this is the exercise and maybe pause",
      "offset": 5655.679,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "the video here and see if you can figure",
      "offset": 5657.28,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "it out yourself as to what we should put",
      "offset": 5658.639,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "here",
      "offset": 5660.48,
      "duration": 2.239
    },
    {
      "lang": "en",
      "text": "okay so",
      "offset": 5666.96,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "you can actually go here and look at",
      "offset": 5669.04,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "derivative rules as an example and we",
      "offset": 5670.8,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "see lots of derivatives that you can",
      "offset": 5672.8,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "hopefully know from calculus in",
      "offset": 5674.32,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "particular what we're looking for is the",
      "offset": 5676,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "power rule",
      "offset": 5677.76,
      "duration": 2.56
    },
    {
      "lang": "en",
      "text": "because that's telling us that if we're",
      "offset": 5679.12,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "trying to take d by dx of x to the n",
      "offset": 5680.32,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "which is what we're doing here",
      "offset": 5682.719,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "then that is just n times x to the n",
      "offset": 5684.56,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "minus 1",
      "offset": 5686.96,
      "duration": 2.56
    },
    {
      "lang": "en",
      "text": "right",
      "offset": 5688.4,
      "duration": 2.48
    },
    {
      "lang": "en",
      "text": "okay",
      "offset": 5689.52,
      "duration": 2.08
    },
    {
      "lang": "en",
      "text": "so",
      "offset": 5690.88,
      "duration": 2.4
    },
    {
      "lang": "en",
      "text": "that's telling us about the local",
      "offset": 5691.6,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "derivative of this power operation",
      "offset": 5693.28,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "so all we want here",
      "offset": 5695.92,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "basically n is now other",
      "offset": 5698.32,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "and self.data is x",
      "offset": 5700.719,
      "duration": 5.361
    },
    {
      "lang": "en",
      "text": "and so this now becomes",
      "offset": 5703.44,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "other which is n times",
      "offset": 5706.08,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "self.data",
      "offset": 5708.56,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "which is now a python in torah float",
      "offset": 5710.239,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "it's not a valley object we're accessing",
      "offset": 5713.04,
      "duration": 3.119
    },
    {
      "lang": "en",
      "text": "the data attribute",
      "offset": 5714.719,
      "duration": 2.561
    },
    {
      "lang": "en",
      "text": "raised",
      "offset": 5716.159,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "to the power of other minus one or n",
      "offset": 5717.28,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "minus one",
      "offset": 5719.76,
      "duration": 2.879
    },
    {
      "lang": "en",
      "text": "i can put brackets around this but this",
      "offset": 5721.119,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "doesn't matter because",
      "offset": 5722.639,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "power takes precedence over multiply and",
      "offset": 5725.28,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "python so that would have been okay",
      "offset": 5727.36,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "and that's the local derivative only but",
      "offset": 5729.44,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "now we have to chain it and we change",
      "offset": 5731.36,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "just simply by multiplying by output",
      "offset": 5733.36,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "grad that's chain rule",
      "offset": 5734.88,
      "duration": 5.839
    },
    {
      "lang": "en",
      "text": "and this should technically work",
      "offset": 5736.719,
      "duration": 6.241
    },
    {
      "lang": "en",
      "text": "and we're going to find out soon but now",
      "offset": 5740.719,
      "duration": 3.121
    },
    {
      "lang": "en",
      "text": "if we",
      "offset": 5742.96,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "do this this should now work",
      "offset": 5743.84,
      "duration": 5.359
    },
    {
      "lang": "en",
      "text": "and we get 0.5 so the forward pass works",
      "offset": 5746.719,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "but does the backward pass work and i",
      "offset": 5749.199,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "realize that we actually also have to",
      "offset": 5751.44,
      "duration": 3.279
    },
    {
      "lang": "en",
      "text": "know how to subtract so",
      "offset": 5752.719,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "right now a minus b will not work",
      "offset": 5754.719,
      "duration": 5.361
    },
    {
      "lang": "en",
      "text": "to make it work we need one more",
      "offset": 5757.36,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "piece of code here",
      "offset": 5760.08,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "and",
      "offset": 5761.76,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "basically this is the",
      "offset": 5762.96,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "subtraction and the way we're going to",
      "offset": 5765.36,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "implement subtraction is we're going to",
      "offset": 5766.8,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "implement it by addition of a negation",
      "offset": 5768.8,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "and then to implement negation we're",
      "offset": 5770.719,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "gonna multiply by negative one so just",
      "offset": 5772.159,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "again using the stuff we've already",
      "offset": 5774.159,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "built and just um expressing it in terms",
      "offset": 5775.52,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "of what we have and a minus b is now",
      "offset": 5777.84,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "working okay so now let's scroll again",
      "offset": 5780.159,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "to this expression here for this neuron",
      "offset": 5782.32,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "and let's just",
      "offset": 5785.199,
      "duration": 3.281
    },
    {
      "lang": "en",
      "text": "compute the backward pass here once",
      "offset": 5786.88,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "we've defined o",
      "offset": 5788.48,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "and let's draw it",
      "offset": 5790,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "so here's the gradients for all these",
      "offset": 5792,
      "duration": 3.119
    },
    {
      "lang": "en",
      "text": "leaf nodes for this two-dimensional",
      "offset": 5793.679,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "neuron that has a 10h that we've seen",
      "offset": 5795.119,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "before so now what i'd like to do is i'd",
      "offset": 5797.28,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "like to break up this 10h",
      "offset": 5799.44,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "into this expression here",
      "offset": 5801.92,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "so let me copy paste this",
      "offset": 5804.32,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "here",
      "offset": 5806.239,
      "duration": 3.041
    },
    {
      "lang": "en",
      "text": "and now instead of we'll preserve the",
      "offset": 5807.36,
      "duration": 2.879
    },
    {
      "lang": "en",
      "text": "label",
      "offset": 5809.28,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "and we will change how we define o",
      "offset": 5810.239,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "so in particular we're going to",
      "offset": 5813.679,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "implement this formula here",
      "offset": 5815.04,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "so we need e to the 2x",
      "offset": 5816.719,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "minus 1 over e to the x plus 1. so e to",
      "offset": 5818.56,
      "duration": 6.159
    },
    {
      "lang": "en",
      "text": "the 2x we need to take 2 times n and we",
      "offset": 5821.44,
      "duration": 5.679
    },
    {
      "lang": "en",
      "text": "need to exponentiate it that's e to the",
      "offset": 5824.719,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "two x and then because we're using it",
      "offset": 5827.119,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "twice let's create an intermediate",
      "offset": 5828.96,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "variable e",
      "offset": 5830.8,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "and then define o as",
      "offset": 5832.56,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "e plus one over",
      "offset": 5834.96,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "e minus one over e plus one",
      "offset": 5836.8,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "e minus one over e plus one",
      "offset": 5839.199,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "and that should be it and then we should",
      "offset": 5842.8,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "be able to draw that of o",
      "offset": 5844.719,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "so now before i run this what do we",
      "offset": 5846.96,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "expect to see",
      "offset": 5849.28,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "number one we're expecting to see a much",
      "offset": 5850.88,
      "duration": 2.799
    },
    {
      "lang": "en",
      "text": "longer",
      "offset": 5852.639,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "graph here because we've broken up 10h",
      "offset": 5853.679,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "into a bunch of other operations",
      "offset": 5855.679,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "but those operations are mathematically",
      "offset": 5857.6,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "equivalent and so what we're expecting",
      "offset": 5859.36,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "to see is number one the same",
      "offset": 5861.04,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "result here so the forward pass works",
      "offset": 5863.76,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "and number two because of that",
      "offset": 5865.92,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "mathematical equivalence we expect to",
      "offset": 5867.199,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "see the same backward pass and the same",
      "offset": 5869.119,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "gradients on these leaf nodes so these",
      "offset": 5871.04,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "gradients should be identical",
      "offset": 5873.199,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "so let's run this",
      "offset": 5875.199,
      "duration": 5.121
    },
    {
      "lang": "en",
      "text": "so number one let's verify that instead",
      "offset": 5878,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "of a single 10h node we have now x and",
      "offset": 5880.32,
      "duration": 6.56
    },
    {
      "lang": "en",
      "text": "we have plus we have times negative one",
      "offset": 5883.28,
      "duration": 5.439
    },
    {
      "lang": "en",
      "text": "uh this is the division",
      "offset": 5886.88,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "and we end up with the same forward pass",
      "offset": 5888.719,
      "duration": 3.041
    },
    {
      "lang": "en",
      "text": "here",
      "offset": 5890.88,
      "duration": 2.239
    },
    {
      "lang": "en",
      "text": "and then the gradients we have to be",
      "offset": 5891.76,
      "duration": 2.479
    },
    {
      "lang": "en",
      "text": "careful because they're in slightly",
      "offset": 5893.119,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "different order potentially the",
      "offset": 5894.239,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "gradients for w2x2 should be 0 and 0.5",
      "offset": 5896.32,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "w2 and x2 are 0 and 0.5",
      "offset": 5899.44,
      "duration": 5.759
    },
    {
      "lang": "en",
      "text": "and w1 x1 are 1 and negative 1.5",
      "offset": 5902,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "1 and negative 1.5",
      "offset": 5905.199,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "so that means that both our forward",
      "offset": 5907.199,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "passes and backward passes were correct",
      "offset": 5908.88,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "because this turned out to be equivalent",
      "offset": 5911.04,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "to",
      "offset": 5913.119,
      "duration": 2.641
    },
    {
      "lang": "en",
      "text": "10h before",
      "offset": 5914,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "and so the reason i wanted to go through",
      "offset": 5915.76,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "this exercise is number one we got to",
      "offset": 5917.28,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "practice a few more operations and uh",
      "offset": 5919.199,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "writing more backwards passes and number",
      "offset": 5921.6,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "two i wanted to illustrate the point",
      "offset": 5923.44,
      "duration": 2.719
    },
    {
      "lang": "en",
      "text": "that",
      "offset": 5925.28,
      "duration": 2.48
    },
    {
      "lang": "en",
      "text": "the um",
      "offset": 5926.159,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "the level at which you implement your",
      "offset": 5927.76,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "operations is totally up to you you can",
      "offset": 5929.52,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "implement backward passes for tiny",
      "offset": 5931.679,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "expressions like a single individual",
      "offset": 5933.44,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "plus or a single times",
      "offset": 5934.88,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "or you can implement them for say",
      "offset": 5936.639,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "10h",
      "offset": 5938.96,
      "duration": 2.719
    },
    {
      "lang": "en",
      "text": "which is a kind of a potentially you can",
      "offset": 5940,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "see it as a composite operation because",
      "offset": 5941.679,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "it's made up of all these more atomic",
      "offset": 5943.199,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "operations but really all of this is",
      "offset": 5945.28,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "kind of like a fake concept all that",
      "offset": 5947.119,
      "duration": 3.281
    },
    {
      "lang": "en",
      "text": "matters is we have some kind of inputs",
      "offset": 5948.8,
      "duration": 2.879
    },
    {
      "lang": "en",
      "text": "and some kind of an output and this",
      "offset": 5950.4,
      "duration": 2.64
    },
    {
      "lang": "en",
      "text": "output is a function of the inputs in",
      "offset": 5951.679,
      "duration": 3.121
    },
    {
      "lang": "en",
      "text": "some way and as long as you can do",
      "offset": 5953.04,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "forward pass and the backward pass of",
      "offset": 5954.8,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "that little operation it doesn't matter",
      "offset": 5956.639,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "what that operation is",
      "offset": 5959.04,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "and how composite it is",
      "offset": 5961.199,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "if you can write the local gradients you",
      "offset": 5963.04,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "can chain the gradient and you can",
      "offset": 5964.639,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "continue back propagation so the design",
      "offset": 5966,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "of what those functions are is",
      "offset": 5968.159,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "completely up to you",
      "offset": 5970.08,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "so now i would like to show you how you",
      "offset": 5971.92,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "can do the exact same thing by using a",
      "offset": 5973.28,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "modern deep neural network library like",
      "offset": 5975.28,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "for example pytorch which i've roughly",
      "offset": 5977.199,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "modeled micrograd",
      "offset": 5980,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "by",
      "offset": 5981.76,
      "duration": 1.919
    },
    {
      "lang": "en",
      "text": "and so",
      "offset": 5982.8,
      "duration": 2.16
    },
    {
      "lang": "en",
      "text": "pytorch is something you would use in",
      "offset": 5983.679,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "production and i'll show you how you can",
      "offset": 5984.96,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "do the exact same thing but in pytorch",
      "offset": 5986.639,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "api so i'm just going to copy paste it",
      "offset": 5988.639,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "in and walk you through it a little bit",
      "offset": 5990.88,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "this is what it looks like",
      "offset": 5992.56,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "so we're going to import pi torch and",
      "offset": 5994.719,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "then we need to define these",
      "offset": 5996.88,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "value objects like we have here",
      "offset": 5999.6,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "now micrograd is a scalar valued",
      "offset": 6001.679,
      "duration": 5.761
    },
    {
      "lang": "en",
      "text": "engine so we only have scalar values",
      "offset": 6004.8,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "like 2.0 but in pi torch everything is",
      "offset": 6007.44,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "based around tensors and like i",
      "offset": 6010,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "mentioned tensors are just n-dimensional",
      "offset": 6011.76,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "arrays of scalars",
      "offset": 6013.84,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "so that's why things get a little bit",
      "offset": 6015.76,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "more complicated here i just need a",
      "offset": 6017.6,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "scalar value to tensor a tensor with",
      "offset": 6019.84,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "just a single element",
      "offset": 6021.76,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "but by default when you work with",
      "offset": 6023.36,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "pytorch you would use um",
      "offset": 6025.28,
      "duration": 5.359
    },
    {
      "lang": "en",
      "text": "more complicated tensors like this so if",
      "offset": 6028.48,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "i import pytorch",
      "offset": 6030.639,
      "duration": 2.961
    },
    {
      "lang": "en",
      "text": "then i can create tensors like this and",
      "offset": 6033.92,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "this tensor for example is a two by",
      "offset": 6036.239,
      "duration": 3.521
    },
    {
      "lang": "en",
      "text": "three array",
      "offset": 6038.32,
      "duration": 2.799
    },
    {
      "lang": "en",
      "text": "of scalar",
      "offset": 6039.76,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "scalars",
      "offset": 6041.119,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "in a single compact representation so we",
      "offset": 6042.48,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "can check its shape we see that it's a",
      "offset": 6045.199,
      "duration": 2.801
    },
    {
      "lang": "en",
      "text": "two by three array",
      "offset": 6046.56,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "and so on",
      "offset": 6048,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "so this is usually what you would work",
      "offset": 6049.44,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "with um in the actual libraries so here",
      "offset": 6050.8,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "i'm creating",
      "offset": 6054,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "a tensor that has only a single element",
      "offset": 6055.28,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "2.0",
      "offset": 6058.08,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "and then i'm casting it to be double",
      "offset": 6060.48,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "because python is by default using",
      "offset": 6063.52,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "double precision for its floating point",
      "offset": 6065.76,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "numbers so i'd like everything to be",
      "offset": 6067.36,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "identical by default the data type of",
      "offset": 6068.96,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "these tensors will be float32 so it's",
      "offset": 6072.4,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "only using a single precision float so",
      "offset": 6074.8,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "i'm casting it to double",
      "offset": 6076.719,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "so that we have float64 just like in",
      "offset": 6078.96,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "python",
      "offset": 6081.199,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "so i'm casting to double and then we get",
      "offset": 6082.639,
      "duration": 5.441
    },
    {
      "lang": "en",
      "text": "something similar to value of two the",
      "offset": 6084.719,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "next thing i have to do is because these",
      "offset": 6088.08,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "are leaf nodes by default pytorch",
      "offset": 6089.6,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "assumes that they do not require",
      "offset": 6091.52,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "gradients so i need to explicitly say",
      "offset": 6092.96,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "that all of these nodes require",
      "offset": 6095.199,
      "duration": 2.721
    },
    {
      "lang": "en",
      "text": "gradients",
      "offset": 6096.8,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "okay so this is going to construct",
      "offset": 6097.92,
      "duration": 5.279
    },
    {
      "lang": "en",
      "text": "scalar valued one element tensors",
      "offset": 6099.84,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "make sure that fighters knows that they",
      "offset": 6103.199,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "require gradients now by default these",
      "offset": 6104.48,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "are set to false by the way because of",
      "offset": 6107.119,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "efficiency reasons because usually you",
      "offset": 6108.88,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "would not want gradients for leaf nodes",
      "offset": 6110.8,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "like the inputs to the network and this",
      "offset": 6113.6,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "is just trying to be efficient in the",
      "offset": 6115.76,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "most common cases",
      "offset": 6117.199,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "so once we've defined all of our values",
      "offset": 6119.199,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "in python we can perform arithmetic just",
      "offset": 6121.44,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "like we can here in microgradlend so",
      "offset": 6123.92,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "this will just work and then there's a",
      "offset": 6126,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "torch.10h also",
      "offset": 6127.6,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "and when we get back is a tensor again",
      "offset": 6129.679,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "and we can",
      "offset": 6132.48,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "just like in micrograd it's got a data",
      "offset": 6133.52,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "attribute and it's got grant attributes",
      "offset": 6135.6,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "so these tensor objects just like in",
      "offset": 6138.239,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "micrograd have a dot data and a dot grad",
      "offset": 6139.92,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "and",
      "offset": 6142.719,
      "duration": 2.4
    },
    {
      "lang": "en",
      "text": "the only difference here is that we need",
      "offset": 6143.44,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "to call it that item because otherwise",
      "offset": 6145.119,
      "duration": 5.281
    },
    {
      "lang": "en",
      "text": "um pi torch",
      "offset": 6148.08,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "that item basically takes",
      "offset": 6150.4,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "a single tensor of one element and it",
      "offset": 6152.159,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "just returns that element stripping out",
      "offset": 6154.08,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "the tensor",
      "offset": 6156.239,
      "duration": 3.281
    },
    {
      "lang": "en",
      "text": "so let me just run this and hopefully we",
      "offset": 6157.76,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "are going to get this is going to print",
      "offset": 6159.52,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "the forward pass",
      "offset": 6161.28,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "which is 0.707",
      "offset": 6162.48,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "and this will be the gradients which",
      "offset": 6164.88,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "hopefully are",
      "offset": 6166.8,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "0.5 0 negative 1.5 and 1.",
      "offset": 6168.32,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "so if we just run this",
      "offset": 6171.119,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "there we go",
      "offset": 6173.76,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "0.7 so the forward pass agrees and then",
      "offset": 6174.96,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "point five zero negative one point five",
      "offset": 6177.44,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "and one",
      "offset": 6179.36,
      "duration": 3.279
    },
    {
      "lang": "en",
      "text": "so pi torch agrees with us",
      "offset": 6180.639,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "and just to show you here basically o",
      "offset": 6182.639,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "here's a tensor with a single element",
      "offset": 6185.04,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "and it's a double",
      "offset": 6188.239,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "and we can call that item on it to just",
      "offset": 6189.52,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "get the single number out",
      "offset": 6192,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "so that's what item does and o is a",
      "offset": 6194.32,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "tensor object like i mentioned and it's",
      "offset": 6196.88,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "got a backward function just like we've",
      "offset": 6198.96,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "implemented",
      "offset": 6200.48,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "and then all of these also have a dot",
      "offset": 6202.08,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "graph so like x2 for example in the grad",
      "offset": 6203.84,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "and it's a tensor and we can pop out the",
      "offset": 6206.08,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "individual number with that actin",
      "offset": 6208.08,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "so basically",
      "offset": 6211.44,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "torches torch can do what we did in",
      "offset": 6212.96,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "micrograph is a special case when your",
      "offset": 6215.04,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "tensors are all single element tensors",
      "offset": 6217.199,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "but the big deal with pytorch is that",
      "offset": 6220.4,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "everything is significantly more",
      "offset": 6222.159,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "efficient because we are working with",
      "offset": 6223.28,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "these tensor objects and we can do lots",
      "offset": 6225.119,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "of operations in parallel on all of",
      "offset": 6227.28,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "these tensors",
      "offset": 6229.199,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "but otherwise what we've built very much",
      "offset": 6231.44,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "agrees with the api of pytorch",
      "offset": 6233.52,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "okay so now that we have some machinery",
      "offset": 6235.6,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "to build out pretty complicated",
      "offset": 6237.199,
      "duration": 3.281
    },
    {
      "lang": "en",
      "text": "mathematical expressions we can also",
      "offset": 6238.32,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "start building out neural nets and as i",
      "offset": 6240.48,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "mentioned neural nets are just a",
      "offset": 6242.239,
      "duration": 3.281
    },
    {
      "lang": "en",
      "text": "specific class of mathematical",
      "offset": 6243.44,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "expressions",
      "offset": 6245.52,
      "duration": 2.639
    },
    {
      "lang": "en",
      "text": "so we're going to start building out a",
      "offset": 6247.04,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "neural net piece by piece and eventually",
      "offset": 6248.159,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "we'll build out a two-layer multi-layer",
      "offset": 6249.84,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "layer perceptron as it's called and i'll",
      "offset": 6252.159,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "show you exactly what that means",
      "offset": 6254.239,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "let's start with a single individual",
      "offset": 6255.92,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "neuron we've implemented one here but",
      "offset": 6257.119,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "here i'm going to implement one that",
      "offset": 6259.679,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "also subscribes to the pytorch api in",
      "offset": 6261.36,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "how it designs its neural network",
      "offset": 6264.159,
      "duration": 3.121
    },
    {
      "lang": "en",
      "text": "modules",
      "offset": 6266.239,
      "duration": 2.641
    },
    {
      "lang": "en",
      "text": "so just like we saw that we can like",
      "offset": 6267.28,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "match the api of pytorch",
      "offset": 6268.88,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "on the auto grad side we're going to try",
      "offset": 6271.199,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "to do that on the neural network modules",
      "offset": 6273.6,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "so here's class neuron",
      "offset": 6275.84,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "and just for the sake of efficiency i'm",
      "offset": 6278.4,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "going to copy paste some sections that",
      "offset": 6280.96,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "are relatively straightforward",
      "offset": 6282.56,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "so the constructor will take",
      "offset": 6285.52,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "number of inputs to this neuron which is",
      "offset": 6287.52,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "how many inputs come to a neuron so this",
      "offset": 6289.76,
      "duration": 5.359
    },
    {
      "lang": "en",
      "text": "one for example has three inputs",
      "offset": 6292.719,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "and then it's going to create a weight",
      "offset": 6295.119,
      "duration": 3.281
    },
    {
      "lang": "en",
      "text": "there is some random number between",
      "offset": 6297.199,
      "duration": 2.881
    },
    {
      "lang": "en",
      "text": "negative one and one for every one of",
      "offset": 6298.4,
      "duration": 2.799
    },
    {
      "lang": "en",
      "text": "those inputs",
      "offset": 6300.08,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "and a bias that controls the overall",
      "offset": 6301.199,
      "duration": 5.281
    },
    {
      "lang": "en",
      "text": "trigger happiness of this neuron",
      "offset": 6303.679,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "and then we're going to implement a def",
      "offset": 6306.48,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "underscore underscore call",
      "offset": 6308.639,
      "duration": 5.361
    },
    {
      "lang": "en",
      "text": "of self and x some input x",
      "offset": 6311.28,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "and really what we don't do here is w",
      "offset": 6314,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "times x plus b",
      "offset": 6315.84,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "where w times x here is a dot product",
      "offset": 6317.36,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "specifically",
      "offset": 6319.52,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "now if you haven't seen",
      "offset": 6321.28,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "call",
      "offset": 6322.8,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "let me just return 0.0 here for now the",
      "offset": 6324,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "way this works now is we can have an x",
      "offset": 6326.719,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "which is say like 2.0 3.0 then we can",
      "offset": 6328.32,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "initialize a neuron that is",
      "offset": 6331.119,
      "duration": 2.801
    },
    {
      "lang": "en",
      "text": "two-dimensional",
      "offset": 6332.48,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "because these are two numbers and then",
      "offset": 6333.92,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "we can feed those two numbers into that",
      "offset": 6335.679,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "neuron to get an output",
      "offset": 6337.04,
      "duration": 5.599
    },
    {
      "lang": "en",
      "text": "and so when you use this notation n of x",
      "offset": 6339.76,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "python will use call",
      "offset": 6342.639,
      "duration": 6.401
    },
    {
      "lang": "en",
      "text": "so currently call just return 0.0",
      "offset": 6345.04,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "now we'd like to actually do the forward",
      "offset": 6350.08,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "pass of this neuron instead",
      "offset": 6352.08,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "so we're going to do here first is we",
      "offset": 6354.88,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "need to basically multiply all of the",
      "offset": 6357.28,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "elements of w with all of the elements",
      "offset": 6358.96,
      "duration": 5.279
    },
    {
      "lang": "en",
      "text": "of x pairwise we need to multiply them",
      "offset": 6361.36,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "so the first thing we're going to do is",
      "offset": 6364.239,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "we're going to zip up",
      "offset": 6365.6,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "celta w and x",
      "offset": 6367.119,
      "duration": 5.441
    },
    {
      "lang": "en",
      "text": "and in python zip takes two iterators",
      "offset": 6369.04,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "and it creates a new iterator that",
      "offset": 6372.56,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "iterates over the tuples of the",
      "offset": 6374.48,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "corresponding entries",
      "offset": 6376.159,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "so for example just to show you we can",
      "offset": 6377.92,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "print this list",
      "offset": 6380,
      "duration": 6.6
    },
    {
      "lang": "en",
      "text": "and still return 0.0 here",
      "offset": 6382,
      "duration": 4.6
    },
    {
      "lang": "en",
      "text": "sorry",
      "offset": 6390.8,
      "duration": 2.24
    },
    {
      "lang": "en",
      "text": "so we see that these w's are paired up",
      "offset": 6394.08,
      "duration": 7
    },
    {
      "lang": "en",
      "text": "with the x's w with x",
      "offset": 6396.239,
      "duration": 4.841
    },
    {
      "lang": "en",
      "text": "and now what we want to do is",
      "offset": 6401.44,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "for w i x i in",
      "offset": 6407.36,
      "duration": 5.359
    },
    {
      "lang": "en",
      "text": "we want to multiply w times",
      "offset": 6410.639,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "w wi times x i",
      "offset": 6412.719,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "and then we want to sum all of that",
      "offset": 6414.88,
      "duration": 2.64
    },
    {
      "lang": "en",
      "text": "together",
      "offset": 6416.56,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "to come up with an activation",
      "offset": 6417.52,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "and add also subnet b on top",
      "offset": 6419.6,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "so that's the raw activation and then of",
      "offset": 6422.159,
      "duration": 3.281
    },
    {
      "lang": "en",
      "text": "course we need to pass that through a",
      "offset": 6424.239,
      "duration": 3.041
    },
    {
      "lang": "en",
      "text": "non-linearity so what we're going to be",
      "offset": 6425.44,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "returning is act.10h",
      "offset": 6427.28,
      "duration": 4.959
    },
    {
      "lang": "en",
      "text": "and here's out",
      "offset": 6429.84,
      "duration": 3.279
    },
    {
      "lang": "en",
      "text": "so",
      "offset": 6432.239,
      "duration": 2.321
    },
    {
      "lang": "en",
      "text": "now we see that we are getting some",
      "offset": 6433.119,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "outputs and we get a different output",
      "offset": 6434.56,
      "duration": 3.119
    },
    {
      "lang": "en",
      "text": "from a neuron each time because we are",
      "offset": 6436.32,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "initializing different weights and by",
      "offset": 6437.679,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "and biases",
      "offset": 6439.44,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "and then to be a bit more efficient here",
      "offset": 6441.199,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "actually sum by the way takes a second",
      "offset": 6442.639,
      "duration": 6.241
    },
    {
      "lang": "en",
      "text": "optional parameter which is the start",
      "offset": 6445.44,
      "duration": 6.239
    },
    {
      "lang": "en",
      "text": "and by default the start is zero so",
      "offset": 6448.88,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "these elements of this sum will be added",
      "offset": 6451.679,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "on top of zero to begin with but",
      "offset": 6454.08,
      "duration": 3.119
    },
    {
      "lang": "en",
      "text": "actually we can just start with cell dot",
      "offset": 6455.76,
      "duration": 2.64
    },
    {
      "lang": "en",
      "text": "b",
      "offset": 6457.199,
      "duration": 2.561
    },
    {
      "lang": "en",
      "text": "and then we just have an expression like",
      "offset": 6458.4,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "this",
      "offset": 6459.76,
      "duration": 2.24
    },
    {
      "lang": "en",
      "text": "and then the generator expression here",
      "offset": 6465.44,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "must be parenthesized in python",
      "offset": 6467.04,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "there we go",
      "offset": 6469.36,
      "duration": 2.48
    },
    {
      "lang": "en",
      "text": "yep so now we can forward a single",
      "offset": 6473.76,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "neuron next up we're going to define a",
      "offset": 6475.679,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "layer of neurons so here we have a",
      "offset": 6477.92,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "schematic for a mlb",
      "offset": 6479.92,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "so we see that these mlps each layer",
      "offset": 6482.48,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "this is one layer has actually a number",
      "offset": 6485.119,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "of neurons and they're not connected to",
      "offset": 6487.04,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "each other but all of them are fully",
      "offset": 6488.639,
      "duration": 2.641
    },
    {
      "lang": "en",
      "text": "connected to the input",
      "offset": 6489.76,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "so what is a layer of neurons it's just",
      "offset": 6491.28,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "it's just a set of neurons evaluated",
      "offset": 6493.44,
      "duration": 3.279
    },
    {
      "lang": "en",
      "text": "independently",
      "offset": 6495.119,
      "duration": 2.641
    },
    {
      "lang": "en",
      "text": "so",
      "offset": 6496.719,
      "duration": 3.281
    },
    {
      "lang": "en",
      "text": "in the interest of time i'm going to do",
      "offset": 6497.76,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "something fairly straightforward here",
      "offset": 6500,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "it's um",
      "offset": 6503.04,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "literally a layer is just a list of",
      "offset": 6505.04,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "neurons",
      "offset": 6507.679,
      "duration": 3.121
    },
    {
      "lang": "en",
      "text": "and then how many neurons do we have we",
      "offset": 6508.96,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "take that as an input argument here how",
      "offset": 6510.8,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "many neurons do you want in your layer",
      "offset": 6512.719,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "number of outputs in this layer",
      "offset": 6514.08,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "and so we just initialize completely",
      "offset": 6516.56,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "independent neurons with this given",
      "offset": 6518.4,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "dimensionality and when we call on it we",
      "offset": 6520.239,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "just independently",
      "offset": 6523.119,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "evaluate them so now instead of a neuron",
      "offset": 6524.96,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "we can make a layer of neurons they are",
      "offset": 6527.76,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "two-dimensional neurons and let's have",
      "offset": 6529.84,
      "duration": 2.48
    },
    {
      "lang": "en",
      "text": "three of them",
      "offset": 6531.199,
      "duration": 2.641
    },
    {
      "lang": "en",
      "text": "and now we see that we have three",
      "offset": 6532.32,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "independent evaluations of three",
      "offset": 6533.84,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "different neurons",
      "offset": 6535.76,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "right",
      "offset": 6537.199,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "okay finally let's complete this picture",
      "offset": 6538.8,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "and define an entire multi-layer",
      "offset": 6540.88,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "perceptron or mlp",
      "offset": 6542.639,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "and as we can see here in an mlp these",
      "offset": 6544.56,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "layers just feed into each other",
      "offset": 6546.639,
      "duration": 2.56
    },
    {
      "lang": "en",
      "text": "sequentially",
      "offset": 6547.84,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "so let's come here and i'm just going to",
      "offset": 6549.199,
      "duration": 5.121
    },
    {
      "lang": "en",
      "text": "copy the code here in interest of time",
      "offset": 6551.6,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "so an mlp is very similar",
      "offset": 6554.32,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "we're taking the number of inputs",
      "offset": 6556.639,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "as before but now instead of taking a",
      "offset": 6558.719,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "single n out which is number of neurons",
      "offset": 6560.8,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "in a single layer we're going to take a",
      "offset": 6562.639,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "list of an outs and this list defines",
      "offset": 6564.719,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "the sizes of all the layers that we want",
      "offset": 6566.96,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "in our mlp",
      "offset": 6568.88,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "so here we just put them all together",
      "offset": 6570.32,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "and then iterate over consecutive pairs",
      "offset": 6571.92,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "of these sizes and create layer objects",
      "offset": 6574.4,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "for them",
      "offset": 6576.8,
      "duration": 2.399
    },
    {
      "lang": "en",
      "text": "and then in the call function we are",
      "offset": 6577.84,
      "duration": 3.279
    },
    {
      "lang": "en",
      "text": "just calling them sequentially so that's",
      "offset": 6579.199,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "an mlp really",
      "offset": 6581.119,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "and let's actually re-implement this",
      "offset": 6582.8,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "picture so we want three input neurons",
      "offset": 6584.159,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "and then two layers of four and an",
      "offset": 6586.56,
      "duration": 3.119
    },
    {
      "lang": "en",
      "text": "output unit",
      "offset": 6588,
      "duration": 2.56
    },
    {
      "lang": "en",
      "text": "so",
      "offset": 6589.679,
      "duration": 2.721
    },
    {
      "lang": "en",
      "text": "we want",
      "offset": 6590.56,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "a three-dimensional input say this is an",
      "offset": 6592.4,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "example input we want three inputs into",
      "offset": 6594.32,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "two layers of four and one output",
      "offset": 6597.52,
      "duration": 6.079
    },
    {
      "lang": "en",
      "text": "and this of course is an mlp",
      "offset": 6600.56,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "and there we go that's a forward pass of",
      "offset": 6603.599,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "an mlp",
      "offset": 6605.44,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "to make this a little bit nicer you see",
      "offset": 6606.719,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "how we have just a single element but",
      "offset": 6608.4,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "it's wrapped in a list because layer",
      "offset": 6609.92,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "always returns lists",
      "offset": 6611.599,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "circle for convenience",
      "offset": 6613.599,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "return outs at zero if len out is",
      "offset": 6615.599,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "exactly a single element",
      "offset": 6618.639,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "else return fullest",
      "offset": 6620.56,
      "duration": 3.039
    },
    {
      "lang": "en",
      "text": "and this will allow us to just get a",
      "offset": 6622.4,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "single value out at the last layer that",
      "offset": 6623.599,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "only has a single neuron",
      "offset": 6625.92,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "and finally we should be able to draw",
      "offset": 6628.08,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "dot of n of x",
      "offset": 6629.599,
      "duration": 2.801
    },
    {
      "lang": "en",
      "text": "and",
      "offset": 6631.36,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "as you might imagine",
      "offset": 6632.4,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "these expressions are now getting",
      "offset": 6634.08,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "relatively involved",
      "offset": 6636.719,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "so this is an entire mlp that we're",
      "offset": 6638.48,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "defining now",
      "offset": 6640.239,
      "duration": 2.641
    },
    {
      "lang": "en",
      "text": "all the way until a single output",
      "offset": 6645.28,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "okay",
      "offset": 6648.239,
      "duration": 2.561
    },
    {
      "lang": "en",
      "text": "and so obviously you would never",
      "offset": 6649.28,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "differentiate on pen and paper these",
      "offset": 6650.8,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "expressions but with micrograd we will",
      "offset": 6652.719,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "be able to back propagate all the way",
      "offset": 6655.199,
      "duration": 2.881
    },
    {
      "lang": "en",
      "text": "through this",
      "offset": 6656.8,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "and back propagate",
      "offset": 6658.08,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "into",
      "offset": 6659.76,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "these weights of all these neurons so",
      "offset": 6660.8,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "let's see how that works okay so let's",
      "offset": 6662.96,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "create ourselves a very simple",
      "offset": 6664.639,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "example data set here",
      "offset": 6666.8,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "so this data set has four examples",
      "offset": 6668.639,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "and so we have four possible",
      "offset": 6671.04,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "inputs into the neural net",
      "offset": 6673.52,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "and we have four desired targets so we'd",
      "offset": 6675.36,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "like the neural net to assign",
      "offset": 6677.679,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "or output 1.0 when it's fed this example",
      "offset": 6681.28,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "negative one when it's fed these",
      "offset": 6684.159,
      "duration": 2.801
    },
    {
      "lang": "en",
      "text": "examples and one when it's fed this",
      "offset": 6685.44,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "example so it's a very simple binary",
      "offset": 6686.96,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "classifier neural net basically that we",
      "offset": 6688.96,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "would like here",
      "offset": 6690.8,
      "duration": 2.879
    },
    {
      "lang": "en",
      "text": "now let's think what the neural net",
      "offset": 6692.32,
      "duration": 2.64
    },
    {
      "lang": "en",
      "text": "currently thinks about these four",
      "offset": 6693.679,
      "duration": 2.881
    },
    {
      "lang": "en",
      "text": "examples we can just get their",
      "offset": 6694.96,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "predictions",
      "offset": 6696.56,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "um basically we can just call n of x for",
      "offset": 6697.84,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "x in axis",
      "offset": 6700.32,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "and then we can",
      "offset": 6702,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "print",
      "offset": 6703.44,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "so these are the outputs of the neural",
      "offset": 6705.04,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "net on those four examples",
      "offset": 6706.639,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "so",
      "offset": 6708.8,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "the first one is 0.91 but we'd like it",
      "offset": 6710.239,
      "duration": 5.041
    },
    {
      "lang": "en",
      "text": "to be one so we should push this one",
      "offset": 6712.719,
      "duration": 5.281
    },
    {
      "lang": "en",
      "text": "higher this one we want to be higher",
      "offset": 6715.28,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "this one says 0.88 and we want this to",
      "offset": 6718,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "be negative one",
      "offset": 6720.96,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "this is 0.8 we want it to be negative",
      "offset": 6722.639,
      "duration": 2.48
    },
    {
      "lang": "en",
      "text": "one",
      "offset": 6724.32,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "and this one is 0.8 we want it to be one",
      "offset": 6725.119,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "so how do we make the neural net and how",
      "offset": 6728.159,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "do we tune the weights",
      "offset": 6730.239,
      "duration": 2.721
    },
    {
      "lang": "en",
      "text": "to",
      "offset": 6732.159,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "better predict the desired targets",
      "offset": 6732.96,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "and the trick used in deep learning to",
      "offset": 6736.719,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "achieve this is to",
      "offset": 6738.48,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "calculate a single number that somehow",
      "offset": 6740.48,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "measures the total performance of your",
      "offset": 6742.239,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "neural net and we call this single",
      "offset": 6744.4,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "number the loss",
      "offset": 6745.92,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "so the loss",
      "offset": 6748,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "first",
      "offset": 6749.76,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "is is a single number that we're going",
      "offset": 6751.04,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "to define that basically measures how",
      "offset": 6752.56,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "well the neural net is performing right",
      "offset": 6754.4,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "now we have the intuitive sense that",
      "offset": 6756.239,
      "duration": 2.561
    },
    {
      "lang": "en",
      "text": "it's not performing very well because",
      "offset": 6757.36,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "we're not very much close to this",
      "offset": 6758.8,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "so the loss will be high and we'll want",
      "offset": 6760.88,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "to minimize the loss",
      "offset": 6763.04,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "so in particular in this case what we're",
      "offset": 6764.639,
      "duration": 3.121
    },
    {
      "lang": "en",
      "text": "going to do is we're going to implement",
      "offset": 6766.08,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "the mean squared error loss",
      "offset": 6767.76,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "so this is doing is we're going to",
      "offset": 6769.679,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "basically iterate um",
      "offset": 6771.36,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "for y ground truth",
      "offset": 6774.08,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "and y output in zip of um",
      "offset": 6776.4,
      "duration": 5.279
    },
    {
      "lang": "en",
      "text": "wise and white red so we're going to",
      "offset": 6779.92,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "pair up the",
      "offset": 6781.679,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "ground truths with the predictions",
      "offset": 6783.119,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "and this zip iterates over tuples of",
      "offset": 6786.08,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "them",
      "offset": 6787.84,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "and for each",
      "offset": 6788.88,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "y ground truth and y output we're going",
      "offset": 6791.52,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "to subtract them",
      "offset": 6793.52,
      "duration": 2.719
    },
    {
      "lang": "en",
      "text": "and square them",
      "offset": 6796.96,
      "duration": 3.279
    },
    {
      "lang": "en",
      "text": "so let's first see what these losses are",
      "offset": 6798.56,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "these are individual loss components",
      "offset": 6800.239,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "and so basically for each",
      "offset": 6802.96,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "one of the four",
      "offset": 6805.199,
      "duration": 3.281
    },
    {
      "lang": "en",
      "text": "we are taking the prediction and the",
      "offset": 6806.4,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "ground truth we are subtracting them and",
      "offset": 6808.48,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "squaring them",
      "offset": 6810.8,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "so because",
      "offset": 6812.639,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "this one is so close to its target 0.91",
      "offset": 6813.84,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "is almost one",
      "offset": 6816.88,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "subtracting them gives a very small",
      "offset": 6818.719,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "number",
      "offset": 6820.4,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "so here we would get like a negative",
      "offset": 6821.92,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "point one and then squaring it",
      "offset": 6823.36,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "just makes sure",
      "offset": 6825.76,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "that regardless of whether we are more",
      "offset": 6827.599,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "negative or more positive we always get",
      "offset": 6829.599,
      "duration": 3.241
    },
    {
      "lang": "en",
      "text": "a positive",
      "offset": 6831.52,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "number instead of squaring we should we",
      "offset": 6832.84,
      "duration": 4.04
    },
    {
      "lang": "en",
      "text": "could also take for example the absolute",
      "offset": 6835.52,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "value we need to discard the sign",
      "offset": 6836.88,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "and so you see that the expression is",
      "offset": 6839.44,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "ranged so that you only get zero exactly",
      "offset": 6840.88,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "when y out is equal to y ground truth",
      "offset": 6843.52,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "when those two are equal so your",
      "offset": 6846.4,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "prediction is exactly the target you are",
      "offset": 6847.76,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "going to get zero",
      "offset": 6849.599,
      "duration": 3.281
    },
    {
      "lang": "en",
      "text": "and if your prediction is not the target",
      "offset": 6850.96,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "you are going to get some other number",
      "offset": 6852.88,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "so here for example we are way off and",
      "offset": 6855.04,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "so that's why the loss is quite high",
      "offset": 6857.119,
      "duration": 5.441
    },
    {
      "lang": "en",
      "text": "and the more off we are the greater the",
      "offset": 6859.84,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "loss will be",
      "offset": 6862.56,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "so we don't want high loss we want low",
      "offset": 6864.08,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "loss",
      "offset": 6866.8,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "and so the final loss here will be just",
      "offset": 6867.76,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "the sum",
      "offset": 6870.56,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "of all of these",
      "offset": 6872,
      "duration": 2.239
    },
    {
      "lang": "en",
      "text": "numbers",
      "offset": 6873.28,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "so you see that this should be zero",
      "offset": 6874.239,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "roughly plus zero roughly",
      "offset": 6876.32,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "but plus",
      "offset": 6878.4,
      "duration": 2.4
    },
    {
      "lang": "en",
      "text": "seven",
      "offset": 6879.679,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "so loss should be about seven",
      "offset": 6880.8,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "here",
      "offset": 6883.599,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "and now we want to minimize the loss we",
      "offset": 6884.8,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "want the loss to be low",
      "offset": 6887.28,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "because if loss is low",
      "offset": 6889.28,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "then every one of the predictions is",
      "offset": 6891.52,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "equal to its target",
      "offset": 6894,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "so the loss the lowest it can be is zero",
      "offset": 6896.32,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "and the greater it is the worse off the",
      "offset": 6898.88,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "neural net is predicting",
      "offset": 6901.52,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "so now of course if we do lost that",
      "offset": 6904.08,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "backward",
      "offset": 6905.599,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "something magical happened when i hit",
      "offset": 6907.92,
      "duration": 2.64
    },
    {
      "lang": "en",
      "text": "enter",
      "offset": 6909.44,
      "duration": 2.719
    },
    {
      "lang": "en",
      "text": "and the magical thing of course that",
      "offset": 6910.56,
      "duration": 3.559
    },
    {
      "lang": "en",
      "text": "happened is that we can look at",
      "offset": 6912.159,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "end.layers.neuron and that layers at say",
      "offset": 6914.119,
      "duration": 4.681
    },
    {
      "lang": "en",
      "text": "like the the first layer",
      "offset": 6916.56,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "that neurons at zero",
      "offset": 6918.8,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "because remember that mlp has the layers",
      "offset": 6922.4,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "which is a list",
      "offset": 6924.88,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "and each layer has a neurons which is a",
      "offset": 6926.48,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "list and that gives us an individual",
      "offset": 6928.08,
      "duration": 2.639
    },
    {
      "lang": "en",
      "text": "neuron",
      "offset": 6929.76,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "and then it's got some weights",
      "offset": 6930.719,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "and so we can for example look at the",
      "offset": 6932.8,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "weights at zero",
      "offset": 6934.48,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "um",
      "offset": 6938.239,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "oops it's not called weights it's called",
      "offset": 6940.159,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "w",
      "offset": 6942.08,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "and that's a value but now this value",
      "offset": 6944.239,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "also has a groud because of the backward",
      "offset": 6946.4,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "pass",
      "offset": 6948.8,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "and so we see that because this gradient",
      "offset": 6950.08,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "here on this particular weight of this",
      "offset": 6952.88,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "particular neuron of this particular",
      "offset": 6954.8,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "layer is negative",
      "offset": 6956.239,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "we see that its influence on the loss is",
      "offset": 6957.84,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "also negative so slightly increasing",
      "offset": 6960.08,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "this particular weight of this neuron of",
      "offset": 6962.48,
      "duration": 6.32
    },
    {
      "lang": "en",
      "text": "this layer would make the loss go down",
      "offset": 6964.639,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "and we actually have this information",
      "offset": 6968.8,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "for every single one of our neurons and",
      "offset": 6970.159,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "all their parameters actually it's worth",
      "offset": 6972.4,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "looking at also the draw dot loss by the",
      "offset": 6973.76,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "way",
      "offset": 6976.88,
      "duration": 2.799
    },
    {
      "lang": "en",
      "text": "so previously we looked at the draw dot",
      "offset": 6977.92,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "of a single neural neuron forward pass",
      "offset": 6979.679,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "and that was already a large expression",
      "offset": 6981.76,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "but what is this expression we actually",
      "offset": 6983.76,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "forwarded",
      "offset": 6985.92,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "every one of those four examples and",
      "offset": 6987.36,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "then we have the loss on top of them",
      "offset": 6989.119,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "with the mean squared error",
      "offset": 6990.88,
      "duration": 5.279
    },
    {
      "lang": "en",
      "text": "and so this is a really massive graph",
      "offset": 6992.8,
      "duration": 5.439
    },
    {
      "lang": "en",
      "text": "because this graph that we've built up",
      "offset": 6996.159,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "now",
      "offset": 6998.239,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "oh my gosh this graph that we've built",
      "offset": 6999.679,
      "duration": 3.281
    },
    {
      "lang": "en",
      "text": "up now",
      "offset": 7001.679,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "which is kind of excessive it's",
      "offset": 7002.96,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "excessive because it has four forward",
      "offset": 7004.719,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "passes of a neural net for every one of",
      "offset": 7006.4,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "the examples and then it has the loss on",
      "offset": 7008.239,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "top",
      "offset": 7010.639,
      "duration": 2.401
    },
    {
      "lang": "en",
      "text": "and it ends with the value of the loss",
      "offset": 7011.44,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "which was 7.12",
      "offset": 7013.04,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "and this loss will now back propagate",
      "offset": 7015.199,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "through all the four forward passes all",
      "offset": 7016.639,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "the way through just every single",
      "offset": 7018.96,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "intermediate value of the neural net",
      "offset": 7020.96,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "all the way back to of course the",
      "offset": 7023.119,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "parameters of the weights which are the",
      "offset": 7025.199,
      "duration": 2.721
    },
    {
      "lang": "en",
      "text": "input",
      "offset": 7026.8,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "so these weight parameters here are",
      "offset": 7027.92,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "inputs to this neural net",
      "offset": 7030.32,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "and",
      "offset": 7032.32,
      "duration": 2.799
    },
    {
      "lang": "en",
      "text": "these numbers here these scalars are",
      "offset": 7033.28,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "inputs to the neural net",
      "offset": 7035.119,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "so if we went around here",
      "offset": 7036.88,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "we'll probably find",
      "offset": 7038.88,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "some of these examples this 1.0",
      "offset": 7040.639,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "potentially maybe this 1.0 or you know",
      "offset": 7042.88,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "some of the others and you'll see that",
      "offset": 7045.28,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "they all have gradients as well",
      "offset": 7046.96,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "the thing is these gradients on the",
      "offset": 7048.8,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "input data are not that useful to us",
      "offset": 7050.8,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "and that's because the input data seems",
      "offset": 7053.44,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "to be not changeable it's it's a given",
      "offset": 7056.08,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "to the problem and so it's a fixed input",
      "offset": 7058.88,
      "duration": 3.279
    },
    {
      "lang": "en",
      "text": "we're not going to be changing it or",
      "offset": 7060.88,
      "duration": 2.799
    },
    {
      "lang": "en",
      "text": "messing with it even though we do have",
      "offset": 7062.159,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "gradients for it",
      "offset": 7063.679,
      "duration": 5.361
    },
    {
      "lang": "en",
      "text": "but some of these gradients here",
      "offset": 7066,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "will be for the neural network",
      "offset": 7069.04,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "parameters the ws and the bs and those",
      "offset": 7070.32,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "we of course we want to change",
      "offset": 7073.28,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "okay so now we're going to want some",
      "offset": 7075.76,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "convenience code to gather up all of the",
      "offset": 7078.08,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "parameters of the neural net so that we",
      "offset": 7079.92,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "can operate on all of them",
      "offset": 7081.679,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "simultaneously and every one of them we",
      "offset": 7083.36,
      "duration": 5.359
    },
    {
      "lang": "en",
      "text": "will nudge a tiny amount",
      "offset": 7085.76,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "based on the gradient information",
      "offset": 7088.719,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "so let's collect the parameters of the",
      "offset": 7090.639,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "neural net all in one array",
      "offset": 7091.92,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "so let's create a parameters of self",
      "offset": 7094.8,
      "duration": 4.04
    },
    {
      "lang": "en",
      "text": "that just",
      "offset": 7097.84,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "returns celta w which is a list",
      "offset": 7098.84,
      "duration": 5.16
    },
    {
      "lang": "en",
      "text": "concatenated with",
      "offset": 7102.239,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "a list of self.b",
      "offset": 7104,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "so this will just return a list",
      "offset": 7107.119,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "list plus list just you know gives you a",
      "offset": 7109.44,
      "duration": 2.719
    },
    {
      "lang": "en",
      "text": "list",
      "offset": 7111.36,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "so that's parameters of neuron and i'm",
      "offset": 7112.159,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "calling it this way because also pi",
      "offset": 7115.04,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "torch has a parameters on every single",
      "offset": 7116.719,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "and in module",
      "offset": 7118.8,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "and uh it does exactly what we're doing",
      "offset": 7120.08,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "here it just returns the",
      "offset": 7122,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "parameter tensors for us as the",
      "offset": 7124,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "parameter scalars",
      "offset": 7126,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "now layer is also a module so it will",
      "offset": 7128.4,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "have parameters",
      "offset": 7130.88,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "itself",
      "offset": 7132.48,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "and basically what we want to do here is",
      "offset": 7134.08,
      "duration": 5.599
    },
    {
      "lang": "en",
      "text": "something like this like",
      "offset": 7136.719,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "params is here and then for",
      "offset": 7140.239,
      "duration": 7.121
    },
    {
      "lang": "en",
      "text": "neuron in salt out neurons",
      "offset": 7143.52,
      "duration": 7.04
    },
    {
      "lang": "en",
      "text": "we want to get neuron.parameters",
      "offset": 7147.36,
      "duration": 6.64
    },
    {
      "lang": "en",
      "text": "and we want to params.extend",
      "offset": 7150.56,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "right so these are the parameters of",
      "offset": 7154,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "this neuron and then we want to put them",
      "offset": 7156,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "on top of params so params dot extend",
      "offset": 7157.92,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "of peace",
      "offset": 7161.44,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "and then we want to return brands",
      "offset": 7162.56,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "so this is way too much code so actually",
      "offset": 7165.36,
      "duration": 6.239
    },
    {
      "lang": "en",
      "text": "there's a way to simplify this which is",
      "offset": 7168.08,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "return",
      "offset": 7171.599,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "p",
      "offset": 7173.92,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "for neuron in self",
      "offset": 7175.52,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "neurons",
      "offset": 7178,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "for",
      "offset": 7179.84,
      "duration": 5.839
    },
    {
      "lang": "en",
      "text": "p in neuron dot parameters",
      "offset": 7181.36,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "so it's a single list comprehension in",
      "offset": 7185.679,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "python you can sort of nest them like",
      "offset": 7187.36,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "this and you can um",
      "offset": 7189.28,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "then create",
      "offset": 7191.599,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "uh the desired",
      "offset": 7192.8,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "array so this is these are identical",
      "offset": 7194.8,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "we can take this out",
      "offset": 7197.44,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "and then let's do the same here",
      "offset": 7200,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "def parameters",
      "offset": 7204.639,
      "duration": 3.281
    },
    {
      "lang": "en",
      "text": "self",
      "offset": 7206.639,
      "duration": 2.801
    },
    {
      "lang": "en",
      "text": "and return",
      "offset": 7207.92,
      "duration": 5.679
    },
    {
      "lang": "en",
      "text": "a parameter for layer in self dot layers",
      "offset": 7209.44,
      "duration": 5.679
    },
    {
      "lang": "en",
      "text": "for",
      "offset": 7213.599,
      "duration": 5.841
    },
    {
      "lang": "en",
      "text": "p in layer dot parameters",
      "offset": 7215.119,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "and that should be good",
      "offset": 7220.96,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "now let me pop out this so",
      "offset": 7223.679,
      "duration": 5.281
    },
    {
      "lang": "en",
      "text": "we don't re-initialize our network",
      "offset": 7226.4,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "because we need to re-initialize",
      "offset": 7228.96,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "our",
      "offset": 7231.36,
      "duration": 2.4
    },
    {
      "lang": "en",
      "text": "okay so unfortunately we will have to",
      "offset": 7235.599,
      "duration": 2.961
    },
    {
      "lang": "en",
      "text": "probably re-initialize the network",
      "offset": 7237.28,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "because we just add functionality",
      "offset": 7238.56,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "because this class of course we i want",
      "offset": 7241.52,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "to get all the and that parameters but",
      "offset": 7243.119,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "that's not going to work because this is",
      "offset": 7245.76,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "the old class",
      "offset": 7247.28,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "okay",
      "offset": 7249.84,
      "duration": 2.16
    },
    {
      "lang": "en",
      "text": "so unfortunately we do have to",
      "offset": 7250.88,
      "duration": 2.64
    },
    {
      "lang": "en",
      "text": "reinitialize the network which will",
      "offset": 7252,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "change some of the numbers",
      "offset": 7253.52,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "but let me do that so that we pick up",
      "offset": 7255.679,
      "duration": 3.121
    },
    {
      "lang": "en",
      "text": "the new api we can now do in the",
      "offset": 7257.199,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "parameters",
      "offset": 7258.8,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "and these are all the weights and biases",
      "offset": 7260.159,
      "duration": 5.601
    },
    {
      "lang": "en",
      "text": "inside the entire neural net",
      "offset": 7262.96,
      "duration": 8.32
    },
    {
      "lang": "en",
      "text": "so in total this mlp has 41 parameters",
      "offset": 7265.76,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "and",
      "offset": 7271.52,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "now we'll be able to change them",
      "offset": 7272.88,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "if we recalculate the loss here we see",
      "offset": 7275.599,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "that unfortunately we have slightly",
      "offset": 7278.4,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "different",
      "offset": 7279.84,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "predictions and slightly different laws",
      "offset": 7282.4,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "but that's okay",
      "offset": 7286.56,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "okay so we see that this neurons",
      "offset": 7288.56,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "gradient is slightly negative we can",
      "offset": 7291.04,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "also look at its data right now",
      "offset": 7293.199,
      "duration": 5.281
    },
    {
      "lang": "en",
      "text": "which is 0.85 so this is the current",
      "offset": 7296.48,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "value of this neuron and this is its",
      "offset": 7298.48,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "gradient on the loss",
      "offset": 7300.639,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "so what we want to do now is we want to",
      "offset": 7303.119,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "iterate for every p in",
      "offset": 7305.04,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "n dot parameters so for all the 41",
      "offset": 7307.76,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "parameters in this neural net",
      "offset": 7309.92,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "we actually want to change p data",
      "offset": 7311.84,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "slightly",
      "offset": 7315.04,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "according to the gradient information",
      "offset": 7316.159,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "okay so",
      "offset": 7319.119,
      "duration": 3.281
    },
    {
      "lang": "en",
      "text": "dot dot to do here",
      "offset": 7320.32,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "but this will be basically a tiny update",
      "offset": 7322.4,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "in this gradient descent scheme in",
      "offset": 7325.199,
      "duration": 5.361
    },
    {
      "lang": "en",
      "text": "gradient descent we are thinking of the",
      "offset": 7328.32,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "gradient as a vector pointing in the",
      "offset": 7330.56,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "direction",
      "offset": 7333.199,
      "duration": 2.161
    },
    {
      "lang": "en",
      "text": "of",
      "offset": 7334.32,
      "duration": 2.48
    },
    {
      "lang": "en",
      "text": "increased",
      "offset": 7335.36,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "loss",
      "offset": 7336.8,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "and so",
      "offset": 7339.119,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "in gradient descent we are modifying",
      "offset": 7340.08,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "p data",
      "offset": 7342.719,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "by a small step size in the direction of",
      "offset": 7344.32,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "the gradient so the step size as an",
      "offset": 7346.56,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "example could be like a very small",
      "offset": 7348.4,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "number like 0.01 is the step size times",
      "offset": 7349.76,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "p dot grad",
      "offset": 7352.8,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "right",
      "offset": 7355.04,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "but we have to think through some of the",
      "offset": 7356.4,
      "duration": 2.48
    },
    {
      "lang": "en",
      "text": "signs here",
      "offset": 7357.76,
      "duration": 2.56
    },
    {
      "lang": "en",
      "text": "so uh",
      "offset": 7358.88,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "in particular working with this specific",
      "offset": 7360.32,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "example here",
      "offset": 7363.04,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "we see that if we just left it like this",
      "offset": 7364.56,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "then this neuron's value",
      "offset": 7367.04,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "would be currently increased by a tiny",
      "offset": 7369.28,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "amount of the gradient",
      "offset": 7371.119,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "the grain is negative so this value of",
      "offset": 7373.44,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "this neuron would go slightly down it",
      "offset": 7376.32,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "would become like 0.8 you know four or",
      "offset": 7378.08,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "something like that",
      "offset": 7380.56,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "but if this neuron's value goes lower",
      "offset": 7382.88,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "that would actually",
      "offset": 7386.48,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "increase the loss",
      "offset": 7388.88,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "that's because",
      "offset": 7390.639,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "the derivative of this neuron is",
      "offset": 7392.48,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "negative so increasing",
      "offset": 7394.239,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "this makes the loss go down so",
      "offset": 7396.56,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "increasing it is what we want to do",
      "offset": 7399.199,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "instead of decreasing it so basically",
      "offset": 7401.44,
      "duration": 2.799
    },
    {
      "lang": "en",
      "text": "what we're missing here is we're",
      "offset": 7403.28,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "actually missing a negative sign",
      "offset": 7404.239,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "and again this other interpretation",
      "offset": 7406.48,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "and that's because we want to minimize",
      "offset": 7409.04,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "the loss we don't want to maximize the",
      "offset": 7410.32,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "loss we want to decrease it",
      "offset": 7411.84,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "and the other interpretation as i",
      "offset": 7413.76,
      "duration": 2.56
    },
    {
      "lang": "en",
      "text": "mentioned is you can think of the",
      "offset": 7414.96,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "gradient vector",
      "offset": 7416.32,
      "duration": 2.879
    },
    {
      "lang": "en",
      "text": "so basically just the vector of all the",
      "offset": 7417.76,
      "duration": 2.64
    },
    {
      "lang": "en",
      "text": "gradients",
      "offset": 7419.199,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "as pointing in the direction of",
      "offset": 7420.4,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "increasing",
      "offset": 7422.719,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "the loss but then we want to decrease it",
      "offset": 7424.239,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "so we actually want to go in the",
      "offset": 7426.4,
      "duration": 2.719
    },
    {
      "lang": "en",
      "text": "opposite direction",
      "offset": 7427.44,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "and so you can convince yourself that",
      "offset": 7429.119,
      "duration": 2.801
    },
    {
      "lang": "en",
      "text": "this sort of plug does the right thing",
      "offset": 7430.32,
      "duration": 2.879
    },
    {
      "lang": "en",
      "text": "here with the negative because we want",
      "offset": 7431.92,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "to minimize the loss",
      "offset": 7433.199,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "so if we nudge all the parameters by",
      "offset": 7435.36,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "tiny amount",
      "offset": 7437.119,
      "duration": 2.641
    },
    {
      "lang": "en",
      "text": "then we'll see that",
      "offset": 7440.639,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "this data will have changed a little bit",
      "offset": 7442.079,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "so now this neuron",
      "offset": 7444.079,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "is a tiny amount greater",
      "offset": 7446.079,
      "duration": 7.761
    },
    {
      "lang": "en",
      "text": "value so 0.854 went to 0.857",
      "offset": 7448.88,
      "duration": 7.199
    },
    {
      "lang": "en",
      "text": "and that's a good thing because slightly",
      "offset": 7453.84,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "increasing this neuron",
      "offset": 7456.079,
      "duration": 2.801
    },
    {
      "lang": "en",
      "text": "uh",
      "offset": 7458.079,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "data makes the loss go down according to",
      "offset": 7458.88,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "the gradient and so the correct thing",
      "offset": 7461.92,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "has happened sign wise",
      "offset": 7463.679,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "and so now what we would expect of",
      "offset": 7466.159,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "course is that",
      "offset": 7467.92,
      "duration": 2.319
    },
    {
      "lang": "en",
      "text": "because we've changed all these",
      "offset": 7469.119,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "parameters we expect that the loss",
      "offset": 7470.239,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "should have gone down a bit",
      "offset": 7472.639,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "so we want to re-evaluate the loss let",
      "offset": 7475.119,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "me basically",
      "offset": 7477.04,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "this is just a data definition that",
      "offset": 7479.92,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "hasn't changed but the forward pass here",
      "offset": 7481.52,
      "duration": 7.04
    },
    {
      "lang": "en",
      "text": "of the network we can recalculate",
      "offset": 7484.88,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "and actually let me do it outside here",
      "offset": 7489.679,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "so that we can compare the two loss",
      "offset": 7491.52,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "values",
      "offset": 7492.88,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "so here if i recalculate the loss",
      "offset": 7494.4,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "we'd expect the new loss now to be",
      "offset": 7497.92,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "slightly lower than this number so",
      "offset": 7499.52,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "hopefully what we're getting now is a",
      "offset": 7501.44,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "tiny bit lower than 4.84",
      "offset": 7503.44,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "4.36",
      "offset": 7506.56,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "okay and remember the way we've arranged",
      "offset": 7508.159,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "this is that low loss means that our",
      "offset": 7510.32,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "predictions are matching the targets so",
      "offset": 7512.96,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "our predictions now are probably",
      "offset": 7515.119,
      "duration": 3.721
    },
    {
      "lang": "en",
      "text": "slightly closer to the",
      "offset": 7516.96,
      "duration": 5.279
    },
    {
      "lang": "en",
      "text": "targets and now all we have to do is we",
      "offset": 7518.84,
      "duration": 5.239
    },
    {
      "lang": "en",
      "text": "have to iterate this process",
      "offset": 7522.239,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "so again um we've done the forward pass",
      "offset": 7524.079,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "and this is the loss",
      "offset": 7526.4,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "now we can lost that backward",
      "offset": 7528,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "let me take these out and we can do a",
      "offset": 7530.239,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "step size",
      "offset": 7532.239,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "and now we should have a slightly lower",
      "offset": 7534.4,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "loss 4.36 goes to 3.9",
      "offset": 7535.92,
      "duration": 5.759
    },
    {
      "lang": "en",
      "text": "and okay so",
      "offset": 7539.76,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "we've done the forward pass here's the",
      "offset": 7541.679,
      "duration": 2.641
    },
    {
      "lang": "en",
      "text": "backward pass",
      "offset": 7543.04,
      "duration": 2.639
    },
    {
      "lang": "en",
      "text": "nudge",
      "offset": 7544.32,
      "duration": 5.399
    },
    {
      "lang": "en",
      "text": "and now the loss is 3.66",
      "offset": 7545.679,
      "duration": 4.04
    },
    {
      "lang": "en",
      "text": "3.47",
      "offset": 7550.159,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "and you get the idea we just continue",
      "offset": 7552.4,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "doing this and this is uh gradient",
      "offset": 7554.239,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "descent we're just iteratively doing",
      "offset": 7556.32,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "forward pass backward pass update",
      "offset": 7558.239,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "forward pass backward pass update and",
      "offset": 7561.199,
      "duration": 3.121
    },
    {
      "lang": "en",
      "text": "the neural net is improving its",
      "offset": 7562.88,
      "duration": 2.799
    },
    {
      "lang": "en",
      "text": "predictions",
      "offset": 7564.32,
      "duration": 5.279
    },
    {
      "lang": "en",
      "text": "so here if we look at why pred now",
      "offset": 7565.679,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "like red",
      "offset": 7569.599,
      "duration": 2.56
    },
    {
      "lang": "en",
      "text": "we see that um",
      "offset": 7572.719,
      "duration": 3.281
    },
    {
      "lang": "en",
      "text": "this value should be getting closer to",
      "offset": 7574.56,
      "duration": 2.24
    },
    {
      "lang": "en",
      "text": "one",
      "offset": 7576,
      "duration": 1.92
    },
    {
      "lang": "en",
      "text": "so this value should be getting more",
      "offset": 7576.8,
      "duration": 2.399
    },
    {
      "lang": "en",
      "text": "positive these should be getting more",
      "offset": 7577.92,
      "duration": 2.64
    },
    {
      "lang": "en",
      "text": "negative and this one should be also",
      "offset": 7579.199,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "getting more positive so if we just",
      "offset": 7580.56,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "iterate this",
      "offset": 7582.4,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "a few more times",
      "offset": 7583.92,
      "duration": 2.719
    },
    {
      "lang": "en",
      "text": "actually we may be able to afford go to",
      "offset": 7586.719,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "go a bit faster let's try a slightly",
      "offset": 7588.239,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "higher learning rate",
      "offset": 7590.239,
      "duration": 2.801
    },
    {
      "lang": "en",
      "text": "oops okay there we go so now we're at",
      "offset": 7594.239,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "0.31",
      "offset": 7595.92,
      "duration": 2.64
    },
    {
      "lang": "en",
      "text": "if you go too fast by the way if you try",
      "offset": 7599.199,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "to make it too big of a step you may",
      "offset": 7601.28,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "actually overstep",
      "offset": 7603.28,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "it's overconfidence because again",
      "offset": 7607.04,
      "duration": 3.039
    },
    {
      "lang": "en",
      "text": "remember we don't actually know exactly",
      "offset": 7608.639,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "about the loss function the loss",
      "offset": 7610.079,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "function has all kinds of structure and",
      "offset": 7611.599,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "we only know about the very local",
      "offset": 7613.679,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "dependence of all these parameters on",
      "offset": 7615.679,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "the loss but if we step too far",
      "offset": 7617.199,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "we may step into you know a part of the",
      "offset": 7619.36,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "loss that is completely different",
      "offset": 7621.52,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "and that can destabilize training and",
      "offset": 7623.44,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "make your loss actually blow up even",
      "offset": 7624.8,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "so the loss is now 0.04 so actually the",
      "offset": 7628.159,
      "duration": 5.281
    },
    {
      "lang": "en",
      "text": "predictions should be really quite close",
      "offset": 7631.04,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "let's take a look",
      "offset": 7633.44,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "so you see how this is almost one",
      "offset": 7635.119,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "almost negative one almost one we can",
      "offset": 7637.119,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "continue going",
      "offset": 7639.76,
      "duration": 2.64
    },
    {
      "lang": "en",
      "text": "uh so",
      "offset": 7641.28,
      "duration": 2.799
    },
    {
      "lang": "en",
      "text": "yep backward",
      "offset": 7642.4,
      "duration": 3.279
    },
    {
      "lang": "en",
      "text": "update",
      "offset": 7644.079,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "oops there we go so we went way too fast",
      "offset": 7645.679,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "and um",
      "offset": 7648.159,
      "duration": 3.281
    },
    {
      "lang": "en",
      "text": "we actually overstepped",
      "offset": 7649.76,
      "duration": 4.959
    },
    {
      "lang": "en",
      "text": "so we got two uh too eager where are we",
      "offset": 7651.44,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "now oops",
      "offset": 7654.719,
      "duration": 2.48
    },
    {
      "lang": "en",
      "text": "okay",
      "offset": 7656.32,
      "duration": 2.879
    },
    {
      "lang": "en",
      "text": "seven e negative nine so this is very",
      "offset": 7657.199,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "very low loss",
      "offset": 7659.199,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "and the predictions",
      "offset": 7661.52,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "are basically perfect",
      "offset": 7663.119,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "so somehow we",
      "offset": 7665.52,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "basically we were doing way too big",
      "offset": 7667.44,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "updates and we briefly exploded but then",
      "offset": 7668.88,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "somehow we ended up getting into a",
      "offset": 7670.56,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "really good spot so usually this",
      "offset": 7671.92,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "learning rate and the tuning of it is a",
      "offset": 7674.8,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "subtle art you want to set your learning",
      "offset": 7676.88,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "rate if it's too low you're going to",
      "offset": 7678.639,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "take way too long to converge but if",
      "offset": 7680.56,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "it's too high the whole thing gets",
      "offset": 7682.4,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "unstable and you might actually even",
      "offset": 7683.84,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "explode the loss",
      "offset": 7685.599,
      "duration": 2.881
    },
    {
      "lang": "en",
      "text": "depending on your loss function",
      "offset": 7687.04,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "so finding the step size to be just",
      "offset": 7688.48,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "right it's it's a pretty subtle art",
      "offset": 7690.8,
      "duration": 3.279
    },
    {
      "lang": "en",
      "text": "sometimes when you're using sort of",
      "offset": 7692.639,
      "duration": 3.281
    },
    {
      "lang": "en",
      "text": "vanilla gradient descent",
      "offset": 7694.079,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "but we happen to get into a good spot we",
      "offset": 7695.92,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "can look at",
      "offset": 7697.84,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "n-dot parameters",
      "offset": 7699.52,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "so this is the setting of weights and",
      "offset": 7702.639,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "biases",
      "offset": 7705.44,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "that makes our network",
      "offset": 7706.56,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "predict",
      "offset": 7709.119,
      "duration": 2.721
    },
    {
      "lang": "en",
      "text": "the desired targets",
      "offset": 7710.159,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "very very close",
      "offset": 7711.84,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "and",
      "offset": 7713.76,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "basically we've successfully trained",
      "offset": 7715.599,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "neural net",
      "offset": 7717.119,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "okay let's make this a tiny bit more",
      "offset": 7718.8,
      "duration": 2.799
    },
    {
      "lang": "en",
      "text": "respectable and implement an actual",
      "offset": 7720.159,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "training loop and what that looks like",
      "offset": 7721.599,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "so this is the data definition that",
      "offset": 7723.76,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "stays this is the forward pass",
      "offset": 7725.199,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "um so",
      "offset": 7727.92,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "for uh k in range you know we're going",
      "offset": 7729.44,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "to",
      "offset": 7732.48,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "take a bunch of steps",
      "offset": 7733.92,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "first you do the forward pass",
      "offset": 7737.599,
      "duration": 5.441
    },
    {
      "lang": "en",
      "text": "we validate the loss",
      "offset": 7740.079,
      "duration": 2.961
    },
    {
      "lang": "en",
      "text": "let's re-initialize the neural net from",
      "offset": 7743.679,
      "duration": 2.56
    },
    {
      "lang": "en",
      "text": "scratch",
      "offset": 7745.04,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "and here's the data",
      "offset": 7746.239,
      "duration": 5.041
    },
    {
      "lang": "en",
      "text": "and we first do before pass then we do",
      "offset": 7748.56,
      "duration": 6.2
    },
    {
      "lang": "en",
      "text": "the backward pass",
      "offset": 7751.28,
      "duration": 3.48
    },
    {
      "lang": "en",
      "text": "and then we do an update that's gradient",
      "offset": 7759.599,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "descent",
      "offset": 7761.36,
      "duration": 2.319
    },
    {
      "lang": "en",
      "text": "and then we should be able to iterate",
      "offset": 7766.32,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "this and we should be able to print the",
      "offset": 7767.52,
      "duration": 2.719
    },
    {
      "lang": "en",
      "text": "current step",
      "offset": 7769.04,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "the current loss um let's just print the",
      "offset": 7770.239,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "sort of",
      "offset": 7773.84,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "number of the loss",
      "offset": 7774.96,
      "duration": 3.279
    },
    {
      "lang": "en",
      "text": "and",
      "offset": 7776.88,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "that should be it",
      "offset": 7778.239,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "and then the learning rate 0.01 is a",
      "offset": 7780.56,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "little too small 0.1 we saw is like a",
      "offset": 7782.48,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "little bit dangerously too high let's go",
      "offset": 7784.4,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "somewhere in between",
      "offset": 7786.239,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "and we'll optimize this for",
      "offset": 7787.92,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "not 10 steps but let's go for say 20",
      "offset": 7790.079,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "steps",
      "offset": 7792.239,
      "duration": 6.4
    },
    {
      "lang": "en",
      "text": "let me erase all of this junk",
      "offset": 7794.56,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "and uh let's run the optimization",
      "offset": 7799.119,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "and you see how we've actually converged",
      "offset": 7803.119,
      "duration": 5.281
    },
    {
      "lang": "en",
      "text": "slower in a more controlled manner and",
      "offset": 7805.28,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "got to a loss that is very low",
      "offset": 7808.4,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "so",
      "offset": 7811.44,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "i expect white bread to be quite good",
      "offset": 7812.48,
      "duration": 5.759
    },
    {
      "lang": "en",
      "text": "there we go",
      "offset": 7815.76,
      "duration": 2.479
    },
    {
      "lang": "en",
      "text": "um",
      "offset": 7819.679,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "and",
      "offset": 7822,
      "duration": 2.32
    },
    {
      "lang": "en",
      "text": "that's it",
      "offset": 7823.119,
      "duration": 2.721
    },
    {
      "lang": "en",
      "text": "okay so this is kind of embarrassing but",
      "offset": 7824.32,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "we actually have a really terrible bug",
      "offset": 7825.84,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "in here and it's a subtle bug and it's a",
      "offset": 7828.239,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "very common bug and i can't believe i've",
      "offset": 7831.04,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "done it for the 20th time in my life",
      "offset": 7833.199,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "especially on camera and i could have",
      "offset": 7836.32,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "reshot the whole thing but i think it's",
      "offset": 7838.159,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "pretty funny and you know you get to",
      "offset": 7839.76,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "appreciate a bit what um working with",
      "offset": 7841.84,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "neural nets maybe",
      "offset": 7844.48,
      "duration": 3.119
    },
    {
      "lang": "en",
      "text": "is like sometimes",
      "offset": 7845.76,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "we are guilty of",
      "offset": 7847.599,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "come bug i've actually tweeted",
      "offset": 7850.239,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "the most common neural net mistakes a",
      "offset": 7852.8,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "long time ago now",
      "offset": 7854.639,
      "duration": 3.281
    },
    {
      "lang": "en",
      "text": "uh and",
      "offset": 7856.639,
      "duration": 2.881
    },
    {
      "lang": "en",
      "text": "i'm not really",
      "offset": 7857.92,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "gonna explain any of these except for we",
      "offset": 7859.52,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "are guilty of number three you forgot to",
      "offset": 7861.679,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "zero grad",
      "offset": 7863.76,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "before that backward what is that",
      "offset": 7864.88,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "basically what's happening and it's a",
      "offset": 7869.44,
      "duration": 2.56
    },
    {
      "lang": "en",
      "text": "subtle bug and i'm not sure if you saw",
      "offset": 7870.719,
      "duration": 2.081
    },
    {
      "lang": "en",
      "text": "it",
      "offset": 7872,
      "duration": 2.4
    },
    {
      "lang": "en",
      "text": "is that",
      "offset": 7872.8,
      "duration": 2.879
    },
    {
      "lang": "en",
      "text": "all of these",
      "offset": 7874.4,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "weights here have a dot data and a dot",
      "offset": 7875.679,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "grad",
      "offset": 7877.76,
      "duration": 4.959
    },
    {
      "lang": "en",
      "text": "and that grad starts at zero",
      "offset": 7879.28,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "and then we do backward and we fill in",
      "offset": 7882.719,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "the gradients",
      "offset": 7884.32,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "and then we do an update on the data but",
      "offset": 7885.92,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "we don't flush the grad",
      "offset": 7887.76,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "it stays there",
      "offset": 7889.76,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "so when we do the second",
      "offset": 7891.599,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "forward pass and we do backward again",
      "offset": 7893.44,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "remember that all the backward",
      "offset": 7895.44,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "operations do a plus equals on the grad",
      "offset": 7896.88,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "and so these gradients just",
      "offset": 7899.44,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "add up and they never get reset to zero",
      "offset": 7901.28,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "so basically we didn't zero grad so",
      "offset": 7904.88,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "here's how we zero grad before",
      "offset": 7907.52,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "backward",
      "offset": 7910.079,
      "duration": 2.08
    },
    {
      "lang": "en",
      "text": "we need to iterate over all the",
      "offset": 7911.119,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "parameters",
      "offset": 7912.159,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "and we need to make sure that p dot grad",
      "offset": 7914.159,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "is set to zero",
      "offset": 7916.4,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "we need to reset it to zero just like it",
      "offset": 7918.639,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "is in the constructor",
      "offset": 7920.88,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "so remember all the way here for all",
      "offset": 7922.719,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "these value nodes grad is reset to zero",
      "offset": 7924.4,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "and then all these backward passes do a",
      "offset": 7927.36,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "plus equals from that grad",
      "offset": 7929.28,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "but we need to make sure that",
      "offset": 7931.28,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "we reset these graphs to zero so that",
      "offset": 7933.36,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "when we do backward",
      "offset": 7935.92,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "all of them start at zero and the actual",
      "offset": 7937.36,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "backward pass accumulates um",
      "offset": 7938.88,
      "duration": 6.719
    },
    {
      "lang": "en",
      "text": "the loss derivatives into the grads",
      "offset": 7941.92,
      "duration": 6.96
    },
    {
      "lang": "en",
      "text": "so this is zero grad in pytorch",
      "offset": 7945.599,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "and uh",
      "offset": 7948.88,
      "duration": 2.719
    },
    {
      "lang": "en",
      "text": "we will slightly get we'll get a",
      "offset": 7950.159,
      "duration": 3.121
    },
    {
      "lang": "en",
      "text": "slightly different optimization let's",
      "offset": 7951.599,
      "duration": 3.281
    },
    {
      "lang": "en",
      "text": "reset the neural net",
      "offset": 7953.28,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "the data is the same this is now i think",
      "offset": 7954.88,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "correct",
      "offset": 7957.28,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "and we get a much more",
      "offset": 7958.48,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "you know we get a much more",
      "offset": 7960,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "slower descent",
      "offset": 7962.4,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "we still end up with pretty good results",
      "offset": 7964.079,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "and we can continue this a bit more",
      "offset": 7966.239,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "to get down lower",
      "offset": 7968.56,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "and lower",
      "offset": 7970.239,
      "duration": 3.521
    },
    {
      "lang": "en",
      "text": "and lower",
      "offset": 7971.36,
      "duration": 2.4
    },
    {
      "lang": "en",
      "text": "yeah",
      "offset": 7974.159,
      "duration": 3.281
    },
    {
      "lang": "en",
      "text": "so the only reason that the previous",
      "offset": 7976.239,
      "duration": 3.521
    },
    {
      "lang": "en",
      "text": "thing worked it's extremely buggy um the",
      "offset": 7977.44,
      "duration": 5.759
    },
    {
      "lang": "en",
      "text": "only reason that worked is that",
      "offset": 7979.76,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "this is a very very simple problem",
      "offset": 7983.199,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "and it's very easy for this neural net",
      "offset": 7985.76,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "to fit this data",
      "offset": 7987.76,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "and so the grads ended up accumulating",
      "offset": 7989.36,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "and it effectively gave us a massive",
      "offset": 7992.079,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "step size and it made us converge",
      "offset": 7993.76,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "extremely fast",
      "offset": 7996.079,
      "duration": 2.881
    },
    {
      "lang": "en",
      "text": "but basically now we have to do more",
      "offset": 7999.36,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "steps to get to very low values of loss",
      "offset": 8000.88,
      "duration": 5.839
    },
    {
      "lang": "en",
      "text": "and get wipe red to be really good we",
      "offset": 8004.32,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "can try to",
      "offset": 8006.719,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "step a bit greater",
      "offset": 8007.92,
      "duration": 3.799
    },
    {
      "lang": "en",
      "text": "yeah we're gonna get closer and closer",
      "offset": 8014.32,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "to one minus one and one",
      "offset": 8016.079,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "so",
      "offset": 8018.48,
      "duration": 3.119
    },
    {
      "lang": "en",
      "text": "working with neural nets is sometimes",
      "offset": 8019.84,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "tricky because",
      "offset": 8021.599,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "uh",
      "offset": 8023.28,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "you may have lots of bugs in the code",
      "offset": 8024.96,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "and uh your network might actually work",
      "offset": 8027.04,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "just like ours worked",
      "offset": 8029.84,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "but chances are is that if we had a more",
      "offset": 8031.28,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "complex problem then actually this bug",
      "offset": 8033.28,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "would have made us not optimize the loss",
      "offset": 8035.679,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "very well and we were only able to get",
      "offset": 8037.52,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "away with it because",
      "offset": 8039.119,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "the problem is very simple",
      "offset": 8041.599,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "so let's now bring everything together",
      "offset": 8043.36,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "and summarize what we learned",
      "offset": 8044.96,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "what are neural nets neural nets are",
      "offset": 8046.88,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "these mathematical expressions",
      "offset": 8049.119,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "fairly simple mathematical expressions",
      "offset": 8051.44,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "in the case of multi-layer perceptron",
      "offset": 8053.04,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "that take",
      "offset": 8055.52,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "input as the data and they take input",
      "offset": 8056.56,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "the weights and the parameters of the",
      "offset": 8059.28,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "neural net mathematical expression for",
      "offset": 8060.719,
      "duration": 4.121
    },
    {
      "lang": "en",
      "text": "the forward pass followed by a loss",
      "offset": 8062.8,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "function and the loss function tries to",
      "offset": 8064.84,
      "duration": 4.52
    },
    {
      "lang": "en",
      "text": "measure the accuracy of the predictions",
      "offset": 8066.96,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "and usually the loss will be low when",
      "offset": 8069.36,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "your predictions are matching your",
      "offset": 8071.679,
      "duration": 2.801
    },
    {
      "lang": "en",
      "text": "targets or where the network is",
      "offset": 8072.96,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "basically behaving well so we we",
      "offset": 8074.48,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "manipulate the loss function so that",
      "offset": 8077.28,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "when the loss is low the network is",
      "offset": 8078.8,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "doing what you want it to do on your",
      "offset": 8080.719,
      "duration": 3.281
    },
    {
      "lang": "en",
      "text": "problem",
      "offset": 8082.4,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "and then we backward the loss",
      "offset": 8084,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "use backpropagation to get the gradient",
      "offset": 8086.48,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "and then we know how to tune all the",
      "offset": 8088.8,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "parameters to decrease the loss locally",
      "offset": 8090,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "but then we have to iterate that process",
      "offset": 8092.8,
      "duration": 2.879
    },
    {
      "lang": "en",
      "text": "many times in what's called the gradient",
      "offset": 8094.239,
      "duration": 2.48
    },
    {
      "lang": "en",
      "text": "descent",
      "offset": 8095.679,
      "duration": 2.641
    },
    {
      "lang": "en",
      "text": "so we simply follow the gradient",
      "offset": 8096.719,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "information and that minimizes the loss",
      "offset": 8098.32,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "and the loss is arranged so that when",
      "offset": 8101.04,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "the loss is minimized the network is",
      "offset": 8102.32,
      "duration": 4.52
    },
    {
      "lang": "en",
      "text": "doing what you want it to do",
      "offset": 8104.32,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "and yeah so we just have a blob of",
      "offset": 8106.84,
      "duration": 4.52
    },
    {
      "lang": "en",
      "text": "neural stuff and we can make it do",
      "offset": 8109.199,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "arbitrary things and that's what gives",
      "offset": 8111.36,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "neural nets their power um",
      "offset": 8113.28,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "it's you know this is a very tiny",
      "offset": 8115.199,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "network with 41 parameters",
      "offset": 8116.719,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "but you can build significantly more",
      "offset": 8119.119,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "complicated neural nets with billions",
      "offset": 8120.8,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "at this point almost trillions of",
      "offset": 8124.079,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "parameters and it's a massive blob of",
      "offset": 8125.36,
      "duration": 5.759
    },
    {
      "lang": "en",
      "text": "neural tissue simulated neural tissue",
      "offset": 8128.079,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "roughly speaking",
      "offset": 8131.119,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "and you can make it do extremely complex",
      "offset": 8132.56,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "problems and these neurons then have all",
      "offset": 8134.96,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "kinds of very fascinating emergent",
      "offset": 8137.36,
      "duration": 2.799
    },
    {
      "lang": "en",
      "text": "properties",
      "offset": 8139.04,
      "duration": 2.48
    },
    {
      "lang": "en",
      "text": "in",
      "offset": 8140.159,
      "duration": 3.281
    },
    {
      "lang": "en",
      "text": "when you try to make them do",
      "offset": 8141.52,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "significantly hard problems as in the",
      "offset": 8143.44,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "case of gpt for example",
      "offset": 8145.679,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "we have massive amounts of text from the",
      "offset": 8147.679,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "internet and we're trying to get a",
      "offset": 8149.84,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "neural net to predict to take like a few",
      "offset": 8151.28,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "words and try to predict the next word",
      "offset": 8153.44,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "in a sequence that's the learning",
      "offset": 8155.199,
      "duration": 2.4
    },
    {
      "lang": "en",
      "text": "problem",
      "offset": 8156.639,
      "duration": 2.08
    },
    {
      "lang": "en",
      "text": "and it turns out that when you train",
      "offset": 8157.599,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "this on all of internet the neural net",
      "offset": 8158.719,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "actually has like really remarkable",
      "offset": 8160.8,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "emergent properties but that neural net",
      "offset": 8162.48,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "would have hundreds of billions of",
      "offset": 8164.48,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "parameters",
      "offset": 8165.76,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "but it works on fundamentally the exact",
      "offset": 8167.44,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "same principles",
      "offset": 8169.36,
      "duration": 2.799
    },
    {
      "lang": "en",
      "text": "the neural net of course will be a bit",
      "offset": 8170.56,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "more complex but otherwise the",
      "offset": 8172.159,
      "duration": 5.361
    },
    {
      "lang": "en",
      "text": "value in the gradient is there",
      "offset": 8175.119,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "and would be identical and the gradient",
      "offset": 8177.52,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "descent would be there and would be",
      "offset": 8179.44,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "basically identical but people usually",
      "offset": 8181.04,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "use slightly different updates this is a",
      "offset": 8183.119,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "very simple stochastic gradient descent",
      "offset": 8185.119,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "update",
      "offset": 8187.199,
      "duration": 2.241
    },
    {
      "lang": "en",
      "text": "um",
      "offset": 8188.48,
      "duration": 2.159
    },
    {
      "lang": "en",
      "text": "and the loss function would not be mean",
      "offset": 8189.44,
      "duration": 2.56
    },
    {
      "lang": "en",
      "text": "squared error they would be using",
      "offset": 8190.639,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "something called the cross-entropy loss",
      "offset": 8192,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "for predicting the next token so there's",
      "offset": 8194,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "a few more details but fundamentally the",
      "offset": 8196,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "neural network setup and neural network",
      "offset": 8197.679,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "training is identical and pervasive and",
      "offset": 8199.04,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "now you understand intuitively",
      "offset": 8202.16,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "how that works under the hood in the",
      "offset": 8204.24,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "beginning of this video i told you that",
      "offset": 8206,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "by the end of it you would understand",
      "offset": 8207.439,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "everything in micrograd and then we'd",
      "offset": 8208.8,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "slowly build it up let me briefly prove",
      "offset": 8210.719,
      "duration": 3.281
    },
    {
      "lang": "en",
      "text": "that to you",
      "offset": 8212.639,
      "duration": 2.481
    },
    {
      "lang": "en",
      "text": "so i'm going to step through all the",
      "offset": 8214,
      "duration": 3.519
    },
    {
      "lang": "en",
      "text": "code that is in micrograd as of today",
      "offset": 8215.12,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "actually potentially some of the code",
      "offset": 8217.519,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "will change by the time you watch this",
      "offset": 8219.04,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "video because i intend to continue",
      "offset": 8220.319,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "developing micrograd",
      "offset": 8221.76,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "but let's look at what we have so far at",
      "offset": 8223.439,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "least init.pi is empty when you go to",
      "offset": 8225.04,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "engine.pi that has the value",
      "offset": 8227.76,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "everything here you should mostly",
      "offset": 8230.08,
      "duration": 3.519
    },
    {
      "lang": "en",
      "text": "recognize so we have the data.grad",
      "offset": 8231.2,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "attributes we have the backward function",
      "offset": 8233.599,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "uh we have the previous set of children",
      "offset": 8235.92,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "and the operation that produced this",
      "offset": 8237.439,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "value",
      "offset": 8239.599,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "we have addition multiplication and",
      "offset": 8240.8,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "raising to a scalar power",
      "offset": 8242.479,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "we have the relu non-linearity which is",
      "offset": 8245.2,
      "duration": 3.279
    },
    {
      "lang": "en",
      "text": "slightly different type of nonlinearity",
      "offset": 8247.04,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "than 10h that we used in this video",
      "offset": 8248.479,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "both of them are non-linearities and",
      "offset": 8250.88,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "notably 10h is not actually present in",
      "offset": 8252.719,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "micrograd as of right now but i intend",
      "offset": 8254.639,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "to add it later",
      "offset": 8257.04,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "with the backward which is identical and",
      "offset": 8258.639,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "then all of these other operations which",
      "offset": 8260.719,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "are built up on top of operations here",
      "offset": 8262.88,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "so values should be very recognizable",
      "offset": 8265.439,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "except for the non-linearity used in",
      "offset": 8267.2,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "this video",
      "offset": 8268.559,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "um there's no massive difference between",
      "offset": 8270.08,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "relu and 10h and sigmoid and these other",
      "offset": 8272.16,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "non-linearities they're all roughly",
      "offset": 8274.16,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "equivalent and can be used in mlps so i",
      "offset": 8275.92,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "use 10h because it's a bit smoother and",
      "offset": 8278.319,
      "duration": 2.961
    },
    {
      "lang": "en",
      "text": "because it's a little bit more",
      "offset": 8280.399,
      "duration": 2.881
    },
    {
      "lang": "en",
      "text": "complicated than relu and therefore it's",
      "offset": 8281.28,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "stressed a little bit more the",
      "offset": 8283.28,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "local gradients and working with those",
      "offset": 8285.92,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "derivatives which i thought would be",
      "offset": 8287.679,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "useful",
      "offset": 8289.04,
      "duration": 3.279
    },
    {
      "lang": "en",
      "text": "and then that pi is the neural networks",
      "offset": 8290.719,
      "duration": 3.521
    },
    {
      "lang": "en",
      "text": "library as i mentioned so you should",
      "offset": 8292.319,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "recognize identical implementation of",
      "offset": 8294.24,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "neuron layer and mlp",
      "offset": 8296.16,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "notably or not so much",
      "offset": 8298.24,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "we have a class module here there is a",
      "offset": 8300.639,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "parent class of all these modules i did",
      "offset": 8302.16,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "that because there's an nn.module class",
      "offset": 8304.479,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "in pytorch and so this exactly matches",
      "offset": 8307.12,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "that api and end.module and pytorch has",
      "offset": 8309.12,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "also a zero grad which i've refactored",
      "offset": 8311.679,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "out here",
      "offset": 8313.679,
      "duration": 2.561
    },
    {
      "lang": "en",
      "text": "so that's the end of micrograd really",
      "offset": 8316.319,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "then there's a test",
      "offset": 8318.479,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "which you'll see",
      "offset": 8320,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "basically creates",
      "offset": 8321.439,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "two chunks of code one in micrograd and",
      "offset": 8322.96,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "one in pi torch and we'll make sure that",
      "offset": 8325.599,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "the forward and the backward pass agree",
      "offset": 8327.599,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "identically",
      "offset": 8329.28,
      "duration": 2.64
    },
    {
      "lang": "en",
      "text": "for a slightly less complicated",
      "offset": 8330.399,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "expression a slightly more complicated",
      "offset": 8331.92,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "expression everything",
      "offset": 8333.359,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "agrees so we agree with pytorch on all",
      "offset": 8335.359,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "of these operations",
      "offset": 8337.359,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "and finally there's a demo.ipymb here",
      "offset": 8338.96,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "and it's a bit more complicated binary",
      "offset": 8341.679,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "classification demo than the one i",
      "offset": 8343.2,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "covered in this lecture so we only had a",
      "offset": 8344.559,
      "duration": 5.361
    },
    {
      "lang": "en",
      "text": "tiny data set of four examples um here",
      "offset": 8347.04,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "we have a bit more complicated example",
      "offset": 8349.92,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "with lots of blue points and lots of red",
      "offset": 8351.679,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "points and we're trying to again build a",
      "offset": 8353.679,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "binary classifier to distinguish uh two",
      "offset": 8355.599,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "dimensional points as red or blue",
      "offset": 8358,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "it's a bit more complicated mlp here",
      "offset": 8360.479,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "with it's a bigger mlp",
      "offset": 8362.319,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "the loss is a bit more complicated",
      "offset": 8364.719,
      "duration": 2.401
    },
    {
      "lang": "en",
      "text": "because",
      "offset": 8366.16,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "it supports batches",
      "offset": 8367.12,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "so because our dataset was so tiny we",
      "offset": 8369.2,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "always did a forward pass on the entire",
      "offset": 8371.2,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "data set of four examples but when your",
      "offset": 8372.8,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "data set is like a million examples what",
      "offset": 8375.2,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "we usually do in practice is we chair we",
      "offset": 8377.359,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "basically pick out some random subset we",
      "offset": 8379.84,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "call that a batch and then we only",
      "offset": 8381.84,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "process the batch forward backward and",
      "offset": 8383.76,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "update so we don't have to forward the",
      "offset": 8385.92,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "entire training set",
      "offset": 8387.599,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "so this supports batching because",
      "offset": 8389.84,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "there's a lot more examples here",
      "offset": 8391.2,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "we do a forward pass the loss is",
      "offset": 8393.52,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "slightly more different this is a max",
      "offset": 8395.68,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "margin loss that i implement here",
      "offset": 8397.6,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "the one that we used was the mean",
      "offset": 8400,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "squared error loss because it's the",
      "offset": 8401.76,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "simplest one",
      "offset": 8403.2,
      "duration": 3.119
    },
    {
      "lang": "en",
      "text": "there's also the binary cross entropy",
      "offset": 8404.64,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "loss all of them can be used for binary",
      "offset": 8406.319,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "classification and don't make too much",
      "offset": 8408.08,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "of a difference in the simple examples",
      "offset": 8410.16,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "that we looked at so far",
      "offset": 8411.6,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "there's something called l2",
      "offset": 8413.52,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "regularization used here this has to do",
      "offset": 8414.8,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "with generalization of the neural net",
      "offset": 8417.439,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "and controls the overfitting in machine",
      "offset": 8419.359,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "learning setting but i did not cover",
      "offset": 8421.84,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "these concepts and concepts in this",
      "offset": 8423.359,
      "duration": 3.521
    },
    {
      "lang": "en",
      "text": "video potentially later",
      "offset": 8424.88,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "and the training loop you should",
      "offset": 8426.88,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "recognize so forward backward with zero",
      "offset": 8427.92,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "grad",
      "offset": 8431.04,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "and update and so on you'll notice that",
      "offset": 8432.319,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "in the update here the learning rate is",
      "offset": 8435.2,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "scaled as a function of number of",
      "offset": 8436.88,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "iterations and it",
      "offset": 8438.64,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "shrinks",
      "offset": 8440.56,
      "duration": 2.48
    },
    {
      "lang": "en",
      "text": "and this is something called learning",
      "offset": 8441.76,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "rate decay so in the beginning you have",
      "offset": 8443.04,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "a high learning rate and as the network",
      "offset": 8444.8,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "sort of stabilizes near the end you",
      "offset": 8447.04,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "bring down the learning rate to get some",
      "offset": 8449.359,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "of the fine details in the end",
      "offset": 8450.72,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "and in the end we see the decision",
      "offset": 8453.12,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "surface of the neural net and we see",
      "offset": 8454.88,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "that it learns to separate out the red",
      "offset": 8456.64,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "and the blue area based on the data",
      "offset": 8458.56,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "points",
      "offset": 8460.56,
      "duration": 2.799
    },
    {
      "lang": "en",
      "text": "so that's the slightly more complicated",
      "offset": 8461.92,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "example and then we'll demo that hyper",
      "offset": 8463.359,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "ymb that you're free to go over",
      "offset": 8465.359,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "but yeah as of today that is micrograd i",
      "offset": 8467.52,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "also wanted to show you a little bit of",
      "offset": 8470.24,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "real stuff so that you get to see how",
      "offset": 8471.439,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "this is actually implemented in",
      "offset": 8473.2,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "production grade library like by torch",
      "offset": 8474.399,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "uh so in particular i wanted to show i",
      "offset": 8476.64,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "wanted to find and show you the backward",
      "offset": 8478.479,
      "duration": 5.121
    },
    {
      "lang": "en",
      "text": "pass for 10h in pytorch so here in",
      "offset": 8480.64,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "micrograd we see that the backward",
      "offset": 8483.6,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "password 10h is one minus t square",
      "offset": 8485.2,
      "duration": 7.84
    },
    {
      "lang": "en",
      "text": "where t is the output of the tanh of x",
      "offset": 8488.479,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "times of that grad which is the chain",
      "offset": 8493.2,
      "duration": 3.279
    },
    {
      "lang": "en",
      "text": "rule so we're looking for something that",
      "offset": 8494.88,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "looks like this",
      "offset": 8496.479,
      "duration": 2.641
    },
    {
      "lang": "en",
      "text": "now",
      "offset": 8498.08,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "i went to pytorch um which has an open",
      "offset": 8499.12,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "source github codebase and uh i looked",
      "offset": 8502.56,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "through a lot of its code",
      "offset": 8505.12,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "and honestly i i i spent about 15",
      "offset": 8507.04,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "minutes and i couldn't find 10h",
      "offset": 8509.52,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "and that's because these libraries",
      "offset": 8511.92,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "unfortunately they grow in size and",
      "offset": 8513.28,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "entropy and if you just search for 10h",
      "offset": 8515.12,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "you get apparently 2 800 results and 400",
      "offset": 8517.92,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "and 406 files so i don't know what these",
      "offset": 8521.04,
      "duration": 6.72
    },
    {
      "lang": "en",
      "text": "files are doing honestly",
      "offset": 8524,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "and why there are so many mentions of",
      "offset": 8527.76,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "10h but unfortunately these libraries",
      "offset": 8529.04,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "are quite complex they're meant to be",
      "offset": 8531.2,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "used not really inspected um",
      "offset": 8532.56,
      "duration": 5.839
    },
    {
      "lang": "en",
      "text": "eventually i did stumble on someone",
      "offset": 8535.6,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "who tries to change the 10 h backward",
      "offset": 8538.399,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "code for some reason",
      "offset": 8541.12,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "and someone here pointed to the cpu",
      "offset": 8542.64,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "kernel and the kuda kernel for 10 inch",
      "offset": 8544.319,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "backward",
      "offset": 8546.319,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "so this so basically depends on if",
      "offset": 8547.76,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "you're using pi torch on a cpu device or",
      "offset": 8549.52,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "on a gpu which these are different",
      "offset": 8551.6,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "devices and i haven't covered this but",
      "offset": 8553.28,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "this is the 10 h backwards kernel",
      "offset": 8555.2,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "for uh cpu",
      "offset": 8557.76,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "and the reason it's so large is that",
      "offset": 8560,
      "duration": 5.439
    },
    {
      "lang": "en",
      "text": "number one this is like if you're using",
      "offset": 8563.92,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "a complex type which we haven't even",
      "offset": 8565.439,
      "duration": 3.281
    },
    {
      "lang": "en",
      "text": "talked about if you're using a specific",
      "offset": 8566.96,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "data type of b-float 16 which we haven't",
      "offset": 8568.72,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "talked about",
      "offset": 8570.72,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "and then if you're not then this is the",
      "offset": 8572.56,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "kernel and deep here we see something",
      "offset": 8574.96,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "that resembles our backward pass so they",
      "offset": 8577.359,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "have a times one minus",
      "offset": 8580.08,
      "duration": 5.279
    },
    {
      "lang": "en",
      "text": "b square uh so this b",
      "offset": 8582.24,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "b here must be the output of the 10h and",
      "offset": 8585.359,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "this is the health.grad so here we found",
      "offset": 8587.6,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "it",
      "offset": 8590.24,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "uh deep inside",
      "offset": 8591.04,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "pi torch from this location for some",
      "offset": 8594.08,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "reason inside binaryops kernel when 10h",
      "offset": 8595.68,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "is not actually a binary op",
      "offset": 8598.56,
      "duration": 6.32
    },
    {
      "lang": "en",
      "text": "and then this is the gpu kernel",
      "offset": 8601.2,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "we're not complex",
      "offset": 8605.04,
      "duration": 2.48
    },
    {
      "lang": "en",
      "text": "we're",
      "offset": 8606.64,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "here and here we go with one line of",
      "offset": 8607.52,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "code",
      "offset": 8609.52,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "so we did find it but basically",
      "offset": 8610.64,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "unfortunately these codepieces are very",
      "offset": 8613.439,
      "duration": 2.801
    },
    {
      "lang": "en",
      "text": "large and",
      "offset": 8614.8,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "micrograd is very very simple but if you",
      "offset": 8616.24,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "actually want to use real stuff uh",
      "offset": 8618.56,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "finding the code for it you'll actually",
      "offset": 8620.399,
      "duration": 3.521
    },
    {
      "lang": "en",
      "text": "find that difficult",
      "offset": 8621.92,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "i also wanted to show you a little",
      "offset": 8623.92,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "example here where pytorch is showing",
      "offset": 8625.84,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "you how can you can register a new type",
      "offset": 8627.76,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "of function that you want to add to",
      "offset": 8629.68,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "pytorch as a lego building block",
      "offset": 8631.12,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "so here if you want to for example add a",
      "offset": 8633.6,
      "duration": 5.759
    },
    {
      "lang": "en",
      "text": "gender polynomial 3",
      "offset": 8635.84,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "here's how you could do it you will",
      "offset": 8639.359,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "register it as a class that",
      "offset": 8640.64,
      "duration": 5.679
    },
    {
      "lang": "en",
      "text": "subclasses storage.org that function",
      "offset": 8643.6,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "and then you have to tell pytorch how to",
      "offset": 8646.319,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "forward your new function",
      "offset": 8647.84,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "and how to backward through it",
      "offset": 8650.8,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "so as long as you can do the forward",
      "offset": 8652.64,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "pass of this little function piece that",
      "offset": 8654.08,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "you want to add and as long as you know",
      "offset": 8655.52,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "the the local derivative the local",
      "offset": 8657.12,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "gradients which are implemented in the",
      "offset": 8659.04,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "backward pi torch will be able to back",
      "offset": 8660.56,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "propagate through your function and then",
      "offset": 8662.72,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "you can use this as a lego block in a",
      "offset": 8664.399,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "larger lego castle of all the different",
      "offset": 8666.399,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "lego blocks that pytorch already has",
      "offset": 8668.399,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "and so that's the only thing you have to",
      "offset": 8671.04,
      "duration": 2.64
    },
    {
      "lang": "en",
      "text": "tell pytorch and everything would just",
      "offset": 8672.319,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "work and you can register new types of",
      "offset": 8673.68,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "functions",
      "offset": 8675.76,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "in this way following this example",
      "offset": 8676.8,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "and that is everything that i wanted to",
      "offset": 8678.96,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "cover in this lecture",
      "offset": 8680.08,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "so i hope you enjoyed building out",
      "offset": 8681.68,
      "duration": 2.719
    },
    {
      "lang": "en",
      "text": "micrograd with me i hope you find it",
      "offset": 8682.8,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "interesting insightful",
      "offset": 8684.399,
      "duration": 3.521
    },
    {
      "lang": "en",
      "text": "and",
      "offset": 8686.399,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "yeah i will post a lot of the links",
      "offset": 8687.92,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "that are related to this video in the",
      "offset": 8690.08,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "video description below i will also",
      "offset": 8691.68,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "probably post a link to a discussion",
      "offset": 8693.92,
      "duration": 2.559
    },
    {
      "lang": "en",
      "text": "forum",
      "offset": 8695.68,
      "duration": 2.48
    },
    {
      "lang": "en",
      "text": "or discussion group where you can ask",
      "offset": 8696.479,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "questions related to this video and then",
      "offset": 8698.16,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "i can answer or someone else can answer",
      "offset": 8700.399,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "your questions and i may also do a",
      "offset": 8702.24,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "follow-up video that answers some of the",
      "offset": 8704.399,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "most common questions",
      "offset": 8706.08,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "but for now that's it i hope you enjoyed",
      "offset": 8708.479,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "it if you did then please like and",
      "offset": 8710.399,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "subscribe so that youtube knows to",
      "offset": 8711.92,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "feature this video to more people",
      "offset": 8713.6,
      "duration": 5.759
    },
    {
      "lang": "en",
      "text": "and that's it for now i'll see you later",
      "offset": 8715.52,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "now here's the problem",
      "offset": 8722.399,
      "duration": 3.281
    },
    {
      "lang": "en",
      "text": "we know",
      "offset": 8724.64,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "dl by",
      "offset": 8725.68,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "wait what is the problem",
      "offset": 8728.56,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "and that's everything i wanted to cover",
      "offset": 8731.92,
      "duration": 2.32
    },
    {
      "lang": "en",
      "text": "in this lecture",
      "offset": 8733.12,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "so i hope",
      "offset": 8734.24,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "you enjoyed us building up microcraft",
      "offset": 8735.84,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "micro crab",
      "offset": 8738.479,
      "duration": 2.641
    },
    {
      "lang": "en",
      "text": "okay now let's do the exact same thing",
      "offset": 8742,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "for multiply because we can't do",
      "offset": 8743.359,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "something like a times two",
      "offset": 8744.8,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "oops",
      "offset": 8747.84,
      "duration": 2.4
    },
    {
      "lang": "en",
      "text": "i know what happened there",
      "offset": 8750.8,
      "duration": 2.88
    }
  ],
  "cleanText": null,
  "dumpedAt": "2025-07-21T18:43:26.197Z"
}