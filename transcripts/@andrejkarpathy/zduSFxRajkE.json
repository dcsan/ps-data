{
  "episodeId": "zduSFxRajkE",
  "channelSlug": "@andrejkarpathy",
  "title": "Let's build the GPT Tokenizer",
  "publishedAt": "2024-02-20T17:11:35.000Z",
  "rawLines": [
    {
      "lang": "en",
      "text": "hi everyone so in this video I'd like us",
      "offset": 0.04,
      "duration": 4.04
    },
    {
      "lang": "en",
      "text": "to cover the process of tokenization in",
      "offset": 2.04,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "large language models now you see here",
      "offset": 4.08,
      "duration": 4.2
    },
    {
      "lang": "en",
      "text": "that I have a set face and that's",
      "offset": 6.44,
      "duration": 3.88
    },
    {
      "lang": "en",
      "text": "because uh tokenization is my least",
      "offset": 8.28,
      "duration": 3.399
    },
    {
      "lang": "en",
      "text": "favorite part of working with large",
      "offset": 10.32,
      "duration": 3.16
    },
    {
      "lang": "en",
      "text": "language models but unfortunately it is",
      "offset": 11.679,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "necessary to understand in some detail",
      "offset": 13.48,
      "duration": 4.12
    },
    {
      "lang": "en",
      "text": "because it it is fairly hairy gnarly and",
      "offset": 15.519,
      "duration": 3.961
    },
    {
      "lang": "en",
      "text": "there's a lot of hidden foot guns to be",
      "offset": 17.6,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "aware of and a lot of oddness with large",
      "offset": 19.48,
      "duration": 5.119
    },
    {
      "lang": "en",
      "text": "language models typically traces back to",
      "offset": 21.84,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "tokenization so what is",
      "offset": 24.599,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "tokenization now in my previous video",
      "offset": 26.64,
      "duration": 4.92
    },
    {
      "lang": "en",
      "text": "Let's Build GPT from scratch uh we",
      "offset": 28.92,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "actually already did tokenization but we",
      "offset": 31.56,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "did a very naive simple version of",
      "offset": 33.48,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "tokenization so when you go to the",
      "offset": 35.8,
      "duration": 4.759
    },
    {
      "lang": "en",
      "text": "Google colab for that video uh you see",
      "offset": 37.48,
      "duration": 5.72
    },
    {
      "lang": "en",
      "text": "here that we loaded our training set and",
      "offset": 40.559,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "our training set was this uh Shakespeare",
      "offset": 43.2,
      "duration": 4.92
    },
    {
      "lang": "en",
      "text": "uh data set now in the beginning the",
      "offset": 45.52,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "Shakespeare data set is just a large",
      "offset": 48.12,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "string in Python it's just text and so",
      "offset": 49.76,
      "duration": 5.08
    },
    {
      "lang": "en",
      "text": "the question is how do we plug text into",
      "offset": 52.44,
      "duration": 5.639
    },
    {
      "lang": "en",
      "text": "large language models and in this case",
      "offset": 54.84,
      "duration": 6.6
    },
    {
      "lang": "en",
      "text": "here we created a vocabulary of 65",
      "offset": 58.079,
      "duration": 5.881
    },
    {
      "lang": "en",
      "text": "possible characters that we saw occur in",
      "offset": 61.44,
      "duration": 4.359
    },
    {
      "lang": "en",
      "text": "this string these were the possible",
      "offset": 63.96,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "characters and we saw that there are 65",
      "offset": 65.799,
      "duration": 4.841
    },
    {
      "lang": "en",
      "text": "of them and then we created a a lookup",
      "offset": 67.96,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "table for converting from every possible",
      "offset": 70.64,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "character a little string piece into a",
      "offset": 73.4,
      "duration": 4.359
    },
    {
      "lang": "en",
      "text": "token an",
      "offset": 76.32,
      "duration": 4.2
    },
    {
      "lang": "en",
      "text": "integer so here for example we tokenized",
      "offset": 77.759,
      "duration": 5.521
    },
    {
      "lang": "en",
      "text": "the string High there and we received",
      "offset": 80.52,
      "duration": 4.2
    },
    {
      "lang": "en",
      "text": "this sequence of",
      "offset": 83.28,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "tokens and here we took the first 1,000",
      "offset": 84.72,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "characters of our data set and we",
      "offset": 87.6,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "encoded it into tokens and because it is",
      "offset": 89.92,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "this is character level we received",
      "offset": 92.56,
      "duration": 6.4
    },
    {
      "lang": "en",
      "text": "1,000 tokens in a sequence so token 18",
      "offset": 94.64,
      "duration": 5.479
    },
    {
      "lang": "en",
      "text": "47",
      "offset": 98.96,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "Etc now later we saw that the way we",
      "offset": 100.119,
      "duration": 5.521
    },
    {
      "lang": "en",
      "text": "plug these tokens into the language",
      "offset": 103.439,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "model is by using an embedding",
      "offset": 105.64,
      "duration": 5.839
    },
    {
      "lang": "en",
      "text": "table and so basically if we have 65",
      "offset": 108.479,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "possible tokens then this embedding",
      "offset": 111.479,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "table is going to have 65 rows and",
      "offset": 113.479,
      "duration": 4.68
    },
    {
      "lang": "en",
      "text": "roughly speaking we're taking the",
      "offset": 116.439,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "integer associated with every single",
      "offset": 118.159,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "sing Le token we're using that as a",
      "offset": 119.799,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "lookup into this table and we're",
      "offset": 121.52,
      "duration": 4.959
    },
    {
      "lang": "en",
      "text": "plucking out the corresponding row and",
      "offset": 124.039,
      "duration": 5.321
    },
    {
      "lang": "en",
      "text": "this row is a uh is trainable parameters",
      "offset": 126.479,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "that we're going to train using back",
      "offset": 129.36,
      "duration": 3.519
    },
    {
      "lang": "en",
      "text": "propagation and this is the vector that",
      "offset": 130.479,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "then feeds into the Transformer um and",
      "offset": 132.879,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "that's how the Transformer Ser of",
      "offset": 135.36,
      "duration": 2.76
    },
    {
      "lang": "en",
      "text": "perceives every single",
      "offset": 136.56,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "token so here we had a very naive",
      "offset": 138.12,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "tokenization process that was a",
      "offset": 141.28,
      "duration": 3.959
    },
    {
      "lang": "en",
      "text": "character level tokenizer but in",
      "offset": 143.12,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "practice in state-ofthe-art uh language",
      "offset": 145.239,
      "duration": 3.72
    },
    {
      "lang": "en",
      "text": "models people use a lot more complicated",
      "offset": 147.28,
      "duration": 3.16
    },
    {
      "lang": "en",
      "text": "schemes unfortunately",
      "offset": 148.959,
      "duration": 5.401
    },
    {
      "lang": "en",
      "text": "uh for constructing these uh token",
      "offset": 150.44,
      "duration": 6.2
    },
    {
      "lang": "en",
      "text": "vocabularies so we're not dealing on the",
      "offset": 154.36,
      "duration": 4.28
    },
    {
      "lang": "en",
      "text": "Character level we're dealing on chunk",
      "offset": 156.64,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "level and the way these um character",
      "offset": 158.64,
      "duration": 5.239
    },
    {
      "lang": "en",
      "text": "chunks are constructed is using",
      "offset": 161.519,
      "duration": 3.961
    },
    {
      "lang": "en",
      "text": "algorithms such as for example the bik",
      "offset": 163.879,
      "duration": 3.08
    },
    {
      "lang": "en",
      "text": "pair in coding algorithm which we're",
      "offset": 165.48,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "going to go into in detail um and cover",
      "offset": 166.959,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "in this video I'd like to briefly show",
      "offset": 171,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "you the paper that introduced a bite",
      "offset": 172.879,
      "duration": 4.041
    },
    {
      "lang": "en",
      "text": "level encoding as a mechanism for",
      "offset": 174.84,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "tokenization in the context of large",
      "offset": 176.92,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "language models and I would say that",
      "offset": 178.44,
      "duration": 4.28
    },
    {
      "lang": "en",
      "text": "that's probably the gpt2 paper and if",
      "offset": 180.599,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "you scroll down here to the section",
      "offset": 182.72,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "input representation this is where they",
      "offset": 185.56,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "cover tokenization the kinds of",
      "offset": 187.72,
      "duration": 2.84
    },
    {
      "lang": "en",
      "text": "properties that you'd like the",
      "offset": 189.48,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "tokenization to have and they conclude",
      "offset": 190.56,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "here that they're going to have a",
      "offset": 193,
      "duration": 4.599
    },
    {
      "lang": "en",
      "text": "tokenizer where you have a vocabulary of",
      "offset": 194.959,
      "duration": 5.721
    },
    {
      "lang": "en",
      "text": "50,2 57 possible",
      "offset": 197.599,
      "duration": 6.801
    },
    {
      "lang": "en",
      "text": "tokens and the context size is going to",
      "offset": 200.68,
      "duration": 6.68
    },
    {
      "lang": "en",
      "text": "be 1,24 tokens so in the in in the",
      "offset": 204.4,
      "duration": 4.839
    },
    {
      "lang": "en",
      "text": "attention layer of the Transformer",
      "offset": 207.36,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "neural network",
      "offset": 209.239,
      "duration": 3.08
    },
    {
      "lang": "en",
      "text": "every single token is attending to the",
      "offset": 210.48,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "previous tokens in the sequence and it's",
      "offset": 212.319,
      "duration": 5.601
    },
    {
      "lang": "en",
      "text": "going to see up to 1,24 tokens so tokens",
      "offset": 214.08,
      "duration": 6.6
    },
    {
      "lang": "en",
      "text": "are this like fundamental unit um the",
      "offset": 217.92,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "atom of uh large language models if you",
      "offset": 220.68,
      "duration": 4.119
    },
    {
      "lang": "en",
      "text": "will and everything is in units of",
      "offset": 223.12,
      "duration": 3.96
    },
    {
      "lang": "en",
      "text": "tokens everything is about tokens and",
      "offset": 224.799,
      "duration": 3.561
    },
    {
      "lang": "en",
      "text": "tokenization is the process for",
      "offset": 227.08,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "translating strings or text into",
      "offset": 228.36,
      "duration": 6.519
    },
    {
      "lang": "en",
      "text": "sequences of tokens and uh vice versa",
      "offset": 231.08,
      "duration": 5.799
    },
    {
      "lang": "en",
      "text": "when you go into the Llama 2 paper as",
      "offset": 234.879,
      "duration": 3.401
    },
    {
      "lang": "en",
      "text": "well I can show you that when you search",
      "offset": 236.879,
      "duration": 4.841
    },
    {
      "lang": "en",
      "text": "token you're going to get get 63 hits um",
      "offset": 238.28,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "and that's because tokens are again",
      "offset": 241.72,
      "duration": 3.4
    },
    {
      "lang": "en",
      "text": "pervasive so here they mentioned that",
      "offset": 243.319,
      "duration": 3.56
    },
    {
      "lang": "en",
      "text": "they trained on two trillion tokens of",
      "offset": 245.12,
      "duration": 3.319
    },
    {
      "lang": "en",
      "text": "data and so",
      "offset": 246.879,
      "duration": 4.2
    },
    {
      "lang": "en",
      "text": "on so we're going to build our own",
      "offset": 248.439,
      "duration": 4.601
    },
    {
      "lang": "en",
      "text": "tokenizer luckily the bite be encoding",
      "offset": 251.079,
      "duration": 4.041
    },
    {
      "lang": "en",
      "text": "algorithm is not uh that super",
      "offset": 253.04,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "complicated and we can build it from",
      "offset": 255.12,
      "duration": 3.399
    },
    {
      "lang": "en",
      "text": "scratch ourselves and we'll see exactly",
      "offset": 256.959,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "how this works before we dive into code",
      "offset": 258.519,
      "duration": 4.041
    },
    {
      "lang": "en",
      "text": "I'd like to give you a brief Taste of",
      "offset": 260.72,
      "duration": 3.4
    },
    {
      "lang": "en",
      "text": "some of the complexities that come from",
      "offset": 262.56,
      "duration": 3.56
    },
    {
      "lang": "en",
      "text": "the tokenization because I just want to",
      "offset": 264.12,
      "duration": 3.079
    },
    {
      "lang": "en",
      "text": "make sure that we motivate it",
      "offset": 266.12,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "sufficiently for why we are doing all",
      "offset": 267.199,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "this and why this is so gross so",
      "offset": 269.479,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "tokenization is at the heart of a lot of",
      "offset": 272.639,
      "duration": 3.481
    },
    {
      "lang": "en",
      "text": "weirdness in large language models and I",
      "offset": 274.199,
      "duration": 3.56
    },
    {
      "lang": "en",
      "text": "would advise that you do not brush it",
      "offset": 276.12,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "off a lot of the issues that may look",
      "offset": 277.759,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "like just issues with the new network",
      "offset": 280.6,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "architecture or the large language model",
      "offset": 282.32,
      "duration": 4.28
    },
    {
      "lang": "en",
      "text": "itself are actually issues with the",
      "offset": 284.52,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "tokenization and fundamentally Trace uh",
      "offset": 286.6,
      "duration": 5.159
    },
    {
      "lang": "en",
      "text": "back to it so if you've noticed any",
      "offset": 289.16,
      "duration": 5.08
    },
    {
      "lang": "en",
      "text": "issues with large language models can't",
      "offset": 291.759,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "you know not able to do spelling tasks",
      "offset": 294.24,
      "duration": 3.72
    },
    {
      "lang": "en",
      "text": "very easily that's usually due to",
      "offset": 296.16,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "tokenization simple string processing",
      "offset": 297.96,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "can be difficult for the large language",
      "offset": 300.16,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "model to perform",
      "offset": 302.28,
      "duration": 3.8
    },
    {
      "lang": "en",
      "text": "natively uh non-english languages can",
      "offset": 303.6,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "work much worse and to a large extent",
      "offset": 306.08,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "this is due to",
      "offset": 308.24,
      "duration": 3.519
    },
    {
      "lang": "en",
      "text": "tokenization sometimes llms are bad at",
      "offset": 309.44,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "simple arithmetic also can trace be",
      "offset": 311.759,
      "duration": 3.72
    },
    {
      "lang": "en",
      "text": "traced to",
      "offset": 314.08,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "tokenization uh gbt2 specifically would",
      "offset": 315.479,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "have had quite a bit more issues with",
      "offset": 317.759,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "python than uh future versions of it due",
      "offset": 319.639,
      "duration": 4.761
    },
    {
      "lang": "en",
      "text": "to tokenization there's a lot of other",
      "offset": 322.16,
      "duration": 3.72
    },
    {
      "lang": "en",
      "text": "issues maybe you've seen weird warnings",
      "offset": 324.4,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "about a trailing whites space this is a",
      "offset": 325.88,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "tokenization issue um",
      "offset": 327.44,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "if you had asked GPT earlier about solid",
      "offset": 330.68,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "gold Magikarp and what it is you would",
      "offset": 333.52,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "see the llm go totally crazy and it",
      "offset": 335.24,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "would start going off about a completely",
      "offset": 337.52,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "unrelated tangent topic maybe you've",
      "offset": 339.56,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "been told to use yl over Json in",
      "offset": 341.919,
      "duration": 3.521
    },
    {
      "lang": "en",
      "text": "structure data all of that has to do",
      "offset": 343.72,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "with tokenization so basically",
      "offset": 345.44,
      "duration": 3.96
    },
    {
      "lang": "en",
      "text": "tokenization is at the heart of many",
      "offset": 347.639,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "issues I will look back around to these",
      "offset": 349.4,
      "duration": 4.68
    },
    {
      "lang": "en",
      "text": "at the end of the video but for now let",
      "offset": 351.88,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "me just um skip over it a little bit and",
      "offset": 354.08,
      "duration": 5.88
    },
    {
      "lang": "en",
      "text": "let's go to this web app um the Tik",
      "offset": 356.919,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "tokenizer bell.app so I have it loaded",
      "offset": 359.96,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "here and what I like about this web app",
      "offset": 362.919,
      "duration": 3.641
    },
    {
      "lang": "en",
      "text": "is that tokenization is running a sort",
      "offset": 364.68,
      "duration": 4.84
    },
    {
      "lang": "en",
      "text": "of live in your browser in JavaScript so",
      "offset": 366.56,
      "duration": 5.4
    },
    {
      "lang": "en",
      "text": "you can just type here stuff hello world",
      "offset": 369.52,
      "duration": 4.679
    },
    {
      "lang": "en",
      "text": "and the whole string",
      "offset": 371.96,
      "duration": 6.519
    },
    {
      "lang": "en",
      "text": "rokenes so here what we see on uh the",
      "offset": 374.199,
      "duration": 6.161
    },
    {
      "lang": "en",
      "text": "left is a string that you put in on the",
      "offset": 378.479,
      "duration": 3.72
    },
    {
      "lang": "en",
      "text": "right we're currently using the gpt2",
      "offset": 380.36,
      "duration": 4.2
    },
    {
      "lang": "en",
      "text": "tokenizer we see that this string that I",
      "offset": 382.199,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "pasted here is currently tokenizing into",
      "offset": 384.56,
      "duration": 5.96
    },
    {
      "lang": "en",
      "text": "300 tokens and here they are sort of uh",
      "offset": 387.08,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "shown explicitly in different colors for",
      "offset": 390.52,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "every single token so for example uh",
      "offset": 392.68,
      "duration": 6.2
    },
    {
      "lang": "en",
      "text": "this word tokenization became two tokens",
      "offset": 395.52,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "the token",
      "offset": 398.88,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "3,642 and",
      "offset": 400.72,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "1,634 the token um space is is token 318",
      "offset": 404,
      "duration": 7.919
    },
    {
      "lang": "en",
      "text": "so be careful on the bottom you can show",
      "offset": 410.16,
      "duration": 4.439
    },
    {
      "lang": "en",
      "text": "white space and keep in mind that there",
      "offset": 411.919,
      "duration": 5.441
    },
    {
      "lang": "en",
      "text": "are spaces and uh sln new line",
      "offset": 414.599,
      "duration": 5.121
    },
    {
      "lang": "en",
      "text": "characters in here but you can hide them",
      "offset": 417.36,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "for",
      "offset": 419.72,
      "duration": 6.28
    },
    {
      "lang": "en",
      "text": "clarity the token space at is token 379",
      "offset": 421.599,
      "duration": 9.481
    },
    {
      "lang": "en",
      "text": "the to the Token space the is 262 Etc so",
      "offset": 426,
      "duration": 6.96
    },
    {
      "lang": "en",
      "text": "you notice here that the space is part",
      "offset": 431.08,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "of that uh token",
      "offset": 432.96,
      "duration": 5.679
    },
    {
      "lang": "en",
      "text": "chunk now so this is kind of like how",
      "offset": 435.96,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "our English sentence broke up and that",
      "offset": 438.639,
      "duration": 5.4
    },
    {
      "lang": "en",
      "text": "seems all well and good now now here I",
      "offset": 441.16,
      "duration": 5.759
    },
    {
      "lang": "en",
      "text": "put in some arithmetic so we see that uh",
      "offset": 444.039,
      "duration": 7.761
    },
    {
      "lang": "en",
      "text": "the token 127 Plus and then token six",
      "offset": 446.919,
      "duration": 7.321
    },
    {
      "lang": "en",
      "text": "space 6 followed by 77 so what's",
      "offset": 451.8,
      "duration": 4.839
    },
    {
      "lang": "en",
      "text": "happening here is that 127 is feeding in",
      "offset": 454.24,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "as a single token into the large",
      "offset": 456.639,
      "duration": 6.041
    },
    {
      "lang": "en",
      "text": "language model but the um number 677",
      "offset": 458.16,
      "duration": 6.68
    },
    {
      "lang": "en",
      "text": "will actually feed in as two separate",
      "offset": 462.68,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "tokens and so the large language model",
      "offset": 464.84,
      "duration": 5.88
    },
    {
      "lang": "en",
      "text": "has to sort of um take account of that",
      "offset": 467,
      "duration": 6.879
    },
    {
      "lang": "en",
      "text": "and process it correctly in its Network",
      "offset": 470.72,
      "duration": 5.479
    },
    {
      "lang": "en",
      "text": "and see here 804 will be broken up into",
      "offset": 473.879,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "two tokens and it's is all completely",
      "offset": 476.199,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "arbitrary and here I have another",
      "offset": 477.96,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "example of four-digit numbers and they",
      "offset": 479.8,
      "duration": 4.119
    },
    {
      "lang": "en",
      "text": "break up in a way that they break up and",
      "offset": 482.039,
      "duration": 3.241
    },
    {
      "lang": "en",
      "text": "it's totally arbitrary sometimes you",
      "offset": 483.919,
      "duration": 4.441
    },
    {
      "lang": "en",
      "text": "have um multiple digits single token",
      "offset": 485.28,
      "duration": 5.08
    },
    {
      "lang": "en",
      "text": "sometimes you have individual digits as",
      "offset": 488.36,
      "duration": 3.88
    },
    {
      "lang": "en",
      "text": "many tokens and it's all kind of pretty",
      "offset": 490.36,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "arbitrary and coming out of the",
      "offset": 492.24,
      "duration": 5.239
    },
    {
      "lang": "en",
      "text": "tokenizer here's another example we have",
      "offset": 494.68,
      "duration": 6.359
    },
    {
      "lang": "en",
      "text": "the string egg and you see here that",
      "offset": 497.479,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "this became two",
      "offset": 501.039,
      "duration": 3.72
    },
    {
      "lang": "en",
      "text": "tokens but for some reason when I say I",
      "offset": 502.36,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "have an egg you see when it's a space",
      "offset": 504.759,
      "duration": 6.081
    },
    {
      "lang": "en",
      "text": "egg it's two token it's sorry it's a",
      "offset": 507.72,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "single token so just egg by itself in",
      "offset": 510.84,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "the beginning of a sentence is two",
      "offset": 513.24,
      "duration": 4.44
    },
    {
      "lang": "en",
      "text": "tokens but here as a space egg is",
      "offset": 514.76,
      "duration": 5.759
    },
    {
      "lang": "en",
      "text": "suddenly a single token uh for the exact",
      "offset": 517.68,
      "duration": 6.52
    },
    {
      "lang": "en",
      "text": "same string okay here lowercase egg",
      "offset": 520.519,
      "duration": 5.721
    },
    {
      "lang": "en",
      "text": "turns out to be a single token and in",
      "offset": 524.2,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "particular notice that the color is",
      "offset": 526.24,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "different so this is a different token",
      "offset": 527.48,
      "duration": 4.28
    },
    {
      "lang": "en",
      "text": "so this is case sensitive and of course",
      "offset": 529.36,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "a capital egg would also be different",
      "offset": 531.76,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "tokens and again um this would be two",
      "offset": 534.56,
      "duration": 5.519
    },
    {
      "lang": "en",
      "text": "tokens arbitrarily so so for the same",
      "offset": 537.44,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "concept egg depending on if it's in the",
      "offset": 540.079,
      "duration": 3.721
    },
    {
      "lang": "en",
      "text": "beginning of a sentence at the end of a",
      "offset": 542.32,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "sentence lowercase uppercase or mixed",
      "offset": 543.8,
      "duration": 4.279
    },
    {
      "lang": "en",
      "text": "all this will be uh basically very",
      "offset": 546.24,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "different tokens and different IDs and",
      "offset": 548.079,
      "duration": 3.961
    },
    {
      "lang": "en",
      "text": "the language model has to learn from raw",
      "offset": 550.32,
      "duration": 3.24
    },
    {
      "lang": "en",
      "text": "data from all the internet text that",
      "offset": 552.04,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "it's going to be training on that these",
      "offset": 553.56,
      "duration": 3.88
    },
    {
      "lang": "en",
      "text": "are actually all the exact same concept",
      "offset": 555.16,
      "duration": 4.119
    },
    {
      "lang": "en",
      "text": "and it has to sort of group them in the",
      "offset": 557.44,
      "duration": 3.88
    },
    {
      "lang": "en",
      "text": "parameters of the neural network and",
      "offset": 559.279,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "understand just based on the data",
      "offset": 561.32,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "patterns that these are all very similar",
      "offset": 562.48,
      "duration": 4.919
    },
    {
      "lang": "en",
      "text": "but maybe not almost exactly similar but",
      "offset": 564.76,
      "duration": 5.4
    },
    {
      "lang": "en",
      "text": "but very very similar",
      "offset": 567.399,
      "duration": 5.401
    },
    {
      "lang": "en",
      "text": "um after the EG demonstration here I",
      "offset": 570.16,
      "duration": 5.48
    },
    {
      "lang": "en",
      "text": "have um an introduction from open a eyes",
      "offset": 572.8,
      "duration": 9.159
    },
    {
      "lang": "en",
      "text": "chbt in Korean so manaso Pang uh Etc uh",
      "offset": 575.64,
      "duration": 8.439
    },
    {
      "lang": "en",
      "text": "so this is in Korean and the reason I",
      "offset": 581.959,
      "duration": 5.801
    },
    {
      "lang": "en",
      "text": "put this here is because you'll notice",
      "offset": 584.079,
      "duration": 6.921
    },
    {
      "lang": "en",
      "text": "that um non-english languages work",
      "offset": 587.76,
      "duration": 6.56
    },
    {
      "lang": "en",
      "text": "slightly worse in Chachi part of this is",
      "offset": 591,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "because of course the training data set",
      "offset": 594.32,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "for Chachi is much larger for English",
      "offset": 595.64,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "and for everything else but the same is",
      "offset": 598.079,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "true not just for the large language",
      "offset": 599.959,
      "duration": 4.361
    },
    {
      "lang": "en",
      "text": "model itself but also for the tokenizer",
      "offset": 601.68,
      "duration": 4.2
    },
    {
      "lang": "en",
      "text": "so when we train the tokenizer we're",
      "offset": 604.32,
      "duration": 2.92
    },
    {
      "lang": "en",
      "text": "going to see that there's a training set",
      "offset": 605.88,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "as well and there's a lot more English",
      "offset": 607.24,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "than non-english and what ends up",
      "offset": 609.24,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "happening is that we're going to have a",
      "offset": 611.32,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "lot more longer tokens for",
      "offset": 613.48,
      "duration": 6.12
    },
    {
      "lang": "en",
      "text": "English so how do I put this if you have",
      "offset": 616.6,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "a single sentence in English and you",
      "offset": 619.6,
      "duration": 3.96
    },
    {
      "lang": "en",
      "text": "tokenize it you might see that it's 10",
      "offset": 621.399,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "tokens or something like that but if you",
      "offset": 623.56,
      "duration": 3.8
    },
    {
      "lang": "en",
      "text": "translate that sentence into say Korean",
      "offset": 625.48,
      "duration": 3.96
    },
    {
      "lang": "en",
      "text": "or Japanese or something else you'll",
      "offset": 627.36,
      "duration": 3.479
    },
    {
      "lang": "en",
      "text": "typically see that the number of tokens",
      "offset": 629.44,
      "duration": 3.959
    },
    {
      "lang": "en",
      "text": "used is much larger and that's because",
      "offset": 630.839,
      "duration": 5.921
    },
    {
      "lang": "en",
      "text": "the chunks here are a lot more broken up",
      "offset": 633.399,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "so we're using a lot more tokens for the",
      "offset": 636.76,
      "duration": 4.6
    },
    {
      "lang": "en",
      "text": "exact same thing and what this does is",
      "offset": 638.519,
      "duration": 5.241
    },
    {
      "lang": "en",
      "text": "it bloats up the sequence length of all",
      "offset": 641.36,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "the documents so you're using up more",
      "offset": 643.76,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "tokens and then in the attention of the",
      "offset": 646.24,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "Transformer when these tokens try to",
      "offset": 648.399,
      "duration": 3.521
    },
    {
      "lang": "en",
      "text": "attend each other you are running out of",
      "offset": 649.92,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "context um in the maximum context length",
      "offset": 651.92,
      "duration": 6.039
    },
    {
      "lang": "en",
      "text": "of that Transformer and so basically all",
      "offset": 655.12,
      "duration": 6.159
    },
    {
      "lang": "en",
      "text": "the non-english text is stretched out",
      "offset": 657.959,
      "duration": 5.481
    },
    {
      "lang": "en",
      "text": "from the perspective of the Transformer",
      "offset": 661.279,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "and this just has to do with the um",
      "offset": 663.44,
      "duration": 4.04
    },
    {
      "lang": "en",
      "text": "trainings that used for the tokenizer",
      "offset": 665.68,
      "duration": 4.36
    },
    {
      "lang": "en",
      "text": "and the tokenization itself so it will",
      "offset": 667.48,
      "duration": 4.599
    },
    {
      "lang": "en",
      "text": "create a lot bigger tokens and a lot",
      "offset": 670.04,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "larger groups in English and it will",
      "offset": 672.079,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "have a lot of little boundaries for all",
      "offset": 674.2,
      "duration": 5.56
    },
    {
      "lang": "en",
      "text": "the other non-english text um so if we",
      "offset": 676.16,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "translated this into English it would be",
      "offset": 679.76,
      "duration": 3.56
    },
    {
      "lang": "en",
      "text": "significantly fewer",
      "offset": 681.92,
      "duration": 3.719
    },
    {
      "lang": "en",
      "text": "tokens the final example I have here is",
      "offset": 683.32,
      "duration": 4.759
    },
    {
      "lang": "en",
      "text": "a little snippet of python for doing FS",
      "offset": 685.639,
      "duration": 5.361
    },
    {
      "lang": "en",
      "text": "buuz and what I'd like you to notice is",
      "offset": 688.079,
      "duration": 5.961
    },
    {
      "lang": "en",
      "text": "look all these individual spaces are all",
      "offset": 691,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "separate tokens they are token",
      "offset": 694.04,
      "duration": 8.72
    },
    {
      "lang": "en",
      "text": "220 so uh 220 220 220 220 and then space",
      "offset": 697,
      "duration": 8.32
    },
    {
      "lang": "en",
      "text": "if is a single token and so what's going",
      "offset": 702.76,
      "duration": 3.96
    },
    {
      "lang": "en",
      "text": "on here is that when the Transformer is",
      "offset": 705.32,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "going to consume or try to uh create",
      "offset": 706.72,
      "duration": 5.919
    },
    {
      "lang": "en",
      "text": "this text it needs to um handle all",
      "offset": 709.32,
      "duration": 5.16
    },
    {
      "lang": "en",
      "text": "these spaces individually they all feed",
      "offset": 712.639,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "in one by one into the entire",
      "offset": 714.48,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "Transformer in the sequence and so this",
      "offset": 716.56,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "is being extremely wasteful tokenizing",
      "offset": 719.12,
      "duration": 5.32
    },
    {
      "lang": "en",
      "text": "it in this way and so as a result of",
      "offset": 721.279,
      "duration": 5.761
    },
    {
      "lang": "en",
      "text": "that gpt2 is not very good with python",
      "offset": 724.44,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "and it's not anything to do with coding",
      "offset": 727.04,
      "duration": 3.64
    },
    {
      "lang": "en",
      "text": "or the language model itself it's just",
      "offset": 728.68,
      "duration": 3.399
    },
    {
      "lang": "en",
      "text": "that if he use a lot of indentation",
      "offset": 730.68,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "using space in Python like we usually do",
      "offset": 732.079,
      "duration": 5.32
    },
    {
      "lang": "en",
      "text": "uh you just end up bloating out all the",
      "offset": 735.399,
      "duration": 3.961
    },
    {
      "lang": "en",
      "text": "text and it's separated across way too",
      "offset": 737.399,
      "duration": 3.641
    },
    {
      "lang": "en",
      "text": "much of the sequence and we are running",
      "offset": 739.36,
      "duration": 3.4
    },
    {
      "lang": "en",
      "text": "out of the context length in the",
      "offset": 741.04,
      "duration": 3.4
    },
    {
      "lang": "en",
      "text": "sequence uh that's roughly speaking",
      "offset": 742.76,
      "duration": 2.879
    },
    {
      "lang": "en",
      "text": "what's what's happening we're being way",
      "offset": 744.44,
      "duration": 2.959
    },
    {
      "lang": "en",
      "text": "too wasteful we're taking up way too",
      "offset": 745.639,
      "duration": 4.041
    },
    {
      "lang": "en",
      "text": "much token space now we can also scroll",
      "offset": 747.399,
      "duration": 4.201
    },
    {
      "lang": "en",
      "text": "up here and we can change the tokenizer",
      "offset": 749.68,
      "duration": 4.36
    },
    {
      "lang": "en",
      "text": "so note here that gpt2 tokenizer creates",
      "offset": 751.6,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "a token count of 300 for this string",
      "offset": 754.04,
      "duration": 5.479
    },
    {
      "lang": "en",
      "text": "here we can change it to CL 100K base",
      "offset": 756.72,
      "duration": 5.119
    },
    {
      "lang": "en",
      "text": "which is the GPT for tokenizer and we",
      "offset": 759.519,
      "duration": 5.041
    },
    {
      "lang": "en",
      "text": "see that the token count drops to 185 so",
      "offset": 761.839,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "for the exact same string we are now",
      "offset": 764.56,
      "duration": 5.24
    },
    {
      "lang": "en",
      "text": "roughly having the number of tokens and",
      "offset": 766.8,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "roughly speaking this is because uh the",
      "offset": 769.8,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "number of tokens in the GPT 4 tokenizer",
      "offset": 771.76,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "is roughly double that of the number of",
      "offset": 774.36,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "tokens in the gpt2 tokenizer so we went",
      "offset": 776.72,
      "duration": 4.919
    },
    {
      "lang": "en",
      "text": "went from roughly 50k to roughly 100K",
      "offset": 778.839,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "now you can imagine that this is a good",
      "offset": 781.639,
      "duration": 4.361
    },
    {
      "lang": "en",
      "text": "thing because the same text is now",
      "offset": 783,
      "duration": 7.199
    },
    {
      "lang": "en",
      "text": "squished into half as many tokens so uh",
      "offset": 786,
      "duration": 6.76
    },
    {
      "lang": "en",
      "text": "this is a lot denser input to the",
      "offset": 790.199,
      "duration": 5.241
    },
    {
      "lang": "en",
      "text": "Transformer and in the Transformer every",
      "offset": 792.76,
      "duration": 4.28
    },
    {
      "lang": "en",
      "text": "single token has a finite number of",
      "offset": 795.44,
      "duration": 2.959
    },
    {
      "lang": "en",
      "text": "tokens before it that it's going to pay",
      "offset": 797.04,
      "duration": 3.4
    },
    {
      "lang": "en",
      "text": "attention to and so what this is doing",
      "offset": 798.399,
      "duration": 5.081
    },
    {
      "lang": "en",
      "text": "is we're roughly able to see twice as",
      "offset": 800.44,
      "duration": 6.079
    },
    {
      "lang": "en",
      "text": "much text as a context for what token to",
      "offset": 803.48,
      "duration": 5.799
    },
    {
      "lang": "en",
      "text": "predict next uh because of this change",
      "offset": 806.519,
      "duration": 4.281
    },
    {
      "lang": "en",
      "text": "but of course just increasing the number",
      "offset": 809.279,
      "duration": 4.12
    },
    {
      "lang": "en",
      "text": "of tokens is uh not strictly better",
      "offset": 810.8,
      "duration": 4.36
    },
    {
      "lang": "en",
      "text": "infinitely uh because as you increase",
      "offset": 813.399,
      "duration": 3.521
    },
    {
      "lang": "en",
      "text": "the number of tokens now your embedding",
      "offset": 815.16,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "table is um sort of getting a lot larger",
      "offset": 816.92,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "and also at the output we are trying to",
      "offset": 819.88,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "predict the next token and there's the",
      "offset": 821.48,
      "duration": 3.64
    },
    {
      "lang": "en",
      "text": "soft Max there and that grows as well",
      "offset": 822.88,
      "duration": 3.519
    },
    {
      "lang": "en",
      "text": "we're going to go into more detail later",
      "offset": 825.12,
      "duration": 3.32
    },
    {
      "lang": "en",
      "text": "on this but there's some kind of a Sweet",
      "offset": 826.399,
      "duration": 4.601
    },
    {
      "lang": "en",
      "text": "Spot somewhere where you have a just",
      "offset": 828.44,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "right number of tokens in your",
      "offset": 831,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "vocabulary where everything is",
      "offset": 832.279,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "appropriately dense and still fairly",
      "offset": 833.88,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "efficient now one thing I would like you",
      "offset": 836.519,
      "duration": 3.641
    },
    {
      "lang": "en",
      "text": "to note specifically for the gp4",
      "offset": 838.36,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "tokenizer is that the handling of the",
      "offset": 840.16,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "white space for python has improved a",
      "offset": 843.56,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "lot you see that here these four spaces",
      "offset": 845.44,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "are represented as one single token for",
      "offset": 848.36,
      "duration": 5.399
    },
    {
      "lang": "en",
      "text": "the three spaces here and then the token",
      "offset": 850.24,
      "duration": 6.519
    },
    {
      "lang": "en",
      "text": "SPF and here seven spaces were all",
      "offset": 853.759,
      "duration": 5.041
    },
    {
      "lang": "en",
      "text": "grouped into a single token so we're",
      "offset": 856.759,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "being a lot more efficient in how we",
      "offset": 858.8,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "represent Python and this was a",
      "offset": 860.199,
      "duration": 3.56
    },
    {
      "lang": "en",
      "text": "deliberate Choice made by open aai when",
      "offset": 861.92,
      "duration": 5.64
    },
    {
      "lang": "en",
      "text": "they designed the gp4 tokenizer and they",
      "offset": 863.759,
      "duration": 5.921
    },
    {
      "lang": "en",
      "text": "group a lot more space into a single",
      "offset": 867.56,
      "duration": 4.519
    },
    {
      "lang": "en",
      "text": "character what this does is this",
      "offset": 869.68,
      "duration": 5.519
    },
    {
      "lang": "en",
      "text": "densifies Python and therefore we can",
      "offset": 872.079,
      "duration": 6.041
    },
    {
      "lang": "en",
      "text": "attend to more code before it when we're",
      "offset": 875.199,
      "duration": 4.521
    },
    {
      "lang": "en",
      "text": "trying to predict the next token in the",
      "offset": 878.12,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "sequence and so the Improvement in the",
      "offset": 879.72,
      "duration": 5.679
    },
    {
      "lang": "en",
      "text": "python coding ability from gbt2 to gp4",
      "offset": 882.04,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "is not just a matter of the language",
      "offset": 885.399,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "model and the architecture and the",
      "offset": 887.079,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "details of the optimization but a lot of",
      "offset": 888.839,
      "duration": 3.401
    },
    {
      "lang": "en",
      "text": "the Improvement here is also coming from",
      "offset": 890.759,
      "duration": 3.481
    },
    {
      "lang": "en",
      "text": "the design of the tokenizer and how it",
      "offset": 892.24,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "groups characters into tokens okay so",
      "offset": 894.24,
      "duration": 5.159
    },
    {
      "lang": "en",
      "text": "let's now start writing some code",
      "offset": 896.959,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "so remember what we want to do we want",
      "offset": 899.399,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "to take strings and feed them into",
      "offset": 901.44,
      "duration": 4.519
    },
    {
      "lang": "en",
      "text": "language models for that we need to",
      "offset": 903.72,
      "duration": 5.08
    },
    {
      "lang": "en",
      "text": "somehow tokenize strings into some",
      "offset": 905.959,
      "duration": 6.401
    },
    {
      "lang": "en",
      "text": "integers in some fixed vocabulary and",
      "offset": 908.8,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "then we will use those integers to make",
      "offset": 912.36,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "a look up into a lookup table of vectors",
      "offset": 914.24,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "and feed those vectors into the",
      "offset": 916.759,
      "duration": 4.601
    },
    {
      "lang": "en",
      "text": "Transformer as an input now the reason",
      "offset": 918,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "this gets a little bit tricky of course",
      "offset": 921.36,
      "duration": 2.64
    },
    {
      "lang": "en",
      "text": "is that we don't just want to support",
      "offset": 922.72,
      "duration": 3.4
    },
    {
      "lang": "en",
      "text": "the simple English alphabet we want to",
      "offset": 924,
      "duration": 4.12
    },
    {
      "lang": "en",
      "text": "support different kinds of languages so",
      "offset": 926.12,
      "duration": 5.519
    },
    {
      "lang": "en",
      "text": "this is anango in Korean which is hello",
      "offset": 928.12,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "and we also want to support many kinds",
      "offset": 931.639,
      "duration": 3.081
    },
    {
      "lang": "en",
      "text": "of special characters that we might find",
      "offset": 933,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "on the internet for example",
      "offset": 934.72,
      "duration": 6.76
    },
    {
      "lang": "en",
      "text": "Emoji so how do we feed this text into",
      "offset": 937.319,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "uh",
      "offset": 941.48,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "Transformers well how's the what is this",
      "offset": 942.199,
      "duration": 4.361
    },
    {
      "lang": "en",
      "text": "text anyway in Python so if you go to",
      "offset": 944.48,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "the documentation of a string in Python",
      "offset": 946.56,
      "duration": 4.959
    },
    {
      "lang": "en",
      "text": "you can see that strings are immutable",
      "offset": 949.6,
      "duration": 4.52
    },
    {
      "lang": "en",
      "text": "sequences of Unicode code",
      "offset": 951.519,
      "duration": 6.361
    },
    {
      "lang": "en",
      "text": "points okay what are Unicode code points",
      "offset": 954.12,
      "duration": 7.36
    },
    {
      "lang": "en",
      "text": "we can go to PDF so Unicode code points",
      "offset": 957.88,
      "duration": 6.8
    },
    {
      "lang": "en",
      "text": "are defined by the Unicode Consortium as",
      "offset": 961.48,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "part of the Unicode standard and what",
      "offset": 964.68,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "this is really is that it's just a",
      "offset": 967.56,
      "duration": 4.279
    },
    {
      "lang": "en",
      "text": "definition of roughly 150,000 characters",
      "offset": 969,
      "duration": 5.72
    },
    {
      "lang": "en",
      "text": "right now and roughly speaking what they",
      "offset": 971.839,
      "duration": 5.721
    },
    {
      "lang": "en",
      "text": "look like and what integers um represent",
      "offset": 974.72,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "those characters so it says 150,000",
      "offset": 977.56,
      "duration": 5.079
    },
    {
      "lang": "en",
      "text": "characters across 161 scripts as of",
      "offset": 979.72,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "right now so if you scroll down here you",
      "offset": 982.639,
      "duration": 3.64
    },
    {
      "lang": "en",
      "text": "can see that the standard is very much",
      "offset": 984.72,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "alive the latest standard 15.1 in",
      "offset": 986.279,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "September",
      "offset": 988.72,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "2023 and basically this is just a way to",
      "offset": 990.199,
      "duration": 6.721
    },
    {
      "lang": "en",
      "text": "define lots of types of",
      "offset": 993.92,
      "duration": 5.24
    },
    {
      "lang": "en",
      "text": "characters like for example all these",
      "offset": 996.92,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "characters across different scripts so",
      "offset": 999.16,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "the way we can access the unic code code",
      "offset": 1001.88,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "Point given Single Character is by using",
      "offset": 1004.04,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "the or function in Python so for example",
      "offset": 1005.959,
      "duration": 5.32
    },
    {
      "lang": "en",
      "text": "I can pass in Ord of H and I can see",
      "offset": 1008.199,
      "duration": 6.521
    },
    {
      "lang": "en",
      "text": "that for the Single Character H the unic",
      "offset": 1011.279,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "code code point is",
      "offset": 1014.72,
      "duration": 5.679
    },
    {
      "lang": "en",
      "text": "104 okay um but this can be arbitr",
      "offset": 1016.48,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "complicated so we can take for example",
      "offset": 1020.399,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "our Emoji here and we can see that the",
      "offset": 1022.16,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "code point for this one is",
      "offset": 1024.16,
      "duration": 6.2
    },
    {
      "lang": "en",
      "text": "128,000 or we can take",
      "offset": 1026.4,
      "duration": 7.32
    },
    {
      "lang": "en",
      "text": "un and this is 50,000 now keep in mind",
      "offset": 1030.36,
      "duration": 6.36
    },
    {
      "lang": "en",
      "text": "you can't plug in strings here because",
      "offset": 1033.72,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "you uh this doesn't have a single code",
      "offset": 1036.72,
      "duration": 3.959
    },
    {
      "lang": "en",
      "text": "point it only takes a single uni code",
      "offset": 1038.439,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "code Point character and tells you its",
      "offset": 1040.679,
      "duration": 6.12
    },
    {
      "lang": "en",
      "text": "integer so in this way we can look",
      "offset": 1043.959,
      "duration": 6.121
    },
    {
      "lang": "en",
      "text": "up all the um characters of this",
      "offset": 1046.799,
      "duration": 5.361
    },
    {
      "lang": "en",
      "text": "specific string and their code points so",
      "offset": 1050.08,
      "duration": 6.68
    },
    {
      "lang": "en",
      "text": "or of X forx in this string and we get",
      "offset": 1052.16,
      "duration": 8.2
    },
    {
      "lang": "en",
      "text": "this encoding here now see here we've",
      "offset": 1056.76,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "already turned the raw code points",
      "offset": 1060.36,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "already have integers so why can't we",
      "offset": 1062.2,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "simply just use these integers and not",
      "offset": 1064.44,
      "duration": 4.119
    },
    {
      "lang": "en",
      "text": "have any tokenization at all why can't",
      "offset": 1066.84,
      "duration": 3.8
    },
    {
      "lang": "en",
      "text": "we just use this natively as is and just",
      "offset": 1068.559,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "use the code Point well one reason for",
      "offset": 1070.64,
      "duration": 3.72
    },
    {
      "lang": "en",
      "text": "that of course is that the vocabulary in",
      "offset": 1072.88,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "that case would be quite long so in this",
      "offset": 1074.36,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "case for Unicode the this is a",
      "offset": 1076.799,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "vocabulary of",
      "offset": 1078.679,
      "duration": 3.961
    },
    {
      "lang": "en",
      "text": "150,000 different code points but more",
      "offset": 1079.799,
      "duration": 5.24
    },
    {
      "lang": "en",
      "text": "worryingly than that I think the Unicode",
      "offset": 1082.64,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "standard is very much alive and it keeps",
      "offset": 1085.039,
      "duration": 4.201
    },
    {
      "lang": "en",
      "text": "changing and so it's not kind of a",
      "offset": 1087.039,
      "duration": 4.041
    },
    {
      "lang": "en",
      "text": "stable representation necessarily that",
      "offset": 1089.24,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "we may want to use directly so for those",
      "offset": 1091.08,
      "duration": 4.68
    },
    {
      "lang": "en",
      "text": "reasons we need something a bit better",
      "offset": 1093.88,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "so to find something better we turn to",
      "offset": 1095.76,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "encodings so if we go to the Wikipedia",
      "offset": 1097.64,
      "duration": 3.64
    },
    {
      "lang": "en",
      "text": "page here we see that the Unicode",
      "offset": 1099.76,
      "duration": 4.039
    },
    {
      "lang": "en",
      "text": "consortion defines three types of",
      "offset": 1101.28,
      "duration": 6.68
    },
    {
      "lang": "en",
      "text": "encodings utf8 UTF 16 and UTF 32 these",
      "offset": 1103.799,
      "duration": 6.921
    },
    {
      "lang": "en",
      "text": "encoding are the way by which we can",
      "offset": 1107.96,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "take Unicode text and translate it into",
      "offset": 1110.72,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "binary data or by streams utf8 is by far",
      "offset": 1113.48,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "the most common uh so this is the utf8",
      "offset": 1117.2,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "page now this Wikipedia page is actually",
      "offset": 1119.96,
      "duration": 4.44
    },
    {
      "lang": "en",
      "text": "quite long but what's important for our",
      "offset": 1122,
      "duration": 4.44
    },
    {
      "lang": "en",
      "text": "purposes is that utf8 takes every single",
      "offset": 1124.4,
      "duration": 5.24
    },
    {
      "lang": "en",
      "text": "Cod point and it translates it to a by",
      "offset": 1126.44,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "stream and this by stream is between one",
      "offset": 1129.64,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "to four bytes so it's a variable length",
      "offset": 1132.36,
      "duration": 4.12
    },
    {
      "lang": "en",
      "text": "encoding so depending on the Unicode",
      "offset": 1134.36,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "Point according to the schema you're",
      "offset": 1136.48,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "going to end up with between 1 to four",
      "offset": 1138.039,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "bytes for each code point on top of that",
      "offset": 1139.76,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "there's utf8 uh",
      "offset": 1143,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "utf16 and UTF 32 UTF 32 is nice because",
      "offset": 1145.12,
      "duration": 5.439
    },
    {
      "lang": "en",
      "text": "it is fixed length instead of variable",
      "offset": 1148.84,
      "duration": 3.64
    },
    {
      "lang": "en",
      "text": "length but it has many other downsides",
      "offset": 1150.559,
      "duration": 6.441
    },
    {
      "lang": "en",
      "text": "as well so the full kind of spectrum of",
      "offset": 1152.48,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "pros and cons of all these different",
      "offset": 1157,
      "duration": 3.48
    },
    {
      "lang": "en",
      "text": "three encodings are beyond the scope of",
      "offset": 1158.32,
      "duration": 4.2
    },
    {
      "lang": "en",
      "text": "this video I just like to point out that",
      "offset": 1160.48,
      "duration": 4.76
    },
    {
      "lang": "en",
      "text": "I enjoyed this block post and this block",
      "offset": 1162.52,
      "duration": 4.519
    },
    {
      "lang": "en",
      "text": "post at the end of it also has a number",
      "offset": 1165.24,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "of references that can be quite useful",
      "offset": 1167.039,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "uh one of them is uh utf8 everywhere",
      "offset": 1169.24,
      "duration": 5.08
    },
    {
      "lang": "en",
      "text": "Manifesto um and this Manifesto",
      "offset": 1172.039,
      "duration": 4.601
    },
    {
      "lang": "en",
      "text": "describes the reason why utf8 is",
      "offset": 1174.32,
      "duration": 5.56
    },
    {
      "lang": "en",
      "text": "significantly preferred and a lot nicer",
      "offset": 1176.64,
      "duration": 5.159
    },
    {
      "lang": "en",
      "text": "than the other encodings and why it is",
      "offset": 1179.88,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "used a lot more prominently um on the",
      "offset": 1181.799,
      "duration": 6.281
    },
    {
      "lang": "en",
      "text": "internet one of the major advantages",
      "offset": 1185.48,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "just just to give you a sense is that",
      "offset": 1188.08,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "utf8 is the only one of these that is",
      "offset": 1189.559,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "backwards compatible to the much simpler",
      "offset": 1192,
      "duration": 5.08
    },
    {
      "lang": "en",
      "text": "asky encoding of text um but I'm not",
      "offset": 1194.2,
      "duration": 4.28
    },
    {
      "lang": "en",
      "text": "going to go into the full detail in this",
      "offset": 1197.08,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "video so suffice to say that we like the",
      "offset": 1198.48,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "utf8 encoding and uh let's try to take",
      "offset": 1201,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "the string and see what we get if we",
      "offset": 1203.84,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "encoded into",
      "offset": 1206.039,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "utf8 the string class in Python actually",
      "offset": 1208,
      "duration": 4.36
    },
    {
      "lang": "en",
      "text": "has do encode and you can give it the",
      "offset": 1210.76,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "encoding which is say utf8 now we get",
      "offset": 1212.36,
      "duration": 5.48
    },
    {
      "lang": "en",
      "text": "out of this is not very nice because",
      "offset": 1215.559,
      "duration": 5.401
    },
    {
      "lang": "en",
      "text": "this is the bytes is a bytes object and",
      "offset": 1217.84,
      "duration": 4.92
    },
    {
      "lang": "en",
      "text": "it's not very nice in the way that it's",
      "offset": 1220.96,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "printed so I personally like to take it",
      "offset": 1222.76,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "through list because then we actually",
      "offset": 1225.039,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "get the raw B",
      "offset": 1226.84,
      "duration": 5.56
    },
    {
      "lang": "en",
      "text": "of this uh encoding so this is the raw",
      "offset": 1228.72,
      "duration": 6.88
    },
    {
      "lang": "en",
      "text": "byes that represent this string",
      "offset": 1232.4,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "according to the utf8 en coding we can",
      "offset": 1235.6,
      "duration": 4.959
    },
    {
      "lang": "en",
      "text": "also look at utf16 we get a slightly",
      "offset": 1238.08,
      "duration": 5.16
    },
    {
      "lang": "en",
      "text": "different by stream and we here we start",
      "offset": 1240.559,
      "duration": 4.921
    },
    {
      "lang": "en",
      "text": "to see one of the disadvantages of utf16",
      "offset": 1243.24,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "you see how we have zero Z something Z",
      "offset": 1245.48,
      "duration": 4.199
    },
    {
      "lang": "en",
      "text": "something Z something we're starting to",
      "offset": 1247.96,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "get a sense that this is a bit of a",
      "offset": 1249.679,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "wasteful encoding and indeed for simple",
      "offset": 1250.84,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "asky characters or English characters",
      "offset": 1253.919,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "here uh we just have the structure of 0",
      "offset": 1256.28,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "something Z something and it's not",
      "offset": 1258.559,
      "duration": 5.681
    },
    {
      "lang": "en",
      "text": "exactly nice same for UTF 32 when we",
      "offset": 1260.76,
      "duration": 5.32
    },
    {
      "lang": "en",
      "text": "expand this we can start to get a sense",
      "offset": 1264.24,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "of the wastefulness of this encoding for",
      "offset": 1266.08,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "our purposes you see a lot of zeros",
      "offset": 1268,
      "duration": 3.4
    },
    {
      "lang": "en",
      "text": "followed by",
      "offset": 1270.4,
      "duration": 4.44
    },
    {
      "lang": "en",
      "text": "something and so uh this is not",
      "offset": 1271.4,
      "duration": 6.44
    },
    {
      "lang": "en",
      "text": "desirable so suffice it to say that we",
      "offset": 1274.84,
      "duration": 6.04
    },
    {
      "lang": "en",
      "text": "would like to stick with utf8 for our",
      "offset": 1277.84,
      "duration": 6.04
    },
    {
      "lang": "en",
      "text": "purposes however if we just use utf8",
      "offset": 1280.88,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "naively these are by streams so that",
      "offset": 1283.88,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "would imply a vocabulary length of only",
      "offset": 1286.4,
      "duration": 6.72
    },
    {
      "lang": "en",
      "text": "256 possible tokens uh but this this",
      "offset": 1289.24,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "vocabulary size is very very small what",
      "offset": 1293.12,
      "duration": 3.559
    },
    {
      "lang": "en",
      "text": "this is going to do if we just were to",
      "offset": 1295.32,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "use it naively is that all of our text",
      "offset": 1296.679,
      "duration": 5.24
    },
    {
      "lang": "en",
      "text": "would be stretched out over very very",
      "offset": 1299.88,
      "duration": 6.279
    },
    {
      "lang": "en",
      "text": "long sequences of bytes and so",
      "offset": 1301.919,
      "duration": 7.401
    },
    {
      "lang": "en",
      "text": "um what what this does is that certainly",
      "offset": 1306.159,
      "duration": 4.841
    },
    {
      "lang": "en",
      "text": "the embeding table is going to be tiny",
      "offset": 1309.32,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "and the prediction at the top at the",
      "offset": 1311,
      "duration": 3.159
    },
    {
      "lang": "en",
      "text": "final layer is going to be very tiny but",
      "offset": 1312.32,
      "duration": 4.12
    },
    {
      "lang": "en",
      "text": "our sequences are very long and remember",
      "offset": 1314.159,
      "duration": 5.161
    },
    {
      "lang": "en",
      "text": "that we have pretty finite um context",
      "offset": 1316.44,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "length and the attention that we can",
      "offset": 1319.32,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "support in a transformer for",
      "offset": 1321,
      "duration": 4.52
    },
    {
      "lang": "en",
      "text": "computational reasons and so we only",
      "offset": 1322.76,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "have as much context length but now we",
      "offset": 1325.52,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "have very very long sequences and this",
      "offset": 1327.48,
      "duration": 3.319
    },
    {
      "lang": "en",
      "text": "is just inefficient and it's not going",
      "offset": 1329.44,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "to allow us to attend to sufficiently",
      "offset": 1330.799,
      "duration": 4.841
    },
    {
      "lang": "en",
      "text": "long text uh before us for the purposes",
      "offset": 1332.799,
      "duration": 5.561
    },
    {
      "lang": "en",
      "text": "of the next token prediction task so we",
      "offset": 1335.64,
      "duration": 5.96
    },
    {
      "lang": "en",
      "text": "don't want to use the raw bytes of the",
      "offset": 1338.36,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "utf8 encoding we want to be able to",
      "offset": 1341.6,
      "duration": 5.319
    },
    {
      "lang": "en",
      "text": "support larger vocabulary size that we",
      "offset": 1344.2,
      "duration": 4.44
    },
    {
      "lang": "en",
      "text": "can tune as a hyper",
      "offset": 1346.919,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "but we want to stick with the utf8",
      "offset": 1348.64,
      "duration": 4.919
    },
    {
      "lang": "en",
      "text": "encoding of these strings so what do we",
      "offset": 1350.84,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "do well the answer of course is we turn",
      "offset": 1353.559,
      "duration": 3.881
    },
    {
      "lang": "en",
      "text": "to the bite pair encoding algorithm",
      "offset": 1355.48,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "which will allow us to compress these",
      "offset": 1357.44,
      "duration": 5.16
    },
    {
      "lang": "en",
      "text": "bite sequences um to a variable amount",
      "offset": 1359.08,
      "duration": 5.599
    },
    {
      "lang": "en",
      "text": "so we'll get to that in a bit but I just",
      "offset": 1362.6,
      "duration": 4.52
    },
    {
      "lang": "en",
      "text": "want to briefly speak to the fact that I",
      "offset": 1364.679,
      "duration": 4.6
    },
    {
      "lang": "en",
      "text": "would love nothing more than to be able",
      "offset": 1367.12,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "to feed raw bite sequences into uh",
      "offset": 1369.279,
      "duration": 5.601
    },
    {
      "lang": "en",
      "text": "language models in fact there's a paper",
      "offset": 1372.96,
      "duration": 4.12
    },
    {
      "lang": "en",
      "text": "about how this could potentially be done",
      "offset": 1374.88,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "uh from Summer last last year now the",
      "offset": 1377.08,
      "duration": 3.88
    },
    {
      "lang": "en",
      "text": "problem is you actually have to go in",
      "offset": 1379.279,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "and you have to modify the Transformer",
      "offset": 1380.96,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "architecture because as I mentioned",
      "offset": 1382.279,
      "duration": 4.361
    },
    {
      "lang": "en",
      "text": "you're going to have a problem where the",
      "offset": 1384.48,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "attention will start to become extremely",
      "offset": 1386.64,
      "duration": 3.72
    },
    {
      "lang": "en",
      "text": "expensive because the sequences are so",
      "offset": 1388.24,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "long and so in this paper they propose",
      "offset": 1390.36,
      "duration": 5.4
    },
    {
      "lang": "en",
      "text": "kind of a hierarchical structuring of",
      "offset": 1393.44,
      "duration": 4.2
    },
    {
      "lang": "en",
      "text": "the Transformer that could allow you to",
      "offset": 1395.76,
      "duration": 4.6
    },
    {
      "lang": "en",
      "text": "just feed in raw bites and so at the end",
      "offset": 1397.64,
      "duration": 4.279
    },
    {
      "lang": "en",
      "text": "they say together these results",
      "offset": 1400.36,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "establish the viability of tokenization",
      "offset": 1401.919,
      "duration": 3.401
    },
    {
      "lang": "en",
      "text": "free autor regressive sequence modeling",
      "offset": 1403.64,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "at scale so tokenization free would",
      "offset": 1405.32,
      "duration": 4.959
    },
    {
      "lang": "en",
      "text": "indeed be amazing we would just feed B",
      "offset": 1407.4,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "streams directly into our models but",
      "offset": 1410.279,
      "duration": 3.88
    },
    {
      "lang": "en",
      "text": "unfortunately I don't know that this has",
      "offset": 1412.279,
      "duration": 3.801
    },
    {
      "lang": "en",
      "text": "really been proven out yet by",
      "offset": 1414.159,
      "duration": 3.081
    },
    {
      "lang": "en",
      "text": "sufficiently many groups and a",
      "offset": 1416.08,
      "duration": 3.16
    },
    {
      "lang": "en",
      "text": "sufficient scale uh but something like",
      "offset": 1417.24,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "this at one point would be amazing and I",
      "offset": 1419.24,
      "duration": 3.08
    },
    {
      "lang": "en",
      "text": "hope someone comes up with it but for",
      "offset": 1420.679,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "now we have to come back and we can't",
      "offset": 1422.32,
      "duration": 4.12
    },
    {
      "lang": "en",
      "text": "feed this directly into language models",
      "offset": 1424.44,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "and we have to compress it using the B",
      "offset": 1426.44,
      "duration": 3.4
    },
    {
      "lang": "en",
      "text": "paare encoding algorithm so let's see",
      "offset": 1428.279,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "how that works so as I mentioned the B",
      "offset": 1429.84,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "paare encoding algorithm is not all that",
      "offset": 1431.64,
      "duration": 3.88
    },
    {
      "lang": "en",
      "text": "complicated and the Wikipedia page is",
      "offset": 1433.52,
      "duration": 3.639
    },
    {
      "lang": "en",
      "text": "actually quite instructive as far as the",
      "offset": 1435.52,
      "duration": 3.96
    },
    {
      "lang": "en",
      "text": "basic idea goes go what we're doing is",
      "offset": 1437.159,
      "duration": 4.601
    },
    {
      "lang": "en",
      "text": "we have some kind of a input sequence uh",
      "offset": 1439.48,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "like for example here we have only four",
      "offset": 1441.76,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "elements in our vocabulary a b c and d",
      "offset": 1443.64,
      "duration": 4.36
    },
    {
      "lang": "en",
      "text": "and we have a sequence of them so",
      "offset": 1446.32,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "instead of bytes let's say we just have",
      "offset": 1448,
      "duration": 4.039
    },
    {
      "lang": "en",
      "text": "four a vocab size of",
      "offset": 1449.76,
      "duration": 4.36
    },
    {
      "lang": "en",
      "text": "four the sequence is too long and we'd",
      "offset": 1452.039,
      "duration": 4.12
    },
    {
      "lang": "en",
      "text": "like to compress it so what we do is",
      "offset": 1454.12,
      "duration": 6.039
    },
    {
      "lang": "en",
      "text": "that we iteratively find the pair of uh",
      "offset": 1456.159,
      "duration": 7.281
    },
    {
      "lang": "en",
      "text": "tokens that occur the most",
      "offset": 1460.159,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "frequently and then once we've",
      "offset": 1463.44,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "identified that pair we repl replace",
      "offset": 1465.279,
      "duration": 5.601
    },
    {
      "lang": "en",
      "text": "that pair with just a single new token",
      "offset": 1468.48,
      "duration": 5.079
    },
    {
      "lang": "en",
      "text": "that we append to our vocabulary so for",
      "offset": 1470.88,
      "duration": 5.399
    },
    {
      "lang": "en",
      "text": "example here the bite pair AA occurs",
      "offset": 1473.559,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "most often so we mint a new token let's",
      "offset": 1476.279,
      "duration": 5.4
    },
    {
      "lang": "en",
      "text": "call it capital Z and we replace every",
      "offset": 1478.919,
      "duration": 7.081
    },
    {
      "lang": "en",
      "text": "single occurrence of AA by Z so now we",
      "offset": 1481.679,
      "duration": 7.24
    },
    {
      "lang": "en",
      "text": "have two Z's here so here we took a",
      "offset": 1486,
      "duration": 5.799
    },
    {
      "lang": "en",
      "text": "sequence of 11 characters with",
      "offset": 1488.919,
      "duration": 5.521
    },
    {
      "lang": "en",
      "text": "vocabulary size four and we've converted",
      "offset": 1491.799,
      "duration": 6.841
    },
    {
      "lang": "en",
      "text": "it to a um sequence of only nine tokens",
      "offset": 1494.44,
      "duration": 6.119
    },
    {
      "lang": "en",
      "text": "but now with a vocabulary of five",
      "offset": 1498.64,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "because we have a fifth vocabulary",
      "offset": 1500.559,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "element that we just created and it's Z",
      "offset": 1502.399,
      "duration": 5.121
    },
    {
      "lang": "en",
      "text": "standing for concatination of AA and we",
      "offset": 1504.96,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "can again repeat this process so we",
      "offset": 1507.52,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "again look at the sequence and identify",
      "offset": 1510.24,
      "duration": 5.4
    },
    {
      "lang": "en",
      "text": "the pair of tokens that are most",
      "offset": 1512.88,
      "duration": 6.279
    },
    {
      "lang": "en",
      "text": "frequent let's say that that is now AB",
      "offset": 1515.64,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "well we are going to replace AB with a",
      "offset": 1519.159,
      "duration": 4.601
    },
    {
      "lang": "en",
      "text": "new token that we meant call Y so y",
      "offset": 1520.76,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "becomes ab and then every single",
      "offset": 1523.76,
      "duration": 4.279
    },
    {
      "lang": "en",
      "text": "occurrence of ab is now replaced with y",
      "offset": 1525.24,
      "duration": 6.2
    },
    {
      "lang": "en",
      "text": "so we end up with this so now we only",
      "offset": 1528.039,
      "duration": 7.12
    },
    {
      "lang": "en",
      "text": "have 1 2 3 4 5 6 seven characters in our",
      "offset": 1531.44,
      "duration": 8.68
    },
    {
      "lang": "en",
      "text": "sequence but we have not just um four",
      "offset": 1535.159,
      "duration": 7.161
    },
    {
      "lang": "en",
      "text": "vocabulary elements or five but now we",
      "offset": 1540.12,
      "duration": 5.679
    },
    {
      "lang": "en",
      "text": "have six and for the final round we",
      "offset": 1542.32,
      "duration": 5.32
    },
    {
      "lang": "en",
      "text": "again look through the sequence find",
      "offset": 1545.799,
      "duration": 4.76
    },
    {
      "lang": "en",
      "text": "that the phrase zy or the pair zy is",
      "offset": 1547.64,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "most common and replace it one more time",
      "offset": 1550.559,
      "duration": 6.081
    },
    {
      "lang": "en",
      "text": "with another um character let's say x so",
      "offset": 1553.32,
      "duration": 6.599
    },
    {
      "lang": "en",
      "text": "X is z y and we replace all curses of zy",
      "offset": 1556.64,
      "duration": 5.48
    },
    {
      "lang": "en",
      "text": "and we get this following sequence so",
      "offset": 1559.919,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "basically after we have gone through",
      "offset": 1562.12,
      "duration": 6.36
    },
    {
      "lang": "en",
      "text": "this process instead of having a um",
      "offset": 1563.6,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "sequence of",
      "offset": 1568.48,
      "duration": 5.16
    },
    {
      "lang": "en",
      "text": "11 uh tokens with a vocabulary length of",
      "offset": 1569.76,
      "duration": 8.399
    },
    {
      "lang": "en",
      "text": "four we now have a sequence of 1 2 3",
      "offset": 1573.64,
      "duration": 7.84
    },
    {
      "lang": "en",
      "text": "four five tokens but our vocabulary",
      "offset": 1578.159,
      "duration": 7
    },
    {
      "lang": "en",
      "text": "length now is seven and so in this way",
      "offset": 1581.48,
      "duration": 5.96
    },
    {
      "lang": "en",
      "text": "we can iteratively compress our sequence",
      "offset": 1585.159,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "I we Mint new tokens so in the in the",
      "offset": 1587.44,
      "duration": 4.959
    },
    {
      "lang": "en",
      "text": "exact same way we start we start out",
      "offset": 1590.279,
      "duration": 5.961
    },
    {
      "lang": "en",
      "text": "with bite sequences so we have 256",
      "offset": 1592.399,
      "duration": 5.801
    },
    {
      "lang": "en",
      "text": "vocabulary size but we're now going to",
      "offset": 1596.24,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "go through these and find the bite pairs",
      "offset": 1598.2,
      "duration": 4.359
    },
    {
      "lang": "en",
      "text": "that occur the most and we're going to",
      "offset": 1600.64,
      "duration": 4.2
    },
    {
      "lang": "en",
      "text": "iteratively start minting new tokens",
      "offset": 1602.559,
      "duration": 4.201
    },
    {
      "lang": "en",
      "text": "appending them to our vocabulary and",
      "offset": 1604.84,
      "duration": 4.04
    },
    {
      "lang": "en",
      "text": "replacing things and in this way we're",
      "offset": 1606.76,
      "duration": 3.48
    },
    {
      "lang": "en",
      "text": "going to end up with a compressed",
      "offset": 1608.88,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "training data set and also an algorithm",
      "offset": 1610.24,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "for taking any arbitrary sequence and",
      "offset": 1612.96,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "encoding it using this uh vocabul",
      "offset": 1615.279,
      "duration": 5.721
    },
    {
      "lang": "en",
      "text": "and also decoding it back to Strings so",
      "offset": 1618.24,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "let's now Implement all that so here's",
      "offset": 1621,
      "duration": 4.679
    },
    {
      "lang": "en",
      "text": "what I did I went to this block post",
      "offset": 1623.24,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "that I enjoyed and I took the first",
      "offset": 1625.679,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "paragraph and I copy pasted it here into",
      "offset": 1627.32,
      "duration": 5.959
    },
    {
      "lang": "en",
      "text": "text so this is one very long line",
      "offset": 1630,
      "duration": 5.96
    },
    {
      "lang": "en",
      "text": "here now to get the tokens as I",
      "offset": 1633.279,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "mentioned we just take our text and we",
      "offset": 1635.96,
      "duration": 4.199
    },
    {
      "lang": "en",
      "text": "encode it into utf8 the tokens here at",
      "offset": 1637.36,
      "duration": 5.4
    },
    {
      "lang": "en",
      "text": "this point will be a raw bites single",
      "offset": 1640.159,
      "duration": 5.441
    },
    {
      "lang": "en",
      "text": "stream of bytes and just so that it's",
      "offset": 1642.76,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "easier to work with instead of just a",
      "offset": 1645.6,
      "duration": 4.36
    },
    {
      "lang": "en",
      "text": "bytes object I'm going to convert all",
      "offset": 1647.64,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "those bytes to integers and then create",
      "offset": 1649.96,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "a list of it just so it's easier for us",
      "offset": 1652.64,
      "duration": 3.24
    },
    {
      "lang": "en",
      "text": "to manipulate and work with in Python",
      "offset": 1654.279,
      "duration": 3.721
    },
    {
      "lang": "en",
      "text": "and visualize and here I'm printing all",
      "offset": 1655.88,
      "duration": 6.2
    },
    {
      "lang": "en",
      "text": "of that so this is the original um this",
      "offset": 1658,
      "duration": 7
    },
    {
      "lang": "en",
      "text": "is the original paragraph and its length",
      "offset": 1662.08,
      "duration": 3.719
    },
    {
      "lang": "en",
      "text": "is",
      "offset": 1665,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "533 uh code points and then here are the",
      "offset": 1665.799,
      "duration": 7.521
    },
    {
      "lang": "en",
      "text": "bytes encoded in ut utf8 and we see that",
      "offset": 1669.799,
      "duration": 6.521
    },
    {
      "lang": "en",
      "text": "this has a length of 616 bytes at this",
      "offset": 1673.32,
      "duration": 5.719
    },
    {
      "lang": "en",
      "text": "point or 616 tokens and the reason this",
      "offset": 1676.32,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "is more is because a lot of these simple",
      "offset": 1679.039,
      "duration": 5.561
    },
    {
      "lang": "en",
      "text": "asky characters or simple characters",
      "offset": 1681.84,
      "duration": 4.6
    },
    {
      "lang": "en",
      "text": "they just become a single bite but a lot",
      "offset": 1684.6,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "of these Unicode more complex characters",
      "offset": 1686.44,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "become multiple bytes up to four and so",
      "offset": 1688.76,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "we are expanding that",
      "offset": 1691.08,
      "duration": 3.719
    },
    {
      "lang": "en",
      "text": "size so now what we'd like to do as a",
      "offset": 1692.76,
      "duration": 3.48
    },
    {
      "lang": "en",
      "text": "first step of the algorithm is we'd like",
      "offset": 1694.799,
      "duration": 4.12
    },
    {
      "lang": "en",
      "text": "to iterate over here and find the pair",
      "offset": 1696.24,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "of bites that occur most frequently",
      "offset": 1698.919,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "because we're then going to merge it so",
      "offset": 1702,
      "duration": 3.799
    },
    {
      "lang": "en",
      "text": "if you are working long on a notebook on",
      "offset": 1704.12,
      "duration": 3.64
    },
    {
      "lang": "en",
      "text": "a side then I encourage you to basically",
      "offset": 1705.799,
      "duration": 4.12
    },
    {
      "lang": "en",
      "text": "click on the link find this notebook and",
      "offset": 1707.76,
      "duration": 4.12
    },
    {
      "lang": "en",
      "text": "try to write that function yourself",
      "offset": 1709.919,
      "duration": 3.041
    },
    {
      "lang": "en",
      "text": "otherwise I'm going to come here and",
      "offset": 1711.88,
      "duration": 3.08
    },
    {
      "lang": "en",
      "text": "Implement first the function that finds",
      "offset": 1712.96,
      "duration": 3.959
    },
    {
      "lang": "en",
      "text": "the most common pair okay so here's what",
      "offset": 1714.96,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "I came up with there are many different",
      "offset": 1716.919,
      "duration": 3.401
    },
    {
      "lang": "en",
      "text": "ways to implement this but I'm calling",
      "offset": 1718.399,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "the function get stats it expects a list",
      "offset": 1720.32,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "of integers I'm using a dictionary to",
      "offset": 1722.159,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "keep track of basically the counts and",
      "offset": 1724.48,
      "duration": 4.36
    },
    {
      "lang": "en",
      "text": "then this is a pythonic way to iterate",
      "offset": 1726.88,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "consecutive elements of this list uh",
      "offset": 1728.84,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "which we covered in the previous video",
      "offset": 1731.44,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "and then here I'm just keeping track of",
      "offset": 1733.72,
      "duration": 4.839
    },
    {
      "lang": "en",
      "text": "just incrementing by one um for all the",
      "offset": 1735.919,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "pairs so if I call this on all the",
      "offset": 1738.559,
      "duration": 4.84
    },
    {
      "lang": "en",
      "text": "tokens here then the stats comes out",
      "offset": 1740.399,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "here so this is the dictionary the keys",
      "offset": 1743.399,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "are these topples of consecutive",
      "offset": 1746.159,
      "duration": 5.441
    },
    {
      "lang": "en",
      "text": "elements and this is the count so just",
      "offset": 1748.919,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "to uh print it in a slightly better way",
      "offset": 1751.6,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "this is one way that I like to do that",
      "offset": 1754.679,
      "duration": 5.88
    },
    {
      "lang": "en",
      "text": "where you it's a little bit compound",
      "offset": 1757.6,
      "duration": 4.76
    },
    {
      "lang": "en",
      "text": "here so you can pause if you like but we",
      "offset": 1760.559,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "iterate all all the items the items",
      "offset": 1762.36,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "called on dictionary returns pairs of",
      "offset": 1765.039,
      "duration": 6.76
    },
    {
      "lang": "en",
      "text": "key value and instead I create a list",
      "offset": 1767.399,
      "duration": 7.721
    },
    {
      "lang": "en",
      "text": "here of value key because if it's a",
      "offset": 1771.799,
      "duration": 5.48
    },
    {
      "lang": "en",
      "text": "value key list then I can call sort on",
      "offset": 1775.12,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "it and by default python will uh use the",
      "offset": 1777.279,
      "duration": 6.28
    },
    {
      "lang": "en",
      "text": "first element which in this case will be",
      "offset": 1781.36,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "value to sort by if it's given tles and",
      "offset": 1783.559,
      "duration": 5.161
    },
    {
      "lang": "en",
      "text": "then reverse so it's descending and",
      "offset": 1786.64,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "print that so basically it looks like",
      "offset": 1788.72,
      "duration": 5.24
    },
    {
      "lang": "en",
      "text": "101 comma 32 was the most commonly",
      "offset": 1790.88,
      "duration": 4.84
    },
    {
      "lang": "en",
      "text": "occurring consecutive pair and it",
      "offset": 1793.96,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "occurred 20 times we can double check",
      "offset": 1795.72,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "that that makes reasonable sense so if I",
      "offset": 1798.2,
      "duration": 3.88
    },
    {
      "lang": "en",
      "text": "just search",
      "offset": 1800.44,
      "duration": 4.76
    },
    {
      "lang": "en",
      "text": "10132 then you see that these are the 20",
      "offset": 1802.08,
      "duration": 8.04
    },
    {
      "lang": "en",
      "text": "occurrences of that um pair and if we'd",
      "offset": 1805.2,
      "duration": 6.319
    },
    {
      "lang": "en",
      "text": "like to take a look at what exactly that",
      "offset": 1810.12,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "pair is we can use Char which is the",
      "offset": 1811.519,
      "duration": 6.321
    },
    {
      "lang": "en",
      "text": "opposite of or in Python so we give it a",
      "offset": 1814.279,
      "duration": 7.76
    },
    {
      "lang": "en",
      "text": "um unic code Cod point so 101 and of 32",
      "offset": 1817.84,
      "duration": 7.16
    },
    {
      "lang": "en",
      "text": "and we see that this is e and space so",
      "offset": 1822.039,
      "duration": 6.041
    },
    {
      "lang": "en",
      "text": "basically there's a lot of E space here",
      "offset": 1825,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "meaning that a lot of these words seem",
      "offset": 1828.08,
      "duration": 4.04
    },
    {
      "lang": "en",
      "text": "to end with e so here's eace as an",
      "offset": 1829.48,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "example so there's a lot of that going",
      "offset": 1832.12,
      "duration": 4.6
    },
    {
      "lang": "en",
      "text": "on here and this is the most common pair",
      "offset": 1834.039,
      "duration": 4.201
    },
    {
      "lang": "en",
      "text": "so now that we've identified the most",
      "offset": 1836.72,
      "duration": 3.64
    },
    {
      "lang": "en",
      "text": "common pair we would like to iterate",
      "offset": 1838.24,
      "duration": 4.439
    },
    {
      "lang": "en",
      "text": "over this sequence we're going to Mint a",
      "offset": 1840.36,
      "duration": 4.439
    },
    {
      "lang": "en",
      "text": "new token with the ID of",
      "offset": 1842.679,
      "duration": 5.161
    },
    {
      "lang": "en",
      "text": "256 right because these tokens currently",
      "offset": 1844.799,
      "duration": 5.841
    },
    {
      "lang": "en",
      "text": "go from Z to 255 so when we create a new",
      "offset": 1847.84,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "token it will have an ID of",
      "offset": 1850.64,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "256 and we're going to iterate over this",
      "offset": 1852.84,
      "duration": 7
    },
    {
      "lang": "en",
      "text": "entire um list and every every time we",
      "offset": 1856,
      "duration": 6.72
    },
    {
      "lang": "en",
      "text": "see 101 comma 32 we're going to swap",
      "offset": 1859.84,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "that out for",
      "offset": 1862.72,
      "duration": 4.52
    },
    {
      "lang": "en",
      "text": "256 so let's Implement that now and feel",
      "offset": 1863.919,
      "duration": 6.041
    },
    {
      "lang": "en",
      "text": "free to uh do that yourself as well so",
      "offset": 1867.24,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "first I commented uh this just so we",
      "offset": 1869.96,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "don't pollute uh the notebook too much",
      "offset": 1871.96,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "this is a nice way of in Python",
      "offset": 1874.96,
      "duration": 5.439
    },
    {
      "lang": "en",
      "text": "obtaining the highest ranking pair so",
      "offset": 1877.96,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "we're basically calling the Max on this",
      "offset": 1880.399,
      "duration": 5.921
    },
    {
      "lang": "en",
      "text": "dictionary stats and this will return",
      "offset": 1883.08,
      "duration": 4.599
    },
    {
      "lang": "en",
      "text": "the maximum",
      "offset": 1886.32,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "key and then the question is how does it",
      "offset": 1887.679,
      "duration": 5.161
    },
    {
      "lang": "en",
      "text": "rank keys so you can provide it with a",
      "offset": 1890.159,
      "duration": 5.041
    },
    {
      "lang": "en",
      "text": "function that ranks keys and that",
      "offset": 1892.84,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "function is just stats. getet uh stats.",
      "offset": 1895.2,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "getet would basically return the value",
      "offset": 1898.2,
      "duration": 4.599
    },
    {
      "lang": "en",
      "text": "and so we're ranking by the value and",
      "offset": 1901.12,
      "duration": 4.36
    },
    {
      "lang": "en",
      "text": "getting the maximum key so it's 101",
      "offset": 1902.799,
      "duration": 6.401
    },
    {
      "lang": "en",
      "text": "comma 32 as we saw now to actually merge",
      "offset": 1905.48,
      "duration": 6.4
    },
    {
      "lang": "en",
      "text": "10132 um this is the function that I",
      "offset": 1909.2,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "wrote but again there are many different",
      "offset": 1911.88,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "versions of it so we're going to take a",
      "offset": 1913.279,
      "duration": 4.441
    },
    {
      "lang": "en",
      "text": "list of IDs and the the pair that we",
      "offset": 1915.72,
      "duration": 4.04
    },
    {
      "lang": "en",
      "text": "want to replace and that pair will be",
      "offset": 1917.72,
      "duration": 4.52
    },
    {
      "lang": "en",
      "text": "replaced with the new index",
      "offset": 1919.76,
      "duration": 5.799
    },
    {
      "lang": "en",
      "text": "idx so iterating through IDs if we find",
      "offset": 1922.24,
      "duration": 6.2
    },
    {
      "lang": "en",
      "text": "the pair swap it out for idx so we",
      "offset": 1925.559,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "create this new list and then we start",
      "offset": 1928.44,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "at zero and then we go through this",
      "offset": 1930.519,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "entire list sequentially from left to",
      "offset": 1932.76,
      "duration": 4.36
    },
    {
      "lang": "en",
      "text": "right and here we are checking for",
      "offset": 1934.84,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "equality at the current position with",
      "offset": 1937.12,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "the",
      "offset": 1939.639,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "pair um so here we are checking that the",
      "offset": 1940.88,
      "duration": 4.6
    },
    {
      "lang": "en",
      "text": "pair matches now here is a bit of a",
      "offset": 1943.399,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "tricky condition that you have to append",
      "offset": 1945.48,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "if you're trying to be careful and that",
      "offset": 1947.24,
      "duration": 4.439
    },
    {
      "lang": "en",
      "text": "is that um you don't want this here to",
      "offset": 1949.08,
      "duration": 4.68
    },
    {
      "lang": "en",
      "text": "be out of Bounds at the very last",
      "offset": 1951.679,
      "duration": 3.72
    },
    {
      "lang": "en",
      "text": "position when you're on the rightmost",
      "offset": 1953.76,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "element of this list otherwise this",
      "offset": 1955.399,
      "duration": 3.88
    },
    {
      "lang": "en",
      "text": "would uh give you an autof bounds error",
      "offset": 1957.12,
      "duration": 3.559
    },
    {
      "lang": "en",
      "text": "so we have to make sure that we're not",
      "offset": 1959.279,
      "duration": 4.76
    },
    {
      "lang": "en",
      "text": "at the very very last element so uh this",
      "offset": 1960.679,
      "duration": 5.921
    },
    {
      "lang": "en",
      "text": "would be false for that so if we find a",
      "offset": 1964.039,
      "duration": 7.041
    },
    {
      "lang": "en",
      "text": "match we append to this new list that",
      "offset": 1966.6,
      "duration": 6.72
    },
    {
      "lang": "en",
      "text": "replacement index and we increment the",
      "offset": 1971.08,
      "duration": 3.719
    },
    {
      "lang": "en",
      "text": "position by two so we skip over that",
      "offset": 1973.32,
      "duration": 3.8
    },
    {
      "lang": "en",
      "text": "entire pair but otherwise if we we",
      "offset": 1974.799,
      "duration": 4.281
    },
    {
      "lang": "en",
      "text": "haven't found a matching pair we just",
      "offset": 1977.12,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "sort of copy over the um element at that",
      "offset": 1979.08,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "position and increment by one then",
      "offset": 1982.12,
      "duration": 5.24
    },
    {
      "lang": "en",
      "text": "return this so here's a very small toy",
      "offset": 1985.24,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "example if we have a list 566 791 and we",
      "offset": 1987.36,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "want to replace the occurrences of 67",
      "offset": 1990.36,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "with 99 then calling this on that will",
      "offset": 1992.36,
      "duration": 6.559
    },
    {
      "lang": "en",
      "text": "give us what we're asking for so here",
      "offset": 1996.36,
      "duration": 5.159
    },
    {
      "lang": "en",
      "text": "the 67 is replaced with",
      "offset": 1998.919,
      "duration": 4.841
    },
    {
      "lang": "en",
      "text": "99 so now I'm going to uncomment this",
      "offset": 2001.519,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "for our actual use case where we want to",
      "offset": 2003.76,
      "duration": 5.759
    },
    {
      "lang": "en",
      "text": "take our tokens we want to take the top",
      "offset": 2007.279,
      "duration": 5.841
    },
    {
      "lang": "en",
      "text": "pair here and replace it with 256 to get",
      "offset": 2009.519,
      "duration": 7.721
    },
    {
      "lang": "en",
      "text": "tokens to if we run this we get the",
      "offset": 2013.12,
      "duration": 7.76
    },
    {
      "lang": "en",
      "text": "following so recall that previously we",
      "offset": 2017.24,
      "duration": 7.88
    },
    {
      "lang": "en",
      "text": "had a length 616 in this list and now we",
      "offset": 2020.88,
      "duration": 7.56
    },
    {
      "lang": "en",
      "text": "have a length 596 right so this",
      "offset": 2025.12,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "decreased by 20 which makes sense",
      "offset": 2028.44,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "because there are 20 occurrences",
      "offset": 2030.159,
      "duration": 5.321
    },
    {
      "lang": "en",
      "text": "moreover we can try to find 256 here and",
      "offset": 2032.36,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "we see plenty of occurrences on off it",
      "offset": 2035.48,
      "duration": 4.28
    },
    {
      "lang": "en",
      "text": "and moreover just double check there",
      "offset": 2038.44,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "should be no occurrence of 10132 so this",
      "offset": 2039.76,
      "duration": 5.24
    },
    {
      "lang": "en",
      "text": "is the original array plenty of them and",
      "offset": 2042.519,
      "duration": 3.64
    },
    {
      "lang": "en",
      "text": "in the second array there are no",
      "offset": 2045,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "occurrences of 1032 so we've",
      "offset": 2046.159,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "successfully merged this single pair and",
      "offset": 2048.52,
      "duration": 5.399
    },
    {
      "lang": "en",
      "text": "now we just uh iterate this so we are",
      "offset": 2051.599,
      "duration": 3.881
    },
    {
      "lang": "en",
      "text": "going to go over the sequence again find",
      "offset": 2053.919,
      "duration": 3.881
    },
    {
      "lang": "en",
      "text": "the most common pair and replace it so",
      "offset": 2055.48,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "let me now write a y Loop that uses",
      "offset": 2057.8,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "these functions to do this um sort of",
      "offset": 2059.48,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "iteratively and how many times do we do",
      "offset": 2061.8,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "it four well that's totally up to us as",
      "offset": 2064.28,
      "duration": 3.119
    },
    {
      "lang": "en",
      "text": "a hyper parameter",
      "offset": 2066.28,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "the more um steps we take the larger",
      "offset": 2067.399,
      "duration": 5.641
    },
    {
      "lang": "en",
      "text": "will be our vocabulary and the shorter",
      "offset": 2070.919,
      "duration": 4.2
    },
    {
      "lang": "en",
      "text": "will be our sequence and there is some",
      "offset": 2073.04,
      "duration": 4.2
    },
    {
      "lang": "en",
      "text": "sweet spot that we usually find works",
      "offset": 2075.119,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "the best in practice and so this is kind",
      "offset": 2077.24,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "of a hyperparameter and we tune it and",
      "offset": 2079.919,
      "duration": 4.281
    },
    {
      "lang": "en",
      "text": "we find good vocabulary sizes as an",
      "offset": 2081.639,
      "duration": 4.361
    },
    {
      "lang": "en",
      "text": "example gp4 currently uses roughly",
      "offset": 2084.2,
      "duration": 5.679
    },
    {
      "lang": "en",
      "text": "100,000 tokens and um bpark that those",
      "offset": 2086,
      "duration": 5.8
    },
    {
      "lang": "en",
      "text": "are reasonable numbers currently instead",
      "offset": 2089.879,
      "duration": 4.04
    },
    {
      "lang": "en",
      "text": "the are large language models so let me",
      "offset": 2091.8,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "now write uh putting putting it all",
      "offset": 2093.919,
      "duration": 4.761
    },
    {
      "lang": "en",
      "text": "together and uh iterating these steps",
      "offset": 2095.96,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "okay now before we dive into the Y loop",
      "offset": 2098.68,
      "duration": 4.6
    },
    {
      "lang": "en",
      "text": "I wanted to add one more cell here where",
      "offset": 2100.52,
      "duration": 4.44
    },
    {
      "lang": "en",
      "text": "I went to the block post and instead of",
      "offset": 2103.28,
      "duration": 3.72
    },
    {
      "lang": "en",
      "text": "grabbing just the first paragraph or two",
      "offset": 2104.96,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "I took the entire block post and I",
      "offset": 2107,
      "duration": 3.96
    },
    {
      "lang": "en",
      "text": "stretched it out in a single line and",
      "offset": 2108.8,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "basically just using longer text will",
      "offset": 2110.96,
      "duration": 2.92
    },
    {
      "lang": "en",
      "text": "allow us to have more representative",
      "offset": 2112.48,
      "duration": 3.8
    },
    {
      "lang": "en",
      "text": "statistics for the bite Pairs and we'll",
      "offset": 2113.88,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "just get a more sensible results out of",
      "offset": 2116.28,
      "duration": 5.48
    },
    {
      "lang": "en",
      "text": "it because it's longer text um so here",
      "offset": 2118.04,
      "duration": 6.319
    },
    {
      "lang": "en",
      "text": "we have the raw text we encode it into",
      "offset": 2121.76,
      "duration": 5.88
    },
    {
      "lang": "en",
      "text": "bytes using the utf8 encoding",
      "offset": 2124.359,
      "duration": 5.72
    },
    {
      "lang": "en",
      "text": "and then here as before we are just",
      "offset": 2127.64,
      "duration": 4.199
    },
    {
      "lang": "en",
      "text": "changing it into a list of integers in",
      "offset": 2130.079,
      "duration": 3.881
    },
    {
      "lang": "en",
      "text": "Python just so it's easier to work with",
      "offset": 2131.839,
      "duration": 4.841
    },
    {
      "lang": "en",
      "text": "instead of the raw byes objects and then",
      "offset": 2133.96,
      "duration": 6.8
    },
    {
      "lang": "en",
      "text": "this is the code that I came up with uh",
      "offset": 2136.68,
      "duration": 7.32
    },
    {
      "lang": "en",
      "text": "to actually do the merging in Loop these",
      "offset": 2140.76,
      "duration": 5.079
    },
    {
      "lang": "en",
      "text": "two functions here are identical to what",
      "offset": 2144,
      "duration": 4.119
    },
    {
      "lang": "en",
      "text": "we had above I only included them here",
      "offset": 2145.839,
      "duration": 4.041
    },
    {
      "lang": "en",
      "text": "just so that you have the point of",
      "offset": 2148.119,
      "duration": 5.24
    },
    {
      "lang": "en",
      "text": "reference here so uh these two are",
      "offset": 2149.88,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "identical and then this is the new code",
      "offset": 2153.359,
      "duration": 3.72
    },
    {
      "lang": "en",
      "text": "that I added so the first first thing we",
      "offset": 2155,
      "duration": 3.56
    },
    {
      "lang": "en",
      "text": "want to do is we want to decide on the",
      "offset": 2157.079,
      "duration": 3.961
    },
    {
      "lang": "en",
      "text": "final vocabulary size that we want our",
      "offset": 2158.56,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "tokenizer to have and as I mentioned",
      "offset": 2161.04,
      "duration": 3.48
    },
    {
      "lang": "en",
      "text": "this is a hyper parameter and you set it",
      "offset": 2162.96,
      "duration": 3.48
    },
    {
      "lang": "en",
      "text": "in some way depending on your best",
      "offset": 2164.52,
      "duration": 3.96
    },
    {
      "lang": "en",
      "text": "performance so let's say for us we're",
      "offset": 2166.44,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "going to use 276 because that way we're",
      "offset": 2168.48,
      "duration": 4.599
    },
    {
      "lang": "en",
      "text": "going to be doing exactly 20",
      "offset": 2170.839,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "merges and uh 20 merges because we",
      "offset": 2173.079,
      "duration": 3.801
    },
    {
      "lang": "en",
      "text": "already have",
      "offset": 2175.72,
      "duration": 5.16
    },
    {
      "lang": "en",
      "text": "256 tokens for the raw bytes and to",
      "offset": 2176.88,
      "duration": 6.8
    },
    {
      "lang": "en",
      "text": "reach 276 we have to do 20 merges uh to",
      "offset": 2180.88,
      "duration": 4.6
    },
    {
      "lang": "en",
      "text": "add 20 new",
      "offset": 2183.68,
      "duration": 4.52
    },
    {
      "lang": "en",
      "text": "tokens here uh this is uh one way in",
      "offset": 2185.48,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "Python to just create a copy of a list",
      "offset": 2188.2,
      "duration": 5.32
    },
    {
      "lang": "en",
      "text": "so I'm taking the tokens list and by",
      "offset": 2191.48,
      "duration": 4.359
    },
    {
      "lang": "en",
      "text": "wrapping it in a list python will",
      "offset": 2193.52,
      "duration": 3.64
    },
    {
      "lang": "en",
      "text": "construct a new list of all the",
      "offset": 2195.839,
      "duration": 2.801
    },
    {
      "lang": "en",
      "text": "individual elements so this is just a",
      "offset": 2197.16,
      "duration": 2.76
    },
    {
      "lang": "en",
      "text": "copy",
      "offset": 2198.64,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "operation then here I'm creating a",
      "offset": 2199.92,
      "duration": 4.919
    },
    {
      "lang": "en",
      "text": "merges uh dictionary so this merges",
      "offset": 2202.079,
      "duration": 4.04
    },
    {
      "lang": "en",
      "text": "dictionary is going to maintain",
      "offset": 2204.839,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "basically the child one child two",
      "offset": 2206.119,
      "duration": 6.401
    },
    {
      "lang": "en",
      "text": "mapping to a new uh token and so what",
      "offset": 2209.4,
      "duration": 4.52
    },
    {
      "lang": "en",
      "text": "we're going to be building up here is a",
      "offset": 2212.52,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "binary tree of merges but actually it's",
      "offset": 2213.92,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "not exactly a tree because a tree would",
      "offset": 2216.92,
      "duration": 4.52
    },
    {
      "lang": "en",
      "text": "have a single root node with a bunch of",
      "offset": 2219.28,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "leaves for us we're starting with the",
      "offset": 2221.44,
      "duration": 3.56
    },
    {
      "lang": "en",
      "text": "leaves on the bottom which are the",
      "offset": 2223.44,
      "duration": 3.48
    },
    {
      "lang": "en",
      "text": "individual bites those are the starting",
      "offset": 2225,
      "duration": 4.52
    },
    {
      "lang": "en",
      "text": "256 tokens and then we're starting to",
      "offset": 2226.92,
      "duration": 4.6
    },
    {
      "lang": "en",
      "text": "like merge two of them at a time and so",
      "offset": 2229.52,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "it's not a tree it's more like a forest",
      "offset": 2231.52,
      "duration": 7.4
    },
    {
      "lang": "en",
      "text": "um uh as we merge these elements",
      "offset": 2234.96,
      "duration": 7.92
    },
    {
      "lang": "en",
      "text": "so for 20 merges we're going to find the",
      "offset": 2238.92,
      "duration": 6.159
    },
    {
      "lang": "en",
      "text": "most commonly occurring pair we're going",
      "offset": 2242.88,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "to Mint a new token integer for it so I",
      "offset": 2245.079,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "here will start at zero so we'll going",
      "offset": 2248.48,
      "duration": 3.879
    },
    {
      "lang": "en",
      "text": "to start at 256 we're going to print",
      "offset": 2250.079,
      "duration": 4.361
    },
    {
      "lang": "en",
      "text": "that we're merging it and we're going to",
      "offset": 2252.359,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "replace all of the occurrences of that",
      "offset": 2254.44,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "pair with the new new lied token and",
      "offset": 2256.2,
      "duration": 5.96
    },
    {
      "lang": "en",
      "text": "we're going to record that this pair of",
      "offset": 2259.56,
      "duration": 5.96
    },
    {
      "lang": "en",
      "text": "integers merged into this new",
      "offset": 2262.16,
      "duration": 6.919
    },
    {
      "lang": "en",
      "text": "integer so running this gives us the",
      "offset": 2265.52,
      "duration": 5.64
    },
    {
      "lang": "en",
      "text": "following",
      "offset": 2269.079,
      "duration": 5.401
    },
    {
      "lang": "en",
      "text": "output so we did 20 merges and for",
      "offset": 2271.16,
      "duration": 5.679
    },
    {
      "lang": "en",
      "text": "example the first merge was exactly as",
      "offset": 2274.48,
      "duration": 4.359
    },
    {
      "lang": "en",
      "text": "before the",
      "offset": 2276.839,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "10132 um tokens merging into a new token",
      "offset": 2278.839,
      "duration": 5.161
    },
    {
      "lang": "en",
      "text": "2556 now keep in mind that the",
      "offset": 2281.8,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "individual uh tokens 101 and 32 can",
      "offset": 2284,
      "duration": 4.44
    },
    {
      "lang": "en",
      "text": "still occur in the sequence after",
      "offset": 2286.599,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "merging it's only when they occur",
      "offset": 2288.44,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "exactly consecutively that that becomes",
      "offset": 2290.359,
      "duration": 3.521
    },
    {
      "lang": "en",
      "text": "256",
      "offset": 2292.599,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "now um and in particular the other thing",
      "offset": 2293.88,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "to notice here is that the token 256",
      "offset": 2296.92,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "which is the newly minted token is also",
      "offset": 2299.16,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "eligible for merging so here on the",
      "offset": 2301.4,
      "duration": 5.439
    },
    {
      "lang": "en",
      "text": "bottom the 20th merge was a merge of 25",
      "offset": 2303.4,
      "duration": 5.48
    },
    {
      "lang": "en",
      "text": "and 259 becoming",
      "offset": 2306.839,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "275 so every time we replace these",
      "offset": 2308.88,
      "duration": 4.76
    },
    {
      "lang": "en",
      "text": "tokens they become eligible for merging",
      "offset": 2311.8,
      "duration": 4.12
    },
    {
      "lang": "en",
      "text": "in the next round of data ration so",
      "offset": 2313.64,
      "duration": 3.479
    },
    {
      "lang": "en",
      "text": "that's why we're building up a small",
      "offset": 2315.92,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "sort of binary Forest instead of a",
      "offset": 2317.119,
      "duration": 3.081
    },
    {
      "lang": "en",
      "text": "single individual",
      "offset": 2318.8,
      "duration": 3.519
    },
    {
      "lang": "en",
      "text": "tree one thing we can take a look at as",
      "offset": 2320.2,
      "duration": 3.8
    },
    {
      "lang": "en",
      "text": "well is we can take a look at the",
      "offset": 2322.319,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "compression ratio that we've achieved so",
      "offset": 2324,
      "duration": 4.359
    },
    {
      "lang": "en",
      "text": "in particular we started off with this",
      "offset": 2326.16,
      "duration": 5.24
    },
    {
      "lang": "en",
      "text": "tokens list um so we started off with",
      "offset": 2328.359,
      "duration": 7.921
    },
    {
      "lang": "en",
      "text": "24,000 bytes and after merging 20 times",
      "offset": 2331.4,
      "duration": 7.12
    },
    {
      "lang": "en",
      "text": "uh we now have only",
      "offset": 2336.28,
      "duration": 5.64
    },
    {
      "lang": "en",
      "text": "19,000 um tokens and so therefore the",
      "offset": 2338.52,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "compression ratio simply just dividing",
      "offset": 2341.92,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "the two is roughly 1.27 so that's the",
      "offset": 2343.64,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "amount of compression we were able to",
      "offset": 2346.8,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "achieve of this text with only 20",
      "offset": 2347.96,
      "duration": 5.159
    },
    {
      "lang": "en",
      "text": "merges um and of course the more",
      "offset": 2350.8,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "vocabulary elements you add uh the",
      "offset": 2353.119,
      "duration": 6.121
    },
    {
      "lang": "en",
      "text": "greater the compression ratio here would",
      "offset": 2355.599,
      "duration": 8.161
    },
    {
      "lang": "en",
      "text": "be finally so that's kind of like um the",
      "offset": 2359.24,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "training of the tokenizer if you will",
      "offset": 2363.76,
      "duration": 4.52
    },
    {
      "lang": "en",
      "text": "now 1 Point I wanted to make is that and",
      "offset": 2365.72,
      "duration": 5.56
    },
    {
      "lang": "en",
      "text": "maybe this is a diagram that can help um",
      "offset": 2368.28,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "kind of illustrate is that tokenizer is",
      "offset": 2371.28,
      "duration": 3.64
    },
    {
      "lang": "en",
      "text": "a completely separate object from the",
      "offset": 2373.079,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "large language model itself so",
      "offset": 2374.92,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "everything in this lecture we're not",
      "offset": 2377,
      "duration": 3.119
    },
    {
      "lang": "en",
      "text": "really touching the llm itself uh we're",
      "offset": 2378.04,
      "duration": 3.799
    },
    {
      "lang": "en",
      "text": "just training the tokenizer this is a",
      "offset": 2380.119,
      "duration": 3.801
    },
    {
      "lang": "en",
      "text": "completely separate pre-processing stage",
      "offset": 2381.839,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "usually so the tokenizer will have its",
      "offset": 2383.92,
      "duration": 4.04
    },
    {
      "lang": "en",
      "text": "own training set just like a large",
      "offset": 2386.24,
      "duration": 3.56
    },
    {
      "lang": "en",
      "text": "language model has a potentially",
      "offset": 2387.96,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "different training set so the tokenizer",
      "offset": 2389.8,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "has a training set of documents on which",
      "offset": 2392.04,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "you're going to train the",
      "offset": 2393.4,
      "duration": 4.36
    },
    {
      "lang": "en",
      "text": "tokenizer and then and um we're",
      "offset": 2394.76,
      "duration": 4.2
    },
    {
      "lang": "en",
      "text": "performing The Bite pair encoding",
      "offset": 2397.76,
      "duration": 3.319
    },
    {
      "lang": "en",
      "text": "algorithm as we saw above to train the",
      "offset": 2398.96,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "vocabulary of this",
      "offset": 2401.079,
      "duration": 3.881
    },
    {
      "lang": "en",
      "text": "tokenizer so it has its own training set",
      "offset": 2402.64,
      "duration": 3.88
    },
    {
      "lang": "en",
      "text": "it is a pre-processing stage that you",
      "offset": 2404.96,
      "duration": 4.28
    },
    {
      "lang": "en",
      "text": "would run a single time in the beginning",
      "offset": 2406.52,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "um and the tokenizer is trained using",
      "offset": 2409.24,
      "duration": 5.119
    },
    {
      "lang": "en",
      "text": "bipar coding algorithm once you have the",
      "offset": 2411.96,
      "duration": 4.359
    },
    {
      "lang": "en",
      "text": "tokenizer once it's trained and you have",
      "offset": 2414.359,
      "duration": 4.681
    },
    {
      "lang": "en",
      "text": "the vocabulary and you have the merges",
      "offset": 2416.319,
      "duration": 5.961
    },
    {
      "lang": "en",
      "text": "uh we can do both encoding and decoding",
      "offset": 2419.04,
      "duration": 5.48
    },
    {
      "lang": "en",
      "text": "so these two arrows here so the",
      "offset": 2422.28,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "tokenizer is a translation layer between",
      "offset": 2424.52,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "raw text which is as we saw the sequence",
      "offset": 2427,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "of Unicode code points it can take raw",
      "offset": 2430.04,
      "duration": 5.4
    },
    {
      "lang": "en",
      "text": "text and turn it into a token sequence",
      "offset": 2432.52,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "and vice versa it can take a token",
      "offset": 2435.44,
      "duration": 5.32
    },
    {
      "lang": "en",
      "text": "sequence and translate it back into raw",
      "offset": 2437,
      "duration": 6.359
    },
    {
      "lang": "en",
      "text": "text so now that we have trained uh",
      "offset": 2440.76,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "tokenizer and we have these merges we",
      "offset": 2443.359,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "are going to turn to how we can do the",
      "offset": 2445.96,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "encoding and the decoding step if you",
      "offset": 2447.44,
      "duration": 3.8
    },
    {
      "lang": "en",
      "text": "give me text here are the tokens and",
      "offset": 2449.48,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "vice versa if you give me tokens here's",
      "offset": 2451.24,
      "duration": 4.04
    },
    {
      "lang": "en",
      "text": "the text once we have that we can",
      "offset": 2453,
      "duration": 4.52
    },
    {
      "lang": "en",
      "text": "translate between these two Realms and",
      "offset": 2455.28,
      "duration": 3.48
    },
    {
      "lang": "en",
      "text": "then the language model is going to be",
      "offset": 2457.52,
      "duration": 4.12
    },
    {
      "lang": "en",
      "text": "trained as a step two afterwards and",
      "offset": 2458.76,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "typically in a in a sort of a",
      "offset": 2461.64,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "state-of-the-art application you might",
      "offset": 2463.64,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "take all of your training data for the",
      "offset": 2465.48,
      "duration": 2.879
    },
    {
      "lang": "en",
      "text": "language model and you might run it",
      "offset": 2466.839,
      "duration": 3.561
    },
    {
      "lang": "en",
      "text": "through the tokenizer and sort of",
      "offset": 2468.359,
      "duration": 3.561
    },
    {
      "lang": "en",
      "text": "translate everything into a massive",
      "offset": 2470.4,
      "duration": 3.24
    },
    {
      "lang": "en",
      "text": "token sequence and then you can throw",
      "offset": 2471.92,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "away the raw text you're just left with",
      "offset": 2473.64,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "the tokens themselves and those are",
      "offset": 2475.44,
      "duration": 4.28
    },
    {
      "lang": "en",
      "text": "stored on disk and that is what the",
      "offset": 2477.72,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "large language model is actually reading",
      "offset": 2479.72,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "when it's training on them so this one",
      "offset": 2481.319,
      "duration": 3.481
    },
    {
      "lang": "en",
      "text": "approach that you can take as a single",
      "offset": 2483.24,
      "duration": 3.64
    },
    {
      "lang": "en",
      "text": "massive pre-processing step a",
      "offset": 2484.8,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "stage um so yeah basically I think the",
      "offset": 2486.88,
      "duration": 4.52
    },
    {
      "lang": "en",
      "text": "most important thing I want to get",
      "offset": 2490.4,
      "duration": 2.199
    },
    {
      "lang": "en",
      "text": "across is that this is completely",
      "offset": 2491.4,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "separate stage it usually has its own",
      "offset": 2492.599,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "entire uh training set you may want to",
      "offset": 2494.4,
      "duration": 3.959
    },
    {
      "lang": "en",
      "text": "have those training sets be different",
      "offset": 2496.839,
      "duration": 2.76
    },
    {
      "lang": "en",
      "text": "between the tokenizer and the logge",
      "offset": 2498.359,
      "duration": 2.921
    },
    {
      "lang": "en",
      "text": "language model so for example when",
      "offset": 2499.599,
      "duration": 3.72
    },
    {
      "lang": "en",
      "text": "you're training the tokenizer as I",
      "offset": 2501.28,
      "duration": 3.799
    },
    {
      "lang": "en",
      "text": "mentioned we don't just care about the",
      "offset": 2503.319,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "performance of English text we care",
      "offset": 2505.079,
      "duration": 4.361
    },
    {
      "lang": "en",
      "text": "about uh multi many different languages",
      "offset": 2506.76,
      "duration": 4.76
    },
    {
      "lang": "en",
      "text": "and we also care about code or not code",
      "offset": 2509.44,
      "duration": 3.8
    },
    {
      "lang": "en",
      "text": "so you may want to look into different",
      "offset": 2511.52,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "kinds of mixtures of different kinds of",
      "offset": 2513.24,
      "duration": 4.119
    },
    {
      "lang": "en",
      "text": "languages and different amounts of code",
      "offset": 2515.2,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "and things like that because the amount",
      "offset": 2517.359,
      "duration": 4.601
    },
    {
      "lang": "en",
      "text": "of different language that you have in",
      "offset": 2520.24,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "your tokenizer training set will",
      "offset": 2521.96,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "determine how many merges of it there",
      "offset": 2523.76,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "will be and therefore that determines",
      "offset": 2526.119,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "the density with which uh this type of",
      "offset": 2528.24,
      "duration": 6.96
    },
    {
      "lang": "en",
      "text": "data is um sort of has in the token",
      "offset": 2531.319,
      "duration": 6.441
    },
    {
      "lang": "en",
      "text": "space and so roughly speaking",
      "offset": 2535.2,
      "duration": 4.52
    },
    {
      "lang": "en",
      "text": "intuitively if you add some amount of",
      "offset": 2537.76,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "data like say you have a ton of Japanese",
      "offset": 2539.72,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "data in your uh tokenizer training set",
      "offset": 2541.359,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "then that means that more Japanese",
      "offset": 2544.04,
      "duration": 2.799
    },
    {
      "lang": "en",
      "text": "tokens will get merged",
      "offset": 2545.359,
      "duration": 3.561
    },
    {
      "lang": "en",
      "text": "and therefore Japanese will have shorter",
      "offset": 2546.839,
      "duration": 3.801
    },
    {
      "lang": "en",
      "text": "sequences uh and that's going to be",
      "offset": 2548.92,
      "duration": 3.48
    },
    {
      "lang": "en",
      "text": "beneficial for the large language model",
      "offset": 2550.64,
      "duration": 3.719
    },
    {
      "lang": "en",
      "text": "which has a finite context length on",
      "offset": 2552.4,
      "duration": 4.199
    },
    {
      "lang": "en",
      "text": "which it can work on in in the token",
      "offset": 2554.359,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "space uh so hopefully that makes sense",
      "offset": 2556.599,
      "duration": 4.601
    },
    {
      "lang": "en",
      "text": "so we're now going to turn to encoding",
      "offset": 2559.24,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "and decoding now that we have trained a",
      "offset": 2561.2,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "tokenizer so we have our merges and now",
      "offset": 2563.079,
      "duration": 5.361
    },
    {
      "lang": "en",
      "text": "how do we do encoding and decoding okay",
      "offset": 2566.4,
      "duration": 4.04
    },
    {
      "lang": "en",
      "text": "so let's begin with decoding which is",
      "offset": 2568.44,
      "duration": 4.28
    },
    {
      "lang": "en",
      "text": "this Arrow over here so given a token",
      "offset": 2570.44,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "sequence let's go through the tokenizer",
      "offset": 2572.72,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "to get back a python string object so",
      "offset": 2574.92,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "the raw text so this is the function",
      "offset": 2577.52,
      "duration": 4.36
    },
    {
      "lang": "en",
      "text": "that we' like to implement um we're",
      "offset": 2579.88,
      "duration": 3.56
    },
    {
      "lang": "en",
      "text": "given the list of integers and we want",
      "offset": 2581.88,
      "duration": 3.8
    },
    {
      "lang": "en",
      "text": "to return a python string if you'd like",
      "offset": 2583.44,
      "duration": 3.399
    },
    {
      "lang": "en",
      "text": "uh try to implement this function",
      "offset": 2585.68,
      "duration": 3.159
    },
    {
      "lang": "en",
      "text": "yourself it's a fun exercise otherwise",
      "offset": 2586.839,
      "duration": 4.441
    },
    {
      "lang": "en",
      "text": "I'm going to start uh pasting in my own",
      "offset": 2588.839,
      "duration": 4.681
    },
    {
      "lang": "en",
      "text": "solution so there are many different",
      "offset": 2591.28,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "ways to do it um here's one way I will",
      "offset": 2593.52,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "create an uh kind of pre-processing",
      "offset": 2596.88,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "variable that I will call",
      "offset": 2598.88,
      "duration": 5.8
    },
    {
      "lang": "en",
      "text": "vocab and vocab is a mapping or a",
      "offset": 2601.04,
      "duration": 6.519
    },
    {
      "lang": "en",
      "text": "dictionary in Python for from the token",
      "offset": 2604.68,
      "duration": 6.84
    },
    {
      "lang": "en",
      "text": "uh ID to the bytes object for that token",
      "offset": 2607.559,
      "duration": 6.241
    },
    {
      "lang": "en",
      "text": "so we begin with the raw bytes for",
      "offset": 2611.52,
      "duration": 5.319
    },
    {
      "lang": "en",
      "text": "tokens from 0 to 255 and then we go in",
      "offset": 2613.8,
      "duration": 5.96
    },
    {
      "lang": "en",
      "text": "order of all the merges and we sort of",
      "offset": 2616.839,
      "duration": 5.441
    },
    {
      "lang": "en",
      "text": "uh populate this vocab list by doing an",
      "offset": 2619.76,
      "duration": 5.96
    },
    {
      "lang": "en",
      "text": "addition here so this is the basically",
      "offset": 2622.28,
      "duration": 5.48
    },
    {
      "lang": "en",
      "text": "the bytes representation of the first",
      "offset": 2625.72,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "child followed by the second one and",
      "offset": 2627.76,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "remember these are bytes objects so this",
      "offset": 2630.04,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "addition here is an addition of two",
      "offset": 2632.079,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "bytes objects just concatenation",
      "offset": 2634.2,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "so that's what we get",
      "offset": 2637.04,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "here one tricky thing to be careful with",
      "offset": 2638.76,
      "duration": 4.12
    },
    {
      "lang": "en",
      "text": "by the way is that I'm iterating a",
      "offset": 2641.2,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "dictionary in Python using a DOT items",
      "offset": 2642.88,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "and uh it really matters that this runs",
      "offset": 2646,
      "duration": 5.48
    },
    {
      "lang": "en",
      "text": "in the order in which we inserted items",
      "offset": 2648.72,
      "duration": 4.839
    },
    {
      "lang": "en",
      "text": "into the merous dictionary luckily",
      "offset": 2651.48,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "starting with python 3.7 this is",
      "offset": 2653.559,
      "duration": 3.481
    },
    {
      "lang": "en",
      "text": "guaranteed to be the case but before",
      "offset": 2655.4,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "python 3.7 this iteration may have been",
      "offset": 2657.04,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "out of order with respect to how we",
      "offset": 2659.16,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "inserted elements into merges and this",
      "offset": 2660.96,
      "duration": 4.84
    },
    {
      "lang": "en",
      "text": "may not have worked but we are using an",
      "offset": 2663.16,
      "duration": 5.64
    },
    {
      "lang": "en",
      "text": "um modern python so we're okay and then",
      "offset": 2665.8,
      "duration": 5.799
    },
    {
      "lang": "en",
      "text": "here uh given the IDS the first thing",
      "offset": 2668.8,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "we're going to do is get the",
      "offset": 2671.599,
      "duration": 5.641
    },
    {
      "lang": "en",
      "text": "tokens so the way I implemented this",
      "offset": 2675.04,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "here is I'm taking I'm iterating over",
      "offset": 2677.24,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "all the IDS I'm using vocap to look up",
      "offset": 2679.599,
      "duration": 4.52
    },
    {
      "lang": "en",
      "text": "their bytes and then here this is one",
      "offset": 2681.88,
      "duration": 4.76
    },
    {
      "lang": "en",
      "text": "way in Python to concatenate all these",
      "offset": 2684.119,
      "duration": 5.601
    },
    {
      "lang": "en",
      "text": "bytes together to create our tokens and",
      "offset": 2686.64,
      "duration": 5.08
    },
    {
      "lang": "en",
      "text": "then these tokens here at this point are",
      "offset": 2689.72,
      "duration": 6.28
    },
    {
      "lang": "en",
      "text": "raw bytes so I have to decode using UTF",
      "offset": 2691.72,
      "duration": 7.48
    },
    {
      "lang": "en",
      "text": "F now back into python strings so",
      "offset": 2696,
      "duration": 5.16
    },
    {
      "lang": "en",
      "text": "previously we called that encode on a",
      "offset": 2699.2,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "string object to get the bytes and now",
      "offset": 2701.16,
      "duration": 4.04
    },
    {
      "lang": "en",
      "text": "we're doing it Opposite we're taking the",
      "offset": 2703.2,
      "duration": 4.6
    },
    {
      "lang": "en",
      "text": "bytes and calling a decode on the bytes",
      "offset": 2705.2,
      "duration": 5.8
    },
    {
      "lang": "en",
      "text": "object to get a string in Python and",
      "offset": 2707.8,
      "duration": 5.519
    },
    {
      "lang": "en",
      "text": "then we can return",
      "offset": 2711,
      "duration": 5.96
    },
    {
      "lang": "en",
      "text": "text so um this is how we can do it now",
      "offset": 2713.319,
      "duration": 7.481
    },
    {
      "lang": "en",
      "text": "this actually has a um issue um in the",
      "offset": 2716.96,
      "duration": 5.159
    },
    {
      "lang": "en",
      "text": "way I implemented it and this could",
      "offset": 2720.8,
      "duration": 3.319
    },
    {
      "lang": "en",
      "text": "actually throw an error so try to think",
      "offset": 2722.119,
      "duration": 4.361
    },
    {
      "lang": "en",
      "text": "figure out why this code could actually",
      "offset": 2724.119,
      "duration": 6.121
    },
    {
      "lang": "en",
      "text": "result in an error if we plug in um uh",
      "offset": 2726.48,
      "duration": 6.119
    },
    {
      "lang": "en",
      "text": "some sequence of IDs that is",
      "offset": 2730.24,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "unlucky so let me demonstrate the issue",
      "offset": 2732.599,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "when I try to decode just something like",
      "offset": 2735.24,
      "duration": 5.839
    },
    {
      "lang": "en",
      "text": "97 I am going to get letter A here back",
      "offset": 2737.16,
      "duration": 7.24
    },
    {
      "lang": "en",
      "text": "so nothing too crazy happening but when",
      "offset": 2741.079,
      "duration": 7.161
    },
    {
      "lang": "en",
      "text": "I try to decode 128 as a single element",
      "offset": 2744.4,
      "duration": 6.919
    },
    {
      "lang": "en",
      "text": "the token 128 is what in string or in",
      "offset": 2748.24,
      "duration": 6.879
    },
    {
      "lang": "en",
      "text": "Python object uni Cod decoder utfa can't",
      "offset": 2751.319,
      "duration": 8.8
    },
    {
      "lang": "en",
      "text": "Decode by um 0x8 which is this in HEX in",
      "offset": 2755.119,
      "duration": 6.801
    },
    {
      "lang": "en",
      "text": "position zero invalid start bite what",
      "offset": 2760.119,
      "duration": 3.521
    },
    {
      "lang": "en",
      "text": "does that mean well to understand what",
      "offset": 2761.92,
      "duration": 2.84
    },
    {
      "lang": "en",
      "text": "this means we have to go back to our",
      "offset": 2763.64,
      "duration": 4.28
    },
    {
      "lang": "en",
      "text": "utf8 page uh that I briefly showed",
      "offset": 2764.76,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "earlier and this is Wikipedia utf8 and",
      "offset": 2767.92,
      "duration": 5.639
    },
    {
      "lang": "en",
      "text": "basically there's a specific schema that",
      "offset": 2770.76,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "utfa bytes take so in particular if you",
      "offset": 2773.559,
      "duration": 6.28
    },
    {
      "lang": "en",
      "text": "have a multi-te object for some of the",
      "offset": 2776.92,
      "duration": 4.6
    },
    {
      "lang": "en",
      "text": "Unicode characters they have to have",
      "offset": 2779.839,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "this special sort of envelope in how the",
      "offset": 2781.52,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "encoding works and so what's happening",
      "offset": 2784.16,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "here is that invalid start pite that's",
      "offset": 2786.52,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "because",
      "offset": 2790,
      "duration": 3.88
    },
    {
      "lang": "en",
      "text": "128 the binary representation of it is",
      "offset": 2791,
      "duration": 6.359
    },
    {
      "lang": "en",
      "text": "one followed by all zeros so we have one",
      "offset": 2793.88,
      "duration": 5.679
    },
    {
      "lang": "en",
      "text": "and then all zero and we see here that",
      "offset": 2797.359,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "that doesn't conform to the format",
      "offset": 2799.559,
      "duration": 3.121
    },
    {
      "lang": "en",
      "text": "because one followed by all zero just",
      "offset": 2801.04,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "doesn't fit any of these rules so to",
      "offset": 2802.68,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "speak so it's an invalid start bite",
      "offset": 2804.96,
      "duration": 5.639
    },
    {
      "lang": "en",
      "text": "which is byte one this one must have a",
      "offset": 2807.64,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "one following it and then a zero",
      "offset": 2810.599,
      "duration": 3.881
    },
    {
      "lang": "en",
      "text": "following it and then the content of",
      "offset": 2812.76,
      "duration": 4.92
    },
    {
      "lang": "en",
      "text": "your uni codee in x here so basically we",
      "offset": 2814.48,
      "duration": 5.48
    },
    {
      "lang": "en",
      "text": "don't um exactly follow the utf8",
      "offset": 2817.68,
      "duration": 4.84
    },
    {
      "lang": "en",
      "text": "standard and this cannot be decoded and",
      "offset": 2819.96,
      "duration": 6.32
    },
    {
      "lang": "en",
      "text": "so the way to fix this um is to",
      "offset": 2822.52,
      "duration": 8.52
    },
    {
      "lang": "en",
      "text": "use this errors equals in bytes. decode",
      "offset": 2826.28,
      "duration": 7.559
    },
    {
      "lang": "en",
      "text": "function of python and by default errors",
      "offset": 2831.04,
      "duration": 6.12
    },
    {
      "lang": "en",
      "text": "is strict so we will throw an error if",
      "offset": 2833.839,
      "duration": 6.441
    },
    {
      "lang": "en",
      "text": "um it's not valid utf8 bytes encoding",
      "offset": 2837.16,
      "duration": 4.52
    },
    {
      "lang": "en",
      "text": "but there are many different things that",
      "offset": 2840.28,
      "duration": 3.4
    },
    {
      "lang": "en",
      "text": "you could put here on error handling",
      "offset": 2841.68,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "this is the full list of all the errors",
      "offset": 2843.68,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "that you can use and in particular",
      "offset": 2845.359,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "instead of strict let's change it to",
      "offset": 2847.359,
      "duration": 4.921
    },
    {
      "lang": "en",
      "text": "replace and that will replace uh with",
      "offset": 2849.359,
      "duration": 6.441
    },
    {
      "lang": "en",
      "text": "this special marker this replacement",
      "offset": 2852.28,
      "duration": 8.24
    },
    {
      "lang": "en",
      "text": "character so errors equals replace and",
      "offset": 2855.8,
      "duration": 7.36
    },
    {
      "lang": "en",
      "text": "now we just get that character",
      "offset": 2860.52,
      "duration": 6.44
    },
    {
      "lang": "en",
      "text": "back so basically not every single by",
      "offset": 2863.16,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "sequence is valid",
      "offset": 2866.96,
      "duration": 4.52
    },
    {
      "lang": "en",
      "text": "utf8 and if it happens that your large",
      "offset": 2868.52,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "language model for example predicts your",
      "offset": 2871.48,
      "duration": 5.16
    },
    {
      "lang": "en",
      "text": "tokens in a bad manner then they might",
      "offset": 2873.88,
      "duration": 6.36
    },
    {
      "lang": "en",
      "text": "not fall into valid utf8 and then we",
      "offset": 2876.64,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "won't be able to decode them so the",
      "offset": 2880.24,
      "duration": 5.4
    },
    {
      "lang": "en",
      "text": "standard practice is to basically uh use",
      "offset": 2882.88,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "errors equals replace and this is what",
      "offset": 2885.64,
      "duration": 4.679
    },
    {
      "lang": "en",
      "text": "you will also find in the openai um code",
      "offset": 2887.52,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "that they released as well but basically",
      "offset": 2890.319,
      "duration": 3.881
    },
    {
      "lang": "en",
      "text": "whenever you see um this kind of a",
      "offset": 2892.72,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "character in your output in that case uh",
      "offset": 2894.2,
      "duration": 3.96
    },
    {
      "lang": "en",
      "text": "something went wrong and the LM output",
      "offset": 2896,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "not was not valid uh sort of sequence of",
      "offset": 2898.16,
      "duration": 5.32
    },
    {
      "lang": "en",
      "text": "tokens okay and now we're going to go",
      "offset": 2901.52,
      "duration": 3.799
    },
    {
      "lang": "en",
      "text": "the other way so we are going to",
      "offset": 2903.48,
      "duration": 2.76
    },
    {
      "lang": "en",
      "text": "implement",
      "offset": 2905.319,
      "duration": 2.641
    },
    {
      "lang": "en",
      "text": "this Arrow right here where we are going",
      "offset": 2906.24,
      "duration": 3.4
    },
    {
      "lang": "en",
      "text": "to be given a string and we want to",
      "offset": 2907.96,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "encode it into",
      "offset": 2909.64,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "tokens so this is the signature of the",
      "offset": 2911.16,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "function that we're interested in and um",
      "offset": 2913.72,
      "duration": 4.44
    },
    {
      "lang": "en",
      "text": "this should basically print a list of",
      "offset": 2916.92,
      "duration": 4.84
    },
    {
      "lang": "en",
      "text": "integers of the tokens so again uh try",
      "offset": 2918.16,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "to maybe implement this yourself if",
      "offset": 2921.76,
      "duration": 3.799
    },
    {
      "lang": "en",
      "text": "you'd like a fun exercise uh and pause",
      "offset": 2923.04,
      "duration": 3.48
    },
    {
      "lang": "en",
      "text": "here otherwise I'm going to start",
      "offset": 2925.559,
      "duration": 2.401
    },
    {
      "lang": "en",
      "text": "putting in my",
      "offset": 2926.52,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "solution so again there are many ways to",
      "offset": 2927.96,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "do this so um this is one of the ways",
      "offset": 2930.28,
      "duration": 7.319
    },
    {
      "lang": "en",
      "text": "that sort of I came came up with so the",
      "offset": 2933.64,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "first thing we're going to do is we are",
      "offset": 2937.599,
      "duration": 2.52
    },
    {
      "lang": "en",
      "text": "going",
      "offset": 2939.16,
      "duration": 4.28
    },
    {
      "lang": "en",
      "text": "to uh take our text encode it into utf8",
      "offset": 2940.119,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "to get the raw bytes and then as before",
      "offset": 2943.44,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "we're going to call list on the bytes",
      "offset": 2945.799,
      "duration": 4.28
    },
    {
      "lang": "en",
      "text": "object to get a list of integers of",
      "offset": 2947.28,
      "duration": 5.48
    },
    {
      "lang": "en",
      "text": "those bytes so those are the starting",
      "offset": 2950.079,
      "duration": 4.52
    },
    {
      "lang": "en",
      "text": "tokens those are the raw bytes of our",
      "offset": 2952.76,
      "duration": 4.2
    },
    {
      "lang": "en",
      "text": "sequence but now of course according to",
      "offset": 2954.599,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "the merges dictionary above and recall",
      "offset": 2956.96,
      "duration": 4.119
    },
    {
      "lang": "en",
      "text": "this was the",
      "offset": 2959.559,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "merges some of the bytes may be merged",
      "offset": 2961.079,
      "duration": 5.48
    },
    {
      "lang": "en",
      "text": "according to this lookup in addition to",
      "offset": 2963.96,
      "duration": 4.2
    },
    {
      "lang": "en",
      "text": "that remember that the merges was built",
      "offset": 2966.559,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "from top to bottom and this is sort of",
      "offset": 2968.16,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "the order in which we inserted stuff",
      "offset": 2969.92,
      "duration": 4.36
    },
    {
      "lang": "en",
      "text": "into merges and so we prefer to do all",
      "offset": 2971.359,
      "duration": 4.76
    },
    {
      "lang": "en",
      "text": "these merges in the beginning before we",
      "offset": 2974.28,
      "duration": 4.92
    },
    {
      "lang": "en",
      "text": "do these merges later because um for",
      "offset": 2976.119,
      "duration": 4.841
    },
    {
      "lang": "en",
      "text": "example this merge over here relies on",
      "offset": 2979.2,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "the 256 which got merged here so we have",
      "offset": 2980.96,
      "duration": 5.96
    },
    {
      "lang": "en",
      "text": "to go in the order from top to bottom",
      "offset": 2984.64,
      "duration": 4.28
    },
    {
      "lang": "en",
      "text": "sort of if we are going to be merging",
      "offset": 2986.92,
      "duration": 4.52
    },
    {
      "lang": "en",
      "text": "anything now we expect to be doing a few",
      "offset": 2988.92,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "merges so we're going to be doing W",
      "offset": 2991.44,
      "duration": 6.639
    },
    {
      "lang": "en",
      "text": "true um and now we want to find a pair",
      "offset": 2994.52,
      "duration": 6.2
    },
    {
      "lang": "en",
      "text": "of byes that is consecutive that we are",
      "offset": 2998.079,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "allowed to merge according to this in",
      "offset": 3000.72,
      "duration": 4.28
    },
    {
      "lang": "en",
      "text": "order to reuse some of the functionality",
      "offset": 3003.599,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "that we've already written I'm going to",
      "offset": 3005,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "reuse the function uh get",
      "offset": 3006.559,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "stats so recall that get stats uh will",
      "offset": 3009.079,
      "duration": 5.161
    },
    {
      "lang": "en",
      "text": "give us the we'll basically count up how",
      "offset": 3012.079,
      "duration": 4.52
    },
    {
      "lang": "en",
      "text": "many times every single pair occurs in",
      "offset": 3014.24,
      "duration": 4.68
    },
    {
      "lang": "en",
      "text": "our sequence of tokens and return that",
      "offset": 3016.599,
      "duration": 5.48
    },
    {
      "lang": "en",
      "text": "as a dictionary and the dictionary was a",
      "offset": 3018.92,
      "duration": 6.679
    },
    {
      "lang": "en",
      "text": "mapping from all the different uh by",
      "offset": 3022.079,
      "duration": 5.321
    },
    {
      "lang": "en",
      "text": "pairs to the number of times that they",
      "offset": 3025.599,
      "duration": 4.681
    },
    {
      "lang": "en",
      "text": "occur right um at this point we don't",
      "offset": 3027.4,
      "duration": 4.959
    },
    {
      "lang": "en",
      "text": "actually care how many times they occur",
      "offset": 3030.28,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "in the sequence we only care what the",
      "offset": 3032.359,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "raw pairs are in that sequence and so",
      "offset": 3034.359,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "I'm only going to be using basically the",
      "offset": 3036.839,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "keys of the dictionary I only care about",
      "offset": 3038.28,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "the set of possible merge candidates if",
      "offset": 3040.44,
      "duration": 3.32
    },
    {
      "lang": "en",
      "text": "that makes",
      "offset": 3042.92,
      "duration": 3.24
    },
    {
      "lang": "en",
      "text": "sense now we want to identify the pair",
      "offset": 3043.76,
      "duration": 3.96
    },
    {
      "lang": "en",
      "text": "that we're going to be merging at this",
      "offset": 3046.16,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "stage of the loop so what do we want we",
      "offset": 3047.72,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "want to find the pair or like the a key",
      "offset": 3050.24,
      "duration": 6.839
    },
    {
      "lang": "en",
      "text": "inside stats that has the lowest index",
      "offset": 3053.24,
      "duration": 6.4
    },
    {
      "lang": "en",
      "text": "in the merges uh dictionary because we",
      "offset": 3057.079,
      "duration": 4.201
    },
    {
      "lang": "en",
      "text": "want to do all the early merges before",
      "offset": 3059.64,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "we work our way to the late",
      "offset": 3061.28,
      "duration": 4.039
    },
    {
      "lang": "en",
      "text": "merges so again there are many different",
      "offset": 3063.079,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "ways to implement this but I'm going to",
      "offset": 3065.319,
      "duration": 5.961
    },
    {
      "lang": "en",
      "text": "do something a little bit fancy",
      "offset": 3067.72,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "here so I'm going to be using the Min",
      "offset": 3071.28,
      "duration": 5.519
    },
    {
      "lang": "en",
      "text": "over an iterator in Python when you call",
      "offset": 3074.2,
      "duration": 4.76
    },
    {
      "lang": "en",
      "text": "Min on an iterator and stats here as a",
      "offset": 3076.799,
      "duration": 4.04
    },
    {
      "lang": "en",
      "text": "dictionary we're going to be iterating",
      "offset": 3078.96,
      "duration": 5.159
    },
    {
      "lang": "en",
      "text": "the keys of this dictionary in Python so",
      "offset": 3080.839,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "we're looking at all the pairs inside",
      "offset": 3084.119,
      "duration": 5.24
    },
    {
      "lang": "en",
      "text": "stats um which are all the consecutive",
      "offset": 3087.079,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "Pairs and we're going to be taking the",
      "offset": 3089.359,
      "duration": 5.081
    },
    {
      "lang": "en",
      "text": "consecutive pair inside tokens that has",
      "offset": 3092.079,
      "duration": 6.801
    },
    {
      "lang": "en",
      "text": "the minimum what the Min takes a key",
      "offset": 3094.44,
      "duration": 5.879
    },
    {
      "lang": "en",
      "text": "which gives us the function that is",
      "offset": 3098.88,
      "duration": 3.479
    },
    {
      "lang": "en",
      "text": "going to return a value over which we're",
      "offset": 3100.319,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "going to do the Min and the one we care",
      "offset": 3102.359,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "about is we're we care about taking",
      "offset": 3104.96,
      "duration": 5.96
    },
    {
      "lang": "en",
      "text": "merges and basically getting um that",
      "offset": 3106.44,
      "duration": 6.399
    },
    {
      "lang": "en",
      "text": "pairs",
      "offset": 3110.92,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "index so basically for any pair inside",
      "offset": 3112.839,
      "duration": 6.881
    },
    {
      "lang": "en",
      "text": "stats we are going to be looking into",
      "offset": 3117.16,
      "duration": 5.919
    },
    {
      "lang": "en",
      "text": "merges at what index it has and we want",
      "offset": 3119.72,
      "duration": 6.119
    },
    {
      "lang": "en",
      "text": "to get the pair with the Min number so",
      "offset": 3123.079,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "as an example if there's a pair 101 and",
      "offset": 3125.839,
      "duration": 4.601
    },
    {
      "lang": "en",
      "text": "32 we definitely want to get that pair",
      "offset": 3127.559,
      "duration": 4.361
    },
    {
      "lang": "en",
      "text": "uh we want to identify it here and",
      "offset": 3130.44,
      "duration": 4.6
    },
    {
      "lang": "en",
      "text": "return it and pair would become 10132 if",
      "offset": 3131.92,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "it",
      "offset": 3135.04,
      "duration": 2.92
    },
    {
      "lang": "en",
      "text": "occurs and the reason that I'm putting a",
      "offset": 3135.76,
      "duration": 5.64
    },
    {
      "lang": "en",
      "text": "float INF here as a fall back is that in",
      "offset": 3137.96,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "the get function when we call uh when we",
      "offset": 3141.4,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "basically consider a pair that doesn't",
      "offset": 3144.2,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "occur in the merges then that pair is",
      "offset": 3146.599,
      "duration": 5.281
    },
    {
      "lang": "en",
      "text": "not eligible to be merged right so if in",
      "offset": 3149,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "the token sequence there's some pair",
      "offset": 3151.88,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "that is not a merging pair it cannot be",
      "offset": 3153.48,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "merged then uh it doesn't actually occur",
      "offset": 3155.559,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "here and it doesn't have an index and uh",
      "offset": 3158.119,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "it cannot be merged which we will denote",
      "offset": 3160.839,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "as float INF and the reason Infinity is",
      "offset": 3162.599,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "nice here is because for sure we're",
      "offset": 3165.079,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "guaranteed that it's not going to",
      "offset": 3166.599,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "participate in the list of candidates",
      "offset": 3168.079,
      "duration": 5.361
    },
    {
      "lang": "en",
      "text": "when we do the men so uh so this is one",
      "offset": 3170.04,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "way to do it so B basically long story",
      "offset": 3173.44,
      "duration": 4.84
    },
    {
      "lang": "en",
      "text": "short this Returns the most eligible",
      "offset": 3175.88,
      "duration": 5.239
    },
    {
      "lang": "en",
      "text": "merging candidate pair uh that occurs in",
      "offset": 3178.28,
      "duration": 5.799
    },
    {
      "lang": "en",
      "text": "the tokens now one thing to be careful",
      "offset": 3181.119,
      "duration": 6.361
    },
    {
      "lang": "en",
      "text": "with here is this uh function here might",
      "offset": 3184.079,
      "duration": 5.801
    },
    {
      "lang": "en",
      "text": "fail in the following way if there's",
      "offset": 3187.48,
      "duration": 6.119
    },
    {
      "lang": "en",
      "text": "nothing to merge then uh uh then there's",
      "offset": 3189.88,
      "duration": 7.04
    },
    {
      "lang": "en",
      "text": "nothing in merges um that satisfi that",
      "offset": 3193.599,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "is satisfied anymore there's nothing to",
      "offset": 3196.92,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "merge everything just returns float imps",
      "offset": 3198.559,
      "duration": 5.121
    },
    {
      "lang": "en",
      "text": "and then the pair I think will just",
      "offset": 3201.72,
      "duration": 5.24
    },
    {
      "lang": "en",
      "text": "become the very first element of stats",
      "offset": 3203.68,
      "duration": 4.679
    },
    {
      "lang": "en",
      "text": "um but this pair is not actually a",
      "offset": 3206.96,
      "duration": 4.2
    },
    {
      "lang": "en",
      "text": "mergeable pair it just becomes the first",
      "offset": 3208.359,
      "duration": 4.921
    },
    {
      "lang": "en",
      "text": "pair inside stats arbitrarily because",
      "offset": 3211.16,
      "duration": 5.159
    },
    {
      "lang": "en",
      "text": "all of these pairs evaluate to float in",
      "offset": 3213.28,
      "duration": 5.279
    },
    {
      "lang": "en",
      "text": "for the merging Criterion so basically",
      "offset": 3216.319,
      "duration": 4.04
    },
    {
      "lang": "en",
      "text": "it could be that this this doesn't look",
      "offset": 3218.559,
      "duration": 3.081
    },
    {
      "lang": "en",
      "text": "succeed because there's no more merging",
      "offset": 3220.359,
      "duration": 4.281
    },
    {
      "lang": "en",
      "text": "pairs so if this pair is not in merges",
      "offset": 3221.64,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "that was returned then this is a signal",
      "offset": 3224.64,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "for us that actually there was nothing",
      "offset": 3226.839,
      "duration": 3.881
    },
    {
      "lang": "en",
      "text": "to merge no single pair can be merged",
      "offset": 3228.4,
      "duration": 4.679
    },
    {
      "lang": "en",
      "text": "anymore in that case we will break",
      "offset": 3230.72,
      "duration": 7.079
    },
    {
      "lang": "en",
      "text": "out um nothing else can be",
      "offset": 3233.079,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "merged you may come up with a different",
      "offset": 3237.88,
      "duration": 3.16
    },
    {
      "lang": "en",
      "text": "implementation by the way this is kind",
      "offset": 3239.839,
      "duration": 4.041
    },
    {
      "lang": "en",
      "text": "of like really trying hard in",
      "offset": 3241.04,
      "duration": 4.92
    },
    {
      "lang": "en",
      "text": "Python um but really we're just trying",
      "offset": 3243.88,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "to find a pair that can be merged with",
      "offset": 3245.96,
      "duration": 3.639
    },
    {
      "lang": "en",
      "text": "the lowest index",
      "offset": 3247.799,
      "duration": 6.081
    },
    {
      "lang": "en",
      "text": "here now if we did find a pair that is",
      "offset": 3249.599,
      "duration": 6.681
    },
    {
      "lang": "en",
      "text": "inside merges with the lowest index then",
      "offset": 3253.88,
      "duration": 5.919
    },
    {
      "lang": "en",
      "text": "we can merge it",
      "offset": 3256.28,
      "duration": 3.519
    },
    {
      "lang": "en",
      "text": "so we're going to look into the merger",
      "offset": 3259.839,
      "duration": 4.441
    },
    {
      "lang": "en",
      "text": "dictionary for that pair to look up the",
      "offset": 3262.04,
      "duration": 5.24
    },
    {
      "lang": "en",
      "text": "index and we're going to now merge that",
      "offset": 3264.28,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "into that index so we're going to do",
      "offset": 3267.28,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "tokens equals and we're going to",
      "offset": 3269.24,
      "duration": 5.4
    },
    {
      "lang": "en",
      "text": "replace the original tokens we're going",
      "offset": 3272.24,
      "duration": 4.52
    },
    {
      "lang": "en",
      "text": "to be replacing the pair pair and we're",
      "offset": 3274.64,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "going to be replacing it with index idx",
      "offset": 3276.76,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "and this returns a new list of tokens",
      "offset": 3278.96,
      "duration": 4.2
    },
    {
      "lang": "en",
      "text": "where every occurrence of pair is",
      "offset": 3281.64,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "replaced with idx so we're doing a merge",
      "offset": 3283.16,
      "duration": 4.439
    },
    {
      "lang": "en",
      "text": "and we're going to be continuing this",
      "offset": 3286.28,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "until eventually nothing can be merged",
      "offset": 3287.599,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "we'll come out here and we'll break out",
      "offset": 3289.28,
      "duration": 4.039
    },
    {
      "lang": "en",
      "text": "and here we just return",
      "offset": 3291.28,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "tokens and so that that's the",
      "offset": 3293.319,
      "duration": 4.121
    },
    {
      "lang": "en",
      "text": "implementation I think so hopefully this",
      "offset": 3295.839,
      "duration": 6.601
    },
    {
      "lang": "en",
      "text": "runs okay cool um yeah and this looks uh",
      "offset": 3297.44,
      "duration": 7.44
    },
    {
      "lang": "en",
      "text": "reasonable so for example 32 is a space",
      "offset": 3302.44,
      "duration": 6.76
    },
    {
      "lang": "en",
      "text": "in asky so that's here um so this looks",
      "offset": 3304.88,
      "duration": 6.6
    },
    {
      "lang": "en",
      "text": "like it worked great okay so let's wrap",
      "offset": 3309.2,
      "duration": 4.28
    },
    {
      "lang": "en",
      "text": "up this section of the video at least I",
      "offset": 3311.48,
      "duration": 3.4
    },
    {
      "lang": "en",
      "text": "wanted to point out that this is not",
      "offset": 3313.48,
      "duration": 2.879
    },
    {
      "lang": "en",
      "text": "quite the right implementation just yet",
      "offset": 3314.88,
      "duration": 3.08
    },
    {
      "lang": "en",
      "text": "because we are leaving out a special",
      "offset": 3316.359,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "case so in particular if uh we try to do",
      "offset": 3317.96,
      "duration": 5.599
    },
    {
      "lang": "en",
      "text": "this this would give us an error and the",
      "offset": 3320.68,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "issue is that um if we only have a",
      "offset": 3323.559,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "single character or an empty string then",
      "offset": 3325.64,
      "duration": 4.199
    },
    {
      "lang": "en",
      "text": "stats is empty and that causes an issue",
      "offset": 3328.039,
      "duration": 4.921
    },
    {
      "lang": "en",
      "text": "inside Min so one way to fight this is",
      "offset": 3329.839,
      "duration": 6.52
    },
    {
      "lang": "en",
      "text": "if L of tokens is at least two because",
      "offset": 3332.96,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "if it's less than two it's just a single",
      "offset": 3336.359,
      "duration": 3.72
    },
    {
      "lang": "en",
      "text": "token or no tokens then let's just uh",
      "offset": 3337.839,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "there's nothing to merge so we just",
      "offset": 3340.079,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "return so that would fix uh that",
      "offset": 3341.52,
      "duration": 6.559
    },
    {
      "lang": "en",
      "text": "case Okay and then second I have a few",
      "offset": 3344.64,
      "duration": 5.8
    },
    {
      "lang": "en",
      "text": "test cases here for us as well so first",
      "offset": 3348.079,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "let's make sure uh about or let's note",
      "offset": 3350.44,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "the following if we take a string and we",
      "offset": 3353.359,
      "duration": 5.281
    },
    {
      "lang": "en",
      "text": "try to encode it and then decode it back",
      "offset": 3356.44,
      "duration": 3.8
    },
    {
      "lang": "en",
      "text": "you'd expect to get the same string back",
      "offset": 3358.64,
      "duration": 5.159
    },
    {
      "lang": "en",
      "text": "right is that true for all",
      "offset": 3360.24,
      "duration": 3.559
    },
    {
      "lang": "en",
      "text": "strings so I think uh so here it is the",
      "offset": 3364.68,
      "duration": 4.04
    },
    {
      "lang": "en",
      "text": "case and I think in general this is",
      "offset": 3367.16,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "probably the case um but notice that",
      "offset": 3368.72,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "going backwards is not is not you're not",
      "offset": 3372.039,
      "duration": 3.881
    },
    {
      "lang": "en",
      "text": "going to have an identity going",
      "offset": 3374.64,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "backwards because as I mentioned us not",
      "offset": 3375.92,
      "duration": 7.04
    },
    {
      "lang": "en",
      "text": "all token sequences are valid utf8 uh",
      "offset": 3379.2,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "sort of by streams and so so therefore",
      "offset": 3382.96,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "you're some of them can't even be",
      "offset": 3385.44,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "decodable um so this only goes in One",
      "offset": 3387.2,
      "duration": 5.72
    },
    {
      "lang": "en",
      "text": "Direction but for that one direction we",
      "offset": 3390.48,
      "duration": 4.28
    },
    {
      "lang": "en",
      "text": "can check uh here if we take the",
      "offset": 3392.92,
      "duration": 3.399
    },
    {
      "lang": "en",
      "text": "training text which is the text that we",
      "offset": 3394.76,
      "duration": 3.24
    },
    {
      "lang": "en",
      "text": "train to tokenizer around we can make",
      "offset": 3396.319,
      "duration": 3.121
    },
    {
      "lang": "en",
      "text": "sure that when we encode and decode we",
      "offset": 3398,
      "duration": 3.96
    },
    {
      "lang": "en",
      "text": "get the same thing back which is true",
      "offset": 3399.44,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "and here I took some validation data so",
      "offset": 3401.96,
      "duration": 3.639
    },
    {
      "lang": "en",
      "text": "I went to I think this web page and I",
      "offset": 3403.839,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "grabbed some text so this is text that",
      "offset": 3405.599,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "the tokenizer has not seen and we can",
      "offset": 3407.76,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "make sure that this also works um okay",
      "offset": 3409.68,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "so that gives us some confidence that",
      "offset": 3412.72,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "this was correctly implemented",
      "offset": 3413.92,
      "duration": 4.119
    },
    {
      "lang": "en",
      "text": "so those are the basics of the bite pair",
      "offset": 3416,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "encoding algorithm we saw how we can uh",
      "offset": 3418.039,
      "duration": 5.641
    },
    {
      "lang": "en",
      "text": "take some training set train a tokenizer",
      "offset": 3420.72,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "the parameters of this tokenizer really",
      "offset": 3423.68,
      "duration": 4.439
    },
    {
      "lang": "en",
      "text": "are just this dictionary of merges and",
      "offset": 3425.44,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "that basically creates the little binary",
      "offset": 3428.119,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "Forest on top of raw",
      "offset": 3429.599,
      "duration": 5.081
    },
    {
      "lang": "en",
      "text": "bites once we have this the merges table",
      "offset": 3431.559,
      "duration": 5.24
    },
    {
      "lang": "en",
      "text": "we can both encode and decode between",
      "offset": 3434.68,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "raw text and token sequences so that's",
      "offset": 3436.799,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "the the simplest setting of The",
      "offset": 3439.4,
      "duration": 3.8
    },
    {
      "lang": "en",
      "text": "tokenizer what we're going to do now",
      "offset": 3441.28,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "though is we're going to look at some of",
      "offset": 3443.2,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "the St the art lar language models and",
      "offset": 3444.48,
      "duration": 3.879
    },
    {
      "lang": "en",
      "text": "the kinds of tokenizers that they use",
      "offset": 3446.559,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "and we're going to see that this picture",
      "offset": 3448.359,
      "duration": 3.281
    },
    {
      "lang": "en",
      "text": "complexifies very quickly so we're going",
      "offset": 3449.559,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "to go through the details of this comp",
      "offset": 3451.64,
      "duration": 5.88
    },
    {
      "lang": "en",
      "text": "complexification one at a time so let's",
      "offset": 3454.599,
      "duration": 4.44
    },
    {
      "lang": "en",
      "text": "kick things off by looking at the GPD",
      "offset": 3457.52,
      "duration": 4.12
    },
    {
      "lang": "en",
      "text": "Series so in particular I have the gpt2",
      "offset": 3459.039,
      "duration": 5.601
    },
    {
      "lang": "en",
      "text": "paper here um and this paper is from",
      "offset": 3461.64,
      "duration": 6.719
    },
    {
      "lang": "en",
      "text": "2019 or so so 5 years ago and let's",
      "offset": 3464.64,
      "duration": 6.64
    },
    {
      "lang": "en",
      "text": "scroll down to input representation this",
      "offset": 3468.359,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "is where they talk about the tokenizer",
      "offset": 3471.28,
      "duration": 4.36
    },
    {
      "lang": "en",
      "text": "that they're using for gpd2 now this is",
      "offset": 3472.68,
      "duration": 4.359
    },
    {
      "lang": "en",
      "text": "all fairly readable so I encourage you",
      "offset": 3475.64,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "to pause and um read this yourself but",
      "offset": 3477.039,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "this is where they motivate the use of",
      "offset": 3480.039,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "the bite pair encoding algorithm on the",
      "offset": 3482,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "bite level representation of utf8",
      "offset": 3484.68,
      "duration": 4.84
    },
    {
      "lang": "en",
      "text": "encoding so this is where they motivate",
      "offset": 3487.52,
      "duration": 3.559
    },
    {
      "lang": "en",
      "text": "it and they talk about the vocabulary",
      "offset": 3489.52,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "sizes and everything now everything here",
      "offset": 3491.079,
      "duration": 4.841
    },
    {
      "lang": "en",
      "text": "is exactly as we've covered it so far",
      "offset": 3493.839,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "but things start to depart around here",
      "offset": 3495.92,
      "duration": 4.52
    },
    {
      "lang": "en",
      "text": "so what they mention is that they don't",
      "offset": 3498.559,
      "duration": 3.721
    },
    {
      "lang": "en",
      "text": "just apply the naive algorithm as we",
      "offset": 3500.44,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "have done it and in particular here's a",
      "offset": 3502.28,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "example suppose that you have common",
      "offset": 3505.16,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "words like dog what will happen is that",
      "offset": 3507,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "dog of course occurs very frequently in",
      "offset": 3509.48,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "the text and it occurs right next to all",
      "offset": 3511.64,
      "duration": 4.76
    },
    {
      "lang": "en",
      "text": "kinds of punctuation as an example so",
      "offset": 3514.28,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "doc dot dog exclamation mark dog",
      "offset": 3516.4,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "question mark Etc and naively you might",
      "offset": 3519.16,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "imagine that the BP algorithm could",
      "offset": 3522.24,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "merge these to be single tokens and then",
      "offset": 3523.64,
      "duration": 3.8
    },
    {
      "lang": "en",
      "text": "you end up with lots of tokens that are",
      "offset": 3525.76,
      "duration": 3.24
    },
    {
      "lang": "en",
      "text": "just like dog with a slightly different",
      "offset": 3527.44,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "punctuation and so it feels like you're",
      "offset": 3529,
      "duration": 3.039
    },
    {
      "lang": "en",
      "text": "clustering things that shouldn't be",
      "offset": 3530.88,
      "duration": 2.76
    },
    {
      "lang": "en",
      "text": "clustered you're combining kind of",
      "offset": 3532.039,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "semantics with",
      "offset": 3533.64,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "uation and this uh feels suboptimal and",
      "offset": 3535.559,
      "duration": 5.401
    },
    {
      "lang": "en",
      "text": "indeed they also say that this is",
      "offset": 3538.92,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "suboptimal according to some of the",
      "offset": 3540.96,
      "duration": 3.24
    },
    {
      "lang": "en",
      "text": "experiments so what they want to do is",
      "offset": 3542.359,
      "duration": 3.96
    },
    {
      "lang": "en",
      "text": "they want to top down in a manual way",
      "offset": 3544.2,
      "duration": 5.399
    },
    {
      "lang": "en",
      "text": "enforce that some types of um characters",
      "offset": 3546.319,
      "duration": 6.441
    },
    {
      "lang": "en",
      "text": "should never be merged together um so",
      "offset": 3549.599,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "they want to enforce these merging rules",
      "offset": 3552.76,
      "duration": 4.92
    },
    {
      "lang": "en",
      "text": "on top of the bite PA encoding algorithm",
      "offset": 3554.799,
      "duration": 5.081
    },
    {
      "lang": "en",
      "text": "so let's take a look um at their code",
      "offset": 3557.68,
      "duration": 3.8
    },
    {
      "lang": "en",
      "text": "and see how they actually enforce this",
      "offset": 3559.88,
      "duration": 3.32
    },
    {
      "lang": "en",
      "text": "and what kinds of mergy they actually do",
      "offset": 3561.48,
      "duration": 4.359
    },
    {
      "lang": "en",
      "text": "perform so I have to to tab open here",
      "offset": 3563.2,
      "duration": 6.44
    },
    {
      "lang": "en",
      "text": "for gpt2 under open AI on GitHub and",
      "offset": 3565.839,
      "duration": 4.841
    },
    {
      "lang": "en",
      "text": "when we go to",
      "offset": 3569.64,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "Source there is an encoder thatp now I",
      "offset": 3570.68,
      "duration": 4.919
    },
    {
      "lang": "en",
      "text": "don't personally love that they call it",
      "offset": 3574.28,
      "duration": 2.799
    },
    {
      "lang": "en",
      "text": "encoder dopy because this is the",
      "offset": 3575.599,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "tokenizer and the tokenizer can do both",
      "offset": 3577.079,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "encode and decode uh so it feels kind of",
      "offset": 3579.359,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "awkward to me that it's called encoder",
      "offset": 3581.88,
      "duration": 4.04
    },
    {
      "lang": "en",
      "text": "but that is the tokenizer and there's a",
      "offset": 3583.2,
      "duration": 3.8
    },
    {
      "lang": "en",
      "text": "lot going on here and we're going to",
      "offset": 3585.92,
      "duration": 3.32
    },
    {
      "lang": "en",
      "text": "step through it in detail at one point",
      "offset": 3587,
      "duration": 4.599
    },
    {
      "lang": "en",
      "text": "for now I just want to focus on this",
      "offset": 3589.24,
      "duration": 5.119
    },
    {
      "lang": "en",
      "text": "part here the create a rigix pattern",
      "offset": 3591.599,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "here that looks very complicated and",
      "offset": 3594.359,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "we're going to go through it in a bit uh",
      "offset": 3596.24,
      "duration": 4.04
    },
    {
      "lang": "en",
      "text": "but this is the core part that allows",
      "offset": 3598.68,
      "duration": 5.32
    },
    {
      "lang": "en",
      "text": "them to enforce rules uh for what parts",
      "offset": 3600.28,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "of the text Will Never Be merged for",
      "offset": 3604,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "sure now notice that re. compile here is",
      "offset": 3605.96,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "a little bit misleading because we're",
      "offset": 3608.64,
      "duration": 3.8
    },
    {
      "lang": "en",
      "text": "not just doing import re which is the",
      "offset": 3610.76,
      "duration": 3.88
    },
    {
      "lang": "en",
      "text": "python re module we're doing import reex",
      "offset": 3612.44,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "as re and reex is a python package that",
      "offset": 3614.64,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "you can install P install r x and it's",
      "offset": 3617.72,
      "duration": 4.359
    },
    {
      "lang": "en",
      "text": "basically an extension of re so it's a",
      "offset": 3620.4,
      "duration": 2.84
    },
    {
      "lang": "en",
      "text": "bit more powerful",
      "offset": 3622.079,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "re um",
      "offset": 3623.24,
      "duration": 5.64
    },
    {
      "lang": "en",
      "text": "so let's take a look at this pattern and",
      "offset": 3626,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "what it's doing and why this is actually",
      "offset": 3628.88,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "doing the separation that they are",
      "offset": 3630.799,
      "duration": 4.121
    },
    {
      "lang": "en",
      "text": "looking for okay so I've copy pasted the",
      "offset": 3632.64,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "pattern here to our jupit notebook where",
      "offset": 3634.92,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "we left off and let's take this pattern",
      "offset": 3637.119,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "for a spin so in the exact same way that",
      "offset": 3639.24,
      "duration": 4.839
    },
    {
      "lang": "en",
      "text": "their code does we're going to call an",
      "offset": 3642.119,
      "duration": 5.161
    },
    {
      "lang": "en",
      "text": "re. findall for this pattern on any",
      "offset": 3644.079,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "arbitrary string that we are interested",
      "offset": 3647.28,
      "duration": 3.319
    },
    {
      "lang": "en",
      "text": "so this is the string that we want to",
      "offset": 3649.359,
      "duration": 5.881
    },
    {
      "lang": "en",
      "text": "encode into tokens um to feed into n llm",
      "offset": 3650.599,
      "duration": 8.44
    },
    {
      "lang": "en",
      "text": "like gpt2 so what exactly is this doing",
      "offset": 3655.24,
      "duration": 5.799
    },
    {
      "lang": "en",
      "text": "well re. findall will take this pattern",
      "offset": 3659.039,
      "duration": 3.8
    },
    {
      "lang": "en",
      "text": "and try to match it against a",
      "offset": 3661.039,
      "duration": 5.08
    },
    {
      "lang": "en",
      "text": "string um the way this works is that you",
      "offset": 3662.839,
      "duration": 5.121
    },
    {
      "lang": "en",
      "text": "are going from left to right in the",
      "offset": 3666.119,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "string and you're trying to match the",
      "offset": 3667.96,
      "duration": 5.839
    },
    {
      "lang": "en",
      "text": "pattern and R.F find all will get all",
      "offset": 3670.28,
      "duration": 6.039
    },
    {
      "lang": "en",
      "text": "the occurrences and organize them into a",
      "offset": 3673.799,
      "duration": 5.361
    },
    {
      "lang": "en",
      "text": "list now when you look at the um when",
      "offset": 3676.319,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "you look at this pattern first of all",
      "offset": 3679.16,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "notice that this is a raw string um and",
      "offset": 3680.88,
      "duration": 5.439
    },
    {
      "lang": "en",
      "text": "then these are three double quotes just",
      "offset": 3683.96,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "to start the string so really the string",
      "offset": 3686.319,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "itself this is the pattern itself",
      "offset": 3688.839,
      "duration": 5.24
    },
    {
      "lang": "en",
      "text": "right and notice that it's made up of a",
      "offset": 3691.319,
      "duration": 5.161
    },
    {
      "lang": "en",
      "text": "lot of ores so see these vertical bars",
      "offset": 3694.079,
      "duration": 6.121
    },
    {
      "lang": "en",
      "text": "those are ores in reg X and so you go",
      "offset": 3696.48,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "from left to right in this pattern and",
      "offset": 3700.2,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "try to match it against the string",
      "offset": 3701.48,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "wherever you are so we have hello and",
      "offset": 3703.16,
      "duration": 5.08
    },
    {
      "lang": "en",
      "text": "we're going to try to match it well it's",
      "offset": 3706.44,
      "duration": 4.359
    },
    {
      "lang": "en",
      "text": "not apostrophe s it's not apostrophe t",
      "offset": 3708.24,
      "duration": 5.72
    },
    {
      "lang": "en",
      "text": "or any of these but it is an optional",
      "offset": 3710.799,
      "duration": 7.32
    },
    {
      "lang": "en",
      "text": "space followed by- P of uh sorry SL P of",
      "offset": 3713.96,
      "duration": 8.359
    },
    {
      "lang": "en",
      "text": "L one or more times what is/ P of L it",
      "offset": 3718.119,
      "duration": 6.601
    },
    {
      "lang": "en",
      "text": "is coming to some documentation that I",
      "offset": 3722.319,
      "duration": 5.681
    },
    {
      "lang": "en",
      "text": "found um there might be other sources as",
      "offset": 3724.72,
      "duration": 6.879
    },
    {
      "lang": "en",
      "text": "well uh SLP is a letter any kind of",
      "offset": 3728,
      "duration": 7.039
    },
    {
      "lang": "en",
      "text": "letter from any language and hello is",
      "offset": 3731.599,
      "duration": 7.921
    },
    {
      "lang": "en",
      "text": "made up of letters h e l Etc so optional",
      "offset": 3735.039,
      "duration": 6.52
    },
    {
      "lang": "en",
      "text": "space followed by a bunch of letters one",
      "offset": 3739.52,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "or more letters is going to match hello",
      "offset": 3741.559,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "but then the match ends because a white",
      "offset": 3744.72,
      "duration": 6.359
    },
    {
      "lang": "en",
      "text": "space is not a letter so from there on",
      "offset": 3747.079,
      "duration": 6.561
    },
    {
      "lang": "en",
      "text": "begins a new sort of attempt to match",
      "offset": 3751.079,
      "duration": 5.361
    },
    {
      "lang": "en",
      "text": "against the string again and starting in",
      "offset": 3753.64,
      "duration": 4.439
    },
    {
      "lang": "en",
      "text": "here we're going to skip over all of",
      "offset": 3756.44,
      "duration": 3.72
    },
    {
      "lang": "en",
      "text": "these again until we get to the exact",
      "offset": 3758.079,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "same Point again and we see that there's",
      "offset": 3760.16,
      "duration": 4.119
    },
    {
      "lang": "en",
      "text": "an optional space this is the optional",
      "offset": 3762.319,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "space followed by a bunch of letters one",
      "offset": 3764.279,
      "duration": 4.441
    },
    {
      "lang": "en",
      "text": "or more of them and so that matches so",
      "offset": 3766.24,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "when we run this we get a list of two",
      "offset": 3768.72,
      "duration": 7
    },
    {
      "lang": "en",
      "text": "elements hello and then space world",
      "offset": 3772,
      "duration": 6.88
    },
    {
      "lang": "en",
      "text": "so how are you if we add more letters we",
      "offset": 3775.72,
      "duration": 5.879
    },
    {
      "lang": "en",
      "text": "would just get them like this now what",
      "offset": 3778.88,
      "duration": 4.76
    },
    {
      "lang": "en",
      "text": "is this doing and why is this important",
      "offset": 3781.599,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "we are taking our string and instead of",
      "offset": 3783.64,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "directly encoding it um for",
      "offset": 3785.92,
      "duration": 5.48
    },
    {
      "lang": "en",
      "text": "tokenization we are first splitting it",
      "offset": 3789,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "up and when you actually step through",
      "offset": 3791.4,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "the code and we'll do that in a bit more",
      "offset": 3793.48,
      "duration": 3.879
    },
    {
      "lang": "en",
      "text": "detail what really is doing on a high",
      "offset": 3795.319,
      "duration": 5.601
    },
    {
      "lang": "en",
      "text": "level is that it first splits your text",
      "offset": 3797.359,
      "duration": 7.281
    },
    {
      "lang": "en",
      "text": "into a list of texts just like this one",
      "offset": 3800.92,
      "duration": 5.639
    },
    {
      "lang": "en",
      "text": "and all these elements of this list are",
      "offset": 3804.64,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "processed independently by the tokenizer",
      "offset": 3806.559,
      "duration": 4.201
    },
    {
      "lang": "en",
      "text": "and all of the results of that",
      "offset": 3809.279,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "processing are simply",
      "offset": 3810.76,
      "duration": 5.16
    },
    {
      "lang": "en",
      "text": "concatenated so hello world oh I I",
      "offset": 3812.279,
      "duration": 7.361
    },
    {
      "lang": "en",
      "text": "missed how hello world how are you we",
      "offset": 3815.92,
      "duration": 5.679
    },
    {
      "lang": "en",
      "text": "have five elements of list all of these",
      "offset": 3819.64,
      "duration": 4.76
    },
    {
      "lang": "en",
      "text": "will independent",
      "offset": 3821.599,
      "duration": 5.401
    },
    {
      "lang": "en",
      "text": "independently go from text to a token",
      "offset": 3824.4,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "sequence and then that token sequence is",
      "offset": 3827,
      "duration": 3.799
    },
    {
      "lang": "en",
      "text": "going to be concatenated it's all going",
      "offset": 3829.2,
      "duration": 5.159
    },
    {
      "lang": "en",
      "text": "to be joined up and roughly speaking",
      "offset": 3830.799,
      "duration": 5.32
    },
    {
      "lang": "en",
      "text": "what that does is you're only ever",
      "offset": 3834.359,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "finding merges between the elements of",
      "offset": 3836.119,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "this list so you can only ever consider",
      "offset": 3838.44,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "merges within every one of these",
      "offset": 3840.359,
      "duration": 2.881
    },
    {
      "lang": "en",
      "text": "elements in",
      "offset": 3841.72,
      "duration": 4.599
    },
    {
      "lang": "en",
      "text": "individually and um after you've done",
      "offset": 3843.24,
      "duration": 4.68
    },
    {
      "lang": "en",
      "text": "all the possible merging for all of",
      "offset": 3846.319,
      "duration": 3.561
    },
    {
      "lang": "en",
      "text": "these elements individually the results",
      "offset": 3847.92,
      "duration": 5.72
    },
    {
      "lang": "en",
      "text": "of all that will be joined um by",
      "offset": 3849.88,
      "duration": 6.36
    },
    {
      "lang": "en",
      "text": "concatenation and so you are basically",
      "offset": 3853.64,
      "duration": 4.76
    },
    {
      "lang": "en",
      "text": "what what you're doing effectively is",
      "offset": 3856.24,
      "duration": 4.76
    },
    {
      "lang": "en",
      "text": "you are never going to be merging this e",
      "offset": 3858.4,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "with this space because they are now",
      "offset": 3861,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "parts of the separate elements of this",
      "offset": 3863.2,
      "duration": 4.52
    },
    {
      "lang": "en",
      "text": "list and so you are saying we are never",
      "offset": 3865.079,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "going to merge",
      "offset": 3867.72,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "eace um because we're breaking it up in",
      "offset": 3868.92,
      "duration": 6.8
    },
    {
      "lang": "en",
      "text": "this way so basically using this regx",
      "offset": 3872.039,
      "duration": 5.921
    },
    {
      "lang": "en",
      "text": "pattern to Chunk Up the text is just one",
      "offset": 3875.72,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "way of enforcing that some merges are",
      "offset": 3877.96,
      "duration": 5.8
    },
    {
      "lang": "en",
      "text": "not to happen and we're going to go into",
      "offset": 3881.72,
      "duration": 3.48
    },
    {
      "lang": "en",
      "text": "more of this text and we'll see that",
      "offset": 3883.76,
      "duration": 2.48
    },
    {
      "lang": "en",
      "text": "what this is trying to do on a high",
      "offset": 3885.2,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "level is we're trying to not merge",
      "offset": 3886.24,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "across letters across numbers across",
      "offset": 3888,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "punctuation and so on so let's see in",
      "offset": 3890.64,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "more detail how that works so let's",
      "offset": 3893.2,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "continue now we have/ P ofn if you go to",
      "offset": 3894.72,
      "duration": 7.119
    },
    {
      "lang": "en",
      "text": "the documentation SLP of n is any kind",
      "offset": 3898,
      "duration": 6.44
    },
    {
      "lang": "en",
      "text": "of numeric character in any script so",
      "offset": 3901.839,
      "duration": 4.76
    },
    {
      "lang": "en",
      "text": "it's numbers so we have an optional",
      "offset": 3904.44,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "space followed by numbers and those",
      "offset": 3906.599,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "would be separated out so letters and",
      "offset": 3908.119,
      "duration": 4.44
    },
    {
      "lang": "en",
      "text": "numbers are being separated so if I do",
      "offset": 3910.359,
      "duration": 5.48
    },
    {
      "lang": "en",
      "text": "Hello World 123 how are you then world",
      "offset": 3912.559,
      "duration": 5.401
    },
    {
      "lang": "en",
      "text": "will stop matching here because one is",
      "offset": 3915.839,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "not a letter anymore but one is a number",
      "offset": 3917.96,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "so this group will match for that and",
      "offset": 3920.64,
      "duration": 5.439
    },
    {
      "lang": "en",
      "text": "we'll get it as a separate entity",
      "offset": 3922.52,
      "duration": 3.559
    },
    {
      "lang": "en",
      "text": "uh let's see how these apostrophes work",
      "offset": 3926.559,
      "duration": 4.441
    },
    {
      "lang": "en",
      "text": "so here if we have",
      "offset": 3928.359,
      "duration": 6.72
    },
    {
      "lang": "en",
      "text": "um uh Slash V or I mean apostrophe V as",
      "offset": 3931,
      "duration": 7.359
    },
    {
      "lang": "en",
      "text": "an example then apostrophe here is not a",
      "offset": 3935.079,
      "duration": 4.441
    },
    {
      "lang": "en",
      "text": "letter or a",
      "offset": 3938.359,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "number so hello will stop matching and",
      "offset": 3939.52,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "then we will exactly match this with",
      "offset": 3942.44,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "that so that will come out as a separate",
      "offset": 3944.96,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "thing so why are they doing the",
      "offset": 3948.2,
      "duration": 4.04
    },
    {
      "lang": "en",
      "text": "apostrophes here honestly I think that",
      "offset": 3950.24,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "these are just like very common",
      "offset": 3952.24,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "apostrophes p uh that are used um",
      "offset": 3953.599,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "typically I don't love that they've done",
      "offset": 3956.96,
      "duration": 3.639
    },
    {
      "lang": "en",
      "text": "this",
      "offset": 3959.359,
      "duration": 3.96
    },
    {
      "lang": "en",
      "text": "because uh let me show you what happens",
      "offset": 3960.599,
      "duration": 4.841
    },
    {
      "lang": "en",
      "text": "when you have uh some Unicode",
      "offset": 3963.319,
      "duration": 4.04
    },
    {
      "lang": "en",
      "text": "apostrophes like for example you can",
      "offset": 3965.44,
      "duration": 5.119
    },
    {
      "lang": "en",
      "text": "have if you have house then this will be",
      "offset": 3967.359,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "separated out because of this matching",
      "offset": 3970.559,
      "duration": 4.76
    },
    {
      "lang": "en",
      "text": "but if you use the Unicode apostrophe",
      "offset": 3973.039,
      "duration": 3.121
    },
    {
      "lang": "en",
      "text": "like",
      "offset": 3975.319,
      "duration": 4.52
    },
    {
      "lang": "en",
      "text": "this then suddenly this does not work",
      "offset": 3976.16,
      "duration": 5.399
    },
    {
      "lang": "en",
      "text": "and so this apostrophe will actually",
      "offset": 3979.839,
      "duration": 5.081
    },
    {
      "lang": "en",
      "text": "become its own thing now and so so um",
      "offset": 3981.559,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "it's basically hardcoded for this",
      "offset": 3984.92,
      "duration": 4.76
    },
    {
      "lang": "en",
      "text": "specific kind of apostrophe and uh",
      "offset": 3986.359,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "otherwise they become completely",
      "offset": 3989.68,
      "duration": 4.359
    },
    {
      "lang": "en",
      "text": "separate tokens in addition to this you",
      "offset": 3991.319,
      "duration": 7.161
    },
    {
      "lang": "en",
      "text": "can go to the gpt2 docs and here when",
      "offset": 3994.039,
      "duration": 6.161
    },
    {
      "lang": "en",
      "text": "they Define the pattern they say should",
      "offset": 3998.48,
      "duration": 4.52
    },
    {
      "lang": "en",
      "text": "have added re. ignore case so BP merges",
      "offset": 4000.2,
      "duration": 4.359
    },
    {
      "lang": "en",
      "text": "can happen for capitalized versions of",
      "offset": 4003,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "contractions so what they're pointing",
      "offset": 4004.559,
      "duration": 3.161
    },
    {
      "lang": "en",
      "text": "out is that you see how this is",
      "offset": 4006.52,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "apostrophe and then lowercase letters",
      "offset": 4007.72,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "well because they didn't do re. ignore",
      "offset": 4010.839,
      "duration": 5.601
    },
    {
      "lang": "en",
      "text": "case then then um these rules will not",
      "offset": 4012.92,
      "duration": 5.96
    },
    {
      "lang": "en",
      "text": "separate out the apostrophes if it's",
      "offset": 4016.44,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "uppercase so",
      "offset": 4018.88,
      "duration": 7.76
    },
    {
      "lang": "en",
      "text": "house would be like this but if I did",
      "offset": 4021.44,
      "duration": 8.8
    },
    {
      "lang": "en",
      "text": "house if I'm uppercase then notice",
      "offset": 4026.64,
      "duration": 5.639
    },
    {
      "lang": "en",
      "text": "suddenly the apostrophe comes by",
      "offset": 4030.24,
      "duration": 5.24
    },
    {
      "lang": "en",
      "text": "itself so the tokenization will work",
      "offset": 4032.279,
      "duration": 5.161
    },
    {
      "lang": "en",
      "text": "differently in uppercase and lower case",
      "offset": 4035.48,
      "duration": 3.559
    },
    {
      "lang": "en",
      "text": "inconsistently separating out these",
      "offset": 4037.44,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "apostrophes so it feels extremely gnarly",
      "offset": 4039.039,
      "duration": 5.481
    },
    {
      "lang": "en",
      "text": "and slightly gross um but that's that's",
      "offset": 4041.119,
      "duration": 6.121
    },
    {
      "lang": "en",
      "text": "how that works okay so let's come back",
      "offset": 4044.52,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "after trying to match a bunch of",
      "offset": 4047.24,
      "duration": 3.039
    },
    {
      "lang": "en",
      "text": "apostrophe Expressions by the way the",
      "offset": 4048.44,
      "duration": 3.639
    },
    {
      "lang": "en",
      "text": "other issue here is that these are quite",
      "offset": 4050.279,
      "duration": 4.28
    },
    {
      "lang": "en",
      "text": "language specific probably so I don't",
      "offset": 4052.079,
      "duration": 3.72
    },
    {
      "lang": "en",
      "text": "know that all the languages for example",
      "offset": 4054.559,
      "duration": 2.921
    },
    {
      "lang": "en",
      "text": "use or don't use apostrophes but that",
      "offset": 4055.799,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "would be inconsistently tokenized as a",
      "offset": 4057.48,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "result then we try to match letters then",
      "offset": 4059.96,
      "duration": 4.92
    },
    {
      "lang": "en",
      "text": "we try to match numbers and then if that",
      "offset": 4062.52,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "doesn't work we fall back to here and",
      "offset": 4064.88,
      "duration": 4.28
    },
    {
      "lang": "en",
      "text": "what this is saying is again optional",
      "offset": 4067.559,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "space followed by something that is not",
      "offset": 4069.16,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "a letter number or a space in one or",
      "offset": 4070.839,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "more of that so what this is doing",
      "offset": 4073.96,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "effectively is this is trying to match",
      "offset": 4075.799,
      "duration": 3.721
    },
    {
      "lang": "en",
      "text": "punctuation roughly speaking not letters",
      "offset": 4077.559,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "and not numbers so this group will try",
      "offset": 4079.52,
      "duration": 4.68
    },
    {
      "lang": "en",
      "text": "to trigger for that so if I do something",
      "offset": 4082.279,
      "duration": 6.201
    },
    {
      "lang": "en",
      "text": "like this then these parts here are not",
      "offset": 4084.2,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "letters or numbers but they will",
      "offset": 4088.48,
      "duration": 3.559
    },
    {
      "lang": "en",
      "text": "actually they are uh they will actually",
      "offset": 4089.96,
      "duration": 4.52
    },
    {
      "lang": "en",
      "text": "get caught here and so they become its",
      "offset": 4092.039,
      "duration": 5.361
    },
    {
      "lang": "en",
      "text": "own group so we've separated out the",
      "offset": 4094.48,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "punctuation and finally this um this is",
      "offset": 4097.4,
      "duration": 4.759
    },
    {
      "lang": "en",
      "text": "also a little bit confusing so this is",
      "offset": 4100.08,
      "duration": 5.279
    },
    {
      "lang": "en",
      "text": "matching white space but this is using a",
      "offset": 4102.159,
      "duration": 6.881
    },
    {
      "lang": "en",
      "text": "negative look ahead assertion in regex",
      "offset": 4105.359,
      "duration": 5.561
    },
    {
      "lang": "en",
      "text": "so what this is doing is it's matching",
      "offset": 4109.04,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "wh space up to but not including the",
      "offset": 4110.92,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "last Whit space",
      "offset": 4113.279,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "character why is this important um this",
      "offset": 4115,
      "duration": 5.279
    },
    {
      "lang": "en",
      "text": "is pretty subtle I think so you see how",
      "offset": 4117.92,
      "duration": 3.799
    },
    {
      "lang": "en",
      "text": "the white space is always included at",
      "offset": 4120.279,
      "duration": 5.241
    },
    {
      "lang": "en",
      "text": "the beginning of the word so um space r",
      "offset": 4121.719,
      "duration": 6.361
    },
    {
      "lang": "en",
      "text": "space u Etc suppose we have a lot of",
      "offset": 4125.52,
      "duration": 3.88
    },
    {
      "lang": "en",
      "text": "spaces",
      "offset": 4128.08,
      "duration": 4.279
    },
    {
      "lang": "en",
      "text": "here what's going to happen here is that",
      "offset": 4129.4,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "these spaces up to not including the",
      "offset": 4132.359,
      "duration": 5.561
    },
    {
      "lang": "en",
      "text": "last character will get caught by this",
      "offset": 4134.6,
      "duration": 5.119
    },
    {
      "lang": "en",
      "text": "and what that will do is it will",
      "offset": 4137.92,
      "duration": 3.96
    },
    {
      "lang": "en",
      "text": "separate out the spaces up to but not",
      "offset": 4139.719,
      "duration": 3.96
    },
    {
      "lang": "en",
      "text": "including the last character so that the",
      "offset": 4141.88,
      "duration": 4.04
    },
    {
      "lang": "en",
      "text": "last character can come here and join",
      "offset": 4143.679,
      "duration": 5.56
    },
    {
      "lang": "en",
      "text": "with the um space you and the reason",
      "offset": 4145.92,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "that's nice is because space you is the",
      "offset": 4149.239,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "common token so if I didn't have these",
      "offset": 4151.44,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "Extra Spaces here you would just have",
      "offset": 4153.799,
      "duration": 4.36
    },
    {
      "lang": "en",
      "text": "space you and if I add tokens if I add",
      "offset": 4155.44,
      "duration": 5.279
    },
    {
      "lang": "en",
      "text": "spaces we still have a space view but",
      "offset": 4158.159,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "now we have all this extra white space",
      "offset": 4160.719,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "so basically the GB to tokenizer really",
      "offset": 4162.96,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "likes to have a space letters or numbers",
      "offset": 4164.719,
      "duration": 5.721
    },
    {
      "lang": "en",
      "text": "um and it it preens these spaces and",
      "offset": 4167.44,
      "duration": 3.96
    },
    {
      "lang": "en",
      "text": "this is just something that it is",
      "offset": 4170.44,
      "duration": 3.239
    },
    {
      "lang": "en",
      "text": "consistent about so that's what that is",
      "offset": 4171.4,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "for and then finally we have all the the",
      "offset": 4173.679,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "last fallback is um whites space",
      "offset": 4176.4,
      "duration": 6.319
    },
    {
      "lang": "en",
      "text": "characters uh so um that would be",
      "offset": 4178.64,
      "duration": 8.039
    },
    {
      "lang": "en",
      "text": "just um if that doesn't get caught then",
      "offset": 4182.719,
      "duration": 5.801
    },
    {
      "lang": "en",
      "text": "this thing will catch any trailing",
      "offset": 4186.679,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "spaces and so on I wanted to show one",
      "offset": 4188.52,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "more real world example here so if we",
      "offset": 4190.759,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "have this string which is a piece of",
      "offset": 4193.159,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "python code and then we try to split it",
      "offset": 4194.44,
      "duration": 3.96
    },
    {
      "lang": "en",
      "text": "up then this is the kind of output we",
      "offset": 4196.36,
      "duration": 4.2
    },
    {
      "lang": "en",
      "text": "get so you'll notice that the list has",
      "offset": 4198.4,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "many elements here and that's because we",
      "offset": 4200.56,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "are splitting up fairly often uh every",
      "offset": 4202.48,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "time sort of a category",
      "offset": 4205.12,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "changes um so there will never be any",
      "offset": 4207.12,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "merges Within These",
      "offset": 4209.36,
      "duration": 4.12
    },
    {
      "lang": "en",
      "text": "elements and um that's what you are",
      "offset": 4210.96,
      "duration": 5.48
    },
    {
      "lang": "en",
      "text": "seeing here now you might think that in",
      "offset": 4213.48,
      "duration": 4.28
    },
    {
      "lang": "en",
      "text": "order to train the",
      "offset": 4216.44,
      "duration": 4.68
    },
    {
      "lang": "en",
      "text": "tokenizer uh open AI has used this to",
      "offset": 4217.76,
      "duration": 6.12
    },
    {
      "lang": "en",
      "text": "split up text into chunks and then run",
      "offset": 4221.12,
      "duration": 4.68
    },
    {
      "lang": "en",
      "text": "just a BP algorithm within all the",
      "offset": 4223.88,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "chunks but that is not exactly what",
      "offset": 4225.8,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "happened and the reason is the following",
      "offset": 4227.96,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "notice that we have the spaces here uh",
      "offset": 4230.28,
      "duration": 5.16
    },
    {
      "lang": "en",
      "text": "those Spaces end up being entire",
      "offset": 4233.32,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "elements but these spaces never actually",
      "offset": 4235.44,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "end up being merged by by open Ai and",
      "offset": 4238.36,
      "duration": 4.12
    },
    {
      "lang": "en",
      "text": "the way you can tell is that if you copy",
      "offset": 4240.64,
      "duration": 3.559
    },
    {
      "lang": "en",
      "text": "paste the exact same chunk here into Tik",
      "offset": 4242.48,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "token U Tik tokenizer you see that all",
      "offset": 4244.199,
      "duration": 5.081
    },
    {
      "lang": "en",
      "text": "the spaces are kept independent and",
      "offset": 4247.28,
      "duration": 3.72
    },
    {
      "lang": "en",
      "text": "they're all token",
      "offset": 4249.28,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "220 so I think opena at some point Point",
      "offset": 4251,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "en Force some rule that these spaces",
      "offset": 4253.84,
      "duration": 5.56
    },
    {
      "lang": "en",
      "text": "would never be merged and so um there's",
      "offset": 4256.04,
      "duration": 5.24
    },
    {
      "lang": "en",
      "text": "some additional rules on top of just",
      "offset": 4259.4,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "chunking and bpe that open ey is not uh",
      "offset": 4261.28,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "clear about now the training code for",
      "offset": 4264.199,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "the gpt2 tokenizer was never released so",
      "offset": 4266.32,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "all we have is uh the code that I've",
      "offset": 4268.679,
      "duration": 4.601
    },
    {
      "lang": "en",
      "text": "already shown you but this code here",
      "offset": 4270.8,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "that they've released is only the",
      "offset": 4273.28,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "inference code for the tokens so this is",
      "offset": 4274.4,
      "duration": 4.68
    },
    {
      "lang": "en",
      "text": "not the training code you can't give it",
      "offset": 4277.679,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "a piece of text and training tokenizer",
      "offset": 4279.08,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "this is just the inference code which",
      "offset": 4281.52,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "Tak takes the merges that we have up",
      "offset": 4283.32,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "above and applies them to a new piece of",
      "offset": 4285.6,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "text and so we don't know exactly how",
      "offset": 4288.32,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "opening ey trained um train the",
      "offset": 4290.56,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "tokenizer but it wasn't as simple as",
      "offset": 4292.48,
      "duration": 5.88
    },
    {
      "lang": "en",
      "text": "chunk it up and BP it uh whatever it was",
      "offset": 4294.64,
      "duration": 5.599
    },
    {
      "lang": "en",
      "text": "next I wanted to introduce you to the",
      "offset": 4298.36,
      "duration": 4.12
    },
    {
      "lang": "en",
      "text": "Tik token library from openai which is",
      "offset": 4300.239,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "the official library for tokenization",
      "offset": 4302.48,
      "duration": 5.88
    },
    {
      "lang": "en",
      "text": "from openai so this is Tik token bip",
      "offset": 4304.8,
      "duration": 6.64
    },
    {
      "lang": "en",
      "text": "install P to Tik token and then um you",
      "offset": 4308.36,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "can do the tokenization in inference",
      "offset": 4311.44,
      "duration": 4.44
    },
    {
      "lang": "en",
      "text": "this is again not training code this is",
      "offset": 4314.36,
      "duration": 3.56
    },
    {
      "lang": "en",
      "text": "only inference code for",
      "offset": 4315.88,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "tokenization um I wanted to show you how",
      "offset": 4317.92,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "you would use it quite simple and",
      "offset": 4320.36,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "running this just gives us the gpt2",
      "offset": 4322.48,
      "duration": 4.44
    },
    {
      "lang": "en",
      "text": "tokens or the GPT 4 tokens so this is",
      "offset": 4324.36,
      "duration": 5.319
    },
    {
      "lang": "en",
      "text": "the tokenizer use for GPT 4 and so in",
      "offset": 4326.92,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "particular we see that the Whit space in",
      "offset": 4329.679,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "gpt2 remains unmerged but in GPT 4 uh",
      "offset": 4331.239,
      "duration": 6.081
    },
    {
      "lang": "en",
      "text": "these Whit spaces merge as we also saw",
      "offset": 4334.48,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "in this one where here they're all",
      "offset": 4337.32,
      "duration": 5.319
    },
    {
      "lang": "en",
      "text": "unmerged but if we go down to GPT 4 uh",
      "offset": 4339.44,
      "duration": 5.799
    },
    {
      "lang": "en",
      "text": "they become merged",
      "offset": 4342.639,
      "duration": 5.121
    },
    {
      "lang": "en",
      "text": "um now in the",
      "offset": 4345.239,
      "duration": 5.801
    },
    {
      "lang": "en",
      "text": "gp4 uh tokenizer they changed the",
      "offset": 4347.76,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "regular expression that they use to",
      "offset": 4351.04,
      "duration": 4.599
    },
    {
      "lang": "en",
      "text": "Chunk Up text so the way to see this is",
      "offset": 4353.12,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "that if you come to your the Tik token",
      "offset": 4355.639,
      "duration": 5.441
    },
    {
      "lang": "en",
      "text": "uh library and then you go to this file",
      "offset": 4358,
      "duration": 6.12
    },
    {
      "lang": "en",
      "text": "Tik token X openi public this is where",
      "offset": 4361.08,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "sort of like the definition of all these",
      "offset": 4364.12,
      "duration": 2.84
    },
    {
      "lang": "en",
      "text": "different tokenizers that openi",
      "offset": 4365.639,
      "duration": 4.921
    },
    {
      "lang": "en",
      "text": "maintains is and so uh necessarily to do",
      "offset": 4366.96,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "the inference they had to publish some",
      "offset": 4370.56,
      "duration": 3.4
    },
    {
      "lang": "en",
      "text": "of the details about the strings",
      "offset": 4371.76,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "so this is the string that we already",
      "offset": 4373.96,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "saw for gpt2 it is slightly different",
      "offset": 4375.36,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "but it is actually equivalent uh to what",
      "offset": 4378.36,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "we discussed here so this pattern that",
      "offset": 4380.36,
      "duration": 4.6
    },
    {
      "lang": "en",
      "text": "we discussed is equivalent to this",
      "offset": 4382.84,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "pattern this one just executes a little",
      "offset": 4384.96,
      "duration": 4.279
    },
    {
      "lang": "en",
      "text": "bit faster so here you see a little bit",
      "offset": 4387,
      "duration": 3.719
    },
    {
      "lang": "en",
      "text": "of a slightly different definition but",
      "offset": 4389.239,
      "duration": 3.48
    },
    {
      "lang": "en",
      "text": "otherwise it's the same we're going to",
      "offset": 4390.719,
      "duration": 4.601
    },
    {
      "lang": "en",
      "text": "go into special tokens in a bit and then",
      "offset": 4392.719,
      "duration": 5.881
    },
    {
      "lang": "en",
      "text": "if you scroll down to CL 100k this is",
      "offset": 4395.32,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "the GPT 4 tokenizer you see that the",
      "offset": 4398.6,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "pattern has changed um and this is kind",
      "offset": 4400.76,
      "duration": 5.32
    },
    {
      "lang": "en",
      "text": "of like the main the major change in",
      "offset": 4403.96,
      "duration": 3.4
    },
    {
      "lang": "en",
      "text": "addition to a bunch of other special",
      "offset": 4406.08,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "tokens which I'll go into in a bit again",
      "offset": 4407.36,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "now some I'm not going to actually go",
      "offset": 4410.4,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "into the full detail of the pattern",
      "offset": 4411.84,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "change because honestly this is my",
      "offset": 4413.28,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "numbing uh I would just advise that you",
      "offset": 4415.44,
      "duration": 4.44
    },
    {
      "lang": "en",
      "text": "pull out chat GPT and the regex",
      "offset": 4417.44,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "documentation and just step through it",
      "offset": 4419.88,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "but really the major changes are number",
      "offset": 4422.159,
      "duration": 5.921
    },
    {
      "lang": "en",
      "text": "one you see this eye here that means",
      "offset": 4424.52,
      "duration": 6.56
    },
    {
      "lang": "en",
      "text": "that the um case sensitivity this is",
      "offset": 4428.08,
      "duration": 5.599
    },
    {
      "lang": "en",
      "text": "case insensitive match and so the",
      "offset": 4431.08,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "comment that we saw earlier on oh we",
      "offset": 4433.679,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "should have used re. uppercase uh",
      "offset": 4436.12,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "basically we're now going to be matching",
      "offset": 4438.4,
      "duration": 6.2
    },
    {
      "lang": "en",
      "text": "these apostrophe s apostrophe D",
      "offset": 4441.8,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "apostrophe M Etc uh we're going to be",
      "offset": 4444.6,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "matching them both in lowercase and in",
      "offset": 4446.92,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "uppercase so that's fixed there's a",
      "offset": 4448.6,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "bunch of different like handling of the",
      "offset": 4451.32,
      "duration": 2.76
    },
    {
      "lang": "en",
      "text": "whites space that I'm not going to go",
      "offset": 4452.76,
      "duration": 3.72
    },
    {
      "lang": "en",
      "text": "into the full details of and then one",
      "offset": 4454.08,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "more thing here is you will notice that",
      "offset": 4456.48,
      "duration": 4.199
    },
    {
      "lang": "en",
      "text": "when they match the numbers they only",
      "offset": 4458.639,
      "duration": 4.921
    },
    {
      "lang": "en",
      "text": "match one to three numbers so so they",
      "offset": 4460.679,
      "duration": 5.441
    },
    {
      "lang": "en",
      "text": "will never merge",
      "offset": 4463.56,
      "duration": 5.32
    },
    {
      "lang": "en",
      "text": "numbers that are in low in more than",
      "offset": 4466.12,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "three digits only up to three digits of",
      "offset": 4468.88,
      "duration": 5.799
    },
    {
      "lang": "en",
      "text": "numbers will ever be merged and uh",
      "offset": 4471.159,
      "duration": 5.161
    },
    {
      "lang": "en",
      "text": "that's one change that they made as well",
      "offset": 4474.679,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "to prevent uh tokens that are very very",
      "offset": 4476.32,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "long number",
      "offset": 4478.6,
      "duration": 3.48
    },
    {
      "lang": "en",
      "text": "sequences uh but again we don't really",
      "offset": 4480,
      "duration": 4.199
    },
    {
      "lang": "en",
      "text": "know why they do any of this stuff uh",
      "offset": 4482.08,
      "duration": 4.2
    },
    {
      "lang": "en",
      "text": "because none of this is documented and",
      "offset": 4484.199,
      "duration": 5.321
    },
    {
      "lang": "en",
      "text": "uh it's just we just get the pattern so",
      "offset": 4486.28,
      "duration": 5.48
    },
    {
      "lang": "en",
      "text": "um yeah it is what it is but those are",
      "offset": 4489.52,
      "duration": 4.84
    },
    {
      "lang": "en",
      "text": "some of the changes that gp4 has made",
      "offset": 4491.76,
      "duration": 4.6
    },
    {
      "lang": "en",
      "text": "and of course the vocabulary size went",
      "offset": 4494.36,
      "duration": 4.04
    },
    {
      "lang": "en",
      "text": "from roughly 50k to roughly",
      "offset": 4496.36,
      "duration": 4.04
    },
    {
      "lang": "en",
      "text": "100K the next thing I would like to do",
      "offset": 4498.4,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "very briefly is to take you through the",
      "offset": 4500.4,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "gpt2 encoder dopy that openi has",
      "offset": 4502.32,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "released uh this is the file that I",
      "offset": 4505.4,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "already mentioned to you briefly now",
      "offset": 4507.36,
      "duration": 5.48
    },
    {
      "lang": "en",
      "text": "this file is uh fairly short and should",
      "offset": 4509.639,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "be relatively understandable to you at",
      "offset": 4512.84,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "this point um starting at the bottom",
      "offset": 4514.639,
      "duration": 6.841
    },
    {
      "lang": "en",
      "text": "here they are loading two files encoder",
      "offset": 4517.96,
      "duration": 6.199
    },
    {
      "lang": "en",
      "text": "Json and vocab bpe and they do some",
      "offset": 4521.48,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "light processing on it and then they",
      "offset": 4524.159,
      "duration": 3.56
    },
    {
      "lang": "en",
      "text": "call this encoder object which is the",
      "offset": 4525.4,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "tokenizer now if you'd like to inspect",
      "offset": 4527.719,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "these two files which together",
      "offset": 4530.12,
      "duration": 4.44
    },
    {
      "lang": "en",
      "text": "constitute their saved tokenizer then",
      "offset": 4531.96,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "you can do that with a piece of code",
      "offset": 4534.56,
      "duration": 2.28
    },
    {
      "lang": "en",
      "text": "like",
      "offset": 4536.12,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "this um this is where you can download",
      "offset": 4536.84,
      "duration": 3.96
    },
    {
      "lang": "en",
      "text": "these two files and you can inspect them",
      "offset": 4539.32,
      "duration": 3.56
    },
    {
      "lang": "en",
      "text": "if you'd like and what you will find is",
      "offset": 4540.8,
      "duration": 4.28
    },
    {
      "lang": "en",
      "text": "that this encoder as they call it in",
      "offset": 4542.88,
      "duration": 4.759
    },
    {
      "lang": "en",
      "text": "their code is exactly equivalent to our",
      "offset": 4545.08,
      "duration": 6.72
    },
    {
      "lang": "en",
      "text": "vocab so remember here where we have",
      "offset": 4547.639,
      "duration": 5.841
    },
    {
      "lang": "en",
      "text": "this vocab object which allowed us us to",
      "offset": 4551.8,
      "duration": 4.2
    },
    {
      "lang": "en",
      "text": "decode very efficiently and basically it",
      "offset": 4553.48,
      "duration": 6.64
    },
    {
      "lang": "en",
      "text": "took us from the integer to the byes uh",
      "offset": 4556,
      "duration": 7.32
    },
    {
      "lang": "en",
      "text": "for that integer so our vocab is exactly",
      "offset": 4560.12,
      "duration": 7.64
    },
    {
      "lang": "en",
      "text": "their encoder and then their vocab bpe",
      "offset": 4563.32,
      "duration": 7.839
    },
    {
      "lang": "en",
      "text": "confusingly is actually are merges so",
      "offset": 4567.76,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "their BP merges which is based on the",
      "offset": 4571.159,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "data inside vocab bpe ends up being",
      "offset": 4574,
      "duration": 6.679
    },
    {
      "lang": "en",
      "text": "equivalent to our merges so uh basically",
      "offset": 4576.679,
      "duration": 7.681
    },
    {
      "lang": "en",
      "text": "they are saving and loading the two uh",
      "offset": 4580.679,
      "duration": 5.56
    },
    {
      "lang": "en",
      "text": "variables that for us are also critical",
      "offset": 4584.36,
      "duration": 3.96
    },
    {
      "lang": "en",
      "text": "the merges variable and the vocab",
      "offset": 4586.239,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "variable using just these two variables",
      "offset": 4588.32,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "you can represent a tokenizer and you",
      "offset": 4591.12,
      "duration": 3.4
    },
    {
      "lang": "en",
      "text": "can both do encoding and decoding once",
      "offset": 4592.56,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "you've trained this",
      "offset": 4594.52,
      "duration": 5.48
    },
    {
      "lang": "en",
      "text": "tokenizer now the only thing that um is",
      "offset": 4596,
      "duration": 6.56
    },
    {
      "lang": "en",
      "text": "actually slightly confusing inside what",
      "offset": 4600,
      "duration": 4.52
    },
    {
      "lang": "en",
      "text": "opening ey does here is that in addition",
      "offset": 4602.56,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "to this encoder and a decoder they also",
      "offset": 4604.52,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "have something called a bite encoder and",
      "offset": 4606.88,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "a bite decoder and this is actually",
      "offset": 4608.52,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "unfortunately just",
      "offset": 4611.28,
      "duration": 4.6
    },
    {
      "lang": "en",
      "text": "kind of a spirous implementation detail",
      "offset": 4613.96,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "and isn't actually deep or interesting",
      "offset": 4615.88,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "in any way so I'm going to skip the",
      "offset": 4617.719,
      "duration": 3.321
    },
    {
      "lang": "en",
      "text": "discussion of it but what opening ey",
      "offset": 4619.08,
      "duration": 3.72
    },
    {
      "lang": "en",
      "text": "does here for reasons that I don't fully",
      "offset": 4621.04,
      "duration": 3.96
    },
    {
      "lang": "en",
      "text": "understand is that not only have they",
      "offset": 4622.8,
      "duration": 3.64
    },
    {
      "lang": "en",
      "text": "this tokenizer which can encode and",
      "offset": 4625,
      "duration": 3.159
    },
    {
      "lang": "en",
      "text": "decode but they have a whole separate",
      "offset": 4626.44,
      "duration": 3.56
    },
    {
      "lang": "en",
      "text": "layer here in addition that is used",
      "offset": 4628.159,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "serially with the tokenizer and so you",
      "offset": 4630,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "first do um bite encode and then encode",
      "offset": 4632.639,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "and then you do decode and then bite",
      "offset": 4636.08,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "decode so that's the loop and they are",
      "offset": 4637.679,
      "duration": 5.161
    },
    {
      "lang": "en",
      "text": "just stacked serial on top of each other",
      "offset": 4640.239,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "and and it's not that interesting so I",
      "offset": 4642.84,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "won't cover it and you can step through",
      "offset": 4644.719,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "it if you'd like otherwise this file if",
      "offset": 4645.96,
      "duration": 4.279
    },
    {
      "lang": "en",
      "text": "you ignore the bite encoder and the bite",
      "offset": 4648.639,
      "duration": 3.241
    },
    {
      "lang": "en",
      "text": "decoder will be algorithmically very",
      "offset": 4650.239,
      "duration": 3.721
    },
    {
      "lang": "en",
      "text": "familiar with you and the meat of it",
      "offset": 4651.88,
      "duration": 5.16
    },
    {
      "lang": "en",
      "text": "here is the what they call bpe function",
      "offset": 4653.96,
      "duration": 5.679
    },
    {
      "lang": "en",
      "text": "and you should recognize this Loop here",
      "offset": 4657.04,
      "duration": 4.92
    },
    {
      "lang": "en",
      "text": "which is very similar to our own y Loop",
      "offset": 4659.639,
      "duration": 3.881
    },
    {
      "lang": "en",
      "text": "where they're trying to identify the",
      "offset": 4661.96,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "Byram uh a pair that they should be",
      "offset": 4663.52,
      "duration": 5.639
    },
    {
      "lang": "en",
      "text": "merging next and then here just like we",
      "offset": 4666.96,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "had they have a for Loop trying to merge",
      "offset": 4669.159,
      "duration": 4.441
    },
    {
      "lang": "en",
      "text": "this pair uh so they will go over all of",
      "offset": 4670.96,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "the sequence and they will merge the",
      "offset": 4673.6,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "pair whenever they find it and they keep",
      "offset": 4675.12,
      "duration": 4.68
    },
    {
      "lang": "en",
      "text": "repeating that until they run out of",
      "offset": 4677.84,
      "duration": 4.52
    },
    {
      "lang": "en",
      "text": "possible merges in the in the text so",
      "offset": 4679.8,
      "duration": 4.76
    },
    {
      "lang": "en",
      "text": "that's the meat of this file and uh",
      "offset": 4682.36,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "there's an encode and a decode function",
      "offset": 4684.56,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "just like we have implemented it so long",
      "offset": 4686.04,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "story short what I want you to take away",
      "offset": 4688.159,
      "duration": 3.48
    },
    {
      "lang": "en",
      "text": "at this point is that unfortunately it's",
      "offset": 4689.719,
      "duration": 3.281
    },
    {
      "lang": "en",
      "text": "a little bit of a messy code that they",
      "offset": 4691.639,
      "duration": 3.481
    },
    {
      "lang": "en",
      "text": "have but algorithmically it is identical",
      "offset": 4693,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "to what we've built up above and what",
      "offset": 4695.12,
      "duration": 4.039
    },
    {
      "lang": "en",
      "text": "we've built up above if you understand",
      "offset": 4697.719,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "it is algorithmically what is necessary",
      "offset": 4699.159,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "to actually build a BP to organizer",
      "offset": 4701.32,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "train it and then both encode and decode",
      "offset": 4703.719,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "the next topic I would like to turn to",
      "offset": 4706.84,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "is that of special tokens so in addition",
      "offset": 4708.28,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "to tokens that are coming from you know",
      "offset": 4710.92,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "raw bytes and the BP merges we can",
      "offset": 4712.6,
      "duration": 4.2
    },
    {
      "lang": "en",
      "text": "insert all kinds of tokens that we are",
      "offset": 4715.239,
      "duration": 3.721
    },
    {
      "lang": "en",
      "text": "going to use to delimit different parts",
      "offset": 4716.8,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "of the data or introduced to create a",
      "offset": 4718.96,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "special structure of the token streams",
      "offset": 4721.04,
      "duration": 6.44
    },
    {
      "lang": "en",
      "text": "so in uh if you look at this encoder",
      "offset": 4724.8,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "object from open AIS gpd2 right here we",
      "offset": 4727.48,
      "duration": 4.679
    },
    {
      "lang": "en",
      "text": "mentioned this is very similar to our",
      "offset": 4730.88,
      "duration": 3.96
    },
    {
      "lang": "en",
      "text": "vocab you'll notice that the length of",
      "offset": 4732.159,
      "duration": 5.601
    },
    {
      "lang": "en",
      "text": "this is",
      "offset": 4734.84,
      "duration": 2.92
    },
    {
      "lang": "en",
      "text": "50257 and as I mentioned it's mapping uh",
      "offset": 4738.88,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "and it's inverted from the mapping of",
      "offset": 4741.84,
      "duration": 4.28
    },
    {
      "lang": "en",
      "text": "our vocab our vocab goes from integer to",
      "offset": 4743.36,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "string and they go the other way around",
      "offset": 4746.12,
      "duration": 5.72
    },
    {
      "lang": "en",
      "text": "for no amazing reason um but the thing",
      "offset": 4748.08,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "to note here is that this the mapping",
      "offset": 4751.84,
      "duration": 3.16
    },
    {
      "lang": "en",
      "text": "table here is",
      "offset": 4753.28,
      "duration": 5.32
    },
    {
      "lang": "en",
      "text": "50257 where does that number come from",
      "offset": 4755,
      "duration": 5.8
    },
    {
      "lang": "en",
      "text": "where what are the tokens as I mentioned",
      "offset": 4758.6,
      "duration": 5.8
    },
    {
      "lang": "en",
      "text": "there are 256 raw bite token",
      "offset": 4760.8,
      "duration": 6.399
    },
    {
      "lang": "en",
      "text": "tokens and then opena actually did",
      "offset": 4764.4,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "50,000",
      "offset": 4767.199,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "merges so those become the other tokens",
      "offset": 4768.639,
      "duration": 5.401
    },
    {
      "lang": "en",
      "text": "but this would have been",
      "offset": 4772,
      "duration": 5.679
    },
    {
      "lang": "en",
      "text": "50256 so what is the 57th token and",
      "offset": 4774.04,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "there is basically one special",
      "offset": 4777.679,
      "duration": 5.56
    },
    {
      "lang": "en",
      "text": "token and that one special token you can",
      "offset": 4780.52,
      "duration": 6.52
    },
    {
      "lang": "en",
      "text": "see is called end of text so this is a",
      "offset": 4783.239,
      "duration": 6.321
    },
    {
      "lang": "en",
      "text": "special token and it's the very last",
      "offset": 4787.04,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "token and this token is used to delimit",
      "offset": 4789.56,
      "duration": 6.2
    },
    {
      "lang": "en",
      "text": "documents ments in the training set so",
      "offset": 4792.48,
      "duration": 4.84
    },
    {
      "lang": "en",
      "text": "when we're creating the training data we",
      "offset": 4795.76,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "have all these documents and we tokenize",
      "offset": 4797.32,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "them and we get a stream of tokens those",
      "offset": 4799.199,
      "duration": 6.081
    },
    {
      "lang": "en",
      "text": "tokens only range from Z to",
      "offset": 4801.8,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "50256 and then in between those",
      "offset": 4805.28,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "documents we put special end of text",
      "offset": 4807.4,
      "duration": 5.4
    },
    {
      "lang": "en",
      "text": "token and we insert that token in",
      "offset": 4810.4,
      "duration": 5.239
    },
    {
      "lang": "en",
      "text": "between documents and we are using this",
      "offset": 4812.8,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "as a signal to the language model that",
      "offset": 4815.639,
      "duration": 5.08
    },
    {
      "lang": "en",
      "text": "the document has ended and what follows",
      "offset": 4818.4,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "is going to be unrelated to the document",
      "offset": 4820.719,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "previously that said the language model",
      "offset": 4823.28,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "has to learn this from data it it needs",
      "offset": 4825.199,
      "duration": 4.52
    },
    {
      "lang": "en",
      "text": "to learn that this token usually means",
      "offset": 4827.199,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "that it should wipe its sort of memory",
      "offset": 4829.719,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "of what came before and what came before",
      "offset": 4831.92,
      "duration": 3.64
    },
    {
      "lang": "en",
      "text": "this token is not actually informative",
      "offset": 4834.04,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "to what comes next but we are expecting",
      "offset": 4835.56,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "the language model to just like learn",
      "offset": 4837.56,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "this but we're giving it the Special",
      "offset": 4839,
      "duration": 5.08
    },
    {
      "lang": "en",
      "text": "sort of the limiter of these documents",
      "offset": 4840.92,
      "duration": 5.759
    },
    {
      "lang": "en",
      "text": "we can go here to Tech tokenizer and um",
      "offset": 4844.08,
      "duration": 5.4
    },
    {
      "lang": "en",
      "text": "this the gpt2 tokenizer uh our code that",
      "offset": 4846.679,
      "duration": 4.761
    },
    {
      "lang": "en",
      "text": "we've been playing with before so we can",
      "offset": 4849.48,
      "duration": 4.199
    },
    {
      "lang": "en",
      "text": "add here right hello world world how are",
      "offset": 4851.44,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "you and we're getting different tokens",
      "offset": 4853.679,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "but now you can see what if what happens",
      "offset": 4855.84,
      "duration": 6.359
    },
    {
      "lang": "en",
      "text": "if I put end of text you see how until I",
      "offset": 4858.239,
      "duration": 5.681
    },
    {
      "lang": "en",
      "text": "finished it these are all different",
      "offset": 4862.199,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "tokens end of",
      "offset": 4863.92,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "text still set different tokens and now",
      "offset": 4866.36,
      "duration": 6.92
    },
    {
      "lang": "en",
      "text": "when I finish it suddenly we get token",
      "offset": 4868.8,
      "duration": 7.08
    },
    {
      "lang": "en",
      "text": "50256 and the reason this works is",
      "offset": 4873.28,
      "duration": 4.959
    },
    {
      "lang": "en",
      "text": "because this didn't actually go through",
      "offset": 4875.88,
      "duration": 6.04
    },
    {
      "lang": "en",
      "text": "the bpe merges instead the code that",
      "offset": 4878.239,
      "duration": 6.761
    },
    {
      "lang": "en",
      "text": "actually outposted tokens has special",
      "offset": 4881.92,
      "duration": 6.12
    },
    {
      "lang": "en",
      "text": "case instructions for handling special",
      "offset": 4885,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "tokens um we did not see these special",
      "offset": 4888.04,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "instructions for handling special tokens",
      "offset": 4890.76,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "in the encoder dopy it's absent there",
      "offset": 4892.84,
      "duration": 5.16
    },
    {
      "lang": "en",
      "text": "but if you go to Tech token Library",
      "offset": 4896.36,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "which is uh implemented in Rust you will",
      "offset": 4898,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "find all kinds of special case handling",
      "offset": 4900.92,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "for these special tokens that you can",
      "offset": 4902.639,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "register uh create adds to the",
      "offset": 4904.52,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "vocabulary and then it looks for them",
      "offset": 4907.12,
      "duration": 3.8
    },
    {
      "lang": "en",
      "text": "and it uh whenever it sees these special",
      "offset": 4909,
      "duration": 4.44
    },
    {
      "lang": "en",
      "text": "tokens like this it will actually come",
      "offset": 4910.92,
      "duration": 5.16
    },
    {
      "lang": "en",
      "text": "in and swap in that special token so",
      "offset": 4913.44,
      "duration": 4.68
    },
    {
      "lang": "en",
      "text": "these things are outside of the typical",
      "offset": 4916.08,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "algorithm of uh B PA en",
      "offset": 4918.12,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "coding so these special tokens are used",
      "offset": 4920.56,
      "duration": 5.079
    },
    {
      "lang": "en",
      "text": "pervasively uh not just in uh basically",
      "offset": 4922.92,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "base language modeling of predicting the",
      "offset": 4925.639,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "next token in the sequence but",
      "offset": 4927.4,
      "duration": 3.279
    },
    {
      "lang": "en",
      "text": "especially when it gets to later to the",
      "offset": 4929.08,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "fine tuning stage and all of the chat uh",
      "offset": 4930.679,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "gbt sort of aspects of it uh because we",
      "offset": 4933.239,
      "duration": 3.641
    },
    {
      "lang": "en",
      "text": "don't just want to Del limit documents",
      "offset": 4935.679,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "we want to delimit entire conversations",
      "offset": 4936.88,
      "duration": 4.68
    },
    {
      "lang": "en",
      "text": "between an assistant and a user so if I",
      "offset": 4938.719,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "refresh this sck tokenizer page the",
      "offset": 4941.56,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "default example that they have here is",
      "offset": 4944.239,
      "duration": 5.881
    },
    {
      "lang": "en",
      "text": "using not sort of base model encoders",
      "offset": 4946.44,
      "duration": 7.16
    },
    {
      "lang": "en",
      "text": "but ftuned model uh sort of tokenizers",
      "offset": 4950.12,
      "duration": 5.72
    },
    {
      "lang": "en",
      "text": "um so for example using the GPT 3.5",
      "offset": 4953.6,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "turbo scheme these here are all special",
      "offset": 4955.84,
      "duration": 7.399
    },
    {
      "lang": "en",
      "text": "tokens I am start I end Etc uh this is",
      "offset": 4958.96,
      "duration": 7.88
    },
    {
      "lang": "en",
      "text": "short for Imaginary mcore start by the",
      "offset": 4963.239,
      "duration": 6.361
    },
    {
      "lang": "en",
      "text": "way but you can see here that there's a",
      "offset": 4966.84,
      "duration": 4.359
    },
    {
      "lang": "en",
      "text": "sort of start and end of every single",
      "offset": 4969.6,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "message and there can be many other",
      "offset": 4971.199,
      "duration": 5.321
    },
    {
      "lang": "en",
      "text": "other tokens lots of tokens um in use to",
      "offset": 4972.56,
      "duration": 6.159
    },
    {
      "lang": "en",
      "text": "delimit these conversations and kind of",
      "offset": 4976.52,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "keep track of the flow of the messages",
      "offset": 4978.719,
      "duration": 5.081
    },
    {
      "lang": "en",
      "text": "here now we can go back to the Tik token",
      "offset": 4980.84,
      "duration": 5.399
    },
    {
      "lang": "en",
      "text": "library and here when you scroll to the",
      "offset": 4983.8,
      "duration": 4.359
    },
    {
      "lang": "en",
      "text": "bottom they talk about how you can",
      "offset": 4986.239,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "extend tick token and I can you can",
      "offset": 4988.159,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "create basically you can Fork uh the um",
      "offset": 4990.239,
      "duration": 7.081
    },
    {
      "lang": "en",
      "text": "CL 100K base tokenizers in gp4 and for",
      "offset": 4993.679,
      "duration": 5.241
    },
    {
      "lang": "en",
      "text": "example you can extend it by adding more",
      "offset": 4997.32,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "special tokens and these are totally up",
      "offset": 4998.92,
      "duration": 2.44
    },
    {
      "lang": "en",
      "text": "to you you can come up with any",
      "offset": 5000.36,
      "duration": 3.4
    },
    {
      "lang": "en",
      "text": "arbitrary tokens and add them with the",
      "offset": 5001.36,
      "duration": 5.16
    },
    {
      "lang": "en",
      "text": "new ID afterwards and the tikken library",
      "offset": 5003.76,
      "duration": 6.12
    },
    {
      "lang": "en",
      "text": "will uh correctly swap them out uh when",
      "offset": 5006.52,
      "duration": 5.24
    },
    {
      "lang": "en",
      "text": "it sees this in the",
      "offset": 5009.88,
      "duration": 5.08
    },
    {
      "lang": "en",
      "text": "strings now we can also go back to this",
      "offset": 5011.76,
      "duration": 5.32
    },
    {
      "lang": "en",
      "text": "file which we've looked at previously",
      "offset": 5014.96,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "and I mentioned that the gpt2 in Tik",
      "offset": 5017.08,
      "duration": 4.36
    },
    {
      "lang": "en",
      "text": "toen open",
      "offset": 5019.679,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "I.P we have the vocabulary we have the",
      "offset": 5021.44,
      "duration": 4.84
    },
    {
      "lang": "en",
      "text": "pattern for splitting and then here we",
      "offset": 5024,
      "duration": 4.04
    },
    {
      "lang": "en",
      "text": "are registering the single special token",
      "offset": 5026.28,
      "duration": 4.04
    },
    {
      "lang": "en",
      "text": "in gpd2 which was the end of text token",
      "offset": 5028.04,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "and we saw that it has this ID",
      "offset": 5030.32,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "in GPT 4 when they defy this here you",
      "offset": 5033,
      "duration": 4.6
    },
    {
      "lang": "en",
      "text": "see that the pattern has changed as",
      "offset": 5036.4,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "we've discussed but also the special",
      "offset": 5037.6,
      "duration": 4.2
    },
    {
      "lang": "en",
      "text": "tokens have changed in this tokenizer so",
      "offset": 5039.36,
      "duration": 4.359
    },
    {
      "lang": "en",
      "text": "we of course have the end of text just",
      "offset": 5041.8,
      "duration": 5.08
    },
    {
      "lang": "en",
      "text": "like in gpd2 but we also see three sorry",
      "offset": 5043.719,
      "duration": 5.801
    },
    {
      "lang": "en",
      "text": "four additional tokens here Thim prefix",
      "offset": 5046.88,
      "duration": 5.48
    },
    {
      "lang": "en",
      "text": "middle and suffix what is fim fim is",
      "offset": 5049.52,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "short for fill in the middle and if",
      "offset": 5052.36,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "you'd like to learn more about this idea",
      "offset": 5054.88,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "it comes from this paper um and I'm not",
      "offset": 5057,
      "duration": 4.199
    },
    {
      "lang": "en",
      "text": "going to go into detail in this video",
      "offset": 5060,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "it's beyond this video and then there's",
      "offset": 5061.199,
      "duration": 5.841
    },
    {
      "lang": "en",
      "text": "one additional uh serve token here so",
      "offset": 5063.44,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "that's that encoding as well so it's",
      "offset": 5067.04,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "very common basically to train a",
      "offset": 5069.92,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "language model and then if you'd like uh",
      "offset": 5071.6,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "you can add special tokens now when you",
      "offset": 5074.719,
      "duration": 5.081
    },
    {
      "lang": "en",
      "text": "add special tokens you of course have to",
      "offset": 5077.52,
      "duration": 4.199
    },
    {
      "lang": "en",
      "text": "um do some model surgery to the",
      "offset": 5079.8,
      "duration": 3.64
    },
    {
      "lang": "en",
      "text": "Transformer and all the parameters",
      "offset": 5081.719,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "involved in that Transformer because you",
      "offset": 5083.44,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "are basically adding an integer and you",
      "offset": 5085.159,
      "duration": 3.401
    },
    {
      "lang": "en",
      "text": "want to make sure that for example your",
      "offset": 5087.119,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "embedding Matrix for the vocabulary",
      "offset": 5088.56,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "tokens has to be extended by adding a",
      "offset": 5090.639,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "row and typically this row would be",
      "offset": 5093.04,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "initialized uh with small random numbers",
      "offset": 5094.88,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "or something like that because we need",
      "offset": 5096.88,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "to have a vector that now stands for",
      "offset": 5098.8,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "that token in addition to that you have",
      "offset": 5101.199,
      "duration": 3.081
    },
    {
      "lang": "en",
      "text": "to go to the final layer of the",
      "offset": 5103.28,
      "duration": 2.399
    },
    {
      "lang": "en",
      "text": "Transformer and you have to make sure",
      "offset": 5104.28,
      "duration": 3.24
    },
    {
      "lang": "en",
      "text": "that that projection at the very end",
      "offset": 5105.679,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "into the classifier uh is extended by",
      "offset": 5107.52,
      "duration": 4.28
    },
    {
      "lang": "en",
      "text": "one as well so basically there's some",
      "offset": 5109.679,
      "duration": 3.801
    },
    {
      "lang": "en",
      "text": "model surgery involved that you have to",
      "offset": 5111.8,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "couple with the tokenization changes if",
      "offset": 5113.48,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "you are going to add special tokens but",
      "offset": 5116.52,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "this is a very common operation that",
      "offset": 5118.92,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "people do especially if they'd like to",
      "offset": 5120.199,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "fine tune the model for example taking",
      "offset": 5121.8,
      "duration": 4.439
    },
    {
      "lang": "en",
      "text": "it from a base model to a chat model",
      "offset": 5123.719,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "like chat",
      "offset": 5126.239,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "GPT okay so at this point you should",
      "offset": 5127.88,
      "duration": 3.16
    },
    {
      "lang": "en",
      "text": "have everything you need in order to",
      "offset": 5129.84,
      "duration": 3.879
    },
    {
      "lang": "en",
      "text": "build your own gp4 tokenizer now in the",
      "offset": 5131.04,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "process of developing this lecture I've",
      "offset": 5133.719,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "done that and I published the code under",
      "offset": 5135.36,
      "duration": 3.56
    },
    {
      "lang": "en",
      "text": "this repository",
      "offset": 5137.239,
      "duration": 5.281
    },
    {
      "lang": "en",
      "text": "MBP so MBP looks like this right now as",
      "offset": 5138.92,
      "duration": 6.44
    },
    {
      "lang": "en",
      "text": "I'm recording but uh the MBP repository",
      "offset": 5142.52,
      "duration": 4.199
    },
    {
      "lang": "en",
      "text": "will probably change quite a bit because",
      "offset": 5145.36,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "I intend to continue working on it um in",
      "offset": 5146.719,
      "duration": 5.041
    },
    {
      "lang": "en",
      "text": "addition to the MBP repository I've",
      "offset": 5149.84,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "published the this uh exercise",
      "offset": 5151.76,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "progression that you can follow so if",
      "offset": 5153.44,
      "duration": 4.92
    },
    {
      "lang": "en",
      "text": "you go to exercise. MD here uh this is",
      "offset": 5155.36,
      "duration": 5.799
    },
    {
      "lang": "en",
      "text": "sort of me breaking up the task ahead of",
      "offset": 5158.36,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "you into four steps that sort of uh",
      "offset": 5161.159,
      "duration": 5.48
    },
    {
      "lang": "en",
      "text": "build up to what can be a gp4 tokenizer",
      "offset": 5163.4,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "and so feel free to follow these steps",
      "offset": 5166.639,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "exactly and follow a little bit of the",
      "offset": 5168.4,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "guidance that I've laid out here and",
      "offset": 5170.4,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "anytime you feel stuck just reference",
      "offset": 5172.48,
      "duration": 5.48
    },
    {
      "lang": "en",
      "text": "the MBP repository here so either the",
      "offset": 5174.639,
      "duration": 5.441
    },
    {
      "lang": "en",
      "text": "tests could be useful or the MBP",
      "offset": 5177.96,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "repository itself I try to keep the code",
      "offset": 5180.08,
      "duration": 6.079
    },
    {
      "lang": "en",
      "text": "fairly clean and understandable and so",
      "offset": 5182.6,
      "duration": 6.32
    },
    {
      "lang": "en",
      "text": "um feel free to reference it whenever um",
      "offset": 5186.159,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "you get",
      "offset": 5188.92,
      "duration": 3.64
    },
    {
      "lang": "en",
      "text": "stuck uh in addition to that basically",
      "offset": 5190.159,
      "duration": 4.52
    },
    {
      "lang": "en",
      "text": "once you write it you should be able to",
      "offset": 5192.56,
      "duration": 4.28
    },
    {
      "lang": "en",
      "text": "reproduce this behavior from Tech token",
      "offset": 5194.679,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "so getting the gb4 tokenizer you can",
      "offset": 5196.84,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "take uh you can encode the string and",
      "offset": 5199.32,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "you should get these tokens and then you",
      "offset": 5201.32,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "can encode and decode the exact same",
      "offset": 5203.239,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "string to recover it and in addition to",
      "offset": 5204.679,
      "duration": 3.721
    },
    {
      "lang": "en",
      "text": "all that you should be able to implement",
      "offset": 5207.239,
      "duration": 3.48
    },
    {
      "lang": "en",
      "text": "your own train function uh which Tik",
      "offset": 5208.4,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "token Library does not provide it's it's",
      "offset": 5210.719,
      "duration": 3.881
    },
    {
      "lang": "en",
      "text": "again only inference code but you could",
      "offset": 5212.48,
      "duration": 5.4
    },
    {
      "lang": "en",
      "text": "write your own train MBP does it as well",
      "offset": 5214.6,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "and that will allow you to train your",
      "offset": 5217.88,
      "duration": 2.839
    },
    {
      "lang": "en",
      "text": "own token",
      "offset": 5219.32,
      "duration": 3.08
    },
    {
      "lang": "en",
      "text": "vocabularies so here are some of the",
      "offset": 5220.719,
      "duration": 5.321
    },
    {
      "lang": "en",
      "text": "code inside M be mean bpe uh shows the",
      "offset": 5222.4,
      "duration": 6.319
    },
    {
      "lang": "en",
      "text": "token vocabularies that you might obtain",
      "offset": 5226.04,
      "duration": 6.36
    },
    {
      "lang": "en",
      "text": "so on the left uh here we have the GPT 4",
      "offset": 5228.719,
      "duration": 7.121
    },
    {
      "lang": "en",
      "text": "merges uh so the first 256 are raw",
      "offset": 5232.4,
      "duration": 5.319
    },
    {
      "lang": "en",
      "text": "individual bytes and then here I am",
      "offset": 5235.84,
      "duration": 3.72
    },
    {
      "lang": "en",
      "text": "visualizing the merges that gp4",
      "offset": 5237.719,
      "duration": 4.041
    },
    {
      "lang": "en",
      "text": "performed during its training so the",
      "offset": 5239.56,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "very first merge that gp4 did was merge",
      "offset": 5241.76,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "two spaces into a single token for you",
      "offset": 5244.92,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "know two spaces and that is a token 256",
      "offset": 5247.6,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "and so this is the order in which things",
      "offset": 5250.84,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "merged during gb4 training and this is",
      "offset": 5252.239,
      "duration": 6.841
    },
    {
      "lang": "en",
      "text": "the merge order that um we obtain in MBP",
      "offset": 5254.679,
      "duration": 6.52
    },
    {
      "lang": "en",
      "text": "by training a tokenizer and in this case",
      "offset": 5259.08,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "I trained it on a Wikipedia page of",
      "offset": 5261.199,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "Taylor Swift uh not because I'm a Swifty",
      "offset": 5263.239,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "but because that is one of the longest",
      "offset": 5265.6,
      "duration": 4.039
    },
    {
      "lang": "en",
      "text": "um Wikipedia Pages apparently that's",
      "offset": 5267.8,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "available but she is pretty cool and",
      "offset": 5269.639,
      "duration": 7
    },
    {
      "lang": "en",
      "text": "um what was I going to say yeah so you",
      "offset": 5274.04,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "can compare these two uh vocabularies",
      "offset": 5276.639,
      "duration": 7.361
    },
    {
      "lang": "en",
      "text": "and so as an example um here GPT for",
      "offset": 5279.08,
      "duration": 7.72
    },
    {
      "lang": "en",
      "text": "merged I in to become in and we've done",
      "offset": 5284,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "the exact same thing on this token 259",
      "offset": 5286.8,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "here space t becomes space t and that",
      "offset": 5290,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "happened for us a little bit later as",
      "offset": 5293.28,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "well so the difference here is again to",
      "offset": 5294.639,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "my understanding only a difference of",
      "offset": 5296.719,
      "duration": 3.561
    },
    {
      "lang": "en",
      "text": "the training set so as an example",
      "offset": 5298.4,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "because I see a lot of white space I",
      "offset": 5300.28,
      "duration": 3.48
    },
    {
      "lang": "en",
      "text": "supect that gp4 probably had a lot of",
      "offset": 5302.08,
      "duration": 3.4
    },
    {
      "lang": "en",
      "text": "python code in its training set I'm not",
      "offset": 5303.76,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "sure uh for the",
      "offset": 5305.48,
      "duration": 4.6
    },
    {
      "lang": "en",
      "text": "tokenizer and uh here we see much less",
      "offset": 5307.6,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "of that of course in the Wikipedia page",
      "offset": 5310.08,
      "duration": 4.599
    },
    {
      "lang": "en",
      "text": "so roughly speaking they look the same",
      "offset": 5312.96,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "and they look the same because they're",
      "offset": 5314.679,
      "duration": 3.401
    },
    {
      "lang": "en",
      "text": "running the same algorithm and when you",
      "offset": 5315.96,
      "duration": 3.239
    },
    {
      "lang": "en",
      "text": "train your own you're probably going to",
      "offset": 5318.08,
      "duration": 3.119
    },
    {
      "lang": "en",
      "text": "get something similar depending on what",
      "offset": 5319.199,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "you train it on okay so we are now going",
      "offset": 5321.199,
      "duration": 3.881
    },
    {
      "lang": "en",
      "text": "to move on from tick token and the way",
      "offset": 5323.28,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "that open AI tokenizes its strings and",
      "offset": 5325.08,
      "duration": 4.119
    },
    {
      "lang": "en",
      "text": "we're going to discuss one more very",
      "offset": 5327.6,
      "duration": 3.4
    },
    {
      "lang": "en",
      "text": "commonly used library for working with",
      "offset": 5329.199,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "tokenization inlm",
      "offset": 5331,
      "duration": 4.36
    },
    {
      "lang": "en",
      "text": "and that is sentence piece so sentence",
      "offset": 5332.719,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "piece is very commonly used in language",
      "offset": 5335.36,
      "duration": 4.759
    },
    {
      "lang": "en",
      "text": "models because unlike Tik token it can",
      "offset": 5338.159,
      "duration": 4.201
    },
    {
      "lang": "en",
      "text": "do both training and inference and is",
      "offset": 5340.119,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "quite efficient at both it supports a",
      "offset": 5342.36,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "number of algorithms for training uh",
      "offset": 5344.84,
      "duration": 4.359
    },
    {
      "lang": "en",
      "text": "vocabularies but one of them is the B",
      "offset": 5346.76,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "pair en coding algorithm that we've been",
      "offset": 5349.199,
      "duration": 4.44
    },
    {
      "lang": "en",
      "text": "looking at so it supports it now",
      "offset": 5350.44,
      "duration": 5.279
    },
    {
      "lang": "en",
      "text": "sentence piece is used both by llama and",
      "offset": 5353.639,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "mistal series and many other models as",
      "offset": 5355.719,
      "duration": 5.041
    },
    {
      "lang": "en",
      "text": "well it is on GitHub under Google",
      "offset": 5358.199,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "sentence piece",
      "offset": 5360.76,
      "duration": 3.64
    },
    {
      "lang": "en",
      "text": "and the big difference with sentence",
      "offset": 5362.76,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "piece and we're going to look at example",
      "offset": 5364.4,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "because this is kind of hard and subtle",
      "offset": 5366.199,
      "duration": 4.841
    },
    {
      "lang": "en",
      "text": "to explain is that they think different",
      "offset": 5367.92,
      "duration": 7.56
    },
    {
      "lang": "en",
      "text": "about the order of operations here so in",
      "offset": 5371.04,
      "duration": 7.52
    },
    {
      "lang": "en",
      "text": "the case of Tik token we first take our",
      "offset": 5375.48,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "code points in the string we encode them",
      "offset": 5378.56,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "using mutf to bytes and then we're",
      "offset": 5381,
      "duration": 3.96
    },
    {
      "lang": "en",
      "text": "merging bytes it's fairly",
      "offset": 5382.88,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "straightforward for sentence piece um it",
      "offset": 5384.96,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "works directly on the level of the code",
      "offset": 5388.88,
      "duration": 3.64
    },
    {
      "lang": "en",
      "text": "points themselves so so it looks at",
      "offset": 5390.4,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "whatever code points are available in",
      "offset": 5392.52,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "your training set and then it starts",
      "offset": 5393.92,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "merging those code points and um the bpe",
      "offset": 5395.88,
      "duration": 5.72
    },
    {
      "lang": "en",
      "text": "is running on the level of code",
      "offset": 5399.76,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "points and if you happen to run out of",
      "offset": 5401.6,
      "duration": 5.16
    },
    {
      "lang": "en",
      "text": "code points so there are maybe some rare",
      "offset": 5404.239,
      "duration": 3.801
    },
    {
      "lang": "en",
      "text": "uh code points that just don't come up",
      "offset": 5406.76,
      "duration": 2.959
    },
    {
      "lang": "en",
      "text": "too often and the Rarity is determined",
      "offset": 5408.04,
      "duration": 3.159
    },
    {
      "lang": "en",
      "text": "by this character coverage hyper",
      "offset": 5409.719,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "parameter then these uh code points will",
      "offset": 5411.199,
      "duration": 5.081
    },
    {
      "lang": "en",
      "text": "either get mapped to a special unknown",
      "offset": 5414.36,
      "duration": 5.16
    },
    {
      "lang": "en",
      "text": "token like ank or if you have the bite",
      "offset": 5416.28,
      "duration": 5.839
    },
    {
      "lang": "en",
      "text": "foldback option turned on then that will",
      "offset": 5419.52,
      "duration": 4.44
    },
    {
      "lang": "en",
      "text": "take those rare Cod points it will",
      "offset": 5422.119,
      "duration": 3.961
    },
    {
      "lang": "en",
      "text": "encode them using utf8 and then the",
      "offset": 5423.96,
      "duration": 3.8
    },
    {
      "lang": "en",
      "text": "individual bytes of that encoding will",
      "offset": 5426.08,
      "duration": 4.039
    },
    {
      "lang": "en",
      "text": "be translated into tokens and there are",
      "offset": 5427.76,
      "duration": 4.439
    },
    {
      "lang": "en",
      "text": "these special bite tokens that basically",
      "offset": 5430.119,
      "duration": 5.401
    },
    {
      "lang": "en",
      "text": "get added to the vocabulary so it uses",
      "offset": 5432.199,
      "duration": 6.04
    },
    {
      "lang": "en",
      "text": "BP on on the code points and then it",
      "offset": 5435.52,
      "duration": 6.28
    },
    {
      "lang": "en",
      "text": "falls back to bytes for rare Cod points",
      "offset": 5438.239,
      "duration": 5.841
    },
    {
      "lang": "en",
      "text": "um and so that's kind of like difference",
      "offset": 5441.8,
      "duration": 3.72
    },
    {
      "lang": "en",
      "text": "personally I find the Tik token we",
      "offset": 5444.08,
      "duration": 3.4
    },
    {
      "lang": "en",
      "text": "significantly cleaner uh but it's kind",
      "offset": 5445.52,
      "duration": 3.32
    },
    {
      "lang": "en",
      "text": "of like a subtle but pretty major",
      "offset": 5447.48,
      "duration": 2.84
    },
    {
      "lang": "en",
      "text": "difference between the way they approach",
      "offset": 5448.84,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "tokenization let's work with with a",
      "offset": 5450.32,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "concrete example because otherwise this",
      "offset": 5452.04,
      "duration": 4.679
    },
    {
      "lang": "en",
      "text": "is kind of hard to um to get your head",
      "offset": 5454,
      "duration": 5.119
    },
    {
      "lang": "en",
      "text": "around so let's work with a concrete",
      "offset": 5456.719,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "example this is how we can import",
      "offset": 5459.119,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "sentence piece and then here we're going",
      "offset": 5461.119,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "to take I think I took like the",
      "offset": 5463.6,
      "duration": 3.16
    },
    {
      "lang": "en",
      "text": "description of sentence piece and I just",
      "offset": 5465.199,
      "duration": 3.48
    },
    {
      "lang": "en",
      "text": "created like a little toy data set it",
      "offset": 5466.76,
      "duration": 3.64
    },
    {
      "lang": "en",
      "text": "really likes to have a file so I created",
      "offset": 5468.679,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "a toy. txt file with this",
      "offset": 5470.4,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "content now what's kind of a little bit",
      "offset": 5473.08,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "crazy about sentence piece is that",
      "offset": 5475.52,
      "duration": 3.159
    },
    {
      "lang": "en",
      "text": "there's a ton of options and",
      "offset": 5476.76,
      "duration": 4.04
    },
    {
      "lang": "en",
      "text": "configurations and the reason this is so",
      "offset": 5478.679,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "is because sentence piece has been",
      "offset": 5480.8,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "around I think for a while and it really",
      "offset": 5482.199,
      "duration": 3.561
    },
    {
      "lang": "en",
      "text": "tries to handle a large diversity of",
      "offset": 5483.84,
      "duration": 4.6
    },
    {
      "lang": "en",
      "text": "things and um because it's been around I",
      "offset": 5485.76,
      "duration": 4.76
    },
    {
      "lang": "en",
      "text": "think it has quite a bit of accumulated",
      "offset": 5488.44,
      "duration": 5.239
    },
    {
      "lang": "en",
      "text": "historical baggage uh as well and so in",
      "offset": 5490.52,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "particular there's like a ton of",
      "offset": 5493.679,
      "duration": 3.281
    },
    {
      "lang": "en",
      "text": "configuration arguments this is not even",
      "offset": 5495.56,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "all of it you can go to here to see all",
      "offset": 5496.96,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "the training",
      "offset": 5499.8,
      "duration": 4.6
    },
    {
      "lang": "en",
      "text": "options um and uh there's also quite",
      "offset": 5500.96,
      "duration": 4.759
    },
    {
      "lang": "en",
      "text": "useful documentation when you look at",
      "offset": 5504.4,
      "duration": 4.2
    },
    {
      "lang": "en",
      "text": "the raw Proto buff uh that is used to",
      "offset": 5505.719,
      "duration": 6.721
    },
    {
      "lang": "en",
      "text": "represent the trainer spec and so on um",
      "offset": 5508.6,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "many of these options are irrelevant to",
      "offset": 5512.44,
      "duration": 4.52
    },
    {
      "lang": "en",
      "text": "us so maybe to point out one example Das",
      "offset": 5514.52,
      "duration": 5.32
    },
    {
      "lang": "en",
      "text": "Das shrinking Factor uh this shrinking",
      "offset": 5516.96,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "factor is not used in the B pair en",
      "offset": 5519.84,
      "duration": 3.319
    },
    {
      "lang": "en",
      "text": "coding algorithm so this is just an",
      "offset": 5521.28,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "argument that is irrelevant to us um it",
      "offset": 5523.159,
      "duration": 5.881
    },
    {
      "lang": "en",
      "text": "applies to a different training",
      "offset": 5525.92,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "algorithm now what I tried to do here is",
      "offset": 5529.52,
      "duration": 4.36
    },
    {
      "lang": "en",
      "text": "I tried to set up sentence piece in a",
      "offset": 5531.92,
      "duration": 3.799
    },
    {
      "lang": "en",
      "text": "way that is very very similar as far as",
      "offset": 5533.88,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "I can tell to maybe identical hopefully",
      "offset": 5535.719,
      "duration": 6.361
    },
    {
      "lang": "en",
      "text": "to the way that llama 2 was strained so",
      "offset": 5538.88,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "the way they trained their own um their",
      "offset": 5542.08,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "own tokenizer and the way I did this was",
      "offset": 5545.04,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "basically you can take the tokenizer",
      "offset": 5547.119,
      "duration": 4.281
    },
    {
      "lang": "en",
      "text": "model file that meta released and you",
      "offset": 5548.719,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "can um open it using the Proto protuff",
      "offset": 5551.4,
      "duration": 6.96
    },
    {
      "lang": "en",
      "text": "uh sort of file that you can generate",
      "offset": 5555.199,
      "duration": 4.52
    },
    {
      "lang": "en",
      "text": "and then you can inspect all the options",
      "offset": 5558.36,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "and I tried to copy over all the options",
      "offset": 5559.719,
      "duration": 3.96
    },
    {
      "lang": "en",
      "text": "that looked relevant so here we set up",
      "offset": 5561.36,
      "duration": 5.24
    },
    {
      "lang": "en",
      "text": "the input it's raw text in this file",
      "offset": 5563.679,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "here's going to be the output so it's",
      "offset": 5566.6,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "going to be for talk 400. model and",
      "offset": 5568.08,
      "duration": 4.36
    },
    {
      "lang": "en",
      "text": "vocab",
      "offset": 5570.76,
      "duration": 2.64
    },
    {
      "lang": "en",
      "text": "we're saying that we're going to use the",
      "offset": 5572.44,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "BP algorithm and we want to Bap size of",
      "offset": 5573.4,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "400 then there's a ton of configurations",
      "offset": 5576.04,
      "duration": 4.84
    },
    {
      "lang": "en",
      "text": "here",
      "offset": 5578.6,
      "duration": 2.28
    },
    {
      "lang": "en",
      "text": "for um for basically pre-processing and",
      "offset": 5581.08,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "normalization rules as they're called",
      "offset": 5585.08,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "normalization used to be very prevalent",
      "offset": 5587.08,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "I would say before llms in natural",
      "offset": 5589.48,
      "duration": 3.32
    },
    {
      "lang": "en",
      "text": "language processing so in machine",
      "offset": 5591.159,
      "duration": 3.721
    },
    {
      "lang": "en",
      "text": "translation and uh text classification",
      "offset": 5592.8,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "and so on you want to normalize and",
      "offset": 5594.88,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "simplify the text and you want to turn",
      "offset": 5596.719,
      "duration": 2.801
    },
    {
      "lang": "en",
      "text": "it all lowercase and you want to remove",
      "offset": 5598,
      "duration": 4.199
    },
    {
      "lang": "en",
      "text": "all double whites space Etc",
      "offset": 5599.52,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "and in language models we prefer not to",
      "offset": 5602.199,
      "duration": 3.081
    },
    {
      "lang": "en",
      "text": "do any of it or at least that is my",
      "offset": 5603.76,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "preference as a deep learning person you",
      "offset": 5605.28,
      "duration": 3.56
    },
    {
      "lang": "en",
      "text": "want to not touch your data you want to",
      "offset": 5606.96,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "keep the raw data as much as possible um",
      "offset": 5608.84,
      "duration": 4.279
    },
    {
      "lang": "en",
      "text": "in a raw",
      "offset": 5611.679,
      "duration": 3.48
    },
    {
      "lang": "en",
      "text": "form so you're basically trying to turn",
      "offset": 5613.119,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "off a lot of this if you can the other",
      "offset": 5615.159,
      "duration": 4.361
    },
    {
      "lang": "en",
      "text": "thing that sentence piece does is that",
      "offset": 5618,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "it has this concept of sentences so",
      "offset": 5619.52,
      "duration": 5.96
    },
    {
      "lang": "en",
      "text": "sentence piece it's back it's kind of",
      "offset": 5623.04,
      "duration": 3.8
    },
    {
      "lang": "en",
      "text": "like was developed I think early in the",
      "offset": 5625.48,
      "duration": 4.679
    },
    {
      "lang": "en",
      "text": "days where there was um an idea that",
      "offset": 5626.84,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "they you're training a tokenizer on a",
      "offset": 5630.159,
      "duration": 4.04
    },
    {
      "lang": "en",
      "text": "bunch of independent sentences so it has",
      "offset": 5631.96,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "a lot of like how many sentences you're",
      "offset": 5634.199,
      "duration": 3.801
    },
    {
      "lang": "en",
      "text": "going to train on what is the maximum",
      "offset": 5636.36,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "sentence length",
      "offset": 5638,
      "duration": 5.719
    },
    {
      "lang": "en",
      "text": "um shuffling sentences and so for it",
      "offset": 5640.679,
      "duration": 4.121
    },
    {
      "lang": "en",
      "text": "sentences are kind of like the",
      "offset": 5643.719,
      "duration": 3.161
    },
    {
      "lang": "en",
      "text": "individual training examples but again",
      "offset": 5644.8,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "in the context of llms I find that this",
      "offset": 5646.88,
      "duration": 3.56
    },
    {
      "lang": "en",
      "text": "is like a very spous and weird",
      "offset": 5648.719,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "distinction like sentences are just like",
      "offset": 5650.44,
      "duration": 5.16
    },
    {
      "lang": "en",
      "text": "don't touch the raw data sentences",
      "offset": 5653.92,
      "duration": 4.759
    },
    {
      "lang": "en",
      "text": "happen to exist but in raw data sets",
      "offset": 5655.6,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "there are a lot of like inet like what",
      "offset": 5658.679,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "exactly is a sentence what isn't a",
      "offset": 5660.6,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "sentence um and so I think like it's",
      "offset": 5662.44,
      "duration": 4.04
    },
    {
      "lang": "en",
      "text": "really hard to Define what an actual",
      "offset": 5665,
      "duration": 3.639
    },
    {
      "lang": "en",
      "text": "sentence is if you really like dig into",
      "offset": 5666.48,
      "duration": 4.44
    },
    {
      "lang": "en",
      "text": "it and there could be different concepts",
      "offset": 5668.639,
      "duration": 3.48
    },
    {
      "lang": "en",
      "text": "of it in different languages or",
      "offset": 5670.92,
      "duration": 2.799
    },
    {
      "lang": "en",
      "text": "something like that so why even",
      "offset": 5672.119,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "introduce the concept it it doesn't",
      "offset": 5673.719,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "honestly make sense to me I would just",
      "offset": 5675.56,
      "duration": 3.639
    },
    {
      "lang": "en",
      "text": "prefer to treat a file as a giant uh",
      "offset": 5676.92,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "stream of",
      "offset": 5679.199,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "bytes it has a lot of treatment around",
      "offset": 5680.36,
      "duration": 4.759
    },
    {
      "lang": "en",
      "text": "rare word characters and when I say word",
      "offset": 5682.8,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "I mean code points we're going to come",
      "offset": 5685.119,
      "duration": 3.56
    },
    {
      "lang": "en",
      "text": "back to this in a second and it has a",
      "offset": 5686.48,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "lot of other rules for um basically",
      "offset": 5688.679,
      "duration": 5.801
    },
    {
      "lang": "en",
      "text": "splitting digits splitting white space",
      "offset": 5691.679,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "and numbers and how you deal with that",
      "offset": 5694.48,
      "duration": 3.719
    },
    {
      "lang": "en",
      "text": "so these are some kind of like merge",
      "offset": 5696.56,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "rules so I think this is a little bit",
      "offset": 5698.199,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "equivalent to tick token using the",
      "offset": 5700.08,
      "duration": 4.44
    },
    {
      "lang": "en",
      "text": "regular expression to split up",
      "offset": 5702.92,
      "duration": 4.12
    },
    {
      "lang": "en",
      "text": "categories there's like kind of",
      "offset": 5704.52,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "equivalence of it if you squint T it in",
      "offset": 5707.04,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "sentence piece where you can also for",
      "offset": 5709.239,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "example split up split up the digits uh",
      "offset": 5710.639,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "and uh so",
      "offset": 5714.199,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "on there's a few more things here that",
      "offset": 5715.84,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "I'll come back to in a bit and then",
      "offset": 5718.199,
      "duration": 2.281
    },
    {
      "lang": "en",
      "text": "there are some special tokens that you",
      "offset": 5719.36,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "can indicate and it hardcodes the UN",
      "offset": 5720.48,
      "duration": 5.08
    },
    {
      "lang": "en",
      "text": "token the beginning of sentence end of",
      "offset": 5723.36,
      "duration": 5.96
    },
    {
      "lang": "en",
      "text": "sentence and a pad token um and the UN",
      "offset": 5725.56,
      "duration": 6.96
    },
    {
      "lang": "en",
      "text": "token must exist for my understanding",
      "offset": 5729.32,
      "duration": 5.399
    },
    {
      "lang": "en",
      "text": "and then some some things so we can",
      "offset": 5732.52,
      "duration": 4.76
    },
    {
      "lang": "en",
      "text": "train and when when I press train it's",
      "offset": 5734.719,
      "duration": 5.4
    },
    {
      "lang": "en",
      "text": "going to create this file talk 400.",
      "offset": 5737.28,
      "duration": 5.879
    },
    {
      "lang": "en",
      "text": "model and talk 400. wab I can then load",
      "offset": 5740.119,
      "duration": 5.441
    },
    {
      "lang": "en",
      "text": "the model file and I can inspect the",
      "offset": 5743.159,
      "duration": 5.401
    },
    {
      "lang": "en",
      "text": "vocabulary off it and so we trained",
      "offset": 5745.56,
      "duration": 7.76
    },
    {
      "lang": "en",
      "text": "vocab size 400 on this text here and",
      "offset": 5748.56,
      "duration": 6.44
    },
    {
      "lang": "en",
      "text": "these are the individual pieces the",
      "offset": 5753.32,
      "duration": 3.56
    },
    {
      "lang": "en",
      "text": "individual tokens that sentence piece",
      "offset": 5755,
      "duration": 3.8
    },
    {
      "lang": "en",
      "text": "will create so in the beginning we see",
      "offset": 5756.88,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "that we have the an token uh with the ID",
      "offset": 5758.8,
      "duration": 5.24
    },
    {
      "lang": "en",
      "text": "zero then we have the beginning of",
      "offset": 5762.08,
      "duration": 5.72
    },
    {
      "lang": "en",
      "text": "sequence end of sequence one and two and",
      "offset": 5764.04,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "then we said that the pad ID is negative",
      "offset": 5767.8,
      "duration": 4.28
    },
    {
      "lang": "en",
      "text": "1 so we chose not to use it so there's",
      "offset": 5769.32,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "no pad ID",
      "offset": 5772.08,
      "duration": 4.76
    },
    {
      "lang": "en",
      "text": "here then these are individual bite",
      "offset": 5773.48,
      "duration": 6.679
    },
    {
      "lang": "en",
      "text": "tokens so here we saw that bite fallback",
      "offset": 5776.84,
      "duration": 6.72
    },
    {
      "lang": "en",
      "text": "in llama was turned on so it's true so",
      "offset": 5780.159,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "what follows are going to be the 256",
      "offset": 5783.56,
      "duration": 3.639
    },
    {
      "lang": "en",
      "text": "bite",
      "offset": 5786.159,
      "duration": 4.681
    },
    {
      "lang": "en",
      "text": "tokens and these are their",
      "offset": 5787.199,
      "duration": 3.641
    },
    {
      "lang": "en",
      "text": "IDs and then at the bottom after the",
      "offset": 5791.719,
      "duration": 5.96
    },
    {
      "lang": "en",
      "text": "bite tokens come the",
      "offset": 5795.04,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "merges and these are the parent nodes in",
      "offset": 5797.679,
      "duration": 4.52
    },
    {
      "lang": "en",
      "text": "the merges so we're not seeing the",
      "offset": 5800.56,
      "duration": 3.159
    },
    {
      "lang": "en",
      "text": "children we're just seeing the parents",
      "offset": 5802.199,
      "duration": 2.401
    },
    {
      "lang": "en",
      "text": "and their",
      "offset": 5803.719,
      "duration": 3.321
    },
    {
      "lang": "en",
      "text": "ID and then after the",
      "offset": 5804.6,
      "duration": 6.119
    },
    {
      "lang": "en",
      "text": "merges comes eventually the individual",
      "offset": 5807.04,
      "duration": 6.52
    },
    {
      "lang": "en",
      "text": "tokens and their IDs and so these are",
      "offset": 5810.719,
      "duration": 4.601
    },
    {
      "lang": "en",
      "text": "the individual tokens so these are the",
      "offset": 5813.56,
      "duration": 4.679
    },
    {
      "lang": "en",
      "text": "individual code Point tokens if you will",
      "offset": 5815.32,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "and they come at the end so that is the",
      "offset": 5818.239,
      "duration": 3.521
    },
    {
      "lang": "en",
      "text": "ordering with which sentence piece sort",
      "offset": 5820.28,
      "duration": 3.64
    },
    {
      "lang": "en",
      "text": "of like represents its vocabularies it",
      "offset": 5821.76,
      "duration": 4.359
    },
    {
      "lang": "en",
      "text": "starts with special tokens then the bike",
      "offset": 5823.92,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "tokens then the merge tokens and then",
      "offset": 5826.119,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "the individual codo tokens and all these",
      "offset": 5828.159,
      "duration": 5.881
    },
    {
      "lang": "en",
      "text": "raw codepoint to tokens are the ones",
      "offset": 5831.639,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "that it encountered in the training",
      "offset": 5834.04,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "set so those individual code points are",
      "offset": 5836.119,
      "duration": 6.04
    },
    {
      "lang": "en",
      "text": "all the the entire set of code points",
      "offset": 5839.8,
      "duration": 4.6
    },
    {
      "lang": "en",
      "text": "that occurred",
      "offset": 5842.159,
      "duration": 5.321
    },
    {
      "lang": "en",
      "text": "here so those all get put in there and",
      "offset": 5844.4,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "then those that are extremely rare as",
      "offset": 5847.48,
      "duration": 3.639
    },
    {
      "lang": "en",
      "text": "determined by character coverage so if a",
      "offset": 5849.28,
      "duration": 3.24
    },
    {
      "lang": "en",
      "text": "code Point occurred only a single time",
      "offset": 5851.119,
      "duration": 4.04
    },
    {
      "lang": "en",
      "text": "out of like a million um sentences or",
      "offset": 5852.52,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "something like that then it would be",
      "offset": 5855.159,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "ignored and it would not be added to our",
      "offset": 5857.08,
      "duration": 3.96
    },
    {
      "lang": "en",
      "text": "uh",
      "offset": 5860.199,
      "duration": 3.161
    },
    {
      "lang": "en",
      "text": "vocabulary once we have a vocabulary we",
      "offset": 5861.04,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "can encode into IDs and we can um sort",
      "offset": 5863.36,
      "duration": 4.04
    },
    {
      "lang": "en",
      "text": "of get a",
      "offset": 5866.48,
      "duration": 4.199
    },
    {
      "lang": "en",
      "text": "list and then here I am also decoding",
      "offset": 5867.4,
      "duration": 6.92
    },
    {
      "lang": "en",
      "text": "the indiv idual tokens back into little",
      "offset": 5870.679,
      "duration": 6.281
    },
    {
      "lang": "en",
      "text": "pieces as they call it so let's take a",
      "offset": 5874.32,
      "duration": 6.76
    },
    {
      "lang": "en",
      "text": "look at what happened here hello space",
      "offset": 5876.96,
      "duration": 7.719
    },
    {
      "lang": "en",
      "text": "on so these are the token IDs we got",
      "offset": 5881.08,
      "duration": 6.4
    },
    {
      "lang": "en",
      "text": "back and when we look here uh a few",
      "offset": 5884.679,
      "duration": 6.841
    },
    {
      "lang": "en",
      "text": "things sort of uh jump to mind number",
      "offset": 5887.48,
      "duration": 6.679
    },
    {
      "lang": "en",
      "text": "one take a look at these characters the",
      "offset": 5891.52,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "Korean characters of course were not",
      "offset": 5894.159,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "part of the training set so sentence",
      "offset": 5895.52,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "piece is encountering code points that",
      "offset": 5898,
      "duration": 4.199
    },
    {
      "lang": "en",
      "text": "it has not seen during training time and",
      "offset": 5899.599,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "those code points do not have a token",
      "offset": 5902.199,
      "duration": 4.201
    },
    {
      "lang": "en",
      "text": "associated with them so suddenly these",
      "offset": 5904.56,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "are un tokens unknown tokens but because",
      "offset": 5906.4,
      "duration": 7.44
    },
    {
      "lang": "en",
      "text": "bite fall back as true instead sentence",
      "offset": 5910.56,
      "duration": 5.88
    },
    {
      "lang": "en",
      "text": "piece falls back to bytes and so it",
      "offset": 5913.84,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "takes this it encodes it with utf8 and",
      "offset": 5916.44,
      "duration": 6.84
    },
    {
      "lang": "en",
      "text": "then it uses these tokens to represent",
      "offset": 5919.84,
      "duration": 5.96
    },
    {
      "lang": "en",
      "text": "uh those bytes and that's what we are",
      "offset": 5923.28,
      "duration": 6.439
    },
    {
      "lang": "en",
      "text": "getting sort of here this is the utf8 uh",
      "offset": 5925.8,
      "duration": 7.08
    },
    {
      "lang": "en",
      "text": "encoding and in this shifted by three uh",
      "offset": 5929.719,
      "duration": 6.52
    },
    {
      "lang": "en",
      "text": "because of these um special tokens here",
      "offset": 5932.88,
      "duration": 5.96
    },
    {
      "lang": "en",
      "text": "that have IDs earlier on so that's what",
      "offset": 5936.239,
      "duration": 6.681
    },
    {
      "lang": "en",
      "text": "happened here now one more thing that um",
      "offset": 5938.84,
      "duration": 6.68
    },
    {
      "lang": "en",
      "text": "well first before I go on with respect",
      "offset": 5942.92,
      "duration": 5.319
    },
    {
      "lang": "en",
      "text": "to the bitef back let me remove bite",
      "offset": 5945.52,
      "duration": 5.32
    },
    {
      "lang": "en",
      "text": "foldback if this is false what's going",
      "offset": 5948.239,
      "duration": 4.281
    },
    {
      "lang": "en",
      "text": "to happen let's",
      "offset": 5950.84,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "retrain so the first thing that happened",
      "offset": 5952.52,
      "duration": 4.76
    },
    {
      "lang": "en",
      "text": "is all the bite tokens disappeared right",
      "offset": 5954.44,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "and now we just have the merges and we",
      "offset": 5957.28,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "have a lot more merges now because we",
      "offset": 5959,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "have a lot more space because we're not",
      "offset": 5960.48,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "taking up space in the wab size uh with",
      "offset": 5961.8,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "all the",
      "offset": 5965.04,
      "duration": 4.04
    },
    {
      "lang": "en",
      "text": "bytes and now if we encode",
      "offset": 5965.96,
      "duration": 7.279
    },
    {
      "lang": "en",
      "text": "this we get a zero so this entire string",
      "offset": 5969.08,
      "duration": 6.039
    },
    {
      "lang": "en",
      "text": "here suddenly there's no bitef back so",
      "offset": 5973.239,
      "duration": 6.161
    },
    {
      "lang": "en",
      "text": "this is unknown and unknown is an and so",
      "offset": 5975.119,
      "duration": 6.921
    },
    {
      "lang": "en",
      "text": "this is zero because the an token is",
      "offset": 5979.4,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "token zero and you have to keep in mind",
      "offset": 5982.04,
      "duration": 4.84
    },
    {
      "lang": "en",
      "text": "that this would feed into your uh",
      "offset": 5984.92,
      "duration": 3.48
    },
    {
      "lang": "en",
      "text": "language model so what is a language",
      "offset": 5986.88,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "model supposed to do when all kinds of",
      "offset": 5988.4,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "different things that are unrecognized",
      "offset": 5989.92,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "because they're rare just end up mapping",
      "offset": 5992.159,
      "duration": 3.96
    },
    {
      "lang": "en",
      "text": "into Unk it's not exactly the property",
      "offset": 5994,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "that you want so that's why I think",
      "offset": 5996.119,
      "duration": 5.921
    },
    {
      "lang": "en",
      "text": "llama correctly uh used by fallback true",
      "offset": 5997.76,
      "duration": 5.959
    },
    {
      "lang": "en",
      "text": "uh because we definitely want to feed",
      "offset": 6002.04,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "these um unknown or rare code points",
      "offset": 6003.719,
      "duration": 4.841
    },
    {
      "lang": "en",
      "text": "into the model and some uh some manner",
      "offset": 6006.04,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "the next thing I want to show you is the",
      "offset": 6008.56,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "following notice here when we are",
      "offset": 6010.679,
      "duration": 4.04
    },
    {
      "lang": "en",
      "text": "decoding all the individual tokens you",
      "offset": 6012.48,
      "duration": 5.56
    },
    {
      "lang": "en",
      "text": "see how spaces uh space here ends up",
      "offset": 6014.719,
      "duration": 6.52
    },
    {
      "lang": "en",
      "text": "being this um bold underline I'm not",
      "offset": 6018.04,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "100% sure by the way why sentence piece",
      "offset": 6021.239,
      "duration": 4.121
    },
    {
      "lang": "en",
      "text": "switches whites space into these bold",
      "offset": 6023.08,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "underscore characters maybe it's for",
      "offset": 6025.36,
      "duration": 4.52
    },
    {
      "lang": "en",
      "text": "visualization I'm not 100% sure why that",
      "offset": 6027.639,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "happens uh but notice this why do we",
      "offset": 6029.88,
      "duration": 7.56
    },
    {
      "lang": "en",
      "text": "have an extra space in the front of",
      "offset": 6032.44,
      "duration": 8.04
    },
    {
      "lang": "en",
      "text": "hello um what where is this coming from",
      "offset": 6037.44,
      "duration": 5.719
    },
    {
      "lang": "en",
      "text": "well it's coming from this option",
      "offset": 6040.48,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "here",
      "offset": 6043.159,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "um add dummy prefix is true and when you",
      "offset": 6045.04,
      "duration": 4.52
    },
    {
      "lang": "en",
      "text": "go to the",
      "offset": 6048.36,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "documentation add D whites space at the",
      "offset": 6049.56,
      "duration": 3.8
    },
    {
      "lang": "en",
      "text": "beginning of text in order to treat",
      "offset": 6051.88,
      "duration": 4.04
    },
    {
      "lang": "en",
      "text": "World in world and hello world in the",
      "offset": 6053.36,
      "duration": 4.6
    },
    {
      "lang": "en",
      "text": "exact same way so what this is trying to",
      "offset": 6055.92,
      "duration": 3.319
    },
    {
      "lang": "en",
      "text": "do is the",
      "offset": 6057.96,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "following if we go back to our tick",
      "offset": 6059.239,
      "duration": 7.081
    },
    {
      "lang": "en",
      "text": "tokenizer world as uh token by itself",
      "offset": 6062.04,
      "duration": 8.199
    },
    {
      "lang": "en",
      "text": "has a different ID than space world so",
      "offset": 6066.32,
      "duration": 8.279
    },
    {
      "lang": "en",
      "text": "we have this is 1917 but this is 14 Etc",
      "offset": 6070.239,
      "duration": 5.761
    },
    {
      "lang": "en",
      "text": "so these are two different tokens for",
      "offset": 6074.599,
      "duration": 2.801
    },
    {
      "lang": "en",
      "text": "the language model and the language",
      "offset": 6076,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "model has to learn from data that they",
      "offset": 6077.4,
      "duration": 2.92
    },
    {
      "lang": "en",
      "text": "are actually kind of like a very similar",
      "offset": 6078.88,
      "duration": 4.12
    },
    {
      "lang": "en",
      "text": "concept so to the language model in the",
      "offset": 6080.32,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "Tik token World um basically words in",
      "offset": 6083,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "the beginning of sentences and words in",
      "offset": 6086,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "the middle of sentences actually look",
      "offset": 6087.639,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "completely different um and it has to",
      "offset": 6089.04,
      "duration": 5.4
    },
    {
      "lang": "en",
      "text": "learned that they are roughly the same",
      "offset": 6092.04,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "so this add dami prefix is trying to",
      "offset": 6094.44,
      "duration": 4.52
    },
    {
      "lang": "en",
      "text": "fight that a little bit and the way that",
      "offset": 6096.92,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "works is that it basically",
      "offset": 6098.96,
      "duration": 7.8
    },
    {
      "lang": "en",
      "text": "uh adds a dummy prefix so for as a as a",
      "offset": 6101.719,
      "duration": 7.361
    },
    {
      "lang": "en",
      "text": "part of pre-processing it will take the",
      "offset": 6106.76,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "string and it will add a space it will",
      "offset": 6109.08,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "do this and that's done in an effort to",
      "offset": 6111.32,
      "duration": 6.2
    },
    {
      "lang": "en",
      "text": "make this world and that world the same",
      "offset": 6114.92,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "they will both be space world so that's",
      "offset": 6117.52,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "one other kind of pre-processing option",
      "offset": 6120.28,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "that is turned on and llama 2 also uh",
      "offset": 6122.159,
      "duration": 5.241
    },
    {
      "lang": "en",
      "text": "uses this option and that's I think",
      "offset": 6125.28,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "everything that I want to say for my",
      "offset": 6127.4,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "preview of sentence piece and how it is",
      "offset": 6128.639,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "different um maybe here what I've done",
      "offset": 6130.44,
      "duration": 6.279
    },
    {
      "lang": "en",
      "text": "is I just uh put in the Raw protocol",
      "offset": 6133.119,
      "duration": 6.721
    },
    {
      "lang": "en",
      "text": "buffer representation basically of the",
      "offset": 6136.719,
      "duration": 6.161
    },
    {
      "lang": "en",
      "text": "tokenizer the too trained so feel free",
      "offset": 6139.84,
      "duration": 4.92
    },
    {
      "lang": "en",
      "text": "to sort of Step through this and if you",
      "offset": 6142.88,
      "duration": 4.12
    },
    {
      "lang": "en",
      "text": "would like uh your tokenization to look",
      "offset": 6144.76,
      "duration": 5.56
    },
    {
      "lang": "en",
      "text": "identical to that of the meta uh llama 2",
      "offset": 6147,
      "duration": 4.679
    },
    {
      "lang": "en",
      "text": "then you would be copy pasting these",
      "offset": 6150.32,
      "duration": 4.44
    },
    {
      "lang": "en",
      "text": "settings as I tried to do up above and",
      "offset": 6151.679,
      "duration": 5.281
    },
    {
      "lang": "en",
      "text": "uh yeah that's I think that's it for",
      "offset": 6154.76,
      "duration": 4.12
    },
    {
      "lang": "en",
      "text": "this section I think my summary for",
      "offset": 6156.96,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "sentence piece from all of this is",
      "offset": 6158.88,
      "duration": 3.56
    },
    {
      "lang": "en",
      "text": "number one I think that there's a lot of",
      "offset": 6160.8,
      "duration": 3.48
    },
    {
      "lang": "en",
      "text": "historical baggage in sentence piece a",
      "offset": 6162.44,
      "duration": 3.239
    },
    {
      "lang": "en",
      "text": "lot of Concepts that I think are",
      "offset": 6164.28,
      "duration": 2.959
    },
    {
      "lang": "en",
      "text": "slightly confusing and I think",
      "offset": 6165.679,
      "duration": 3.721
    },
    {
      "lang": "en",
      "text": "potentially um contain foot guns like",
      "offset": 6167.239,
      "duration": 3.561
    },
    {
      "lang": "en",
      "text": "this concept of a sentence and it's",
      "offset": 6169.4,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "maximum length and stuff like that um",
      "offset": 6170.8,
      "duration": 5.08
    },
    {
      "lang": "en",
      "text": "otherwise it is fairly commonly used in",
      "offset": 6173.719,
      "duration": 5.161
    },
    {
      "lang": "en",
      "text": "the industry um because it is efficient",
      "offset": 6175.88,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "and can do both training and inference",
      "offset": 6178.88,
      "duration": 3.88
    },
    {
      "lang": "en",
      "text": "uh it has a few quirks like for example",
      "offset": 6181,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "un token must exist and the way the bite",
      "offset": 6182.76,
      "duration": 3.8
    },
    {
      "lang": "en",
      "text": "fallbacks are done and so on I don't",
      "offset": 6185.08,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "find particularly elegant and",
      "offset": 6186.56,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "unfortunately I have to say it's not",
      "offset": 6188.36,
      "duration": 3.08
    },
    {
      "lang": "en",
      "text": "very well documented so it took me a lot",
      "offset": 6189.56,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "of time working with this myself um and",
      "offset": 6191.44,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "just visualizing things and trying to",
      "offset": 6194.76,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "really understand what is happening here",
      "offset": 6196.159,
      "duration": 3.121
    },
    {
      "lang": "en",
      "text": "because uh the documentation",
      "offset": 6197.8,
      "duration": 3.64
    },
    {
      "lang": "en",
      "text": "unfortunately is in my opion not not",
      "offset": 6199.28,
      "duration": 5.399
    },
    {
      "lang": "en",
      "text": "super amazing but it is a very nice repo",
      "offset": 6201.44,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "that is available to you if you'd like",
      "offset": 6204.679,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "to train your own tokenizer right now",
      "offset": 6206.159,
      "duration": 3.48
    },
    {
      "lang": "en",
      "text": "okay let me now switch gears again as",
      "offset": 6208.199,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "we're starting to slowly wrap up here I",
      "offset": 6209.639,
      "duration": 3.721
    },
    {
      "lang": "en",
      "text": "want to revisit this issue in a bit more",
      "offset": 6211.719,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "detail of how we should set the vocap",
      "offset": 6213.36,
      "duration": 2.839
    },
    {
      "lang": "en",
      "text": "size and what are some of the",
      "offset": 6215.32,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "considerations around it so for this I'd",
      "offset": 6216.199,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "like to go back to the model",
      "offset": 6219.639,
      "duration": 2.52
    },
    {
      "lang": "en",
      "text": "architecture that we developed in the",
      "offset": 6220.84,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "last video when we built the GPT from",
      "offset": 6222.159,
      "duration": 5.241
    },
    {
      "lang": "en",
      "text": "scratch so this here was uh the file",
      "offset": 6224.679,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "that we built in the previous video and",
      "offset": 6227.4,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "we defined the Transformer model and and",
      "offset": 6229.08,
      "duration": 3.8
    },
    {
      "lang": "en",
      "text": "let's specifically look at Bap size and",
      "offset": 6231.32,
      "duration": 3.879
    },
    {
      "lang": "en",
      "text": "where it appears in this file so here we",
      "offset": 6232.88,
      "duration": 5.279
    },
    {
      "lang": "en",
      "text": "Define the voap size uh at this time it",
      "offset": 6235.199,
      "duration": 4.761
    },
    {
      "lang": "en",
      "text": "was 65 or something like that extremely",
      "offset": 6238.159,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "small number so this will grow much",
      "offset": 6239.96,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "larger you'll see that Bap size doesn't",
      "offset": 6242.08,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "come up too much in most of these layers",
      "offset": 6244.28,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "the only place that it comes up to is in",
      "offset": 6246.159,
      "duration": 5.321
    },
    {
      "lang": "en",
      "text": "exactly these two places here so when we",
      "offset": 6248.52,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "Define the language model there's the",
      "offset": 6251.48,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "token embedding table which is this",
      "offset": 6253.56,
      "duration": 4.52
    },
    {
      "lang": "en",
      "text": "two-dimensional array where the vocap",
      "offset": 6255.8,
      "duration": 5.399
    },
    {
      "lang": "en",
      "text": "size is basically the number of rows and",
      "offset": 6258.08,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "uh each vocabulary element each token",
      "offset": 6261.199,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "has a vector that we're going to train",
      "offset": 6263.92,
      "duration": 4.04
    },
    {
      "lang": "en",
      "text": "using back propagation that Vector is of",
      "offset": 6265.92,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "size and embed which is number of",
      "offset": 6267.96,
      "duration": 3.639
    },
    {
      "lang": "en",
      "text": "channels in the Transformer and",
      "offset": 6269.44,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "basically as voap size increases this",
      "offset": 6271.599,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "embedding table as I mentioned earlier",
      "offset": 6273.679,
      "duration": 3.321
    },
    {
      "lang": "en",
      "text": "is going to also grow we're going to be",
      "offset": 6275.679,
      "duration": 4.04
    },
    {
      "lang": "en",
      "text": "adding rows in addition to that at the",
      "offset": 6277,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "end of the Transformer there's this LM",
      "offset": 6279.719,
      "duration": 4.52
    },
    {
      "lang": "en",
      "text": "head layer which is a linear layer and",
      "offset": 6281.88,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "you'll notice that that layer is used at",
      "offset": 6284.239,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "the very end to produce the logits uh",
      "offset": 6286.28,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "which become the probabilities for the",
      "offset": 6288.639,
      "duration": 3.121
    },
    {
      "lang": "en",
      "text": "next token in sequence and so",
      "offset": 6289.96,
      "duration": 3.96
    },
    {
      "lang": "en",
      "text": "intuitively we're trying to produce a",
      "offset": 6291.76,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "probability for every single token that",
      "offset": 6293.92,
      "duration": 4.92
    },
    {
      "lang": "en",
      "text": "might come next at every point in time",
      "offset": 6296.239,
      "duration": 4.841
    },
    {
      "lang": "en",
      "text": "of that Transformer and if we have more",
      "offset": 6298.84,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "and more tokens we need to produce more",
      "offset": 6301.08,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "and more probabilities so every single",
      "offset": 6302.679,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "token is going to introduce an",
      "offset": 6304.92,
      "duration": 3.239
    },
    {
      "lang": "en",
      "text": "additional dot product that we have to",
      "offset": 6306.199,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "do here in this linear layer for this",
      "offset": 6308.159,
      "duration": 3.281
    },
    {
      "lang": "en",
      "text": "final layer in a",
      "offset": 6310.199,
      "duration": 4.361
    },
    {
      "lang": "en",
      "text": "Transformer so why can't vocap size be",
      "offset": 6311.44,
      "duration": 5.08
    },
    {
      "lang": "en",
      "text": "infinite why can't we grow to Infinity",
      "offset": 6314.56,
      "duration": 3.639
    },
    {
      "lang": "en",
      "text": "well number one your token embedding",
      "offset": 6316.52,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "table is going to grow uh your linear",
      "offset": 6318.199,
      "duration": 5.4
    },
    {
      "lang": "en",
      "text": "layer is going to grow so we're going to",
      "offset": 6321.56,
      "duration": 3.559
    },
    {
      "lang": "en",
      "text": "be doing a lot more computation here",
      "offset": 6323.599,
      "duration": 2.961
    },
    {
      "lang": "en",
      "text": "because this LM head layer will become",
      "offset": 6325.119,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "more computational expensive number two",
      "offset": 6326.56,
      "duration": 4.28
    },
    {
      "lang": "en",
      "text": "because we have more parameters we could",
      "offset": 6329.119,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "be worried that we are going to be under",
      "offset": 6330.84,
      "duration": 4.359
    },
    {
      "lang": "en",
      "text": "trining some of these",
      "offset": 6333.44,
      "duration": 3.96
    },
    {
      "lang": "en",
      "text": "parameters so intuitively if you have a",
      "offset": 6335.199,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "very large vocabulary size say we have a",
      "offset": 6337.4,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "million uh tokens then every one of",
      "offset": 6338.96,
      "duration": 3.719
    },
    {
      "lang": "en",
      "text": "these tokens is going to come up more",
      "offset": 6341.32,
      "duration": 3.72
    },
    {
      "lang": "en",
      "text": "and more rarely in the training data",
      "offset": 6342.679,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "because there's a lot more other tokens",
      "offset": 6345.04,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "all over the place and so we're going to",
      "offset": 6346.52,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "be seeing fewer and fewer examples uh",
      "offset": 6348.56,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "for each individual token and you might",
      "offset": 6351,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "be worried that basically the vectors",
      "offset": 6353.28,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "associated with every token will be",
      "offset": 6355,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "undertrained as a result because they",
      "offset": 6356.28,
      "duration": 3.64
    },
    {
      "lang": "en",
      "text": "just don't come up too often and they",
      "offset": 6358.28,
      "duration": 2.68
    },
    {
      "lang": "en",
      "text": "don't participate in the forward",
      "offset": 6359.92,
      "duration": 3.279
    },
    {
      "lang": "en",
      "text": "backward pass in addition to that as",
      "offset": 6360.96,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "your vocab size grows you're going to",
      "offset": 6363.199,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "start shrinking your sequences a lot",
      "offset": 6364.88,
      "duration": 4.44
    },
    {
      "lang": "en",
      "text": "right and that's really nice because",
      "offset": 6367.04,
      "duration": 3.079
    },
    {
      "lang": "en",
      "text": "that means that we're going to be",
      "offset": 6369.32,
      "duration": 2.68
    },
    {
      "lang": "en",
      "text": "attending to more and more text so",
      "offset": 6370.119,
      "duration": 3.48
    },
    {
      "lang": "en",
      "text": "that's nice but also you might be",
      "offset": 6372,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "worrying that two large of chunks are",
      "offset": 6373.599,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "being squished into single tokens and so",
      "offset": 6375.92,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "the model just doesn't have as much of",
      "offset": 6378.56,
      "duration": 6.52
    },
    {
      "lang": "en",
      "text": "time to think per sort of um some number",
      "offset": 6380.719,
      "duration": 5.96
    },
    {
      "lang": "en",
      "text": "of characters in the text or you can",
      "offset": 6385.08,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "think about it that way right so",
      "offset": 6386.679,
      "duration": 2.801
    },
    {
      "lang": "en",
      "text": "basically we're squishing too much",
      "offset": 6388.08,
      "duration": 3.559
    },
    {
      "lang": "en",
      "text": "information into a single token and then",
      "offset": 6389.48,
      "duration": 3.56
    },
    {
      "lang": "en",
      "text": "the forward pass of the Transformer is",
      "offset": 6391.639,
      "duration": 2.761
    },
    {
      "lang": "en",
      "text": "not enough to actually process that",
      "offset": 6393.04,
      "duration": 3.4
    },
    {
      "lang": "en",
      "text": "information appropriately and so these",
      "offset": 6394.4,
      "duration": 3.08
    },
    {
      "lang": "en",
      "text": "are some of the considerations you're",
      "offset": 6396.44,
      "duration": 2.199
    },
    {
      "lang": "en",
      "text": "thinking about when you're designing the",
      "offset": 6397.48,
      "duration": 3.159
    },
    {
      "lang": "en",
      "text": "vocab size as I mentioned this is mostly",
      "offset": 6398.639,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "an empirical hyperparameter and it seems",
      "offset": 6400.639,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "like in state-of-the-art architectures",
      "offset": 6402.88,
      "duration": 3.88
    },
    {
      "lang": "en",
      "text": "today this is usually in the high 10,000",
      "offset": 6404.239,
      "duration": 5.121
    },
    {
      "lang": "en",
      "text": "or somewhere around 100,000 today and",
      "offset": 6406.76,
      "duration": 4.12
    },
    {
      "lang": "en",
      "text": "the next consideration I want to briefly",
      "offset": 6409.36,
      "duration": 3.64
    },
    {
      "lang": "en",
      "text": "talk about is what if we want to take a",
      "offset": 6410.88,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "pre-trained model and we want to extend",
      "offset": 6413,
      "duration": 4.36
    },
    {
      "lang": "en",
      "text": "the vocap size and this is done fairly",
      "offset": 6415.199,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "commonly actually so for example when",
      "offset": 6417.36,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "you're doing fine-tuning for cha GPT um",
      "offset": 6418.88,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "a lot more new special tokens get",
      "offset": 6422.159,
      "duration": 3.641
    },
    {
      "lang": "en",
      "text": "introduced on top of the base model to",
      "offset": 6423.76,
      "duration": 4.28
    },
    {
      "lang": "en",
      "text": "maintain the metadata and all the",
      "offset": 6425.8,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "structure of conversation objects",
      "offset": 6428.04,
      "duration": 3.88
    },
    {
      "lang": "en",
      "text": "between a user and an assistant so that",
      "offset": 6429.88,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "takes a lot of special tokens you might",
      "offset": 6431.92,
      "duration": 3.96
    },
    {
      "lang": "en",
      "text": "also try to throw in more special tokens",
      "offset": 6434.04,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "for example for using the browser or any",
      "offset": 6435.88,
      "duration": 4.759
    },
    {
      "lang": "en",
      "text": "other tool and so it's very tempting to",
      "offset": 6437.8,
      "duration": 4.359
    },
    {
      "lang": "en",
      "text": "add a lot of tokens for all kinds of",
      "offset": 6440.639,
      "duration": 3.881
    },
    {
      "lang": "en",
      "text": "special functionality so if you want to",
      "offset": 6442.159,
      "duration": 3.641
    },
    {
      "lang": "en",
      "text": "be adding a token that's totally",
      "offset": 6444.52,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "possible Right all we have to do is we",
      "offset": 6445.8,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "have to resize this embedding so we have",
      "offset": 6447.719,
      "duration": 4.761
    },
    {
      "lang": "en",
      "text": "to add rows we would initialize these uh",
      "offset": 6449.88,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "parameters from scratch to be small",
      "offset": 6452.48,
      "duration": 3.639
    },
    {
      "lang": "en",
      "text": "random numbers and then we have to",
      "offset": 6454.44,
      "duration": 4.84
    },
    {
      "lang": "en",
      "text": "extend the weight inside this linear uh",
      "offset": 6456.119,
      "duration": 5.321
    },
    {
      "lang": "en",
      "text": "so we have to start making dot products",
      "offset": 6459.28,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "um with the associated parameters as",
      "offset": 6461.44,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "well to basically calculate the",
      "offset": 6463.199,
      "duration": 3.561
    },
    {
      "lang": "en",
      "text": "probabilities for these new tokens so",
      "offset": 6464.56,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "both of these are just a resizing",
      "offset": 6466.76,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "operation it's a very mild",
      "offset": 6468.639,
      "duration": 3.96
    },
    {
      "lang": "en",
      "text": "model surgery and can be done fairly",
      "offset": 6470.84,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "easily and it's quite common that",
      "offset": 6472.599,
      "duration": 2.761
    },
    {
      "lang": "en",
      "text": "basically you would freeze the base",
      "offset": 6474.04,
      "duration": 3.4
    },
    {
      "lang": "en",
      "text": "model you introduce these new parameters",
      "offset": 6475.36,
      "duration": 3.279
    },
    {
      "lang": "en",
      "text": "and then you only train these new",
      "offset": 6477.44,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "parameters to introduce new tokens into",
      "offset": 6478.639,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "the architecture um and so you can",
      "offset": 6480.56,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "freeze arbitrary parts of it or you can",
      "offset": 6483.119,
      "duration": 3.281
    },
    {
      "lang": "en",
      "text": "train arbitrary parts of it and that's",
      "offset": 6484.96,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "totally up to you but basically minor",
      "offset": 6486.4,
      "duration": 3.719
    },
    {
      "lang": "en",
      "text": "surgery required if you'd like to",
      "offset": 6488.32,
      "duration": 3.56
    },
    {
      "lang": "en",
      "text": "introduce new tokens and finally I'd",
      "offset": 6490.119,
      "duration": 3.241
    },
    {
      "lang": "en",
      "text": "like to mention that actually there's an",
      "offset": 6491.88,
      "duration": 4.04
    },
    {
      "lang": "en",
      "text": "entire design space of applications in",
      "offset": 6493.36,
      "duration": 4.279
    },
    {
      "lang": "en",
      "text": "terms of introducing new tokens into a",
      "offset": 6495.92,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "vocabulary that go Way Beyond just",
      "offset": 6497.639,
      "duration": 3.56
    },
    {
      "lang": "en",
      "text": "adding special tokens and special new",
      "offset": 6499.36,
      "duration": 3.64
    },
    {
      "lang": "en",
      "text": "functionality so just to give you a",
      "offset": 6501.199,
      "duration": 3.161
    },
    {
      "lang": "en",
      "text": "sense of the design space but this could",
      "offset": 6503,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "be an entire video just by itself uh",
      "offset": 6504.36,
      "duration": 4.279
    },
    {
      "lang": "en",
      "text": "this is a paper on learning to compress",
      "offset": 6506.599,
      "duration": 4.441
    },
    {
      "lang": "en",
      "text": "prompts with what they called uh gist",
      "offset": 6508.639,
      "duration": 4.761
    },
    {
      "lang": "en",
      "text": "tokens and the rough idea is suppose",
      "offset": 6511.04,
      "duration": 3.639
    },
    {
      "lang": "en",
      "text": "that you're using language models in a",
      "offset": 6513.4,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "setting that requires very long prompts",
      "offset": 6514.679,
      "duration": 4.121
    },
    {
      "lang": "en",
      "text": "while these long prompts just slow",
      "offset": 6517.159,
      "duration": 2.681
    },
    {
      "lang": "en",
      "text": "everything down because you have to",
      "offset": 6518.8,
      "duration": 2.6
    },
    {
      "lang": "en",
      "text": "encode them and then you have to use",
      "offset": 6519.84,
      "duration": 3.279
    },
    {
      "lang": "en",
      "text": "them and then you're tending over them",
      "offset": 6521.4,
      "duration": 3.719
    },
    {
      "lang": "en",
      "text": "and it's just um you know heavy to have",
      "offset": 6523.119,
      "duration": 4.52
    },
    {
      "lang": "en",
      "text": "very large prompts so instead what they",
      "offset": 6525.119,
      "duration": 5.56
    },
    {
      "lang": "en",
      "text": "do here in this paper is they introduce",
      "offset": 6527.639,
      "duration": 6.921
    },
    {
      "lang": "en",
      "text": "new tokens and um imagine basically",
      "offset": 6530.679,
      "duration": 5.721
    },
    {
      "lang": "en",
      "text": "having a few new tokens you put them in",
      "offset": 6534.56,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "a sequence and then you train the model",
      "offset": 6536.4,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "by distillation so you are keeping the",
      "offset": 6539.36,
      "duration": 3.799
    },
    {
      "lang": "en",
      "text": "entire model Frozen and you're only",
      "offset": 6541.52,
      "duration": 3.48
    },
    {
      "lang": "en",
      "text": "training the representations of the new",
      "offset": 6543.159,
      "duration": 3.801
    },
    {
      "lang": "en",
      "text": "tokens their embeddings and you're",
      "offset": 6545,
      "duration": 4.44
    },
    {
      "lang": "en",
      "text": "optimizing over the new tokens such that",
      "offset": 6546.96,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "the behavior of the language model is",
      "offset": 6549.44,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "identical uh to the model that has a",
      "offset": 6551.92,
      "duration": 5.759
    },
    {
      "lang": "en",
      "text": "very long prompt that works for you and",
      "offset": 6555.04,
      "duration": 3.96
    },
    {
      "lang": "en",
      "text": "so it's a compression technique of",
      "offset": 6557.679,
      "duration": 3.121
    },
    {
      "lang": "en",
      "text": "compressing that very long prompt into",
      "offset": 6559,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "those few new gist tokens and so you can",
      "offset": 6560.8,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "train this and then at test time you can",
      "offset": 6563.8,
      "duration": 2.919
    },
    {
      "lang": "en",
      "text": "discard your old prompt and just swap in",
      "offset": 6565.04,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "those tokens and they sort of like uh",
      "offset": 6566.719,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "stand in for that very long prompt and",
      "offset": 6568.639,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "have an almost identical performance and",
      "offset": 6571.119,
      "duration": 5.361
    },
    {
      "lang": "en",
      "text": "so this is one um technique and a class",
      "offset": 6573.679,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "of parameter efficient fine-tuning",
      "offset": 6576.48,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "techniques where most of the model is",
      "offset": 6578,
      "duration": 3.88
    },
    {
      "lang": "en",
      "text": "basically fixed and there's no training",
      "offset": 6579.92,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "of the model weights there's no training",
      "offset": 6581.88,
      "duration": 3.56
    },
    {
      "lang": "en",
      "text": "of Laura or anything like that of new",
      "offset": 6583.599,
      "duration": 3.64
    },
    {
      "lang": "en",
      "text": "parameters the the parameters that",
      "offset": 6585.44,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "you're training are now just the uh",
      "offset": 6587.239,
      "duration": 3.96
    },
    {
      "lang": "en",
      "text": "token embeddings so that's just one",
      "offset": 6589.119,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "example but this could again be like an",
      "offset": 6591.199,
      "duration": 3.321
    },
    {
      "lang": "en",
      "text": "entire video but just to give you a",
      "offset": 6592.88,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "sense that there's a whole design space",
      "offset": 6594.52,
      "duration": 2.84
    },
    {
      "lang": "en",
      "text": "here that is potentially worth exploring",
      "offset": 6595.76,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "in the future the next thing I want to",
      "offset": 6597.36,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "briefly address is that I think recently",
      "offset": 6599.199,
      "duration": 3.881
    },
    {
      "lang": "en",
      "text": "there's a lot of momentum in how you",
      "offset": 6601.199,
      "duration": 3.881
    },
    {
      "lang": "en",
      "text": "actually could construct Transformers",
      "offset": 6603.08,
      "duration": 3.72
    },
    {
      "lang": "en",
      "text": "that can simultaneously process not just",
      "offset": 6605.08,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "text as the input modality but a lot of",
      "offset": 6606.8,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "other modalities so be it images videos",
      "offset": 6608.84,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "audio Etc and how do you feed in all",
      "offset": 6611.52,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "these modalities and potentially predict",
      "offset": 6614.28,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "these modalities from a Transformer uh",
      "offset": 6616,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "do you have to change the architecture",
      "offset": 6618.84,
      "duration": 2.759
    },
    {
      "lang": "en",
      "text": "in some fundamental way and I think what",
      "offset": 6619.84,
      "duration": 3.279
    },
    {
      "lang": "en",
      "text": "a lot of people are starting to converge",
      "offset": 6621.599,
      "duration": 2.681
    },
    {
      "lang": "en",
      "text": "towards is that you're not changing the",
      "offset": 6623.119,
      "duration": 2.321
    },
    {
      "lang": "en",
      "text": "architecture you stick with the",
      "offset": 6624.28,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "Transformer you just kind of tokenize",
      "offset": 6625.44,
      "duration": 4.52
    },
    {
      "lang": "en",
      "text": "your input domains and then call the day",
      "offset": 6627.56,
      "duration": 3.96
    },
    {
      "lang": "en",
      "text": "and pretend it's just text tokens and",
      "offset": 6629.96,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "just do everything else identical in an",
      "offset": 6631.52,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "identical manner so here for example",
      "offset": 6633.96,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "there was a early paper that has nice",
      "offset": 6636.08,
      "duration": 3.519
    },
    {
      "lang": "en",
      "text": "graphic for how you can take an image",
      "offset": 6637.56,
      "duration": 4.599
    },
    {
      "lang": "en",
      "text": "and you can chunc at it into",
      "offset": 6639.599,
      "duration": 5.801
    },
    {
      "lang": "en",
      "text": "integers um and these sometimes uh so",
      "offset": 6642.159,
      "duration": 4.681
    },
    {
      "lang": "en",
      "text": "these will basically become the tokens",
      "offset": 6645.4,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "of images as an example and uh these",
      "offset": 6646.84,
      "duration": 5.359
    },
    {
      "lang": "en",
      "text": "tokens can be uh hard tokens where you",
      "offset": 6649.56,
      "duration": 4.36
    },
    {
      "lang": "en",
      "text": "force them to be integers they can also",
      "offset": 6652.199,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "be soft tokens where you uh sort of",
      "offset": 6653.92,
      "duration": 6.319
    },
    {
      "lang": "en",
      "text": "don't require uh these to be discrete",
      "offset": 6657,
      "duration": 5.159
    },
    {
      "lang": "en",
      "text": "but you do Force these representations",
      "offset": 6660.239,
      "duration": 4.521
    },
    {
      "lang": "en",
      "text": "to go through bottlenecks like in Auto",
      "offset": 6662.159,
      "duration": 4.761
    },
    {
      "lang": "en",
      "text": "encoders uh also in this paper that came",
      "offset": 6664.76,
      "duration": 4.12
    },
    {
      "lang": "en",
      "text": "out from open a SORA which I think",
      "offset": 6666.92,
      "duration": 4.92
    },
    {
      "lang": "en",
      "text": "really um uh blew the mind of many",
      "offset": 6668.88,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "people and inspired a lot of people in",
      "offset": 6671.84,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "terms of what's possible they have a",
      "offset": 6673.52,
      "duration": 3.4
    },
    {
      "lang": "en",
      "text": "Graphic here and they talk briefly about",
      "offset": 6675.199,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "how llms have text tokens Sora has",
      "offset": 6676.92,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "visual patches so again they came up",
      "offset": 6680.159,
      "duration": 4.761
    },
    {
      "lang": "en",
      "text": "with a way to chunc a videos into",
      "offset": 6682.52,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "basically tokens when they own",
      "offset": 6684.92,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "vocabularies and then you can either",
      "offset": 6686.52,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "process discrete tokens say with autog",
      "offset": 6688.52,
      "duration": 3.559
    },
    {
      "lang": "en",
      "text": "regressive models or even soft tokens",
      "offset": 6690.04,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "with diffusion models and uh all of that",
      "offset": 6692.079,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "is sort of uh being actively worked on",
      "offset": 6695.239,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "designed on and is beyond the scope of",
      "offset": 6698.239,
      "duration": 2.641
    },
    {
      "lang": "en",
      "text": "this video but just something I wanted",
      "offset": 6699.639,
      "duration": 3.321
    },
    {
      "lang": "en",
      "text": "to mention briefly okay now that we have",
      "offset": 6700.88,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "come quite deep into the tokenization",
      "offset": 6702.96,
      "duration": 3.8
    },
    {
      "lang": "en",
      "text": "algorithm and we understand a lot more",
      "offset": 6705.119,
      "duration": 3.801
    },
    {
      "lang": "en",
      "text": "about how it works let's loop back",
      "offset": 6706.76,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "around to the beginning of this video",
      "offset": 6708.92,
      "duration": 2.679
    },
    {
      "lang": "en",
      "text": "and go through some of these bullet",
      "offset": 6710.52,
      "duration": 4.36
    },
    {
      "lang": "en",
      "text": "points and really see why they happen so",
      "offset": 6711.599,
      "duration": 5.361
    },
    {
      "lang": "en",
      "text": "first of all why can't my llm spell",
      "offset": 6714.88,
      "duration": 3.88
    },
    {
      "lang": "en",
      "text": "words very well or do other spell",
      "offset": 6716.96,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "related",
      "offset": 6718.76,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "tasks so fundamentally this is because",
      "offset": 6720.56,
      "duration": 5.119
    },
    {
      "lang": "en",
      "text": "as we saw these characters are chunked",
      "offset": 6722.92,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "up into tokens and some of these tokens",
      "offset": 6725.679,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "are actually fairly long so as an",
      "offset": 6727.96,
      "duration": 4.84
    },
    {
      "lang": "en",
      "text": "example I went to the gp4 vocabulary and",
      "offset": 6730.4,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "I looked at uh one of the longer tokens",
      "offset": 6732.8,
      "duration": 5.08
    },
    {
      "lang": "en",
      "text": "so that default style turns out to be a",
      "offset": 6735.28,
      "duration": 4.439
    },
    {
      "lang": "en",
      "text": "single individual token so that's a lot",
      "offset": 6737.88,
      "duration": 4.279
    },
    {
      "lang": "en",
      "text": "of characters for a single token so my",
      "offset": 6739.719,
      "duration": 4.041
    },
    {
      "lang": "en",
      "text": "suspicion is that there's just too much",
      "offset": 6742.159,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "crammed into this single token and my",
      "offset": 6743.76,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "suspicion was that the model should not",
      "offset": 6746.079,
      "duration": 4.281
    },
    {
      "lang": "en",
      "text": "be very good at tasks related to",
      "offset": 6747.76,
      "duration": 6.919
    },
    {
      "lang": "en",
      "text": "spelling of this uh single token so I",
      "offset": 6750.36,
      "duration": 6.64
    },
    {
      "lang": "en",
      "text": "asked how many letters L are there in",
      "offset": 6754.679,
      "duration": 6.801
    },
    {
      "lang": "en",
      "text": "the word default style and of course my",
      "offset": 6757,
      "duration": 7.36
    },
    {
      "lang": "en",
      "text": "prompt is intentionally done that way",
      "offset": 6761.48,
      "duration": 4.28
    },
    {
      "lang": "en",
      "text": "and you see how default style will be a",
      "offset": 6764.36,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "single token so this is what the model",
      "offset": 6765.76,
      "duration": 3.64
    },
    {
      "lang": "en",
      "text": "sees so my suspicion is that it wouldn't",
      "offset": 6767.36,
      "duration": 3.96
    },
    {
      "lang": "en",
      "text": "be very good at this and indeed it is",
      "offset": 6769.4,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "not it doesn't actually know how many",
      "offset": 6771.32,
      "duration": 3.319
    },
    {
      "lang": "en",
      "text": "L's are in there it thinks there are",
      "offset": 6773.159,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "three and actually there are four if I'm",
      "offset": 6774.639,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "not getting this wrong myself so that",
      "offset": 6777,
      "duration": 5.32
    },
    {
      "lang": "en",
      "text": "didn't go extremely well let's look look",
      "offset": 6779.639,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "at another kind of uh character level",
      "offset": 6782.32,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "task so for example here I asked uh gp4",
      "offset": 6784.599,
      "duration": 6.56
    },
    {
      "lang": "en",
      "text": "to reverse the string default style and",
      "offset": 6788.4,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "they tried to use a code interpreter and",
      "offset": 6791.159,
      "duration": 4.281
    },
    {
      "lang": "en",
      "text": "I stopped it and I said just do it just",
      "offset": 6793.199,
      "duration": 6.361
    },
    {
      "lang": "en",
      "text": "try it and uh it gave me jumble so it",
      "offset": 6795.44,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "doesn't actually really know how to",
      "offset": 6799.56,
      "duration": 4.2
    },
    {
      "lang": "en",
      "text": "reverse this string going from right to",
      "offset": 6801.44,
      "duration": 5.32
    },
    {
      "lang": "en",
      "text": "left uh so it gave a wrong result so",
      "offset": 6803.76,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "again like working with this working",
      "offset": 6806.76,
      "duration": 3.24
    },
    {
      "lang": "en",
      "text": "hypothesis that maybe this is due to the",
      "offset": 6808.32,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "tokenization I tried a different",
      "offset": 6810,
      "duration": 4.119
    },
    {
      "lang": "en",
      "text": "approach I said okay let's reverse the",
      "offset": 6811.84,
      "duration": 4.6
    },
    {
      "lang": "en",
      "text": "exact same string but take the following",
      "offset": 6814.119,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "approach step one just print out every",
      "offset": 6816.44,
      "duration": 4.279
    },
    {
      "lang": "en",
      "text": "single character separated by spaces and",
      "offset": 6818.679,
      "duration": 4.601
    },
    {
      "lang": "en",
      "text": "then as a step two reverse that list and",
      "offset": 6820.719,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "it again Tred to use a tool but when I",
      "offset": 6823.28,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "stopped it it uh first uh produced all",
      "offset": 6824.8,
      "duration": 4.12
    },
    {
      "lang": "en",
      "text": "the characters and that was actually",
      "offset": 6827.76,
      "duration": 3.16
    },
    {
      "lang": "en",
      "text": "correct and then It reversed them and",
      "offset": 6828.92,
      "duration": 4.12
    },
    {
      "lang": "en",
      "text": "that was correct once it had this so",
      "offset": 6830.92,
      "duration": 3.96
    },
    {
      "lang": "en",
      "text": "somehow it can't reverse it directly but",
      "offset": 6833.04,
      "duration": 4.36
    },
    {
      "lang": "en",
      "text": "when you go just first uh you know",
      "offset": 6834.88,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "listing it out in order it can do that",
      "offset": 6837.4,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "somehow and then it can once it's uh",
      "offset": 6839.28,
      "duration": 4.6
    },
    {
      "lang": "en",
      "text": "broken up this way this becomes all",
      "offset": 6841.88,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "these individual characters and so now",
      "offset": 6843.88,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "this is much easier for it to see these",
      "offset": 6846.04,
      "duration": 4.039
    },
    {
      "lang": "en",
      "text": "individual tokens and reverse them and",
      "offset": 6847.88,
      "duration": 5.64
    },
    {
      "lang": "en",
      "text": "print them out so that is kind of",
      "offset": 6850.079,
      "duration": 6.761
    },
    {
      "lang": "en",
      "text": "interesting so let's continue now why",
      "offset": 6853.52,
      "duration": 6.88
    },
    {
      "lang": "en",
      "text": "are llms worse at uh non-english langu",
      "offset": 6856.84,
      "duration": 5.839
    },
    {
      "lang": "en",
      "text": "and I briefly covered this already but",
      "offset": 6860.4,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "basically um it's not only that the",
      "offset": 6862.679,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "language model sees less non-english",
      "offset": 6864.88,
      "duration": 3.88
    },
    {
      "lang": "en",
      "text": "data during training of the model",
      "offset": 6867.159,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "parameters but also the tokenizer is not",
      "offset": 6868.76,
      "duration": 5.879
    },
    {
      "lang": "en",
      "text": "um is not sufficiently trained on",
      "offset": 6871.639,
      "duration": 5.641
    },
    {
      "lang": "en",
      "text": "non-english data and so here for example",
      "offset": 6874.639,
      "duration": 5.881
    },
    {
      "lang": "en",
      "text": "hello how are you is five tokens and its",
      "offset": 6877.28,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "translation is 15 tokens so this is a",
      "offset": 6880.52,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "three times blow up and so for example",
      "offset": 6882.88,
      "duration": 5.759
    },
    {
      "lang": "en",
      "text": "anang is uh just hello basically in",
      "offset": 6885.8,
      "duration": 4.52
    },
    {
      "lang": "en",
      "text": "Korean and that end up being three",
      "offset": 6888.639,
      "duration": 3.161
    },
    {
      "lang": "en",
      "text": "tokens I'm actually kind of surprised by",
      "offset": 6890.32,
      "duration": 2.799
    },
    {
      "lang": "en",
      "text": "that because that is a very common",
      "offset": 6891.8,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "phrase there just the typical greeting",
      "offset": 6893.119,
      "duration": 3.881
    },
    {
      "lang": "en",
      "text": "of like hello and that ends up being",
      "offset": 6895.159,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "three tokens whereas our hello is a",
      "offset": 6897,
      "duration": 3.56
    },
    {
      "lang": "en",
      "text": "single token and so basically everything",
      "offset": 6898.76,
      "duration": 3.56
    },
    {
      "lang": "en",
      "text": "is a lot more bloated and diffuse and",
      "offset": 6900.56,
      "duration": 3.519
    },
    {
      "lang": "en",
      "text": "this is I think partly the reason that",
      "offset": 6902.32,
      "duration": 4.68
    },
    {
      "lang": "en",
      "text": "the model Works worse on other",
      "offset": 6904.079,
      "duration": 5.961
    },
    {
      "lang": "en",
      "text": "languages uh coming back why is LM bad",
      "offset": 6907,
      "duration": 6.159
    },
    {
      "lang": "en",
      "text": "at simple arithmetic um that has to do",
      "offset": 6910.04,
      "duration": 7.32
    },
    {
      "lang": "en",
      "text": "with the tokenization of numbers and so",
      "offset": 6913.159,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "um you'll notice that for example",
      "offset": 6917.36,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "addition is very sort of",
      "offset": 6919.079,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "like uh there's an algorithm that is",
      "offset": 6920.96,
      "duration": 4.759
    },
    {
      "lang": "en",
      "text": "like character level for doing addition",
      "offset": 6923.079,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "so for example here we would first add",
      "offset": 6925.719,
      "duration": 3.48
    },
    {
      "lang": "en",
      "text": "the ones and then the tens and then the",
      "offset": 6927.639,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "hundreds you have to refer to specific",
      "offset": 6929.199,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "parts of these digits but uh these",
      "offset": 6931.079,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "numbers are represented completely",
      "offset": 6934.719,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "arbitrarily based on whatever happened",
      "offset": 6936.199,
      "duration": 3.081
    },
    {
      "lang": "en",
      "text": "to merge or not merge during the",
      "offset": 6937.679,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "tokenization process there's an entire",
      "offset": 6939.28,
      "duration": 3.56
    },
    {
      "lang": "en",
      "text": "blog post about this that I think is",
      "offset": 6941.44,
      "duration": 3.279
    },
    {
      "lang": "en",
      "text": "quite good integer tokenization is",
      "offset": 6942.84,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "insane and this person basically",
      "offset": 6944.719,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "systematically explores the tokenization",
      "offset": 6946.679,
      "duration": 5.361
    },
    {
      "lang": "en",
      "text": "of numbers in I believe this is gpt2 and",
      "offset": 6948.719,
      "duration": 5.041
    },
    {
      "lang": "en",
      "text": "so they notice that for example for the",
      "offset": 6952.04,
      "duration": 5.24
    },
    {
      "lang": "en",
      "text": "for um four-digit numbers you can take a",
      "offset": 6953.76,
      "duration": 6.439
    },
    {
      "lang": "en",
      "text": "look at whether it is uh a single token",
      "offset": 6957.28,
      "duration": 4.839
    },
    {
      "lang": "en",
      "text": "or whether it is two tokens that is a 1",
      "offset": 6960.199,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "three or a 2 two or a 31 combination and",
      "offset": 6962.119,
      "duration": 4.441
    },
    {
      "lang": "en",
      "text": "so all the different numbers are all the",
      "offset": 6964.92,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "different combinations and you can",
      "offset": 6966.56,
      "duration": 2.639
    },
    {
      "lang": "en",
      "text": "imagine this is all completely",
      "offset": 6968.04,
      "duration": 3.24
    },
    {
      "lang": "en",
      "text": "arbitrarily so and the model",
      "offset": 6969.199,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "unfortunately sometimes sees uh four um",
      "offset": 6971.28,
      "duration": 5.319
    },
    {
      "lang": "en",
      "text": "a token for for all four digits",
      "offset": 6974.159,
      "duration": 3.881
    },
    {
      "lang": "en",
      "text": "sometimes for three sometimes for two",
      "offset": 6976.599,
      "duration": 3.401
    },
    {
      "lang": "en",
      "text": "sometimes for one and it's in an",
      "offset": 6978.04,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "arbitrary uh Manner and so this is",
      "offset": 6980,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "definitely a headwind if you will for",
      "offset": 6982.52,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "the language model and it's kind of",
      "offset": 6985,
      "duration": 2.92
    },
    {
      "lang": "en",
      "text": "incredible that it can kind of do it and",
      "offset": 6986.36,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "deal with it but it's also kind of not",
      "offset": 6987.92,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "ideal and so that's why for example we",
      "offset": 6990.119,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "saw that meta when they train the Llama",
      "offset": 6992,
      "duration": 4.44
    },
    {
      "lang": "en",
      "text": "2 algorithm and they use sentence piece",
      "offset": 6994.199,
      "duration": 5.321
    },
    {
      "lang": "en",
      "text": "they make sure to split up all the um",
      "offset": 6996.44,
      "duration": 5.88
    },
    {
      "lang": "en",
      "text": "all the digits as an example for uh",
      "offset": 6999.52,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "llama 2 and this is partly to improve a",
      "offset": 7002.32,
      "duration": 4.6
    },
    {
      "lang": "en",
      "text": "simple arithmetic kind of",
      "offset": 7004.88,
      "duration": 5.64
    },
    {
      "lang": "en",
      "text": "performance and finally why is gpt2 not",
      "offset": 7006.92,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "as good in Python again this is partly a",
      "offset": 7010.52,
      "duration": 4.36
    },
    {
      "lang": "en",
      "text": "modeling issue on in the architecture",
      "offset": 7012.92,
      "duration": 3.719
    },
    {
      "lang": "en",
      "text": "and the data set and the strength of the",
      "offset": 7014.88,
      "duration": 3.319
    },
    {
      "lang": "en",
      "text": "model but it's also partially",
      "offset": 7016.639,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "tokenization because as we saw here with",
      "offset": 7018.199,
      "duration": 4.841
    },
    {
      "lang": "en",
      "text": "the simple python example the encoding",
      "offset": 7020.32,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "efficiency of the tokenizer for handling",
      "offset": 7023.04,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "spaces in Python is terrible and every",
      "offset": 7025.199,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "single space is an individual token and",
      "offset": 7027.36,
      "duration": 3.719
    },
    {
      "lang": "en",
      "text": "this dramatically reduces the context",
      "offset": 7029.44,
      "duration": 3.08
    },
    {
      "lang": "en",
      "text": "length that the model can attend to",
      "offset": 7031.079,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "cross so that's almost like a",
      "offset": 7032.52,
      "duration": 4.28
    },
    {
      "lang": "en",
      "text": "tokenization bug for gpd2 and that was",
      "offset": 7034.079,
      "duration": 5.921
    },
    {
      "lang": "en",
      "text": "later fixed with gp4 okay so here's",
      "offset": 7036.8,
      "duration": 5.72
    },
    {
      "lang": "en",
      "text": "another fun one my llm abruptly halts",
      "offset": 7040,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "when it sees the string end of text so",
      "offset": 7042.52,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "here's um here's a very strange Behavior",
      "offset": 7045.28,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "print a string end of text is what I",
      "offset": 7048.04,
      "duration": 4.199
    },
    {
      "lang": "en",
      "text": "told jt4 and it says could you please",
      "offset": 7050.079,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "specify the string and I'm I'm telling",
      "offset": 7052.239,
      "duration": 4.92
    },
    {
      "lang": "en",
      "text": "it give me end of text and it seems like",
      "offset": 7055.119,
      "duration": 4.12
    },
    {
      "lang": "en",
      "text": "there's an issue it's not seeing end of",
      "offset": 7057.159,
      "duration": 4.44
    },
    {
      "lang": "en",
      "text": "text and then I give it end of text is",
      "offset": 7059.239,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "the string and then here's a string and",
      "offset": 7061.599,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "then it just doesn't print it so",
      "offset": 7064.239,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "obviously something is breaking here",
      "offset": 7065.84,
      "duration": 2.48
    },
    {
      "lang": "en",
      "text": "with respect to the handling of the",
      "offset": 7067.119,
      "duration": 3.08
    },
    {
      "lang": "en",
      "text": "special token and I don't actually know",
      "offset": 7068.32,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "what open ey is doing under the hood",
      "offset": 7070.199,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "here and whether they are potentially",
      "offset": 7072.639,
      "duration": 6.321
    },
    {
      "lang": "en",
      "text": "parsing this as an um as an actual token",
      "offset": 7074.52,
      "duration": 6.639
    },
    {
      "lang": "en",
      "text": "instead of this just being uh end of",
      "offset": 7078.96,
      "duration": 5.639
    },
    {
      "lang": "en",
      "text": "text um as like individual sort of",
      "offset": 7081.159,
      "duration": 5.281
    },
    {
      "lang": "en",
      "text": "pieces of it without the special token",
      "offset": 7084.599,
      "duration": 4.921
    },
    {
      "lang": "en",
      "text": "handling logic and so it might be that",
      "offset": 7086.44,
      "duration": 5.32
    },
    {
      "lang": "en",
      "text": "someone when they're calling do encode",
      "offset": 7089.52,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "uh they are passing in the allowed",
      "offset": 7091.76,
      "duration": 4.439
    },
    {
      "lang": "en",
      "text": "special and they are allowing end of",
      "offset": 7093.36,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "text as a special character in the user",
      "offset": 7096.199,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "prompt but the user prompt of course is",
      "offset": 7098.36,
      "duration": 5.16
    },
    {
      "lang": "en",
      "text": "is a sort of um attacker controlled text",
      "offset": 7100.84,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "so you would hope that they don't really",
      "offset": 7103.52,
      "duration": 5.24
    },
    {
      "lang": "en",
      "text": "parse or use special tokens or you know",
      "offset": 7105.32,
      "duration": 5.279
    },
    {
      "lang": "en",
      "text": "from that kind of input but it appears",
      "offset": 7108.76,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "that there's something definitely going",
      "offset": 7110.599,
      "duration": 4.201
    },
    {
      "lang": "en",
      "text": "wrong here and um so your knowledge of",
      "offset": 7111.76,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "these special tokens ends up being in a",
      "offset": 7114.8,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "tax surface potentially and so if you'd",
      "offset": 7116.4,
      "duration": 6.6
    },
    {
      "lang": "en",
      "text": "like to confuse llms then just um try to",
      "offset": 7118.88,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "give them some special tokens and see if",
      "offset": 7123,
      "duration": 3.4
    },
    {
      "lang": "en",
      "text": "you're breaking something by chance okay",
      "offset": 7124.32,
      "duration": 5.16
    },
    {
      "lang": "en",
      "text": "so this next one is a really fun one uh",
      "offset": 7126.4,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "the trailing whites space issue so if",
      "offset": 7129.48,
      "duration": 6.52
    },
    {
      "lang": "en",
      "text": "you come to playground and uh we come",
      "offset": 7132.88,
      "duration": 5.56
    },
    {
      "lang": "en",
      "text": "here to GPT 3.5 turbo instruct so this",
      "offset": 7136,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "is not a chat model this is a completion",
      "offset": 7138.44,
      "duration": 4.44
    },
    {
      "lang": "en",
      "text": "model so think of it more like it's a",
      "offset": 7140.32,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "lot more closer to a base model it does",
      "offset": 7142.88,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "completion it will continue the token",
      "offset": 7145.28,
      "duration": 4.6
    },
    {
      "lang": "en",
      "text": "sequence so here's a tagline for ice",
      "offset": 7147.599,
      "duration": 4.04
    },
    {
      "lang": "en",
      "text": "cream shop and we want to continue the",
      "offset": 7149.88,
      "duration": 4.359
    },
    {
      "lang": "en",
      "text": "sequence and so we can submit and get a",
      "offset": 7151.639,
      "duration": 6.6
    },
    {
      "lang": "en",
      "text": "bunch of tokens okay no problem but now",
      "offset": 7154.239,
      "duration": 6.601
    },
    {
      "lang": "en",
      "text": "suppose I do this but instead of",
      "offset": 7158.239,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "pressing submit here I do here's a",
      "offset": 7160.84,
      "duration": 5.16
    },
    {
      "lang": "en",
      "text": "tagline for ice cream shop space so I",
      "offset": 7163.119,
      "duration": 5.841
    },
    {
      "lang": "en",
      "text": "have a space here before I click",
      "offset": 7166,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "submit we get a warning your text ends",
      "offset": 7168.96,
      "duration": 4.44
    },
    {
      "lang": "en",
      "text": "in a trail Ling space which causes worse",
      "offset": 7171.84,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "performance due to how API splits text",
      "offset": 7173.4,
      "duration": 4.839
    },
    {
      "lang": "en",
      "text": "into tokens so what's happening here it",
      "offset": 7175.84,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "still gave us a uh sort of completion",
      "offset": 7178.239,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "here but let's take a look at what's",
      "offset": 7180.56,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "happening so here's a tagline for an ice",
      "offset": 7182.8,
      "duration": 5.879
    },
    {
      "lang": "en",
      "text": "cream shop and then what does this look",
      "offset": 7184.88,
      "duration": 5.279
    },
    {
      "lang": "en",
      "text": "like in the actual actual training data",
      "offset": 7188.679,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "suppose you found the completion in the",
      "offset": 7190.159,
      "duration": 3.401
    },
    {
      "lang": "en",
      "text": "training document somewhere on the",
      "offset": 7192.28,
      "duration": 3.399
    },
    {
      "lang": "en",
      "text": "internet and the llm trained on this",
      "offset": 7193.56,
      "duration": 4.76
    },
    {
      "lang": "en",
      "text": "data so maybe it's something like oh",
      "offset": 7195.679,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "yeah maybe that's the tagline that's a",
      "offset": 7198.32,
      "duration": 4.44
    },
    {
      "lang": "en",
      "text": "terrible tagline but notice here that",
      "offset": 7200.4,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "when I create o you see that because",
      "offset": 7202.76,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "there's the the space character is",
      "offset": 7205.76,
      "duration": 5.399
    },
    {
      "lang": "en",
      "text": "always a prefix to these tokens in GPT",
      "offset": 7207.8,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "so it's not an O token it's a space o",
      "offset": 7211.159,
      "duration": 5.601
    },
    {
      "lang": "en",
      "text": "token the space is part of the O and",
      "offset": 7213.48,
      "duration": 5.759
    },
    {
      "lang": "en",
      "text": "together they are token 8840 that's",
      "offset": 7216.76,
      "duration": 5.16
    },
    {
      "lang": "en",
      "text": "that's space o so what's What's",
      "offset": 7219.239,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "Happening Here is that when I just have",
      "offset": 7221.92,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "it like this and I let it complete the",
      "offset": 7224.119,
      "duration": 5.921
    },
    {
      "lang": "en",
      "text": "next token it can sample the space o",
      "offset": 7227.04,
      "duration": 5.559
    },
    {
      "lang": "en",
      "text": "token but instead if I have this and I",
      "offset": 7230.04,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "add my space then what I'm doing here",
      "offset": 7232.599,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "when I incode this string is I have",
      "offset": 7234.76,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "basically here's a t line for an ice",
      "offset": 7237.639,
      "duration": 4.361
    },
    {
      "lang": "en",
      "text": "cream uh shop and this space at the very",
      "offset": 7239.079,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "end becomes a token",
      "offset": 7242,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "220 and so we've added token 220 and",
      "offset": 7244.079,
      "duration": 5.681
    },
    {
      "lang": "en",
      "text": "this token otherwise would be part of",
      "offset": 7247.84,
      "duration": 4.04
    },
    {
      "lang": "en",
      "text": "the tagline because if there actually is",
      "offset": 7249.76,
      "duration": 5.479
    },
    {
      "lang": "en",
      "text": "a tagline here so space o is the token",
      "offset": 7251.88,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "and so this is suddenly a of",
      "offset": 7255.239,
      "duration": 4.44
    },
    {
      "lang": "en",
      "text": "distribution for the model because this",
      "offset": 7257.32,
      "duration": 4.2
    },
    {
      "lang": "en",
      "text": "space is part of the next token but",
      "offset": 7259.679,
      "duration": 4.361
    },
    {
      "lang": "en",
      "text": "we're putting it here like this and the",
      "offset": 7261.52,
      "duration": 5.679
    },
    {
      "lang": "en",
      "text": "model has seen very very little data of",
      "offset": 7264.04,
      "duration": 6.039
    },
    {
      "lang": "en",
      "text": "actual Space by itself and we're asking",
      "offset": 7267.199,
      "duration": 4.52
    },
    {
      "lang": "en",
      "text": "it to complete the sequence like add in",
      "offset": 7270.079,
      "duration": 3.401
    },
    {
      "lang": "en",
      "text": "more tokens but the problem is that",
      "offset": 7271.719,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "we've sort of begun the first token and",
      "offset": 7273.48,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "now it's been split up and now we're out",
      "offset": 7276.36,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "of this distribution and now arbitrary",
      "offset": 7278.76,
      "duration": 4.28
    },
    {
      "lang": "en",
      "text": "bad things happen and it's just a very",
      "offset": 7280.76,
      "duration": 3.8
    },
    {
      "lang": "en",
      "text": "rare example for it to see something",
      "offset": 7283.04,
      "duration": 3.88
    },
    {
      "lang": "en",
      "text": "like that and uh that's why we get the",
      "offset": 7284.56,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "warning so the fundamental issue here is",
      "offset": 7286.92,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "of course that um the llm is on top of",
      "offset": 7289.119,
      "duration": 5.48
    },
    {
      "lang": "en",
      "text": "these tokens and these tokens are text",
      "offset": 7292.44,
      "duration": 4.12
    },
    {
      "lang": "en",
      "text": "chunks they're not characters in a way",
      "offset": 7294.599,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "you and I would think of them they are",
      "offset": 7296.56,
      "duration": 3.8
    },
    {
      "lang": "en",
      "text": "these are the atoms of what the LM is",
      "offset": 7298.199,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "seeing and there's a bunch of weird",
      "offset": 7300.36,
      "duration": 3.279
    },
    {
      "lang": "en",
      "text": "stuff that comes out of it let's go back",
      "offset": 7301.8,
      "duration": 6.2
    },
    {
      "lang": "en",
      "text": "to our default cell style I bet you that",
      "offset": 7303.639,
      "duration": 6.321
    },
    {
      "lang": "en",
      "text": "the model has never in its training set",
      "offset": 7308,
      "duration": 6.199
    },
    {
      "lang": "en",
      "text": "seen default cell sta without Le in",
      "offset": 7309.96,
      "duration": 6.639
    },
    {
      "lang": "en",
      "text": "there it's always seen this as a single",
      "offset": 7314.199,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "group because uh this is some kind of a",
      "offset": 7316.599,
      "duration": 5.401
    },
    {
      "lang": "en",
      "text": "function in um I'm guess I don't",
      "offset": 7319.239,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "actually know what this is part of this",
      "offset": 7322,
      "duration": 3.119
    },
    {
      "lang": "en",
      "text": "is some kind of API but I bet you that",
      "offset": 7323.079,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "it's never seen this combination of",
      "offset": 7325.119,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "tokens uh in its training data because",
      "offset": 7327.079,
      "duration": 5.281
    },
    {
      "lang": "en",
      "text": "or I think it would be extremely rare so",
      "offset": 7330.639,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "I took this and I copy pasted it here",
      "offset": 7332.36,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "and I had I tried to complete from it",
      "offset": 7334.719,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "and the it immediately gave me a big",
      "offset": 7337.48,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "error and it said the model predicted to",
      "offset": 7339.199,
      "duration": 3.121
    },
    {
      "lang": "en",
      "text": "completion that begins with a stop",
      "offset": 7341.079,
      "duration": 3.08
    },
    {
      "lang": "en",
      "text": "sequence resulting in no output consider",
      "offset": 7342.32,
      "duration": 4.04
    },
    {
      "lang": "en",
      "text": "adjusting your prompt or stop sequences",
      "offset": 7344.159,
      "duration": 3.48
    },
    {
      "lang": "en",
      "text": "so what happened here when I clicked",
      "offset": 7346.36,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "submit is that immediately the model",
      "offset": 7347.639,
      "duration": 4.6
    },
    {
      "lang": "en",
      "text": "emitted and sort of like end of text",
      "offset": 7350.199,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "token I think or something like that it",
      "offset": 7352.239,
      "duration": 4.201
    },
    {
      "lang": "en",
      "text": "basically predicted the stop sequence",
      "offset": 7354.44,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "immediately so it had no completion and",
      "offset": 7356.44,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "so this is why I'm getting a warning",
      "offset": 7358.76,
      "duration": 3.399
    },
    {
      "lang": "en",
      "text": "again because we're off the data",
      "offset": 7360.199,
      "duration": 4.92
    },
    {
      "lang": "en",
      "text": "distribution and the model is just uh",
      "offset": 7362.159,
      "duration": 5.48
    },
    {
      "lang": "en",
      "text": "predicting just totally arbitrary things",
      "offset": 7365.119,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "it's just really confused basically this",
      "offset": 7367.639,
      "duration": 3.281
    },
    {
      "lang": "en",
      "text": "is uh this is giving it brain damage",
      "offset": 7369.44,
      "duration": 3.88
    },
    {
      "lang": "en",
      "text": "it's never seen this before it's shocked",
      "offset": 7370.92,
      "duration": 3.64
    },
    {
      "lang": "en",
      "text": "and it's predicting end of text or",
      "offset": 7373.32,
      "duration": 3.72
    },
    {
      "lang": "en",
      "text": "something I tried it again here and it",
      "offset": 7374.56,
      "duration": 4.519
    },
    {
      "lang": "en",
      "text": "in this case it completed it but then",
      "offset": 7377.04,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "for some reason this request May violate",
      "offset": 7379.079,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "our usage policies this was",
      "offset": 7381.44,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "flagged um basically something just like",
      "offset": 7383.639,
      "duration": 4.04
    },
    {
      "lang": "en",
      "text": "goes wrong and there's something like",
      "offset": 7386.639,
      "duration": 2.881
    },
    {
      "lang": "en",
      "text": "Jank you can just feel the Jank because",
      "offset": 7387.679,
      "duration": 3.721
    },
    {
      "lang": "en",
      "text": "the model is like extremely unhappy with",
      "offset": 7389.52,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "just this and it doesn't know how to",
      "offset": 7391.4,
      "duration": 2.759
    },
    {
      "lang": "en",
      "text": "complete it because it's never occurred",
      "offset": 7392.96,
      "duration": 3.239
    },
    {
      "lang": "en",
      "text": "in training set in a training set it",
      "offset": 7394.159,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "always appears like this and becomes a",
      "offset": 7396.199,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "single token",
      "offset": 7398.32,
      "duration": 3.64
    },
    {
      "lang": "en",
      "text": "so these kinds of issues where tokens",
      "offset": 7400.04,
      "duration": 4.199
    },
    {
      "lang": "en",
      "text": "are either you sort of like complete the",
      "offset": 7401.96,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "first character of the next token or you",
      "offset": 7404.239,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "are sort of you have long tokens that",
      "offset": 7406.76,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "you then have just some of the",
      "offset": 7408.56,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "characters off all of these are kind of",
      "offset": 7409.8,
      "duration": 5.56
    },
    {
      "lang": "en",
      "text": "like issues with partial tokens is how I",
      "offset": 7412.32,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "would describe it and if you actually",
      "offset": 7415.36,
      "duration": 4.44
    },
    {
      "lang": "en",
      "text": "dig into the T token",
      "offset": 7417.76,
      "duration": 4.2
    },
    {
      "lang": "en",
      "text": "repository go to the rust code and",
      "offset": 7419.8,
      "duration": 4.359
    },
    {
      "lang": "en",
      "text": "search for",
      "offset": 7421.96,
      "duration": 5.119
    },
    {
      "lang": "en",
      "text": "unstable and you'll see um en code",
      "offset": 7424.159,
      "duration": 5.08
    },
    {
      "lang": "en",
      "text": "unstable native unstable token tokens",
      "offset": 7427.079,
      "duration": 4.441
    },
    {
      "lang": "en",
      "text": "and a lot of like special case handling",
      "offset": 7429.239,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "none of this stuff about unstable tokens",
      "offset": 7431.52,
      "duration": 3.96
    },
    {
      "lang": "en",
      "text": "is documented anywhere but there's a ton",
      "offset": 7433.4,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "of code dealing with unstable tokens and",
      "offset": 7435.48,
      "duration": 5.32
    },
    {
      "lang": "en",
      "text": "unstable tokens is exactly kind of like",
      "offset": 7438.36,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "what I'm describing here what you would",
      "offset": 7440.8,
      "duration": 4.439
    },
    {
      "lang": "en",
      "text": "like out of a completion API is",
      "offset": 7442.76,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "something a lot more fancy like if we're",
      "offset": 7445.239,
      "duration": 3.721
    },
    {
      "lang": "en",
      "text": "putting in default cell sta if we're",
      "offset": 7446.599,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "asking for the next token sequence we're",
      "offset": 7448.96,
      "duration": 3.279
    },
    {
      "lang": "en",
      "text": "not actually trying to append the next",
      "offset": 7450.679,
      "duration": 3.96
    },
    {
      "lang": "en",
      "text": "token exactly after this list we're",
      "offset": 7452.239,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "actually trying to append we're trying",
      "offset": 7454.639,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "to consider lots of tokens um",
      "offset": 7456.48,
      "duration": 5.679
    },
    {
      "lang": "en",
      "text": "that if we were or I guess like we're",
      "offset": 7459.52,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "trying to search over characters that if",
      "offset": 7462.159,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "we retened would be of high probability",
      "offset": 7465.76,
      "duration": 4.919
    },
    {
      "lang": "en",
      "text": "if that makes sense um so that we can",
      "offset": 7468.159,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "actually add a single individual",
      "offset": 7470.679,
      "duration": 3.801
    },
    {
      "lang": "en",
      "text": "character uh instead of just like adding",
      "offset": 7472.32,
      "duration": 4.359
    },
    {
      "lang": "en",
      "text": "the next full token that comes after",
      "offset": 7474.48,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "this partial token list so I this is",
      "offset": 7476.679,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "very tricky to describe and I invite you",
      "offset": 7479.36,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "to maybe like look through this it ends",
      "offset": 7481.32,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "up being extremely gnarly and hairy kind",
      "offset": 7483.04,
      "duration": 3.32
    },
    {
      "lang": "en",
      "text": "of topic it and it comes from",
      "offset": 7484.679,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "tokenization fundamentally so um maybe I",
      "offset": 7486.36,
      "duration": 4.44
    },
    {
      "lang": "en",
      "text": "can even spend an entire video talking",
      "offset": 7489.4,
      "duration": 2.719
    },
    {
      "lang": "en",
      "text": "about unstable tokens sometime in the",
      "offset": 7490.8,
      "duration": 3.399
    },
    {
      "lang": "en",
      "text": "future okay and I'm really saving the",
      "offset": 7492.119,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "best for last my favorite one by far is",
      "offset": 7494.199,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "the solid gold",
      "offset": 7496.599,
      "duration": 4.761
    },
    {
      "lang": "en",
      "text": "Magikarp and it just okay so this comes",
      "offset": 7499.199,
      "duration": 4.44
    },
    {
      "lang": "en",
      "text": "from this blog post uh solid gold",
      "offset": 7501.36,
      "duration": 5.64
    },
    {
      "lang": "en",
      "text": "Magikarp and uh this is um internet",
      "offset": 7503.639,
      "duration": 6.44
    },
    {
      "lang": "en",
      "text": "famous now for those of us in llms and",
      "offset": 7507,
      "duration": 4.84
    },
    {
      "lang": "en",
      "text": "basically I I would advise you to uh",
      "offset": 7510.079,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "read this block Post in full but",
      "offset": 7511.84,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "basically what this person was doing is",
      "offset": 7513.679,
      "duration": 5.56
    },
    {
      "lang": "en",
      "text": "this person went to the um",
      "offset": 7516.559,
      "duration": 5.761
    },
    {
      "lang": "en",
      "text": "token embedding stable and clustered the",
      "offset": 7519.239,
      "duration": 5.561
    },
    {
      "lang": "en",
      "text": "tokens based on their embedding",
      "offset": 7522.32,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "representation and this person noticed",
      "offset": 7524.8,
      "duration": 4.439
    },
    {
      "lang": "en",
      "text": "that there's a cluster of tokens that",
      "offset": 7527.28,
      "duration": 3.879
    },
    {
      "lang": "en",
      "text": "look really strange so there's a cluster",
      "offset": 7529.239,
      "duration": 4.84
    },
    {
      "lang": "en",
      "text": "here at rot e stream Fame solid gold",
      "offset": 7531.159,
      "duration": 4.841
    },
    {
      "lang": "en",
      "text": "Magikarp Signet message like really",
      "offset": 7534.079,
      "duration": 5.881
    },
    {
      "lang": "en",
      "text": "weird tokens in uh basically in this",
      "offset": 7536,
      "duration": 6.239
    },
    {
      "lang": "en",
      "text": "embedding cluster and so what are these",
      "offset": 7539.96,
      "duration": 3.719
    },
    {
      "lang": "en",
      "text": "tokens and where do they even come from",
      "offset": 7542.239,
      "duration": 3.161
    },
    {
      "lang": "en",
      "text": "like what is solid gold magikarpet makes",
      "offset": 7543.679,
      "duration": 5.281
    },
    {
      "lang": "en",
      "text": "no sense and then they found bunch of",
      "offset": 7545.4,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "these",
      "offset": 7548.96,
      "duration": 3.159
    },
    {
      "lang": "en",
      "text": "tokens and then they notice that",
      "offset": 7550.199,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "actually the plot thickens here because",
      "offset": 7552.119,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "if you ask the model about these tokens",
      "offset": 7553.559,
      "duration": 5.08
    },
    {
      "lang": "en",
      "text": "like you ask it uh some very benign",
      "offset": 7556.04,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "question like please can you repeat back",
      "offset": 7558.639,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "to me the string sold gold Magikarp uh",
      "offset": 7560.199,
      "duration": 4.601
    },
    {
      "lang": "en",
      "text": "then you get a variety of basically",
      "offset": 7562.96,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "totally broken llm Behavior so either",
      "offset": 7564.8,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "you get evasion so I'm sorry I can't",
      "offset": 7567.76,
      "duration": 3.64
    },
    {
      "lang": "en",
      "text": "hear you or you get a bunch of",
      "offset": 7569.84,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "hallucinations as a response um you can",
      "offset": 7571.4,
      "duration": 5.88
    },
    {
      "lang": "en",
      "text": "even get back like insults so you ask it",
      "offset": 7574.559,
      "duration": 5.441
    },
    {
      "lang": "en",
      "text": "uh about streamer bot it uh tells the",
      "offset": 7577.28,
      "duration": 4.76
    },
    {
      "lang": "en",
      "text": "and the model actually just calls you",
      "offset": 7580,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "names uh or it kind of comes up with",
      "offset": 7582.04,
      "duration": 4.199
    },
    {
      "lang": "en",
      "text": "like weird humor like you're actually",
      "offset": 7584.159,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "breaking the model by asking about these",
      "offset": 7586.239,
      "duration": 4.281
    },
    {
      "lang": "en",
      "text": "very simple strings like at Roth and",
      "offset": 7588.48,
      "duration": 4.36
    },
    {
      "lang": "en",
      "text": "sold gold Magikarp so like what the hell",
      "offset": 7590.52,
      "duration": 3.96
    },
    {
      "lang": "en",
      "text": "is happening and there's a variety of",
      "offset": 7592.84,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "here documented behaviors uh there's a",
      "offset": 7594.48,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "bunch of tokens not just so good",
      "offset": 7597.079,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "Magikarp that have that kind of a",
      "offset": 7598.48,
      "duration": 3.639
    },
    {
      "lang": "en",
      "text": "behavior and so basically there's a",
      "offset": 7600.28,
      "duration": 3.879
    },
    {
      "lang": "en",
      "text": "bunch of like trigger words and if you",
      "offset": 7602.119,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "ask the model about these trigger words",
      "offset": 7604.159,
      "duration": 3.881
    },
    {
      "lang": "en",
      "text": "or you just include them in your prompt",
      "offset": 7606.04,
      "duration": 3.96
    },
    {
      "lang": "en",
      "text": "the model goes haywire and has all kinds",
      "offset": 7608.04,
      "duration": 4.76
    },
    {
      "lang": "en",
      "text": "of uh really Strange Behaviors including",
      "offset": 7610,
      "duration": 4.84
    },
    {
      "lang": "en",
      "text": "sort of ones that violate typical safety",
      "offset": 7612.8,
      "duration": 4.2
    },
    {
      "lang": "en",
      "text": "guidelines uh and the alignment of the",
      "offset": 7614.84,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "model like it's swearing back at you so",
      "offset": 7617,
      "duration": 4.76
    },
    {
      "lang": "en",
      "text": "what is happening here and how can this",
      "offset": 7619.84,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "possibly be true well this again comes",
      "offset": 7621.76,
      "duration": 4.959
    },
    {
      "lang": "en",
      "text": "down to tokenization so what's happening",
      "offset": 7624.559,
      "duration": 4.201
    },
    {
      "lang": "en",
      "text": "here is that sold gold Magikarp if you",
      "offset": 7626.719,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "actually dig into it is a Reddit user so",
      "offset": 7628.76,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "there's a u Sol gold",
      "offset": 7631.719,
      "duration": 5.081
    },
    {
      "lang": "en",
      "text": "Magikarp and probably what happened here",
      "offset": 7634.04,
      "duration": 3.96
    },
    {
      "lang": "en",
      "text": "even though I I don't know that this has",
      "offset": 7636.8,
      "duration": 3.64
    },
    {
      "lang": "en",
      "text": "been like really definitively explored",
      "offset": 7638,
      "duration": 5.159
    },
    {
      "lang": "en",
      "text": "but what is thought to have happened is",
      "offset": 7640.44,
      "duration": 5.119
    },
    {
      "lang": "en",
      "text": "that the tokenization data set was very",
      "offset": 7643.159,
      "duration": 4.841
    },
    {
      "lang": "en",
      "text": "different from the training data set for",
      "offset": 7645.559,
      "duration": 4.361
    },
    {
      "lang": "en",
      "text": "the actual language model so in the",
      "offset": 7648,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "tokenization data set there was a ton of",
      "offset": 7649.92,
      "duration": 4.679
    },
    {
      "lang": "en",
      "text": "redded data potentially where the user",
      "offset": 7651.52,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "solid gold Magikarp was mentioned in the",
      "offset": 7654.599,
      "duration": 4.6
    },
    {
      "lang": "en",
      "text": "text because solid gold Magikarp was a",
      "offset": 7656.4,
      "duration": 5.279
    },
    {
      "lang": "en",
      "text": "very common um sort of uh person who",
      "offset": 7659.199,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "would post a lot uh this would be a",
      "offset": 7661.679,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "string that occurs many times in a",
      "offset": 7663.679,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "tokenization data set because it occurs",
      "offset": 7665.28,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "many times in a tokenization data set",
      "offset": 7668,
      "duration": 3.48
    },
    {
      "lang": "en",
      "text": "these tokens would end up getting merged",
      "offset": 7670,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "to the single individual token for that",
      "offset": 7671.48,
      "duration": 4.92
    },
    {
      "lang": "en",
      "text": "single Reddit user sold gold Magikarp so",
      "offset": 7673.52,
      "duration": 4.84
    },
    {
      "lang": "en",
      "text": "they would have a dedicated token in a",
      "offset": 7676.4,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "vocabulary of was it 50,000 tokens in",
      "offset": 7678.36,
      "duration": 5.759
    },
    {
      "lang": "en",
      "text": "gpd2 that is devoted to that Reddit user",
      "offset": 7680.719,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "and then what happens is the",
      "offset": 7684.119,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "tokenization data set has those strings",
      "offset": 7685.599,
      "duration": 5.321
    },
    {
      "lang": "en",
      "text": "but then later when you train the model",
      "offset": 7688.599,
      "duration": 5.321
    },
    {
      "lang": "en",
      "text": "the language model itself um this data",
      "offset": 7690.92,
      "duration": 5.759
    },
    {
      "lang": "en",
      "text": "from Reddit was not present and so",
      "offset": 7693.92,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "therefore in the entire training set for",
      "offset": 7696.679,
      "duration": 4.601
    },
    {
      "lang": "en",
      "text": "the language model sold gold Magikarp",
      "offset": 7698.8,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "never occurs that token never appears in",
      "offset": 7701.28,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "the training set for the actual language",
      "offset": 7704.32,
      "duration": 4.6
    },
    {
      "lang": "en",
      "text": "model later so this token never gets",
      "offset": 7705.84,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "activated it's initialized at random in",
      "offset": 7708.92,
      "duration": 3.96
    },
    {
      "lang": "en",
      "text": "the beginning of optimization then you",
      "offset": 7711.04,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "have forward backward passes and updates",
      "offset": 7712.88,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "to the model and this token is just",
      "offset": 7714.48,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "never updated in the embedding table",
      "offset": 7716,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "that row Vector never gets sampled it",
      "offset": 7717.92,
      "duration": 4.12
    },
    {
      "lang": "en",
      "text": "never gets used so it never gets trained",
      "offset": 7720,
      "duration": 3.88
    },
    {
      "lang": "en",
      "text": "and it's completely untrained it's kind",
      "offset": 7722.04,
      "duration": 4.36
    },
    {
      "lang": "en",
      "text": "of like unallocated memory in a typical",
      "offset": 7723.88,
      "duration": 4.279
    },
    {
      "lang": "en",
      "text": "binary program written in C or something",
      "offset": 7726.4,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "like that that so it's unallocated",
      "offset": 7728.159,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "memory and then at test time if you",
      "offset": 7730,
      "duration": 4.28
    },
    {
      "lang": "en",
      "text": "evoke this token then you're basically",
      "offset": 7731.84,
      "duration": 3.799
    },
    {
      "lang": "en",
      "text": "plucking out a row of the embedding",
      "offset": 7734.28,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "table that is completely untrained and",
      "offset": 7735.639,
      "duration": 3.281
    },
    {
      "lang": "en",
      "text": "that feeds into a Transformer and",
      "offset": 7737.32,
      "duration": 3.64
    },
    {
      "lang": "en",
      "text": "creates undefined behavior and that's",
      "offset": 7738.92,
      "duration": 3.239
    },
    {
      "lang": "en",
      "text": "what we're seeing here this completely",
      "offset": 7740.96,
      "duration": 2.92
    },
    {
      "lang": "en",
      "text": "undefined never before seen in a",
      "offset": 7742.159,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "training behavior and so any of these",
      "offset": 7743.88,
      "duration": 4.12
    },
    {
      "lang": "en",
      "text": "kind of like weird tokens would evoke",
      "offset": 7746.559,
      "duration": 2.761
    },
    {
      "lang": "en",
      "text": "this Behavior because fundamentally the",
      "offset": 7748,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "model is um is uh uh out of sample out",
      "offset": 7749.32,
      "duration": 7.44
    },
    {
      "lang": "en",
      "text": "of distribution okay and the very last",
      "offset": 7754.48,
      "duration": 4.04
    },
    {
      "lang": "en",
      "text": "thing I wanted to just briefly mention",
      "offset": 7756.76,
      "duration": 2.919
    },
    {
      "lang": "en",
      "text": "point out although I think a lot of",
      "offset": 7758.52,
      "duration": 3.119
    },
    {
      "lang": "en",
      "text": "people are quite aware of this is that",
      "offset": 7759.679,
      "duration": 3.48
    },
    {
      "lang": "en",
      "text": "different kinds of formats and different",
      "offset": 7761.639,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "representations and different languages",
      "offset": 7763.159,
      "duration": 3.721
    },
    {
      "lang": "en",
      "text": "and so on might be more or less",
      "offset": 7765,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "efficient with GPD tokenizers uh or any",
      "offset": 7766.88,
      "duration": 4.52
    },
    {
      "lang": "en",
      "text": "tokenizers for any other L for that",
      "offset": 7769.8,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "matter so for example Json is actually",
      "offset": 7771.4,
      "duration": 4.92
    },
    {
      "lang": "en",
      "text": "really dense in tokens and yaml is a lot",
      "offset": 7773.559,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "more efficient in tokens um so for",
      "offset": 7776.32,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "example this are these are the same in",
      "offset": 7779.239,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "Json and in yaml the Json is",
      "offset": 7781.32,
      "duration": 6.799
    },
    {
      "lang": "en",
      "text": "116 and the yaml is 99 so quite a bit of",
      "offset": 7784.599,
      "duration": 7.04
    },
    {
      "lang": "en",
      "text": "an Improvement and so in the token",
      "offset": 7788.119,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "economy where we are paying uh per token",
      "offset": 7791.639,
      "duration": 4.04
    },
    {
      "lang": "en",
      "text": "in many ways and you are paying in the",
      "offset": 7793.639,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "context length and you're paying in um",
      "offset": 7795.679,
      "duration": 4.201
    },
    {
      "lang": "en",
      "text": "dollar amount for uh the cost of",
      "offset": 7797.639,
      "duration": 3.56
    },
    {
      "lang": "en",
      "text": "processing all this kind of structured",
      "offset": 7799.88,
      "duration": 3.64
    },
    {
      "lang": "en",
      "text": "data when you have to um so prefer to",
      "offset": 7801.199,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "use theal over Json and in general kind",
      "offset": 7803.52,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "of like the tokenization density is",
      "offset": 7806.079,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "something that you have to um sort of",
      "offset": 7807.599,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "care about and worry about at all times",
      "offset": 7809.84,
      "duration": 3.56
    },
    {
      "lang": "en",
      "text": "and try to find efficient encoding",
      "offset": 7811.679,
      "duration": 3.721
    },
    {
      "lang": "en",
      "text": "schemes and spend a lot of time in tick",
      "offset": 7813.4,
      "duration": 3.48
    },
    {
      "lang": "en",
      "text": "tokenizer and measure the different",
      "offset": 7815.4,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "token efficiencies of different formats",
      "offset": 7816.88,
      "duration": 4.12
    },
    {
      "lang": "en",
      "text": "and settings and so on okay so that",
      "offset": 7818.92,
      "duration": 4.44
    },
    {
      "lang": "en",
      "text": "concludes my fairly long video on",
      "offset": 7821,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "tokenization I know it's a try I know",
      "offset": 7823.36,
      "duration": 5.08
    },
    {
      "lang": "en",
      "text": "it's annoying I know it's irritating I",
      "offset": 7825.96,
      "duration": 4.92
    },
    {
      "lang": "en",
      "text": "personally really dislike the stage what",
      "offset": 7828.44,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "I do have to say at this point is don't",
      "offset": 7830.88,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "brush it off there's a lot of foot guns",
      "offset": 7832.599,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "sharp edges here security issues uh AI",
      "offset": 7834.96,
      "duration": 4.92
    },
    {
      "lang": "en",
      "text": "safety issues as we saw plugging in",
      "offset": 7838.119,
      "duration": 3.96
    },
    {
      "lang": "en",
      "text": "unallocated memory into uh language",
      "offset": 7839.88,
      "duration": 5.279
    },
    {
      "lang": "en",
      "text": "models so um it's worth understanding",
      "offset": 7842.079,
      "duration": 6.401
    },
    {
      "lang": "en",
      "text": "this stage um that said I will say that",
      "offset": 7845.159,
      "duration": 5.161
    },
    {
      "lang": "en",
      "text": "eternal glory goes to anyone who can get",
      "offset": 7848.48,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "rid of it uh I showed you one possible",
      "offset": 7850.32,
      "duration": 4.359
    },
    {
      "lang": "en",
      "text": "paper that tried to uh do that and I",
      "offset": 7852.559,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "think I hope a lot more can follow over",
      "offset": 7854.679,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "time and my final recommendations for",
      "offset": 7857.04,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "the application right now are if you can",
      "offset": 7859.4,
      "duration": 3.64
    },
    {
      "lang": "en",
      "text": "reuse the GPT 4 tokens and the",
      "offset": 7861.44,
      "duration": 3.56
    },
    {
      "lang": "en",
      "text": "vocabulary uh in your application then",
      "offset": 7863.04,
      "duration": 3.159
    },
    {
      "lang": "en",
      "text": "that's something you should consider and",
      "offset": 7865,
      "duration": 2.84
    },
    {
      "lang": "en",
      "text": "just use Tech token because it is very",
      "offset": 7866.199,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "efficient and nice library for inference",
      "offset": 7867.84,
      "duration": 5.879
    },
    {
      "lang": "en",
      "text": "for bpe I also really like the bite",
      "offset": 7871.239,
      "duration": 6.081
    },
    {
      "lang": "en",
      "text": "level BP that uh Tik toen and openi uses",
      "offset": 7873.719,
      "duration": 5.321
    },
    {
      "lang": "en",
      "text": "uh if you for some reason want to train",
      "offset": 7877.32,
      "duration": 5.359
    },
    {
      "lang": "en",
      "text": "your own vocabulary from scratch um then",
      "offset": 7879.04,
      "duration": 5.96
    },
    {
      "lang": "en",
      "text": "I would use uh the bpe with sentence",
      "offset": 7882.679,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "piece um oops as I mentioned I'm not a",
      "offset": 7885,
      "duration": 5.679
    },
    {
      "lang": "en",
      "text": "huge fan of sentence piece I don't like",
      "offset": 7888.119,
      "duration": 5.801
    },
    {
      "lang": "en",
      "text": "its uh bite fallback and I don't like",
      "offset": 7890.679,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "that it's doing BP on unic code code",
      "offset": 7893.92,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "points I think it's uh it also has like",
      "offset": 7895.559,
      "duration": 3.56
    },
    {
      "lang": "en",
      "text": "a million settings and I think there's a",
      "offset": 7897.76,
      "duration": 2.64
    },
    {
      "lang": "en",
      "text": "lot of foot gonss here and I think it's",
      "offset": 7899.119,
      "duration": 3.08
    },
    {
      "lang": "en",
      "text": "really easy to Mis calibrate them and",
      "offset": 7900.4,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "you end up cropping your sentences or",
      "offset": 7902.199,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "something like that uh because of some",
      "offset": 7903.76,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "type of parameter that you don't fully",
      "offset": 7905.8,
      "duration": 3.64
    },
    {
      "lang": "en",
      "text": "understand so so be very careful with",
      "offset": 7907.28,
      "duration": 4.439
    },
    {
      "lang": "en",
      "text": "the settings try to copy paste exactly",
      "offset": 7909.44,
      "duration": 4.84
    },
    {
      "lang": "en",
      "text": "maybe where what meta did or basically",
      "offset": 7911.719,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "spend a lot of time looking at all the",
      "offset": 7914.28,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "hyper parameters and go through the code",
      "offset": 7916.119,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "of sentence piece and make sure that you",
      "offset": 7917.48,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "have this correct um but even if you",
      "offset": 7919.079,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "have all the settings correct I still",
      "offset": 7922.04,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "think that the algorithm is kind of",
      "offset": 7923.48,
      "duration": 4.199
    },
    {
      "lang": "en",
      "text": "inferior to what's happening here and",
      "offset": 7924.92,
      "duration": 4.6
    },
    {
      "lang": "en",
      "text": "maybe the best if you really need to",
      "offset": 7927.679,
      "duration": 3.641
    },
    {
      "lang": "en",
      "text": "train your vocabulary maybe the best",
      "offset": 7929.52,
      "duration": 3.639
    },
    {
      "lang": "en",
      "text": "thing is to just wait for M bpe to",
      "offset": 7931.32,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "becomes as efficient as possible and uh",
      "offset": 7933.159,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "that's something that maybe I hope to",
      "offset": 7936.84,
      "duration": 3.96
    },
    {
      "lang": "en",
      "text": "work on and at some point maybe we can",
      "offset": 7938.159,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "be training basically really what we",
      "offset": 7940.8,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "want is we want tick token but training",
      "offset": 7942.88,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "code and that is the ideal thing that",
      "offset": 7944.96,
      "duration": 6.4
    },
    {
      "lang": "en",
      "text": "currently does not exist and MBP is um",
      "offset": 7947.84,
      "duration": 5.399
    },
    {
      "lang": "en",
      "text": "is in implementation of it but currently",
      "offset": 7951.36,
      "duration": 4.52
    },
    {
      "lang": "en",
      "text": "it's in Python so that's currently what",
      "offset": 7953.239,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "I have to say for uh tokenization there",
      "offset": 7955.88,
      "duration": 4.52
    },
    {
      "lang": "en",
      "text": "might be an advanced video that has even",
      "offset": 7958.199,
      "duration": 3.721
    },
    {
      "lang": "en",
      "text": "drier and even more detailed in the",
      "offset": 7960.4,
      "duration": 3.239
    },
    {
      "lang": "en",
      "text": "future but for now I think we're going",
      "offset": 7961.92,
      "duration": 4.84
    },
    {
      "lang": "en",
      "text": "to leave things off here and uh I hope",
      "offset": 7963.639,
      "duration": 6.401
    },
    {
      "lang": "en",
      "text": "that was helpful bye",
      "offset": 7966.76,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "and uh they increase this contact size",
      "offset": 7974.119,
      "duration": 8.56
    },
    {
      "lang": "en",
      "text": "from gpt1 of 512 uh to 1024 and GPT 4",
      "offset": 7976.04,
      "duration": 9.4
    },
    {
      "lang": "en",
      "text": "two the",
      "offset": 7982.679,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "next okay next I would like us to",
      "offset": 7985.44,
      "duration": 4.36
    },
    {
      "lang": "en",
      "text": "briefly walk through the code from open",
      "offset": 7987.639,
      "duration": 5.721
    },
    {
      "lang": "en",
      "text": "AI on the gpt2 encoded",
      "offset": 7989.8,
      "duration": 3.56
    },
    {
      "lang": "en",
      "text": "ATP I'm sorry I'm gonna sneeze",
      "offset": 7995.84,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "and then what's Happening Here",
      "offset": 7999.119,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "is this is a spous layer that I will",
      "offset": 8001.84,
      "duration": 4.279
    },
    {
      "lang": "en",
      "text": "explain in a",
      "offset": 8004.639,
      "duration": 5.48
    },
    {
      "lang": "en",
      "text": "bit What's Happening Here",
      "offset": 8006.119,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "is",
      "offset": 8013.159,
      "duration": 3
    }
  ],
  "cleanText": null,
  "dumpedAt": "2025-07-21T18:43:24.972Z"
}