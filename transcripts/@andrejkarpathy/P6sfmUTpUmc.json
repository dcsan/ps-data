{
  "episodeId": "P6sfmUTpUmc",
  "channelSlug": "@andrejkarpathy",
  "title": "Building makemore Part 3: Activations & Gradients, BatchNorm",
  "publishedAt": "2022-10-04T16:41:03.000Z",
  "rawLines": [
    {
      "lang": "en",
      "text": "hi everyone today we are continuing our",
      "offset": 0.04,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "implementation of make more now in the",
      "offset": 2.32,
      "duration": 3.8
    },
    {
      "lang": "en",
      "text": "last lecture we implemented the multier",
      "offset": 4.52,
      "duration": 4.28
    },
    {
      "lang": "en",
      "text": "perceptron along the lines of benj 2003",
      "offset": 6.12,
      "duration": 4.68
    },
    {
      "lang": "en",
      "text": "for character level language modeling so",
      "offset": 8.8,
      "duration": 3.879
    },
    {
      "lang": "en",
      "text": "we followed this paper took in a few",
      "offset": 10.8,
      "duration": 4.039
    },
    {
      "lang": "en",
      "text": "characters in the past and used an MLP",
      "offset": 12.679,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "to predict the next character in a",
      "offset": 14.839,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "sequence so what we'd like to do now is",
      "offset": 16.279,
      "duration": 3.881
    },
    {
      "lang": "en",
      "text": "we'd like to move on to more complex and",
      "offset": 18.359,
      "duration": 3.641
    },
    {
      "lang": "en",
      "text": "larger neural networks like recurrent",
      "offset": 20.16,
      "duration": 3.48
    },
    {
      "lang": "en",
      "text": "neural networks and their variations",
      "offset": 22,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "like the grw lstm and so on now before",
      "offset": 23.64,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "we do that though we have to stick",
      "offset": 27,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "around the level of malalia perception",
      "offset": 28.279,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "on for a bit longer and I'd like to do",
      "offset": 30,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "this because I would like us to have a",
      "offset": 32.279,
      "duration": 3.561
    },
    {
      "lang": "en",
      "text": "very good intuitive understanding of the",
      "offset": 33.6,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "activations in the neural net during",
      "offset": 35.84,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "training and especially the gradients",
      "offset": 37.6,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "that are flowing backwards and how they",
      "offset": 39.52,
      "duration": 4.12
    },
    {
      "lang": "en",
      "text": "behave and what they look like and this",
      "offset": 41.44,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "is going to be very important to",
      "offset": 43.64,
      "duration": 2.36
    },
    {
      "lang": "en",
      "text": "understand the history of the",
      "offset": 45.12,
      "duration": 2.959
    },
    {
      "lang": "en",
      "text": "development of these architectures",
      "offset": 46,
      "duration": 3.32
    },
    {
      "lang": "en",
      "text": "because we'll see that recurr neural",
      "offset": 48.079,
      "duration": 3.881
    },
    {
      "lang": "en",
      "text": "networks while they are very expressive",
      "offset": 49.32,
      "duration": 3.8
    },
    {
      "lang": "en",
      "text": "in that they are a universal",
      "offset": 51.96,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "approximator and can in principle",
      "offset": 53.12,
      "duration": 4.919
    },
    {
      "lang": "en",
      "text": "Implement uh all the algorithms uh we'll",
      "offset": 54.68,
      "duration": 4.679
    },
    {
      "lang": "en",
      "text": "see that they are not very easily",
      "offset": 58.039,
      "duration": 3.16
    },
    {
      "lang": "en",
      "text": "optimizable with the first order",
      "offset": 59.359,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "gradient based techniques that we have",
      "offset": 61.199,
      "duration": 2.601
    },
    {
      "lang": "en",
      "text": "available to us and that we use all the",
      "offset": 62.399,
      "duration": 4.04
    },
    {
      "lang": "en",
      "text": "time and the key to understanding why",
      "offset": 63.8,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "they are not optimizable easily is to",
      "offset": 66.439,
      "duration": 4.201
    },
    {
      "lang": "en",
      "text": "understand the the activations and the",
      "offset": 69.24,
      "duration": 2.68
    },
    {
      "lang": "en",
      "text": "gradients and how they behave during",
      "offset": 70.64,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "training and we'll see that a lot of the",
      "offset": 71.92,
      "duration": 4.28
    },
    {
      "lang": "en",
      "text": "variants since recur neural networks",
      "offset": 73.64,
      "duration": 5.72
    },
    {
      "lang": "en",
      "text": "have tried to improve that situation and",
      "offset": 76.2,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "so that's the path that we have to take",
      "offset": 79.36,
      "duration": 4.04
    },
    {
      "lang": "en",
      "text": "and uh let's get started so the starting",
      "offset": 81.64,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "code for this lecture is largely the",
      "offset": 83.4,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "code from before but I've cleaned it up",
      "offset": 85.64,
      "duration": 3.519
    },
    {
      "lang": "en",
      "text": "a little bit so you'll see that we are",
      "offset": 87.159,
      "duration": 3.481
    },
    {
      "lang": "en",
      "text": "importing",
      "offset": 89.159,
      "duration": 4.28
    },
    {
      "lang": "en",
      "text": "all the torch and math plb utilities",
      "offset": 90.64,
      "duration": 4.2
    },
    {
      "lang": "en",
      "text": "we're reading in the words just like",
      "offset": 93.439,
      "duration": 3.72
    },
    {
      "lang": "en",
      "text": "before these are eight example words",
      "offset": 94.84,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "there's a total of 32,000 of them here's",
      "offset": 97.159,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "a vocabulary of all the lowercase",
      "offset": 99.399,
      "duration": 5.121
    },
    {
      "lang": "en",
      "text": "letters and the special dot token here",
      "offset": 101.24,
      "duration": 5.479
    },
    {
      "lang": "en",
      "text": "we are reading the data set and",
      "offset": 104.52,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "processing it and um creating three",
      "offset": 106.719,
      "duration": 6.921
    },
    {
      "lang": "en",
      "text": "splits the train Dev and the test split",
      "offset": 109.96,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "now in MLP this is the identical same",
      "offset": 113.64,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "MLP except you see that I removed a",
      "offset": 115.64,
      "duration": 4.04
    },
    {
      "lang": "en",
      "text": "bunch of magic numbers that we had here",
      "offset": 117.799,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "and instead we have the dimensionality",
      "offset": 119.68,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "of the embedding space of the characters",
      "offset": 121.56,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "and the number of hidden units in the",
      "offset": 123.52,
      "duration": 3.32
    },
    {
      "lang": "en",
      "text": "hidden layer and so I've pulled them",
      "offset": 125.24,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "outside here uh so that we don't have to",
      "offset": 126.84,
      "duration": 3.559
    },
    {
      "lang": "en",
      "text": "go and change all these magic numbers",
      "offset": 129,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "all the time we have the same neural net",
      "offset": 130.399,
      "duration": 4.521
    },
    {
      "lang": "en",
      "text": "with 11,000 parameters that we optimize",
      "offset": 132.599,
      "duration": 4.601
    },
    {
      "lang": "en",
      "text": "now over 200,000 steps with a batch size",
      "offset": 134.92,
      "duration": 5.319
    },
    {
      "lang": "en",
      "text": "of 32 and you'll see that I refactor I",
      "offset": 137.2,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "refactored the code here a little bit",
      "offset": 140.239,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "but there are no functional changes I",
      "offset": 142.16,
      "duration": 3.88
    },
    {
      "lang": "en",
      "text": "just created a few extra variables a few",
      "offset": 143.68,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "more comments and I removed all the",
      "offset": 146.04,
      "duration": 4.6
    },
    {
      "lang": "en",
      "text": "magic numbers and otherwise is the exact",
      "offset": 148.08,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "same thing then when we optimize we saw",
      "offset": 150.64,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "that our loss looked something like this",
      "offset": 153.44,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "we saw that the train and Val loss were",
      "offset": 156,
      "duration": 3.08
    },
    {
      "lang": "en",
      "text": "about",
      "offset": 158.08,
      "duration": 5.519
    },
    {
      "lang": "en",
      "text": "2.16 and so on here I refactored the uh",
      "offset": 159.08,
      "duration": 6.6
    },
    {
      "lang": "en",
      "text": "code a little bit for the evaluation of",
      "offset": 163.599,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "arbitary splits so you pass in a string",
      "offset": 165.68,
      "duration": 4.44
    },
    {
      "lang": "en",
      "text": "of which split you'd like to evaluate",
      "offset": 168.159,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "and then here depending on train Val or",
      "offset": 170.12,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "test I index in and I get the correct",
      "offset": 172.48,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "split and then this is the forward pass",
      "offset": 174.76,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "of the network and evaluation of the",
      "offset": 176.64,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "loss and printing it so just making that",
      "offset": 178.12,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "nicer uh one thing that you'll notice",
      "offset": 181.76,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "here is I'm using a decorator torch.",
      "offset": 184.04,
      "duration": 5.72
    },
    {
      "lang": "en",
      "text": "nograd which you can also um look up and",
      "offset": 186.959,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "read the documentation of basically what",
      "offset": 189.76,
      "duration": 4.68
    },
    {
      "lang": "en",
      "text": "this decorator does on top of a function",
      "offset": 191.92,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "is that whatever happens in this",
      "offset": 194.44,
      "duration": 5.879
    },
    {
      "lang": "en",
      "text": "function is assumed by uh torch to never",
      "offset": 196.64,
      "duration": 6.239
    },
    {
      "lang": "en",
      "text": "require any gradients so it will not do",
      "offset": 200.319,
      "duration": 4.92
    },
    {
      "lang": "en",
      "text": "any of the bookkeeping that it does to",
      "offset": 202.879,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "keep track of all the gradients in",
      "offset": 205.239,
      "duration": 3.401
    },
    {
      "lang": "en",
      "text": "anticipation of an eventual backward",
      "offset": 206.879,
      "duration": 3.64
    },
    {
      "lang": "en",
      "text": "pass it's it's almost as if all the",
      "offset": 208.64,
      "duration": 3.519
    },
    {
      "lang": "en",
      "text": "tensors that get created here have a",
      "offset": 210.519,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "required grad of false and so it just",
      "offset": 212.159,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "makes everything much more efficient",
      "offset": 215.08,
      "duration": 2.68
    },
    {
      "lang": "en",
      "text": "because you're telling torch that I will",
      "offset": 216.4,
      "duration": 3.479
    },
    {
      "lang": "en",
      "text": "not call that backward on any of this",
      "offset": 217.76,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "computation and you don't need to",
      "offset": 219.879,
      "duration": 4.201
    },
    {
      "lang": "en",
      "text": "maintain the graph under the hood so",
      "offset": 221.12,
      "duration": 5.119
    },
    {
      "lang": "en",
      "text": "that's what this does and you can also",
      "offset": 224.08,
      "duration": 5.32
    },
    {
      "lang": "en",
      "text": "use a context manager uh with torch du",
      "offset": 226.239,
      "duration": 5.761
    },
    {
      "lang": "en",
      "text": "nograd and you can look those",
      "offset": 229.4,
      "duration": 4.919
    },
    {
      "lang": "en",
      "text": "up then here we have the sampling from a",
      "offset": 232,
      "duration": 5.4
    },
    {
      "lang": "en",
      "text": "model um just as before just a for",
      "offset": 234.319,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "Passive neural nut getting the",
      "offset": 237.4,
      "duration": 3.72
    },
    {
      "lang": "en",
      "text": "distribution sent from it adjusting the",
      "offset": 238.64,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "context window and repeating until we",
      "offset": 241.12,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "get the special end token and we see",
      "offset": 243.12,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "that we are starting to get much nicer",
      "offset": 245.36,
      "duration": 4.68
    },
    {
      "lang": "en",
      "text": "looking words simple from the model it's",
      "offset": 247.439,
      "duration": 4.36
    },
    {
      "lang": "en",
      "text": "still not amazing and they're still not",
      "offset": 250.04,
      "duration": 4.119
    },
    {
      "lang": "en",
      "text": "fully name like uh but it's much better",
      "offset": 251.799,
      "duration": 4.921
    },
    {
      "lang": "en",
      "text": "than what we had with the BAM",
      "offset": 254.159,
      "duration": 5.241
    },
    {
      "lang": "en",
      "text": "model so that's our starting point now",
      "offset": 256.72,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "the first thing I would like to",
      "offset": 259.4,
      "duration": 2.239
    },
    {
      "lang": "en",
      "text": "scrutinize is the",
      "offset": 260.16,
      "duration": 3.88
    },
    {
      "lang": "en",
      "text": "initialization I can tell that our",
      "offset": 261.639,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "network is very improperly configured at",
      "offset": 264.04,
      "duration": 4.28
    },
    {
      "lang": "en",
      "text": "initialization and there's multiple",
      "offset": 266.6,
      "duration": 2.84
    },
    {
      "lang": "en",
      "text": "things wrong with it but let's just",
      "offset": 268.32,
      "duration": 3.319
    },
    {
      "lang": "en",
      "text": "start with the first one look here on",
      "offset": 269.44,
      "duration": 4.44
    },
    {
      "lang": "en",
      "text": "the zeroth iteration the very first",
      "offset": 271.639,
      "duration": 5.441
    },
    {
      "lang": "en",
      "text": "iteration we are recording a loss of 27",
      "offset": 273.88,
      "duration": 4.84
    },
    {
      "lang": "en",
      "text": "and this rapidly comes down to roughly",
      "offset": 277.08,
      "duration": 3.88
    },
    {
      "lang": "en",
      "text": "one or two or so so I can tell that the",
      "offset": 278.72,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "initialization is all messed up because",
      "offset": 280.96,
      "duration": 3.88
    },
    {
      "lang": "en",
      "text": "this is way too high in training of",
      "offset": 282.479,
      "duration": 4.361
    },
    {
      "lang": "en",
      "text": "neural Nets it is almost always the case",
      "offset": 284.84,
      "duration": 3.56
    },
    {
      "lang": "en",
      "text": "that you will have a rough idea for what",
      "offset": 286.84,
      "duration": 4.28
    },
    {
      "lang": "en",
      "text": "loss to expect at initialization and",
      "offset": 288.4,
      "duration": 4.44
    },
    {
      "lang": "en",
      "text": "that just depends on the loss function",
      "offset": 291.12,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "and the problem setup in this case I do",
      "offset": 292.84,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "not expect 27 I expect a much lower",
      "offset": 295.759,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "number and we can calculate it together",
      "offset": 297.96,
      "duration": 5.519
    },
    {
      "lang": "en",
      "text": "basically at initialization what we like",
      "offset": 300.639,
      "duration": 5.601
    },
    {
      "lang": "en",
      "text": "is that um there's 27 characters that",
      "offset": 303.479,
      "duration": 4.601
    },
    {
      "lang": "en",
      "text": "could come next for any one training",
      "offset": 306.24,
      "duration": 4.28
    },
    {
      "lang": "en",
      "text": "example at initialization we have no",
      "offset": 308.08,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "reason to believe any characters to be",
      "offset": 310.52,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "much more likely than others and so we'd",
      "offset": 311.919,
      "duration": 4.041
    },
    {
      "lang": "en",
      "text": "expect that the propy distribution that",
      "offset": 314.44,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "comes out initially is a uniform",
      "offset": 315.96,
      "duration": 4.359
    },
    {
      "lang": "en",
      "text": "distribution assigning about equal",
      "offset": 318.36,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "probability to all the 27",
      "offset": 320.319,
      "duration": 4.761
    },
    {
      "lang": "en",
      "text": "characters so basically what we' like is",
      "offset": 322.44,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "the probability for any character would",
      "offset": 325.08,
      "duration": 5.88
    },
    {
      "lang": "en",
      "text": "be roughly 1 over 20",
      "offset": 327.639,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "7 that is the probability we should",
      "offset": 330.96,
      "duration": 4.28
    },
    {
      "lang": "en",
      "text": "record and then the loss is the negative",
      "offset": 332.919,
      "duration": 5.761
    },
    {
      "lang": "en",
      "text": "log probability so let's wrap this in a",
      "offset": 335.24,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "tensor and then then we can take the log",
      "offset": 338.68,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "of it and then the negative log",
      "offset": 341.24,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "probability is the loss we would expect",
      "offset": 343.319,
      "duration": 6.561
    },
    {
      "lang": "en",
      "text": "which is 3.29 much much lower than 27",
      "offset": 346.039,
      "duration": 5.561
    },
    {
      "lang": "en",
      "text": "and so what's happening right now is",
      "offset": 349.88,
      "duration": 3.64
    },
    {
      "lang": "en",
      "text": "that at initialization the neural nut is",
      "offset": 351.6,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "creating probity distributions that are",
      "offset": 353.52,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "all messed up some characters are very",
      "offset": 355.36,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "confident and some characters are very",
      "offset": 357.68,
      "duration": 3.4
    },
    {
      "lang": "en",
      "text": "not confident confident and then",
      "offset": 359.199,
      "duration": 3.041
    },
    {
      "lang": "en",
      "text": "basically what's happening is that the",
      "offset": 361.08,
      "duration": 5.48
    },
    {
      "lang": "en",
      "text": "network is very confidently wrong and uh",
      "offset": 362.24,
      "duration": 7.679
    },
    {
      "lang": "en",
      "text": "that that's what makes it um record very",
      "offset": 366.56,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "high loss so here's a smaller",
      "offset": 369.919,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "four-dimensional example of the issue",
      "offset": 371.36,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "let's say we only have four characters",
      "offset": 373.36,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "and then we have logits that come out of",
      "offset": 376.08,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "the neural net and they are very very",
      "offset": 377.919,
      "duration": 3.641
    },
    {
      "lang": "en",
      "text": "close to zero then when we take the",
      "offset": 379.36,
      "duration": 4.76
    },
    {
      "lang": "en",
      "text": "softmax of all zeros we get",
      "offset": 381.56,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "probabilities there are a diffused",
      "offset": 384.12,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "distribution so sums to one and is",
      "offset": 386.52,
      "duration": 3.64
    },
    {
      "lang": "en",
      "text": "exactly",
      "offset": 389.24,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "uniform and then in this case if the",
      "offset": 390.16,
      "duration": 4.36
    },
    {
      "lang": "en",
      "text": "label is say two it doesn't actually",
      "offset": 392.36,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "matter if this if the label is two or",
      "offset": 394.52,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "three or one or zero because it's a",
      "offset": 396.759,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "uniform distribution we're recording the",
      "offset": 398.68,
      "duration": 4.519
    },
    {
      "lang": "en",
      "text": "exact same loss in this case 1.38 so",
      "offset": 400.52,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "this is the loss we would expect for a",
      "offset": 403.199,
      "duration": 3.4
    },
    {
      "lang": "en",
      "text": "four-dimensional example and now you can",
      "offset": 404.599,
      "duration": 3.521
    },
    {
      "lang": "en",
      "text": "see of course that as we start to",
      "offset": 406.599,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "manipulate these logits uh we're going",
      "offset": 408.12,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "to be changing the law here so it could",
      "offset": 410.759,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "be that we lock out and by chance uh",
      "offset": 413,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "this could be a very high number like",
      "offset": 415.759,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "you know five or something like that",
      "offset": 417.479,
      "duration": 3.56
    },
    {
      "lang": "en",
      "text": "then case we'll record a very low loss",
      "offset": 419.199,
      "duration": 2.961
    },
    {
      "lang": "en",
      "text": "because we're assigning the correct",
      "offset": 421.039,
      "duration": 3.481
    },
    {
      "lang": "en",
      "text": "probability at initialization by chance",
      "offset": 422.16,
      "duration": 5.599
    },
    {
      "lang": "en",
      "text": "to the correct label much more likely it",
      "offset": 424.52,
      "duration": 7.119
    },
    {
      "lang": "en",
      "text": "is that some other dimension will have a",
      "offset": 427.759,
      "duration": 6.921
    },
    {
      "lang": "en",
      "text": "high uh logit and then what will happen",
      "offset": 431.639,
      "duration": 5.481
    },
    {
      "lang": "en",
      "text": "is we start to record much higher loss",
      "offset": 434.68,
      "duration": 3.959
    },
    {
      "lang": "en",
      "text": "and what can come what can happen is",
      "offset": 437.12,
      "duration": 3.4
    },
    {
      "lang": "en",
      "text": "basically the logits come out like",
      "offset": 438.639,
      "duration": 3.801
    },
    {
      "lang": "en",
      "text": "something like this you know and they",
      "offset": 440.52,
      "duration": 4.84
    },
    {
      "lang": "en",
      "text": "take on Extreme values and we record",
      "offset": 442.44,
      "duration": 5.08
    },
    {
      "lang": "en",
      "text": "really high loss",
      "offset": 445.36,
      "duration": 5.279
    },
    {
      "lang": "en",
      "text": "um for example if we have to 4. random",
      "offset": 447.52,
      "duration": 6.72
    },
    {
      "lang": "en",
      "text": "of four so these are uniform um sorry",
      "offset": 450.639,
      "duration": 6.601
    },
    {
      "lang": "en",
      "text": "these are normally distributed um",
      "offset": 454.24,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "numbers uh four of",
      "offset": 457.24,
      "duration": 5.56
    },
    {
      "lang": "en",
      "text": "them then here we can also print the",
      "offset": 459.44,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "logits probabilities that come out of it",
      "offset": 462.8,
      "duration": 5.679
    },
    {
      "lang": "en",
      "text": "and the loss and so because these logits",
      "offset": 465.36,
      "duration": 5.839
    },
    {
      "lang": "en",
      "text": "are near zero for the most part the loss",
      "offset": 468.479,
      "duration": 6.481
    },
    {
      "lang": "en",
      "text": "that comes out is is okay uh but suppose",
      "offset": 471.199,
      "duration": 6.56
    },
    {
      "lang": "en",
      "text": "this is like times 10",
      "offset": 474.96,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "now you see how because these are more",
      "offset": 477.759,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "extreme values it's very unlikely that",
      "offset": 480.4,
      "duration": 4.919
    },
    {
      "lang": "en",
      "text": "you're going to be guessing the correct",
      "offset": 482.759,
      "duration": 4.761
    },
    {
      "lang": "en",
      "text": "bucket and then you're confidently wrong",
      "offset": 485.319,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "and recording very high loss if your",
      "offset": 487.52,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "loes are coming out even more",
      "offset": 489.879,
      "duration": 5.16
    },
    {
      "lang": "en",
      "text": "extreme you might get extremely insane",
      "offset": 491.759,
      "duration": 5.72
    },
    {
      "lang": "en",
      "text": "losses like infinity even at",
      "offset": 495.039,
      "duration": 4.361
    },
    {
      "lang": "en",
      "text": "initialization",
      "offset": 497.479,
      "duration": 4.361
    },
    {
      "lang": "en",
      "text": "um so basically this is not good and we",
      "offset": 499.4,
      "duration": 6.12
    },
    {
      "lang": "en",
      "text": "want the loges to be roughly zero um",
      "offset": 501.84,
      "duration": 6.28
    },
    {
      "lang": "en",
      "text": "when the network is initialized in fact",
      "offset": 505.52,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "the lits can don't have to be just zero",
      "offset": 508.12,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "they just have to be equal so for",
      "offset": 510.08,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "example if all the logits are one then",
      "offset": 511.56,
      "duration": 4.2
    },
    {
      "lang": "en",
      "text": "because of the normalization inside the",
      "offset": 514.32,
      "duration": 4.199
    },
    {
      "lang": "en",
      "text": "softmax this will actually come out okay",
      "offset": 515.76,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "but by symmetry we don't want it to be",
      "offset": 518.519,
      "duration": 3.041
    },
    {
      "lang": "en",
      "text": "any arbitrary positive or negative",
      "offset": 520,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "number we just want it to be all zeros",
      "offset": 521.56,
      "duration": 3.959
    },
    {
      "lang": "en",
      "text": "and record the loss that we expect at",
      "offset": 523.599,
      "duration": 3.721
    },
    {
      "lang": "en",
      "text": "initialization so let's now concretely",
      "offset": 525.519,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "see where things go wrong in our example",
      "offset": 527.32,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "here we have the initialization let me",
      "offset": 529.76,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "reinitialize the neuronet and here let",
      "offset": 531.88,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "me break after the very first iteration",
      "offset": 534.08,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "so we only see the initial loss which is",
      "offset": 536.04,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "27",
      "offset": 538.64,
      "duration": 3.4
    },
    {
      "lang": "en",
      "text": "so that's way too high and intuitively",
      "offset": 540.12,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "now we can expect the variables involved",
      "offset": 542.04,
      "duration": 4.28
    },
    {
      "lang": "en",
      "text": "and we see that the logits here if we",
      "offset": 544.279,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "just print some of",
      "offset": 546.32,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "these if we just print the first row we",
      "offset": 548.519,
      "duration": 3.801
    },
    {
      "lang": "en",
      "text": "see that the Lo just take on quite",
      "offset": 551.04,
      "duration": 3.479
    },
    {
      "lang": "en",
      "text": "extreme values and that's what's",
      "offset": 552.32,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "creating the fake confidence in",
      "offset": 554.519,
      "duration": 6.241
    },
    {
      "lang": "en",
      "text": "incorrect answers and makes the loss um",
      "offset": 556.56,
      "duration": 6.279
    },
    {
      "lang": "en",
      "text": "get very very high so these loes should",
      "offset": 560.76,
      "duration": 5.079
    },
    {
      "lang": "en",
      "text": "be much much closer to zero so now let's",
      "offset": 562.839,
      "duration": 5.161
    },
    {
      "lang": "en",
      "text": "think through how we can achieve logits",
      "offset": 565.839,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "coming out of this neur not to be more",
      "offset": 568,
      "duration": 5.519
    },
    {
      "lang": "en",
      "text": "closer to zero you see here that loes",
      "offset": 570.56,
      "duration": 4.44
    },
    {
      "lang": "en",
      "text": "are calculated as the hidden states",
      "offset": 573.519,
      "duration": 5.081
    },
    {
      "lang": "en",
      "text": "multip by W2 plus B2 so first of all",
      "offset": 575,
      "duration": 5.839
    },
    {
      "lang": "en",
      "text": "currently we're initializing B2 as",
      "offset": 578.6,
      "duration": 6.64
    },
    {
      "lang": "en",
      "text": "random values uh of the right size but",
      "offset": 580.839,
      "duration": 6.081
    },
    {
      "lang": "en",
      "text": "because we want roughly zero we don't",
      "offset": 585.24,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "actually want to be adding a bias of",
      "offset": 586.92,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "random numbers so in fact I'm going to",
      "offset": 588.36,
      "duration": 4.12
    },
    {
      "lang": "en",
      "text": "add a times zero here to make sure that",
      "offset": 590.2,
      "duration": 6.28
    },
    {
      "lang": "en",
      "text": "B2 is just um basically zero at",
      "offset": 592.48,
      "duration": 6.44
    },
    {
      "lang": "en",
      "text": "initialization and second this is H",
      "offset": 596.48,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "multip by W2 so if we want logits to be",
      "offset": 598.92,
      "duration": 4.76
    },
    {
      "lang": "en",
      "text": "very very small then we would be",
      "offset": 601.519,
      "duration": 4.521
    },
    {
      "lang": "en",
      "text": "multiplying W2 and making that",
      "offset": 603.68,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "smaller so for example if we scale down",
      "offset": 606.04,
      "duration": 6.799
    },
    {
      "lang": "en",
      "text": "W2 by 0.1 all the elements then if I do",
      "offset": 608.48,
      "duration": 6.039
    },
    {
      "lang": "en",
      "text": "again just a very first iteration you",
      "offset": 612.839,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "see that we are getting much closer to",
      "offset": 614.519,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "what we expect so rough roughly what we",
      "offset": 616.2,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "want is about",
      "offset": 618.44,
      "duration": 2.839
    },
    {
      "lang": "en",
      "text": "3.29 this is",
      "offset": 619.48,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "4.2 I can make this maybe even",
      "offset": 621.279,
      "duration": 5.961
    },
    {
      "lang": "en",
      "text": "smaller 3.32 okay so we're getting",
      "offset": 624.519,
      "duration": 5.641
    },
    {
      "lang": "en",
      "text": "closer and closer now you're probably",
      "offset": 627.24,
      "duration": 5.96
    },
    {
      "lang": "en",
      "text": "wondering can we just set this to zero",
      "offset": 630.16,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "then we get of course exactly what we're",
      "offset": 633.2,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "looking for um at",
      "offset": 634.8,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "initialization and the reason I don't",
      "offset": 637.2,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "usually do this is because I'm I'm very",
      "offset": 639.44,
      "duration": 4.28
    },
    {
      "lang": "en",
      "text": "nervous and I'll show you in a second",
      "offset": 641.68,
      "duration": 4.52
    },
    {
      "lang": "en",
      "text": "why you don't want to be setting W's or",
      "offset": 643.72,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "weights of a neural nut exactly to zero",
      "offset": 646.2,
      "duration": 4.36
    },
    {
      "lang": "en",
      "text": "um you you usually want it to be small",
      "offset": 648.44,
      "duration": 5.079
    },
    {
      "lang": "en",
      "text": "numbers instead of exactly zero um for",
      "offset": 650.56,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "this output layer in this specific case",
      "offset": 653.519,
      "duration": 4.201
    },
    {
      "lang": "en",
      "text": "I think it would be fine but I'll show",
      "offset": 655.6,
      "duration": 3.32
    },
    {
      "lang": "en",
      "text": "you in a second where things go wrong",
      "offset": 657.72,
      "duration": 3.08
    },
    {
      "lang": "en",
      "text": "very quick quickly if you do that so",
      "offset": 658.92,
      "duration": 3.039
    },
    {
      "lang": "en",
      "text": "let's just go with",
      "offset": 660.8,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "0.01 in that case our loss is close",
      "offset": 661.959,
      "duration": 5.161
    },
    {
      "lang": "en",
      "text": "enough but has some entropy it's not",
      "offset": 664.48,
      "duration": 5.32
    },
    {
      "lang": "en",
      "text": "exactly zero it's got some little",
      "offset": 667.12,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "entropy and that's used for symmetry",
      "offset": 669.8,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "breaking as we'll see in a second the",
      "offset": 671.36,
      "duration": 4.039
    },
    {
      "lang": "en",
      "text": "logits are now coming out much closer to",
      "offset": 673.639,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "zero and everything is well and good so",
      "offset": 675.399,
      "duration": 6.88
    },
    {
      "lang": "en",
      "text": "if I just erase these and I now take",
      "offset": 678.44,
      "duration": 5.519
    },
    {
      "lang": "en",
      "text": "away the break",
      "offset": 682.279,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "statement we can run the optimization",
      "offset": 683.959,
      "duration": 4.841
    },
    {
      "lang": "en",
      "text": "with this new initialization and let's",
      "offset": 686.6,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "just see",
      "offset": 688.8,
      "duration": 4.52
    },
    {
      "lang": "en",
      "text": "what losses we record okay so I let it",
      "offset": 690.12,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "run and you see that we started off good",
      "offset": 693.32,
      "duration": 3.959
    },
    {
      "lang": "en",
      "text": "and then we came down a",
      "offset": 695.72,
      "duration": 4.6
    },
    {
      "lang": "en",
      "text": "bit the plot of the loss uh now doesn't",
      "offset": 697.279,
      "duration": 6.281
    },
    {
      "lang": "en",
      "text": "have this hockey shape appearance um",
      "offset": 700.32,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "because basically what's happening in",
      "offset": 703.56,
      "duration": 3.32
    },
    {
      "lang": "en",
      "text": "the hockey stick the very first few",
      "offset": 704.72,
      "duration": 3.88
    },
    {
      "lang": "en",
      "text": "iterations of the loss what's happening",
      "offset": 706.88,
      "duration": 3.72
    },
    {
      "lang": "en",
      "text": "during the optimization is the",
      "offset": 708.6,
      "duration": 3.72
    },
    {
      "lang": "en",
      "text": "optimization is just squashing down the",
      "offset": 710.6,
      "duration": 3.479
    },
    {
      "lang": "en",
      "text": "logits and then it's rearranging the",
      "offset": 712.32,
      "duration": 4.36
    },
    {
      "lang": "en",
      "text": "logits so basically we took away this",
      "offset": 714.079,
      "duration": 4.76
    },
    {
      "lang": "en",
      "text": "easy part of the loss function where",
      "offset": 716.68,
      "duration": 3.64
    },
    {
      "lang": "en",
      "text": "just the the weights were just being",
      "offset": 718.839,
      "duration": 4.601
    },
    {
      "lang": "en",
      "text": "shrunk down and so therefore we're we",
      "offset": 720.32,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "don't we don't get these easy gains in",
      "offset": 723.44,
      "duration": 3.079
    },
    {
      "lang": "en",
      "text": "the beginning and we're just getting",
      "offset": 724.959,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "some of the hard gains of training the",
      "offset": 726.519,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "actual neural nut and so there's no",
      "offset": 727.839,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "hockey stick appearance so good things",
      "offset": 729.519,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "are happening in that both number one",
      "offset": 732.72,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "losset initialization is what we expect",
      "offset": 734.48,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "and the the loss doesn't look like a",
      "offset": 737.12,
      "duration": 4.76
    },
    {
      "lang": "en",
      "text": "hockey stick and this is true for any",
      "offset": 739.519,
      "duration": 4.281
    },
    {
      "lang": "en",
      "text": "neuron that you might train um and",
      "offset": 741.88,
      "duration": 4.84
    },
    {
      "lang": "en",
      "text": "something to look out for and second the",
      "offset": 743.8,
      "duration": 4.839
    },
    {
      "lang": "en",
      "text": "loss that came out is actually quite a",
      "offset": 746.72,
      "duration": 4.04
    },
    {
      "lang": "en",
      "text": "bit improved unfortunately I erased what",
      "offset": 748.639,
      "duration": 5.281
    },
    {
      "lang": "en",
      "text": "we had here before I believe this was 2.",
      "offset": 750.76,
      "duration": 7
    },
    {
      "lang": "en",
      "text": "um2 and this was this was 2.16 so we get",
      "offset": 753.92,
      "duration": 6.359
    },
    {
      "lang": "en",
      "text": "a slightly improved result and the",
      "offset": 757.76,
      "duration": 4.199
    },
    {
      "lang": "en",
      "text": "reason for that is uh because we're",
      "offset": 760.279,
      "duration": 4.041
    },
    {
      "lang": "en",
      "text": "spending more Cycles more time",
      "offset": 761.959,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "optimizing the neuronet actually instead",
      "offset": 764.32,
      "duration": 4.959
    },
    {
      "lang": "en",
      "text": "of just uh spending the first several",
      "offset": 766.76,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "thousand iterations probably just",
      "offset": 769.279,
      "duration": 2.961
    },
    {
      "lang": "en",
      "text": "squashing down the",
      "offset": 770.76,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "weights because they are so way too high",
      "offset": 772.24,
      "duration": 4.52
    },
    {
      "lang": "en",
      "text": "in the beginning in the initialization",
      "offset": 774.68,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "so something to look out for and uh",
      "offset": 776.76,
      "duration": 3.879
    },
    {
      "lang": "en",
      "text": "that's number one now let's look at the",
      "offset": 778.839,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "second problem let me reinitialize our",
      "offset": 780.639,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "neural net and let me reintroduce The",
      "offset": 782.68,
      "duration": 5.08
    },
    {
      "lang": "en",
      "text": "Brak statement so we have a reasonable",
      "offset": 784.639,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "initial loss so even though everything",
      "offset": 787.76,
      "duration": 3.16
    },
    {
      "lang": "en",
      "text": "is looking good on the level of the loss",
      "offset": 789.36,
      "duration": 3.24
    },
    {
      "lang": "en",
      "text": "and we get something that we expect",
      "offset": 790.92,
      "duration": 3.64
    },
    {
      "lang": "en",
      "text": "there's still a deeper problem looking",
      "offset": 792.6,
      "duration": 3.88
    },
    {
      "lang": "en",
      "text": "inside this neural net and its",
      "offset": 794.56,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "initialization so the logits are now",
      "offset": 796.48,
      "duration": 5.24
    },
    {
      "lang": "en",
      "text": "okay the problem now is with the values",
      "offset": 799.12,
      "duration": 5.24
    },
    {
      "lang": "en",
      "text": "of H the activations of the Hidden",
      "offset": 801.72,
      "duration": 5.119
    },
    {
      "lang": "en",
      "text": "States now if we just visualize this",
      "offset": 804.36,
      "duration": 5.08
    },
    {
      "lang": "en",
      "text": "Vector sorry this tensor h it's kind of",
      "offset": 806.839,
      "duration": 4.521
    },
    {
      "lang": "en",
      "text": "hard to see but the problem here roughly",
      "offset": 809.44,
      "duration": 3.16
    },
    {
      "lang": "en",
      "text": "speaking is you see how many of the",
      "offset": 811.36,
      "duration": 4.76
    },
    {
      "lang": "en",
      "text": "elements are one or negative 1 now",
      "offset": 812.6,
      "duration": 6.56
    },
    {
      "lang": "en",
      "text": "recall that torch. 10 the 10 function is",
      "offset": 816.12,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "a squashing function it takes arbitrary",
      "offset": 819.16,
      "duration": 3.799
    },
    {
      "lang": "en",
      "text": "numbers and it squashes them into a",
      "offset": 821.32,
      "duration": 3.319
    },
    {
      "lang": "en",
      "text": "range of negative 1 and one and it does",
      "offset": 822.959,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "so smoothly so let's look at the",
      "offset": 824.639,
      "duration": 4.361
    },
    {
      "lang": "en",
      "text": "histogram of H to get a better idea of",
      "offset": 826.8,
      "duration": 4.2
    },
    {
      "lang": "en",
      "text": "the distribution of the values inside",
      "offset": 829,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "this tensor we can do this",
      "offset": 831,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "first well we can see that H is 32",
      "offset": 834.04,
      "duration": 5.96
    },
    {
      "lang": "en",
      "text": "examples and 200 activations in each",
      "offset": 837.48,
      "duration": 5.599
    },
    {
      "lang": "en",
      "text": "example we can view it as1 to stretch it",
      "offset": 840,
      "duration": 5.48
    },
    {
      "lang": "en",
      "text": "out into one large",
      "offset": 843.079,
      "duration": 5.801
    },
    {
      "lang": "en",
      "text": "vector and we can then call two list to",
      "offset": 845.48,
      "duration": 6.56
    },
    {
      "lang": "en",
      "text": "convert this into one large python list",
      "offset": 848.88,
      "duration": 6.199
    },
    {
      "lang": "en",
      "text": "of floats and then we can pass this into",
      "offset": 852.04,
      "duration": 6.52
    },
    {
      "lang": "en",
      "text": "PLT doist for histogram and we say we",
      "offset": 855.079,
      "duration": 6.401
    },
    {
      "lang": "en",
      "text": "want 50 bins and a semicolon to suppress",
      "offset": 858.56,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "a bunch of output we don't",
      "offset": 861.48,
      "duration": 4.359
    },
    {
      "lang": "en",
      "text": "want so we see this histogram and we see",
      "offset": 863.279,
      "duration": 5.161
    },
    {
      "lang": "en",
      "text": "that most the values by far take on",
      "offset": 865.839,
      "duration": 5.321
    },
    {
      "lang": "en",
      "text": "value of netive one and one so this 10 H",
      "offset": 868.44,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "is very very active and we can also look",
      "offset": 871.16,
      "duration": 7.119
    },
    {
      "lang": "en",
      "text": "at basically why that is we can look at",
      "offset": 874.92,
      "duration": 6.8
    },
    {
      "lang": "en",
      "text": "the pre activations that feed into the",
      "offset": 878.279,
      "duration": 5.841
    },
    {
      "lang": "en",
      "text": "10 and we can see that the distribution",
      "offset": 881.72,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "of the pre activations are is very very",
      "offset": 884.12,
      "duration": 5.24
    },
    {
      "lang": "en",
      "text": "broad these take numbers between -5 and",
      "offset": 886.759,
      "duration": 5.241
    },
    {
      "lang": "en",
      "text": "15 and that's why in a torure 10",
      "offset": 889.36,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "everything is being squashed and capped",
      "offset": 892,
      "duration": 3.72
    },
    {
      "lang": "en",
      "text": "to be in the range of negative 1 and one",
      "offset": 893.759,
      "duration": 3.481
    },
    {
      "lang": "en",
      "text": "and lots of numbers here take on very",
      "offset": 895.72,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "extreme values now if you are new to",
      "offset": 897.24,
      "duration": 4.68
    },
    {
      "lang": "en",
      "text": "neural networks you might not actually",
      "offset": 900.199,
      "duration": 3.88
    },
    {
      "lang": "en",
      "text": "see this as an issue but if you're well",
      "offset": 901.92,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "vered in the dark arts of back",
      "offset": 904.079,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "propagation and then having an intuitive",
      "offset": 905.6,
      "duration": 3.4
    },
    {
      "lang": "en",
      "text": "sense of how these gradients flow",
      "offset": 907.519,
      "duration": 3.521
    },
    {
      "lang": "en",
      "text": "through a neural net you are looking at",
      "offset": 909,
      "duration": 4.04
    },
    {
      "lang": "en",
      "text": "your distribution of 10h activations",
      "offset": 911.04,
      "duration": 4.52
    },
    {
      "lang": "en",
      "text": "here and you are sweating so let me show",
      "offset": 913.04,
      "duration": 4.2
    },
    {
      "lang": "en",
      "text": "you why we have to keep in mind that",
      "offset": 915.56,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "during back propagation just like we saw",
      "offset": 917.24,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "in microad we are doing backward passs",
      "offset": 918.92,
      "duration": 3.96
    },
    {
      "lang": "en",
      "text": "starting at the loss and flowing through",
      "offset": 921.16,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "the network backwards in particular",
      "offset": 922.88,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "we're going to back propagate through",
      "offset": 925.24,
      "duration": 2.48
    },
    {
      "lang": "en",
      "text": "this torch.",
      "offset": 926.48,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "10h and this layer here is made up of",
      "offset": 927.72,
      "duration": 5.08
    },
    {
      "lang": "en",
      "text": "200 neurons for each one of these",
      "offset": 930.8,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "examples and uh it implements an",
      "offset": 932.8,
      "duration": 4.519
    },
    {
      "lang": "en",
      "text": "elementwise 10 so let's look at what",
      "offset": 935.12,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "happens in 10h in the backward pass we",
      "offset": 937.319,
      "duration": 4.361
    },
    {
      "lang": "en",
      "text": "can actually go back to our previous uh",
      "offset": 939.839,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "microgr code in the very first lecture",
      "offset": 941.68,
      "duration": 5.519
    },
    {
      "lang": "en",
      "text": "and see how we implemented 10 AG we saw",
      "offset": 944.399,
      "duration": 5.161
    },
    {
      "lang": "en",
      "text": "that the input here was X and then we",
      "offset": 947.199,
      "duration": 5.32
    },
    {
      "lang": "en",
      "text": "calculate T which is the 10 age of X so",
      "offset": 949.56,
      "duration": 5.399
    },
    {
      "lang": "en",
      "text": "that's T and T is between 1 and 1 it's",
      "offset": 952.519,
      "duration": 4.361
    },
    {
      "lang": "en",
      "text": "the output of the 10 H and then in the",
      "offset": 954.959,
      "duration": 3.481
    },
    {
      "lang": "en",
      "text": "backward pass how do we back propagate",
      "offset": 956.88,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "through a 10 H we take out that grad um",
      "offset": 958.44,
      "duration": 5.8
    },
    {
      "lang": "en",
      "text": "and then we multiply it this is the",
      "offset": 962.8,
      "duration": 3.519
    },
    {
      "lang": "en",
      "text": "chain rule with the local gradient which",
      "offset": 964.24,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "took the form of 1 - t ^2 so what",
      "offset": 966.319,
      "duration": 5.281
    },
    {
      "lang": "en",
      "text": "happens if the outputs of your t h are",
      "offset": 969.44,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "very close to1 or 1 if you plug in t one",
      "offset": 971.6,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "here you're going to get a zero",
      "offset": 975.36,
      "duration": 4.76
    },
    {
      "lang": "en",
      "text": "multiplying out. grad no matter what",
      "offset": 977.44,
      "duration": 5.48
    },
    {
      "lang": "en",
      "text": "out. grad is we are killing the gradient",
      "offset": 980.12,
      "duration": 4.959
    },
    {
      "lang": "en",
      "text": "and we're stopping effectively the back",
      "offset": 982.92,
      "duration": 4.52
    },
    {
      "lang": "en",
      "text": "propagation through this 10 unit",
      "offset": 985.079,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "similarly when t is1 this will again",
      "offset": 987.44,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "become zero and out that grad just stops",
      "offset": 989.72,
      "duration": 5.559
    },
    {
      "lang": "en",
      "text": "and intuitively this makes sense because",
      "offset": 993.04,
      "duration": 3.64
    },
    {
      "lang": "en",
      "text": "this is a 10h",
      "offset": 995.279,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "neuron and what's happening is if its",
      "offset": 996.68,
      "duration": 4.92
    },
    {
      "lang": "en",
      "text": "output is very close to one then we are",
      "offset": 999.44,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "in the tail of this",
      "offset": 1001.6,
      "duration": 6.479
    },
    {
      "lang": "en",
      "text": "10 and so changing basically the",
      "offset": 1003.04,
      "duration": 7.719
    },
    {
      "lang": "en",
      "text": "input is not going to impact the output",
      "offset": 1008.079,
      "duration": 5.361
    },
    {
      "lang": "en",
      "text": "of the 10 too much because it's it's so",
      "offset": 1010.759,
      "duration": 5.241
    },
    {
      "lang": "en",
      "text": "it's in a flat region of the 10 H and so",
      "offset": 1013.44,
      "duration": 5.079
    },
    {
      "lang": "en",
      "text": "therefore there's no impact on the loss",
      "offset": 1016,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "and so so indeed the the weights and the",
      "offset": 1018.519,
      "duration": 5.8
    },
    {
      "lang": "en",
      "text": "biases along with the 10h neuron do not",
      "offset": 1022,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "impact the loss because the output of",
      "offset": 1024.319,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "the 10 unit is in the flat region of the",
      "offset": 1026.319,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "10 and there's no influence we can we",
      "offset": 1028.079,
      "duration": 4.36
    },
    {
      "lang": "en",
      "text": "can be changing them whatever we want",
      "offset": 1030.24,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "however we want and the loss is not",
      "offset": 1032.439,
      "duration": 3.081
    },
    {
      "lang": "en",
      "text": "impacted that's so that's another way to",
      "offset": 1033.679,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "justify that indeed the gradient would",
      "offset": 1035.52,
      "duration": 4.439
    },
    {
      "lang": "en",
      "text": "be basically zero it",
      "offset": 1037.839,
      "duration": 6.921
    },
    {
      "lang": "en",
      "text": "vanishes indeed uh when T equals zero we",
      "offset": 1039.959,
      "duration": 8.401
    },
    {
      "lang": "en",
      "text": "get one times out that grad so when the",
      "offset": 1044.76,
      "duration": 7.2
    },
    {
      "lang": "en",
      "text": "10 h takes on exactly value of zero then",
      "offset": 1048.36,
      "duration": 6.72
    },
    {
      "lang": "en",
      "text": "out grad is just passed through so",
      "offset": 1051.96,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "basically what this is doing right is if",
      "offset": 1055.08,
      "duration": 4.92
    },
    {
      "lang": "en",
      "text": "T is equal to zero then this the 10 unit",
      "offset": 1057,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "is uh sort of inactive and uh gradient",
      "offset": 1060,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "just passes through but the more you are",
      "offset": 1063.48,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "in the flat tails the more the gradient",
      "offset": 1065.6,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "is squashed so in fact you'll see that",
      "offset": 1068.039,
      "duration": 5.041
    },
    {
      "lang": "en",
      "text": "the the gradient flowing through 10 can",
      "offset": 1070.799,
      "duration": 4.521
    },
    {
      "lang": "en",
      "text": "only ever decrease and the amount that",
      "offset": 1073.08,
      "duration": 6.12
    },
    {
      "lang": "en",
      "text": "it decreases is um proportional through",
      "offset": 1075.32,
      "duration": 6.839
    },
    {
      "lang": "en",
      "text": "a square here um depending on how far",
      "offset": 1079.2,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "you are in the flat tail so this 10 H",
      "offset": 1082.159,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "and so that's kind of what's Happening",
      "offset": 1085.12,
      "duration": 4.84
    },
    {
      "lang": "en",
      "text": "Here and through this the concern here",
      "offset": 1086.48,
      "duration": 6.439
    },
    {
      "lang": "en",
      "text": "is that if all of these um outputs H are",
      "offset": 1089.96,
      "duration": 4.44
    },
    {
      "lang": "en",
      "text": "in the flat regions of negative 1 and",
      "offset": 1092.919,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "one then the gradients that are flowing",
      "offset": 1094.4,
      "duration": 3.8
    },
    {
      "lang": "en",
      "text": "through the network will just get",
      "offset": 1096.679,
      "duration": 3.48
    },
    {
      "lang": "en",
      "text": "destroyed at this",
      "offset": 1098.2,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "layer now there is some redeeming",
      "offset": 1100.159,
      "duration": 4.841
    },
    {
      "lang": "en",
      "text": "quality here and that we can actually",
      "offset": 1103,
      "duration": 3.72
    },
    {
      "lang": "en",
      "text": "get a sense of the problem here as",
      "offset": 1105,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "follows I wrote some code here and",
      "offset": 1106.72,
      "duration": 4.199
    },
    {
      "lang": "en",
      "text": "basically what we want to do here is we",
      "offset": 1109.32,
      "duration": 4.28
    },
    {
      "lang": "en",
      "text": "want to take a look at H take the the",
      "offset": 1110.919,
      "duration": 5.481
    },
    {
      "lang": "en",
      "text": "absolute value and see how often it is",
      "offset": 1113.6,
      "duration": 6.6
    },
    {
      "lang": "en",
      "text": "in the in a flat uh region so say",
      "offset": 1116.4,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "greater than",
      "offset": 1120.2,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "099 and what you get is the following",
      "offset": 1121.36,
      "duration": 5.679
    },
    {
      "lang": "en",
      "text": "and this is a Boolean tensor so uh in",
      "offset": 1124.36,
      "duration": 5.08
    },
    {
      "lang": "en",
      "text": "the Boolean tensor you get a white if",
      "offset": 1127.039,
      "duration": 4.441
    },
    {
      "lang": "en",
      "text": "this is true and a black if this is",
      "offset": 1129.44,
      "duration": 4.119
    },
    {
      "lang": "en",
      "text": "false and so basically what we have here",
      "offset": 1131.48,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "is the 32 examples and 200 hidden",
      "offset": 1133.559,
      "duration": 5.841
    },
    {
      "lang": "en",
      "text": "neurons and we see that a lot of this is",
      "offset": 1136.44,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "white and what that's telling us is that",
      "offset": 1139.4,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "all these 10h neurons were very very",
      "offset": 1141.72,
      "duration": 7.52
    },
    {
      "lang": "en",
      "text": "active and uh they're in a flat tail and",
      "offset": 1145.24,
      "duration": 7.4
    },
    {
      "lang": "en",
      "text": "so in all these cases uh the back the",
      "offset": 1149.24,
      "duration": 6.36
    },
    {
      "lang": "en",
      "text": "backward gradient would get uh",
      "offset": 1152.64,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "destroyed now we would be in a lot of",
      "offset": 1155.6,
      "duration": 5.88
    },
    {
      "lang": "en",
      "text": "trouble if for for any one of these 200",
      "offset": 1158.08,
      "duration": 6.04
    },
    {
      "lang": "en",
      "text": "neurons if it was the case that the",
      "offset": 1161.48,
      "duration": 5.319
    },
    {
      "lang": "en",
      "text": "entire column is white because in that",
      "offset": 1164.12,
      "duration": 4.6
    },
    {
      "lang": "en",
      "text": "case we have what's called a dead neuron",
      "offset": 1166.799,
      "duration": 3.401
    },
    {
      "lang": "en",
      "text": "and this is could be a 10 neuron where",
      "offset": 1168.72,
      "duration": 2.64
    },
    {
      "lang": "en",
      "text": "the initialization of the weights and",
      "offset": 1170.2,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "the biases could be such that no single",
      "offset": 1171.36,
      "duration": 5.96
    },
    {
      "lang": "en",
      "text": "example ever activates uh this 10h in",
      "offset": 1173.72,
      "duration": 6.199
    },
    {
      "lang": "en",
      "text": "the um sort of active part of the 10age",
      "offset": 1177.32,
      "duration": 5.64
    },
    {
      "lang": "en",
      "text": "if all the examples land in the tail",
      "offset": 1179.919,
      "duration": 5.041
    },
    {
      "lang": "en",
      "text": "then this neuron will never learn it is",
      "offset": 1182.96,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "a dead neuron and so just scrutinizing",
      "offset": 1184.96,
      "duration": 5.64
    },
    {
      "lang": "en",
      "text": "this and looking for Columns of",
      "offset": 1188.48,
      "duration": 4.6
    },
    {
      "lang": "en",
      "text": "completely white uh we see that this is",
      "offset": 1190.6,
      "duration": 5.959
    },
    {
      "lang": "en",
      "text": "not the case so uh I don't see a single",
      "offset": 1193.08,
      "duration": 6.36
    },
    {
      "lang": "en",
      "text": "neuron that is all of uh you know white",
      "offset": 1196.559,
      "duration": 4.521
    },
    {
      "lang": "en",
      "text": "and so therefore it is the case that for",
      "offset": 1199.44,
      "duration": 5.16
    },
    {
      "lang": "en",
      "text": "every one of these 10h neurons uh we do",
      "offset": 1201.08,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "have some examples that activate them in",
      "offset": 1204.6,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "the uh active part of the 10 and so some",
      "offset": 1207,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "gradients will flow through and this",
      "offset": 1209.48,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "neuron will learn and the neuron will",
      "offset": 1210.84,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "change and it will move and it will do",
      "offset": 1213.24,
      "duration": 4.439
    },
    {
      "lang": "en",
      "text": "something but you can sometimes get get",
      "offset": 1215.4,
      "duration": 4.04
    },
    {
      "lang": "en",
      "text": "yourself in cases where you have dead",
      "offset": 1217.679,
      "duration": 4.041
    },
    {
      "lang": "en",
      "text": "neurons and the way this manifests is",
      "offset": 1219.44,
      "duration": 5.08
    },
    {
      "lang": "en",
      "text": "that um for 10h neuron this would be",
      "offset": 1221.72,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "when no matter what inputs you plug in",
      "offset": 1224.52,
      "duration": 3.519
    },
    {
      "lang": "en",
      "text": "from your data set this 10h neuron",
      "offset": 1226.36,
      "duration": 2.799
    },
    {
      "lang": "en",
      "text": "always fir",
      "offset": 1228.039,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "completely one or completely negative",
      "offset": 1229.159,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "one and then it will just not learn",
      "offset": 1230.919,
      "duration": 3.88
    },
    {
      "lang": "en",
      "text": "because all the gradients will be just",
      "offset": 1233.48,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "zeroed out uh this is true not just for",
      "offset": 1234.799,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "10 but for a lot of other nonlinearities",
      "offset": 1237.48,
      "duration": 3.96
    },
    {
      "lang": "en",
      "text": "that people use in neural networks so we",
      "offset": 1239.44,
      "duration": 4.44
    },
    {
      "lang": "en",
      "text": "certainly used 10 a lot but sigmoid will",
      "offset": 1241.44,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "have the exact same issue because it is",
      "offset": 1243.88,
      "duration": 4.36
    },
    {
      "lang": "en",
      "text": "a squashing neuron and so the same will",
      "offset": 1245.76,
      "duration": 7.36
    },
    {
      "lang": "en",
      "text": "be true for sigmoid uh but um but um you",
      "offset": 1248.24,
      "duration": 7.52
    },
    {
      "lang": "en",
      "text": "know um basically the same will actually",
      "offset": 1253.12,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "apply to sigmoid the same will also",
      "offset": 1255.76,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "apply to reu",
      "offset": 1257.679,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "so reu has a completely flat region here",
      "offset": 1259.12,
      "duration": 5.88
    },
    {
      "lang": "en",
      "text": "below zero so if you have a reu neuron",
      "offset": 1262,
      "duration": 5.919
    },
    {
      "lang": "en",
      "text": "then it is a pass through um if it is",
      "offset": 1265,
      "duration": 4.76
    },
    {
      "lang": "en",
      "text": "positive and if it's if the",
      "offset": 1267.919,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "preactivation is negative it will just",
      "offset": 1269.76,
      "duration": 4.039
    },
    {
      "lang": "en",
      "text": "shut it off since the region here is",
      "offset": 1271.6,
      "duration": 4.439
    },
    {
      "lang": "en",
      "text": "completely flat then during back",
      "offset": 1273.799,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "propagation uh this would be exactly",
      "offset": 1276.039,
      "duration": 5.281
    },
    {
      "lang": "en",
      "text": "zeroing out the gradient um like all of",
      "offset": 1278.6,
      "duration": 4.04
    },
    {
      "lang": "en",
      "text": "the gradient would be set exactly to",
      "offset": 1281.32,
      "duration": 2.719
    },
    {
      "lang": "en",
      "text": "zero instead of just like a very very",
      "offset": 1282.64,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "small number depending on how positive",
      "offset": 1284.039,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "or negative T is and so you can get for",
      "offset": 1286.48,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "example a dead reu neuron and a dead reu",
      "offset": 1289.24,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "neuron would basically look like",
      "offset": 1292.64,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "basically what it is is if a neuron with",
      "offset": 1295.24,
      "duration": 6.52
    },
    {
      "lang": "en",
      "text": "a reu nonlinearity never activates so",
      "offset": 1297.6,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "for any examples that you plug in in the",
      "offset": 1301.76,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "data set it never turns on it's always",
      "offset": 1303.36,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "in this flat region then this re neuron",
      "offset": 1305.76,
      "duration": 5.399
    },
    {
      "lang": "en",
      "text": "is a dead neuron its weights and bias",
      "offset": 1308.32,
      "duration": 4.44
    },
    {
      "lang": "en",
      "text": "will never learn they will never get a",
      "offset": 1311.159,
      "duration": 3.561
    },
    {
      "lang": "en",
      "text": "gradient because the neuron never",
      "offset": 1312.76,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "activated and this can sometimes happen",
      "offset": 1314.72,
      "duration": 4.04
    },
    {
      "lang": "en",
      "text": "at initialization uh because the way and",
      "offset": 1316.6,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "a biases just make it so that by chance",
      "offset": 1318.76,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "some neurons are just forever dead but",
      "offset": 1320.6,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "it can also happen during optimization",
      "offset": 1322.919,
      "duration": 3.64
    },
    {
      "lang": "en",
      "text": "if you have like a too high of learning",
      "offset": 1324.919,
      "duration": 3.24
    },
    {
      "lang": "en",
      "text": "rate for example sometimes you have",
      "offset": 1326.559,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "these neurons that get too much of a",
      "offset": 1328.159,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "gradient and they get knocked out off",
      "offset": 1329.679,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "the data",
      "offset": 1331.76,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "manifold and what happens is that from",
      "offset": 1332.679,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "then on no example ever activates this",
      "offset": 1334.96,
      "duration": 3.959
    },
    {
      "lang": "en",
      "text": "neuron so this neuron remains dead",
      "offset": 1337.32,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "forever so it's kind of like a permanent",
      "offset": 1338.919,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "brain damage in a in a mind of a network",
      "offset": 1340.44,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "and so sometimes what can happen is if",
      "offset": 1343.88,
      "duration": 2.919
    },
    {
      "lang": "en",
      "text": "your learning rate is very high for",
      "offset": 1345.44,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "example and you have a neural net with",
      "offset": 1346.799,
      "duration": 3.961
    },
    {
      "lang": "en",
      "text": "neurons you train the neuron net and you",
      "offset": 1348.72,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "get some last loss but then actually",
      "offset": 1350.76,
      "duration": 4.519
    },
    {
      "lang": "en",
      "text": "what you do is you go through the entire",
      "offset": 1353.2,
      "duration": 5.4
    },
    {
      "lang": "en",
      "text": "training set and you forward um your",
      "offset": 1355.279,
      "duration": 5.561
    },
    {
      "lang": "en",
      "text": "examples and you can find neurons that",
      "offset": 1358.6,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "never activate they are dead neurons in",
      "offset": 1360.84,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "your network and so those neurons will",
      "offset": 1363.08,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "will never turn on and usually what",
      "offset": 1365.24,
      "duration": 3.24
    },
    {
      "lang": "en",
      "text": "happens is that during training these",
      "offset": 1367,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "Rel neurons are changing moving Etc and",
      "offset": 1368.48,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "then because of a high gradient",
      "offset": 1370.76,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "somewhere by chance they get knocked off",
      "offset": 1372,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "and then nothing ever activates them and",
      "offset": 1374.6,
      "duration": 4.52
    },
    {
      "lang": "en",
      "text": "from then on they are just dead uh so",
      "offset": 1376.48,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "that's kind of like a permanent brain",
      "offset": 1379.12,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "damage that can happen to some of these",
      "offset": 1380.32,
      "duration": 4.44
    },
    {
      "lang": "en",
      "text": "neurons these other nonlinearities like",
      "offset": 1382.24,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "leyu will not suffer from this issue as",
      "offset": 1384.76,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "much because you can see that it doesn't",
      "offset": 1386.88,
      "duration": 5.159
    },
    {
      "lang": "en",
      "text": "have flat Tails you'll almost always get",
      "offset": 1388.44,
      "duration": 6.52
    },
    {
      "lang": "en",
      "text": "gradients and uh elu is also fairly uh",
      "offset": 1392.039,
      "duration": 5.321
    },
    {
      "lang": "en",
      "text": "frequently used um it also might suffer",
      "offset": 1394.96,
      "duration": 4.36
    },
    {
      "lang": "en",
      "text": "from this issue because it has flat",
      "offset": 1397.36,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "parts so that's just something to be",
      "offset": 1399.32,
      "duration": 4.2
    },
    {
      "lang": "en",
      "text": "aware of and something to be concerned",
      "offset": 1401.76,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "about and in this case we have way too",
      "offset": 1403.52,
      "duration": 5.639
    },
    {
      "lang": "en",
      "text": "many um activations AG that take on",
      "offset": 1406.24,
      "duration": 5.559
    },
    {
      "lang": "en",
      "text": "Extreme values and because there's no",
      "offset": 1409.159,
      "duration": 5.161
    },
    {
      "lang": "en",
      "text": "column of white I think we will be okay",
      "offset": 1411.799,
      "duration": 3.961
    },
    {
      "lang": "en",
      "text": "and indeed the network optimizes and",
      "offset": 1414.32,
      "duration": 3.719
    },
    {
      "lang": "en",
      "text": "gives us a pretty decent loss but it's",
      "offset": 1415.76,
      "duration": 3.799
    },
    {
      "lang": "en",
      "text": "just not optimal and this is not",
      "offset": 1418.039,
      "duration": 3.321
    },
    {
      "lang": "en",
      "text": "something you want especially during",
      "offset": 1419.559,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "initialization and so basically what's",
      "offset": 1421.36,
      "duration": 4.439
    },
    {
      "lang": "en",
      "text": "happening is that uh this H",
      "offset": 1423.559,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "preactivation that's floating to 10 H",
      "offset": 1425.799,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "it's it's too extreme it's too large",
      "offset": 1428.679,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "it's creating very um it's creating a",
      "offset": 1431,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "distribution that is too saturated in",
      "offset": 1434.039,
      "duration": 3.64
    },
    {
      "lang": "en",
      "text": "both sides of the 10 H and it's not",
      "offset": 1435.96,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "something you want because it means that",
      "offset": 1437.679,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "there's less training uh for these",
      "offset": 1439.559,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "neurons because they update um less",
      "offset": 1441.679,
      "duration": 6.12
    },
    {
      "lang": "en",
      "text": "frequently so how do we fix this well H",
      "offset": 1444.919,
      "duration": 7.76
    },
    {
      "lang": "en",
      "text": "preactivation is MCAT which comes from C",
      "offset": 1447.799,
      "duration": 7.48
    },
    {
      "lang": "en",
      "text": "so these are uniform gsan but then it's",
      "offset": 1452.679,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "multiply by W1 plus B1 and H preact is",
      "offset": 1455.279,
      "duration": 5.4
    },
    {
      "lang": "en",
      "text": "too far off from zero and that's causing",
      "offset": 1458.679,
      "duration": 4.761
    },
    {
      "lang": "en",
      "text": "the issue so we want this reactivation",
      "offset": 1460.679,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "to be closer to zero very similar to",
      "offset": 1463.44,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "what we had with",
      "offset": 1465.32,
      "duration": 3.56
    },
    {
      "lang": "en",
      "text": "logits so here",
      "offset": 1466.44,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "we want actually something very very",
      "offset": 1468.88,
      "duration": 4.919
    },
    {
      "lang": "en",
      "text": "similar now it's okay to set the biases",
      "offset": 1470.52,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "to very small number we can either",
      "offset": 1473.799,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "multiply by 0 01 to get like a little",
      "offset": 1475.48,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "bit of entropy um I sometimes like to do",
      "offset": 1477.399,
      "duration": 6.561
    },
    {
      "lang": "en",
      "text": "that um just so that there's like a",
      "offset": 1480.6,
      "duration": 5.319
    },
    {
      "lang": "en",
      "text": "little bit of variation and diversity in",
      "offset": 1483.96,
      "duration": 4.52
    },
    {
      "lang": "en",
      "text": "the original initialization of these 10",
      "offset": 1485.919,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "H neurons and I find in practice that",
      "offset": 1488.48,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "that can help optimization a little bit",
      "offset": 1490.64,
      "duration": 4.919
    },
    {
      "lang": "en",
      "text": "and then the weights we can also just",
      "offset": 1493.84,
      "duration": 3.559
    },
    {
      "lang": "en",
      "text": "like squash so let's multiply everything",
      "offset": 1495.559,
      "duration": 3.641
    },
    {
      "lang": "en",
      "text": "by 0.1",
      "offset": 1497.399,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "let's rerun the first batch and now",
      "offset": 1499.2,
      "duration": 5.4
    },
    {
      "lang": "en",
      "text": "let's look at this and well first let's",
      "offset": 1501.88,
      "duration": 4.12
    },
    {
      "lang": "en",
      "text": "look",
      "offset": 1504.6,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "here you see now because we multiply dou",
      "offset": 1506,
      "duration": 5.159
    },
    {
      "lang": "en",
      "text": "by 0.1 we have a much better histogram",
      "offset": 1508.679,
      "duration": 3.801
    },
    {
      "lang": "en",
      "text": "and that's because the pre activations",
      "offset": 1511.159,
      "duration": 4.441
    },
    {
      "lang": "en",
      "text": "are now between 1.5 and 1.5 and this we",
      "offset": 1512.48,
      "duration": 6.799
    },
    {
      "lang": "en",
      "text": "expect much much less white okay there's",
      "offset": 1515.6,
      "duration": 7.079
    },
    {
      "lang": "en",
      "text": "no white so basically that's because",
      "offset": 1519.279,
      "duration": 5.481
    },
    {
      "lang": "en",
      "text": "there are no neurons that saturated",
      "offset": 1522.679,
      "duration": 5.401
    },
    {
      "lang": "en",
      "text": "above 99 in either direction so this",
      "offset": 1524.76,
      "duration": 7.2
    },
    {
      "lang": "en",
      "text": "actually a pretty decent place to be um",
      "offset": 1528.08,
      "duration": 7.56
    },
    {
      "lang": "en",
      "text": "maybe we can go up a little",
      "offset": 1531.96,
      "duration": 7.319
    },
    {
      "lang": "en",
      "text": "bit sorry am I am I changing W1 here so",
      "offset": 1535.64,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "maybe we can go to 0",
      "offset": 1539.279,
      "duration": 5.4
    },
    {
      "lang": "en",
      "text": "2 okay so maybe something like this is",
      "offset": 1541.32,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "is a nice distribution so maybe this is",
      "offset": 1544.679,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "what our initialization should be so let",
      "offset": 1547.08,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "me now",
      "offset": 1549.399,
      "duration": 3.081
    },
    {
      "lang": "en",
      "text": "erase",
      "offset": 1550.679,
      "duration": 5.041
    },
    {
      "lang": "en",
      "text": "these and let me starting with",
      "offset": 1552.48,
      "duration": 5.319
    },
    {
      "lang": "en",
      "text": "initialization let me run the full",
      "offset": 1555.72,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "optimization",
      "offset": 1557.799,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "without the break and uh let's see what",
      "offset": 1558.84,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "we get okay so the optimization finished",
      "offset": 1562.36,
      "duration": 4.6
    },
    {
      "lang": "en",
      "text": "and I re the loss and this is the result",
      "offset": 1564.76,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "that we get and then just as a reminder",
      "offset": 1566.96,
      "duration": 3.64
    },
    {
      "lang": "en",
      "text": "I put down all the losses that we saw",
      "offset": 1569.159,
      "duration": 4.041
    },
    {
      "lang": "en",
      "text": "previously in this lecture so we see",
      "offset": 1570.6,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "that we actually do get an improvement",
      "offset": 1573.2,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "here and just as a reminder we started",
      "offset": 1574.76,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "off with a validation loss of 2.17 when",
      "offset": 1576.799,
      "duration": 4.441
    },
    {
      "lang": "en",
      "text": "we started by fixing the softmax being",
      "offset": 1579,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "confidently wrong we came down to 2.13",
      "offset": 1581.24,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "and by fixing the 10h layer being way",
      "offset": 1584.039,
      "duration": 4.841
    },
    {
      "lang": "en",
      "text": "too saturated we came down to 2.10",
      "offset": 1585.64,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "and the reason this is happening of",
      "offset": 1588.88,
      "duration": 2.6
    },
    {
      "lang": "en",
      "text": "course is because our initialization is",
      "offset": 1590.12,
      "duration": 3.039
    },
    {
      "lang": "en",
      "text": "better and so we're spending more time",
      "offset": 1591.48,
      "duration": 5.439
    },
    {
      "lang": "en",
      "text": "doing productive training instead of um",
      "offset": 1593.159,
      "duration": 5.321
    },
    {
      "lang": "en",
      "text": "not very productive training because our",
      "offset": 1596.919,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "gradients are set to zero and uh we have",
      "offset": 1598.48,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "to learn very simple things like uh the",
      "offset": 1601.08,
      "duration": 3.719
    },
    {
      "lang": "en",
      "text": "overconfidence of the softmax in the",
      "offset": 1603.36,
      "duration": 3.16
    },
    {
      "lang": "en",
      "text": "beginning and we're spending Cycles just",
      "offset": 1604.799,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "like squashing down the weight Matrix so",
      "offset": 1606.52,
      "duration": 5.879
    },
    {
      "lang": "en",
      "text": "this is illustrating um basically",
      "offset": 1610,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "initialization and its impacts on",
      "offset": 1612.399,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "performance uh just by being aware of",
      "offset": 1614.64,
      "duration": 3.96
    },
    {
      "lang": "en",
      "text": "the internals of these neural net and",
      "offset": 1617.039,
      "duration": 4.281
    },
    {
      "lang": "en",
      "text": "their activations their gradients now",
      "offset": 1618.6,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "we're working with a very small Network",
      "offset": 1621.32,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "this is just one layer multi-layer",
      "offset": 1622.919,
      "duration": 4.201
    },
    {
      "lang": "en",
      "text": "perception so because the network is so",
      "offset": 1624.6,
      "duration": 4.199
    },
    {
      "lang": "en",
      "text": "shallow the optimization problem is",
      "offset": 1627.12,
      "duration": 4.36
    },
    {
      "lang": "en",
      "text": "actually quite easy and very forgiving",
      "offset": 1628.799,
      "duration": 4.12
    },
    {
      "lang": "en",
      "text": "so even though our initialization was",
      "offset": 1631.48,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "terrible the network still learned",
      "offset": 1632.919,
      "duration": 3.521
    },
    {
      "lang": "en",
      "text": "eventually it just got a bit worse",
      "offset": 1634.84,
      "duration": 4.28
    },
    {
      "lang": "en",
      "text": "result this is not the case in general",
      "offset": 1636.44,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "though once we actually start um working",
      "offset": 1639.12,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "with much deeper networks that have say",
      "offset": 1641.48,
      "duration": 4.76
    },
    {
      "lang": "en",
      "text": "50 layers uh things can get uh much more",
      "offset": 1643.279,
      "duration": 6.201
    },
    {
      "lang": "en",
      "text": "complicated and uh these problems stack",
      "offset": 1646.24,
      "duration": 6.52
    },
    {
      "lang": "en",
      "text": "up and so you can actually get into a",
      "offset": 1649.48,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "place where the network is basically not",
      "offset": 1652.76,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "training at all if your initialization",
      "offset": 1654.279,
      "duration": 4.201
    },
    {
      "lang": "en",
      "text": "is bad enough and the deeper your",
      "offset": 1655.88,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "network is and the more complex it is",
      "offset": 1658.48,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "the less forgiving it is to some of",
      "offset": 1659.88,
      "duration": 5.24
    },
    {
      "lang": "en",
      "text": "these errors and so um something to",
      "offset": 1661.84,
      "duration": 5.719
    },
    {
      "lang": "en",
      "text": "definitely be aware of and uh something",
      "offset": 1665.12,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "to scrutinize something to plot and",
      "offset": 1667.559,
      "duration": 6.081
    },
    {
      "lang": "en",
      "text": "something to be careful with and um yeah",
      "offset": 1669.6,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "okay so that's great that that worked",
      "offset": 1673.64,
      "duration": 3.72
    },
    {
      "lang": "en",
      "text": "for us but what we have here now is all",
      "offset": 1674.799,
      "duration": 4.6
    },
    {
      "lang": "en",
      "text": "these magic numbers like0 2 like where",
      "offset": 1677.36,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "do I come up with this and how am I",
      "offset": 1679.399,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "supposed to set these if I have a large",
      "offset": 1681.12,
      "duration": 4.2
    },
    {
      "lang": "en",
      "text": "neural net with lots and lots of layers",
      "offset": 1682.679,
      "duration": 4.36
    },
    {
      "lang": "en",
      "text": "and so obviously no one does this by",
      "offset": 1685.32,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "hand there's actually some relatively",
      "offset": 1687.039,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "principled ways of setting these scales",
      "offset": 1688.76,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "um that I would like to introduce to you",
      "offset": 1691.519,
      "duration": 4.121
    },
    {
      "lang": "en",
      "text": "now so let me paste some code here that",
      "offset": 1693,
      "duration": 4.2
    },
    {
      "lang": "en",
      "text": "I prepared just to motivate the",
      "offset": 1695.64,
      "duration": 2.84
    },
    {
      "lang": "en",
      "text": "discussion of",
      "offset": 1697.2,
      "duration": 3.88
    },
    {
      "lang": "en",
      "text": "this so what I'm doing here is we have",
      "offset": 1698.48,
      "duration": 5.439
    },
    {
      "lang": "en",
      "text": "some random input here x that is drawn",
      "offset": 1701.08,
      "duration": 6.079
    },
    {
      "lang": "en",
      "text": "from a gan and there's 1,000 examples",
      "offset": 1703.919,
      "duration": 4.841
    },
    {
      "lang": "en",
      "text": "that are 10 dimensional",
      "offset": 1707.159,
      "duration": 3.64
    },
    {
      "lang": "en",
      "text": "and then we have a waiting layer here",
      "offset": 1708.76,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "that is also initialized using caution",
      "offset": 1710.799,
      "duration": 5.281
    },
    {
      "lang": "en",
      "text": "just like we did here and we these",
      "offset": 1713.08,
      "duration": 4.839
    },
    {
      "lang": "en",
      "text": "neurons in the hidden layer look at 10",
      "offset": 1716.08,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "inputs and there are 200 neurons in this",
      "offset": 1717.919,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "hidden layer and then we have here just",
      "offset": 1720.399,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "like here um in this case the",
      "offset": 1723.039,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "multiplication X multip by W to get the",
      "offset": 1725.36,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "pre activations of these",
      "offset": 1727.679,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "neurons and basically the analysis here",
      "offset": 1729.84,
      "duration": 4.6
    },
    {
      "lang": "en",
      "text": "looks at okay suppose these are uniform",
      "offset": 1732.32,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "gion and these weights are uniform gion",
      "offset": 1734.44,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "if I do X W and we forget for now the",
      "offset": 1737.2,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "bias and the",
      "offset": 1740.36,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "nonlinearity then what is the mean and",
      "offset": 1742.32,
      "duration": 4.92
    },
    {
      "lang": "en",
      "text": "the standard deviation of these gions so",
      "offset": 1744.519,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "in the beginning here the input is uh",
      "offset": 1747.24,
      "duration": 4.52
    },
    {
      "lang": "en",
      "text": "just a normal Gan distribution mean zero",
      "offset": 1749.32,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "and the standard deviation is one and",
      "offset": 1751.76,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "the standard deviation again is just the",
      "offset": 1753.64,
      "duration": 3.879
    },
    {
      "lang": "en",
      "text": "measure of a spread of the",
      "offset": 1755.12,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "gion but then once we multiply here and",
      "offset": 1757.519,
      "duration": 6.241
    },
    {
      "lang": "en",
      "text": "we look at the um histogram of Y we see",
      "offset": 1759.919,
      "duration": 5.681
    },
    {
      "lang": "en",
      "text": "that the mean of course stays the same",
      "offset": 1763.76,
      "duration": 3.799
    },
    {
      "lang": "en",
      "text": "it's about zero because this is a",
      "offset": 1765.6,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "symmetric operation but we see here that",
      "offset": 1767.559,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "the standard deviation has expanded to",
      "offset": 1769.519,
      "duration": 4.201
    },
    {
      "lang": "en",
      "text": "three so the input standard deviation",
      "offset": 1771.64,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "was one but now we've grown to three and",
      "offset": 1773.72,
      "duration": 3.959
    },
    {
      "lang": "en",
      "text": "so what you're seeing in the histogram",
      "offset": 1776.44,
      "duration": 3.719
    },
    {
      "lang": "en",
      "text": "is that this Gan is",
      "offset": 1777.679,
      "duration": 5.72
    },
    {
      "lang": "en",
      "text": "expanding and so um we're expanding this",
      "offset": 1780.159,
      "duration": 6.36
    },
    {
      "lang": "en",
      "text": "Gan um from the input and we don't want",
      "offset": 1783.399,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "that we want most of the neural net to",
      "offset": 1786.519,
      "duration": 4.04
    },
    {
      "lang": "en",
      "text": "have relatively similar activations uh",
      "offset": 1788.12,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "so unit gion roughly throughout the",
      "offset": 1790.559,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "neural net and so the question is how do",
      "offset": 1792.76,
      "duration": 6.2
    },
    {
      "lang": "en",
      "text": "we scale these W's to preserve the uh um",
      "offset": 1794.88,
      "duration": 6.799
    },
    {
      "lang": "en",
      "text": "to preserve this distribution to uh",
      "offset": 1798.96,
      "duration": 3.88
    },
    {
      "lang": "en",
      "text": "remain",
      "offset": 1801.679,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "aan and so intuitively if I multiply",
      "offset": 1802.84,
      "duration": 6.319
    },
    {
      "lang": "en",
      "text": "here uh these elements of w by a larger",
      "offset": 1805.76,
      "duration": 5.279
    },
    {
      "lang": "en",
      "text": "number let's say by",
      "offset": 1809.159,
      "duration": 5.561
    },
    {
      "lang": "en",
      "text": "five then this gsan gross and gross in",
      "offset": 1811.039,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "standard deviation so now we're at 15 so",
      "offset": 1814.72,
      "duration": 4.36
    },
    {
      "lang": "en",
      "text": "basically these numbers here in the",
      "offset": 1817.519,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "output y take on more and more extreme",
      "offset": 1819.08,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "values but if we scale it down like .2",
      "offset": 1821.519,
      "duration": 6.681
    },
    {
      "lang": "en",
      "text": "then conversely this Gan is getting",
      "offset": 1825.32,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "smaller and smaller and it's shrinking",
      "offset": 1828.2,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "and you can see that the standard",
      "offset": 1831.08,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "deviation is 6 and so the question is",
      "offset": 1832.12,
      "duration": 5.48
    },
    {
      "lang": "en",
      "text": "what do I multiply by here to exactly",
      "offset": 1834.76,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "preserve the standard deviation to be",
      "offset": 1837.6,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "one and it turns out that the correct",
      "offset": 1839.96,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "answer mathematically when you work out",
      "offset": 1842,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "through the variance of uh this",
      "offset": 1843.48,
      "duration": 4.76
    },
    {
      "lang": "en",
      "text": "multiplication here is that you are",
      "offset": 1845.84,
      "duration": 5.559
    },
    {
      "lang": "en",
      "text": "supposed to divide by the square root of",
      "offset": 1848.24,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "the fan in the fan in is the basically",
      "offset": 1851.399,
      "duration": 6.601
    },
    {
      "lang": "en",
      "text": "the uh number of input elements here 10",
      "offset": 1854.48,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "so we are supposed to divide by 10",
      "offset": 1858,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "square root and this is one way to do",
      "offset": 1859.48,
      "duration": 3.88
    },
    {
      "lang": "en",
      "text": "the square root you raise it to a power",
      "offset": 1861.84,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "of 0. five that's the same as doing a",
      "offset": 1863.36,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "square root so when you divide by the um",
      "offset": 1865.84,
      "duration": 6.76
    },
    {
      "lang": "en",
      "text": "square root of 10 then we see that the",
      "offset": 1869.2,
      "duration": 6.719
    },
    {
      "lang": "en",
      "text": "output caution it has exactly standard",
      "offset": 1872.6,
      "duration": 6.199
    },
    {
      "lang": "en",
      "text": "deviation of one now unsurprisingly a",
      "offset": 1875.919,
      "duration": 5.161
    },
    {
      "lang": "en",
      "text": "number of papers have looked into how",
      "offset": 1878.799,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "but to best initialized neural networks",
      "offset": 1881.08,
      "duration": 3.64
    },
    {
      "lang": "en",
      "text": "and in the case of multilayer",
      "offset": 1883.519,
      "duration": 2.76
    },
    {
      "lang": "en",
      "text": "perceptrons we can have fairly deep",
      "offset": 1884.72,
      "duration": 3.559
    },
    {
      "lang": "en",
      "text": "networks that have these nonlinearity in",
      "offset": 1886.279,
      "duration": 3.961
    },
    {
      "lang": "en",
      "text": "between and we want to make sure that",
      "offset": 1888.279,
      "duration": 3.721
    },
    {
      "lang": "en",
      "text": "the activations are well behaved and",
      "offset": 1890.24,
      "duration": 3.799
    },
    {
      "lang": "en",
      "text": "they don't expand to infinity or Shrink",
      "offset": 1892,
      "duration": 3.88
    },
    {
      "lang": "en",
      "text": "all the way to zero and the question is",
      "offset": 1894.039,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "how do we initialize the weights so that",
      "offset": 1895.88,
      "duration": 3.039
    },
    {
      "lang": "en",
      "text": "these activations take on reasonable",
      "offset": 1897.399,
      "duration": 4.28
    },
    {
      "lang": "en",
      "text": "values throughout the network now one",
      "offset": 1898.919,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "paper that has studied this in quite a",
      "offset": 1901.679,
      "duration": 3.321
    },
    {
      "lang": "en",
      "text": "bit of detail that is often referenced",
      "offset": 1903,
      "duration": 4.279
    },
    {
      "lang": "en",
      "text": "is this paper by King hatal called",
      "offset": 1905,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "delving deep into rectifiers now in this",
      "offset": 1907.279,
      "duration": 3.961
    },
    {
      "lang": "en",
      "text": "case they actually study convolution",
      "offset": 1909.799,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "neur neurals and they study especially",
      "offset": 1911.24,
      "duration": 5.679
    },
    {
      "lang": "en",
      "text": "the reu nonlinearity and the p",
      "offset": 1914.279,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "nonlinearity instead of a 10h",
      "offset": 1916.919,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "nonlinearity but the analysis is very",
      "offset": 1918.84,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "similar and um basically what happens",
      "offset": 1920.919,
      "duration": 6.041
    },
    {
      "lang": "en",
      "text": "here is for them the the relu",
      "offset": 1924.2,
      "duration": 4.04
    },
    {
      "lang": "en",
      "text": "nonlinearity that they care about quite",
      "offset": 1926.96,
      "duration": 4.959
    },
    {
      "lang": "en",
      "text": "a bit here is a squashing function where",
      "offset": 1928.24,
      "duration": 6.039
    },
    {
      "lang": "en",
      "text": "all the negative numbers are simply",
      "offset": 1931.919,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "clamped to zero so the positive numbers",
      "offset": 1934.279,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "are pass through but everything negative",
      "offset": 1936.799,
      "duration": 5.041
    },
    {
      "lang": "en",
      "text": "is just set to zero and because uh you",
      "offset": 1938.679,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "are basically throwing away half of the",
      "offset": 1941.84,
      "duration": 4.12
    },
    {
      "lang": "en",
      "text": "distribution they find in their analysis",
      "offset": 1943.679,
      "duration": 4.761
    },
    {
      "lang": "en",
      "text": "of the forward activations in the neural",
      "offset": 1945.96,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "that you have to compensate for that",
      "offset": 1948.44,
      "duration": 2.719
    },
    {
      "lang": "en",
      "text": "with a",
      "offset": 1949.88,
      "duration": 4.919
    },
    {
      "lang": "en",
      "text": "gain and so here they find that",
      "offset": 1951.159,
      "duration": 5.681
    },
    {
      "lang": "en",
      "text": "basically when they initialize their",
      "offset": 1954.799,
      "duration": 3.48
    },
    {
      "lang": "en",
      "text": "weights they have to do it with a zero",
      "offset": 1956.84,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "mean Gan whose standard deviation is",
      "offset": 1958.279,
      "duration": 5.4
    },
    {
      "lang": "en",
      "text": "square &lt; TK of 2 over the Fanon what we",
      "offset": 1960.44,
      "duration": 5.88
    },
    {
      "lang": "en",
      "text": "have here is we are initializing gashin",
      "offset": 1963.679,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "with the square root of Fanon this NL",
      "offset": 1966.32,
      "duration": 6.04
    },
    {
      "lang": "en",
      "text": "here is the Fanon so what we have is",
      "offset": 1969.519,
      "duration": 6.201
    },
    {
      "lang": "en",
      "text": "sare root of one over the Fanon because",
      "offset": 1972.36,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "we have the division here",
      "offset": 1975.72,
      "duration": 4.36
    },
    {
      "lang": "en",
      "text": "now they have to add this factor of two",
      "offset": 1978.2,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "because of the reu which basically",
      "offset": 1980.08,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "discards half of the distribution and",
      "offset": 1982.039,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "clamps it at zero and so that's where",
      "offset": 1984.08,
      "duration": 4.28
    },
    {
      "lang": "en",
      "text": "you get an additional Factor now in",
      "offset": 1986.039,
      "duration": 4.681
    },
    {
      "lang": "en",
      "text": "addition to that this paper also studies",
      "offset": 1988.36,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "not just the uh sort of behavior of the",
      "offset": 1990.72,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "activations in the forward pass of the",
      "offset": 1992.919,
      "duration": 3.88
    },
    {
      "lang": "en",
      "text": "neural net but it also studies the back",
      "offset": 1994.32,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "propagation and we have to make sure",
      "offset": 1996.799,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "that the gradients also are well behaved",
      "offset": 1998.48,
      "duration": 5.439
    },
    {
      "lang": "en",
      "text": "and so um because ultimately they end up",
      "offset": 2001.039,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "updating our parameters and what they",
      "offset": 2003.919,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "find here through a lot of analysis that",
      "offset": 2006.24,
      "duration": 3.72
    },
    {
      "lang": "en",
      "text": "I invite you to read through but it's",
      "offset": 2008.399,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "not exactly approachable what they find",
      "offset": 2009.96,
      "duration": 5.24
    },
    {
      "lang": "en",
      "text": "is basically if you properly initialize",
      "offset": 2012.32,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "the forward pass the backward pass is",
      "offset": 2015.2,
      "duration": 4.839
    },
    {
      "lang": "en",
      "text": "also approximately initialized up to a",
      "offset": 2016.96,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "constant factor that has to do with the",
      "offset": 2020.039,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "size of the number of um hidden neurons",
      "offset": 2022.24,
      "duration": 5.799
    },
    {
      "lang": "en",
      "text": "in an early and a late",
      "offset": 2025.159,
      "duration": 5.441
    },
    {
      "lang": "en",
      "text": "layer and uh but basically they find",
      "offset": 2028.039,
      "duration": 4.12
    },
    {
      "lang": "en",
      "text": "empirically that this is not a choice",
      "offset": 2030.6,
      "duration": 4.199
    },
    {
      "lang": "en",
      "text": "that matters too much now this timing",
      "offset": 2032.159,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "initialization is also implemented in",
      "offset": 2034.799,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "pytorch so if you go to torch. and then.",
      "offset": 2037.159,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "init documentation you'll find climing",
      "offset": 2039.6,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "normal and in my opinion this is",
      "offset": 2041.48,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "probably the most common way of",
      "offset": 2043.519,
      "duration": 4.12
    },
    {
      "lang": "en",
      "text": "initializing neural networks now and it",
      "offset": 2045.08,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "takes a few keyword arguments here so",
      "offset": 2047.639,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "number one it wants to know the mode",
      "offset": 2049.96,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "would you like to normalize the",
      "offset": 2052.839,
      "duration": 2.481
    },
    {
      "lang": "en",
      "text": "activations or would you like to",
      "offset": 2054.04,
      "duration": 4.2
    },
    {
      "lang": "en",
      "text": "normalize the gradients to to be always",
      "offset": 2055.32,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "uh gsh in with zero mean and a unit or",
      "offset": 2058.24,
      "duration": 4.919
    },
    {
      "lang": "en",
      "text": "one standard deviation and because they",
      "offset": 2060.76,
      "duration": 3.56
    },
    {
      "lang": "en",
      "text": "find in the paper that this doesn't",
      "offset": 2063.159,
      "duration": 2.68
    },
    {
      "lang": "en",
      "text": "matter too much most of the people just",
      "offset": 2064.32,
      "duration": 3.96
    },
    {
      "lang": "en",
      "text": "leave it as the default which is Fan in",
      "offset": 2065.839,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "and then second passing the nonlinearity",
      "offset": 2068.28,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "that you are using because depending on",
      "offset": 2070.159,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "the nonlinearity we need to calculate a",
      "offset": 2072.2,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "slightly different gain and so if your",
      "offset": 2074.48,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "nonlinearity is just um linear so",
      "offset": 2076.44,
      "duration": 4.84
    },
    {
      "lang": "en",
      "text": "there's no nonlinearity then the gain",
      "offset": 2079.44,
      "duration": 3.719
    },
    {
      "lang": "en",
      "text": "here will be one and we have the exact",
      "offset": 2081.28,
      "duration": 3.879
    },
    {
      "lang": "en",
      "text": "same uh kind of formula that we've come",
      "offset": 2083.159,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "up here but if the nonlinearity is",
      "offset": 2085.159,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "something else we're going to get a",
      "offset": 2087.24,
      "duration": 3.119
    },
    {
      "lang": "en",
      "text": "slightly different gain and so if we",
      "offset": 2088.359,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "come up here to the top we see that for",
      "offset": 2090.359,
      "duration": 4.681
    },
    {
      "lang": "en",
      "text": "example in the case of reu this gain is",
      "offset": 2092.76,
      "duration": 4.04
    },
    {
      "lang": "en",
      "text": "a square root of two and the reason it's",
      "offset": 2095.04,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "a square root because in this",
      "offset": 2096.8,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "paper you see how the two is inside of",
      "offset": 2102.04,
      "duration": 5.559
    },
    {
      "lang": "en",
      "text": "the square root so the gain is a square",
      "offset": 2105.16,
      "duration": 5.88
    },
    {
      "lang": "en",
      "text": "root of two in the case of linear or",
      "offset": 2107.599,
      "duration": 6.401
    },
    {
      "lang": "en",
      "text": "identity we just get a gain of one in a",
      "offset": 2111.04,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "case of 10 H which is what we're using",
      "offset": 2114,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "here the advised gain is a 5 over3 and",
      "offset": 2115.599,
      "duration": 5.681
    },
    {
      "lang": "en",
      "text": "intuitively why do we need a gain on top",
      "offset": 2119,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "of the initialization is because 10 just",
      "offset": 2121.28,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "like reu is a contractive uh",
      "offset": 2123.8,
      "duration": 4.76
    },
    {
      "lang": "en",
      "text": "transformation so that means is you're",
      "offset": 2126.32,
      "duration": 3.799
    },
    {
      "lang": "en",
      "text": "taking the output distribution from this",
      "offset": 2128.56,
      "duration": 3.519
    },
    {
      "lang": "en",
      "text": "matrix multiplication and then you are",
      "offset": 2130.119,
      "duration": 4.041
    },
    {
      "lang": "en",
      "text": "squashing it in some way now reu",
      "offset": 2132.079,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "squashes it by taking everything below",
      "offset": 2134.16,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "zero and clamping it to zero 10 also",
      "offset": 2135.839,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "squashes it because it's a contractive",
      "offset": 2138.4,
      "duration": 3.64
    },
    {
      "lang": "en",
      "text": "operation it will take the Tails and it",
      "offset": 2140.16,
      "duration": 4.959
    },
    {
      "lang": "en",
      "text": "will squeeze them in and so in order to",
      "offset": 2142.04,
      "duration": 5.799
    },
    {
      "lang": "en",
      "text": "fight the squeezing in we need to boost",
      "offset": 2145.119,
      "duration": 4.281
    },
    {
      "lang": "en",
      "text": "the weights a little bit so that we",
      "offset": 2147.839,
      "duration": 3.641
    },
    {
      "lang": "en",
      "text": "renormalize everything back to standard",
      "offset": 2149.4,
      "duration": 5.08
    },
    {
      "lang": "en",
      "text": "unit standard deviation so that's why",
      "offset": 2151.48,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "there's a little bit of a gain that",
      "offset": 2154.48,
      "duration": 3.16
    },
    {
      "lang": "en",
      "text": "comes out now I'm skipping through this",
      "offset": 2155.48,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "section A little bit quickly and I'm",
      "offset": 2157.64,
      "duration": 3.4
    },
    {
      "lang": "en",
      "text": "doing that actually intentionally and",
      "offset": 2159,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "the reason for that is because about 7",
      "offset": 2161.04,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "years ago when this paper was written",
      "offset": 2163.72,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "you had to actually be extremely careful",
      "offset": 2166.04,
      "duration": 3.72
    },
    {
      "lang": "en",
      "text": "with the activations and ingredients and",
      "offset": 2167.8,
      "duration": 4.039
    },
    {
      "lang": "en",
      "text": "their ranges and their histograms and",
      "offset": 2169.76,
      "duration": 3.319
    },
    {
      "lang": "en",
      "text": "you had to be very careful with the",
      "offset": 2171.839,
      "duration": 2.641
    },
    {
      "lang": "en",
      "text": "precise setting of gains and the",
      "offset": 2173.079,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "scrutinizing of the nonlinearities used",
      "offset": 2174.48,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "and so on and everything was very",
      "offset": 2176.119,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "finicky and very fragile and to be very",
      "offset": 2178.16,
      "duration": 3.88
    },
    {
      "lang": "en",
      "text": "properly arranged for the neural nut to",
      "offset": 2180.28,
      "duration": 3.48
    },
    {
      "lang": "en",
      "text": "train especially if your neural nut was",
      "offset": 2182.04,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "very deep but there are a number of",
      "offset": 2183.76,
      "duration": 2.92
    },
    {
      "lang": "en",
      "text": "modern innovations that have made",
      "offset": 2185.48,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "everything significantly more stable and",
      "offset": 2186.68,
      "duration": 3.72
    },
    {
      "lang": "en",
      "text": "more well behaved and it's become less",
      "offset": 2188.44,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "important to initialize these networks",
      "offset": 2190.4,
      "duration": 4.36
    },
    {
      "lang": "en",
      "text": "exactly right and some of those modern",
      "offset": 2192.28,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "Innovations for example are residual",
      "offset": 2194.76,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "connections which we will cover in the",
      "offset": 2196.839,
      "duration": 4.361
    },
    {
      "lang": "en",
      "text": "future the use of a number of uh",
      "offset": 2198.2,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "normalization uh layers like for example",
      "offset": 2201.2,
      "duration": 4.68
    },
    {
      "lang": "en",
      "text": "batch normalization layer normalization",
      "offset": 2203.8,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "group normalization we're going to go",
      "offset": 2205.88,
      "duration": 3.4
    },
    {
      "lang": "en",
      "text": "into a lot of these as well and number",
      "offset": 2207.48,
      "duration": 3.639
    },
    {
      "lang": "en",
      "text": "three much better optimizers not just",
      "offset": 2209.28,
      "duration": 3.72
    },
    {
      "lang": "en",
      "text": "stochastic gradient descent the simple",
      "offset": 2211.119,
      "duration": 4.281
    },
    {
      "lang": "en",
      "text": "Optimizer we're basically using here but",
      "offset": 2213,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "a slightly more complex optimizers like",
      "offset": 2215.4,
      "duration": 4.6
    },
    {
      "lang": "en",
      "text": "ARS prop and especially Adam and so all",
      "offset": 2217.56,
      "duration": 4.44
    },
    {
      "lang": "en",
      "text": "of these modern Innovations make it less",
      "offset": 2220,
      "duration": 3.96
    },
    {
      "lang": "en",
      "text": "important for you to precisely calibrate",
      "offset": 2222,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "the neutralization of the neural net all",
      "offset": 2223.96,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "that being said in practice uh what",
      "offset": 2226.24,
      "duration": 4.2
    },
    {
      "lang": "en",
      "text": "should we do in practice when I",
      "offset": 2228.359,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "initialize these neurals I basically",
      "offset": 2230.44,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "just uh normalize my weights by the",
      "offset": 2232.44,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "square root of the Fanon uh so basically",
      "offset": 2234.04,
      "duration": 6.76
    },
    {
      "lang": "en",
      "text": "uh roughly what we did here is what I do",
      "offset": 2237.64,
      "duration": 4.76
    },
    {
      "lang": "en",
      "text": "now if we want to be exactly accurate",
      "offset": 2240.8,
      "duration": 6.12
    },
    {
      "lang": "en",
      "text": "here we and go by um in it of uh timing",
      "offset": 2242.4,
      "duration": 7.24
    },
    {
      "lang": "en",
      "text": "normal this is how it would implemented",
      "offset": 2246.92,
      "duration": 4.28
    },
    {
      "lang": "en",
      "text": "we want to set the standard deviation to",
      "offset": 2249.64,
      "duration": 4.92
    },
    {
      "lang": "en",
      "text": "be gain over the square root of fan in",
      "offset": 2251.2,
      "duration": 6.44
    },
    {
      "lang": "en",
      "text": "right so to set the standard deviation",
      "offset": 2254.56,
      "duration": 5.72
    },
    {
      "lang": "en",
      "text": "of our weights we will proceed as",
      "offset": 2257.64,
      "duration": 5.32
    },
    {
      "lang": "en",
      "text": "follows basically when we have a torch.",
      "offset": 2260.28,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "Ranon and let's say I just create a th",
      "offset": 2262.96,
      "duration": 3.72
    },
    {
      "lang": "en",
      "text": "numbers we can look at the standard",
      "offset": 2265.079,
      "duration": 3.24
    },
    {
      "lang": "en",
      "text": "deviation of this and of course that's",
      "offset": 2266.68,
      "duration": 3.48
    },
    {
      "lang": "en",
      "text": "one that's the amount of spread let's",
      "offset": 2268.319,
      "duration": 3.241
    },
    {
      "lang": "en",
      "text": "make this a bit bigger so it's closer to",
      "offset": 2270.16,
      "duration": 5.159
    },
    {
      "lang": "en",
      "text": "one so that's the spread of the Gan of",
      "offset": 2271.56,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "zero mean and unit standard deviation",
      "offset": 2275.319,
      "duration": 4.52
    },
    {
      "lang": "en",
      "text": "now basically when you take these and",
      "offset": 2278.04,
      "duration": 3.4
    },
    {
      "lang": "en",
      "text": "you multiply by",
      "offset": 2279.839,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "say2 that basically scales down the Gan",
      "offset": 2281.44,
      "duration": 5.56
    },
    {
      "lang": "en",
      "text": "and that makes it standard deviation 02",
      "offset": 2284.48,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "so basically the number that you",
      "offset": 2287,
      "duration": 2.599
    },
    {
      "lang": "en",
      "text": "multiply by here ends up being the",
      "offset": 2288.079,
      "duration": 4.361
    },
    {
      "lang": "en",
      "text": "standard deviation of this caution so",
      "offset": 2289.599,
      "duration": 5.641
    },
    {
      "lang": "en",
      "text": "here this is a um standard deviation",
      "offset": 2292.44,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "point2 caution here when we sample our",
      "offset": 2295.24,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "W1 but we want to set the standard",
      "offset": 2298.28,
      "duration": 5.16
    },
    {
      "lang": "en",
      "text": "deviation to gain over square root of",
      "offset": 2300.44,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "fan mode which is Fanon so in other",
      "offset": 2303.44,
      "duration": 6.32
    },
    {
      "lang": "en",
      "text": "words we want to mul mly by uh gain",
      "offset": 2306.52,
      "duration": 6.92
    },
    {
      "lang": "en",
      "text": "which for 10 H is 5",
      "offset": 2309.76,
      "duration": 8.4
    },
    {
      "lang": "en",
      "text": "over3 5 over3 is the gain and then",
      "offset": 2313.44,
      "duration": 7.72
    },
    {
      "lang": "en",
      "text": "times",
      "offset": 2318.16,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "um or I guess sorry",
      "offset": 2323.319,
      "duration": 8.361
    },
    {
      "lang": "en",
      "text": "divide uh square root of the fan in and",
      "offset": 2326.76,
      "duration": 7.079
    },
    {
      "lang": "en",
      "text": "in this example here the fan in was 10",
      "offset": 2331.68,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "and I just noticed that actually here",
      "offset": 2333.839,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "the fan in for W1 is is actually an",
      "offset": 2335.839,
      "duration": 4.361
    },
    {
      "lang": "en",
      "text": "embed times block size which as you all",
      "offset": 2338.079,
      "duration": 4.441
    },
    {
      "lang": "en",
      "text": "recall is actually 30 and that's because",
      "offset": 2340.2,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "each character is 10 dimensional but",
      "offset": 2342.52,
      "duration": 2.64
    },
    {
      "lang": "en",
      "text": "then we have three of them and we can",
      "offset": 2344.04,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "catenate them so actually the fan in",
      "offset": 2345.16,
      "duration": 3.88
    },
    {
      "lang": "en",
      "text": "here was 30 and I should have used 30",
      "offset": 2346.92,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "here probably but basically we want 30",
      "offset": 2349.04,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "uh square root so this is the number",
      "offset": 2351.96,
      "duration": 4.119
    },
    {
      "lang": "en",
      "text": "this is what our standard deviation we",
      "offset": 2354.56,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "want to be and this number turns out to",
      "offset": 2356.079,
      "duration": 5.121
    },
    {
      "lang": "en",
      "text": "be3 whereas here just by fiddling with",
      "offset": 2358.56,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "it and looking at the distribution and",
      "offset": 2361.2,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "making sure it looks okay uh we came up",
      "offset": 2362.56,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "with 02 and so instead what we want to",
      "offset": 2364.64,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "do here is we want to make the standard",
      "offset": 2367.119,
      "duration": 5.161
    },
    {
      "lang": "en",
      "text": "deviation b",
      "offset": 2369.44,
      "duration": 6.84
    },
    {
      "lang": "en",
      "text": "um 5 over3 which is our gain",
      "offset": 2372.28,
      "duration": 5.799
    },
    {
      "lang": "en",
      "text": "divide this",
      "offset": 2376.28,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "amount times2 square root and these",
      "offset": 2378.079,
      "duration": 6.161
    },
    {
      "lang": "en",
      "text": "brackets here are not that uh necessary",
      "offset": 2381.88,
      "duration": 4.28
    },
    {
      "lang": "en",
      "text": "but I'll just put them here for clarity",
      "offset": 2384.24,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "this is basically what we want this is",
      "offset": 2386.16,
      "duration": 5.08
    },
    {
      "lang": "en",
      "text": "the timing in it in our case for a 10h",
      "offset": 2387.76,
      "duration": 5.319
    },
    {
      "lang": "en",
      "text": "nonlinearity and this is how we would",
      "offset": 2391.24,
      "duration": 4.119
    },
    {
      "lang": "en",
      "text": "initialize the neural net and so we're",
      "offset": 2393.079,
      "duration": 5.841
    },
    {
      "lang": "en",
      "text": "multiplying by .3 instead of multiplying",
      "offset": 2395.359,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "by",
      "offset": 2398.92,
      "duration": 5.48
    },
    {
      "lang": "en",
      "text": ".2 and so we can we can initialize this",
      "offset": 2400.079,
      "duration": 6.561
    },
    {
      "lang": "en",
      "text": "way and then we can train the neural net",
      "offset": 2404.4,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "and see what we get okay so I trained",
      "offset": 2406.64,
      "duration": 4.6
    },
    {
      "lang": "en",
      "text": "the neural net and we end up in roughly",
      "offset": 2408.64,
      "duration": 4.12
    },
    {
      "lang": "en",
      "text": "the same spot so looking at the",
      "offset": 2411.24,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "validation loss we now get 2.10 and",
      "offset": 2412.76,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "previously we also had 2.10 there's a",
      "offset": 2415.319,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "little bit of a difference but that's",
      "offset": 2417.8,
      "duration": 2.68
    },
    {
      "lang": "en",
      "text": "just the randomness of the process I",
      "offset": 2419,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "suspect but the big deal of course is we",
      "offset": 2420.48,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "get to the same spot but we did not have",
      "offset": 2422.599,
      "duration": 6.52
    },
    {
      "lang": "en",
      "text": "to introduce any um magic numbers that",
      "offset": 2425.2,
      "duration": 5.919
    },
    {
      "lang": "en",
      "text": "we got from just looking at histograms",
      "offset": 2429.119,
      "duration": 3.801
    },
    {
      "lang": "en",
      "text": "and guessing checking we have something",
      "offset": 2431.119,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "that is semi- principled and will scale",
      "offset": 2432.92,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "us to uh much bigger networks and uh",
      "offset": 2434.72,
      "duration": 4.76
    },
    {
      "lang": "en",
      "text": "something that we can sort of use as a",
      "offset": 2437.72,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "guide so I mentioned that the precise",
      "offset": 2439.48,
      "duration": 4.2
    },
    {
      "lang": "en",
      "text": "setting of these initializations is not",
      "offset": 2441.24,
      "duration": 3.879
    },
    {
      "lang": "en",
      "text": "as important today due to some Modern",
      "offset": 2443.68,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "Innovations and I think now is a pretty",
      "offset": 2445.119,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "good time to introduce one of those",
      "offset": 2446.88,
      "duration": 3.239
    },
    {
      "lang": "en",
      "text": "modern Innovations and that is batch",
      "offset": 2448.079,
      "duration": 4.201
    },
    {
      "lang": "en",
      "text": "normalization so bat normalization came",
      "offset": 2450.119,
      "duration": 6.041
    },
    {
      "lang": "en",
      "text": "out in uh 2015 from a team at Google and",
      "offset": 2452.28,
      "duration": 6.319
    },
    {
      "lang": "en",
      "text": "it was an extremely impact paper because",
      "offset": 2456.16,
      "duration": 4.28
    },
    {
      "lang": "en",
      "text": "it made it possible to train very deep",
      "offset": 2458.599,
      "duration": 5.121
    },
    {
      "lang": "en",
      "text": "neuron Nets quite reliably and uh it",
      "offset": 2460.44,
      "duration": 5.159
    },
    {
      "lang": "en",
      "text": "basically just worked so here's what",
      "offset": 2463.72,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "bash rization does and let's implement",
      "offset": 2465.599,
      "duration": 3.321
    },
    {
      "lang": "en",
      "text": "it",
      "offset": 2467.319,
      "duration": 4.361
    },
    {
      "lang": "en",
      "text": "um basically we have these uh hidden",
      "offset": 2468.92,
      "duration": 5.159
    },
    {
      "lang": "en",
      "text": "States H preact right and we were",
      "offset": 2471.68,
      "duration": 4.52
    },
    {
      "lang": "en",
      "text": "talking about how we don't want these uh",
      "offset": 2474.079,
      "duration": 5.721
    },
    {
      "lang": "en",
      "text": "these um preactivation states to be way",
      "offset": 2476.2,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "too small because the then the 10 H is",
      "offset": 2479.8,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "not um doing anything but we don't want",
      "offset": 2481.88,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "them to be too large because then the 10",
      "offset": 2484.359,
      "duration": 3.96
    },
    {
      "lang": "en",
      "text": "H is saturated in fact we want them to",
      "offset": 2485.8,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "be roughly roughly Gan so zero mean and",
      "offset": 2488.319,
      "duration": 5.881
    },
    {
      "lang": "en",
      "text": "a unit or one standard deviation at",
      "offset": 2491.4,
      "duration": 3.719
    },
    {
      "lang": "en",
      "text": "least at",
      "offset": 2494.2,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "initialization so the Insight from the",
      "offset": 2495.119,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "bachor liation paper is okay you have",
      "offset": 2497.8,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "these hidden States and you'd like them",
      "offset": 2499.92,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "to be roughly Gan then why not take the",
      "offset": 2502.119,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "hidden States and uh just normalize them",
      "offset": 2504.72,
      "duration": 3.16
    },
    {
      "lang": "en",
      "text": "to be",
      "offset": 2507,
      "duration": 3.4
    },
    {
      "lang": "en",
      "text": "Gan and it sounds kind of crazy but you",
      "offset": 2507.88,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "can just do that because uh",
      "offset": 2510.4,
      "duration": 5.32
    },
    {
      "lang": "en",
      "text": "standardizing hidden States so that",
      "offset": 2513.4,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "their unit caution is a perfect ly",
      "offset": 2515.72,
      "duration": 3.16
    },
    {
      "lang": "en",
      "text": "differentiable operation as we'll soon",
      "offset": 2517.4,
      "duration": 3.32
    },
    {
      "lang": "en",
      "text": "see and so that was kind of like the big",
      "offset": 2518.88,
      "duration": 3.959
    },
    {
      "lang": "en",
      "text": "Insight in this paper and when I first",
      "offset": 2520.72,
      "duration": 4.04
    },
    {
      "lang": "en",
      "text": "read it my mind was blown because you",
      "offset": 2522.839,
      "duration": 3.401
    },
    {
      "lang": "en",
      "text": "can just normalize these hidden States",
      "offset": 2524.76,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "and if you'd like unit Gan States in",
      "offset": 2526.24,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "your network uh at least initialization",
      "offset": 2528.28,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "you can just normalize them to be unit",
      "offset": 2531.28,
      "duration": 5.24
    },
    {
      "lang": "en",
      "text": "gion so uh let's see how that works so",
      "offset": 2533.28,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "we're going to scroll to our",
      "offset": 2536.52,
      "duration": 2.799
    },
    {
      "lang": "en",
      "text": "preactivation here just before they",
      "offset": 2537.44,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "enter into the 10h now the idea again is",
      "offset": 2539.319,
      "duration": 4.201
    },
    {
      "lang": "en",
      "text": "remember we're trying to make these",
      "offset": 2542.319,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "roughly Gan and that's because if these",
      "offset": 2543.52,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "are way too small numbers then the 10 H",
      "offset": 2545.839,
      "duration": 5.121
    },
    {
      "lang": "en",
      "text": "here is kind of inactive but if these",
      "offset": 2548.4,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "are very large numbers then the 10 H is",
      "offset": 2550.96,
      "duration": 5.599
    },
    {
      "lang": "en",
      "text": "way too saturated and gr is no flow so",
      "offset": 2553.92,
      "duration": 5.399
    },
    {
      "lang": "en",
      "text": "we'd like this to be roughly goshan so",
      "offset": 2556.559,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "the Insight in Bat normalization again",
      "offset": 2559.319,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "is that we can just standardize these",
      "offset": 2561.52,
      "duration": 5.599
    },
    {
      "lang": "en",
      "text": "activations so they are exactly Gan so",
      "offset": 2563.559,
      "duration": 5.321
    },
    {
      "lang": "en",
      "text": "here H",
      "offset": 2567.119,
      "duration": 5.281
    },
    {
      "lang": "en",
      "text": "preact has a shapee of 32 by 200 32",
      "offset": 2568.88,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "examples by 200 neurons in the hidden",
      "offset": 2572.4,
      "duration": 4.84
    },
    {
      "lang": "en",
      "text": "layer so basically what we can do is we",
      "offset": 2574.96,
      "duration": 4.04
    },
    {
      "lang": "en",
      "text": "can take H pract and we can just",
      "offset": 2577.24,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "calculate the mean um and the mean we",
      "offset": 2579,
      "duration": 5.48
    },
    {
      "lang": "en",
      "text": "want to calculate across the zero",
      "offset": 2582.119,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "Dimension and we want to also keep them",
      "offset": 2584.48,
      "duration": 6.119
    },
    {
      "lang": "en",
      "text": "as true so that we can easily broadcast",
      "offset": 2586.92,
      "duration": 8
    },
    {
      "lang": "en",
      "text": "this so the shape of this is 1 by 200 in",
      "offset": 2590.599,
      "duration": 6.401
    },
    {
      "lang": "en",
      "text": "other words we are doing the mean over",
      "offset": 2594.92,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "all the uh elements in the",
      "offset": 2597,
      "duration": 5.8
    },
    {
      "lang": "en",
      "text": "batch and similarly we can calculate the",
      "offset": 2599.88,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "standard deviation of these",
      "offset": 2602.8,
      "duration": 5.64
    },
    {
      "lang": "en",
      "text": "activations and that will also be 1 by",
      "offset": 2605.88,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "200 now in this paper they have",
      "offset": 2608.44,
      "duration": 6.96
    },
    {
      "lang": "en",
      "text": "the uh sort of prescription here and see",
      "offset": 2611.4,
      "duration": 5.679
    },
    {
      "lang": "en",
      "text": "here we are calculating the mean which",
      "offset": 2615.4,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "is just taking uh the average",
      "offset": 2617.079,
      "duration": 7
    },
    {
      "lang": "en",
      "text": "value of any neurons activation and then",
      "offset": 2620.44,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "the standard deviation is basically kind",
      "offset": 2624.079,
      "duration": 4.361
    },
    {
      "lang": "en",
      "text": "of like um this the measure of the",
      "offset": 2625.8,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "spread that we've been using which is",
      "offset": 2628.44,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "the distance of every one of these",
      "offset": 2630.839,
      "duration": 5.441
    },
    {
      "lang": "en",
      "text": "values away from the mean and that",
      "offset": 2633.44,
      "duration": 4.36
    },
    {
      "lang": "en",
      "text": "squared and",
      "offset": 2636.28,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "averaged that's the that's the variance",
      "offset": 2637.8,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "and then if you want to take the",
      "offset": 2641.64,
      "duration": 2.84
    },
    {
      "lang": "en",
      "text": "standard deviation you would square root",
      "offset": 2642.52,
      "duration": 4.28
    },
    {
      "lang": "en",
      "text": "the variance to get the standard",
      "offset": 2644.48,
      "duration": 4.52
    },
    {
      "lang": "en",
      "text": "deviation so these are the two that",
      "offset": 2646.8,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "we're calculating and now we're going to",
      "offset": 2649,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "normalize or standardize these X's by",
      "offset": 2650.88,
      "duration": 5.16
    },
    {
      "lang": "en",
      "text": "subtracting the mean and um dividing by",
      "offset": 2653.319,
      "duration": 5.401
    },
    {
      "lang": "en",
      "text": "the standard deviation so basically",
      "offset": 2656.04,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "we're taking in pract and we",
      "offset": 2658.72,
      "duration": 7.599
    },
    {
      "lang": "en",
      "text": "subtract the mean",
      "offset": 2662.2,
      "duration": 4.119
    },
    {
      "lang": "en",
      "text": "and then we divide by the standard",
      "offset": 2669.599,
      "duration": 6.121
    },
    {
      "lang": "en",
      "text": "deviation this is exactly what these two",
      "offset": 2673.4,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "STD and mean are calculating",
      "offset": 2675.72,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "oops sorry this is the mean and this is",
      "offset": 2679.48,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "the variance you see how the sigma is a",
      "offset": 2682.2,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "standard deviation usually so this is",
      "offset": 2684.48,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "Sigma Square which the variance is the",
      "offset": 2685.8,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "square of the standard",
      "offset": 2687.76,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "deviation so this is how you standardize",
      "offset": 2689.88,
      "duration": 4.28
    },
    {
      "lang": "en",
      "text": "these values and what this will do is",
      "offset": 2692.24,
      "duration": 3.879
    },
    {
      "lang": "en",
      "text": "that every single neuron now and its",
      "offset": 2694.16,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "firing rate will be exactly unit Gan on",
      "offset": 2696.119,
      "duration": 5.601
    },
    {
      "lang": "en",
      "text": "these 32 examples at least of this batch",
      "offset": 2699.16,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "that's why it's called batch",
      "offset": 2701.72,
      "duration": 4.119
    },
    {
      "lang": "en",
      "text": "normalization we are normalizing these",
      "offset": 2702.44,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "batches and then we could in principle",
      "offset": 2705.839,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "train this notice that calculating the",
      "offset": 2708.68,
      "duration": 3.72
    },
    {
      "lang": "en",
      "text": "mean and your standard deviation these",
      "offset": 2710.72,
      "duration": 3.16
    },
    {
      "lang": "en",
      "text": "are just mathematical formulas they're",
      "offset": 2712.4,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "perfectly differentiable all of this is",
      "offset": 2713.88,
      "duration": 3.4
    },
    {
      "lang": "en",
      "text": "perfectly differentiable and we can just",
      "offset": 2715.68,
      "duration": 4.12
    },
    {
      "lang": "en",
      "text": "train this the problem is you actually",
      "offset": 2717.28,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "won't achieve a very good result with",
      "offset": 2719.8,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "this and the reason for that",
      "offset": 2722.4,
      "duration": 5.32
    },
    {
      "lang": "en",
      "text": "is we want these to be roughly Gan but",
      "offset": 2724.839,
      "duration": 5.601
    },
    {
      "lang": "en",
      "text": "only at initialization uh but we don't",
      "offset": 2727.72,
      "duration": 5.48
    },
    {
      "lang": "en",
      "text": "want these be to be forced to be Garian",
      "offset": 2730.44,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "always we we'd like to allow the neuron",
      "offset": 2733.2,
      "duration": 4.919
    },
    {
      "lang": "en",
      "text": "net to move this around to potentially",
      "offset": 2735.48,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "make it more diffuse to make it more",
      "offset": 2738.119,
      "duration": 4.121
    },
    {
      "lang": "en",
      "text": "sharp to make some 10 neurons maybe be",
      "offset": 2739.8,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "more trigger more trigger happy or less",
      "offset": 2742.24,
      "duration": 3.8
    },
    {
      "lang": "en",
      "text": "trigger happy so we'd like this",
      "offset": 2744.359,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "distribution to move around and we'd",
      "offset": 2746.04,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "like the back propagation to tell us how",
      "offset": 2747.8,
      "duration": 4.759
    },
    {
      "lang": "en",
      "text": "the distribution should move around and",
      "offset": 2749.8,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "so in addition to this idea of",
      "offset": 2752.559,
      "duration": 4.601
    },
    {
      "lang": "en",
      "text": "standardizing the activations that any",
      "offset": 2754.839,
      "duration": 5.24
    },
    {
      "lang": "en",
      "text": "point in the network uh we have to also",
      "offset": 2757.16,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "introduce this additional component in",
      "offset": 2760.079,
      "duration": 4.76
    },
    {
      "lang": "en",
      "text": "the paper here described as scale and",
      "offset": 2761.64,
      "duration": 5.16
    },
    {
      "lang": "en",
      "text": "shift and so basically what we're doing",
      "offset": 2764.839,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "is we're taking these normalized inputs",
      "offset": 2766.8,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "and we are additionally scaling them by",
      "offset": 2769.24,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "some gain and offsetting them by some",
      "offset": 2771.2,
      "duration": 5.639
    },
    {
      "lang": "en",
      "text": "bias to get our final output from this",
      "offset": 2773.48,
      "duration": 5.879
    },
    {
      "lang": "en",
      "text": "layer and so what that amounts to is the",
      "offset": 2776.839,
      "duration": 5.081
    },
    {
      "lang": "en",
      "text": "following we are going to allow a batch",
      "offset": 2779.359,
      "duration": 6.321
    },
    {
      "lang": "en",
      "text": "normalization gain to be initialized at",
      "offset": 2781.92,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "just uh once",
      "offset": 2785.68,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "and the ones will be in the shape of 1",
      "offset": 2787.52,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "by n",
      "offset": 2789.68,
      "duration": 4.6
    },
    {
      "lang": "en",
      "text": "hidden and then we also will have a BN",
      "offset": 2791.44,
      "duration": 6.52
    },
    {
      "lang": "en",
      "text": "bias which will be torch. zeros and it",
      "offset": 2794.28,
      "duration": 6.96
    },
    {
      "lang": "en",
      "text": "will also be of the shape n by 1 by n",
      "offset": 2797.96,
      "duration": 7.24
    },
    {
      "lang": "en",
      "text": "hidden and then here the BN gain will",
      "offset": 2801.24,
      "duration": 5.119
    },
    {
      "lang": "en",
      "text": "multiply",
      "offset": 2805.2,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "this and the BN bias will offset it",
      "offset": 2806.359,
      "duration": 6.321
    },
    {
      "lang": "en",
      "text": "here so because this is initialized to",
      "offset": 2810.2,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "one and this to",
      "offset": 2812.68,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "zero at initialization each neurons",
      "offset": 2813.88,
      "duration": 6.56
    },
    {
      "lang": "en",
      "text": "firing values in this batch will be",
      "offset": 2817.68,
      "duration": 5.08
    },
    {
      "lang": "en",
      "text": "exactly unit gion and will have nice",
      "offset": 2820.44,
      "duration": 4.28
    },
    {
      "lang": "en",
      "text": "numbers no matter what the distribution",
      "offset": 2822.76,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "of the H pract is coming in coming out",
      "offset": 2824.72,
      "duration": 5.119
    },
    {
      "lang": "en",
      "text": "it will be un Gan for each neuron and",
      "offset": 2827.64,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "that's roughly what we want at least at",
      "offset": 2829.839,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "initialization um and then during",
      "offset": 2832.119,
      "duration": 3.96
    },
    {
      "lang": "en",
      "text": "optimization we'll be able to back",
      "offset": 2834.72,
      "duration": 3.879
    },
    {
      "lang": "en",
      "text": "propagate into BN gain and BM bias and",
      "offset": 2836.079,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "change them so the network is given the",
      "offset": 2838.599,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "full ability to do with this whatever it",
      "offset": 2840.48,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "wants uh",
      "offset": 2843.079,
      "duration": 3.561
    },
    {
      "lang": "en",
      "text": "internally here we just have to make",
      "offset": 2844.8,
      "duration": 6.039
    },
    {
      "lang": "en",
      "text": "sure sure that we um include these in",
      "offset": 2846.64,
      "duration": 5.719
    },
    {
      "lang": "en",
      "text": "the parameters of the neural nut because",
      "offset": 2850.839,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "they will be trained with back",
      "offset": 2852.359,
      "duration": 5.441
    },
    {
      "lang": "en",
      "text": "propagation so let's initialize this and",
      "offset": 2854.68,
      "duration": 6.28
    },
    {
      "lang": "en",
      "text": "then we should be able to",
      "offset": 2857.8,
      "duration": 3.16
    },
    {
      "lang": "en",
      "text": "train and then we're going to also copy",
      "offset": 2864.68,
      "duration": 5.72
    },
    {
      "lang": "en",
      "text": "this line which is the batch",
      "offset": 2868.319,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "normalization layer here on a single",
      "offset": 2870.4,
      "duration": 4.04
    },
    {
      "lang": "en",
      "text": "line of code and we're going to swing",
      "offset": 2872.48,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "down here and we're also going to do the",
      "offset": 2874.44,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "exact same thing at test time",
      "offset": 2876.88,
      "duration": 3.16
    },
    {
      "lang": "en",
      "text": "here so similar to train time we're",
      "offset": 2880.8,
      "duration": 5.88
    },
    {
      "lang": "en",
      "text": "going to normalize uh and then scale and",
      "offset": 2883.4,
      "duration": 4.919
    },
    {
      "lang": "en",
      "text": "that's going to give us our train and",
      "offset": 2886.68,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "validation",
      "offset": 2888.319,
      "duration": 3.321
    },
    {
      "lang": "en",
      "text": "loss and we'll see in a second that",
      "offset": 2889.72,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "we're actually going to change this a",
      "offset": 2891.64,
      "duration": 2.479
    },
    {
      "lang": "en",
      "text": "little bit but for now I'm going to keep",
      "offset": 2892.68,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "it this way so I'm just going to wait",
      "offset": 2894.119,
      "duration": 4.281
    },
    {
      "lang": "en",
      "text": "for this to converge okay so I allowed",
      "offset": 2896.359,
      "duration": 3.881
    },
    {
      "lang": "en",
      "text": "the neural nut to converge here and when",
      "offset": 2898.4,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "we scroll down we see that our",
      "offset": 2900.24,
      "duration": 4.52
    },
    {
      "lang": "en",
      "text": "validation loss here is 2.10 roughly",
      "offset": 2901.599,
      "duration": 5.361
    },
    {
      "lang": "en",
      "text": "which I wrote down here and we see that",
      "offset": 2904.76,
      "duration": 3.48
    },
    {
      "lang": "en",
      "text": "this is actually kind of comparable to",
      "offset": 2906.96,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "some of the results that we've achieved",
      "offset": 2908.24,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "uh previously now I'm not actually",
      "offset": 2909.76,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "expecting an improvement in this case",
      "offset": 2912.48,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "and that's because we are dealing with a",
      "offset": 2914.88,
      "duration": 2.84
    },
    {
      "lang": "en",
      "text": "very simple neural nut that has just a",
      "offset": 2916.079,
      "duration": 5.121
    },
    {
      "lang": "en",
      "text": "single hidden layer so in fact in this",
      "offset": 2917.72,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "very simple case of just one hidden",
      "offset": 2921.2,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "layer we were able to actually calculate",
      "offset": 2922.52,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "what the scale of w should be to make",
      "offset": 2924.4,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "these pre activations already have a",
      "offset": 2927.319,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "roughly Gan shape so the bat",
      "offset": 2928.96,
      "duration": 4.359
    },
    {
      "lang": "en",
      "text": "normalization is not doing much here but",
      "offset": 2930.599,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "you might imagine that once you have a",
      "offset": 2933.319,
      "duration": 3.48
    },
    {
      "lang": "en",
      "text": "much deeper neural nut that has lots of",
      "offset": 2934.76,
      "duration": 4.52
    },
    {
      "lang": "en",
      "text": "different types of operations and",
      "offset": 2936.799,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "there's also for example residual",
      "offset": 2939.28,
      "duration": 3.559
    },
    {
      "lang": "en",
      "text": "connections which we'll cover and so on",
      "offset": 2940.559,
      "duration": 4.441
    },
    {
      "lang": "en",
      "text": "it will become basically very very",
      "offset": 2942.839,
      "duration": 4.76
    },
    {
      "lang": "en",
      "text": "difficult to tune the scales of your",
      "offset": 2945,
      "duration": 4.52
    },
    {
      "lang": "en",
      "text": "weight matrices such that all the",
      "offset": 2947.599,
      "duration": 3.401
    },
    {
      "lang": "en",
      "text": "activations throughout the neural nut",
      "offset": 2949.52,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "are roughly gsen and so that's going to",
      "offset": 2951,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "become very quickly intractable but",
      "offset": 2954,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "compared to that it's going to be much",
      "offset": 2956.52,
      "duration": 3.079
    },
    {
      "lang": "en",
      "text": "much easier to sprinkle batch",
      "offset": 2957.839,
      "duration": 3.081
    },
    {
      "lang": "en",
      "text": "normalization layers throughout the",
      "offset": 2959.599,
      "duration": 4.52
    },
    {
      "lang": "en",
      "text": "neural net so in particular it's common",
      "offset": 2960.92,
      "duration": 5.399
    },
    {
      "lang": "en",
      "text": "to to look at every single linear layer",
      "offset": 2964.119,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "like this one one this is a linear layer",
      "offset": 2966.319,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "multiplying by a weight Matrix and",
      "offset": 2967.88,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "adding a bias or for example",
      "offset": 2969.119,
      "duration": 4.361
    },
    {
      "lang": "en",
      "text": "convolutions which we'll cover later and",
      "offset": 2971.48,
      "duration": 4.76
    },
    {
      "lang": "en",
      "text": "also perform basically a multiplication",
      "offset": 2973.48,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "with a weight Matrix but in a more",
      "offset": 2976.24,
      "duration": 4.68
    },
    {
      "lang": "en",
      "text": "spatially structured format it's custom",
      "offset": 2977.96,
      "duration": 4.2
    },
    {
      "lang": "en",
      "text": "it's customary to take these linear",
      "offset": 2980.92,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "layer or convolutional layer and append",
      "offset": 2982.16,
      "duration": 5.48
    },
    {
      "lang": "en",
      "text": "a b rization layer right after it to",
      "offset": 2985.079,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "control the scale of these activations",
      "offset": 2987.64,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "at every point in the neural nut so we'd",
      "offset": 2989.96,
      "duration": 3.8
    },
    {
      "lang": "en",
      "text": "be adding these bom layers throughout",
      "offset": 2992.119,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "the neural nut and then this controls",
      "offset": 2993.76,
      "duration": 3.64
    },
    {
      "lang": "en",
      "text": "the scale of these AC ations throughout",
      "offset": 2995.799,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "the neural net it doesn't require us to",
      "offset": 2997.4,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "do uh perfect mathematics and care about",
      "offset": 2999.64,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "the activation distributions uh for all",
      "offset": 3002.4,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "these different types of neural network",
      "offset": 3004.359,
      "duration": 3.161
    },
    {
      "lang": "en",
      "text": "uh Lego building blocks that you might",
      "offset": 3006.24,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "want to introduce into your neural net",
      "offset": 3007.52,
      "duration": 4.12
    },
    {
      "lang": "en",
      "text": "and it significantly stabilizes uh the",
      "offset": 3009.44,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "training and that's why these uh layers",
      "offset": 3011.64,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "are quite popular now the stability",
      "offset": 3013.68,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "offered by bash normalization actually",
      "offset": 3015.64,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "comes at a terrible cost and that cost",
      "offset": 3017.2,
      "duration": 3.879
    },
    {
      "lang": "en",
      "text": "is that if you think about what's",
      "offset": 3019.88,
      "duration": 3.32
    },
    {
      "lang": "en",
      "text": "Happening Here something something",
      "offset": 3021.079,
      "duration": 4.28
    },
    {
      "lang": "en",
      "text": "terribly strange and unnatural is",
      "offset": 3023.2,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "happening it used to be that we have a",
      "offset": 3025.359,
      "duration": 5.121
    },
    {
      "lang": "en",
      "text": "single example feeding into a neural nut",
      "offset": 3028.24,
      "duration": 4.68
    },
    {
      "lang": "en",
      "text": "and then uh we calculate its activations",
      "offset": 3030.48,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "and its loits and this is a",
      "offset": 3032.92,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "deterministic sort of process so you",
      "offset": 3035.2,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "arrive at some logits for this example",
      "offset": 3037.72,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "and then because of efficiency of",
      "offset": 3040.2,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "training we suddenly started to use",
      "offset": 3041.72,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "batches of examples but those batches of",
      "offset": 3043.559,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "examples were processed independently",
      "offset": 3045.64,
      "duration": 4.439
    },
    {
      "lang": "en",
      "text": "and it was just an efficiency thing but",
      "offset": 3047.559,
      "duration": 4.04
    },
    {
      "lang": "en",
      "text": "now suddenly in batch normalization",
      "offset": 3050.079,
      "duration": 2.921
    },
    {
      "lang": "en",
      "text": "because of the normalization through the",
      "offset": 3051.599,
      "duration": 4.041
    },
    {
      "lang": "en",
      "text": "batch we are coupling these examples",
      "offset": 3053,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "mathematically and in the forward pass",
      "offset": 3055.64,
      "duration": 4.12
    },
    {
      "lang": "en",
      "text": "and the backward pass of a neural l so",
      "offset": 3057.48,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "now the hidden State activations H pract",
      "offset": 3059.76,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "in your log jits for any one input",
      "offset": 3062.76,
      "duration": 4.039
    },
    {
      "lang": "en",
      "text": "example are not just a function of that",
      "offset": 3064.88,
      "duration": 4.36
    },
    {
      "lang": "en",
      "text": "example and its input but they're also a",
      "offset": 3066.799,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "function of all the other examples that",
      "offset": 3069.24,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "happen to come for a ride in that batch",
      "offset": 3071.119,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "and these examples are sampled randomly",
      "offset": 3074.52,
      "duration": 3.279
    },
    {
      "lang": "en",
      "text": "and so what's happening is for example",
      "offset": 3076.559,
      "duration": 2.721
    },
    {
      "lang": "en",
      "text": "when you look at H pract that's going to",
      "offset": 3077.799,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "feed into H the hidden State activations",
      "offset": 3079.28,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "for for example for for any one of these",
      "offset": 3082.04,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "input examples is going to actually",
      "offset": 3084.079,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "change slightly depending on what other",
      "offset": 3086.119,
      "duration": 4.921
    },
    {
      "lang": "en",
      "text": "examples there are in a batch and and",
      "offset": 3088.559,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "depending on what other examples happen",
      "offset": 3091.04,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "to come for a ride H is going to change",
      "offset": 3092.48,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "subtly and it's going to like Jitter if",
      "offset": 3095.76,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "you imagine sampling different examples",
      "offset": 3097.52,
      "duration": 3.72
    },
    {
      "lang": "en",
      "text": "because the the statistics of the mean",
      "offset": 3099.599,
      "duration": 2.841
    },
    {
      "lang": "en",
      "text": "and the standard deviation are going to",
      "offset": 3101.24,
      "duration": 3.72
    },
    {
      "lang": "en",
      "text": "be impacted and so you'll get a Jitter",
      "offset": 3102.44,
      "duration": 5.159
    },
    {
      "lang": "en",
      "text": "for H and you'll get a Jitter for",
      "offset": 3104.96,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "loits and you think that this would be a",
      "offset": 3107.599,
      "duration": 5.401
    },
    {
      "lang": "en",
      "text": "bug uh or something undesirable but in a",
      "offset": 3109.96,
      "duration": 5.399
    },
    {
      "lang": "en",
      "text": "very strange way this actually turns out",
      "offset": 3113,
      "duration": 5.559
    },
    {
      "lang": "en",
      "text": "to be good in your Network training and",
      "offset": 3115.359,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "as a side effect and the reason for that",
      "offset": 3118.559,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "is that you can think of this as kind of",
      "offset": 3120.559,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "like a regularizer because what's",
      "offset": 3122.16,
      "duration": 3.48
    },
    {
      "lang": "en",
      "text": "happening is you have your input and you",
      "offset": 3124.319,
      "duration": 2.961
    },
    {
      "lang": "en",
      "text": "get your age and then depending on the",
      "offset": 3125.64,
      "duration": 4.36
    },
    {
      "lang": "en",
      "text": "other examples this is jittering a bit",
      "offset": 3127.28,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "and so what that does is that it's",
      "offset": 3130,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "effectively padding out any one of these",
      "offset": 3131.2,
      "duration": 3.879
    },
    {
      "lang": "en",
      "text": "input examples and it's introducing a",
      "offset": 3133.44,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "little bit of entropy and um because of",
      "offset": 3135.079,
      "duration": 4.28
    },
    {
      "lang": "en",
      "text": "the padding out it's actually kind of",
      "offset": 3138,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "like a form of a data augmentation which",
      "offset": 3139.359,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "we'll cover in the future and it's kind",
      "offset": 3141.52,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "of like augmenting the input a little",
      "offset": 3143.52,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "bit and jittering it and that makes it",
      "offset": 3145.599,
      "duration": 3.96
    },
    {
      "lang": "en",
      "text": "harder for the neural nut to overfit to",
      "offset": 3147.52,
      "duration": 4.76
    },
    {
      "lang": "en",
      "text": "these concrete specific examples so by",
      "offset": 3149.559,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "introducing all this noise it actually",
      "offset": 3152.28,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "like Pats out the examples and it",
      "offset": 3154.2,
      "duration": 3.879
    },
    {
      "lang": "en",
      "text": "regularizes the neural nut and that's",
      "offset": 3156.2,
      "duration": 4.52
    },
    {
      "lang": "en",
      "text": "one of the reasons why uh deceivingly as",
      "offset": 3158.079,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "a second order effect uh this is",
      "offset": 3160.72,
      "duration": 3.639
    },
    {
      "lang": "en",
      "text": "actually a regularizer and that has made",
      "offset": 3162.4,
      "duration": 4.52
    },
    {
      "lang": "en",
      "text": "it harder uh for us to remove the use of",
      "offset": 3164.359,
      "duration": 3.281
    },
    {
      "lang": "en",
      "text": "batch",
      "offset": 3166.92,
      "duration": 2.439
    },
    {
      "lang": "en",
      "text": "normalization uh because basically no",
      "offset": 3167.64,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "one likes this property that the the",
      "offset": 3169.359,
      "duration": 3.96
    },
    {
      "lang": "en",
      "text": "examples in the batch are coupled",
      "offset": 3171.799,
      "duration": 3.961
    },
    {
      "lang": "en",
      "text": "mathematically and in the forward pass",
      "offset": 3173.319,
      "duration": 4.441
    },
    {
      "lang": "en",
      "text": "and at least all kinds of like strange",
      "offset": 3175.76,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "uh results uh we'll go into some of that",
      "offset": 3177.76,
      "duration": 4.68
    },
    {
      "lang": "en",
      "text": "in a second as well um and it leads to a",
      "offset": 3179.52,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "lot of bugs and um and so on and so no",
      "offset": 3182.44,
      "duration": 5.119
    },
    {
      "lang": "en",
      "text": "one likes this property uh and so people",
      "offset": 3185.2,
      "duration": 5.119
    },
    {
      "lang": "en",
      "text": "have tried to um deprecate the use of",
      "offset": 3187.559,
      "duration": 4.201
    },
    {
      "lang": "en",
      "text": "bat normalization and move to other",
      "offset": 3190.319,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "normalization techniques that do not",
      "offset": 3191.76,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "couple the examples of a batch examples",
      "offset": 3193.319,
      "duration": 3.881
    },
    {
      "lang": "en",
      "text": "are ler normalization instance",
      "offset": 3195.359,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "normalization group normalization and so",
      "offset": 3197.2,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "on and we'll come we'll come some these",
      "offset": 3199.52,
      "duration": 5.599
    },
    {
      "lang": "en",
      "text": "uh later um but basically long story",
      "offset": 3201.76,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "short bat normalization was the first",
      "offset": 3205.119,
      "duration": 3.24
    },
    {
      "lang": "en",
      "text": "kind of normalization layer to be",
      "offset": 3207.2,
      "duration": 3.96
    },
    {
      "lang": "en",
      "text": "introduced it worked extremely well it",
      "offset": 3208.359,
      "duration": 4.281
    },
    {
      "lang": "en",
      "text": "happened to have this regularizing",
      "offset": 3211.16,
      "duration": 5.48
    },
    {
      "lang": "en",
      "text": "effect it stabilized training and people",
      "offset": 3212.64,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "have been trying to remove it and move",
      "offset": 3216.64,
      "duration": 3.159
    },
    {
      "lang": "en",
      "text": "to some of the other normalization",
      "offset": 3218.48,
      "duration": 4.28
    },
    {
      "lang": "en",
      "text": "techniques uh but it's been hard because",
      "offset": 3219.799,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "it it just works quite well and some of",
      "offset": 3222.76,
      "duration": 3.64
    },
    {
      "lang": "en",
      "text": "the reason that it works quite well is",
      "offset": 3224.68,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "again because of this regular rizing",
      "offset": 3226.4,
      "duration": 3.399
    },
    {
      "lang": "en",
      "text": "effect and because of the because it is",
      "offset": 3227.68,
      "duration": 4.36
    },
    {
      "lang": "en",
      "text": "quite effective at um controlling the",
      "offset": 3229.799,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "activations and their",
      "offset": 3232.04,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "distributions uh so that's kind of like",
      "offset": 3233.4,
      "duration": 4.12
    },
    {
      "lang": "en",
      "text": "the brief story of Bash normalization",
      "offset": 3235.16,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "and I'd like to show you one of the",
      "offset": 3237.52,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "other weird sort of outcomes of this",
      "offset": 3239.48,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "coupling so here's one of the strange",
      "offset": 3242.72,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "outcomes that I only glossed over",
      "offset": 3244.68,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "previously when I was evaluating the",
      "offset": 3246.64,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "loss on the validation set basically",
      "offset": 3248.68,
      "duration": 4.76
    },
    {
      "lang": "en",
      "text": "once we've trained a neural net we'd",
      "offset": 3251.28,
      "duration": 3.88
    },
    {
      "lang": "en",
      "text": "like to deploy it in some kind of a",
      "offset": 3253.44,
      "duration": 3.48
    },
    {
      "lang": "en",
      "text": "setting and we'd like to be able to feed",
      "offset": 3255.16,
      "duration": 3.959
    },
    {
      "lang": "en",
      "text": "in a single individual example and get a",
      "offset": 3256.92,
      "duration": 4.679
    },
    {
      "lang": "en",
      "text": "prediction out from our neural net but",
      "offset": 3259.119,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "how do we do that when our neural net",
      "offset": 3261.599,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "now in a forward pass estimates the",
      "offset": 3263.28,
      "duration": 3.4
    },
    {
      "lang": "en",
      "text": "statistics of the mean understand",
      "offset": 3265.119,
      "duration": 3.321
    },
    {
      "lang": "en",
      "text": "deviation of a batch the neur lot",
      "offset": 3266.68,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "expects batches as an input now so how",
      "offset": 3268.44,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "do we feed in a single example and get",
      "offset": 3270.68,
      "duration": 5.119
    },
    {
      "lang": "en",
      "text": "sensible results out and so the proposal",
      "offset": 3272.28,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "in the batch normalization paper is the",
      "offset": 3275.799,
      "duration": 4.52
    },
    {
      "lang": "en",
      "text": "following what we would like to do here",
      "offset": 3277.88,
      "duration": 4.199
    },
    {
      "lang": "en",
      "text": "is we would like to basically have a",
      "offset": 3280.319,
      "duration": 5.961
    },
    {
      "lang": "en",
      "text": "step after training that uh calculates",
      "offset": 3282.079,
      "duration": 6.961
    },
    {
      "lang": "en",
      "text": "and sets the bach uh mean and standard",
      "offset": 3286.28,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "deviation a single time over the",
      "offset": 3289.04,
      "duration": 4.519
    },
    {
      "lang": "en",
      "text": "training set and so I wrote this code",
      "offset": 3290.839,
      "duration": 4.921
    },
    {
      "lang": "en",
      "text": "here in interest of time and we're going",
      "offset": 3293.559,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "to call what's called calibrate the",
      "offset": 3295.76,
      "duration": 4.039
    },
    {
      "lang": "en",
      "text": "bachor statistics and basically what we",
      "offset": 3297.24,
      "duration": 5.64
    },
    {
      "lang": "en",
      "text": "do is torch torch. nograd telling",
      "offset": 3299.799,
      "duration": 5.401
    },
    {
      "lang": "en",
      "text": "pytorch that none of this we will call",
      "offset": 3302.88,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "Dot backward on and it's going to be a",
      "offset": 3305.2,
      "duration": 4.2
    },
    {
      "lang": "en",
      "text": "bit more efficient we're going to take",
      "offset": 3307.359,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "the training set get the pre activations",
      "offset": 3309.4,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "for every single training example and",
      "offset": 3311.68,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "then one single time estimate the mean",
      "offset": 3313.72,
      "duration": 3.079
    },
    {
      "lang": "en",
      "text": "and standard deviation over the entire",
      "offset": 3315.359,
      "duration": 3.521
    },
    {
      "lang": "en",
      "text": "training set and then we're going to get",
      "offset": 3316.799,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "B and mean and B and standard deviation",
      "offset": 3318.88,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "and now these are fixed numbers",
      "offset": 3320.96,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "estimating over the entire training set",
      "offset": 3322.799,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "and here instead of estimating it",
      "offset": 3325.28,
      "duration": 6.68
    },
    {
      "lang": "en",
      "text": "dynamically we are going to instead here",
      "offset": 3328.799,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "use B and",
      "offset": 3331.96,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "mean and here we're just going to use B",
      "offset": 3333.359,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "and standard",
      "offset": 3335.799,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "deviation and so at test time we are",
      "offset": 3337.119,
      "duration": 4.601
    },
    {
      "lang": "en",
      "text": "going to fix these clamp them and use",
      "offset": 3339.48,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "them during inference and",
      "offset": 3341.72,
      "duration": 5.16
    },
    {
      "lang": "en",
      "text": "now you see that we get basically",
      "offset": 3344.48,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "identical result uh but the benefit that",
      "offset": 3346.88,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "we've gained is that we can now also",
      "offset": 3349.92,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "forward a single example because the",
      "offset": 3351.76,
      "duration": 3.16
    },
    {
      "lang": "en",
      "text": "mean and standard deviation are now",
      "offset": 3353.52,
      "duration": 3.96
    },
    {
      "lang": "en",
      "text": "fixed uh sort of tensor",
      "offset": 3354.92,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "that said nobody actually wants to",
      "offset": 3357.48,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "estimate this mean and standard",
      "offset": 3358.92,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "deviation as a second stage after neural",
      "offset": 3360.28,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "network training because everyone is",
      "offset": 3363.24,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "lazy and so this batch normalization",
      "offset": 3364.92,
      "duration": 4.679
    },
    {
      "lang": "en",
      "text": "paper actually introduced one more idea",
      "offset": 3367.48,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "which is that we are can we can estimate",
      "offset": 3369.599,
      "duration": 3.321
    },
    {
      "lang": "en",
      "text": "the mean and standard deviation in a",
      "offset": 3371.319,
      "duration": 4.121
    },
    {
      "lang": "en",
      "text": "running man running manner during",
      "offset": 3372.92,
      "duration": 4.679
    },
    {
      "lang": "en",
      "text": "training of the neuron nut and then we",
      "offset": 3375.44,
      "duration": 3.8
    },
    {
      "lang": "en",
      "text": "can uh simply just have a single stage",
      "offset": 3377.599,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "of training and on the side of that",
      "offset": 3379.24,
      "duration": 3.799
    },
    {
      "lang": "en",
      "text": "training we are estimating the running",
      "offset": 3381.039,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "mean and standard deviation so let's see",
      "offset": 3383.039,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "what that would look like let me",
      "offset": 3384.88,
      "duration": 4.12
    },
    {
      "lang": "en",
      "text": "basically take the mean here that we are",
      "offset": 3386.96,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "estimating on the batch and let me call",
      "offset": 3389,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "this B and mean on the I",
      "offset": 3390.72,
      "duration": 8.399
    },
    {
      "lang": "en",
      "text": "iteration um and then here this is BN",
      "offset": 3393.64,
      "duration": 11.52
    },
    {
      "lang": "en",
      "text": "sdd um bnsd at I",
      "offset": 3399.119,
      "duration": 6.041
    },
    {
      "lang": "en",
      "text": "okay uh and the mean comes here and the",
      "offset": 3405.92,
      "duration": 7.919
    },
    {
      "lang": "en",
      "text": "STD comes here so so far I've done",
      "offset": 3410.76,
      "duration": 4.76
    },
    {
      "lang": "en",
      "text": "nothing I've just uh moved around and I",
      "offset": 3413.839,
      "duration": 3.081
    },
    {
      "lang": "en",
      "text": "created these EXT extra variables for",
      "offset": 3415.52,
      "duration": 3.4
    },
    {
      "lang": "en",
      "text": "the mean and standard deviation and I've",
      "offset": 3416.92,
      "duration": 3.8
    },
    {
      "lang": "en",
      "text": "put them here so so far nothing has",
      "offset": 3418.92,
      "duration": 3.72
    },
    {
      "lang": "en",
      "text": "changed but what we're going to do now",
      "offset": 3420.72,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "is we're going to keep running mean of",
      "offset": 3422.64,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "both of these values during training so",
      "offset": 3424.48,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "let me swing up here and let me create a",
      "offset": 3426.799,
      "duration": 5.641
    },
    {
      "lang": "en",
      "text": "BN meanor running and I'm going to",
      "offset": 3428.559,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "initialize it at uh",
      "offset": 3432.44,
      "duration": 6.76
    },
    {
      "lang": "en",
      "text": "zeros and then BN STD running which I'll",
      "offset": 3435.039,
      "duration": 7.28
    },
    {
      "lang": "en",
      "text": "initialize at",
      "offset": 3439.2,
      "duration": 6.399
    },
    {
      "lang": "en",
      "text": "once because um in the beginning because",
      "offset": 3442.319,
      "duration": 7.441
    },
    {
      "lang": "en",
      "text": "of the way we ized W1 uh and B1 H pract",
      "offset": 3445.599,
      "duration": 6.041
    },
    {
      "lang": "en",
      "text": "will be roughly unit gion so the mean",
      "offset": 3449.76,
      "duration": 3.24
    },
    {
      "lang": "en",
      "text": "will be roughly zero and a standard",
      "offset": 3451.64,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "deviation roughly one so I'm going to",
      "offset": 3453,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "initialize these that way but then here",
      "offset": 3455.24,
      "duration": 5.4
    },
    {
      "lang": "en",
      "text": "I'm going to update these and in pytorch",
      "offset": 3457.64,
      "duration": 6.12
    },
    {
      "lang": "en",
      "text": "um uh these uh mean and standard",
      "offset": 3460.64,
      "duration": 4.919
    },
    {
      "lang": "en",
      "text": "deviation that are running uh they're",
      "offset": 3463.76,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "not actually part of the gradient based",
      "offset": 3465.559,
      "duration": 2.961
    },
    {
      "lang": "en",
      "text": "optimization we're never going to derive",
      "offset": 3466.96,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "gradients with respect to them they're",
      "offset": 3468.52,
      "duration": 5.079
    },
    {
      "lang": "en",
      "text": "they're updated on the side of training",
      "offset": 3470.64,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "and so what we're going to do here is",
      "offset": 3473.599,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "we're going to say with torch. nograd",
      "offset": 3474.88,
      "duration": 6.479
    },
    {
      "lang": "en",
      "text": "telling pytorch that the update here is",
      "offset": 3478,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "not supposed to be building out a graph",
      "offset": 3481.359,
      "duration": 3.041
    },
    {
      "lang": "en",
      "text": "because there will be no dot",
      "offset": 3482.96,
      "duration": 4.44
    },
    {
      "lang": "en",
      "text": "backward but this running is basically",
      "offset": 3484.4,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "going to be",
      "offset": 3487.4,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "0.99 uh9 times the current",
      "offset": 3489.039,
      "duration": 9.361
    },
    {
      "lang": "en",
      "text": "Value Plus 0.001 times the um this value",
      "offset": 3492.599,
      "duration": 6.881
    },
    {
      "lang": "en",
      "text": "this new",
      "offset": 3498.4,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "mean and in the same way bnsd running",
      "offset": 3499.48,
      "duration": 8.639
    },
    {
      "lang": "en",
      "text": "will be mostly what it used to be",
      "offset": 3503.039,
      "duration": 5.08
    },
    {
      "lang": "en",
      "text": "but it will receive a small update in",
      "offset": 3508.64,
      "duration": 3.56
    },
    {
      "lang": "en",
      "text": "the direction of what the current",
      "offset": 3510.359,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "standard deviation",
      "offset": 3512.2,
      "duration": 4.599
    },
    {
      "lang": "en",
      "text": "is and as you're seeing here this update",
      "offset": 3514.039,
      "duration": 5.32
    },
    {
      "lang": "en",
      "text": "is outside and on the side of the",
      "offset": 3516.799,
      "duration": 4.841
    },
    {
      "lang": "en",
      "text": "gradient based optimization and it's",
      "offset": 3519.359,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "simply being updated not using gradient",
      "offset": 3521.64,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "scent it's just being updated using U",
      "offset": 3523.599,
      "duration": 7.24
    },
    {
      "lang": "en",
      "text": "janky like Smooth um sort of uh running",
      "offset": 3525.64,
      "duration": 6.64
    },
    {
      "lang": "en",
      "text": "mean",
      "offset": 3530.839,
      "duration": 4.041
    },
    {
      "lang": "en",
      "text": "Manner and so while the network is",
      "offset": 3532.28,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "training and these pre activations are",
      "offset": 3534.88,
      "duration": 3.719
    },
    {
      "lang": "en",
      "text": "sort of changing and shifting around",
      "offset": 3537,
      "duration": 3.96
    },
    {
      "lang": "en",
      "text": "during during back propagation we are",
      "offset": 3538.599,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "keeping track of the typical mean and",
      "offset": 3540.96,
      "duration": 3.32
    },
    {
      "lang": "en",
      "text": "standard deviation and we're estimating",
      "offset": 3542.52,
      "duration": 5.799
    },
    {
      "lang": "en",
      "text": "them once and when I run",
      "offset": 3544.28,
      "duration": 6.2
    },
    {
      "lang": "en",
      "text": "this now I'm keeping track of this in",
      "offset": 3548.319,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "the running Manner and what we're hoping",
      "offset": 3550.48,
      "duration": 4.359
    },
    {
      "lang": "en",
      "text": "for of course is that the me BN meore",
      "offset": 3552.72,
      "duration": 5.319
    },
    {
      "lang": "en",
      "text": "running and BN meore STD are going to be",
      "offset": 3554.839,
      "duration": 5.401
    },
    {
      "lang": "en",
      "text": "very similar to the ones that we",
      "offset": 3558.039,
      "duration": 5.08
    },
    {
      "lang": "en",
      "text": "calculated here before and that way we",
      "offset": 3560.24,
      "duration": 4.76
    },
    {
      "lang": "en",
      "text": "don't need a second stage because we've",
      "offset": 3563.119,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "sort of combined the two stages and",
      "offset": 3565,
      "duration": 3.64
    },
    {
      "lang": "en",
      "text": "we've put them on the side of each other",
      "offset": 3566.96,
      "duration": 4.04
    },
    {
      "lang": "en",
      "text": "if you want to look at it that way and",
      "offset": 3568.64,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "this is how this is also implemented in",
      "offset": 3571,
      "duration": 3.319
    },
    {
      "lang": "en",
      "text": "The Bash normalization uh layer impi",
      "offset": 3572.4,
      "duration": 5.32
    },
    {
      "lang": "en",
      "text": "torch so during training um the exact",
      "offset": 3574.319,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "same thing will happen and then later",
      "offset": 3577.72,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "when you're using inference it will use",
      "offset": 3579.52,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "the estimated running mean of both the",
      "offset": 3581.88,
      "duration": 4.28
    },
    {
      "lang": "en",
      "text": "mean and standard deviation of those",
      "offset": 3584.319,
      "duration": 4.601
    },
    {
      "lang": "en",
      "text": "hidden States so let's wait for the",
      "offset": 3586.16,
      "duration": 4.52
    },
    {
      "lang": "en",
      "text": "optimization to converge and hopefully",
      "offset": 3588.92,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "the running mean and standard deviation",
      "offset": 3590.68,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "are roughly equal to these two and then",
      "offset": 3592.28,
      "duration": 4.039
    },
    {
      "lang": "en",
      "text": "we can simply use it here and we don't",
      "offset": 3594.039,
      "duration": 4.361
    },
    {
      "lang": "en",
      "text": "need this stage of explicit calibration",
      "offset": 3596.319,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "at the end okay so the optimization",
      "offset": 3598.4,
      "duration": 4.439
    },
    {
      "lang": "en",
      "text": "finished I'll rerun the explicit",
      "offset": 3600.319,
      "duration": 5.081
    },
    {
      "lang": "en",
      "text": "estimation and then the B and mean from",
      "offset": 3602.839,
      "duration": 5.24
    },
    {
      "lang": "en",
      "text": "the explicit estimation is here and B",
      "offset": 3605.4,
      "duration": 5.639
    },
    {
      "lang": "en",
      "text": "and mean from the running estimation",
      "offset": 3608.079,
      "duration": 5.801
    },
    {
      "lang": "en",
      "text": "during the during the optimization you",
      "offset": 3611.039,
      "duration": 5.56
    },
    {
      "lang": "en",
      "text": "can see is very very similar it's not",
      "offset": 3613.88,
      "duration": 4.679
    },
    {
      "lang": "en",
      "text": "identical but it's pretty",
      "offset": 3616.599,
      "duration": 5.921
    },
    {
      "lang": "en",
      "text": "close and the same way BN STD is this",
      "offset": 3618.559,
      "duration": 8.04
    },
    {
      "lang": "en",
      "text": "and BN STD running is this and so you",
      "offset": 3622.52,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "can see that once again they are fairly",
      "offset": 3626.599,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "similar values not identical but pretty",
      "offset": 3628.44,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "close and so then here instead of being",
      "offset": 3630.839,
      "duration": 5.081
    },
    {
      "lang": "en",
      "text": "mean we can use the BN mean running",
      "offset": 3633.48,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "instead of bnsd we can use bnsd",
      "offset": 3635.92,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "running and uh hopefully the validation",
      "offset": 3638.92,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "loss will not be impacted too",
      "offset": 3641.68,
      "duration": 5.08
    },
    {
      "lang": "en",
      "text": "much okay so it's basically identical",
      "offset": 3643.559,
      "duration": 5.681
    },
    {
      "lang": "en",
      "text": "and this way we've eliminated the need",
      "offset": 3646.76,
      "duration": 4.839
    },
    {
      "lang": "en",
      "text": "for this explicit stage of calibration",
      "offset": 3649.24,
      "duration": 4.119
    },
    {
      "lang": "en",
      "text": "because we are doing it in line over",
      "offset": 3651.599,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "here okay so we're almost done with",
      "offset": 3653.359,
      "duration": 3.401
    },
    {
      "lang": "en",
      "text": "batch normalization there are only two",
      "offset": 3655.119,
      "duration": 3.561
    },
    {
      "lang": "en",
      "text": "more notes that I'd like to make number",
      "offset": 3656.76,
      "duration": 3.559
    },
    {
      "lang": "en",
      "text": "one I've skipped a discussion over what",
      "offset": 3658.68,
      "duration": 3.639
    },
    {
      "lang": "en",
      "text": "is this plus Epsilon doing here this",
      "offset": 3660.319,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "Epsilon is usually like some small fixed",
      "offset": 3662.319,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "number for example one5 by default and",
      "offset": 3664.559,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "what it's doing is that it's basically",
      "offset": 3667.359,
      "duration": 3.641
    },
    {
      "lang": "en",
      "text": "preventing a division by zero in the",
      "offset": 3668.72,
      "duration": 5.72
    },
    {
      "lang": "en",
      "text": "case that the variance over your batch",
      "offset": 3671,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "is exactly zero in that case uh here we",
      "offset": 3674.44,
      "duration": 4.679
    },
    {
      "lang": "en",
      "text": "normally have a division by zero but",
      "offset": 3677.48,
      "duration": 3.559
    },
    {
      "lang": "en",
      "text": "because of the plus Epsilon uh this is",
      "offset": 3679.119,
      "duration": 3.281
    },
    {
      "lang": "en",
      "text": "going to become a small number in the",
      "offset": 3681.039,
      "duration": 3.121
    },
    {
      "lang": "en",
      "text": "denominator instead and things will be",
      "offset": 3682.4,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "more well behaved so feel free to also",
      "offset": 3684.16,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "add a plus Epsilon here of a very small",
      "offset": 3686.64,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "number it doesn't actually substantially",
      "offset": 3688.559,
      "duration": 3.321
    },
    {
      "lang": "en",
      "text": "change the result I'm going to skip it",
      "offset": 3690.16,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "in our case just because uh this is",
      "offset": 3691.88,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "unlikely to happen in our very simple",
      "offset": 3693.359,
      "duration": 3.72
    },
    {
      "lang": "en",
      "text": "example here and the second thing I want",
      "offset": 3694.88,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "you to notice is that we're being",
      "offset": 3697.079,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "wasteful here and it's very subtle but",
      "offset": 3698.72,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "right here where we are adding the bias",
      "offset": 3701.64,
      "duration": 5.24
    },
    {
      "lang": "en",
      "text": "into H preact these biases now are",
      "offset": 3703.599,
      "duration": 5.121
    },
    {
      "lang": "en",
      "text": "actually useless because we're adding",
      "offset": 3706.88,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "them to the H preact but then we are",
      "offset": 3708.72,
      "duration": 4.68
    },
    {
      "lang": "en",
      "text": "calculating the mean for every one of",
      "offset": 3710.96,
      "duration": 5.119
    },
    {
      "lang": "en",
      "text": "these neurons and subtracting it so",
      "offset": 3713.4,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "whatever bias you add here is going to",
      "offset": 3716.079,
      "duration": 5.401
    },
    {
      "lang": "en",
      "text": "get subtracted right here and so these",
      "offset": 3718.44,
      "duration": 5.08
    },
    {
      "lang": "en",
      "text": "biases are not doing anything in fact",
      "offset": 3721.48,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "they're being subtracted out and they",
      "offset": 3723.52,
      "duration": 3.64
    },
    {
      "lang": "en",
      "text": "don't impact the rest of the calculation",
      "offset": 3724.839,
      "duration": 4.28
    },
    {
      "lang": "en",
      "text": "so if you look at b1. grad it's actually",
      "offset": 3727.16,
      "duration": 3.72
    },
    {
      "lang": "en",
      "text": "going to be zero because it's being",
      "offset": 3729.119,
      "duration": 3.321
    },
    {
      "lang": "en",
      "text": "subtracted out and doesn't actually have",
      "offset": 3730.88,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "any effect and so whenever you're using",
      "offset": 3732.44,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "bash normalization layers then if you",
      "offset": 3734.559,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "have any weight layers before like a",
      "offset": 3736.44,
      "duration": 4.119
    },
    {
      "lang": "en",
      "text": "linear or a c or something like that",
      "offset": 3738.319,
      "duration": 4.201
    },
    {
      "lang": "en",
      "text": "you're better off coming here and just",
      "offset": 3740.559,
      "duration": 4.601
    },
    {
      "lang": "en",
      "text": "like not using bias so you don't want to",
      "offset": 3742.52,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "use bias and then here you don't want to",
      "offset": 3745.16,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "add it because it's that spirous instead",
      "offset": 3748.44,
      "duration": 5.24
    },
    {
      "lang": "en",
      "text": "we have this B normalization bias here",
      "offset": 3751.16,
      "duration": 4.679
    },
    {
      "lang": "en",
      "text": "and that b rization bias is now in",
      "offset": 3753.68,
      "duration": 4.28
    },
    {
      "lang": "en",
      "text": "charge of the biasing of this",
      "offset": 3755.839,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "distribution instead of this B1 that we",
      "offset": 3757.96,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "had here originally and so uh basically",
      "offset": 3760.48,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "bash normalization layer has its own",
      "offset": 3763.96,
      "duration": 4.04
    },
    {
      "lang": "en",
      "text": "bias and there's no need to have a bias",
      "offset": 3765.279,
      "duration": 4.681
    },
    {
      "lang": "en",
      "text": "in the layer before it because that bias",
      "offset": 3768,
      "duration": 4.119
    },
    {
      "lang": "en",
      "text": "is going to be subtracted out anyway so",
      "offset": 3769.96,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "that's the other small detail to be",
      "offset": 3772.119,
      "duration": 3.121
    },
    {
      "lang": "en",
      "text": "careful with sometimes it's not going to",
      "offset": 3773.4,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "do anything catastrophic this B1 will",
      "offset": 3775.24,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "just be useless it will never get any",
      "offset": 3777.4,
      "duration": 3.959
    },
    {
      "lang": "en",
      "text": "gradient uh it will not learn it will",
      "offset": 3779.48,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "stay constant and it's just wasteful but",
      "offset": 3781.359,
      "duration": 4.521
    },
    {
      "lang": "en",
      "text": "it doesn't actually really uh impact",
      "offset": 3783.24,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "anything otherwise okay so I rearranged",
      "offset": 3785.88,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "the code a little bit with comments and",
      "offset": 3787.96,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "I just wanted to give a very quick",
      "offset": 3789.96,
      "duration": 3.72
    },
    {
      "lang": "en",
      "text": "summary of The Bash normalization layer",
      "offset": 3791.24,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "we are using bash normalization to",
      "offset": 3793.68,
      "duration": 4.599
    },
    {
      "lang": "en",
      "text": "control the statistics of activations in",
      "offset": 3795.4,
      "duration": 5.24
    },
    {
      "lang": "en",
      "text": "the neural net it is common to sprinkle",
      "offset": 3798.279,
      "duration": 4.201
    },
    {
      "lang": "en",
      "text": "bash normalization layer across the",
      "offset": 3800.64,
      "duration": 4.12
    },
    {
      "lang": "en",
      "text": "neural net and usually we will place it",
      "offset": 3802.48,
      "duration": 5.24
    },
    {
      "lang": "en",
      "text": "after layer that have multiplications",
      "offset": 3804.76,
      "duration": 4.92
    },
    {
      "lang": "en",
      "text": "like for example a linear layer or",
      "offset": 3807.72,
      "duration": 3.8
    },
    {
      "lang": "en",
      "text": "convolutional layer which we may cover",
      "offset": 3809.68,
      "duration": 2.599
    },
    {
      "lang": "en",
      "text": "in the",
      "offset": 3811.52,
      "duration": 3.16
    },
    {
      "lang": "en",
      "text": "future now the bat normalization",
      "offset": 3812.279,
      "duration": 6.201
    },
    {
      "lang": "en",
      "text": "internally has parameters for the gain",
      "offset": 3814.68,
      "duration": 5.56
    },
    {
      "lang": "en",
      "text": "and the bias and these are trained using",
      "offset": 3818.48,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "back propagation it also has two buffers",
      "offset": 3820.24,
      "duration": 5.72
    },
    {
      "lang": "en",
      "text": "the buffers are the mean and the",
      "offset": 3824.4,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "standard deviation the running mean and",
      "offset": 3825.96,
      "duration": 3.96
    },
    {
      "lang": "en",
      "text": "the running mean of the standard",
      "offset": 3827.92,
      "duration": 3.879
    },
    {
      "lang": "en",
      "text": "deviation and these are not trained",
      "offset": 3829.92,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "using back propagation these are trained",
      "offset": 3831.799,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "using this uh janky update of kind of",
      "offset": 3833.599,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "like a running mean",
      "offset": 3836.599,
      "duration": 3.72
    },
    {
      "lang": "en",
      "text": "update so",
      "offset": 3838,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "um these are sort of the parameters and",
      "offset": 3840.319,
      "duration": 5.361
    },
    {
      "lang": "en",
      "text": "the buffers of Bator layer and then",
      "offset": 3843,
      "duration": 3.88
    },
    {
      "lang": "en",
      "text": "really what it's doing is it's",
      "offset": 3845.68,
      "duration": 2.84
    },
    {
      "lang": "en",
      "text": "calculating the mean and a standard",
      "offset": 3846.88,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "deviation of the activations uh that are",
      "offset": 3848.52,
      "duration": 5.48
    },
    {
      "lang": "en",
      "text": "feeding into the Bator layer over that",
      "offset": 3850.72,
      "duration": 6.119
    },
    {
      "lang": "en",
      "text": "batch then it's centering that batch to",
      "offset": 3854,
      "duration": 5.559
    },
    {
      "lang": "en",
      "text": "be unit gion and then it's offsetting",
      "offset": 3856.839,
      "duration": 6.321
    },
    {
      "lang": "en",
      "text": "and scaling it by the Learned bias and",
      "offset": 3859.559,
      "duration": 5.8
    },
    {
      "lang": "en",
      "text": "gain and then on top of that it's",
      "offset": 3863.16,
      "duration": 3.72
    },
    {
      "lang": "en",
      "text": "keeping track of the mean and standard",
      "offset": 3865.359,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "deviation of the inputs and it's",
      "offset": 3866.88,
      "duration": 4.36
    },
    {
      "lang": "en",
      "text": "maintaining this running mean and",
      "offset": 3870.079,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "standard deviation and this will later",
      "offset": 3871.24,
      "duration": 4.44
    },
    {
      "lang": "en",
      "text": "be used at inference so that we don't",
      "offset": 3873.52,
      "duration": 3.72
    },
    {
      "lang": "en",
      "text": "have to reestimate the mean stand",
      "offset": 3875.68,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "deviation all the time and in addition",
      "offset": 3877.24,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "that allows us to basically forward",
      "offset": 3880,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "individual examples at test time so",
      "offset": 3881.4,
      "duration": 4.52
    },
    {
      "lang": "en",
      "text": "that's the bash normalization layer it's",
      "offset": 3884.4,
      "duration": 4.439
    },
    {
      "lang": "en",
      "text": "a fairly complicated layer um but this",
      "offset": 3885.92,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "is what it's doing internally now I",
      "offset": 3888.839,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "wanted to show you a little bit of a",
      "offset": 3890.72,
      "duration": 4.52
    },
    {
      "lang": "en",
      "text": "real example so you can search resnet",
      "offset": 3891.839,
      "duration": 6.321
    },
    {
      "lang": "en",
      "text": "which is a residual neural network and",
      "offset": 3895.24,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "these are common types of neural",
      "offset": 3898.16,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "networks used for image",
      "offset": 3899.4,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "classification and of course we haven't",
      "offset": 3901.2,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "come resnets in detail so I'm not going",
      "offset": 3903.16,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "to explain all the pieces of it but for",
      "offset": 3905.92,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "now just note that the image feeds into",
      "offset": 3908.039,
      "duration": 4.681
    },
    {
      "lang": "en",
      "text": "a reset on the top here and there's many",
      "offset": 3910.559,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "many layers with repeating structure all",
      "offset": 3912.72,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "the way to predictions of what's inside",
      "offset": 3915.279,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "that image this repeating structure is",
      "offset": 3916.96,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "made up of these blocks and these blocks",
      "offset": 3919.44,
      "duration": 4.2
    },
    {
      "lang": "en",
      "text": "are just sequentially stacked up in this",
      "offset": 3921.44,
      "duration": 5.48
    },
    {
      "lang": "en",
      "text": "deep neural network now the code for",
      "offset": 3923.64,
      "duration": 6.04
    },
    {
      "lang": "en",
      "text": "this uh the block basically that's used",
      "offset": 3926.92,
      "duration": 5.639
    },
    {
      "lang": "en",
      "text": "and repeated sequentially in series is",
      "offset": 3929.68,
      "duration": 5.56
    },
    {
      "lang": "en",
      "text": "called this bottleneck block bottleneck",
      "offset": 3932.559,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "block and there's a lot here this is all",
      "offset": 3935.24,
      "duration": 4.68
    },
    {
      "lang": "en",
      "text": "pych and of course we haven't covered",
      "offset": 3937.839,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "all of it but I want to point out some",
      "offset": 3939.92,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "small pieces of it here in the init is",
      "offset": 3941.279,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "where we initialize the neuronet so this",
      "offset": 3944.319,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "code of block here is basically the kind",
      "offset": 3946.16,
      "duration": 2.919
    },
    {
      "lang": "en",
      "text": "of stuff we're doing here we're",
      "offset": 3947.92,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "initializing all the layers and in the",
      "offset": 3949.079,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "forward we are specifying how the neuron",
      "offset": 3951.52,
      "duration": 3.48
    },
    {
      "lang": "en",
      "text": "lot acts once you actually have the",
      "offset": 3953.48,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "input so this code here is along the",
      "offset": 3955,
      "duration": 5.64
    },
    {
      "lang": "en",
      "text": "lines of what we're doing",
      "offset": 3957.64,
      "duration": 6.36
    },
    {
      "lang": "en",
      "text": "here and now these blocks are replicated",
      "offset": 3960.64,
      "duration": 5.88
    },
    {
      "lang": "en",
      "text": "and stacked up serially and that's what",
      "offset": 3964,
      "duration": 5.24
    },
    {
      "lang": "en",
      "text": "a residual Network would be and so",
      "offset": 3966.52,
      "duration": 6.2
    },
    {
      "lang": "en",
      "text": "notice What's Happening Here com one um",
      "offset": 3969.24,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "these are convolution layers and these",
      "offset": 3972.72,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "convolution layers basically they're the",
      "offset": 3975.24,
      "duration": 4.68
    },
    {
      "lang": "en",
      "text": "same thing as a linear layer except",
      "offset": 3977.359,
      "duration": 5.521
    },
    {
      "lang": "en",
      "text": "convolutional layers don't apply um",
      "offset": 3979.92,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "convolutional layers are used for images",
      "offset": 3982.88,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "and so they have SP structure and",
      "offset": 3984.799,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "basically this linear multiplication and",
      "offset": 3986.72,
      "duration": 5.48
    },
    {
      "lang": "en",
      "text": "bias offset are done on patches instead",
      "offset": 3988.559,
      "duration": 6.361
    },
    {
      "lang": "en",
      "text": "of math instead of the full input so",
      "offset": 3992.2,
      "duration": 4.44
    },
    {
      "lang": "en",
      "text": "because these images have structure",
      "offset": 3994.92,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "spatial structure convolutions just",
      "offset": 3996.64,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "basically do WX plus b but they do it on",
      "offset": 3998.68,
      "duration": 5.399
    },
    {
      "lang": "en",
      "text": "overlapping patches of the input but",
      "offset": 4001.76,
      "duration": 4.039
    },
    {
      "lang": "en",
      "text": "otherwise it's WX plus",
      "offset": 4004.079,
      "duration": 4.361
    },
    {
      "lang": "en",
      "text": "P then we have the norm layer which by",
      "offset": 4005.799,
      "duration": 4.361
    },
    {
      "lang": "en",
      "text": "default here is initialized to be a bash",
      "offset": 4008.44,
      "duration": 3.8
    },
    {
      "lang": "en",
      "text": "Norm in 2D so two- dimensional bash",
      "offset": 4010.16,
      "duration": 4.6
    },
    {
      "lang": "en",
      "text": "normalization layer and then we have a",
      "offset": 4012.24,
      "duration": 6.2
    },
    {
      "lang": "en",
      "text": "nonlinearity like reu so instead of uh",
      "offset": 4014.76,
      "duration": 6.44
    },
    {
      "lang": "en",
      "text": "here they use reu we are using 10 in",
      "offset": 4018.44,
      "duration": 5.399
    },
    {
      "lang": "en",
      "text": "this case but both both are just",
      "offset": 4021.2,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "nonlinearities and you can just use them",
      "offset": 4023.839,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "relatively interchangeably for very deep",
      "offset": 4025.92,
      "duration": 4.28
    },
    {
      "lang": "en",
      "text": "networks re typically empirically work a",
      "offset": 4027.839,
      "duration": 5.401
    },
    {
      "lang": "en",
      "text": "bit better so see the motif that's being",
      "offset": 4030.2,
      "duration": 5.119
    },
    {
      "lang": "en",
      "text": "repeated here we have convolution bat",
      "offset": 4033.24,
      "duration": 4.119
    },
    {
      "lang": "en",
      "text": "normalization reu convolution bat",
      "offset": 4035.319,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "normalization re Etc and then here this",
      "offset": 4037.359,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "is residual connection that we haven't",
      "offset": 4039.96,
      "duration": 3.879
    },
    {
      "lang": "en",
      "text": "covered yet but basically that's the",
      "offset": 4041.52,
      "duration": 4.12
    },
    {
      "lang": "en",
      "text": "exact same pattern we have here with we",
      "offset": 4043.839,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "have a weight layer like a convolution",
      "offset": 4045.64,
      "duration": 5.88
    },
    {
      "lang": "en",
      "text": "or like a linear layer bash",
      "offset": 4048.64,
      "duration": 5.919
    },
    {
      "lang": "en",
      "text": "normalization and then 10h which is",
      "offset": 4051.52,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "nonlinearity but basically a weight",
      "offset": 4054.559,
      "duration": 4.04
    },
    {
      "lang": "en",
      "text": "layer a normalization layer and",
      "offset": 4056.72,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "nonlinearity and that's the motif that",
      "offset": 4058.599,
      "duration": 3.48
    },
    {
      "lang": "en",
      "text": "you would be stacking up when you create",
      "offset": 4060.48,
      "duration": 3.72
    },
    {
      "lang": "en",
      "text": "these deep neural networks exactly as",
      "offset": 4062.079,
      "duration": 4.2
    },
    {
      "lang": "en",
      "text": "it's done here and one more thing I'd",
      "offset": 4064.2,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "like you to notice is that here when",
      "offset": 4066.279,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "they are initializing the com layers",
      "offset": 4067.88,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "like com 1 by one the depth for that is",
      "offset": 4070.2,
      "duration": 5.48
    },
    {
      "lang": "en",
      "text": "right here and so it's initializing an",
      "offset": 4073.16,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "nn. Tod which is a convolution layer in",
      "offset": 4075.68,
      "duration": 4.2
    },
    {
      "lang": "en",
      "text": "pytorch and there's a bunch of keyword",
      "offset": 4078.039,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "arguments here that I'm not going to",
      "offset": 4079.88,
      "duration": 3.64
    },
    {
      "lang": "en",
      "text": "explain yet but you see how there's bias",
      "offset": 4081.24,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "equals false the bias equals false is",
      "offset": 4083.52,
      "duration": 4.839
    },
    {
      "lang": "en",
      "text": "exactly for the same reason as bias is",
      "offset": 4085.799,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "not used in our case you see how I eras",
      "offset": 4088.359,
      "duration": 4.68
    },
    {
      "lang": "en",
      "text": "the use of bias and the use of bias is",
      "offset": 4090.839,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "spous because after this weight layer",
      "offset": 4093.039,
      "duration": 3.881
    },
    {
      "lang": "en",
      "text": "there's a bash normalization and The",
      "offset": 4095.16,
      "duration": 3.999
    },
    {
      "lang": "en",
      "text": "Bash normalization subtracts that bias",
      "offset": 4096.92,
      "duration": 4.12
    },
    {
      "lang": "en",
      "text": "and then has its own bias so there's no",
      "offset": 4099.159,
      "duration": 3.961
    },
    {
      "lang": "en",
      "text": "need to introduce these spous parameters",
      "offset": 4101.04,
      "duration": 3.56
    },
    {
      "lang": "en",
      "text": "it wouldn't hurt performance it's just",
      "offset": 4103.12,
      "duration": 4.28
    },
    {
      "lang": "en",
      "text": "useless and so because they have this",
      "offset": 4104.6,
      "duration": 5.559
    },
    {
      "lang": "en",
      "text": "motif of C Bast umbrell they don't need",
      "offset": 4107.4,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "a bias here because there's a bias",
      "offset": 4110.159,
      "duration": 5.761
    },
    {
      "lang": "en",
      "text": "inside here so by the way this example",
      "offset": 4111.96,
      "duration": 5.319
    },
    {
      "lang": "en",
      "text": "here is very easy to find just do",
      "offset": 4115.92,
      "duration": 2.6
    },
    {
      "lang": "en",
      "text": "resonet pie",
      "offset": 4117.279,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "torch and uh it's this example here so",
      "offset": 4118.52,
      "duration": 4.44
    },
    {
      "lang": "en",
      "text": "this is kind of like the stock",
      "offset": 4121.92,
      "duration": 2.919
    },
    {
      "lang": "en",
      "text": "implementation of a residual neural",
      "offset": 4122.96,
      "duration": 4.359
    },
    {
      "lang": "en",
      "text": "network in pytorch and uh you can find",
      "offset": 4124.839,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "that here but of course I haven't",
      "offset": 4127.319,
      "duration": 3.641
    },
    {
      "lang": "en",
      "text": "covered many of these parts yet and I",
      "offset": 4128.839,
      "duration": 3.96
    },
    {
      "lang": "en",
      "text": "would also like to briefly descend into",
      "offset": 4130.96,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "the definitions of these pytorch layers",
      "offset": 4132.799,
      "duration": 4.201
    },
    {
      "lang": "en",
      "text": "and the the parameters that they take",
      "offset": 4135.12,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "now instead of a convolutional layer",
      "offset": 4137,
      "duration": 4.359
    },
    {
      "lang": "en",
      "text": "we're going to look at a linear layer uh",
      "offset": 4138.319,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "because that's the one that we're using",
      "offset": 4141.359,
      "duration": 2.721
    },
    {
      "lang": "en",
      "text": "here this is a linear layer and I",
      "offset": 4142.56,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "haven't cover covered convolutions yet",
      "offset": 4144.08,
      "duration": 3.4
    },
    {
      "lang": "en",
      "text": "but as I mentioned convolutions are",
      "offset": 4146.159,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "basically linear layers except on",
      "offset": 4147.48,
      "duration": 6.44
    },
    {
      "lang": "en",
      "text": "patches so a linear layer performs a WX",
      "offset": 4150.239,
      "duration": 5.801
    },
    {
      "lang": "en",
      "text": "plus b except here they're calling the W",
      "offset": 4153.92,
      "duration": 3.08
    },
    {
      "lang": "en",
      "text": "A",
      "offset": 4156.04,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "transpose um so to calcul WX plus b very",
      "offset": 4157,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "much like we did here to initialize this",
      "offset": 4160.12,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "layer you need to know the fan in the",
      "offset": 4162.199,
      "duration": 5.441
    },
    {
      "lang": "en",
      "text": "fan out and that's so that they can",
      "offset": 4164.279,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "initialize this W this is the fan in and",
      "offset": 4167.64,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "the fan out so they know how how big the",
      "offset": 4170.759,
      "duration": 5.281
    },
    {
      "lang": "en",
      "text": "weight Matrix should be you need to also",
      "offset": 4173.48,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "pass in whether you whether or not you",
      "offset": 4176.04,
      "duration": 4.199
    },
    {
      "lang": "en",
      "text": "want a bias and if you set it to false",
      "offset": 4177.56,
      "duration": 5.32
    },
    {
      "lang": "en",
      "text": "then no bias will be uh inside this",
      "offset": 4180.239,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "layer um and you may want to do that",
      "offset": 4182.88,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "exactly like in our case if your layer",
      "offset": 4185.679,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "is followed by a normalization layer",
      "offset": 4187.759,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "such as batch",
      "offset": 4189.6,
      "duration": 3.32
    },
    {
      "lang": "en",
      "text": "Norm so this allows you to basically",
      "offset": 4190.759,
      "duration": 4.281
    },
    {
      "lang": "en",
      "text": "disable a bias now in terms of the",
      "offset": 4192.92,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "initial ation if we swing down here this",
      "offset": 4195.04,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "is reporting the variables used inside",
      "offset": 4197.239,
      "duration": 5.161
    },
    {
      "lang": "en",
      "text": "this linear layer and our linear layer",
      "offset": 4199.44,
      "duration": 5.4
    },
    {
      "lang": "en",
      "text": "here has two parameters the weight and",
      "offset": 4202.4,
      "duration": 4.52
    },
    {
      "lang": "en",
      "text": "the bias in the same way they have a",
      "offset": 4204.84,
      "duration": 4.359
    },
    {
      "lang": "en",
      "text": "weight and a bias and they're talking",
      "offset": 4206.92,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "about how they initialize it by default",
      "offset": 4209.199,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "so by default P will initialize your",
      "offset": 4211.719,
      "duration": 3.721
    },
    {
      "lang": "en",
      "text": "weights by taking the",
      "offset": 4213.52,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "Fanon and then um doing one over fanin",
      "offset": 4215.44,
      "duration": 7.12
    },
    {
      "lang": "en",
      "text": "square root and then instead of a normal",
      "offset": 4219.12,
      "duration": 5.4
    },
    {
      "lang": "en",
      "text": "distribution they are using a uniform",
      "offset": 4222.56,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "distribution",
      "offset": 4224.52,
      "duration": 3.719
    },
    {
      "lang": "en",
      "text": "so it's very much the same thing but",
      "offset": 4225.84,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "they are using a one instead of 5 over",
      "offset": 4228.239,
      "duration": 3.401
    },
    {
      "lang": "en",
      "text": "three so there's no gain being",
      "offset": 4230.159,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "calculated here the gain is just one but",
      "offset": 4231.64,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "otherwise is exactly one over the square",
      "offset": 4233.92,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "root of fan in exactly as we have",
      "offset": 4236.679,
      "duration": 5.841
    },
    {
      "lang": "en",
      "text": "here so one over the square root of K is",
      "offset": 4239.52,
      "duration": 6.159
    },
    {
      "lang": "en",
      "text": "the is the scale of the weights but when",
      "offset": 4242.52,
      "duration": 4.44
    },
    {
      "lang": "en",
      "text": "they are drawing the numbers they're not",
      "offset": 4245.679,
      "duration": 3.641
    },
    {
      "lang": "en",
      "text": "using a gussion by default they're using",
      "offset": 4246.96,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "a uniform distribution by default and so",
      "offset": 4249.32,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "they draw uniformly from negative of K",
      "offset": 4251.679,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "to squ of K",
      "offset": 4254.32,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "but it's the exact same thing and the",
      "offset": 4256.239,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "same motivation from for with respect to",
      "offset": 4257.92,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "what we've seen in this lecture and the",
      "offset": 4260.719,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "reason they're doing this is if you have",
      "offset": 4263.36,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "a roughly gsan input this will ensure",
      "offset": 4264.96,
      "duration": 5.08
    },
    {
      "lang": "en",
      "text": "that out of this layer you will have a",
      "offset": 4268,
      "duration": 5.32
    },
    {
      "lang": "en",
      "text": "roughly Gan output and you you basically",
      "offset": 4270.04,
      "duration": 5.639
    },
    {
      "lang": "en",
      "text": "achieve that by scaling the weights by",
      "offset": 4273.32,
      "duration": 4.68
    },
    {
      "lang": "en",
      "text": "one over the square root of fan in so",
      "offset": 4275.679,
      "duration": 3.48
    },
    {
      "lang": "en",
      "text": "that's what this is",
      "offset": 4278,
      "duration": 3.88
    },
    {
      "lang": "en",
      "text": "doing and then the second thing is the",
      "offset": 4279.159,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "bash normalization layer so let's look",
      "offset": 4281.88,
      "duration": 4.279
    },
    {
      "lang": "en",
      "text": "at what that looks like in pytorch",
      "offset": 4283.64,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "so here we have a onedimensional b",
      "offset": 4286.159,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "normalization layer exactly as we are",
      "offset": 4287.719,
      "duration": 3.801
    },
    {
      "lang": "en",
      "text": "using here and there are a number of",
      "offset": 4289.52,
      "duration": 4.04
    },
    {
      "lang": "en",
      "text": "keyword arguments going into it as well",
      "offset": 4291.52,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "so we need to know the number of",
      "offset": 4293.56,
      "duration": 4.119
    },
    {
      "lang": "en",
      "text": "features uh for us that is 200 and that",
      "offset": 4294.64,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "is needed so that we can initialize",
      "offset": 4297.679,
      "duration": 4.761
    },
    {
      "lang": "en",
      "text": "these parameters here the gain the bias",
      "offset": 4299.44,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "and the buffers for the running uh mean",
      "offset": 4302.44,
      "duration": 3.56
    },
    {
      "lang": "en",
      "text": "and standard",
      "offset": 4304.8,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "deviation then they need to know the",
      "offset": 4306,
      "duration": 4.6
    },
    {
      "lang": "en",
      "text": "value of Epsilon here and by default",
      "offset": 4307.92,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "this is one5 you don't typically change",
      "offset": 4310.6,
      "duration": 4.119
    },
    {
      "lang": "en",
      "text": "this too much then they need to know the",
      "offset": 4312.719,
      "duration": 3.321
    },
    {
      "lang": "en",
      "text": "momentum",
      "offset": 4314.719,
      "duration": 3.721
    },
    {
      "lang": "en",
      "text": "and the momentum here as they explain is",
      "offset": 4316.04,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "basically used for these uh running mean",
      "offset": 4318.44,
      "duration": 4.68
    },
    {
      "lang": "en",
      "text": "and running standard deviation so by",
      "offset": 4321.04,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "default the momentum here is 0.1 the",
      "offset": 4323.12,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "momentum we are using here in this",
      "offset": 4325.199,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "example is",
      "offset": 4326.719,
      "duration": 5.361
    },
    {
      "lang": "en",
      "text": "0.001 and basically rough you may want",
      "offset": 4328.88,
      "duration": 5.319
    },
    {
      "lang": "en",
      "text": "to change this sometimes and roughly",
      "offset": 4332.08,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "speaking if you have a very large batch",
      "offset": 4334.199,
      "duration": 4.761
    },
    {
      "lang": "en",
      "text": "size then typically what you'll see is",
      "offset": 4336.48,
      "duration": 3.56
    },
    {
      "lang": "en",
      "text": "that when you estimate the mean and the",
      "offset": 4338.96,
      "duration": 3.32
    },
    {
      "lang": "en",
      "text": "standard deviation for every single",
      "offset": 4340.04,
      "duration": 3.8
    },
    {
      "lang": "en",
      "text": "batch size if it's large enough you're",
      "offset": 4342.28,
      "duration": 3.879
    },
    {
      "lang": "en",
      "text": "going to get roughly the same result",
      "offset": 4343.84,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "and so therefore you can use slightly",
      "offset": 4346.159,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "higher momentum like",
      "offset": 4348.719,
      "duration": 5.881
    },
    {
      "lang": "en",
      "text": "0.1 but for a batch size as small as 32",
      "offset": 4350.159,
      "duration": 5.921
    },
    {
      "lang": "en",
      "text": "the mean and standard deviation here",
      "offset": 4354.6,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "might take on slightly different numbers",
      "offset": 4356.08,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "because there's only 32 examples we are",
      "offset": 4357.8,
      "duration": 3.24
    },
    {
      "lang": "en",
      "text": "using to estimate the mean and standard",
      "offset": 4359.36,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "deviation so the value is changing",
      "offset": 4361.04,
      "duration": 5.159
    },
    {
      "lang": "en",
      "text": "around a lot and if your momentum is 0.1",
      "offset": 4362.96,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "that that might not be good enough for",
      "offset": 4366.199,
      "duration": 5.401
    },
    {
      "lang": "en",
      "text": "this value to settle and um converge to",
      "offset": 4367.92,
      "duration": 5.239
    },
    {
      "lang": "en",
      "text": "the actual mean and standard deviation",
      "offset": 4371.6,
      "duration": 3.8
    },
    {
      "lang": "en",
      "text": "over the entire training set and so",
      "offset": 4373.159,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "basically if your batch size is very",
      "offset": 4375.4,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "small uh momentum of 0.1 is potentially",
      "offset": 4376.679,
      "duration": 4.201
    },
    {
      "lang": "en",
      "text": "dangerous and it might make it so that",
      "offset": 4379.08,
      "duration": 3.8
    },
    {
      "lang": "en",
      "text": "the running uh mean and stand deviation",
      "offset": 4380.88,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "are is thrashing too much during",
      "offset": 4382.88,
      "duration": 3.48
    },
    {
      "lang": "en",
      "text": "training and it's not actually",
      "offset": 4384.639,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "converging",
      "offset": 4386.36,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "properly uh aine equals true determines",
      "offset": 4388.239,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "whether this batch normalization layer",
      "offset": 4391.4,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "has these learnable Aline parameters the",
      "offset": 4393.04,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "uh the gain and the bias and this is",
      "offset": 4396.36,
      "duration": 4.68
    },
    {
      "lang": "en",
      "text": "almost always kept to true I'm not",
      "offset": 4399.28,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "actually sure why you would want to",
      "offset": 4401.04,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "change this to false um",
      "offset": 4402.8,
      "duration": 5.8
    },
    {
      "lang": "en",
      "text": "then track running stats is determining",
      "offset": 4406.48,
      "duration": 3.88
    },
    {
      "lang": "en",
      "text": "whether or not B rization layer of",
      "offset": 4408.6,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "pytorch will be doing",
      "offset": 4410.36,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "this and um one reason you may you may",
      "offset": 4411.88,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "want to skip the running stats is",
      "offset": 4415.239,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "because you may want to for example",
      "offset": 4417.88,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "estimate them at the end as a stage two",
      "offset": 4419.159,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "like this and in that case you don't",
      "offset": 4422.52,
      "duration": 2.84
    },
    {
      "lang": "en",
      "text": "want the bat normalization layer to be",
      "offset": 4423.96,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "doing all this extra compute that you're",
      "offset": 4425.36,
      "duration": 2.6
    },
    {
      "lang": "en",
      "text": "not going to",
      "offset": 4426.76,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "use and uh finally we need to know which",
      "offset": 4427.96,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "device we're going to run this bash",
      "offset": 4430.84,
      "duration": 4.52
    },
    {
      "lang": "en",
      "text": "normalization on a CPU or a GPU and what",
      "offset": 4432.44,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "the data type should be uh half",
      "offset": 4435.36,
      "duration": 3.08
    },
    {
      "lang": "en",
      "text": "Precision single Precision double",
      "offset": 4437.08,
      "duration": 2.92
    },
    {
      "lang": "en",
      "text": "precision and so",
      "offset": 4438.44,
      "duration": 4.12
    },
    {
      "lang": "en",
      "text": "on so that's the bat normalization layer",
      "offset": 4440,
      "duration": 4.12
    },
    {
      "lang": "en",
      "text": "otherwise they link to the paper is the",
      "offset": 4442.56,
      "duration": 3.96
    },
    {
      "lang": "en",
      "text": "same formula we've implemented and",
      "offset": 4444.12,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "everything is the same exactly as we've",
      "offset": 4446.52,
      "duration": 3.4
    },
    {
      "lang": "en",
      "text": "done",
      "offset": 4448.679,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "here okay so that's everything that I",
      "offset": 4449.92,
      "duration": 4.12
    },
    {
      "lang": "en",
      "text": "wanted to cover for this lecture really",
      "offset": 4451.88,
      "duration": 3.4
    },
    {
      "lang": "en",
      "text": "what I wanted to talk about is the",
      "offset": 4454.04,
      "duration": 2.679
    },
    {
      "lang": "en",
      "text": "importance of understanding the",
      "offset": 4455.28,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "activations and the gradients and their",
      "offset": 4456.719,
      "duration": 4.121
    },
    {
      "lang": "en",
      "text": "statistics in neural networks and this",
      "offset": 4458.56,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "becomes increasingly important",
      "offset": 4460.84,
      "duration": 2.399
    },
    {
      "lang": "en",
      "text": "especially as you make your neural",
      "offset": 4462.08,
      "duration": 3.72
    },
    {
      "lang": "en",
      "text": "networks bigger larger and deeper",
      "offset": 4463.239,
      "duration": 4.041
    },
    {
      "lang": "en",
      "text": "we looked at the distributions basically",
      "offset": 4465.8,
      "duration": 3.48
    },
    {
      "lang": "en",
      "text": "at the output layer and we saw that if",
      "offset": 4467.28,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "you have two confident mispredictions",
      "offset": 4469.28,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "because the activations are too messed",
      "offset": 4471.84,
      "duration": 3.879
    },
    {
      "lang": "en",
      "text": "up at the last layer you can end up with",
      "offset": 4473.679,
      "duration": 4.761
    },
    {
      "lang": "en",
      "text": "these hockey stick losses and if you fix",
      "offset": 4475.719,
      "duration": 4.201
    },
    {
      "lang": "en",
      "text": "this you get a better loss at the end of",
      "offset": 4478.44,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "training because your training is not",
      "offset": 4479.92,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "doing wasteful work then we also saw",
      "offset": 4481.8,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "that we need to control the activations",
      "offset": 4484.56,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "we don't want them to uh you know squash",
      "offset": 4485.96,
      "duration": 5.16
    },
    {
      "lang": "en",
      "text": "to zero or explode to infinity and",
      "offset": 4488.32,
      "duration": 4.2
    },
    {
      "lang": "en",
      "text": "because that you can run into a lot of",
      "offset": 4491.12,
      "duration": 2.76
    },
    {
      "lang": "en",
      "text": "trouble with all of these uh",
      "offset": 4492.52,
      "duration": 3.48
    },
    {
      "lang": "en",
      "text": "nonlinearities and these neural Nets and",
      "offset": 4493.88,
      "duration": 3.279
    },
    {
      "lang": "en",
      "text": "basically you want everything to be",
      "offset": 4496,
      "duration": 2.76
    },
    {
      "lang": "en",
      "text": "fairly homogeneous throughout the neural",
      "offset": 4497.159,
      "duration": 3.321
    },
    {
      "lang": "en",
      "text": "net you want roughly goshan activations",
      "offset": 4498.76,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "throughout the neural net let me talked",
      "offset": 4500.48,
      "duration": 5.239
    },
    {
      "lang": "en",
      "text": "about okay if we want roughly Gan",
      "offset": 4503.239,
      "duration": 4.841
    },
    {
      "lang": "en",
      "text": "activations how do we scale these weight",
      "offset": 4505.719,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "matrices and biases during",
      "offset": 4508.08,
      "duration": 3.639
    },
    {
      "lang": "en",
      "text": "initialization of the neural nut so that",
      "offset": 4509.719,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "we don't get um you know so everything",
      "offset": 4511.719,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "is as controlled as",
      "offset": 4513.8,
      "duration": 4.76
    },
    {
      "lang": "en",
      "text": "possible um so that give us a large",
      "offset": 4515.719,
      "duration": 5.081
    },
    {
      "lang": "en",
      "text": "boost in Improvement and then I talked",
      "offset": 4518.56,
      "duration": 5.8
    },
    {
      "lang": "en",
      "text": "about how that strategy is not actually",
      "offset": 4520.8,
      "duration": 6.68
    },
    {
      "lang": "en",
      "text": "uh Poss for much much deeper neural nuts",
      "offset": 4524.36,
      "duration": 5.319
    },
    {
      "lang": "en",
      "text": "because um when you have much deeper",
      "offset": 4527.48,
      "duration": 3.719
    },
    {
      "lang": "en",
      "text": "neural nuts with lots of different types",
      "offset": 4529.679,
      "duration": 4.121
    },
    {
      "lang": "en",
      "text": "of layers it becomes really really hard",
      "offset": 4531.199,
      "duration": 4.681
    },
    {
      "lang": "en",
      "text": "to precisely set the weights and the",
      "offset": 4533.8,
      "duration": 3.879
    },
    {
      "lang": "en",
      "text": "biases in such a way that the",
      "offset": 4535.88,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "activations are roughly uniform",
      "offset": 4537.679,
      "duration": 4.121
    },
    {
      "lang": "en",
      "text": "throughout the neural nut so then I",
      "offset": 4539.719,
      "duration": 3.96
    },
    {
      "lang": "en",
      "text": "introduced the notion of a normalization",
      "offset": 4541.8,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "layer now there are many normalization",
      "offset": 4543.679,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "layers that that people use in practice",
      "offset": 4545.56,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "bat normalization layer normalization",
      "offset": 4547.84,
      "duration": 3.799
    },
    {
      "lang": "en",
      "text": "instance normalization group",
      "offset": 4550.199,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "normalization we haven't covered most of",
      "offset": 4551.639,
      "duration": 3.961
    },
    {
      "lang": "en",
      "text": "them but I've introduced the first one",
      "offset": 4553.719,
      "duration": 4.041
    },
    {
      "lang": "en",
      "text": "and also the one that I believe came out",
      "offset": 4555.6,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "first and that's called Bat",
      "offset": 4557.76,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "normalization and we saw how bat",
      "offset": 4559.679,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "normalization Works uh this is a layer",
      "offset": 4561.36,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "that you can sprinkle throughout your",
      "offset": 4563.76,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "deep neural net and the basic idea is if",
      "offset": 4565.04,
      "duration": 5.56
    },
    {
      "lang": "en",
      "text": "you want roughly gsh in activations well",
      "offset": 4568.239,
      "duration": 5.041
    },
    {
      "lang": "en",
      "text": "then take your activations and um take",
      "offset": 4570.6,
      "duration": 4.36
    },
    {
      "lang": "en",
      "text": "the mean and the standard deviation and",
      "offset": 4573.28,
      "duration": 4.28
    },
    {
      "lang": "en",
      "text": "Center your data and you can do that",
      "offset": 4574.96,
      "duration": 5.48
    },
    {
      "lang": "en",
      "text": "because the centering operation is",
      "offset": 4577.56,
      "duration": 5.119
    },
    {
      "lang": "en",
      "text": "differentiable but and on top of that we",
      "offset": 4580.44,
      "duration": 3.719
    },
    {
      "lang": "en",
      "text": "actually had to add a lot of bells and",
      "offset": 4582.679,
      "duration": 3.56
    },
    {
      "lang": "en",
      "text": "whistles and that gave you a sense of",
      "offset": 4584.159,
      "duration": 3.161
    },
    {
      "lang": "en",
      "text": "the complexities of the batch",
      "offset": 4586.239,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "normalization layer because now we're",
      "offset": 4587.32,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "centering the data that's great but",
      "offset": 4589.44,
      "duration": 3.96
    },
    {
      "lang": "en",
      "text": "suddenly we need the gain and the bias",
      "offset": 4591.08,
      "duration": 3.72
    },
    {
      "lang": "en",
      "text": "and now those are",
      "offset": 4593.4,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "trainable and then because we are",
      "offset": 4594.8,
      "duration": 3.8
    },
    {
      "lang": "en",
      "text": "coupling all of the training examples",
      "offset": 4596.76,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "now suddenly the question is how do you",
      "offset": 4598.6,
      "duration": 3.48
    },
    {
      "lang": "en",
      "text": "do the inference where to do to do the",
      "offset": 4599.88,
      "duration": 5.319
    },
    {
      "lang": "en",
      "text": "inference we need to now estimate these",
      "offset": 4602.08,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "um mean and standard deviation once uh",
      "offset": 4605.199,
      "duration": 5.121
    },
    {
      "lang": "en",
      "text": "or the entire training set and then use",
      "offset": 4608.56,
      "duration": 3.96
    },
    {
      "lang": "en",
      "text": "those at inference but then no one likes",
      "offset": 4610.32,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "to do stage two so instead we fold",
      "offset": 4612.52,
      "duration": 3.639
    },
    {
      "lang": "en",
      "text": "everything everything into the bat",
      "offset": 4614.639,
      "duration": 3.281
    },
    {
      "lang": "en",
      "text": "normalization later during training and",
      "offset": 4616.159,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "try to estimate these in the running",
      "offset": 4617.92,
      "duration": 3.799
    },
    {
      "lang": "en",
      "text": "manner so that everything is a bit",
      "offset": 4619.92,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "simpler and that gives us the bat",
      "offset": 4621.719,
      "duration": 5.401
    },
    {
      "lang": "en",
      "text": "normalization layer um and as I",
      "offset": 4623.84,
      "duration": 5.799
    },
    {
      "lang": "en",
      "text": "mentioned no one likes this layer it",
      "offset": 4627.12,
      "duration": 6.039
    },
    {
      "lang": "en",
      "text": "causes a huge amount of bugs um and",
      "offset": 4629.639,
      "duration": 6.04
    },
    {
      "lang": "en",
      "text": "intuitively it's because it is coupling",
      "offset": 4633.159,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "examples um in the for pass of a neural",
      "offset": 4635.679,
      "duration": 6.281
    },
    {
      "lang": "en",
      "text": "nut and uh I've shot myself in the foot",
      "offset": 4637.96,
      "duration": 6.279
    },
    {
      "lang": "en",
      "text": "with this layer over and over again in",
      "offset": 4641.96,
      "duration": 4.84
    },
    {
      "lang": "en",
      "text": "my life and I don't want you to suffer",
      "offset": 4644.239,
      "duration": 5.521
    },
    {
      "lang": "en",
      "text": "the same uh so basically try to avoid it",
      "offset": 4646.8,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "as much as possible uh some of the other",
      "offset": 4649.76,
      "duration": 4.28
    },
    {
      "lang": "en",
      "text": "alternatives to these layers are for",
      "offset": 4652.56,
      "duration": 3.079
    },
    {
      "lang": "en",
      "text": "example group normalization or layer",
      "offset": 4654.04,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "normalization and those have become more",
      "offset": 4655.639,
      "duration": 5.121
    },
    {
      "lang": "en",
      "text": "common uh in more recent deep learning",
      "offset": 4657.639,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "uh but we haven't covered those yet uh",
      "offset": 4660.76,
      "duration": 3.879
    },
    {
      "lang": "en",
      "text": "but definitely bash normalization was",
      "offset": 4663.239,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "very influential at the time when it",
      "offset": 4664.639,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "came out in roughly 2015 because it was",
      "offset": 4666.44,
      "duration": 4.44
    },
    {
      "lang": "en",
      "text": "kind of the first time that you could",
      "offset": 4669.44,
      "duration": 5.08
    },
    {
      "lang": "en",
      "text": "train reliably uh much deeper neural",
      "offset": 4670.88,
      "duration": 5.64
    },
    {
      "lang": "en",
      "text": "nuts and fundamentally the reason for",
      "offset": 4674.52,
      "duration": 4.119
    },
    {
      "lang": "en",
      "text": "that is because this layer was very",
      "offset": 4676.52,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "effective at controlling the statistics",
      "offset": 4678.639,
      "duration": 5.161
    },
    {
      "lang": "en",
      "text": "of the activations in the neural nut so",
      "offset": 4680.76,
      "duration": 6.04
    },
    {
      "lang": "en",
      "text": "that's the story so far and um that's",
      "offset": 4683.8,
      "duration": 4.839
    },
    {
      "lang": "en",
      "text": "all I wanted to cover and in the future",
      "offset": 4686.8,
      "duration": 3.16
    },
    {
      "lang": "en",
      "text": "lectures hopefully we can start going",
      "offset": 4688.639,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "into recurrent R Nets and um recurring",
      "offset": 4689.96,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "neural Nets as we'll see are just very",
      "offset": 4693.199,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "very deep networks because you uh you",
      "offset": 4694.76,
      "duration": 5.08
    },
    {
      "lang": "en",
      "text": "unroll the loop and uh when you actually",
      "offset": 4697.44,
      "duration": 4.68
    },
    {
      "lang": "en",
      "text": "optimize these neurals and that's where",
      "offset": 4699.84,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "a lot of this",
      "offset": 4702.12,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "um analysis around the activation",
      "offset": 4703.76,
      "duration": 4.84
    },
    {
      "lang": "en",
      "text": "statistics and all these normalization",
      "offset": 4706.36,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "layers will become very very important",
      "offset": 4708.6,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "for uh good performance so we'll see",
      "offset": 4710.84,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "that next time bye okay so I lied I",
      "offset": 4713.08,
      "duration": 4.76
    },
    {
      "lang": "en",
      "text": "would like us to do one more summary",
      "offset": 4716.44,
      "duration": 3.88
    },
    {
      "lang": "en",
      "text": "here as a bonus and I think it's useful",
      "offset": 4717.84,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "as to have one more summary of",
      "offset": 4720.32,
      "duration": 2.839
    },
    {
      "lang": "en",
      "text": "everything I've presented in this",
      "offset": 4722,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "lecture but also I would like us to",
      "offset": 4723.159,
      "duration": 4.121
    },
    {
      "lang": "en",
      "text": "start by torify our code a little bit so",
      "offset": 4724.96,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "it looks much more like what you would",
      "offset": 4727.28,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "encounter in PCH so you'll see that I",
      "offset": 4728.48,
      "duration": 4.759
    },
    {
      "lang": "en",
      "text": "will structure our code into these",
      "offset": 4730.96,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "modules like a link",
      "offset": 4733.239,
      "duration": 5.881
    },
    {
      "lang": "en",
      "text": "uh module and a borm module and I'm",
      "offset": 4735.679,
      "duration": 5.801
    },
    {
      "lang": "en",
      "text": "putting the code inside these modules so",
      "offset": 4739.12,
      "duration": 3.72
    },
    {
      "lang": "en",
      "text": "that we can construct neural networks",
      "offset": 4741.48,
      "duration": 2.64
    },
    {
      "lang": "en",
      "text": "very much like we would construct them",
      "offset": 4742.84,
      "duration": 2.839
    },
    {
      "lang": "en",
      "text": "in pytorch and I will go through this in",
      "offset": 4744.12,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "detail so we'll create our neural net",
      "offset": 4745.679,
      "duration": 5.441
    },
    {
      "lang": "en",
      "text": "then we will do the optimization loop as",
      "offset": 4748.84,
      "duration": 4.52
    },
    {
      "lang": "en",
      "text": "we did before and then the one more",
      "offset": 4751.12,
      "duration": 3.4
    },
    {
      "lang": "en",
      "text": "thing that I want to do here is I want",
      "offset": 4753.36,
      "duration": 2.839
    },
    {
      "lang": "en",
      "text": "to look at the activation statistics",
      "offset": 4754.52,
      "duration": 3.639
    },
    {
      "lang": "en",
      "text": "both in the forward pass and in the",
      "offset": 4756.199,
      "duration": 4.04
    },
    {
      "lang": "en",
      "text": "backward pass and then here we have the",
      "offset": 4758.159,
      "duration": 4.761
    },
    {
      "lang": "en",
      "text": "evaluation and sampling just like before",
      "offset": 4760.239,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "so let me rewind all the way up here and",
      "offset": 4762.92,
      "duration": 4.52
    },
    {
      "lang": "en",
      "text": "and go a little bit slower so here I",
      "offset": 4764.719,
      "duration": 5.121
    },
    {
      "lang": "en",
      "text": "creating a linear layer you'll notice",
      "offset": 4767.44,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "that torch.nn has lots of different",
      "offset": 4769.84,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "types of layers and one of those layers",
      "offset": 4771.679,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "is the linear layer torch. n. linear",
      "offset": 4773.52,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "takes a number of input features output",
      "offset": 4776.08,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "features whether or not we should have a",
      "offset": 4777.679,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "bias and then the device that we want to",
      "offset": 4779.12,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "place this layer on and the data type so",
      "offset": 4781.199,
      "duration": 5.601
    },
    {
      "lang": "en",
      "text": "I will emit these two but otherwise we",
      "offset": 4784.08,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "have the exact same thing we have the",
      "offset": 4786.8,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "fan in which is the number of inputs fan",
      "offset": 4788.88,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "out the number of outputs and whether or",
      "offset": 4790.96,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "not we want to use a bias",
      "offset": 4793.76,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "and internally inside this layer there's",
      "offset": 4795.28,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "a weight and a bias if you'd like it it",
      "offset": 4797.04,
      "duration": 5.32
    },
    {
      "lang": "en",
      "text": "is typical to initialize the weight",
      "offset": 4800,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "using um say random numbers drawn from",
      "offset": 4802.36,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "aashan and then here's the coming",
      "offset": 4805.04,
      "duration": 4.28
    },
    {
      "lang": "en",
      "text": "initialization um that we discussed",
      "offset": 4807.32,
      "duration": 3.879
    },
    {
      "lang": "en",
      "text": "already in this lecture and that's a",
      "offset": 4809.32,
      "duration": 3.72
    },
    {
      "lang": "en",
      "text": "good default and also the default that I",
      "offset": 4811.199,
      "duration": 4.281
    },
    {
      "lang": "en",
      "text": "believe pytor chooses and by default the",
      "offset": 4813.04,
      "duration": 5.48
    },
    {
      "lang": "en",
      "text": "bias is usually initialized to zeros now",
      "offset": 4815.48,
      "duration": 5.56
    },
    {
      "lang": "en",
      "text": "when you call this module uh this will",
      "offset": 4818.52,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "basically calculate W * X plus b if you",
      "offset": 4821.04,
      "duration": 4.76
    },
    {
      "lang": "en",
      "text": "have a b and then when you also call",
      "offset": 4823.56,
      "duration": 4.28
    },
    {
      "lang": "en",
      "text": "that parameters on this module it will",
      "offset": 4825.8,
      "duration": 4.52
    },
    {
      "lang": "en",
      "text": "return uh the tensors that are the",
      "offset": 4827.84,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "parameters of this layer now next we",
      "offset": 4830.32,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "have the bash normalization layer so",
      "offset": 4832.88,
      "duration": 5.72
    },
    {
      "lang": "en",
      "text": "I've written that here and this is very",
      "offset": 4835.199,
      "duration": 7.321
    },
    {
      "lang": "en",
      "text": "similar to pytorch nn. bashor 1D layer",
      "offset": 4838.6,
      "duration": 4.92
    },
    {
      "lang": "en",
      "text": "as shown",
      "offset": 4842.52,
      "duration": 4.119
    },
    {
      "lang": "en",
      "text": "here so I'm kind of um taking these",
      "offset": 4843.52,
      "duration": 5.719
    },
    {
      "lang": "en",
      "text": "three parameters here the dimensionality",
      "offset": 4846.639,
      "duration": 3.961
    },
    {
      "lang": "en",
      "text": "the Epsilon that we will use in the",
      "offset": 4849.239,
      "duration": 3.561
    },
    {
      "lang": "en",
      "text": "division and the momentum that we will",
      "offset": 4850.6,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "use in keeping track of these running",
      "offset": 4852.8,
      "duration": 4.12
    },
    {
      "lang": "en",
      "text": "stats the running mean and the running",
      "offset": 4854.679,
      "duration": 4.841
    },
    {
      "lang": "en",
      "text": "variance um now py actually takes quite",
      "offset": 4856.92,
      "duration": 4.36
    },
    {
      "lang": "en",
      "text": "a few more things but I'm assuming some",
      "offset": 4859.52,
      "duration": 3.88
    },
    {
      "lang": "en",
      "text": "of their settings so for us Aline will",
      "offset": 4861.28,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "be true that means that we will be using",
      "offset": 4863.4,
      "duration": 4.6
    },
    {
      "lang": "en",
      "text": "a gamma and beta after the normalization",
      "offset": 4865.12,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "the track running stats will be true so",
      "offset": 4868,
      "duration": 3.159
    },
    {
      "lang": "en",
      "text": "we will be keeping track of the running",
      "offset": 4869.76,
      "duration": 3.32
    },
    {
      "lang": "en",
      "text": "mean and the running variance in the in",
      "offset": 4871.159,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "the bat Norm our device by default is",
      "offset": 4873.08,
      "duration": 5.8
    },
    {
      "lang": "en",
      "text": "the CPU and the data type by default is",
      "offset": 4875.96,
      "duration": 5.239
    },
    {
      "lang": "en",
      "text": "uh float float",
      "offset": 4878.88,
      "duration": 5.359
    },
    {
      "lang": "en",
      "text": "32 so those are the defaults otherwise",
      "offset": 4881.199,
      "duration": 4.841
    },
    {
      "lang": "en",
      "text": "uh we are taking all the same parameters",
      "offset": 4884.239,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "in this bachom layer so first I'm just",
      "offset": 4886.04,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "saving them now here's something new",
      "offset": 4888.159,
      "duration": 4.441
    },
    {
      "lang": "en",
      "text": "there's a doc training which by default",
      "offset": 4891,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "is true and pytorch andn modules also",
      "offset": 4892.6,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "have this attribute. training and that's",
      "offset": 4895,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "because many modules in borm is included",
      "offset": 4897.4,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "in that have a different Behavior",
      "offset": 4900.76,
      "duration": 3.959
    },
    {
      "lang": "en",
      "text": "whether you are training your interet",
      "offset": 4903.08,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "and or whether you are running it in an",
      "offset": 4904.719,
      "duration": 3.96
    },
    {
      "lang": "en",
      "text": "evaluation mode and calculating your",
      "offset": 4906.36,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "evaluation loss or using it for",
      "offset": 4908.679,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "inference on some test examples and",
      "offset": 4910.28,
      "duration": 4.68
    },
    {
      "lang": "en",
      "text": "bashor is an example of this because",
      "offset": 4913.239,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "when we are training we are going to be",
      "offset": 4914.96,
      "duration": 2.84
    },
    {
      "lang": "en",
      "text": "using the mean and the variance",
      "offset": 4916.6,
      "duration": 3.48
    },
    {
      "lang": "en",
      "text": "estimated from the current batch but",
      "offset": 4917.8,
      "duration": 3.96
    },
    {
      "lang": "en",
      "text": "during inference we are using the",
      "offset": 4920.08,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "running mean and running variance and so",
      "offset": 4921.76,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "also if we are training we are updating",
      "offset": 4924.96,
      "duration": 3.88
    },
    {
      "lang": "en",
      "text": "mean and variance but if we are testing",
      "offset": 4926.76,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "then these are not being updated they're",
      "offset": 4928.84,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "kept fixed and so this flag is necessary",
      "offset": 4930.199,
      "duration": 5.121
    },
    {
      "lang": "en",
      "text": "and by default true just like in",
      "offset": 4933.32,
      "duration": 4.76
    },
    {
      "lang": "en",
      "text": "pytorch now the parameters of B 1D are",
      "offset": 4935.32,
      "duration": 5.48
    },
    {
      "lang": "en",
      "text": "the gamma and the beta",
      "offset": 4938.08,
      "duration": 4.84
    },
    {
      "lang": "en",
      "text": "here and then the running mean and",
      "offset": 4940.8,
      "duration": 4.839
    },
    {
      "lang": "en",
      "text": "running variance are called buffers in",
      "offset": 4942.92,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "pyto",
      "offset": 4945.639,
      "duration": 3.801
    },
    {
      "lang": "en",
      "text": "nomenclature and these buffers are",
      "offset": 4946.6,
      "duration": 5.4
    },
    {
      "lang": "en",
      "text": "trained using exponential moving average",
      "offset": 4949.44,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "here explicitly and they are not part of",
      "offset": 4952,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "the back propagation and stochastic",
      "offset": 4954.639,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "radient descent so they are not sort of",
      "offset": 4956,
      "duration": 4.12
    },
    {
      "lang": "en",
      "text": "like parameters of this layer and that's",
      "offset": 4957.679,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "why when we C when we have a parameters",
      "offset": 4960.12,
      "duration": 4.68
    },
    {
      "lang": "en",
      "text": "here we only return gamma and beta we do",
      "offset": 4962.4,
      "duration": 4.2
    },
    {
      "lang": "en",
      "text": "not return the mean and the variance",
      "offset": 4964.8,
      "duration": 4.04
    },
    {
      "lang": "en",
      "text": "this is trained sort of like internally",
      "offset": 4966.6,
      "duration": 5.24
    },
    {
      "lang": "en",
      "text": "here um every forward pass using",
      "offset": 4968.84,
      "duration": 7
    },
    {
      "lang": "en",
      "text": "exponential moving average so that's the",
      "offset": 4971.84,
      "duration": 6.64
    },
    {
      "lang": "en",
      "text": "initialization now in a forward pass if",
      "offset": 4975.84,
      "duration": 4.839
    },
    {
      "lang": "en",
      "text": "we are training then we use the mean and",
      "offset": 4978.48,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "the variance estimated by the batch let",
      "offset": 4980.679,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "me pull up the paper",
      "offset": 4983.36,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "here we calculate the mean and the",
      "offset": 4984.92,
      "duration": 6.04
    },
    {
      "lang": "en",
      "text": "variance now up above I was estimating",
      "offset": 4987.92,
      "duration": 4.84
    },
    {
      "lang": "en",
      "text": "the standard deviation and keeping track",
      "offset": 4990.96,
      "duration": 4.12
    },
    {
      "lang": "en",
      "text": "of the standard deviation here in the",
      "offset": 4992.76,
      "duration": 3.959
    },
    {
      "lang": "en",
      "text": "running standard deviation instead of",
      "offset": 4995.08,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "running variance but let's follow the",
      "offset": 4996.719,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "paper exactly here they calculate the",
      "offset": 4998.84,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "variance which is the standard deviation",
      "offset": 5001.6,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "squared and that's what's get track of",
      "offset": 5003.159,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "in a running variance instead of a",
      "offset": 5005.199,
      "duration": 3.48
    },
    {
      "lang": "en",
      "text": "running standard",
      "offset": 5007.08,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "deviation uh but those two would be very",
      "offset": 5008.679,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "very similar I",
      "offset": 5011,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "believe um if we are not training then",
      "offset": 5012.6,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "we use running mean and variance we",
      "offset": 5014.84,
      "duration": 5.96
    },
    {
      "lang": "en",
      "text": "normalize and then here I am calculating",
      "offset": 5018.12,
      "duration": 4.44
    },
    {
      "lang": "en",
      "text": "the output of this layer and I'm also",
      "offset": 5020.8,
      "duration": 4.76
    },
    {
      "lang": "en",
      "text": "assigning it to an attribute called out",
      "offset": 5022.56,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "now out is something that I'm using in",
      "offset": 5025.56,
      "duration": 4.84
    },
    {
      "lang": "en",
      "text": "our modules here uh this is not what you",
      "offset": 5028.08,
      "duration": 3.88
    },
    {
      "lang": "en",
      "text": "would find in pytorch we are slightly",
      "offset": 5030.4,
      "duration": 4.04
    },
    {
      "lang": "en",
      "text": "deviating from it I'm creating a DOT out",
      "offset": 5031.96,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "because I would like to very easily um",
      "offset": 5034.44,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "maintain all those variables so that we",
      "offset": 5037.32,
      "duration": 3.399
    },
    {
      "lang": "en",
      "text": "can create statistics of them and plot",
      "offset": 5039,
      "duration": 4.199
    },
    {
      "lang": "en",
      "text": "them but pytorch and modules will not",
      "offset": 5040.719,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "have a do out attribute and finally here",
      "offset": 5043.199,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "we are updating the buffers using again",
      "offset": 5045.92,
      "duration": 3.4
    },
    {
      "lang": "en",
      "text": "as I mentioned exponential moving",
      "offset": 5048.08,
      "duration": 3.88
    },
    {
      "lang": "en",
      "text": "average uh provide given the provided",
      "offset": 5049.32,
      "duration": 4.839
    },
    {
      "lang": "en",
      "text": "momentum and importantly you'll notice",
      "offset": 5051.96,
      "duration": 4.36
    },
    {
      "lang": "en",
      "text": "that I'm using the torch. nogra context",
      "offset": 5054.159,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "manager and I doing this because if we",
      "offset": 5056.32,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "don't use this then pytorch will start",
      "offset": 5058.8,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "building out an entire computational",
      "offset": 5060.8,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "graph out of these tensors because it is",
      "offset": 5062.719,
      "duration": 3.96
    },
    {
      "lang": "en",
      "text": "expecting that we will eventually call",
      "offset": 5065.12,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "Dot backward but we are never going to",
      "offset": 5066.679,
      "duration": 3.721
    },
    {
      "lang": "en",
      "text": "be calling dot backward on anything that",
      "offset": 5068.719,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "includes running mean and running",
      "offset": 5070.4,
      "duration": 3.48
    },
    {
      "lang": "en",
      "text": "variance so that's why we need to use",
      "offset": 5071.679,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "this context manager so that we are not",
      "offset": 5073.88,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "um sort of maintaining them using all",
      "offset": 5076.239,
      "duration": 4.601
    },
    {
      "lang": "en",
      "text": "this additional memory um so this will",
      "offset": 5078.44,
      "duration": 3.799
    },
    {
      "lang": "en",
      "text": "make it more efficient and it's just",
      "offset": 5080.84,
      "duration": 2.6
    },
    {
      "lang": "en",
      "text": "telling pyour that there will no",
      "offset": 5082.239,
      "duration": 3.081
    },
    {
      "lang": "en",
      "text": "backward we just have a bunch of tensors",
      "offset": 5083.44,
      "duration": 4.6
    },
    {
      "lang": "en",
      "text": "we want to update them that's it and",
      "offset": 5085.32,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "then we",
      "offset": 5088.04,
      "duration": 3.56
    },
    {
      "lang": "en",
      "text": "return okay now scrolling down we have",
      "offset": 5089.32,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "the 10h layer this is very very similar",
      "offset": 5091.6,
      "duration": 5.88
    },
    {
      "lang": "en",
      "text": "to uh torch. 10h and it doesn't do too",
      "offset": 5093.8,
      "duration": 5.839
    },
    {
      "lang": "en",
      "text": "much it just calculates 10 as you might",
      "offset": 5097.48,
      "duration": 5.639
    },
    {
      "lang": "en",
      "text": "expect so uh that's torch. 10h and uh",
      "offset": 5099.639,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "there's no parameters in this layer but",
      "offset": 5103.119,
      "duration": 4.681
    },
    {
      "lang": "en",
      "text": "because these are layers um it now",
      "offset": 5105.639,
      "duration": 3.801
    },
    {
      "lang": "en",
      "text": "becomes very easy to sort of like stack",
      "offset": 5107.8,
      "duration": 5.64
    },
    {
      "lang": "en",
      "text": "them up into uh basically just a list um",
      "offset": 5109.44,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "and uh we can do all the initializations",
      "offset": 5113.44,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "that we're used to so we have the",
      "offset": 5115.6,
      "duration": 4.119
    },
    {
      "lang": "en",
      "text": "initial sort of embedding Matrix we have",
      "offset": 5117.6,
      "duration": 3.72
    },
    {
      "lang": "en",
      "text": "our layers and we can call them",
      "offset": 5119.719,
      "duration": 4.361
    },
    {
      "lang": "en",
      "text": "sequentially and then again with Tor no",
      "offset": 5121.32,
      "duration": 4.2
    },
    {
      "lang": "en",
      "text": "grb but there's some initializations",
      "offset": 5124.08,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "here so we want to make the output",
      "offset": 5125.52,
      "duration": 4.84
    },
    {
      "lang": "en",
      "text": "softmax a bit less confident like we saw",
      "offset": 5127.44,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "and in addition to that because we are",
      "offset": 5130.36,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "using a six layer multi-layer percep on",
      "offset": 5131.84,
      "duration": 4.68
    },
    {
      "lang": "en",
      "text": "here so you see how I'm stacking linear",
      "offset": 5134.36,
      "duration": 5.48
    },
    {
      "lang": "en",
      "text": "10age linear Tage Etc uh I'm going to be",
      "offset": 5136.52,
      "duration": 5.32
    },
    {
      "lang": "en",
      "text": "using the gain here and I'm going to",
      "offset": 5139.84,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "play with this in a second so you'll see",
      "offset": 5141.84,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "how uh when we change this what happens",
      "offset": 5143.52,
      "duration": 2.719
    },
    {
      "lang": "en",
      "text": "to the",
      "offset": 5145.28,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "statistics finally the parameters are",
      "offset": 5146.239,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "basically the embedding Matrix and all",
      "offset": 5148.4,
      "duration": 4.12
    },
    {
      "lang": "en",
      "text": "the parameters in all the layers and",
      "offset": 5150.639,
      "duration": 3.641
    },
    {
      "lang": "en",
      "text": "notice here I'm using a double list",
      "offset": 5152.52,
      "duration": 3.719
    },
    {
      "lang": "en",
      "text": "apprehension if you want to call it that",
      "offset": 5154.28,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "but for every layer in layers and for",
      "offset": 5156.239,
      "duration": 4.521
    },
    {
      "lang": "en",
      "text": "every parameter in each of those layers",
      "offset": 5158.52,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "we are just stacking up all those piece",
      "offset": 5160.76,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "uh all those parameters now in total we",
      "offset": 5163.239,
      "duration": 5.161
    },
    {
      "lang": "en",
      "text": "have 46,000 um",
      "offset": 5165.8,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "parameters and I'm telling P that all of",
      "offset": 5168.4,
      "duration": 5.319
    },
    {
      "lang": "en",
      "text": "them require",
      "offset": 5170.84,
      "duration": 2.879
    },
    {
      "lang": "en",
      "text": "gradient then here uh we have everything",
      "offset": 5175.08,
      "duration": 5.599
    },
    {
      "lang": "en",
      "text": "here we are actually mostly used to uh",
      "offset": 5178.52,
      "duration": 4.04
    },
    {
      "lang": "en",
      "text": "we are sampling a batch we are doing a",
      "offset": 5180.679,
      "duration": 3.881
    },
    {
      "lang": "en",
      "text": "forward pass the forward pass now is",
      "offset": 5182.56,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "just the linear application of all the",
      "offset": 5184.56,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "layers in order followed by the cross",
      "offset": 5185.92,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "entropy and then in the backward pass",
      "offset": 5188.4,
      "duration": 3.319
    },
    {
      "lang": "en",
      "text": "you'll notice that for every single",
      "offset": 5190.4,
      "duration": 3.719
    },
    {
      "lang": "en",
      "text": "layer I now iterate over all the outputs",
      "offset": 5191.719,
      "duration": 4.121
    },
    {
      "lang": "en",
      "text": "and I'm telling pytorch to retain the",
      "offset": 5194.119,
      "duration": 4.281
    },
    {
      "lang": "en",
      "text": "gradient of them and then here we are",
      "offset": 5195.84,
      "duration": 4.52
    },
    {
      "lang": "en",
      "text": "already used to uh all the all the",
      "offset": 5198.4,
      "duration": 4.04
    },
    {
      "lang": "en",
      "text": "gradient set To None do the backward to",
      "offset": 5200.36,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "fill in the gradients uh do an update",
      "offset": 5202.44,
      "duration": 4.44
    },
    {
      "lang": "en",
      "text": "using stochastic gradient sent and then",
      "offset": 5204.44,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "uh track some statistics and then I am",
      "offset": 5206.88,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "going to break after a single iteration",
      "offset": 5209.4,
      "duration": 4.839
    },
    {
      "lang": "en",
      "text": "now here in this cell in this diagram I",
      "offset": 5212.08,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "I'm visualizing the histogram the",
      "offset": 5214.239,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "histograms of the for pass activations",
      "offset": 5216.08,
      "duration": 4.36
    },
    {
      "lang": "en",
      "text": "and I'm specifically doing it at the 10",
      "offset": 5218.639,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "each layers so iterating over all the",
      "offset": 5220.44,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "layers except for the very last one",
      "offset": 5223.44,
      "duration": 5.08
    },
    {
      "lang": "en",
      "text": "which is basically just the U soft Max",
      "offset": 5225.48,
      "duration": 6.44
    },
    {
      "lang": "en",
      "text": "layer um if it is a 10h layer and I'm",
      "offset": 5228.52,
      "duration": 5.119
    },
    {
      "lang": "en",
      "text": "using a 10h layer just because they have",
      "offset": 5231.92,
      "duration": 3.719
    },
    {
      "lang": "en",
      "text": "a finite output netive 1 to 1 and so",
      "offset": 5233.639,
      "duration": 3.881
    },
    {
      "lang": "en",
      "text": "it's very easy to visualize here so you",
      "offset": 5235.639,
      "duration": 4.361
    },
    {
      "lang": "en",
      "text": "see 1 to one and it's a finite range and",
      "offset": 5237.52,
      "duration": 5.639
    },
    {
      "lang": "en",
      "text": "easy to work with I take the out tensor",
      "offset": 5240,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "from that layer into T and then I'm",
      "offset": 5243.159,
      "duration": 4.201
    },
    {
      "lang": "en",
      "text": "calculating the mean the standard",
      "offset": 5245.84,
      "duration": 3.799
    },
    {
      "lang": "en",
      "text": "deviation and the percent saturation of",
      "offset": 5247.36,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "T and the way I Define the percent",
      "offset": 5249.639,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "saturation is that t. absolute value is",
      "offset": 5251.76,
      "duration": 4.52
    },
    {
      "lang": "en",
      "text": "greater than 97 so that means we are",
      "offset": 5253.96,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "here at the tals of the 10 H and",
      "offset": 5256.28,
      "duration": 3.72
    },
    {
      "lang": "en",
      "text": "remember that when we are in the tales",
      "offset": 5258.76,
      "duration": 3.08
    },
    {
      "lang": "en",
      "text": "of the 10 H that will actually stop",
      "offset": 5260,
      "duration": 3.88
    },
    {
      "lang": "en",
      "text": "gradients so we don't want this to be",
      "offset": 5261.84,
      "duration": 2.839
    },
    {
      "lang": "en",
      "text": "too",
      "offset": 5263.88,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "high now here I'm calling torch.",
      "offset": 5264.679,
      "duration": 5.48
    },
    {
      "lang": "en",
      "text": "histogram and then I am plotting this",
      "offset": 5268.04,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "histogram so basically what this is",
      "offset": 5270.159,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "doing is that every different type of",
      "offset": 5271.96,
      "duration": 3.48
    },
    {
      "lang": "en",
      "text": "layer and they have a different color we",
      "offset": 5273.6,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "are looking at how many um values in",
      "offset": 5275.44,
      "duration": 6.36
    },
    {
      "lang": "en",
      "text": "these tensors take on any of the values",
      "offset": 5278.96,
      "duration": 5.679
    },
    {
      "lang": "en",
      "text": "Below on this axis here so the first",
      "offset": 5281.8,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "layer is fairly saturated uh here at 20%",
      "offset": 5284.639,
      "duration": 5.841
    },
    {
      "lang": "en",
      "text": "so you can see that it's got Tails here",
      "offset": 5287.96,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "but then everything sort of stabilizes",
      "offset": 5290.48,
      "duration": 3.56
    },
    {
      "lang": "en",
      "text": "and if we had more layers here it would",
      "offset": 5292.52,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "actually just stabilize at around the",
      "offset": 5294.04,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "standard deviation of about 65 and the",
      "offset": 5295.4,
      "duration": 5.759
    },
    {
      "lang": "en",
      "text": "saturation would be roughly 5% and the",
      "offset": 5298.28,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "reason that the stabilizes and gives us",
      "offset": 5301.159,
      "duration": 4.121
    },
    {
      "lang": "en",
      "text": "a nice distribution here is because gain",
      "offset": 5302.76,
      "duration": 4.12
    },
    {
      "lang": "en",
      "text": "is set to 5",
      "offset": 5305.28,
      "duration": 6.359
    },
    {
      "lang": "en",
      "text": "over3 now here this gain you see that by",
      "offset": 5306.88,
      "duration": 7.64
    },
    {
      "lang": "en",
      "text": "default we initialize with 1 /un of fan",
      "offset": 5311.639,
      "duration": 5.401
    },
    {
      "lang": "en",
      "text": "in but then here during initialization I",
      "offset": 5314.52,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "come in and I erator all the layers and",
      "offset": 5317.04,
      "duration": 3.88
    },
    {
      "lang": "en",
      "text": "if it's a linear layer I boost that by",
      "offset": 5318.84,
      "duration": 5.839
    },
    {
      "lang": "en",
      "text": "the gain now we saw that one so",
      "offset": 5320.92,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "basically if we just do not use a gain",
      "offset": 5324.679,
      "duration": 5.881
    },
    {
      "lang": "en",
      "text": "then what happens if I redraw this you",
      "offset": 5327.08,
      "duration": 6.039
    },
    {
      "lang": "en",
      "text": "will see that the standard deviation is",
      "offset": 5330.56,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "shrinking and the saturation is coming",
      "offset": 5333.119,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "to zero and basically what's happening",
      "offset": 5335.84,
      "duration": 4.279
    },
    {
      "lang": "en",
      "text": "is the first layer is you know pretty",
      "offset": 5338,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "decent but then further layers are just",
      "offset": 5340.119,
      "duration": 4.921
    },
    {
      "lang": "en",
      "text": "kind of like shrinking down to zero and",
      "offset": 5342.48,
      "duration": 4.12
    },
    {
      "lang": "en",
      "text": "it's happening slowly but it's shrinking",
      "offset": 5345.04,
      "duration": 4.679
    },
    {
      "lang": "en",
      "text": "to zero and the reason for that is when",
      "offset": 5346.6,
      "duration": 5.119
    },
    {
      "lang": "en",
      "text": "you just have a sandwich of linear",
      "offset": 5349.719,
      "duration": 6.241
    },
    {
      "lang": "en",
      "text": "layers alone then a then initializing",
      "offset": 5351.719,
      "duration": 6.641
    },
    {
      "lang": "en",
      "text": "our weights in this manner we saw",
      "offset": 5355.96,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "previously would have conserved the",
      "offset": 5358.36,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "standard deviation of one but because we",
      "offset": 5360.36,
      "duration": 5.16
    },
    {
      "lang": "en",
      "text": "have this interspersed 10 in layers in",
      "offset": 5362.76,
      "duration": 5.959
    },
    {
      "lang": "en",
      "text": "there these 10h layers are squashing",
      "offset": 5365.52,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "functions and so they take your",
      "offset": 5368.719,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "distribution and they slightly squash it",
      "offset": 5370.48,
      "duration": 5.48
    },
    {
      "lang": "en",
      "text": "and so some gain is necessary to keep",
      "offset": 5372.96,
      "duration": 6.12
    },
    {
      "lang": "en",
      "text": "expanding it to fight the",
      "offset": 5375.96,
      "duration": 5.64
    },
    {
      "lang": "en",
      "text": "squashing so it just turns out that 5",
      "offset": 5379.08,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "over3 is a good value so if we have",
      "offset": 5381.6,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "something too small like one we saw that",
      "offset": 5384.08,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "things will come toward zero but if it's",
      "offset": 5386.32,
      "duration": 5.16
    },
    {
      "lang": "en",
      "text": "something too high let's do",
      "offset": 5389.36,
      "duration": 6.68
    },
    {
      "lang": "en",
      "text": "two then here we see that um",
      "offset": 5391.48,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "well let me do something a bit more",
      "offset": 5396.8,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "extreme because so it's a bit more",
      "offset": 5398.28,
      "duration": 3.32
    },
    {
      "lang": "en",
      "text": "visible let's try",
      "offset": 5400,
      "duration": 3.48
    },
    {
      "lang": "en",
      "text": "three okay so we see here that the",
      "offset": 5401.6,
      "duration": 3.88
    },
    {
      "lang": "en",
      "text": "saturations are going to be way too",
      "offset": 5403.48,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "large okay so three would create way too",
      "offset": 5405.48,
      "duration": 6.759
    },
    {
      "lang": "en",
      "text": "saturated activations so 5 over3 is a",
      "offset": 5408.6,
      "duration": 7.2
    },
    {
      "lang": "en",
      "text": "good setting for a sandwich of linear",
      "offset": 5412.239,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "layers with 10h activations and it",
      "offset": 5415.8,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "roughly stabilizes the standard",
      "offset": 5418.159,
      "duration": 4.921
    },
    {
      "lang": "en",
      "text": "deviation at a reasonable point now",
      "offset": 5419.8,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "honestly I have no idea where 5 over3",
      "offset": 5423.08,
      "duration": 4.52
    },
    {
      "lang": "en",
      "text": "came from in pytorch um when we were",
      "offset": 5425,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "looking at the coming initialization um",
      "offset": 5427.6,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "I see empirically that it stabilizes",
      "offset": 5430.04,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "this sandwich of linear an 10age and",
      "offset": 5432.08,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "that the saturation is in a good range",
      "offset": 5434.44,
      "duration": 3.48
    },
    {
      "lang": "en",
      "text": "um but I don't actually know if this",
      "offset": 5436.639,
      "duration": 3.321
    },
    {
      "lang": "en",
      "text": "came out of some math formula I tried",
      "offset": 5437.92,
      "duration": 4.04
    },
    {
      "lang": "en",
      "text": "searching briefly for where this comes",
      "offset": 5439.96,
      "duration": 3.88
    },
    {
      "lang": "en",
      "text": "from uh but I wasn't able to find",
      "offset": 5441.96,
      "duration": 3.64
    },
    {
      "lang": "en",
      "text": "anything uh but certainly we see that",
      "offset": 5443.84,
      "duration": 3.48
    },
    {
      "lang": "en",
      "text": "empirically these are very nice ranges",
      "offset": 5445.6,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "our saturation is roughly 5% which is a",
      "offset": 5447.32,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "pretty good number and uh this is a good",
      "offset": 5449.76,
      "duration": 5.359
    },
    {
      "lang": "en",
      "text": "setting of The gain in this context",
      "offset": 5452.36,
      "duration": 4.68
    },
    {
      "lang": "en",
      "text": "similarly we can do the exact same thing",
      "offset": 5455.119,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "with the gradients so here is a very",
      "offset": 5457.04,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "same Loop if it's a 10h but instead of",
      "offset": 5459.44,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "taking a layer do out I'm taking the",
      "offset": 5461.84,
      "duration": 3.64
    },
    {
      "lang": "en",
      "text": "grad and then I'm also showing the mean",
      "offset": 5463.44,
      "duration": 4.04
    },
    {
      "lang": "en",
      "text": "and the standard deviation and I'm",
      "offset": 5465.48,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "plotting the histogram of these values",
      "offset": 5467.48,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "and so you'll see that the gradient",
      "offset": 5469.96,
      "duration": 3.719
    },
    {
      "lang": "en",
      "text": "distribution is uh fairly reasonable and",
      "offset": 5471,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "in particular what we're looking for is",
      "offset": 5473.679,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "that all the different layers in this",
      "offset": 5475,
      "duration": 4.52
    },
    {
      "lang": "en",
      "text": "sandwich has roughly the same gradient",
      "offset": 5476.719,
      "duration": 5.801
    },
    {
      "lang": "en",
      "text": "things are not shrinking or exploding so",
      "offset": 5479.52,
      "duration": 4.599
    },
    {
      "lang": "en",
      "text": "uh we can for example come here and we",
      "offset": 5482.52,
      "duration": 3.24
    },
    {
      "lang": "en",
      "text": "can take a look at what happens if this",
      "offset": 5484.119,
      "duration": 5.481
    },
    {
      "lang": "en",
      "text": "gain was way too small so this was",
      "offset": 5485.76,
      "duration": 6.879
    },
    {
      "lang": "en",
      "text": "0.5 then you see the first of all the",
      "offset": 5489.6,
      "duration": 4.84
    },
    {
      "lang": "en",
      "text": "activations are shrinking to zero but",
      "offset": 5492.639,
      "duration": 3.161
    },
    {
      "lang": "en",
      "text": "also the gradients are doing something",
      "offset": 5494.44,
      "duration": 3.799
    },
    {
      "lang": "en",
      "text": "weird the gradients started out here and",
      "offset": 5495.8,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "then now they're like expanding",
      "offset": 5498.239,
      "duration": 5.281
    },
    {
      "lang": "en",
      "text": "out and similarly if we for example have",
      "offset": 5500.28,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "a too high of a gain so like",
      "offset": 5503.52,
      "duration": 4.04
    },
    {
      "lang": "en",
      "text": "three then we see that also the",
      "offset": 5505.48,
      "duration": 3.639
    },
    {
      "lang": "en",
      "text": "gradients have there's some asymmetry",
      "offset": 5507.56,
      "duration": 3.48
    },
    {
      "lang": "en",
      "text": "going on where as you go into deeper and",
      "offset": 5509.119,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "deeper layers the activation CS are",
      "offset": 5511.04,
      "duration": 4.36
    },
    {
      "lang": "en",
      "text": "changing and so that's not what we want",
      "offset": 5513.04,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "and in this case we saw that without the",
      "offset": 5515.4,
      "duration": 4.04
    },
    {
      "lang": "en",
      "text": "use of batro as we are going through",
      "offset": 5517.44,
      "duration": 4.759
    },
    {
      "lang": "en",
      "text": "right now we had to very carefully set",
      "offset": 5519.44,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "those gains to get nice activations in",
      "offset": 5522.199,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "both the forward pass and the backward",
      "offset": 5524.96,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "pass now before we move on to bat",
      "offset": 5527,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "normalization I would also like to take",
      "offset": 5529.119,
      "duration": 3.401
    },
    {
      "lang": "en",
      "text": "a look at what happens when we have no",
      "offset": 5531.08,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "10h units here so erasing all the 10",
      "offset": 5532.52,
      "duration": 5.88
    },
    {
      "lang": "en",
      "text": "nonlinearities but keeping the gain at 5",
      "offset": 5535.32,
      "duration": 5.799
    },
    {
      "lang": "en",
      "text": "over3 we now have just a giant linear",
      "offset": 5538.4,
      "duration": 4.44
    },
    {
      "lang": "en",
      "text": "sandwich so let's see what happens to",
      "offset": 5541.119,
      "duration": 3.281
    },
    {
      "lang": "en",
      "text": "the activations",
      "offset": 5542.84,
      "duration": 3.879
    },
    {
      "lang": "en",
      "text": "as we saw before the correct gain here",
      "offset": 5544.4,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "is one that is the standard deviation",
      "offset": 5546.719,
      "duration": 6.92
    },
    {
      "lang": "en",
      "text": "preserving gain so 1.66 7 is too high",
      "offset": 5548.56,
      "duration": 7.639
    },
    {
      "lang": "en",
      "text": "and so what's going to happen now is the",
      "offset": 5553.639,
      "duration": 4.841
    },
    {
      "lang": "en",
      "text": "following uh I have to change this to be",
      "offset": 5556.199,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "linear so we are because there's no more",
      "offset": 5558.48,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "10h layers and let me change this to",
      "offset": 5560.92,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "linear as",
      "offset": 5563.84,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "well so what we're seeing is um the",
      "offset": 5565,
      "duration": 6.6
    },
    {
      "lang": "en",
      "text": "activations started out on the blue and",
      "offset": 5568.48,
      "duration": 6.6
    },
    {
      "lang": "en",
      "text": "have by layer four become very diffuse",
      "offset": 5571.6,
      "duration": 4.76
    },
    {
      "lang": "en",
      "text": "so what's happening to the activations",
      "offset": 5575.08,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "is this and with the gradients on the",
      "offset": 5576.36,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "top layer the activation the gradient",
      "offset": 5579.88,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "statistics are the purple and then they",
      "offset": 5582.6,
      "duration": 4.28
    },
    {
      "lang": "en",
      "text": "diminish as you go down deeper in the",
      "offset": 5585.08,
      "duration": 3.4
    },
    {
      "lang": "en",
      "text": "layers and so basically you have an",
      "offset": 5586.88,
      "duration": 4.2
    },
    {
      "lang": "en",
      "text": "asymmetry like in the neuron net and you",
      "offset": 5588.48,
      "duration": 3.88
    },
    {
      "lang": "en",
      "text": "might imagine that if you have very deep",
      "offset": 5591.08,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "neural networks say like 50 layers or",
      "offset": 5592.36,
      "duration": 4.52
    },
    {
      "lang": "en",
      "text": "something like that this just uh this is",
      "offset": 5594.08,
      "duration": 5.8
    },
    {
      "lang": "en",
      "text": "not a good place to be uh so that's why",
      "offset": 5596.88,
      "duration": 4.92
    },
    {
      "lang": "en",
      "text": "before bash normalization this was",
      "offset": 5599.88,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "incredibly tricky to to set in",
      "offset": 5601.8,
      "duration": 4.6
    },
    {
      "lang": "en",
      "text": "particular if this is too large of a",
      "offset": 5604.44,
      "duration": 3.48
    },
    {
      "lang": "en",
      "text": "gain this happens and if it's too little",
      "offset": 5606.4,
      "duration": 2.36
    },
    {
      "lang": "en",
      "text": "of a",
      "offset": 5607.92,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "gain then this happens so the opposite",
      "offset": 5608.76,
      "duration": 6.2
    },
    {
      "lang": "en",
      "text": "of that basically happens here we have a",
      "offset": 5612,
      "duration": 7.6
    },
    {
      "lang": "en",
      "text": "um shrinking and a uh diffusion",
      "offset": 5614.96,
      "duration": 5.96
    },
    {
      "lang": "en",
      "text": "depending on which direction you look at",
      "offset": 5619.6,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "it from and so certainly this is not",
      "offset": 5620.92,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "what you want and in this case the",
      "offset": 5623.52,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "correct setting of The gain is exactly",
      "offset": 5624.92,
      "duration": 4.279
    },
    {
      "lang": "en",
      "text": "one just like we're doing at",
      "offset": 5627.44,
      "duration": 4.92
    },
    {
      "lang": "en",
      "text": "initialization and then we see that the",
      "offset": 5629.199,
      "duration": 5.081
    },
    {
      "lang": "en",
      "text": "uh statistics for the forward and a",
      "offset": 5632.36,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "backward pass are well behaved and so",
      "offset": 5634.28,
      "duration": 5.64
    },
    {
      "lang": "en",
      "text": "the reason I want to show you this is",
      "offset": 5637.32,
      "duration": 4.76
    },
    {
      "lang": "en",
      "text": "that basically like getting neural nness",
      "offset": 5639.92,
      "duration": 3.799
    },
    {
      "lang": "en",
      "text": "to train before these normalization",
      "offset": 5642.08,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "layers and before the use of advanced",
      "offset": 5643.719,
      "duration": 3.801
    },
    {
      "lang": "en",
      "text": "optimizers like adom which we still have",
      "offset": 5645.84,
      "duration": 3.799
    },
    {
      "lang": "en",
      "text": "to cover and residual connections and so",
      "offset": 5647.52,
      "duration": 4.52
    },
    {
      "lang": "en",
      "text": "on uh training neurs basically looked",
      "offset": 5649.639,
      "duration": 4.841
    },
    {
      "lang": "en",
      "text": "like this it's like a total Balancing",
      "offset": 5652.04,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "Act you have to make sure that",
      "offset": 5654.48,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "everything is precisely orchestrated and",
      "offset": 5655.8,
      "duration": 3.72
    },
    {
      "lang": "en",
      "text": "you have to care about the activations",
      "offset": 5658.32,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "and the gradients and their statistics",
      "offset": 5659.52,
      "duration": 3.8
    },
    {
      "lang": "en",
      "text": "and then maybe you can train something",
      "offset": 5661.32,
      "duration": 2.799
    },
    {
      "lang": "en",
      "text": "uh but it was it was basically",
      "offset": 5663.32,
      "duration": 2.399
    },
    {
      "lang": "en",
      "text": "impossible to train very deep networks",
      "offset": 5664.119,
      "duration": 3.08
    },
    {
      "lang": "en",
      "text": "and this is fundamentally the the reason",
      "offset": 5665.719,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "for that you'd have to be very very",
      "offset": 5667.199,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "careful with your",
      "offset": 5669.159,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "initialization um the other point here",
      "offset": 5670.56,
      "duration": 4.92
    },
    {
      "lang": "en",
      "text": "is you might be asking yourself by the",
      "offset": 5673.48,
      "duration": 3.8
    },
    {
      "lang": "en",
      "text": "way I'm not sure if I covered this why",
      "offset": 5675.48,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "do we need these 10h layers at all uh",
      "offset": 5677.28,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "why do we include them and then have to",
      "offset": 5680.76,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "worry about the gain and uh the reason",
      "offset": 5682.159,
      "duration": 3.48
    },
    {
      "lang": "en",
      "text": "for that of course is that if you just",
      "offset": 5684.44,
      "duration": 3.799
    },
    {
      "lang": "en",
      "text": "have a stack of linear layers then",
      "offset": 5685.639,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "certainly we're getting very easily nice",
      "offset": 5688.239,
      "duration": 4.841
    },
    {
      "lang": "en",
      "text": "activations and so on uh but this is",
      "offset": 5690.6,
      "duration": 4.119
    },
    {
      "lang": "en",
      "text": "just massive linear sandwich and it",
      "offset": 5693.08,
      "duration": 3.48
    },
    {
      "lang": "en",
      "text": "turns out that it collapses to a single",
      "offset": 5694.719,
      "duration": 3.48
    },
    {
      "lang": "en",
      "text": "linear layer in terms of its uh",
      "offset": 5696.56,
      "duration": 4.2
    },
    {
      "lang": "en",
      "text": "representation power so if you were to",
      "offset": 5698.199,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "plot the output as a function of the",
      "offset": 5700.76,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "input you're just getting a linear",
      "offset": 5702.36,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "function no matter how many linear",
      "offset": 5703.8,
      "duration": 3.399
    },
    {
      "lang": "en",
      "text": "layers you stack up you still just end",
      "offset": 5705.4,
      "duration": 3.96
    },
    {
      "lang": "en",
      "text": "up with a linear transformation all the",
      "offset": 5707.199,
      "duration": 6.04
    },
    {
      "lang": "en",
      "text": "WX plus BS just collapse into a large WX",
      "offset": 5709.36,
      "duration": 5.799
    },
    {
      "lang": "en",
      "text": "plus b with slightly different W's and",
      "offset": 5713.239,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "slightly different B um but",
      "offset": 5715.159,
      "duration": 3.881
    },
    {
      "lang": "en",
      "text": "interestingly even though the forward",
      "offset": 5717.8,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "pass collapses to just a linear layer",
      "offset": 5719.04,
      "duration": 4.679
    },
    {
      "lang": "en",
      "text": "because of back propagation and uh the",
      "offset": 5721.239,
      "duration": 4.92
    },
    {
      "lang": "en",
      "text": "dynamics of the backward pass the",
      "offset": 5723.719,
      "duration": 5.041
    },
    {
      "lang": "en",
      "text": "optimization natur is not identical you",
      "offset": 5726.159,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "actually end up with uh all kinds of",
      "offset": 5728.76,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "interesting um Dynamics in the backward",
      "offset": 5730.48,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "pass uh because of the uh the way the",
      "offset": 5733.76,
      "duration": 4.52
    },
    {
      "lang": "en",
      "text": "chain Ru is calculating it and so",
      "offset": 5736,
      "duration": 5.08
    },
    {
      "lang": "en",
      "text": "optimizing a linear layer by itself and",
      "offset": 5738.28,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "optimizing a sandwich of 10 linear",
      "offset": 5741.08,
      "duration": 3.8
    },
    {
      "lang": "en",
      "text": "layers in both cases those are just a",
      "offset": 5743,
      "duration": 3.239
    },
    {
      "lang": "en",
      "text": "linear transformation in the forward",
      "offset": 5744.88,
      "duration": 3.08
    },
    {
      "lang": "en",
      "text": "pass but the training Dynamics would be",
      "offset": 5746.239,
      "duration": 3.521
    },
    {
      "lang": "en",
      "text": "different and there's entire papers that",
      "offset": 5747.96,
      "duration": 4.759
    },
    {
      "lang": "en",
      "text": "analyze in fact like infinitely layered",
      "offset": 5749.76,
      "duration": 5.359
    },
    {
      "lang": "en",
      "text": "uh linear layers and and so on and so",
      "offset": 5752.719,
      "duration": 3.801
    },
    {
      "lang": "en",
      "text": "there's a lot of things to that you can",
      "offset": 5755.119,
      "duration": 2.481
    },
    {
      "lang": "en",
      "text": "play with",
      "offset": 5756.52,
      "duration": 3.24
    },
    {
      "lang": "en",
      "text": "there uh but basically the tal",
      "offset": 5757.6,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "linearities allow us to",
      "offset": 5759.76,
      "duration": 7.479
    },
    {
      "lang": "en",
      "text": "um turn this sandwich from just a",
      "offset": 5762.56,
      "duration": 8.32
    },
    {
      "lang": "en",
      "text": "linear uh function into uh a neural",
      "offset": 5767.239,
      "duration": 5.841
    },
    {
      "lang": "en",
      "text": "network that can in principle um",
      "offset": 5770.88,
      "duration": 4.839
    },
    {
      "lang": "en",
      "text": "approximate any arbitrary function okay",
      "offset": 5773.08,
      "duration": 4.84
    },
    {
      "lang": "en",
      "text": "so now I've reset the code to use the",
      "offset": 5775.719,
      "duration": 5.161
    },
    {
      "lang": "en",
      "text": "linear tanh sandwich like before and I",
      "offset": 5777.92,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "reset everything so the gain is 5 over",
      "offset": 5780.88,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "three uh we can run a single step of",
      "offset": 5783.04,
      "duration": 4.119
    },
    {
      "lang": "en",
      "text": "optimization and we can look at the",
      "offset": 5785.28,
      "duration": 3.48
    },
    {
      "lang": "en",
      "text": "activation statistics of the forward",
      "offset": 5787.159,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "pass and the backward pass but I've",
      "offset": 5788.76,
      "duration": 3.72
    },
    {
      "lang": "en",
      "text": "added one more plot here that I think is",
      "offset": 5790.76,
      "duration": 3.08
    },
    {
      "lang": "en",
      "text": "really important to look at when you're",
      "offset": 5792.48,
      "duration": 2.759
    },
    {
      "lang": "en",
      "text": "training your neural nuts and to",
      "offset": 5793.84,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "consider and ultimately what we're doing",
      "offset": 5795.239,
      "duration": 4.041
    },
    {
      "lang": "en",
      "text": "is we're updating the parameters of the",
      "offset": 5797.52,
      "duration": 3.24
    },
    {
      "lang": "en",
      "text": "neural nut so we care about the",
      "offset": 5799.28,
      "duration": 4.12
    },
    {
      "lang": "en",
      "text": "parameters and their values and their",
      "offset": 5800.76,
      "duration": 4.76
    },
    {
      "lang": "en",
      "text": "gradients so here what I'm doing is I'm",
      "offset": 5803.4,
      "duration": 3.279
    },
    {
      "lang": "en",
      "text": "actually iterating over all the",
      "offset": 5805.52,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "parameters available and then I'm only",
      "offset": 5806.679,
      "duration": 5.161
    },
    {
      "lang": "en",
      "text": "um restricting it to the two-dimensional",
      "offset": 5809.119,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "parameters which are basically the",
      "offset": 5811.84,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "weights of the linear layers and I'm",
      "offset": 5812.96,
      "duration": 4.679
    },
    {
      "lang": "en",
      "text": "skipping the biases and I'm skipping the",
      "offset": 5814.96,
      "duration": 5.56
    },
    {
      "lang": "en",
      "text": "um gamas and the betas in the bom just",
      "offset": 5817.639,
      "duration": 5.641
    },
    {
      "lang": "en",
      "text": "for Simplicity but you can also take a",
      "offset": 5820.52,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "look at those as well but what's",
      "offset": 5823.28,
      "duration": 3.399
    },
    {
      "lang": "en",
      "text": "happening with the weights is um",
      "offset": 5824.76,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "instructive by",
      "offset": 5826.679,
      "duration": 3.801
    },
    {
      "lang": "en",
      "text": "itself so here we have all the different",
      "offset": 5827.96,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "weights their shapes uh so this is the",
      "offset": 5830.48,
      "duration": 5.08
    },
    {
      "lang": "en",
      "text": "embedding layer the first linear layer",
      "offset": 5833.4,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "all the way to the very last linear",
      "offset": 5835.56,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "layer and then we have the mean the",
      "offset": 5836.92,
      "duration": 4.04
    },
    {
      "lang": "en",
      "text": "standard deviation of all these",
      "offset": 5838.6,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "parameters the histogram and you can see",
      "offset": 5840.96,
      "duration": 3.88
    },
    {
      "lang": "en",
      "text": "that actually doesn't look that amazing",
      "offset": 5843.239,
      "duration": 3.641
    },
    {
      "lang": "en",
      "text": "so there's some trouble in Paradise even",
      "offset": 5844.84,
      "duration": 3.879
    },
    {
      "lang": "en",
      "text": "though these gradients looked okay",
      "offset": 5846.88,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "there's something weird going on here",
      "offset": 5848.719,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "I'll get to that in a second and the",
      "offset": 5850.32,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "last thing here is the gradient to data",
      "offset": 5852.4,
      "duration": 4.759
    },
    {
      "lang": "en",
      "text": "ratio so sometimes I like to visualize",
      "offset": 5854.88,
      "duration": 4.279
    },
    {
      "lang": "en",
      "text": "this as well because what this gives you",
      "offset": 5857.159,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "a sense of is what is the scale of the",
      "offset": 5859.159,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "gradient compared to the scale of the",
      "offset": 5861.88,
      "duration": 4.6
    },
    {
      "lang": "en",
      "text": "actual values and this is important",
      "offset": 5864.199,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "because we're going to end up taking a",
      "offset": 5866.48,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "step update um that is the learning rate",
      "offset": 5868.28,
      "duration": 5.839
    },
    {
      "lang": "en",
      "text": "times the gradient onto the data",
      "offset": 5871.48,
      "duration": 4.44
    },
    {
      "lang": "en",
      "text": "and so if the gradient has too large of",
      "offset": 5874.119,
      "duration": 3.48
    },
    {
      "lang": "en",
      "text": "magnitude if the numbers in there are",
      "offset": 5875.92,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "too large compared to the numbers in",
      "offset": 5877.599,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "data then you'd be in trouble but in",
      "offset": 5879.52,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "this case the gradient to data is our",
      "offset": 5882,
      "duration": 5.4
    },
    {
      "lang": "en",
      "text": "low numbers so the values inside grad",
      "offset": 5884.239,
      "duration": 5.721
    },
    {
      "lang": "en",
      "text": "are 1,000 times smaller than the values",
      "offset": 5887.4,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "inside data in these weights most of",
      "offset": 5889.96,
      "duration": 5.96
    },
    {
      "lang": "en",
      "text": "them now notably that is not true about",
      "offset": 5893,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "the last layer and so the last layer",
      "offset": 5895.92,
      "duration": 3.719
    },
    {
      "lang": "en",
      "text": "actually here the output layer is a bit",
      "offset": 5898.04,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "of a troublemaker in the way that this",
      "offset": 5899.639,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "is currently arranged because you can",
      "offset": 5901.32,
      "duration": 7.2
    },
    {
      "lang": "en",
      "text": "see that the um last layer here in pink",
      "offset": 5903,
      "duration": 7.679
    },
    {
      "lang": "en",
      "text": "takes on values that are much larger",
      "offset": 5908.52,
      "duration": 6.28
    },
    {
      "lang": "en",
      "text": "than some of the values inside um inside",
      "offset": 5910.679,
      "duration": 5.721
    },
    {
      "lang": "en",
      "text": "the neural nut so the standard",
      "offset": 5914.8,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "deviations are roughly 1 and3 throughout",
      "offset": 5916.4,
      "duration": 5.48
    },
    {
      "lang": "en",
      "text": "except for the last last uh layer which",
      "offset": 5919.04,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "actually has roughly one -2 standard",
      "offset": 5921.88,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "deviation of gradients and so the",
      "offset": 5924.32,
      "duration": 3.399
    },
    {
      "lang": "en",
      "text": "gradients on the last layer are",
      "offset": 5926.36,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "currently about 100 times greater sorry",
      "offset": 5927.719,
      "duration": 5.801
    },
    {
      "lang": "en",
      "text": "10 times greater than all the other",
      "offset": 5931.159,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "weights inside the neural net and so",
      "offset": 5933.52,
      "duration": 4.84
    },
    {
      "lang": "en",
      "text": "that's problematic because in the simple",
      "offset": 5936.159,
      "duration": 4.44
    },
    {
      "lang": "en",
      "text": "stochastic rting theend setup you would",
      "offset": 5938.36,
      "duration": 4.359
    },
    {
      "lang": "en",
      "text": "be training this last layer about 10",
      "offset": 5940.599,
      "duration": 3.961
    },
    {
      "lang": "en",
      "text": "times faster than you would be training",
      "offset": 5942.719,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "the other layers at",
      "offset": 5944.56,
      "duration": 3.72
    },
    {
      "lang": "en",
      "text": "initialization now this actually like",
      "offset": 5946.239,
      "duration": 3.96
    },
    {
      "lang": "en",
      "text": "kind of fixes itself a little bit if you",
      "offset": 5948.28,
      "duration": 3.8
    },
    {
      "lang": "en",
      "text": "train for a bit longer so for example if",
      "offset": 5950.199,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "I greater than 1,000 only then do a",
      "offset": 5952.08,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "break let me reinitialize and then let",
      "offset": 5955.199,
      "duration": 5.721
    },
    {
      "lang": "en",
      "text": "me do it 1,000 steps and after 1,000",
      "offset": 5957.76,
      "duration": 6.479
    },
    {
      "lang": "en",
      "text": "steps we can look at the forward pass",
      "offset": 5960.92,
      "duration": 5.08
    },
    {
      "lang": "en",
      "text": "okay so you see how the neurons are a",
      "offset": 5964.239,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "bit are saturating a bit and we can also",
      "offset": 5966,
      "duration": 4.52
    },
    {
      "lang": "en",
      "text": "look at the backward pass but otherwise",
      "offset": 5968.4,
      "duration": 4.199
    },
    {
      "lang": "en",
      "text": "they look good they're about equal and",
      "offset": 5970.52,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "there's no shrinking to zero or",
      "offset": 5972.599,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "exploding to Infinities and you can see",
      "offset": 5974.199,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "that here in the weights uh things are",
      "offset": 5976.679,
      "duration": 4.361
    },
    {
      "lang": "en",
      "text": "also stabilizing a little bit so the",
      "offset": 5979,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "Tails of the last pink layer are",
      "offset": 5981.04,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "actually coming coming in during the",
      "offset": 5982.92,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "optimization but certainly this is like",
      "offset": 5985.28,
      "duration": 4.04
    },
    {
      "lang": "en",
      "text": "a little bit of troubling especially if",
      "offset": 5987.239,
      "duration": 3.801
    },
    {
      "lang": "en",
      "text": "you are using a very simple update rule",
      "offset": 5989.32,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "like stochastic gradient descent instead",
      "offset": 5991.04,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "of a modern Optimizer like Adam now I'd",
      "offset": 5992.84,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "like to show you one more plot that I",
      "offset": 5995.52,
      "duration": 2.599
    },
    {
      "lang": "en",
      "text": "usually look at when I train neural",
      "offset": 5996.84,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "networks and basically the gradient to",
      "offset": 5998.119,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "data ratio is not actually that",
      "offset": 6001.159,
      "duration": 3.08
    },
    {
      "lang": "en",
      "text": "informative because what matters at the",
      "offset": 6002.52,
      "duration": 3.719
    },
    {
      "lang": "en",
      "text": "end is not the gradient to data ratio",
      "offset": 6004.239,
      "duration": 4.44
    },
    {
      "lang": "en",
      "text": "but the update to the data ratio because",
      "offset": 6006.239,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "that is the amount by which we will",
      "offset": 6008.679,
      "duration": 3.241
    },
    {
      "lang": "en",
      "text": "actually change the data in these",
      "offset": 6010,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "tensors so coming up here what I'd like",
      "offset": 6011.92,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "to do is I'd like to introduce a new",
      "offset": 6014.719,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "update to data uh ratio it's going to be",
      "offset": 6016.639,
      "duration": 4.681
    },
    {
      "lang": "en",
      "text": "list and we're going to build it out",
      "offset": 6020.159,
      "duration": 3.721
    },
    {
      "lang": "en",
      "text": "every single iteration and here I'd like",
      "offset": 6021.32,
      "duration": 5.319
    },
    {
      "lang": "en",
      "text": "to keep track of basically the",
      "offset": 6023.88,
      "duration": 5.279
    },
    {
      "lang": "en",
      "text": "ratio every single",
      "offset": 6026.639,
      "duration": 7.121
    },
    {
      "lang": "en",
      "text": "iteration so without any gradients I'm",
      "offset": 6029.159,
      "duration": 6.401
    },
    {
      "lang": "en",
      "text": "comparing the update which is learning",
      "offset": 6033.76,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "rate times the times the",
      "offset": 6035.56,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "gradient that is the update that we're",
      "offset": 6037.92,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "going to apply to every",
      "offset": 6039.8,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "parameter uh so see I'm iterating over",
      "offset": 6041.44,
      "duration": 3.64
    },
    {
      "lang": "en",
      "text": "all the parameters and then I'm taking",
      "offset": 6043.48,
      "duration": 3.08
    },
    {
      "lang": "en",
      "text": "the basically standard deviation of the",
      "offset": 6045.08,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "update we're going to apply and divided",
      "offset": 6046.56,
      "duration": 6.88
    },
    {
      "lang": "en",
      "text": "by the um actual content the data of of",
      "offset": 6049.96,
      "duration": 5.159
    },
    {
      "lang": "en",
      "text": "that parameter and its standard",
      "offset": 6053.44,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "deviation so this is the ratio of",
      "offset": 6055.119,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "basically how great are the updates to",
      "offset": 6057.599,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "the values in these tensors then we're",
      "offset": 6060.239,
      "duration": 3.641
    },
    {
      "lang": "en",
      "text": "going to take a log of it and actually",
      "offset": 6062.56,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "I'd like to take a log",
      "offset": 6063.88,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "10 um just so it's a nicer",
      "offset": 6065.28,
      "duration": 5.64
    },
    {
      "lang": "en",
      "text": "visualization um so we're going to be",
      "offset": 6069.08,
      "duration": 5.4
    },
    {
      "lang": "en",
      "text": "basically looking at the exponents of uh",
      "offset": 6070.92,
      "duration": 6.44
    },
    {
      "lang": "en",
      "text": "the of this division here and then that",
      "offset": 6074.48,
      "duration": 5.119
    },
    {
      "lang": "en",
      "text": "item to pop out the float and we're",
      "offset": 6077.36,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "going to be keeping track of this for",
      "offset": 6079.599,
      "duration": 2.801
    },
    {
      "lang": "en",
      "text": "all the parameters and adding it to",
      "offset": 6080.8,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "these UD answer so now let me",
      "offset": 6082.4,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "reinitialize and run a th iterations we",
      "offset": 6084.88,
      "duration": 6.04
    },
    {
      "lang": "en",
      "text": "can look at the activations the",
      "offset": 6087.76,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "gradients and the parameter gradients as",
      "offset": 6090.92,
      "duration": 4.199
    },
    {
      "lang": "en",
      "text": "we did before but now I have one more",
      "offset": 6093.04,
      "duration": 3.48
    },
    {
      "lang": "en",
      "text": "plot here to",
      "offset": 6095.119,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "introduce and what's Happening Here is",
      "offset": 6096.52,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "we're are interval parameters and I'm",
      "offset": 6098.48,
      "duration": 4.199
    },
    {
      "lang": "en",
      "text": "constraining it again like I did here to",
      "offset": 6100.76,
      "duration": 2.959
    },
    {
      "lang": "en",
      "text": "just the",
      "offset": 6102.679,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "weights so the number of dimensions in",
      "offset": 6103.719,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "these sensors is two and then I'm",
      "offset": 6106.36,
      "duration": 4.6
    },
    {
      "lang": "en",
      "text": "basically plotting all of these um",
      "offset": 6108.199,
      "duration": 6.201
    },
    {
      "lang": "en",
      "text": "update ratios over time",
      "offset": 6110.96,
      "duration": 6.6
    },
    {
      "lang": "en",
      "text": "so when I plot this I plot those ratios",
      "offset": 6114.4,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "and you can see that they evolve over",
      "offset": 6117.56,
      "duration": 3.159
    },
    {
      "lang": "en",
      "text": "time during initialization they take on",
      "offset": 6118.88,
      "duration": 3.96
    },
    {
      "lang": "en",
      "text": "certain values and then these updates s",
      "offset": 6120.719,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "of like start stabilizing usually during",
      "offset": 6122.84,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "training then the other thing that I'm",
      "offset": 6124.96,
      "duration": 3.32
    },
    {
      "lang": "en",
      "text": "plotting here is I'm plotting here like",
      "offset": 6126.76,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "an approximate value that is a Rough",
      "offset": 6128.28,
      "duration": 4.68
    },
    {
      "lang": "en",
      "text": "Guide for what it roughly should be and",
      "offset": 6130.44,
      "duration": 4.04
    },
    {
      "lang": "en",
      "text": "it should be like roughly",
      "offset": 6132.96,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "one3 and so that means that basically",
      "offset": 6134.48,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "there's some values in the tensor um and",
      "offset": 6137.36,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "they take on certain values and the",
      "offset": 6140.32,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "updates to them at every iteration are",
      "offset": 6142.239,
      "duration": 5.281
    },
    {
      "lang": "en",
      "text": "no more than roughly 1,000th of the",
      "offset": 6144.4,
      "duration": 6.239
    },
    {
      "lang": "en",
      "text": "actual like magnitude in those tensors",
      "offset": 6147.52,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "uh if this was much larger like for",
      "offset": 6150.639,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "example if this was um if the log of",
      "offset": 6152.4,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "this was like say negative 1 this is",
      "offset": 6156.159,
      "duration": 3.48
    },
    {
      "lang": "en",
      "text": "actually updating those values quite a",
      "offset": 6157.92,
      "duration": 4.199
    },
    {
      "lang": "en",
      "text": "lot they're undergoing a lot of change",
      "offset": 6159.639,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "but the reason that the final rate the",
      "offset": 6162.119,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "final uh layer here is an outlier is",
      "offset": 6164.119,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "because this layer was artificially",
      "offset": 6166.76,
      "duration": 5.08
    },
    {
      "lang": "en",
      "text": "shrunk down to keep the soft Max um",
      "offset": 6169.08,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "incom unconfident",
      "offset": 6171.84,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "so here you see how we multiplied The",
      "offset": 6174.44,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "Weight by",
      "offset": 6177.28,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "0.1 uh in the initialization to make the",
      "offset": 6178.199,
      "duration": 6.081
    },
    {
      "lang": "en",
      "text": "last layer prediction less confident",
      "offset": 6180.88,
      "duration": 6.2
    },
    {
      "lang": "en",
      "text": "that made that artificially made the",
      "offset": 6184.28,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "values inside that tensor way too low",
      "offset": 6187.08,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "and that's why we're getting temporarily",
      "offset": 6189.32,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "a very high ratio but you see that that",
      "offset": 6190.76,
      "duration": 4.959
    },
    {
      "lang": "en",
      "text": "stabilizes over time once uh that weight",
      "offset": 6192.76,
      "duration": 5.32
    },
    {
      "lang": "en",
      "text": "starts to learn starts to learn but",
      "offset": 6195.719,
      "duration": 3.4
    },
    {
      "lang": "en",
      "text": "basically I like to look at the",
      "offset": 6198.08,
      "duration": 3.559
    },
    {
      "lang": "en",
      "text": "evolution of this update ratio for all",
      "offset": 6199.119,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "my parameters usually and I like to make",
      "offset": 6201.639,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "sure that it's not too much above onean",
      "offset": 6203.76,
      "duration": 8.24
    },
    {
      "lang": "en",
      "text": "neg3 roughly uh so around3 on this log",
      "offset": 6207.159,
      "duration": 7.641
    },
    {
      "lang": "en",
      "text": "plot if it's below -3 usually that means",
      "offset": 6212,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "that the parameters are not trained fast",
      "offset": 6214.8,
      "duration": 3.799
    },
    {
      "lang": "en",
      "text": "enough so if our learning rate was very",
      "offset": 6216.56,
      "duration": 4.039
    },
    {
      "lang": "en",
      "text": "low let's do that",
      "offset": 6218.599,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "experiment uh let's initialize and then",
      "offset": 6220.599,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "let's actually do a learning rate of say",
      "offset": 6223.4,
      "duration": 5.16
    },
    {
      "lang": "en",
      "text": "one3 here so",
      "offset": 6225.32,
      "duration": 5.399
    },
    {
      "lang": "en",
      "text": "0.001 if your learning rate is way too",
      "offset": 6228.56,
      "duration": 5.159
    },
    {
      "lang": "en",
      "text": "low",
      "offset": 6230.719,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "this plot will typically reveal it so",
      "offset": 6233.84,
      "duration": 5.48
    },
    {
      "lang": "en",
      "text": "you see how all of these updates are way",
      "offset": 6236.52,
      "duration": 5.599
    },
    {
      "lang": "en",
      "text": "too small so the size of the update is",
      "offset": 6239.32,
      "duration": 7.24
    },
    {
      "lang": "en",
      "text": "uh basically uh 10,000 times um in",
      "offset": 6242.119,
      "duration": 7.12
    },
    {
      "lang": "en",
      "text": "magnitude to the size of the numbers in",
      "offset": 6246.56,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "that tensor in the first place so this",
      "offset": 6249.239,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "is a symptom of training way too",
      "offset": 6250.96,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "slow so this is another way to sometimes",
      "offset": 6253.56,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "set the learning rate and to get a sense",
      "offset": 6256,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "of what that learning rate should be and",
      "offset": 6257.4,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "ultimately this is something that you",
      "offset": 6259.28,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "would uh keep track of",
      "offset": 6260.44,
      "duration": 3.96
    },
    {
      "lang": "en",
      "text": "if anything the learning rate here is a",
      "offset": 6265.04,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "little bit on the higher side uh because",
      "offset": 6267.639,
      "duration": 5.401
    },
    {
      "lang": "en",
      "text": "you see that um we're above the black",
      "offset": 6270.08,
      "duration": 5.88
    },
    {
      "lang": "en",
      "text": "line of3 we're somewhere around -2.5",
      "offset": 6273.04,
      "duration": 5.4
    },
    {
      "lang": "en",
      "text": "it's like okay and uh but everything is",
      "offset": 6275.96,
      "duration": 4.12
    },
    {
      "lang": "en",
      "text": "like somewhat stabilizing and so this",
      "offset": 6278.44,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "looks like a pretty decent setting of of",
      "offset": 6280.08,
      "duration": 4.28
    },
    {
      "lang": "en",
      "text": "um learning rates and so on but this is",
      "offset": 6282.28,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "something to look at and when things are",
      "offset": 6284.36,
      "duration": 3.4
    },
    {
      "lang": "en",
      "text": "miscalibrated you will you will see very",
      "offset": 6286.04,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "quickly so for",
      "offset": 6287.76,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "example everything looks pretty well",
      "offset": 6289.4,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "behaved right but just as a comparison",
      "offset": 6291.28,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "when things are not properly calibrated",
      "offset": 6293.48,
      "duration": 3.639
    },
    {
      "lang": "en",
      "text": "what does that look like let me come up",
      "offset": 6295.04,
      "duration": 5.32
    },
    {
      "lang": "en",
      "text": "here and let's say that for example uh",
      "offset": 6297.119,
      "duration": 5.56
    },
    {
      "lang": "en",
      "text": "what do we do let's say that we forgot",
      "offset": 6300.36,
      "duration": 5.56
    },
    {
      "lang": "en",
      "text": "to apply this a fan in normalization so",
      "offset": 6302.679,
      "duration": 4.681
    },
    {
      "lang": "en",
      "text": "the weights inside the linear layers are",
      "offset": 6305.92,
      "duration": 3.799
    },
    {
      "lang": "en",
      "text": "just sampled from aaan and all the",
      "offset": 6307.36,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "stages what happens to our how do we",
      "offset": 6309.719,
      "duration": 5.281
    },
    {
      "lang": "en",
      "text": "notice that something's off well the",
      "offset": 6312.48,
      "duration": 4.28
    },
    {
      "lang": "en",
      "text": "activation plot will tell you whoa your",
      "offset": 6315,
      "duration": 3.96
    },
    {
      "lang": "en",
      "text": "neurons are way too saturated uh the",
      "offset": 6316.76,
      "duration": 4.52
    },
    {
      "lang": "en",
      "text": "gradients are going to be all messed up",
      "offset": 6318.96,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "uh the histogram for these weights are",
      "offset": 6321.28,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "going to be all messed up as well and",
      "offset": 6322.56,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "there's a lot of asymmetry and then if",
      "offset": 6325.36,
      "duration": 3.72
    },
    {
      "lang": "en",
      "text": "we look here I suspect it's all going to",
      "offset": 6327.56,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "be also pretty messed up so uh you see",
      "offset": 6329.08,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "there's a lot of uh discrepancy in how",
      "offset": 6331.96,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "fast these layers are learning and some",
      "offset": 6334.44,
      "duration": 6.239
    },
    {
      "lang": "en",
      "text": "of them are learning way too fast so uh1",
      "offset": 6336.599,
      "duration": 6.04
    },
    {
      "lang": "en",
      "text": "1.5 those are very large numbers in",
      "offset": 6340.679,
      "duration": 4.121
    },
    {
      "lang": "en",
      "text": "terms of this ratio again you should be",
      "offset": 6342.639,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "somewhere around3 and not much more",
      "offset": 6344.8,
      "duration": 4.919
    },
    {
      "lang": "en",
      "text": "about that um so this is how",
      "offset": 6346.8,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "miscalibrations of your neuron nuts are",
      "offset": 6349.719,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "going to manifest and these kinds of",
      "offset": 6351.599,
      "duration": 5.281
    },
    {
      "lang": "en",
      "text": "plots here are a good way of um sort of",
      "offset": 6353.56,
      "duration": 6.159
    },
    {
      "lang": "en",
      "text": "bringing um those miscalibrations sort",
      "offset": 6356.88,
      "duration": 6.44
    },
    {
      "lang": "en",
      "text": "of uh to your attention and so you can",
      "offset": 6359.719,
      "duration": 5.761
    },
    {
      "lang": "en",
      "text": "address them okay so so far we've seen",
      "offset": 6363.32,
      "duration": 3.879
    },
    {
      "lang": "en",
      "text": "that when we have this linear tanh",
      "offset": 6365.48,
      "duration": 3.719
    },
    {
      "lang": "en",
      "text": "sandwich we can actually precisely",
      "offset": 6367.199,
      "duration": 3.561
    },
    {
      "lang": "en",
      "text": "calibrate the gains and make the",
      "offset": 6369.199,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "activations the gradients and the",
      "offset": 6370.76,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "parameters and the updates all look",
      "offset": 6372.56,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "pretty decent but it definitely feels a",
      "offset": 6374.599,
      "duration": 5.321
    },
    {
      "lang": "en",
      "text": "little bit like balancing of a pencil on",
      "offset": 6376.719,
      "duration": 5.96
    },
    {
      "lang": "en",
      "text": "your finger and that's because this gain",
      "offset": 6379.92,
      "duration": 6.12
    },
    {
      "lang": "en",
      "text": "has to be very precisely calibrated so",
      "offset": 6382.679,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "now let's introduce bat normalization",
      "offset": 6386.04,
      "duration": 4.119
    },
    {
      "lang": "en",
      "text": "layers into the fix into the mix and",
      "offset": 6387.32,
      "duration": 5.56
    },
    {
      "lang": "en",
      "text": "let's let's see how that helps fix the",
      "offset": 6390.159,
      "duration": 4.841
    },
    {
      "lang": "en",
      "text": "problem so",
      "offset": 6392.88,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "here I'm going to take the bachom 1D",
      "offset": 6395,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "class and I'm going to start placing it",
      "offset": 6397.36,
      "duration": 5.879
    },
    {
      "lang": "en",
      "text": "inside and as I mentioned before the",
      "offset": 6400.04,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "standard typical place you would place",
      "offset": 6403.239,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "it is between the linear layer so right",
      "offset": 6404.679,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "after it but before the nonlinearity but",
      "offset": 6407.32,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "people have definitely played with that",
      "offset": 6409.32,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "and uh in fact you can get very similar",
      "offset": 6411.32,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "results even if you place it after the",
      "offset": 6413.48,
      "duration": 5.08
    },
    {
      "lang": "en",
      "text": "nonlinearity um and the other thing that",
      "offset": 6415.88,
      "duration": 3.96
    },
    {
      "lang": "en",
      "text": "I wanted to mention is it's totally fine",
      "offset": 6418.56,
      "duration": 3.559
    },
    {
      "lang": "en",
      "text": "to also place it at the end uh after the",
      "offset": 6419.84,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "last linear layer and before the L",
      "offset": 6422.119,
      "duration": 4.841
    },
    {
      "lang": "en",
      "text": "function so this is potentially fine as",
      "offset": 6424.08,
      "duration": 6.4
    },
    {
      "lang": "en",
      "text": "well um and in this case this would be",
      "offset": 6426.96,
      "duration": 5.8
    },
    {
      "lang": "en",
      "text": "output would be WAP",
      "offset": 6430.48,
      "duration": 5.56
    },
    {
      "lang": "en",
      "text": "size um now because the last layer is",
      "offset": 6432.76,
      "duration": 5.879
    },
    {
      "lang": "en",
      "text": "Bash we would not be changing the weight",
      "offset": 6436.04,
      "duration": 4.76
    },
    {
      "lang": "en",
      "text": "to make the softmax less confident we'd",
      "offset": 6438.639,
      "duration": 5.241
    },
    {
      "lang": "en",
      "text": "be changing the gamma because gamma",
      "offset": 6440.8,
      "duration": 5.96
    },
    {
      "lang": "en",
      "text": "remember in the bathroom is the variable",
      "offset": 6443.88,
      "duration": 4.52
    },
    {
      "lang": "en",
      "text": "that multiplicatively interacts with the",
      "offset": 6446.76,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "output of that",
      "offset": 6448.4,
      "duration": 2.839
    },
    {
      "lang": "en",
      "text": "normalization so we can initialize this",
      "offset": 6451.719,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "sandwich now we can train and we can see",
      "offset": 6454.679,
      "duration": 5.161
    },
    {
      "lang": "en",
      "text": "that the activations uh are going to of",
      "offset": 6457.719,
      "duration": 4.92
    },
    {
      "lang": "en",
      "text": "course look uh very good and they are",
      "offset": 6459.84,
      "duration": 4.44
    },
    {
      "lang": "en",
      "text": "going to necessarily look good because",
      "offset": 6462.639,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "now before every single 10h layer there",
      "offset": 6464.28,
      "duration": 5.72
    },
    {
      "lang": "en",
      "text": "is a normalization in the bashor so this",
      "offset": 6466.96,
      "duration": 5.679
    },
    {
      "lang": "en",
      "text": "is unsurprisingly all uh looks pretty",
      "offset": 6470,
      "duration": 4.199
    },
    {
      "lang": "en",
      "text": "good it's going to be standard deviation",
      "offset": 6472.639,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "of roughly 65 2% and roughly equal",
      "offset": 6474.199,
      "duration": 5.081
    },
    {
      "lang": "en",
      "text": "standard deviation throughout the entire",
      "offset": 6477.36,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "layers so everything looks very",
      "offset": 6479.28,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "homogeneous the gradients look good the",
      "offset": 6481.679,
      "duration": 6.601
    },
    {
      "lang": "en",
      "text": "weights look good and their",
      "offset": 6484.88,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "distributions and then the",
      "offset": 6488.28,
      "duration": 5.64
    },
    {
      "lang": "en",
      "text": "updates also look um pretty reasonable",
      "offset": 6490.32,
      "duration": 6.12
    },
    {
      "lang": "en",
      "text": "uh we are going above3 a little bit but",
      "offset": 6493.92,
      "duration": 5.279
    },
    {
      "lang": "en",
      "text": "not by too much so all the parameters",
      "offset": 6496.44,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "are training at roughly the same rate um",
      "offset": 6499.199,
      "duration": 5.48
    },
    {
      "lang": "en",
      "text": "here",
      "offset": 6502.28,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "but now what we've gained is um we are",
      "offset": 6504.679,
      "duration": 5.681
    },
    {
      "lang": "en",
      "text": "going to be slightly less",
      "offset": 6506.76,
      "duration": 6.76
    },
    {
      "lang": "en",
      "text": "um brittle with respect to the gain of",
      "offset": 6510.36,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "these so for example I can make the gain",
      "offset": 6513.52,
      "duration": 6.639
    },
    {
      "lang": "en",
      "text": "be say2 here um which is much much much",
      "offset": 6515.48,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "slower than what we had with the tan",
      "offset": 6520.159,
      "duration": 4.361
    },
    {
      "lang": "en",
      "text": "H but as we'll see the activations will",
      "offset": 6521.96,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "actually be exactly unaffected uh and",
      "offset": 6524.52,
      "duration": 4.119
    },
    {
      "lang": "en",
      "text": "that's because of again this explicit",
      "offset": 6526.96,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "normalization the gradients are going to",
      "offset": 6528.639,
      "duration": 3.721
    },
    {
      "lang": "en",
      "text": "look okay the weight gradients are going",
      "offset": 6530.56,
      "duration": 4.039
    },
    {
      "lang": "en",
      "text": "to look okay okay but actually the",
      "offset": 6532.36,
      "duration": 3.64
    },
    {
      "lang": "en",
      "text": "updates will",
      "offset": 6534.599,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "change and so even though the forward",
      "offset": 6536,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "and backward pass to a very large extent",
      "offset": 6539.239,
      "duration": 3.48
    },
    {
      "lang": "en",
      "text": "look okay because of the backward pass",
      "offset": 6540.8,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "of the Bator and how the scale of the",
      "offset": 6542.719,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "incoming activations interacts in the",
      "offset": 6544.88,
      "duration": 5.719
    },
    {
      "lang": "en",
      "text": "Bator and its uh backward pass this is",
      "offset": 6547.44,
      "duration": 6.679
    },
    {
      "lang": "en",
      "text": "actually changing the um the scale of",
      "offset": 6550.599,
      "duration": 5.841
    },
    {
      "lang": "en",
      "text": "the updates on these parameters so the",
      "offset": 6554.119,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "grades on gradients of these weights are",
      "offset": 6556.44,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "affected so we still don't get it",
      "offset": 6558.679,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "completely free pass to pass in arbitral",
      "offset": 6561.08,
      "duration": 5.639
    },
    {
      "lang": "en",
      "text": "um weights here but it everything else",
      "offset": 6563.48,
      "duration": 5.719
    },
    {
      "lang": "en",
      "text": "is significantly more robust in terms of",
      "offset": 6566.719,
      "duration": 5.4
    },
    {
      "lang": "en",
      "text": "the forward backward and the weight",
      "offset": 6569.199,
      "duration": 4.681
    },
    {
      "lang": "en",
      "text": "gradients it's just that you may have to",
      "offset": 6572.119,
      "duration": 3.801
    },
    {
      "lang": "en",
      "text": "retune your learning rate if you are",
      "offset": 6573.88,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "changing sufficiently the the scale of",
      "offset": 6575.92,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "the activations that are coming into the",
      "offset": 6578.88,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "batch Norms so here for example this um",
      "offset": 6580.239,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "we changed the gains of these linear",
      "offset": 6583.44,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "layers to be greater and we're seeing",
      "offset": 6585.679,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "that the updates are coming out lower as",
      "offset": 6587.599,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "a",
      "offset": 6589.28,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "result and then finally we can also so",
      "offset": 6590.719,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "if we are using borms we don't actually",
      "offset": 6593.199,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "need to necessarily let me reset this to",
      "offset": 6595.28,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "one so there's no gain we don't",
      "offset": 6597.44,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "necessarily even have to um normalize by",
      "offset": 6599.52,
      "duration": 4.84
    },
    {
      "lang": "en",
      "text": "fan in sometimes so if I take out the",
      "offset": 6602.239,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "fan in so these are just now uh random",
      "offset": 6604.36,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "gsh in we'll see that because of borm",
      "offset": 6606.8,
      "duration": 4.12
    },
    {
      "lang": "en",
      "text": "this will actually be relatively well",
      "offset": 6609.48,
      "duration": 4.44
    },
    {
      "lang": "en",
      "text": "behaved",
      "offset": 6610.92,
      "duration": 5.279
    },
    {
      "lang": "en",
      "text": "so the statistic look of course in the",
      "offset": 6613.92,
      "duration": 4.44
    },
    {
      "lang": "en",
      "text": "forward pass look good the gradients",
      "offset": 6616.199,
      "duration": 5.761
    },
    {
      "lang": "en",
      "text": "look good the uh backward uh the weight",
      "offset": 6618.36,
      "duration": 6.12
    },
    {
      "lang": "en",
      "text": "updates look okay A little bit of fat",
      "offset": 6621.96,
      "duration": 3.8
    },
    {
      "lang": "en",
      "text": "tails on some of the",
      "offset": 6624.48,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "layers and uh this looks okay as well",
      "offset": 6625.76,
      "duration": 6.439
    },
    {
      "lang": "en",
      "text": "but as you as you can see uh we're",
      "offset": 6629.28,
      "duration": 4.76
    },
    {
      "lang": "en",
      "text": "significantly below ne3 so we'd have to",
      "offset": 6632.199,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "bump up the learning rate of this bachor",
      "offset": 6634.04,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "uh so that we are training more properly",
      "offset": 6636.52,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "and in particular looking at this",
      "offset": 6639.04,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "roughly looks like we have to 10x the",
      "offset": 6640.76,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "learning rate to get to about",
      "offset": 6642.48,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "one3 so we' come here and we would",
      "offset": 6645.76,
      "duration": 6.28
    },
    {
      "lang": "en",
      "text": "change this to be update of 1.0 and if I",
      "offset": 6648,
      "duration": 6.639
    },
    {
      "lang": "en",
      "text": "reinitialize",
      "offset": 6652.04,
      "duration": 2.599
    },
    {
      "lang": "en",
      "text": "then we'll see that everything still of",
      "offset": 6659.119,
      "duration": 4.761
    },
    {
      "lang": "en",
      "text": "course looks good and now we are roughly",
      "offset": 6660.36,
      "duration": 5.319
    },
    {
      "lang": "en",
      "text": "here and we expect this to be an okay",
      "offset": 6663.88,
      "duration": 4.44
    },
    {
      "lang": "en",
      "text": "training run so long story short we are",
      "offset": 6665.679,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "significantly more robust to the gain of",
      "offset": 6668.32,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "these linear layers whether or not we",
      "offset": 6670.48,
      "duration": 4.44
    },
    {
      "lang": "en",
      "text": "have to apply the fan in and then we can",
      "offset": 6672.4,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "change the gain uh but we actually do",
      "offset": 6674.92,
      "duration": 3.799
    },
    {
      "lang": "en",
      "text": "have to worry a little bit about the",
      "offset": 6677.52,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "update um scales and making sure that uh",
      "offset": 6678.719,
      "duration": 4.761
    },
    {
      "lang": "en",
      "text": "the learning rate is properly calibrated",
      "offset": 6681.84,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "here but this the activations of the",
      "offset": 6683.48,
      "duration": 4.119
    },
    {
      "lang": "en",
      "text": "forward backward pass and the updates",
      "offset": 6685.76,
      "duration": 3.959
    },
    {
      "lang": "en",
      "text": "are are looking significantly more well",
      "offset": 6687.599,
      "duration": 4.441
    },
    {
      "lang": "en",
      "text": "behaved except for the global scale that",
      "offset": 6689.719,
      "duration": 5.121
    },
    {
      "lang": "en",
      "text": "is potentially being adjusted here okay",
      "offset": 6692.04,
      "duration": 4.92
    },
    {
      "lang": "en",
      "text": "so now let me summarize there are three",
      "offset": 6694.84,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "things I was hoping to achieve with this",
      "offset": 6696.96,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "section number one I wanted to introduce",
      "offset": 6698.679,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "you to bat normalization which is one of",
      "offset": 6700.88,
      "duration": 3.64
    },
    {
      "lang": "en",
      "text": "the first modern innovations that we're",
      "offset": 6702.679,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "looking into that helped stabilize very",
      "offset": 6704.52,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "deep neural networks and their training",
      "offset": 6707.079,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "and I hope you understand how the B",
      "offset": 6709.8,
      "duration": 4.2
    },
    {
      "lang": "en",
      "text": "normalization works and um how it would",
      "offset": 6711.32,
      "duration": 5.319
    },
    {
      "lang": "en",
      "text": "be used in a neural network number two I",
      "offset": 6714,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "was hoping to py torify some of our code",
      "offset": 6716.639,
      "duration": 5.361
    },
    {
      "lang": "en",
      "text": "and wrap it up into these uh modules so",
      "offset": 6719.28,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "like linear bash 1D 10h Etc these are",
      "offset": 6722,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "layers or modules and they can be",
      "offset": 6724.96,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "stacked up into neural nuts like Lego",
      "offset": 6727.52,
      "duration": 4.92
    },
    {
      "lang": "en",
      "text": "building blocks and these layers",
      "offset": 6729.52,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "actually exist in pytorch and if you",
      "offset": 6732.44,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "import torch NN then you can actually",
      "offset": 6735.36,
      "duration": 4.2
    },
    {
      "lang": "en",
      "text": "the way I've constructed it you can",
      "offset": 6737.96,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "simply just use pytorch by prepending n",
      "offset": 6739.56,
      "duration": 4.76
    },
    {
      "lang": "en",
      "text": "and Dot to all these different",
      "offset": 6741.8,
      "duration": 5.359
    },
    {
      "lang": "en",
      "text": "layers and actually everything will just",
      "offset": 6744.32,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "work because the API that I've developed",
      "offset": 6747.159,
      "duration": 4.121
    },
    {
      "lang": "en",
      "text": "here is identical to the API that",
      "offset": 6749.52,
      "duration": 4.44
    },
    {
      "lang": "en",
      "text": "pytorch uses and the implementation also",
      "offset": 6751.28,
      "duration": 4.919
    },
    {
      "lang": "en",
      "text": "is basically as far as I'm Weare",
      "offset": 6753.96,
      "duration": 4.52
    },
    {
      "lang": "en",
      "text": "identical to the one in pytorch and",
      "offset": 6756.199,
      "duration": 3.88
    },
    {
      "lang": "en",
      "text": "number three I tried to introduce you to",
      "offset": 6758.48,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "the diagnostic tools that you would use",
      "offset": 6760.079,
      "duration": 3.721
    },
    {
      "lang": "en",
      "text": "to understand whether your neural",
      "offset": 6762.4,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "network is in a good State dynamically",
      "offset": 6763.8,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "so we are looking at the statistics and",
      "offset": 6766.239,
      "duration": 4.121
    },
    {
      "lang": "en",
      "text": "histograms and activation of the forward",
      "offset": 6768.36,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "pass activ activations the backward pass",
      "offset": 6770.36,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "gradients and then also we're looking at",
      "offset": 6773.079,
      "duration": 3.881
    },
    {
      "lang": "en",
      "text": "the weights that are going to be updated",
      "offset": 6775.36,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "as part of stochastic gradi in ascent",
      "offset": 6776.96,
      "duration": 3.119
    },
    {
      "lang": "en",
      "text": "and we're looking at their means",
      "offset": 6778.96,
      "duration": 3.8
    },
    {
      "lang": "en",
      "text": "standard deviations and also the ratio",
      "offset": 6780.079,
      "duration": 5.761
    },
    {
      "lang": "en",
      "text": "of gradients to data or even better the",
      "offset": 6782.76,
      "duration": 6.2
    },
    {
      "lang": "en",
      "text": "updates to data and we saw that",
      "offset": 6785.84,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "typically we don't actually look at it",
      "offset": 6788.96,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "as a single snapshot Frozen in time at",
      "offset": 6790.239,
      "duration": 3.88
    },
    {
      "lang": "en",
      "text": "some particular iteration typically",
      "offset": 6792.159,
      "duration": 4.44
    },
    {
      "lang": "en",
      "text": "people look at this as a over time just",
      "offset": 6794.119,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "like I've done here and they look at",
      "offset": 6796.599,
      "duration": 3.241
    },
    {
      "lang": "en",
      "text": "these update to data ratios and they",
      "offset": 6798.28,
      "duration": 3.48
    },
    {
      "lang": "en",
      "text": "make sure everything looks okay and in",
      "offset": 6799.84,
      "duration": 4.52
    },
    {
      "lang": "en",
      "text": "particular I said said that um",
      "offset": 6801.76,
      "duration": 5.879
    },
    {
      "lang": "en",
      "text": "W3 or basically ne3 on the lock scale is",
      "offset": 6804.36,
      "duration": 5.879
    },
    {
      "lang": "en",
      "text": "a good uh rough euristic for what you",
      "offset": 6807.639,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "want this ratio to be and if it's way",
      "offset": 6810.239,
      "duration": 4.121
    },
    {
      "lang": "en",
      "text": "too high then probably the learning rate",
      "offset": 6812.36,
      "duration": 4.12
    },
    {
      "lang": "en",
      "text": "or the updates are a little too too big",
      "offset": 6814.36,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "and if it's way too small that the",
      "offset": 6816.48,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "learning rate is probably too small so",
      "offset": 6817.56,
      "duration": 3.519
    },
    {
      "lang": "en",
      "text": "that's just some of the things that you",
      "offset": 6819.92,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "may want to play with when you try to",
      "offset": 6821.079,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "get your neural network to uh work with",
      "offset": 6822.92,
      "duration": 2.92
    },
    {
      "lang": "en",
      "text": "very",
      "offset": 6825,
      "duration": 2.76
    },
    {
      "lang": "en",
      "text": "well now there's a number of things I",
      "offset": 6825.84,
      "duration": 4.2
    },
    {
      "lang": "en",
      "text": "did not try to achieve I did not try to",
      "offset": 6827.76,
      "duration": 3.8
    },
    {
      "lang": "en",
      "text": "beat our previous performance as an",
      "offset": 6830.04,
      "duration": 3.079
    },
    {
      "lang": "en",
      "text": "example by introducing using the bash",
      "offset": 6831.56,
      "duration": 4.44
    },
    {
      "lang": "en",
      "text": "layer actually I did try um and I found",
      "offset": 6833.119,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "the new I used the learning rate finding",
      "offset": 6836,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "mechanism that I've described before I",
      "offset": 6838.239,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "tried to train a borm layer a borm",
      "offset": 6840,
      "duration": 4.84
    },
    {
      "lang": "en",
      "text": "neural nut and uh I actually ended up",
      "offset": 6842.239,
      "duration": 4.041
    },
    {
      "lang": "en",
      "text": "with results that are very very similar",
      "offset": 6844.84,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "to what we've obtained before and that's",
      "offset": 6846.28,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "because our performance now is not",
      "offset": 6848.92,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "bottlenecked by the optimization which",
      "offset": 6850.76,
      "duration": 4.68
    },
    {
      "lang": "en",
      "text": "is what borm is helping with the",
      "offset": 6853.159,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "performance at this stage is bottleneck",
      "offset": 6855.44,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "by what I suspect is the context length",
      "offset": 6857.079,
      "duration": 5.721
    },
    {
      "lang": "en",
      "text": "of our context so currently we are",
      "offset": 6859.84,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "taking three characters to predict the",
      "offset": 6862.8,
      "duration": 2.76
    },
    {
      "lang": "en",
      "text": "fourth one and I think we need to go",
      "offset": 6864.079,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "beyond that and we need to look at more",
      "offset": 6865.56,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "powerful architectures like recurrent",
      "offset": 6867.199,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "neural networks and Transformers in",
      "offset": 6869.32,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "order to further push um the lock",
      "offset": 6870.96,
      "duration": 3.719
    },
    {
      "lang": "en",
      "text": "probabilities that we're achieving on",
      "offset": 6873.32,
      "duration": 2.319
    },
    {
      "lang": "en",
      "text": "this data",
      "offset": 6874.679,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "set and I also did not try to have a",
      "offset": 6875.639,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "full explanation of all of these",
      "offset": 6879.079,
      "duration": 3.56
    },
    {
      "lang": "en",
      "text": "activations the gradients and the",
      "offset": 6881.239,
      "duration": 3.161
    },
    {
      "lang": "en",
      "text": "backward pass and the statistics of all",
      "offset": 6882.639,
      "duration": 3.48
    },
    {
      "lang": "en",
      "text": "these gradients and so you may have",
      "offset": 6884.4,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "found some of the parts here un",
      "offset": 6886.119,
      "duration": 2.56
    },
    {
      "lang": "en",
      "text": "intuitive and maybe you're slightly",
      "offset": 6887.36,
      "duration": 3.96
    },
    {
      "lang": "en",
      "text": "confused about okay if I change the uh",
      "offset": 6888.679,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "gain here how come that we need a",
      "offset": 6891.32,
      "duration": 3.48
    },
    {
      "lang": "en",
      "text": "different learning rate and I didn't go",
      "offset": 6893,
      "duration": 3.079
    },
    {
      "lang": "en",
      "text": "into the full detail because you'd have",
      "offset": 6894.8,
      "duration": 2.68
    },
    {
      "lang": "en",
      "text": "to actually look at the backward pass of",
      "offset": 6896.079,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "all these different layers and get an",
      "offset": 6897.48,
      "duration": 3.08
    },
    {
      "lang": "en",
      "text": "intuitive understanding of how that",
      "offset": 6899.079,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "works and I did not go into that in this",
      "offset": 6900.56,
      "duration": 4.679
    },
    {
      "lang": "en",
      "text": "lecture the purpose really was just to",
      "offset": 6903.159,
      "duration": 3.96
    },
    {
      "lang": "en",
      "text": "introduce you to the diagnostic tools",
      "offset": 6905.239,
      "duration": 3.321
    },
    {
      "lang": "en",
      "text": "and what they look like but there's",
      "offset": 6907.119,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "still a lot of work remaining on the",
      "offset": 6908.56,
      "duration": 3.4
    },
    {
      "lang": "en",
      "text": "intuitive level to understand the",
      "offset": 6910.079,
      "duration": 3.881
    },
    {
      "lang": "en",
      "text": "initialization the backward pass and how",
      "offset": 6911.96,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "all of that interacts uh but you",
      "offset": 6913.96,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "shouldn't feel too bad because honestly",
      "offset": 6915.96,
      "duration": 5.4
    },
    {
      "lang": "en",
      "text": "we are getting to The Cutting Edge of",
      "offset": 6918.199,
      "duration": 4.681
    },
    {
      "lang": "en",
      "text": "where the field is",
      "offset": 6921.36,
      "duration": 3.48
    },
    {
      "lang": "en",
      "text": "we certainly haven't I would say soled",
      "offset": 6922.88,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "initialization and we haven't soled back",
      "offset": 6924.84,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "propagation and these are still very",
      "offset": 6927.199,
      "duration": 3.721
    },
    {
      "lang": "en",
      "text": "much an active area of research people",
      "offset": 6929,
      "duration": 3.079
    },
    {
      "lang": "en",
      "text": "are still trying to figure out what is",
      "offset": 6930.92,
      "duration": 2.199
    },
    {
      "lang": "en",
      "text": "the best way to initialize these",
      "offset": 6932.079,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "networks what is the best update rule to",
      "offset": 6933.119,
      "duration": 5.08
    },
    {
      "lang": "en",
      "text": "use um and so on so none of this is",
      "offset": 6935.079,
      "duration": 4.681
    },
    {
      "lang": "en",
      "text": "really solved and we don't really have",
      "offset": 6938.199,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "all the answers to all the to you know",
      "offset": 6939.76,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "all these cases but at least uh you know",
      "offset": 6942.76,
      "duration": 3.879
    },
    {
      "lang": "en",
      "text": "we're making progress and at least we",
      "offset": 6945.44,
      "duration": 3.08
    },
    {
      "lang": "en",
      "text": "have some tools to tell us uh whether or",
      "offset": 6946.639,
      "duration": 4.121
    },
    {
      "lang": "en",
      "text": "not things are on the right track for",
      "offset": 6948.52,
      "duration": 4.52
    },
    {
      "lang": "en",
      "text": "now so",
      "offset": 6950.76,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "I think we've made positive progress in",
      "offset": 6953.04,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "this lecture and I hope you enjoyed that",
      "offset": 6954.44,
      "duration": 4.679
    },
    {
      "lang": "en",
      "text": "and I will see you next time",
      "offset": 6956.04,
      "duration": 3.079
    }
  ],
  "cleanText": null,
  "dumpedAt": "2025-07-21T18:43:24.465Z"
}