{
  "episodeId": "kCc8FmEb1nY",
  "channelSlug": "@andrejkarpathy",
  "title": "Let's build GPT: from scratch, in code, spelled out.",
  "publishedAt": "2023-01-17T16:33:27.000Z",
  "rawLines": [
    {
      "lang": "en",
      "text": "hi everyone so by now you have probably",
      "offset": 0.199,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "heard of chat GPT it has taken the world",
      "offset": 2.52,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "and AI Community by storm and it is a",
      "offset": 4.92,
      "duration": 4.92
    },
    {
      "lang": "en",
      "text": "system that allows you to interact with",
      "offset": 7.56,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "an AI and give it text based tasks so",
      "offset": 9.84,
      "duration": 5.24
    },
    {
      "lang": "en",
      "text": "for example we can ask chat GPT to write",
      "offset": 12.84,
      "duration": 4.12
    },
    {
      "lang": "en",
      "text": "us a small Hau about how important it is",
      "offset": 15.08,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "that people understand Ai and then they",
      "offset": 16.96,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "can use it to improve the world and make",
      "offset": 18.76,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "it more prosperous so when we run this",
      "offset": 20.16,
      "duration": 5.48
    },
    {
      "lang": "en",
      "text": "AI knowledge brings prosperity for all",
      "offset": 23.72,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "to see Embrace its",
      "offset": 25.64,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "power okay not bad and so you could see",
      "offset": 27.4,
      "duration": 4.6
    },
    {
      "lang": "en",
      "text": "that chpt went from left to right and",
      "offset": 29.8,
      "duration": 5.24
    },
    {
      "lang": "en",
      "text": "generated all these words SE sort of",
      "offset": 32,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "sequentially now I asked it already the",
      "offset": 35.04,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "exact same prompt a little bit earlier",
      "offset": 37.44,
      "duration": 3.959
    },
    {
      "lang": "en",
      "text": "and it generated a slightly different",
      "offset": 39.6,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "outcome ai's power to grow ignorance",
      "offset": 41.399,
      "duration": 5.961
    },
    {
      "lang": "en",
      "text": "holds us back learn Prosperity weights",
      "offset": 44,
      "duration": 5.16
    },
    {
      "lang": "en",
      "text": "so uh pretty good in both cases and",
      "offset": 47.36,
      "duration": 3.4
    },
    {
      "lang": "en",
      "text": "slightly different so you can see that",
      "offset": 49.16,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "chat GPT is a probabilistic system and",
      "offset": 50.76,
      "duration": 3.88
    },
    {
      "lang": "en",
      "text": "for any one prompt it can give us",
      "offset": 52.84,
      "duration": 4.519
    },
    {
      "lang": "en",
      "text": "multiple answers sort of uh replying to",
      "offset": 54.64,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "it now this is just one example of a",
      "offset": 57.359,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "problem people have come up with many",
      "offset": 59.76,
      "duration": 3.479
    },
    {
      "lang": "en",
      "text": "many examples and there are entire",
      "offset": 61.44,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "websites that index interactions with",
      "offset": 63.239,
      "duration": 5.441
    },
    {
      "lang": "en",
      "text": "chpt and so many of them are quite",
      "offset": 66,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "humorous explain HTML to me like I'm a",
      "offset": 68.68,
      "duration": 5.88
    },
    {
      "lang": "en",
      "text": "dog uh write release notes for chess 2",
      "offset": 70.96,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "write a note about Elon Musk buying a",
      "offset": 74.56,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "Twitter and so on so as an example uh",
      "offset": 76.4,
      "duration": 5.48
    },
    {
      "lang": "en",
      "text": "please write a breaking news article",
      "offset": 80.56,
      "duration": 2.919
    },
    {
      "lang": "en",
      "text": "about a leaf falling from a",
      "offset": 81.88,
      "duration": 4.68
    },
    {
      "lang": "en",
      "text": "tree uh and a shocking turn of events a",
      "offset": 83.479,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "leaf has fallen from a tree in the local",
      "offset": 86.56,
      "duration": 3.519
    },
    {
      "lang": "en",
      "text": "park Witnesses report that the leaf",
      "offset": 88.04,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "which was previously attached to a",
      "offset": 90.079,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "branch of a tree attached itself and",
      "offset": 91.4,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "fell to the ground very dramatic so you",
      "offset": 93.439,
      "duration": 4.521
    },
    {
      "lang": "en",
      "text": "can see that this is a pretty remarkable",
      "offset": 96.28,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "system and it is what we call a language",
      "offset": 97.96,
      "duration": 5.799
    },
    {
      "lang": "en",
      "text": "model uh because it um it models the",
      "offset": 100.36,
      "duration": 6.039
    },
    {
      "lang": "en",
      "text": "sequence of words or characters or",
      "offset": 103.759,
      "duration": 5.32
    },
    {
      "lang": "en",
      "text": "tokens more generally and it knows how",
      "offset": 106.399,
      "duration": 4.281
    },
    {
      "lang": "en",
      "text": "sort of words follow each other in",
      "offset": 109.079,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "English language and so from its",
      "offset": 110.68,
      "duration": 4.84
    },
    {
      "lang": "en",
      "text": "perspective what it is doing is it is",
      "offset": 112.92,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "completing the sequence so I give it the",
      "offset": 115.52,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "start of a sequence and it completes the",
      "offset": 117.92,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "sequence with the outcome and so it's a",
      "offset": 120.079,
      "duration": 5.281
    },
    {
      "lang": "en",
      "text": "language model in that sense now I would",
      "offset": 122.56,
      "duration": 5.239
    },
    {
      "lang": "en",
      "text": "like to focus on the under the hood of",
      "offset": 125.36,
      "duration": 4.519
    },
    {
      "lang": "en",
      "text": "um under the hood components of what",
      "offset": 127.799,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "makes CH GPT work so what is the neural",
      "offset": 129.879,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "network under the hood that models the",
      "offset": 132.2,
      "duration": 5.679
    },
    {
      "lang": "en",
      "text": "sequence of these words and that comes",
      "offset": 134.36,
      "duration": 5.08
    },
    {
      "lang": "en",
      "text": "from this paper called attention is all",
      "offset": 137.879,
      "duration": 5.241
    },
    {
      "lang": "en",
      "text": "you need in 2017 a landmark paper a",
      "offset": 139.44,
      "duration": 6.32
    },
    {
      "lang": "en",
      "text": "landmark paper in AI that produced and",
      "offset": 143.12,
      "duration": 4.68
    },
    {
      "lang": "en",
      "text": "proposed the Transformer",
      "offset": 145.76,
      "duration": 5.32
    },
    {
      "lang": "en",
      "text": "architecture so GPT is uh short for",
      "offset": 147.8,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "generally generatively pre-trained",
      "offset": 151.08,
      "duration": 4.519
    },
    {
      "lang": "en",
      "text": "Transformer so Transformer is the neuron",
      "offset": 153.4,
      "duration": 3.559
    },
    {
      "lang": "en",
      "text": "nut that actually does all the heavy",
      "offset": 155.599,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "lifting under the hood it comes from",
      "offset": 156.959,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "this paper in 2017 now if you read this",
      "offset": 159.2,
      "duration": 5.319
    },
    {
      "lang": "en",
      "text": "paper this uh reads like a pretty random",
      "offset": 161.92,
      "duration": 4.28
    },
    {
      "lang": "en",
      "text": "machine translation paper and that's",
      "offset": 164.519,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "because I think the authors didn't fully",
      "offset": 166.2,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "anticipate the impact that the",
      "offset": 167.519,
      "duration": 3.881
    },
    {
      "lang": "en",
      "text": "Transformer would have on the field and",
      "offset": 169,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "this architecture that they produced in",
      "offset": 171.4,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "the context of machine translation in",
      "offset": 172.92,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "their case actually ended up taking over",
      "offset": 174.84,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "uh the rest of AI in the next 5 years",
      "offset": 177.319,
      "duration": 5.361
    },
    {
      "lang": "en",
      "text": "after and so this architecture with",
      "offset": 180.04,
      "duration": 5.479
    },
    {
      "lang": "en",
      "text": "minor changes was copy pasted into a",
      "offset": 182.68,
      "duration": 4.919
    },
    {
      "lang": "en",
      "text": "huge amount of applications in AI in",
      "offset": 185.519,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "more recent years and that includes at",
      "offset": 187.599,
      "duration": 6.121
    },
    {
      "lang": "en",
      "text": "the core of chat GPT now we are not",
      "offset": 190.4,
      "duration": 5.08
    },
    {
      "lang": "en",
      "text": "going to what I'd like to do now is I'd",
      "offset": 193.72,
      "duration": 3.879
    },
    {
      "lang": "en",
      "text": "like to build out something like chat",
      "offset": 195.48,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "GPT but uh we're not going to be able to",
      "offset": 197.599,
      "duration": 4.121
    },
    {
      "lang": "en",
      "text": "of course reproduce chat GPT this is a",
      "offset": 199.799,
      "duration": 4.121
    },
    {
      "lang": "en",
      "text": "very serious production grade system it",
      "offset": 201.72,
      "duration": 5.239
    },
    {
      "lang": "en",
      "text": "is trained on uh a good chunk of",
      "offset": 203.92,
      "duration": 5.399
    },
    {
      "lang": "en",
      "text": "internet and then there's a lot of uh",
      "offset": 206.959,
      "duration": 4.28
    },
    {
      "lang": "en",
      "text": "pre-training and fine-tuning stages to",
      "offset": 209.319,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "it and so it's very complicated what I'd",
      "offset": 211.239,
      "duration": 5.161
    },
    {
      "lang": "en",
      "text": "like to focus on is just to train a",
      "offset": 213.4,
      "duration": 5.479
    },
    {
      "lang": "en",
      "text": "Transformer based language model and in",
      "offset": 216.4,
      "duration": 4.039
    },
    {
      "lang": "en",
      "text": "our case it's going to be a character",
      "offset": 218.879,
      "duration": 4.201
    },
    {
      "lang": "en",
      "text": "level language model I still think that",
      "offset": 220.439,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "is uh very educational with respect to",
      "offset": 223.08,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "how these systems work so I don't want",
      "offset": 225,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "to train on the chunk of Internet we",
      "offset": 227.239,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "need a smaller data set in this case I",
      "offset": 228.76,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "propose that we work with uh my favorite",
      "offset": 231.4,
      "duration": 3.72
    },
    {
      "lang": "en",
      "text": "toy data set it's called tiny",
      "offset": 233.319,
      "duration": 4.12
    },
    {
      "lang": "en",
      "text": "Shakespeare and um what it is is",
      "offset": 235.12,
      "duration": 4.039
    },
    {
      "lang": "en",
      "text": "basically it's a concatenation of all of",
      "offset": 237.439,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "the works of sh Shakespeare in my",
      "offset": 239.159,
      "duration": 3.521
    },
    {
      "lang": "en",
      "text": "understanding and so this is all of",
      "offset": 240.879,
      "duration": 4.44
    },
    {
      "lang": "en",
      "text": "Shakespeare in a single file uh this",
      "offset": 242.68,
      "duration": 5.279
    },
    {
      "lang": "en",
      "text": "file is about 1 megab and it's just all",
      "offset": 245.319,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "of",
      "offset": 247.959,
      "duration": 2.84
    },
    {
      "lang": "en",
      "text": "Shakespeare and what we are going to do",
      "offset": 248.68,
      "duration": 4.119
    },
    {
      "lang": "en",
      "text": "now is we're going to basically model",
      "offset": 250.799,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "how these characters uh follow each",
      "offset": 252.799,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "other so for example given a chunk of",
      "offset": 254.48,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "these characters like this uh given some",
      "offset": 256.72,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "context of characters in the past the",
      "offset": 259.68,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "Transformer neural network will look at",
      "offset": 262.56,
      "duration": 3.56
    },
    {
      "lang": "en",
      "text": "the characters that I've highlighted and",
      "offset": 264.56,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "is going to predict that g is likely to",
      "offset": 266.12,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "come next in the sequence and it's going",
      "offset": 268.32,
      "duration": 3.4
    },
    {
      "lang": "en",
      "text": "to do that because we're going to train",
      "offset": 270.36,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "that Transformer on Shakespeare and it's",
      "offset": 271.72,
      "duration": 4.919
    },
    {
      "lang": "en",
      "text": "just going to try to produce uh",
      "offset": 274.36,
      "duration": 5.119
    },
    {
      "lang": "en",
      "text": "character sequences that look like this",
      "offset": 276.639,
      "duration": 4.28
    },
    {
      "lang": "en",
      "text": "and in that process is going to model",
      "offset": 279.479,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "all the patterns inside this data so",
      "offset": 280.919,
      "duration": 4.361
    },
    {
      "lang": "en",
      "text": "once we've trained the system i' just",
      "offset": 283.56,
      "duration": 4.199
    },
    {
      "lang": "en",
      "text": "like to give you a preview we can",
      "offset": 285.28,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "generate infinite Shakespeare and of",
      "offset": 287.759,
      "duration": 4.201
    },
    {
      "lang": "en",
      "text": "course it's a fake thing that looks kind",
      "offset": 289.759,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "of like",
      "offset": 291.96,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "Shakespeare",
      "offset": 293.36,
      "duration": 5.679
    },
    {
      "lang": "en",
      "text": "um apologies for there's some Jank that",
      "offset": 295.96,
      "duration": 6.92
    },
    {
      "lang": "en",
      "text": "I'm not able to resolve in in here but",
      "offset": 299.039,
      "duration": 6.201
    },
    {
      "lang": "en",
      "text": "um you can see how this is going",
      "offset": 302.88,
      "duration": 4.2
    },
    {
      "lang": "en",
      "text": "character by character and it's kind of",
      "offset": 305.24,
      "duration": 4.2
    },
    {
      "lang": "en",
      "text": "like predicting Shakespeare like",
      "offset": 307.08,
      "duration": 5.24
    },
    {
      "lang": "en",
      "text": "language so verily my Lord the sites",
      "offset": 309.44,
      "duration": 6.12
    },
    {
      "lang": "en",
      "text": "have left the again the king coming with",
      "offset": 312.32,
      "duration": 6.719
    },
    {
      "lang": "en",
      "text": "my curses with precious pale and then",
      "offset": 315.56,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "tranos say something else Etc and this",
      "offset": 319.039,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "is just coming out of the Transformer in",
      "offset": 321.24,
      "duration": 3.88
    },
    {
      "lang": "en",
      "text": "a very similar manner as it would come",
      "offset": 323.039,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "out in chat GPT in our case character by",
      "offset": 325.12,
      "duration": 6.199
    },
    {
      "lang": "en",
      "text": "character in chat GPT uh it's coming out",
      "offset": 327.68,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "on the token by token level and tokens",
      "offset": 331.319,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "are these sort of like little subword",
      "offset": 333.28,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "pieces so they're not Word level they're",
      "offset": 335.319,
      "duration": 3.521
    },
    {
      "lang": "en",
      "text": "kind of like word chunk",
      "offset": 336.96,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "level um and now I've already written",
      "offset": 338.84,
      "duration": 6.4
    },
    {
      "lang": "en",
      "text": "this entire code uh to train these",
      "offset": 343.12,
      "duration": 5.72
    },
    {
      "lang": "en",
      "text": "Transformers um and it is in a GitHub",
      "offset": 345.24,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "repository that you can find and it's",
      "offset": 348.84,
      "duration": 2.919
    },
    {
      "lang": "en",
      "text": "called nanog",
      "offset": 350.44,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "GPT so nanog GPT is a repository that",
      "offset": 351.759,
      "duration": 4.921
    },
    {
      "lang": "en",
      "text": "you can find in my GitHub and it's a",
      "offset": 354.36,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "repository for training Transformers um",
      "offset": 356.68,
      "duration": 5.4
    },
    {
      "lang": "en",
      "text": "on any given text and what I think is",
      "offset": 359.24,
      "duration": 3.88
    },
    {
      "lang": "en",
      "text": "interesting about it because there's",
      "offset": 362.08,
      "duration": 2.959
    },
    {
      "lang": "en",
      "text": "many ways to train Transformers but this",
      "offset": 363.12,
      "duration": 3.799
    },
    {
      "lang": "en",
      "text": "is a very simple implementation so it's",
      "offset": 365.039,
      "duration": 5.041
    },
    {
      "lang": "en",
      "text": "just two files of 300 lines of code each",
      "offset": 366.919,
      "duration": 5.56
    },
    {
      "lang": "en",
      "text": "one file defines the GPT model the",
      "offset": 370.08,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "Transformer and one file trains it on",
      "offset": 372.479,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "some given Text data set and here I'm",
      "offset": 374.639,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "showing that if you train it on a open",
      "offset": 377.039,
      "duration": 3.241
    },
    {
      "lang": "en",
      "text": "web Text data set which is a fairly",
      "offset": 378.88,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "large data set of web pages then I",
      "offset": 380.28,
      "duration": 5.16
    },
    {
      "lang": "en",
      "text": "reproduce the the performance of",
      "offset": 382.24,
      "duration": 6.88
    },
    {
      "lang": "en",
      "text": "gpt2 so gpt2 is an early version of open",
      "offset": 385.44,
      "duration": 7.4
    },
    {
      "lang": "en",
      "text": "AI GPT uh from 2017 if I recall",
      "offset": 389.12,
      "duration": 5.359
    },
    {
      "lang": "en",
      "text": "correctly and I've only so far",
      "offset": 392.84,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "reproduced the the smallest 124 million",
      "offset": 394.479,
      "duration": 4.361
    },
    {
      "lang": "en",
      "text": "parameter model uh but basically this is",
      "offset": 396.84,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "just proving that the codebase is",
      "offset": 398.84,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "correctly arranged and I'm able to load",
      "offset": 399.96,
      "duration": 5.4
    },
    {
      "lang": "en",
      "text": "the uh neural network weights that openi",
      "offset": 402.84,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "has released later so you can take a",
      "offset": 405.36,
      "duration": 5.399
    },
    {
      "lang": "en",
      "text": "look at the finished code here in N GPT",
      "offset": 408.28,
      "duration": 3.639
    },
    {
      "lang": "en",
      "text": "but what I would like to do in this",
      "offset": 410.759,
      "duration": 4.361
    },
    {
      "lang": "en",
      "text": "lecture is I would like to basically uh",
      "offset": 411.919,
      "duration": 5.4
    },
    {
      "lang": "en",
      "text": "write this repository from scratch so",
      "offset": 415.12,
      "duration": 4.04
    },
    {
      "lang": "en",
      "text": "we're going to begin with an empty file",
      "offset": 417.319,
      "duration": 3.56
    },
    {
      "lang": "en",
      "text": "and we're we're going to define a",
      "offset": 419.16,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "Transformer piece by piece we're going",
      "offset": 420.879,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "to train it on the tiny Shakespeare data",
      "offset": 423.72,
      "duration": 4.68
    },
    {
      "lang": "en",
      "text": "set and we'll see how we can then uh",
      "offset": 425.759,
      "duration": 5.041
    },
    {
      "lang": "en",
      "text": "generate infinite Shakespeare and of",
      "offset": 428.4,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "course this can copy paste to any",
      "offset": 430.8,
      "duration": 4.04
    },
    {
      "lang": "en",
      "text": "arbitrary Text data set uh that you like",
      "offset": 432.16,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "uh but my goal really here is to just",
      "offset": 434.84,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "make you understand and appreciate uh",
      "offset": 436.4,
      "duration": 6.12
    },
    {
      "lang": "en",
      "text": "how under the hood chat GPT works and um",
      "offset": 438.759,
      "duration": 5.241
    },
    {
      "lang": "en",
      "text": "really all that's required is a",
      "offset": 442.52,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "Proficiency in Python and uh some basic",
      "offset": 444,
      "duration": 5.16
    },
    {
      "lang": "en",
      "text": "understanding of um calculus and",
      "offset": 447.4,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "statistics",
      "offset": 449.16,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "and it would help if you also see my",
      "offset": 450.28,
      "duration": 3.72
    },
    {
      "lang": "en",
      "text": "previous videos on the same YouTube",
      "offset": 452.12,
      "duration": 3.799
    },
    {
      "lang": "en",
      "text": "channel in particular my make more",
      "offset": 454,
      "duration": 6.479
    },
    {
      "lang": "en",
      "text": "series where I um Define smaller and",
      "offset": 455.919,
      "duration": 6.921
    },
    {
      "lang": "en",
      "text": "simpler neural network language models",
      "offset": 460.479,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "uh so multi perceptrons and so on it",
      "offset": 462.84,
      "duration": 4.039
    },
    {
      "lang": "en",
      "text": "really introduces the language modeling",
      "offset": 465.199,
      "duration": 4.201
    },
    {
      "lang": "en",
      "text": "framework and then uh here in this video",
      "offset": 466.879,
      "duration": 3.88
    },
    {
      "lang": "en",
      "text": "we're going to focus on the Transformer",
      "offset": 469.4,
      "duration": 4.12
    },
    {
      "lang": "en",
      "text": "neural network itself okay so I created",
      "offset": 470.759,
      "duration": 6.361
    },
    {
      "lang": "en",
      "text": "a new Google collab uh jup notebook here",
      "offset": 473.52,
      "duration": 5.239
    },
    {
      "lang": "en",
      "text": "and this will allow me to later easily",
      "offset": 477.12,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "share this code that we're going to",
      "offset": 478.759,
      "duration": 3.081
    },
    {
      "lang": "en",
      "text": "develop together uh with you so you can",
      "offset": 480.12,
      "duration": 3.519
    },
    {
      "lang": "en",
      "text": "follow along so this will be in a video",
      "offset": 481.84,
      "duration": 5.24
    },
    {
      "lang": "en",
      "text": "description uh later now here I've just",
      "offset": 483.639,
      "duration": 5.56
    },
    {
      "lang": "en",
      "text": "done some preliminaries I downloaded the",
      "offset": 487.08,
      "duration": 3.72
    },
    {
      "lang": "en",
      "text": "data set the tiny Shakespeare data set",
      "offset": 489.199,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "at this URL and you can see that it's",
      "offset": 490.8,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "about a 1 Megabyte file then here I open",
      "offset": 492.56,
      "duration": 5.24
    },
    {
      "lang": "en",
      "text": "the input.txt file and just read in all",
      "offset": 495.84,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "the text of the string and we see that",
      "offset": 497.8,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "we are working with 1 million characters",
      "offset": 500.08,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "roughly and the first 1,000 characters",
      "offset": 502.44,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "if we just print them out are basically",
      "offset": 504.8,
      "duration": 3.32
    },
    {
      "lang": "en",
      "text": "what you would expect this is the first",
      "offset": 506.68,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "1,000 characters of the tiny Shakespeare",
      "offset": 508.12,
      "duration": 6.64
    },
    {
      "lang": "en",
      "text": "data set roughly up to here so so far so",
      "offset": 510.84,
      "duration": 6.559
    },
    {
      "lang": "en",
      "text": "good next we're going to take this text",
      "offset": 514.76,
      "duration": 4.519
    },
    {
      "lang": "en",
      "text": "and the text is a sequence of characters",
      "offset": 517.399,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "in Python so when I call the set",
      "offset": 519.279,
      "duration": 4.76
    },
    {
      "lang": "en",
      "text": "Constructor on it I'm just going to get",
      "offset": 521.24,
      "duration": 5.48
    },
    {
      "lang": "en",
      "text": "the set of all the characters that occur",
      "offset": 524.039,
      "duration": 5.681
    },
    {
      "lang": "en",
      "text": "in this text and then I call list on",
      "offset": 526.72,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "that to create a list of those",
      "offset": 529.72,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "characters instead of just a set so that",
      "offset": 531.72,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "I have an ordering an arbitrary ordering",
      "offset": 533.399,
      "duration": 5.641
    },
    {
      "lang": "en",
      "text": "and then I sort that so basically we get",
      "offset": 536.279,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "just all the characters that occur in",
      "offset": 539.04,
      "duration": 3.799
    },
    {
      "lang": "en",
      "text": "the entire data set and they're sorted",
      "offset": 540.6,
      "duration": 3.72
    },
    {
      "lang": "en",
      "text": "now the number of them is going to be",
      "offset": 542.839,
      "duration": 3.481
    },
    {
      "lang": "en",
      "text": "our vocabulary size these are the",
      "offset": 544.32,
      "duration": 4.68
    },
    {
      "lang": "en",
      "text": "possible elements of our sequences and",
      "offset": 546.32,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "we see that when I print here the",
      "offset": 549,
      "duration": 5.32
    },
    {
      "lang": "en",
      "text": "characters there's 65 of them in total",
      "offset": 551.32,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "there's a space character and then all",
      "offset": 554.32,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "kinds of special characters and then U",
      "offset": 556.2,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "capitals and lowercase letters so that's",
      "offset": 559.2,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "our vocabulary and that's the sort of",
      "offset": 561.56,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "like possible uh characters that the",
      "offset": 563.2,
      "duration": 6.04
    },
    {
      "lang": "en",
      "text": "model can see or emit okay so next we",
      "offset": 565.8,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "will would like to develop some strategy",
      "offset": 569.24,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "to tokenize the input text now when",
      "offset": 571,
      "duration": 5.88
    },
    {
      "lang": "en",
      "text": "people say tokenize they mean convert",
      "offset": 575,
      "duration": 4.2
    },
    {
      "lang": "en",
      "text": "the raw text as a string to some",
      "offset": 576.88,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "sequence of integers According to some",
      "offset": 579.2,
      "duration": 4.44
    },
    {
      "lang": "en",
      "text": "uh notebook According to some vocabulary",
      "offset": 581.6,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "of possible elements so as an example",
      "offset": 583.64,
      "duration": 4.52
    },
    {
      "lang": "en",
      "text": "here we are going to be building a",
      "offset": 586.88,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "character level language model so we're",
      "offset": 588.16,
      "duration": 2.76
    },
    {
      "lang": "en",
      "text": "simply going to be translating",
      "offset": 589.76,
      "duration": 3.88
    },
    {
      "lang": "en",
      "text": "individual characters into integers so",
      "offset": 590.92,
      "duration": 4.52
    },
    {
      "lang": "en",
      "text": "let me show you uh a chunk of code that",
      "offset": 593.64,
      "duration": 3.8
    },
    {
      "lang": "en",
      "text": "sort of does that for us so we're",
      "offset": 595.44,
      "duration": 3.48
    },
    {
      "lang": "en",
      "text": "building both the encoder and the",
      "offset": 597.44,
      "duration": 2.76
    },
    {
      "lang": "en",
      "text": "decoder",
      "offset": 598.92,
      "duration": 2.28
    },
    {
      "lang": "en",
      "text": "and let me just talk through what's",
      "offset": 600.2,
      "duration": 2.04
    },
    {
      "lang": "en",
      "text": "happening",
      "offset": 601.2,
      "duration": 4.199
    },
    {
      "lang": "en",
      "text": "here when we encode an arbitrary text",
      "offset": 602.24,
      "duration": 5.96
    },
    {
      "lang": "en",
      "text": "like hi there we're going to receive a",
      "offset": 605.399,
      "duration": 5.361
    },
    {
      "lang": "en",
      "text": "list of integers that represents that",
      "offset": 608.2,
      "duration": 6.72
    },
    {
      "lang": "en",
      "text": "string so for example 46 47 Etc and then",
      "offset": 610.76,
      "duration": 6.28
    },
    {
      "lang": "en",
      "text": "we also have the reverse mapping so we",
      "offset": 614.92,
      "duration": 5.4
    },
    {
      "lang": "en",
      "text": "can take this list and decode it to get",
      "offset": 617.04,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "back the exact same string so it's",
      "offset": 620.32,
      "duration": 3.8
    },
    {
      "lang": "en",
      "text": "really just like a translation to",
      "offset": 622.64,
      "duration": 4.28
    },
    {
      "lang": "en",
      "text": "integers and back for arbitrary string",
      "offset": 624.12,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "and for us it is done on a character",
      "offset": 626.92,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "level",
      "offset": 628.6,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "now the way this was achieved is we just",
      "offset": 630.12,
      "duration": 4.04
    },
    {
      "lang": "en",
      "text": "iterate over all the characters here and",
      "offset": 631.959,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "create a lookup table from the character",
      "offset": 634.16,
      "duration": 4.28
    },
    {
      "lang": "en",
      "text": "to the integer and vice versa and then",
      "offset": 635.88,
      "duration": 4.759
    },
    {
      "lang": "en",
      "text": "to encode some string we simply",
      "offset": 638.44,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "translate all the characters",
      "offset": 640.639,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "individually and to decode it back we",
      "offset": 641.639,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "use the reverse mapping and concatenate",
      "offset": 644.56,
      "duration": 4.92
    },
    {
      "lang": "en",
      "text": "all of it now this is only one of many",
      "offset": 646.44,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "possible encodings or many possible sort",
      "offset": 649.48,
      "duration": 4.68
    },
    {
      "lang": "en",
      "text": "of tokenizers and it's a very simple one",
      "offset": 651.44,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "but there's many other schemas that",
      "offset": 654.16,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "people have come up with in practice so",
      "offset": 655.68,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "for example Google uses a sentence",
      "offset": 657.6,
      "duration": 4.52
    },
    {
      "lang": "en",
      "text": "piece uh so sentence piece will also",
      "offset": 659.839,
      "duration": 5.8
    },
    {
      "lang": "en",
      "text": "encode text into um integers but in a",
      "offset": 662.12,
      "duration": 6.399
    },
    {
      "lang": "en",
      "text": "different schema and using a different",
      "offset": 665.639,
      "duration": 5.161
    },
    {
      "lang": "en",
      "text": "vocabulary and sentence piece is a",
      "offset": 668.519,
      "duration": 5.081
    },
    {
      "lang": "en",
      "text": "subword uh sort of tokenizer and what",
      "offset": 670.8,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "that means is that um you're not",
      "offset": 673.6,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "encoding entire words but you're not",
      "offset": 675.519,
      "duration": 4.12
    },
    {
      "lang": "en",
      "text": "also encoding individual characters it's",
      "offset": 677.279,
      "duration": 5.321
    },
    {
      "lang": "en",
      "text": "it's a subword unit level and that's",
      "offset": 679.639,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "usually what's adopted in practice for",
      "offset": 682.6,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "example also openai has this Library",
      "offset": 684.519,
      "duration": 4.361
    },
    {
      "lang": "en",
      "text": "called tick token that uses a bite pair",
      "offset": 686.36,
      "duration": 3.56
    },
    {
      "lang": "en",
      "text": "encode",
      "offset": 688.88,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "tokenizer um and that's what GPT uses",
      "offset": 689.92,
      "duration": 5.8
    },
    {
      "lang": "en",
      "text": "and you can also just encode words into",
      "offset": 693.36,
      "duration": 5.479
    },
    {
      "lang": "en",
      "text": "like hell world into a list of integers",
      "offset": 695.72,
      "duration": 5.08
    },
    {
      "lang": "en",
      "text": "so as an example I'm using the Tik token",
      "offset": 698.839,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "Library here I'm getting the encoding",
      "offset": 700.8,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "for gpt2 or that was used for gpt2",
      "offset": 703.16,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "instead of just having 65 possible",
      "offset": 706.32,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "characters or tokens they have 50,000",
      "offset": 708.16,
      "duration": 6.28
    },
    {
      "lang": "en",
      "text": "tokens and so when they encode the exact",
      "offset": 711.92,
      "duration": 5.24
    },
    {
      "lang": "en",
      "text": "same string High there we only get a",
      "offset": 714.44,
      "duration": 4.839
    },
    {
      "lang": "en",
      "text": "list of three integers but those",
      "offset": 717.16,
      "duration": 4.6
    },
    {
      "lang": "en",
      "text": "integers are not between 0 and 64 they",
      "offset": 719.279,
      "duration": 6.521
    },
    {
      "lang": "en",
      "text": "are between Z and 5,",
      "offset": 721.76,
      "duration": 7.44
    },
    {
      "lang": "en",
      "text": "5,256 so basically you can trade off the",
      "offset": 725.8,
      "duration": 6.2
    },
    {
      "lang": "en",
      "text": "code book size and the sequence lengths",
      "offset": 729.2,
      "duration": 4.759
    },
    {
      "lang": "en",
      "text": "so you can have very long sequences of",
      "offset": 732,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "integers with very small vocabularies or",
      "offset": 733.959,
      "duration": 6.361
    },
    {
      "lang": "en",
      "text": "we can have short um sequences of",
      "offset": 736.399,
      "duration": 6.961
    },
    {
      "lang": "en",
      "text": "integers with very large vocabularies",
      "offset": 740.32,
      "duration": 5.56
    },
    {
      "lang": "en",
      "text": "and so typically people use in practice",
      "offset": 743.36,
      "duration": 5.24
    },
    {
      "lang": "en",
      "text": "these subword encodings but I'd like to",
      "offset": 745.88,
      "duration": 4.68
    },
    {
      "lang": "en",
      "text": "keep our token ier very simple so we're",
      "offset": 748.6,
      "duration": 4.44
    },
    {
      "lang": "en",
      "text": "using character level tokenizer and that",
      "offset": 750.56,
      "duration": 4.44
    },
    {
      "lang": "en",
      "text": "means that we have very small code books",
      "offset": 753.04,
      "duration": 4.12
    },
    {
      "lang": "en",
      "text": "we have very simple encode and decode",
      "offset": 755,
      "duration": 5.24
    },
    {
      "lang": "en",
      "text": "functions uh but we do get very long",
      "offset": 757.16,
      "duration": 5.32
    },
    {
      "lang": "en",
      "text": "sequences as a result but that's the",
      "offset": 760.24,
      "duration": 3.48
    },
    {
      "lang": "en",
      "text": "level at which we're going to stick with",
      "offset": 762.48,
      "duration": 2.799
    },
    {
      "lang": "en",
      "text": "this lecture because it's the simplest",
      "offset": 763.72,
      "duration": 2.919
    },
    {
      "lang": "en",
      "text": "thing okay so now that we have an",
      "offset": 765.279,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "encoder and a decoder effectively a",
      "offset": 766.639,
      "duration": 4.681
    },
    {
      "lang": "en",
      "text": "tokenizer we can tokenize the entire",
      "offset": 769.04,
      "duration": 4.68
    },
    {
      "lang": "en",
      "text": "training set of Shakespeare so here's a",
      "offset": 771.32,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "chunk of code that does that and I'm",
      "offset": 773.72,
      "duration": 3.239
    },
    {
      "lang": "en",
      "text": "going to start to use the pytorch",
      "offset": 775.72,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "library and specifically the torch.",
      "offset": 776.959,
      "duration": 4.361
    },
    {
      "lang": "en",
      "text": "tensor from the pytorch library so we're",
      "offset": 778.76,
      "duration": 4.6
    },
    {
      "lang": "en",
      "text": "going to take all of the text in tiny",
      "offset": 781.32,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "Shakespeare encode it and then wrap it",
      "offset": 783.36,
      "duration": 4.919
    },
    {
      "lang": "en",
      "text": "into a torch. tensor to get the data",
      "offset": 785.88,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "tensor so here's what the data tensor",
      "offset": 788.279,
      "duration": 3.721
    },
    {
      "lang": "en",
      "text": "looks like when I look at just the first",
      "offset": 790.36,
      "duration": 4.12
    },
    {
      "lang": "en",
      "text": "1,000 characters or the 1,000 elements",
      "offset": 792,
      "duration": 4.76
    },
    {
      "lang": "en",
      "text": "of it so we see that we have a massive",
      "offset": 794.48,
      "duration": 4.44
    },
    {
      "lang": "en",
      "text": "sequence of integers and this sequence",
      "offset": 796.76,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "of integers here is basically an",
      "offset": 798.92,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "identical translation of the first",
      "offset": 800.92,
      "duration": 3.56
    },
    {
      "lang": "en",
      "text": "10,000 characters",
      "offset": 802.519,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "here so I believe for example that zero",
      "offset": 804.48,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "is a new line character and maybe one",
      "offset": 807,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "one is a space not 100% sure but from",
      "offset": 809.12,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "now on the entire data set of text is",
      "offset": 812.199,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "re-represented as just it's just",
      "offset": 814.12,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "stretched out as a single very large uh",
      "offset": 815.8,
      "duration": 3.56
    },
    {
      "lang": "en",
      "text": "sequence of",
      "offset": 818.279,
      "duration": 3.24
    },
    {
      "lang": "en",
      "text": "integers let me do one more thing before",
      "offset": 819.36,
      "duration": 4.2
    },
    {
      "lang": "en",
      "text": "we move on here I'd like to separate out",
      "offset": 821.519,
      "duration": 4.201
    },
    {
      "lang": "en",
      "text": "our data set into a train and a",
      "offset": 823.56,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "validation split so in particular we're",
      "offset": 825.72,
      "duration": 5.32
    },
    {
      "lang": "en",
      "text": "going to take the first 90% of the data",
      "offset": 828.199,
      "duration": 4.601
    },
    {
      "lang": "en",
      "text": "set and consider that to be the training",
      "offset": 831.04,
      "duration": 3.479
    },
    {
      "lang": "en",
      "text": "data for the Transformer and we're going",
      "offset": 832.8,
      "duration": 4.12
    },
    {
      "lang": "en",
      "text": "to withhold the last 10% at the end of",
      "offset": 834.519,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "it to be the validation data and this",
      "offset": 836.92,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "will help us understand to what extent",
      "offset": 839.48,
      "duration": 3.56
    },
    {
      "lang": "en",
      "text": "our model is overfitting so we're going",
      "offset": 841.16,
      "duration": 3.479
    },
    {
      "lang": "en",
      "text": "to basically hide and keep the",
      "offset": 843.04,
      "duration": 3.719
    },
    {
      "lang": "en",
      "text": "validation data on the side because we",
      "offset": 844.639,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "don't want just a perfect memorization",
      "offset": 846.759,
      "duration": 4.361
    },
    {
      "lang": "en",
      "text": "of this exact Shakespeare we want a",
      "offset": 848.56,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "neural network that sort of creates",
      "offset": 851.12,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "Shakespeare like uh text and so it",
      "offset": 852.56,
      "duration": 4.519
    },
    {
      "lang": "en",
      "text": "should be fairly likely for it to",
      "offset": 855.36,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "produce the actual like stowed away uh",
      "offset": 857.079,
      "duration": 7.521
    },
    {
      "lang": "en",
      "text": "true Shakespeare text um and so we're",
      "offset": 861.44,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "going to use this to uh get a sense of",
      "offset": 864.6,
      "duration": 3.56
    },
    {
      "lang": "en",
      "text": "the overfitting okay so now we would",
      "offset": 866.48,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "like to start plugging these text",
      "offset": 868.16,
      "duration": 4.039
    },
    {
      "lang": "en",
      "text": "sequences or integer sequences into the",
      "offset": 870.399,
      "duration": 3.721
    },
    {
      "lang": "en",
      "text": "Transformer so that it can train and",
      "offset": 872.199,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "learn those patterns now the important",
      "offset": 874.12,
      "duration": 4.519
    },
    {
      "lang": "en",
      "text": "thing to realize is we're never going to",
      "offset": 876.68,
      "duration": 3.88
    },
    {
      "lang": "en",
      "text": "actually feed entire text into a",
      "offset": 878.639,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "Transformer all at once that would be",
      "offset": 880.56,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "computationally very expensive and",
      "offset": 882.24,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "prohibitive so when we actually train a",
      "offset": 884.24,
      "duration": 3.959
    },
    {
      "lang": "en",
      "text": "Transformer on a lot of these data sets",
      "offset": 886.32,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "we only work with chunks of the data set",
      "offset": 888.199,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "and when we train the Transformer we",
      "offset": 890.399,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "basically sample random little chunks",
      "offset": 892.04,
      "duration": 3.799
    },
    {
      "lang": "en",
      "text": "out of the training set and train on",
      "offset": 893.839,
      "duration": 4.521
    },
    {
      "lang": "en",
      "text": "just chunks at a time and these chunks",
      "offset": 895.839,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "have basically some kind of a length and",
      "offset": 898.36,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "some maximum length now the maximum",
      "offset": 901.839,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "length typically at least in the code I",
      "offset": 904.36,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "usually write is called block size you",
      "offset": 906.24,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "can you can uh find it under different",
      "offset": 908.68,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "names like context length or something",
      "offset": 910.959,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "like that let's start with the block",
      "offset": 912.6,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "size of just eight and let me look at",
      "offset": 914.079,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "the first train data characters the",
      "offset": 916.12,
      "duration": 4.6
    },
    {
      "lang": "en",
      "text": "first block size plus one characters",
      "offset": 918.72,
      "duration": 4.039
    },
    {
      "lang": "en",
      "text": "I'll explain why plus one in a",
      "offset": 920.72,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "second so this is the first nine",
      "offset": 922.759,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "characters in the sequence in the",
      "offset": 924.959,
      "duration": 5.041
    },
    {
      "lang": "en",
      "text": "training set now what I'd like to point",
      "offset": 927.72,
      "duration": 3.96
    },
    {
      "lang": "en",
      "text": "out is that when you sample a chunk of",
      "offset": 930,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "data like this so say the these nine",
      "offset": 931.68,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "characters out of the training set this",
      "offset": 934.24,
      "duration": 4.36
    },
    {
      "lang": "en",
      "text": "actually has multiple examples packed",
      "offset": 936.56,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "into it and uh that's because all of",
      "offset": 938.6,
      "duration": 5.239
    },
    {
      "lang": "en",
      "text": "these characters follow each other and",
      "offset": 941.199,
      "duration": 5.88
    },
    {
      "lang": "en",
      "text": "so what this thing is going to say when",
      "offset": 943.839,
      "duration": 5.401
    },
    {
      "lang": "en",
      "text": "we plug it into a Transformer is we're",
      "offset": 947.079,
      "duration": 3.56
    },
    {
      "lang": "en",
      "text": "going to actually simultaneously train",
      "offset": 949.24,
      "duration": 3.24
    },
    {
      "lang": "en",
      "text": "it to make prediction at every one of",
      "offset": 950.639,
      "duration": 2.721
    },
    {
      "lang": "en",
      "text": "these",
      "offset": 952.48,
      "duration": 4.279
    },
    {
      "lang": "en",
      "text": "positions now in the in a chunk of nine",
      "offset": 953.36,
      "duration": 5.599
    },
    {
      "lang": "en",
      "text": "characters there's actually eight indiv",
      "offset": 956.759,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "ual examples packed in there so there's",
      "offset": 958.959,
      "duration": 5.721
    },
    {
      "lang": "en",
      "text": "the example that when 18 when in the",
      "offset": 961.56,
      "duration": 6.759
    },
    {
      "lang": "en",
      "text": "context of 18 47 likely comes next in a",
      "offset": 964.68,
      "duration": 7.44
    },
    {
      "lang": "en",
      "text": "context of 18 and 47 56 comes next in a",
      "offset": 968.319,
      "duration": 8.241
    },
    {
      "lang": "en",
      "text": "context of 18 47 56 57 can come next and",
      "offset": 972.12,
      "duration": 6.68
    },
    {
      "lang": "en",
      "text": "so on so that's the eight individual",
      "offset": 976.56,
      "duration": 4.279
    },
    {
      "lang": "en",
      "text": "examples let me actually spell it out",
      "offset": 978.8,
      "duration": 2.839
    },
    {
      "lang": "en",
      "text": "with",
      "offset": 980.839,
      "duration": 3.281
    },
    {
      "lang": "en",
      "text": "code so here's a chunk of code to",
      "offset": 981.639,
      "duration": 4.841
    },
    {
      "lang": "en",
      "text": "illustrate X are the inputs to the",
      "offset": 984.12,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "Transformer it will just be the first",
      "offset": 986.48,
      "duration": 6.12
    },
    {
      "lang": "en",
      "text": "block size characters y will be the uh",
      "offset": 988.12,
      "duration": 6.399
    },
    {
      "lang": "en",
      "text": "next block size characters so it's",
      "offset": 992.6,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "offset by one and that's because y are",
      "offset": 994.519,
      "duration": 5.921
    },
    {
      "lang": "en",
      "text": "the targets for each position in the",
      "offset": 997.399,
      "duration": 5.24
    },
    {
      "lang": "en",
      "text": "input and then here I'm iterating over",
      "offset": 1000.44,
      "duration": 4.959
    },
    {
      "lang": "en",
      "text": "all the block size of eight and the",
      "offset": 1002.639,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "context is always all the characters in",
      "offset": 1005.399,
      "duration": 6.081
    },
    {
      "lang": "en",
      "text": "x uh up to T and including T and the",
      "offset": 1007.12,
      "duration": 6.56
    },
    {
      "lang": "en",
      "text": "target is always the teth character but",
      "offset": 1011.48,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "in the targets array y so let me just",
      "offset": 1013.68,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "run",
      "offset": 1016.6,
      "duration": 3.239
    },
    {
      "lang": "en",
      "text": "this and basically it spells out what I",
      "offset": 1017.36,
      "duration": 4.839
    },
    {
      "lang": "en",
      "text": "said in words uh these are the eight",
      "offset": 1019.839,
      "duration": 4.681
    },
    {
      "lang": "en",
      "text": "examples hidden in a chunk of nine",
      "offset": 1022.199,
      "duration": 6.12
    },
    {
      "lang": "en",
      "text": "characters that we uh sampled from the",
      "offset": 1024.52,
      "duration": 6.519
    },
    {
      "lang": "en",
      "text": "training set I want to mention one more",
      "offset": 1028.319,
      "duration": 6.081
    },
    {
      "lang": "en",
      "text": "thing we train on all the eight examples",
      "offset": 1031.039,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "here with context between one all the",
      "offset": 1034.4,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "way up to context of block size and we",
      "offset": 1036.799,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "train on that not just for computational",
      "offset": 1039.28,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "reasons because we happen to have the",
      "offset": 1040.88,
      "duration": 2.799
    },
    {
      "lang": "en",
      "text": "sequence already or something like that",
      "offset": 1042.16,
      "duration": 4.039
    },
    {
      "lang": "en",
      "text": "it's not just done for efficiency it's",
      "offset": 1043.679,
      "duration": 5.24
    },
    {
      "lang": "en",
      "text": "also done um to make the Transformer",
      "offset": 1046.199,
      "duration": 5.801
    },
    {
      "lang": "en",
      "text": "Network be used to seeing contexts all",
      "offset": 1048.919,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "the way from as little as one all the",
      "offset": 1052,
      "duration": 4.6
    },
    {
      "lang": "en",
      "text": "way to block size and we'd like the",
      "offset": 1053.919,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "transform to be used to seeing",
      "offset": 1056.6,
      "duration": 3.319
    },
    {
      "lang": "en",
      "text": "everything in between and that's going",
      "offset": 1058.24,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "to be useful later during inference",
      "offset": 1059.919,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "because while we're sampling we can",
      "offset": 1061.679,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "start the sampling generation with as",
      "offset": 1063.6,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "little as one character of context and",
      "offset": 1065.6,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "the Transformer knows how to predict the",
      "offset": 1067.679,
      "duration": 3.321
    },
    {
      "lang": "en",
      "text": "next character with all the way up to",
      "offset": 1069.2,
      "duration": 4.44
    },
    {
      "lang": "en",
      "text": "just context of one and so then it can",
      "offset": 1071,
      "duration": 4.6
    },
    {
      "lang": "en",
      "text": "predict everything up to block size and",
      "offset": 1073.64,
      "duration": 3.279
    },
    {
      "lang": "en",
      "text": "after block size we have to start",
      "offset": 1075.6,
      "duration": 3.24
    },
    {
      "lang": "en",
      "text": "truncating because the Transformer will",
      "offset": 1076.919,
      "duration": 4.281
    },
    {
      "lang": "en",
      "text": "will never um receive more than block",
      "offset": 1078.84,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "size inputs when it's predicting the",
      "offset": 1081.2,
      "duration": 2.56
    },
    {
      "lang": "en",
      "text": "next",
      "offset": 1083,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "character Okay so we've looked at the",
      "offset": 1083.76,
      "duration": 4.2
    },
    {
      "lang": "en",
      "text": "time dimension of the tensors that are",
      "offset": 1086.2,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "going to be feeding into the Transformer",
      "offset": 1087.96,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "there's one more Dimension to care about",
      "offset": 1089.559,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "and that is the batch Dimension and so",
      "offset": 1091,
      "duration": 4.52
    },
    {
      "lang": "en",
      "text": "as we're sampling these chunks of text",
      "offset": 1093.32,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "we're going to be actually every time",
      "offset": 1095.52,
      "duration": 2.92
    },
    {
      "lang": "en",
      "text": "we're going to feed them into a",
      "offset": 1097.559,
      "duration": 2.641
    },
    {
      "lang": "en",
      "text": "Transformer we're going to have many",
      "offset": 1098.44,
      "duration": 3.719
    },
    {
      "lang": "en",
      "text": "batches of multiple chunks of text that",
      "offset": 1100.2,
      "duration": 3.479
    },
    {
      "lang": "en",
      "text": "are all like stacked up in a single",
      "offset": 1102.159,
      "duration": 3.281
    },
    {
      "lang": "en",
      "text": "tensor and that's just done for",
      "offset": 1103.679,
      "duration": 3.321
    },
    {
      "lang": "en",
      "text": "efficiency just so that we can keep the",
      "offset": 1105.44,
      "duration": 4.04
    },
    {
      "lang": "en",
      "text": "gpus busy uh because they are very good",
      "offset": 1107,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "at parallel processing of um of data and",
      "offset": 1109.48,
      "duration": 5.72
    },
    {
      "lang": "en",
      "text": "so we just want to process multiple",
      "offset": 1113.24,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "chunks all at the same time but those",
      "offset": 1115.2,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "chunks are processed completely",
      "offset": 1117.32,
      "duration": 2.359
    },
    {
      "lang": "en",
      "text": "independently they don't talk to each",
      "offset": 1118.4,
      "duration": 4.04
    },
    {
      "lang": "en",
      "text": "other and so on so let me basically just",
      "offset": 1119.679,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "generalize this and introduce a batch",
      "offset": 1122.44,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "Dimension here's a chunk of",
      "offset": 1124.24,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "code let me just run it and then I'm",
      "offset": 1126.28,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "going to explain what it",
      "offset": 1128.48,
      "duration": 4.36
    },
    {
      "lang": "en",
      "text": "does so here because we're going to",
      "offset": 1130.76,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "start sampling random locations in the",
      "offset": 1132.84,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "data set to pull chunks from I am",
      "offset": 1134.679,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "setting the seed so that um in the",
      "offset": 1137,
      "duration": 4.44
    },
    {
      "lang": "en",
      "text": "random number generator so that the",
      "offset": 1140.039,
      "duration": 2.801
    },
    {
      "lang": "en",
      "text": "numbers I see here are going to be the",
      "offset": 1141.44,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "same numbers you see later if you try to",
      "offset": 1142.84,
      "duration": 4.52
    },
    {
      "lang": "en",
      "text": "reproduce this now the batch size here",
      "offset": 1144.72,
      "duration": 4.6
    },
    {
      "lang": "en",
      "text": "is how many independent sequences we are",
      "offset": 1147.36,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "processing every forward backward pass",
      "offset": 1149.32,
      "duration": 3.4
    },
    {
      "lang": "en",
      "text": "of the",
      "offset": 1151.52,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "Transformer the block size as I",
      "offset": 1152.72,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "explained is the maximum context length",
      "offset": 1154.48,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "to make those predictions so let's say B",
      "offset": 1156.64,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "size four block size eight and then",
      "offset": 1159.2,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "here's how we get batch for any",
      "offset": 1161.2,
      "duration": 4.28
    },
    {
      "lang": "en",
      "text": "arbitrary split if the split is a",
      "offset": 1163.36,
      "duration": 3.48
    },
    {
      "lang": "en",
      "text": "training split then we're going to look",
      "offset": 1165.48,
      "duration": 4.52
    },
    {
      "lang": "en",
      "text": "at train data otherwise at valid data",
      "offset": 1166.84,
      "duration": 6.68
    },
    {
      "lang": "en",
      "text": "that gives us the data array and then",
      "offset": 1170,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "when I Generate random positions to grab",
      "offset": 1173.52,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "a chunk out of I actually grab I",
      "offset": 1175.76,
      "duration": 5.88
    },
    {
      "lang": "en",
      "text": "actually generate batch size number of",
      "offset": 1178.64,
      "duration": 6.159
    },
    {
      "lang": "en",
      "text": "Random offsets so because this is four",
      "offset": 1181.64,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "we are ex is going to be a uh four",
      "offset": 1184.799,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "numbers that are randomly generated",
      "offset": 1187.24,
      "duration": 4.28
    },
    {
      "lang": "en",
      "text": "between zero and Len of data minus block",
      "offset": 1189.2,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "size so it's just random offsets into",
      "offset": 1191.52,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "the training",
      "offset": 1193.76,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "set and then X's as I explained are the",
      "offset": 1194.88,
      "duration": 5.56
    },
    {
      "lang": "en",
      "text": "first first block size characters",
      "offset": 1198.4,
      "duration": 6.6
    },
    {
      "lang": "en",
      "text": "starting at I the Y's are the offset by",
      "offset": 1200.44,
      "duration": 8.08
    },
    {
      "lang": "en",
      "text": "one of that so just add plus one and",
      "offset": 1205,
      "duration": 5.48
    },
    {
      "lang": "en",
      "text": "then we're going to get those chunks for",
      "offset": 1208.52,
      "duration": 5.519
    },
    {
      "lang": "en",
      "text": "every one of integers I INX and use a",
      "offset": 1210.48,
      "duration": 7.199
    },
    {
      "lang": "en",
      "text": "torch. stack to take all those uh uh",
      "offset": 1214.039,
      "duration": 6.401
    },
    {
      "lang": "en",
      "text": "one-dimensional tensors as we saw here",
      "offset": 1217.679,
      "duration": 6.401
    },
    {
      "lang": "en",
      "text": "and we're going to um stack them up at",
      "offset": 1220.44,
      "duration": 6.8
    },
    {
      "lang": "en",
      "text": "rows and so they all become a row in a",
      "offset": 1224.08,
      "duration": 5.479
    },
    {
      "lang": "en",
      "text": "4x8 tensor",
      "offset": 1227.24,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "so here's where I'm printing then when I",
      "offset": 1229.559,
      "duration": 5.721
    },
    {
      "lang": "en",
      "text": "sample a batch XB and YB the inputs to",
      "offset": 1232.039,
      "duration": 6.961
    },
    {
      "lang": "en",
      "text": "the Transformer now are the input X is",
      "offset": 1235.28,
      "duration": 8.84
    },
    {
      "lang": "en",
      "text": "the 4x8 tensor four uh rows of eight",
      "offset": 1239,
      "duration": 8.12
    },
    {
      "lang": "en",
      "text": "columns and each one of these is a chunk",
      "offset": 1244.12,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "of the training",
      "offset": 1247.12,
      "duration": 5.08
    },
    {
      "lang": "en",
      "text": "set and then the targets here are in the",
      "offset": 1248.679,
      "duration": 5.721
    },
    {
      "lang": "en",
      "text": "associated array Y and they will come in",
      "offset": 1252.2,
      "duration": 3.56
    },
    {
      "lang": "en",
      "text": "to the Transformer all the way at the",
      "offset": 1254.4,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "end uh to um create the loss function",
      "offset": 1255.76,
      "duration": 5.24
    },
    {
      "lang": "en",
      "text": "uh so they will give us the correct",
      "offset": 1259.4,
      "duration": 4.44
    },
    {
      "lang": "en",
      "text": "answer for every single position inside",
      "offset": 1261,
      "duration": 5.4
    },
    {
      "lang": "en",
      "text": "X and then these are the four",
      "offset": 1263.84,
      "duration": 4.12
    },
    {
      "lang": "en",
      "text": "independent",
      "offset": 1266.4,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "rows so spelled out as we did",
      "offset": 1267.96,
      "duration": 6.88
    },
    {
      "lang": "en",
      "text": "before uh this 4x8 array contains a",
      "offset": 1271.279,
      "duration": 6.161
    },
    {
      "lang": "en",
      "text": "total of 32 examples and they're",
      "offset": 1274.84,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "completely independent as far as the",
      "offset": 1277.44,
      "duration": 3.08
    },
    {
      "lang": "en",
      "text": "Transformer is",
      "offset": 1279.159,
      "duration": 6.081
    },
    {
      "lang": "en",
      "text": "concerned uh so when the input is 24 the",
      "offset": 1280.52,
      "duration": 7.72
    },
    {
      "lang": "en",
      "text": "target is 43 or rather 43 here in the Y",
      "offset": 1285.24,
      "duration": 3.96
    },
    {
      "lang": "en",
      "text": "array",
      "offset": 1288.24,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "when the input is 2443 the target is",
      "offset": 1289.2,
      "duration": 5.719
    },
    {
      "lang": "en",
      "text": "58 uh when the input is 24 43 58 the",
      "offset": 1291.919,
      "duration": 6.36
    },
    {
      "lang": "en",
      "text": "target is 5 Etc or like when it is a 52",
      "offset": 1294.919,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "581 the target is",
      "offset": 1298.279,
      "duration": 5.241
    },
    {
      "lang": "en",
      "text": "58 right so you can sort of see this",
      "offset": 1300.919,
      "duration": 4.921
    },
    {
      "lang": "en",
      "text": "spelled out these are the 32 independent",
      "offset": 1303.52,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "examples packed in to a single batch of",
      "offset": 1305.84,
      "duration": 5.959
    },
    {
      "lang": "en",
      "text": "the input X and then the desired targets",
      "offset": 1308.72,
      "duration": 8.68
    },
    {
      "lang": "en",
      "text": "are in y and so now this integer tensor",
      "offset": 1311.799,
      "duration": 8.721
    },
    {
      "lang": "en",
      "text": "of um X is going to feed into the",
      "offset": 1317.4,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "Transformer and that Transformer is",
      "offset": 1320.52,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "going to simultaneously process all",
      "offset": 1322.76,
      "duration": 3.24
    },
    {
      "lang": "en",
      "text": "these examples and then look up the",
      "offset": 1324.279,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "correct um integers to predict in every",
      "offset": 1326,
      "duration": 5.96
    },
    {
      "lang": "en",
      "text": "one of these positions in the tensor y",
      "offset": 1328.919,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "okay so now that we have our batch of",
      "offset": 1331.96,
      "duration": 3.079
    },
    {
      "lang": "en",
      "text": "input that we'd like to feed into a",
      "offset": 1333.799,
      "duration": 2.921
    },
    {
      "lang": "en",
      "text": "Transformer let's start basically",
      "offset": 1335.039,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "feeding this into neural networks now",
      "offset": 1336.72,
      "duration": 3.559
    },
    {
      "lang": "en",
      "text": "we're going to start off with the",
      "offset": 1339.2,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "simplest possible neural network which",
      "offset": 1340.279,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "in the case of language modeling in my",
      "offset": 1342.08,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "opinion is the Byram language model and",
      "offset": 1343.48,
      "duration": 3.48
    },
    {
      "lang": "en",
      "text": "we've covered the Byram language model",
      "offset": 1345.52,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "in my make more series in a lot of depth",
      "offset": 1346.96,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "and so here I'm going to sort of go",
      "offset": 1349.919,
      "duration": 3.561
    },
    {
      "lang": "en",
      "text": "faster and let's just Implement pytorch",
      "offset": 1351.52,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "module directly that implements the byr",
      "offset": 1353.48,
      "duration": 3.4
    },
    {
      "lang": "en",
      "text": "language",
      "offset": 1356.08,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "model so I'm importing the pytorch um NN",
      "offset": 1356.88,
      "duration": 6.32
    },
    {
      "lang": "en",
      "text": "module uh for",
      "offset": 1361.12,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "reproducibility and then here I'm",
      "offset": 1363.2,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "constructing a Byram language model",
      "offset": 1364.799,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "which is a subass of NN",
      "offset": 1366.32,
      "duration": 4.76
    },
    {
      "lang": "en",
      "text": "module and then I'm calling it and I'm",
      "offset": 1368.24,
      "duration": 5.559
    },
    {
      "lang": "en",
      "text": "passing it the inputs and the targets",
      "offset": 1371.08,
      "duration": 4.52
    },
    {
      "lang": "en",
      "text": "and I'm just printing now when the",
      "offset": 1373.799,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "inputs on targets come here you see that",
      "offset": 1375.6,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "I'm just taking the index uh the inputs",
      "offset": 1377.799,
      "duration": 5.441
    },
    {
      "lang": "en",
      "text": "X here which I rename to idx and I'm",
      "offset": 1380.72,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "just passing them into this token",
      "offset": 1383.24,
      "duration": 4.039
    },
    {
      "lang": "en",
      "text": "embedding table so it's going on here is",
      "offset": 1384.48,
      "duration": 5.319
    },
    {
      "lang": "en",
      "text": "that here in the Constructor we are",
      "offset": 1387.279,
      "duration": 5.081
    },
    {
      "lang": "en",
      "text": "creating a token embedding table and it",
      "offset": 1389.799,
      "duration": 5.681
    },
    {
      "lang": "en",
      "text": "is of size vocap size by vocap",
      "offset": 1392.36,
      "duration": 5.88
    },
    {
      "lang": "en",
      "text": "size and we're using an. embedding which",
      "offset": 1395.48,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "is a very thin wrapper around basically",
      "offset": 1398.24,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "a tensor of shape voap size by vocab",
      "offset": 1400.52,
      "duration": 5.159
    },
    {
      "lang": "en",
      "text": "size and what's happening here is that",
      "offset": 1403.2,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "when we pass idx here every single",
      "offset": 1405.679,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "integer in our input is going to refer",
      "offset": 1408.08,
      "duration": 4.28
    },
    {
      "lang": "en",
      "text": "to this embedding table and it's going",
      "offset": 1410.4,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "to pluck out a row of that embedding",
      "offset": 1412.36,
      "duration": 5.439
    },
    {
      "lang": "en",
      "text": "table corresponding to its index so 24",
      "offset": 1414.48,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "here will go into the embedding table",
      "offset": 1417.799,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "and we'll pluck out the 24th row and",
      "offset": 1419.6,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "then 43 will go here and pluck out the",
      "offset": 1422.12,
      "duration": 5.159
    },
    {
      "lang": "en",
      "text": "43d row Etc and then pytorch is going to",
      "offset": 1424.24,
      "duration": 6.12
    },
    {
      "lang": "en",
      "text": "arrange all of this into a batch by Time",
      "offset": 1427.279,
      "duration": 6.321
    },
    {
      "lang": "en",
      "text": "by channel uh tensor in this case batch",
      "offset": 1430.36,
      "duration": 7.4
    },
    {
      "lang": "en",
      "text": "is four time is eight and C which is the",
      "offset": 1433.6,
      "duration": 7.559
    },
    {
      "lang": "en",
      "text": "channels is vocab size or 65 and so",
      "offset": 1437.76,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "we're just going to pluck out all those",
      "offset": 1441.159,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "rows arrange them in a b by T by C and",
      "offset": 1442.48,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "now we're going to interpret this as the",
      "offset": 1445.96,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "logits which are basically the scores",
      "offset": 1447.36,
      "duration": 5.319
    },
    {
      "lang": "en",
      "text": "for the next character in the sequence",
      "offset": 1450.2,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "and so what's happening here is we are",
      "offset": 1452.679,
      "duration": 4.521
    },
    {
      "lang": "en",
      "text": "predicting what comes next based on just",
      "offset": 1454.679,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "the individual identity of a single",
      "offset": 1457.2,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "token and you can do that because um I",
      "offset": 1459.159,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "mean currently the tokens are not",
      "offset": 1462.48,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "talking to each other and they're not",
      "offset": 1463.72,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "seeing any context except for they're",
      "offset": 1465.2,
      "duration": 4.28
    },
    {
      "lang": "en",
      "text": "just seeing themselves so I'm a f I'm a",
      "offset": 1466.76,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "token number five and then I can",
      "offset": 1469.48,
      "duration": 4.12
    },
    {
      "lang": "en",
      "text": "actually make pretty decent predictions",
      "offset": 1472.12,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "about what comes next just by knowing",
      "offset": 1473.6,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "that I'm token five because some",
      "offset": 1475.32,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "characters uh know um C follow other",
      "offset": 1477.12,
      "duration": 5.799
    },
    {
      "lang": "en",
      "text": "characters in in typical scenarios so we",
      "offset": 1479.96,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "saw a lot of this in a lot more depth in",
      "offset": 1482.919,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "the make more series and here if I just",
      "offset": 1484.96,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "run this then we currently get the",
      "offset": 1486.919,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "predictions the scores the lits for",
      "offset": 1489.36,
      "duration": 6.439
    },
    {
      "lang": "en",
      "text": "every one of the 4x8 positions now that",
      "offset": 1493.159,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "we've made predictions about what comes",
      "offset": 1495.799,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "next we'd like to evaluate the loss",
      "offset": 1497.159,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "function and so in make more series we",
      "offset": 1498.799,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "saw that a good way to measure a loss or",
      "offset": 1500.919,
      "duration": 4.76
    },
    {
      "lang": "en",
      "text": "like a quality of the predictions is to",
      "offset": 1503.44,
      "duration": 4.52
    },
    {
      "lang": "en",
      "text": "use the negative log likelihood loss",
      "offset": 1505.679,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "which is also implemented in pytorch",
      "offset": 1507.96,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "under the name cross entropy so what we'",
      "offset": 1509.44,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "like to do here is loss is the cross",
      "offset": 1512.2,
      "duration": 5.599
    },
    {
      "lang": "en",
      "text": "entropy on the predictions and the",
      "offset": 1515.44,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "targets and so this measures the quality",
      "offset": 1517.799,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "of the logits with respect to the",
      "offset": 1520.159,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "Targets in other words we have the",
      "offset": 1521.72,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "identity of the next character so how",
      "offset": 1524.159,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "well are we predicting the next",
      "offset": 1526.2,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "character based on the lits and",
      "offset": 1528.08,
      "duration": 5.479
    },
    {
      "lang": "en",
      "text": "intuitively the correct um the correct",
      "offset": 1530.279,
      "duration": 6.4
    },
    {
      "lang": "en",
      "text": "dimension of low jits uh depending on",
      "offset": 1533.559,
      "duration": 4.521
    },
    {
      "lang": "en",
      "text": "whatever the target is should have a",
      "offset": 1536.679,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "very high number and all the other",
      "offset": 1538.08,
      "duration": 3.64
    },
    {
      "lang": "en",
      "text": "dimensions should be very low number",
      "offset": 1539.559,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "right now the issue is that this won't",
      "offset": 1541.72,
      "duration": 4.439
    },
    {
      "lang": "en",
      "text": "actually this is what we want we want to",
      "offset": 1544.12,
      "duration": 5.919
    },
    {
      "lang": "en",
      "text": "basically output the logits and the",
      "offset": 1546.159,
      "duration": 6.321
    },
    {
      "lang": "en",
      "text": "loss this is what we want but",
      "offset": 1550.039,
      "duration": 5.921
    },
    {
      "lang": "en",
      "text": "unfortunately uh this won't actually run",
      "offset": 1552.48,
      "duration": 5.439
    },
    {
      "lang": "en",
      "text": "we get an error message but intuitively",
      "offset": 1555.96,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "we want to uh measure this now when we",
      "offset": 1557.919,
      "duration": 6.921
    },
    {
      "lang": "en",
      "text": "go to the pytorch um cross entropy",
      "offset": 1561.08,
      "duration": 7
    },
    {
      "lang": "en",
      "text": "documentation here um we're trying to",
      "offset": 1564.84,
      "duration": 5.4
    },
    {
      "lang": "en",
      "text": "call the cross entropy in its functional",
      "offset": 1568.08,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "form uh so that means we don't have to",
      "offset": 1570.24,
      "duration": 4.6
    },
    {
      "lang": "en",
      "text": "create like a module for it but here",
      "offset": 1571.76,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "when we go to the documentation you have",
      "offset": 1574.84,
      "duration": 3.959
    },
    {
      "lang": "en",
      "text": "to look into the details of how pitor",
      "offset": 1576.96,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "expects these inputs and basically the",
      "offset": 1578.799,
      "duration": 5.281
    },
    {
      "lang": "en",
      "text": "issue here is ptor expects if you have",
      "offset": 1580.96,
      "duration": 4.959
    },
    {
      "lang": "en",
      "text": "multi-dimensional input which we do",
      "offset": 1584.08,
      "duration": 4.68
    },
    {
      "lang": "en",
      "text": "because we have a b BYT by C tensor then",
      "offset": 1585.919,
      "duration": 5.281
    },
    {
      "lang": "en",
      "text": "it actually really wants the channels to",
      "offset": 1588.76,
      "duration": 6.36
    },
    {
      "lang": "en",
      "text": "be the second uh Dimension here so if",
      "offset": 1591.2,
      "duration": 7.719
    },
    {
      "lang": "en",
      "text": "you um so basically it wants a b by C",
      "offset": 1595.12,
      "duration": 7.799
    },
    {
      "lang": "en",
      "text": "BYT instead of a b by T by C and so it's",
      "offset": 1598.919,
      "duration": 6.681
    },
    {
      "lang": "en",
      "text": "just the details of how P torch treats",
      "offset": 1602.919,
      "duration": 6.64
    },
    {
      "lang": "en",
      "text": "um these kinds of inputs and so we don't",
      "offset": 1605.6,
      "duration": 5.64
    },
    {
      "lang": "en",
      "text": "actually want to deal with that so what",
      "offset": 1609.559,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "we're going to do instead is we need to",
      "offset": 1611.24,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "basically reshape our logits so here's",
      "offset": 1612.559,
      "duration": 3.961
    },
    {
      "lang": "en",
      "text": "what I like to do I like to take",
      "offset": 1614.76,
      "duration": 3.639
    },
    {
      "lang": "en",
      "text": "basically give names to the dimensions",
      "offset": 1616.52,
      "duration": 4.92
    },
    {
      "lang": "en",
      "text": "so lit. shape is B BYT by C and unpack",
      "offset": 1618.399,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "those numbers and then let's uh say that",
      "offset": 1621.44,
      "duration": 6.28
    },
    {
      "lang": "en",
      "text": "logits equals lit. View and we want it",
      "offset": 1624.159,
      "duration": 7.161
    },
    {
      "lang": "en",
      "text": "to be a b * c b * T by C so just a two-",
      "offset": 1627.72,
      "duration": 5.079
    },
    {
      "lang": "en",
      "text": "dimensional",
      "offset": 1631.32,
      "duration": 3.719
    },
    {
      "lang": "en",
      "text": "array right so we're going to take all",
      "offset": 1632.799,
      "duration": 6.161
    },
    {
      "lang": "en",
      "text": "the we're going to take all of these um",
      "offset": 1635.039,
      "duration": 5.64
    },
    {
      "lang": "en",
      "text": "positions here and we're going to uh",
      "offset": 1638.96,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "stretch them out in a onedimensional",
      "offset": 1640.679,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "sequence and uh preserve the channel",
      "offset": 1642.799,
      "duration": 3.961
    },
    {
      "lang": "en",
      "text": "Dimension as the second",
      "offset": 1645,
      "duration": 3.48
    },
    {
      "lang": "en",
      "text": "dimension so we're just kind of like",
      "offset": 1646.76,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "stretching out the array so it's two-",
      "offset": 1648.48,
      "duration": 3.079
    },
    {
      "lang": "en",
      "text": "dimensional and in that case it's going",
      "offset": 1649.88,
      "duration": 4.039
    },
    {
      "lang": "en",
      "text": "to better conform to what pytorch uh",
      "offset": 1651.559,
      "duration": 5.041
    },
    {
      "lang": "en",
      "text": "sort of expects in its Dimensions now we",
      "offset": 1653.919,
      "duration": 4.76
    },
    {
      "lang": "en",
      "text": "have to do the same to targets because",
      "offset": 1656.6,
      "duration": 8
    },
    {
      "lang": "en",
      "text": "currently targets are um of shape B by T",
      "offset": 1658.679,
      "duration": 8.48
    },
    {
      "lang": "en",
      "text": "and we want it to be just B * T so",
      "offset": 1664.6,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "onedimensional now alternatively you",
      "offset": 1667.159,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "could always still just do minus one",
      "offset": 1669.64,
      "duration": 3.96
    },
    {
      "lang": "en",
      "text": "because pytor will guess what this",
      "offset": 1671.88,
      "duration": 3.56
    },
    {
      "lang": "en",
      "text": "should be if you want to lay it out uh",
      "offset": 1673.6,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "but let me just be explicit and say p *",
      "offset": 1675.44,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "t once we've reshaped this it will match",
      "offset": 1677.12,
      "duration": 6.039
    },
    {
      "lang": "en",
      "text": "the cross entropy case and then we",
      "offset": 1680.44,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "should be able to evaluate our",
      "offset": 1683.159,
      "duration": 6.88
    },
    {
      "lang": "en",
      "text": "loss okay so that R now and we can do",
      "offset": 1686.12,
      "duration": 5.88
    },
    {
      "lang": "en",
      "text": "loss and So currently we see that the",
      "offset": 1690.039,
      "duration": 3.64
    },
    {
      "lang": "en",
      "text": "loss is",
      "offset": 1692,
      "duration": 5.24
    },
    {
      "lang": "en",
      "text": "4.87 now because our uh we have 65",
      "offset": 1693.679,
      "duration": 5.641
    },
    {
      "lang": "en",
      "text": "possible vocabulary elements we can",
      "offset": 1697.24,
      "duration": 3.559
    },
    {
      "lang": "en",
      "text": "actually guess at what the loss should",
      "offset": 1699.32,
      "duration": 3.16
    },
    {
      "lang": "en",
      "text": "be and in",
      "offset": 1700.799,
      "duration": 3.641
    },
    {
      "lang": "en",
      "text": "particular we covered negative log",
      "offset": 1702.48,
      "duration": 3.96
    },
    {
      "lang": "en",
      "text": "likelihood in a lot of detail we are",
      "offset": 1704.44,
      "duration": 7.8
    },
    {
      "lang": "en",
      "text": "expecting log or lawn of um 1 over 65",
      "offset": 1706.44,
      "duration": 8
    },
    {
      "lang": "en",
      "text": "and negative of that so we're expecting",
      "offset": 1712.24,
      "duration": 5.48
    },
    {
      "lang": "en",
      "text": "the loss to be about 4.1 17 but we're",
      "offset": 1714.44,
      "duration": 5.64
    },
    {
      "lang": "en",
      "text": "getting 4.87 and so that's telling us",
      "offset": 1717.72,
      "duration": 4.28
    },
    {
      "lang": "en",
      "text": "that the initial predictions are not uh",
      "offset": 1720.08,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "super diffuse they've got a little bit",
      "offset": 1722,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "of entropy and so we're guessing wrong",
      "offset": 1723.76,
      "duration": 6.799
    },
    {
      "lang": "en",
      "text": "uh so uh yes but actually we're I a we",
      "offset": 1727.039,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "are able to evaluate the loss okay so",
      "offset": 1730.559,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "now that we can evaluate the quality of",
      "offset": 1733.039,
      "duration": 4.601
    },
    {
      "lang": "en",
      "text": "the model on some data we'd like to also",
      "offset": 1734.96,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "be able to generate from the model so",
      "offset": 1737.64,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "let's do the generation now I'm going to",
      "offset": 1739.44,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "go again a little bit faster here",
      "offset": 1741.72,
      "duration": 2.959
    },
    {
      "lang": "en",
      "text": "because I covered all this already in",
      "offset": 1743.039,
      "duration": 2.721
    },
    {
      "lang": "en",
      "text": "previous",
      "offset": 1744.679,
      "duration": 2.761
    },
    {
      "lang": "en",
      "text": "videos",
      "offset": 1745.76,
      "duration": 5.56
    },
    {
      "lang": "en",
      "text": "so here's a generate function for the",
      "offset": 1747.44,
      "duration": 7.599
    },
    {
      "lang": "en",
      "text": "model so we take some uh we take the the",
      "offset": 1751.32,
      "duration": 7.199
    },
    {
      "lang": "en",
      "text": "same kind of input idx here and",
      "offset": 1755.039,
      "duration": 6.961
    },
    {
      "lang": "en",
      "text": "basically this is the current uh context",
      "offset": 1758.519,
      "duration": 6.241
    },
    {
      "lang": "en",
      "text": "of some characters in a batch in some",
      "offset": 1762,
      "duration": 6.399
    },
    {
      "lang": "en",
      "text": "batch so it's also B BYT and the job of",
      "offset": 1764.76,
      "duration": 6.039
    },
    {
      "lang": "en",
      "text": "generate is to basically take this B BYT",
      "offset": 1768.399,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "and extend it to be B BYT + 1 plus 2",
      "offset": 1770.799,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "plus 3 and so it's just basically it",
      "offset": 1772.96,
      "duration": 3.48
    },
    {
      "lang": "en",
      "text": "continues the generation in all the",
      "offset": 1774.799,
      "duration": 4.441
    },
    {
      "lang": "en",
      "text": "batch dimensions in the time Dimension",
      "offset": 1776.44,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "So that's its job and it will do that",
      "offset": 1779.24,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "for Max new tokens so you can see here",
      "offset": 1781.08,
      "duration": 3.959
    },
    {
      "lang": "en",
      "text": "on the bottom there's going to be some",
      "offset": 1783.64,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "stuff here but on the bottom whatever is",
      "offset": 1785.039,
      "duration": 5.401
    },
    {
      "lang": "en",
      "text": "predicted is concatenated on top of the",
      "offset": 1787.399,
      "duration": 5.64
    },
    {
      "lang": "en",
      "text": "previous idx along the First Dimension",
      "offset": 1790.44,
      "duration": 4.28
    },
    {
      "lang": "en",
      "text": "which is the time Dimension to create a",
      "offset": 1793.039,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "b BYT + one so that becomes a new idx so",
      "offset": 1794.72,
      "duration": 5.72
    },
    {
      "lang": "en",
      "text": "the job of generate is to take a b BYT",
      "offset": 1798.399,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "and make it a b BYT plus 1 plus 2 plus",
      "offset": 1800.44,
      "duration": 5.32
    },
    {
      "lang": "en",
      "text": "three as many as we want Max new tokens",
      "offset": 1802.72,
      "duration": 5.64
    },
    {
      "lang": "en",
      "text": "so this is the generation from the model",
      "offset": 1805.76,
      "duration": 4.279
    },
    {
      "lang": "en",
      "text": "now inside the generation what what are",
      "offset": 1808.36,
      "duration": 3.319
    },
    {
      "lang": "en",
      "text": "we doing we're taking the current",
      "offset": 1810.039,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "indices we're getting the predictions so",
      "offset": 1811.679,
      "duration": 6.641
    },
    {
      "lang": "en",
      "text": "we get uh those are in the low jits and",
      "offset": 1815.039,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "then the loss here is going to be",
      "offset": 1818.32,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "ignored because um we're not we're not",
      "offset": 1819.519,
      "duration": 4.28
    },
    {
      "lang": "en",
      "text": "using that and we have no targets that",
      "offset": 1821.519,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "are sort of ground truth targets that",
      "offset": 1823.799,
      "duration": 4.841
    },
    {
      "lang": "en",
      "text": "we're going to be comparing with",
      "offset": 1825.519,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "then once we get the logits we are only",
      "offset": 1828.64,
      "duration": 4.759
    },
    {
      "lang": "en",
      "text": "focusing on the last step so instead of",
      "offset": 1830.48,
      "duration": 5.919
    },
    {
      "lang": "en",
      "text": "a b by T by C we're going to pluck out",
      "offset": 1833.399,
      "duration": 5.321
    },
    {
      "lang": "en",
      "text": "the negative-1 the last element in the",
      "offset": 1836.399,
      "duration": 4.28
    },
    {
      "lang": "en",
      "text": "time Dimension because those are the",
      "offset": 1838.72,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "predictions for what comes next so that",
      "offset": 1840.679,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "gives us the logits which we then",
      "offset": 1842.96,
      "duration": 4.68
    },
    {
      "lang": "en",
      "text": "convert to probabilities via softmax and",
      "offset": 1844.44,
      "duration": 4.959
    },
    {
      "lang": "en",
      "text": "then we use tor. multinomial to sample",
      "offset": 1847.64,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "from those probabilities and we ask",
      "offset": 1849.399,
      "duration": 5.321
    },
    {
      "lang": "en",
      "text": "pytorch to give us one sample and so idx",
      "offset": 1851.24,
      "duration": 6.319
    },
    {
      "lang": "en",
      "text": "next will become a b by one because in",
      "offset": 1854.72,
      "duration": 5.439
    },
    {
      "lang": "en",
      "text": "each uh one of the batch Dimensions",
      "offset": 1857.559,
      "duration": 3.881
    },
    {
      "lang": "en",
      "text": "we're going to have a single prediction",
      "offset": 1860.159,
      "duration": 3.48
    },
    {
      "lang": "en",
      "text": "for what comes next so this num samples",
      "offset": 1861.44,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "equals one will make this be a",
      "offset": 1863.639,
      "duration": 4.681
    },
    {
      "lang": "en",
      "text": "one and then we're going to take those",
      "offset": 1866.32,
      "duration": 4.199
    },
    {
      "lang": "en",
      "text": "integers that come from the sampling",
      "offset": 1868.32,
      "duration": 3.479
    },
    {
      "lang": "en",
      "text": "process according to the probability",
      "offset": 1870.519,
      "duration": 3.241
    },
    {
      "lang": "en",
      "text": "distribution given here and those",
      "offset": 1871.799,
      "duration": 4.041
    },
    {
      "lang": "en",
      "text": "integers got just concatenated on top of",
      "offset": 1873.76,
      "duration": 4.12
    },
    {
      "lang": "en",
      "text": "the current sort of like running stream",
      "offset": 1875.84,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "of integers and this gives us a b BYT +",
      "offset": 1877.88,
      "duration": 6.159
    },
    {
      "lang": "en",
      "text": "one and then we can return that now one",
      "offset": 1880.72,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "thing here is you see how I'm calling",
      "offset": 1884.039,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "self of idx which will end up going to",
      "offset": 1886.88,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "the forward function I'm not providing",
      "offset": 1889.559,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "any Targets So currently this would give",
      "offset": 1891.679,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "an error because targets is uh is uh",
      "offset": 1893.96,
      "duration": 5.16
    },
    {
      "lang": "en",
      "text": "sort of like not given so targets has to",
      "offset": 1896.639,
      "duration": 4.681
    },
    {
      "lang": "en",
      "text": "be optional so targets is none by",
      "offset": 1899.12,
      "duration": 5.439
    },
    {
      "lang": "en",
      "text": "default and then if targets is none then",
      "offset": 1901.32,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "there's no loss to create so it's just",
      "offset": 1904.559,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "loss is none but else all of this",
      "offset": 1907.48,
      "duration": 5.799
    },
    {
      "lang": "en",
      "text": "happens and we can create a loss so this",
      "offset": 1910.399,
      "duration": 5.681
    },
    {
      "lang": "en",
      "text": "will make it so um if we have the",
      "offset": 1913.279,
      "duration": 4.28
    },
    {
      "lang": "en",
      "text": "targets we provide them and get a loss",
      "offset": 1916.08,
      "duration": 3.16
    },
    {
      "lang": "en",
      "text": "if we have no targets it will'll just",
      "offset": 1917.559,
      "duration": 2.921
    },
    {
      "lang": "en",
      "text": "get the",
      "offset": 1919.24,
      "duration": 3.72
    },
    {
      "lang": "en",
      "text": "loits so this here will generate from",
      "offset": 1920.48,
      "duration": 6.36
    },
    {
      "lang": "en",
      "text": "the model um and let's take that for a",
      "offset": 1922.96,
      "duration": 5.559
    },
    {
      "lang": "en",
      "text": "ride",
      "offset": 1926.84,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "now oops so I have another code chunk",
      "offset": 1928.519,
      "duration": 5.4
    },
    {
      "lang": "en",
      "text": "here which will generate for the model",
      "offset": 1931.88,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "from the model and okay this is kind of",
      "offset": 1933.919,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "crazy so maybe let me let me break this",
      "offset": 1935.799,
      "duration": 7.161
    },
    {
      "lang": "en",
      "text": "down so these are the idx",
      "offset": 1938.32,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "right I'm creating a batch will be just",
      "offset": 1943.88,
      "duration": 6.159
    },
    {
      "lang": "en",
      "text": "one time will be just one so I'm",
      "offset": 1946.919,
      "duration": 5.24
    },
    {
      "lang": "en",
      "text": "creating a little one by one tensor and",
      "offset": 1950.039,
      "duration": 5.041
    },
    {
      "lang": "en",
      "text": "it's holding a zero and the D type the",
      "offset": 1952.159,
      "duration": 5.88
    },
    {
      "lang": "en",
      "text": "data type is uh integer so zero is going",
      "offset": 1955.08,
      "duration": 5.24
    },
    {
      "lang": "en",
      "text": "to be how we kick off the generation and",
      "offset": 1958.039,
      "duration": 6.081
    },
    {
      "lang": "en",
      "text": "remember that zero is uh is the element",
      "offset": 1960.32,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "standing for a new line character so",
      "offset": 1964.12,
      "duration": 3.32
    },
    {
      "lang": "en",
      "text": "it's kind of like a reasonable thing to",
      "offset": 1965.84,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "to feed in as the very first character",
      "offset": 1967.44,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "in a sequence to be the new",
      "offset": 1969.039,
      "duration": 5.921
    },
    {
      "lang": "en",
      "text": "line um so it's going to be idx which",
      "offset": 1971.519,
      "duration": 4.841
    },
    {
      "lang": "en",
      "text": "we're going to feed in here then we're",
      "offset": 1974.96,
      "duration": 3.8
    },
    {
      "lang": "en",
      "text": "going to ask for 100 tokens",
      "offset": 1976.36,
      "duration": 5.48
    },
    {
      "lang": "en",
      "text": "and then. generate will continue that",
      "offset": 1978.76,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "now because uh generate works on the",
      "offset": 1981.84,
      "duration": 5.439
    },
    {
      "lang": "en",
      "text": "level of batches we we then have to",
      "offset": 1985,
      "duration": 4.919
    },
    {
      "lang": "en",
      "text": "index into the zero throw to basically",
      "offset": 1987.279,
      "duration": 6.601
    },
    {
      "lang": "en",
      "text": "unplug the um the single batch Dimension",
      "offset": 1989.919,
      "duration": 9
    },
    {
      "lang": "en",
      "text": "that exists and then that gives us a um",
      "offset": 1993.88,
      "duration": 7.039
    },
    {
      "lang": "en",
      "text": "time steps just a onedimensional array",
      "offset": 1998.919,
      "duration": 4.521
    },
    {
      "lang": "en",
      "text": "of all the indices which we will convert",
      "offset": 2000.919,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "to simple python list from pytorch",
      "offset": 2003.44,
      "duration": 5.239
    },
    {
      "lang": "en",
      "text": "tensor so that that can feed into our",
      "offset": 2006.279,
      "duration": 6.041
    },
    {
      "lang": "en",
      "text": "decode function and uh convert those",
      "offset": 2008.679,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "integers into text so let me bring this",
      "offset": 2012.32,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "back and we're generating 100 tokens",
      "offset": 2014.919,
      "duration": 3.041
    },
    {
      "lang": "en",
      "text": "let's",
      "offset": 2017.2,
      "duration": 3.719
    },
    {
      "lang": "en",
      "text": "run and uh here's the generation that we",
      "offset": 2017.96,
      "duration": 5.439
    },
    {
      "lang": "en",
      "text": "achieved so obviously it's garbage and",
      "offset": 2020.919,
      "duration": 3.88
    },
    {
      "lang": "en",
      "text": "the reason it's garbage is because this",
      "offset": 2023.399,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "is a totally random model so next up",
      "offset": 2024.799,
      "duration": 4.441
    },
    {
      "lang": "en",
      "text": "we're going to want to train this model",
      "offset": 2027,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "now one more thing I wanted to point out",
      "offset": 2029.24,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "here is this function is written to be",
      "offset": 2030.6,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "General but it's kind of like ridiculous",
      "offset": 2033.48,
      "duration": 4.52
    },
    {
      "lang": "en",
      "text": "right now because",
      "offset": 2035.799,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "we're feeding in all this we're building",
      "offset": 2038,
      "duration": 4.12
    },
    {
      "lang": "en",
      "text": "out this context and we're concatenating",
      "offset": 2039.799,
      "duration": 5.24
    },
    {
      "lang": "en",
      "text": "it all and we're always feeding it all",
      "offset": 2042.12,
      "duration": 5.48
    },
    {
      "lang": "en",
      "text": "into the model but that's kind of",
      "offset": 2045.039,
      "duration": 4.001
    },
    {
      "lang": "en",
      "text": "ridiculous because this is just a simple",
      "offset": 2047.6,
      "duration": 3.519
    },
    {
      "lang": "en",
      "text": "Byram model so to make for example this",
      "offset": 2049.04,
      "duration": 5.359
    },
    {
      "lang": "en",
      "text": "prediction about K we only needed this W",
      "offset": 2051.119,
      "duration": 4.601
    },
    {
      "lang": "en",
      "text": "but actually what we fed into the model",
      "offset": 2054.399,
      "duration": 3.641
    },
    {
      "lang": "en",
      "text": "is we fed the entire sequence and then",
      "offset": 2055.72,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "we only looked at the very last piece",
      "offset": 2058.04,
      "duration": 5.24
    },
    {
      "lang": "en",
      "text": "and predicted K so the only reason I'm",
      "offset": 2060.44,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "writing it in this way is because right",
      "offset": 2063.28,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "now this is a byr model but I'd like to",
      "offset": 2065.44,
      "duration": 3.959
    },
    {
      "lang": "en",
      "text": "keep keep this function fixed and I'd",
      "offset": 2067.52,
      "duration": 5.079
    },
    {
      "lang": "en",
      "text": "like it to work um later when our",
      "offset": 2069.399,
      "duration": 6.28
    },
    {
      "lang": "en",
      "text": "characters actually um basically look",
      "offset": 2072.599,
      "duration": 5.08
    },
    {
      "lang": "en",
      "text": "further in the history and so right now",
      "offset": 2075.679,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "the history is not used so this looks",
      "offset": 2077.679,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "silly uh but eventually the history will",
      "offset": 2079.44,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "be used and so that's why we want to uh",
      "offset": 2082.32,
      "duration": 4.2
    },
    {
      "lang": "en",
      "text": "do it this way so just a quick comment",
      "offset": 2084.639,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "on that so now we see that this is um",
      "offset": 2086.52,
      "duration": 5.24
    },
    {
      "lang": "en",
      "text": "random so let's train the model so it",
      "offset": 2089.599,
      "duration": 4.361
    },
    {
      "lang": "en",
      "text": "becomes a bit less random okay let's Now",
      "offset": 2091.76,
      "duration": 4.2
    },
    {
      "lang": "en",
      "text": "train the model so first what I'm going",
      "offset": 2093.96,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "to do is I'm going to create a pyour",
      "offset": 2095.96,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "optimization object so here we are using",
      "offset": 2097.96,
      "duration": 7.08
    },
    {
      "lang": "en",
      "text": "the optimizer ATM W um now in a make",
      "offset": 2100.839,
      "duration": 5.921
    },
    {
      "lang": "en",
      "text": "more series we've only ever use tastic",
      "offset": 2105.04,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "gradi in descent the simplest possible",
      "offset": 2106.76,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "Optimizer which you can get using the",
      "offset": 2108.32,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "SGD instead but I want to use Adam which",
      "offset": 2110.119,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "is a much more advanced and popular",
      "offset": 2112.56,
      "duration": 4.039
    },
    {
      "lang": "en",
      "text": "Optimizer and it works extremely well",
      "offset": 2114.2,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "for uh typical good setting for the",
      "offset": 2116.599,
      "duration": 5.921
    },
    {
      "lang": "en",
      "text": "learning rate is roughly 3 E4 uh but for",
      "offset": 2119.32,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "very very small networks like is the",
      "offset": 2122.52,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "case here you can get away with much",
      "offset": 2123.96,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "much higher learning rates R3 or even",
      "offset": 2125.64,
      "duration": 4.959
    },
    {
      "lang": "en",
      "text": "higher probably but let me create the",
      "offset": 2128.2,
      "duration": 4.919
    },
    {
      "lang": "en",
      "text": "optimizer object which will basically",
      "offset": 2130.599,
      "duration": 4.52
    },
    {
      "lang": "en",
      "text": "take the gradients and uh update the",
      "offset": 2133.119,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "parameters using the",
      "offset": 2135.119,
      "duration": 5.081
    },
    {
      "lang": "en",
      "text": "gradients and then here our batch size",
      "offset": 2136.8,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "up above was only four so let me",
      "offset": 2140.2,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "actually use something bigger let's say",
      "offset": 2141.8,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "32 and then for some number of steps um",
      "offset": 2143.24,
      "duration": 5.359
    },
    {
      "lang": "en",
      "text": "we are sampling a new batch of data",
      "offset": 2146.599,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "we're evaluating the loss uh we're",
      "offset": 2148.599,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "zeroing out all the gradients from the",
      "offset": 2151.319,
      "duration": 3.561
    },
    {
      "lang": "en",
      "text": "previous step getting the gradients for",
      "offset": 2152.76,
      "duration": 4.2
    },
    {
      "lang": "en",
      "text": "all the parameters and then using those",
      "offset": 2154.88,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "gradients to up update our parameters so",
      "offset": 2156.96,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "typical training loop as we saw in the",
      "offset": 2158.96,
      "duration": 5.639
    },
    {
      "lang": "en",
      "text": "make more series so let me now uh run",
      "offset": 2160.96,
      "duration": 6.32
    },
    {
      "lang": "en",
      "text": "this for say 100 iterations and let's",
      "offset": 2164.599,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "see what kind of losses we're going to",
      "offset": 2167.28,
      "duration": 5.4
    },
    {
      "lang": "en",
      "text": "get so we started around",
      "offset": 2169.8,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "4.7 and now we're getting to down to",
      "offset": 2172.68,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "like 4.6 4.5 Etc so the optimization is",
      "offset": 2174.68,
      "duration": 7.399
    },
    {
      "lang": "en",
      "text": "definitely happening but um let's uh",
      "offset": 2178.52,
      "duration": 5.4
    },
    {
      "lang": "en",
      "text": "sort of try to increase number of",
      "offset": 2182.079,
      "duration": 3.801
    },
    {
      "lang": "en",
      "text": "iterations and only print at the",
      "offset": 2183.92,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "end because we probably want train for",
      "offset": 2185.88,
      "duration": 8.12
    },
    {
      "lang": "en",
      "text": "longer okay so we're down to 3.6",
      "offset": 2189.44,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "roughly roughly down to",
      "offset": 2194.68,
      "duration": 3.639
    },
    {
      "lang": "en",
      "text": "three this is the most janky",
      "offset": 2200.56,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "optimization okay it's working let's",
      "offset": 2206.319,
      "duration": 4.201
    },
    {
      "lang": "en",
      "text": "just do",
      "offset": 2208.119,
      "duration": 5.401
    },
    {
      "lang": "en",
      "text": "10,000 and then from here we want to",
      "offset": 2210.52,
      "duration": 6.319
    },
    {
      "lang": "en",
      "text": "copy this and hopefully that we're going",
      "offset": 2213.52,
      "duration": 4.68
    },
    {
      "lang": "en",
      "text": "to get something reason and of course",
      "offset": 2216.839,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "it's not going to be Shakespeare from a",
      "offset": 2218.2,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "byr model but at least we see that the",
      "offset": 2220.04,
      "duration": 5.079
    },
    {
      "lang": "en",
      "text": "loss is improving and uh hopefully we're",
      "offset": 2221.88,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "expecting something a bit more",
      "offset": 2225.119,
      "duration": 3.72
    },
    {
      "lang": "en",
      "text": "reasonable okay so we're down at about",
      "offset": 2226.88,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "2.5 is let's see what we get okay",
      "offset": 2228.839,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "dramatic improvements certainly on what",
      "offset": 2232.88,
      "duration": 4.68
    },
    {
      "lang": "en",
      "text": "we had here so let me just increase the",
      "offset": 2234.359,
      "duration": 5.521
    },
    {
      "lang": "en",
      "text": "number of tokens okay so we see that",
      "offset": 2237.56,
      "duration": 4.279
    },
    {
      "lang": "en",
      "text": "we're starting to get something at least",
      "offset": 2239.88,
      "duration": 5.8
    },
    {
      "lang": "en",
      "text": "like reasonable is",
      "offset": 2241.839,
      "duration": 7.561
    },
    {
      "lang": "en",
      "text": "um certainly not shakes spear but uh the",
      "offset": 2245.68,
      "duration": 5.8
    },
    {
      "lang": "en",
      "text": "model is making progress so that is the",
      "offset": 2249.4,
      "duration": 3.719
    },
    {
      "lang": "en",
      "text": "simplest possible",
      "offset": 2251.48,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "model so now what I'd like to do",
      "offset": 2253.119,
      "duration": 6.561
    },
    {
      "lang": "en",
      "text": "is obviously this is a very simple model",
      "offset": 2256.68,
      "duration": 4.36
    },
    {
      "lang": "en",
      "text": "because the tokens are not talking to",
      "offset": 2259.68,
      "duration": 3.48
    },
    {
      "lang": "en",
      "text": "each other so given the previous context",
      "offset": 2261.04,
      "duration": 4.36
    },
    {
      "lang": "en",
      "text": "of whatever was generated we're only",
      "offset": 2263.16,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "looking at the very last character to",
      "offset": 2265.4,
      "duration": 2.679
    },
    {
      "lang": "en",
      "text": "make the predictions about what comes",
      "offset": 2266.839,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "next so now these uh now these tokens",
      "offset": 2268.079,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "have to start talking to each other and",
      "offset": 2270.839,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "figuring out what is in the context so",
      "offset": 2273.04,
      "duration": 3.4
    },
    {
      "lang": "en",
      "text": "that they can make better predictions",
      "offset": 2275.319,
      "duration": 2.641
    },
    {
      "lang": "en",
      "text": "for what comes next and this is how",
      "offset": 2276.44,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "we're going to kick off the uh",
      "offset": 2277.96,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "Transformer okay so next I took the code",
      "offset": 2279.64,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "that we developed in this juper notebook",
      "offset": 2282.04,
      "duration": 3.72
    },
    {
      "lang": "en",
      "text": "and I converted it to be a script and",
      "offset": 2283.48,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "I'm doing this because I just want to",
      "offset": 2285.76,
      "duration": 4.359
    },
    {
      "lang": "en",
      "text": "simplify our intermediate work into just",
      "offset": 2288.04,
      "duration": 4.12
    },
    {
      "lang": "en",
      "text": "the final product that we have at this",
      "offset": 2290.119,
      "duration": 5.161
    },
    {
      "lang": "en",
      "text": "point so in the top here I put all the",
      "offset": 2292.16,
      "duration": 4.76
    },
    {
      "lang": "en",
      "text": "hyp parameters that we to find I",
      "offset": 2295.28,
      "duration": 3.079
    },
    {
      "lang": "en",
      "text": "introduced a few and I'm going to speak",
      "offset": 2296.92,
      "duration": 3.8
    },
    {
      "lang": "en",
      "text": "to that in a little bit otherwise a lot",
      "offset": 2298.359,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "of this should be recognizable uh",
      "offset": 2300.72,
      "duration": 4.84
    },
    {
      "lang": "en",
      "text": "reproducibility read data get the",
      "offset": 2303.16,
      "duration": 4.52
    },
    {
      "lang": "en",
      "text": "encoder and the decoder create the train",
      "offset": 2305.56,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "into splits uh use the uh kind of like",
      "offset": 2307.68,
      "duration": 6.32
    },
    {
      "lang": "en",
      "text": "data loader um that gets a batch of the",
      "offset": 2310.92,
      "duration": 6.04
    },
    {
      "lang": "en",
      "text": "inputs and Targets this is new and I'll",
      "offset": 2314,
      "duration": 5.4
    },
    {
      "lang": "en",
      "text": "talk about it in a second now this is",
      "offset": 2316.96,
      "duration": 3.8
    },
    {
      "lang": "en",
      "text": "the Byram language model that we",
      "offset": 2319.4,
      "duration": 3.719
    },
    {
      "lang": "en",
      "text": "developed and it can forward and give us",
      "offset": 2320.76,
      "duration": 5.079
    },
    {
      "lang": "en",
      "text": "a logits and loss and it can",
      "offset": 2323.119,
      "duration": 5.041
    },
    {
      "lang": "en",
      "text": "generate and then here we are creating",
      "offset": 2325.839,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "the optimizer and this is the training",
      "offset": 2328.16,
      "duration": 5.159
    },
    {
      "lang": "en",
      "text": "Loop so everything here should look",
      "offset": 2331.04,
      "duration": 4.2
    },
    {
      "lang": "en",
      "text": "pretty familiar now some of the small",
      "offset": 2333.319,
      "duration": 4.52
    },
    {
      "lang": "en",
      "text": "things that I added number one I added",
      "offset": 2335.24,
      "duration": 4.839
    },
    {
      "lang": "en",
      "text": "the ability to run on a GPU if you have",
      "offset": 2337.839,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "it so if you have a GPU then you can",
      "offset": 2340.079,
      "duration": 4.841
    },
    {
      "lang": "en",
      "text": "this will use Cuda instead of just CPU",
      "offset": 2342.56,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "and everything will be a lot more faster",
      "offset": 2344.92,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "now when device becomes Cuda then we",
      "offset": 2347.28,
      "duration": 3.96
    },
    {
      "lang": "en",
      "text": "need to make sure that when we load the",
      "offset": 2349.72,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "data we move it to",
      "offset": 2351.24,
      "duration": 4.68
    },
    {
      "lang": "en",
      "text": "device when we create the model we want",
      "offset": 2353.079,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "to move uh the model parameters to",
      "offset": 2355.92,
      "duration": 5.399
    },
    {
      "lang": "en",
      "text": "device so as an example here we have the",
      "offset": 2358.119,
      "duration": 5.281
    },
    {
      "lang": "en",
      "text": "N an embedding table and it's got a",
      "offset": 2361.319,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "weight inside it which stores the uh",
      "offset": 2363.4,
      "duration": 4.439
    },
    {
      "lang": "en",
      "text": "sort of lookup table so so that would be",
      "offset": 2366.04,
      "duration": 3.64
    },
    {
      "lang": "en",
      "text": "moved to the GPU so that all the",
      "offset": 2367.839,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "calculations here happen on the GPU and",
      "offset": 2369.68,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "they can be a lot faster and then",
      "offset": 2372,
      "duration": 3.48
    },
    {
      "lang": "en",
      "text": "finally here when I'm creating the",
      "offset": 2374.24,
      "duration": 3.56
    },
    {
      "lang": "en",
      "text": "context that feeds in to generate I have",
      "offset": 2375.48,
      "duration": 3.96
    },
    {
      "lang": "en",
      "text": "to make sure that I create it on the",
      "offset": 2377.8,
      "duration": 5.279
    },
    {
      "lang": "en",
      "text": "device number two what I introduced is",
      "offset": 2379.44,
      "duration": 7.159
    },
    {
      "lang": "en",
      "text": "uh the fact that here in the training",
      "offset": 2383.079,
      "duration": 7.721
    },
    {
      "lang": "en",
      "text": "Loop here I was just printing the um l.",
      "offset": 2386.599,
      "duration": 6.681
    },
    {
      "lang": "en",
      "text": "item inside the training Loop but this",
      "offset": 2390.8,
      "duration": 3.72
    },
    {
      "lang": "en",
      "text": "is a very noisy measurement of the",
      "offset": 2393.28,
      "duration": 3.4
    },
    {
      "lang": "en",
      "text": "current loss because every batch will be",
      "offset": 2394.52,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "more or less lucky and so what I want to",
      "offset": 2396.68,
      "duration": 6.2
    },
    {
      "lang": "en",
      "text": "do usually um is uh I have an estimate",
      "offset": 2399.52,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "loss function and the estimate loss",
      "offset": 2402.88,
      "duration": 7.64
    },
    {
      "lang": "en",
      "text": "basically then um goes up here and it",
      "offset": 2405.52,
      "duration": 7.16
    },
    {
      "lang": "en",
      "text": "averages up the loss over multiple",
      "offset": 2410.52,
      "duration": 4.599
    },
    {
      "lang": "en",
      "text": "batches so in particular we're going to",
      "offset": 2412.68,
      "duration": 5.159
    },
    {
      "lang": "en",
      "text": "iterate eval iter times and we're going",
      "offset": 2415.119,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "to basically get our loss and then we're",
      "offset": 2417.839,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "going to get the average loss for both",
      "offset": 2419.68,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "splits and so this will be a lot less",
      "offset": 2421.599,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "noisy so here when we call the estimate",
      "offset": 2424.079,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "loss we're we're going to report the uh",
      "offset": 2426.48,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "pretty accurate train and validation",
      "offset": 2428.48,
      "duration": 4.92
    },
    {
      "lang": "en",
      "text": "loss now when we come back up you'll",
      "offset": 2431.04,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "notice a few things here I'm setting the",
      "offset": 2433.4,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "model to evaluation phase and down here",
      "offset": 2435.28,
      "duration": 5.559
    },
    {
      "lang": "en",
      "text": "I'm resetting it back to training phase",
      "offset": 2438.2,
      "duration": 4.6
    },
    {
      "lang": "en",
      "text": "now right now for our model as is this",
      "offset": 2440.839,
      "duration": 3.961
    },
    {
      "lang": "en",
      "text": "doesn't actually do anything because the",
      "offset": 2442.8,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "only thing inside this model is this uh",
      "offset": 2444.8,
      "duration": 6.76
    },
    {
      "lang": "en",
      "text": "nn. embedding and um this this um",
      "offset": 2446.8,
      "duration": 6.76
    },
    {
      "lang": "en",
      "text": "Network would behave both would behave",
      "offset": 2451.56,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "the same in both evaluation mode and",
      "offset": 2453.56,
      "duration": 4.12
    },
    {
      "lang": "en",
      "text": "training mode we have no drop off layers",
      "offset": 2455.4,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "we have no batm layers Etc but it is a",
      "offset": 2457.68,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "good practice to Think Through what mode",
      "offset": 2460.119,
      "duration": 4.521
    },
    {
      "lang": "en",
      "text": "your neural network is in because some",
      "offset": 2462.48,
      "duration": 4.599
    },
    {
      "lang": "en",
      "text": "layers will have different Behavior Uh",
      "offset": 2464.64,
      "duration": 6.439
    },
    {
      "lang": "en",
      "text": "at inference time or training time and",
      "offset": 2467.079,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "there's also this context manager torch",
      "offset": 2471.079,
      "duration": 3.481
    },
    {
      "lang": "en",
      "text": "up nograd and this is just telling",
      "offset": 2472.839,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "pytorch that everything that happens",
      "offset": 2474.56,
      "duration": 4.039
    },
    {
      "lang": "en",
      "text": "inside this function we will not call do",
      "offset": 2476.119,
      "duration": 5.521
    },
    {
      "lang": "en",
      "text": "backward on and so pytorch can be a lot",
      "offset": 2478.599,
      "duration": 4.76
    },
    {
      "lang": "en",
      "text": "more efficient with its memory use",
      "offset": 2481.64,
      "duration": 3.479
    },
    {
      "lang": "en",
      "text": "because it doesn't have to store all the",
      "offset": 2483.359,
      "duration": 3.641
    },
    {
      "lang": "en",
      "text": "intermediate variables uh because we're",
      "offset": 2485.119,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "never going to call backward and so it",
      "offset": 2487,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "can it can be a lot more memory",
      "offset": 2489.28,
      "duration": 3.4
    },
    {
      "lang": "en",
      "text": "efficient in that way so also a good",
      "offset": 2490.599,
      "duration": 4.601
    },
    {
      "lang": "en",
      "text": "practice to tpy torch when we don't",
      "offset": 2492.68,
      "duration": 4.04
    },
    {
      "lang": "en",
      "text": "intend to do back",
      "offset": 2495.2,
      "duration": 4.76
    },
    {
      "lang": "en",
      "text": "propagation so right now this script is",
      "offset": 2496.72,
      "duration": 6.399
    },
    {
      "lang": "en",
      "text": "about 120 lines of code of and that's",
      "offset": 2499.96,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "kind of our starter code I'm calling it",
      "offset": 2503.119,
      "duration": 5.761
    },
    {
      "lang": "en",
      "text": "b.p and I'm going to release it later",
      "offset": 2505.96,
      "duration": 4.119
    },
    {
      "lang": "en",
      "text": "now running this",
      "offset": 2508.88,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "script gives us output in the terminal",
      "offset": 2510.079,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "and it looks something like this it",
      "offset": 2512.64,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "basically as I ran this code uh it was",
      "offset": 2514.96,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "giving me the train loss and Val loss",
      "offset": 2517.68,
      "duration": 3.399
    },
    {
      "lang": "en",
      "text": "and we see that we convert to somewhere",
      "offset": 2519.68,
      "duration": 2.2
    },
    {
      "lang": "en",
      "text": "around",
      "offset": 2521.079,
      "duration": 3.801
    },
    {
      "lang": "en",
      "text": "2.5 with the pyr model and then here's",
      "offset": 2521.88,
      "duration": 5.56
    },
    {
      "lang": "en",
      "text": "the sample that we produced at the",
      "offset": 2524.88,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "end and so we have everything packaged",
      "offset": 2527.44,
      "duration": 4.2
    },
    {
      "lang": "en",
      "text": "up in the script and we're in a good",
      "offset": 2529.76,
      "duration": 3.96
    },
    {
      "lang": "en",
      "text": "position now to iterate on this okay so",
      "offset": 2531.64,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "we are almost ready to start writing our",
      "offset": 2533.72,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "very first self attention block for",
      "offset": 2535.96,
      "duration": 6.68
    },
    {
      "lang": "en",
      "text": "processing these uh tokens now before we",
      "offset": 2538.28,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "actually get there I want to get you",
      "offset": 2542.64,
      "duration": 3.56
    },
    {
      "lang": "en",
      "text": "used to a mathematical trick that is",
      "offset": 2544.2,
      "duration": 3.879
    },
    {
      "lang": "en",
      "text": "used in the self attention inside a",
      "offset": 2546.2,
      "duration": 3.96
    },
    {
      "lang": "en",
      "text": "Transformer and is really just like at",
      "offset": 2548.079,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "the heart of an an efficient",
      "offset": 2550.16,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "implementation of self attention and so",
      "offset": 2552.079,
      "duration": 4.361
    },
    {
      "lang": "en",
      "text": "I want to work with this toy example to",
      "offset": 2554.88,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "just get you used to this operation and",
      "offset": 2556.44,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "then it's going to make it much more",
      "offset": 2558.48,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "clear once we actually get to um to it",
      "offset": 2559.8,
      "duration": 4.6
    },
    {
      "lang": "en",
      "text": "uh in the script",
      "offset": 2563.119,
      "duration": 4.521
    },
    {
      "lang": "en",
      "text": "again so let's create a b BYT by C where",
      "offset": 2564.4,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "BT and C are just 48 and two in the toy",
      "offset": 2567.64,
      "duration": 5.439
    },
    {
      "lang": "en",
      "text": "example and these are basically channels",
      "offset": 2570.24,
      "duration": 5.56
    },
    {
      "lang": "en",
      "text": "and we have uh batches and we have the",
      "offset": 2573.079,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "time component and we have information",
      "offset": 2575.8,
      "duration": 5.279
    },
    {
      "lang": "en",
      "text": "at each point in the sequence so",
      "offset": 2578.04,
      "duration": 5.4
    },
    {
      "lang": "en",
      "text": "see now what we would like to do is we",
      "offset": 2581.079,
      "duration": 5.121
    },
    {
      "lang": "en",
      "text": "would like these um tokens so we have up",
      "offset": 2583.44,
      "duration": 5.32
    },
    {
      "lang": "en",
      "text": "to eight tokens here in a batch and",
      "offset": 2586.2,
      "duration": 4.119
    },
    {
      "lang": "en",
      "text": "these eight tokens are currently not",
      "offset": 2588.76,
      "duration": 2.839
    },
    {
      "lang": "en",
      "text": "talking to each other and we would like",
      "offset": 2590.319,
      "duration": 2.841
    },
    {
      "lang": "en",
      "text": "them to talk to each other we'd like to",
      "offset": 2591.599,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "couple them and in particular we don't",
      "offset": 2593.16,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "we we want to couple them in a very",
      "offset": 2597.359,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "specific way so the token for example at",
      "offset": 2598.76,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "the fifth location it should not",
      "offset": 2601.44,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "communicate with tokens in the sixth",
      "offset": 2603.76,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "seventh and eighth location",
      "offset": 2605.68,
      "duration": 4.04
    },
    {
      "lang": "en",
      "text": "because uh those are future tokens in",
      "offset": 2607.76,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "the sequence the token on the fifth",
      "offset": 2609.72,
      "duration": 3.8
    },
    {
      "lang": "en",
      "text": "location should only talk to the one in",
      "offset": 2611.839,
      "duration": 4.361
    },
    {
      "lang": "en",
      "text": "the fourth third second and first so",
      "offset": 2613.52,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "it's only so information only flows from",
      "offset": 2616.2,
      "duration": 4.28
    },
    {
      "lang": "en",
      "text": "previous context to the current time",
      "offset": 2618.72,
      "duration": 3.8
    },
    {
      "lang": "en",
      "text": "step and we cannot get any information",
      "offset": 2620.48,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "from the future because we are about to",
      "offset": 2622.52,
      "duration": 3.039
    },
    {
      "lang": "en",
      "text": "try to predict the",
      "offset": 2624.079,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "future so what is the easiest way for",
      "offset": 2625.559,
      "duration": 6.56
    },
    {
      "lang": "en",
      "text": "tokens to communicate okay the easiest",
      "offset": 2629.28,
      "duration": 5.319
    },
    {
      "lang": "en",
      "text": "way I would say is okay if we're up to",
      "offset": 2632.119,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "if we're a fifth token and I'd like to",
      "offset": 2634.599,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "communicate with my past the simplest",
      "offset": 2636.28,
      "duration": 4.279
    },
    {
      "lang": "en",
      "text": "way we can do that is to just do a",
      "offset": 2638.44,
      "duration": 5.119
    },
    {
      "lang": "en",
      "text": "weight is to just do an average of all",
      "offset": 2640.559,
      "duration": 5.681
    },
    {
      "lang": "en",
      "text": "the um of all the preceding elements so",
      "offset": 2643.559,
      "duration": 4.601
    },
    {
      "lang": "en",
      "text": "for example if I'm the fif token I would",
      "offset": 2646.24,
      "duration": 4.44
    },
    {
      "lang": "en",
      "text": "like to take the channels uh that make",
      "offset": 2648.16,
      "duration": 5.8
    },
    {
      "lang": "en",
      "text": "up that are information at my step but",
      "offset": 2650.68,
      "duration": 4.919
    },
    {
      "lang": "en",
      "text": "then also the channels from the fourth",
      "offset": 2653.96,
      "duration": 3.159
    },
    {
      "lang": "en",
      "text": "step third step second step and the",
      "offset": 2655.599,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "first step I'd like to average those up",
      "offset": 2657.119,
      "duration": 4.041
    },
    {
      "lang": "en",
      "text": "and then that would become sort of like",
      "offset": 2659.76,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "a feature Vector that summarizes me in",
      "offset": 2661.16,
      "duration": 5.159
    },
    {
      "lang": "en",
      "text": "the context of my history now of course",
      "offset": 2663.839,
      "duration": 4.841
    },
    {
      "lang": "en",
      "text": "just doing a sum or like an average is",
      "offset": 2666.319,
      "duration": 4.121
    },
    {
      "lang": "en",
      "text": "an extremely weak form of interaction",
      "offset": 2668.68,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "like this communication is uh extremely",
      "offset": 2670.44,
      "duration": 3.56
    },
    {
      "lang": "en",
      "text": "lossy we've lost a ton of information",
      "offset": 2672.2,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "about the spatial Arrangements of all",
      "offset": 2674,
      "duration": 4.119
    },
    {
      "lang": "en",
      "text": "those tokens uh but that's okay for now",
      "offset": 2675.72,
      "duration": 3.56
    },
    {
      "lang": "en",
      "text": "we'll see how we can bring that",
      "offset": 2678.119,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "information back later for now what we",
      "offset": 2679.28,
      "duration": 4.52
    },
    {
      "lang": "en",
      "text": "would like to do is for every single",
      "offset": 2681.559,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "batch element independently for every",
      "offset": 2683.8,
      "duration": 5.759
    },
    {
      "lang": "en",
      "text": "teeth token in that sequence we'd like",
      "offset": 2686.52,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "to now calculate the average of all the",
      "offset": 2689.559,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "vectors in all the previous tokens and",
      "offset": 2693,
      "duration": 5.559
    },
    {
      "lang": "en",
      "text": "also at this token so let's write that",
      "offset": 2695.559,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "out um I have a small snippet here and",
      "offset": 2698.559,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "instead of just fumbling around let me",
      "offset": 2701.559,
      "duration": 3.961
    },
    {
      "lang": "en",
      "text": "just copy paste it and talk to",
      "offset": 2703.76,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "it so in other words we're going to",
      "offset": 2705.52,
      "duration": 6.88
    },
    {
      "lang": "en",
      "text": "create X and B is short for bag of words",
      "offset": 2708,
      "duration": 7.079
    },
    {
      "lang": "en",
      "text": "because bag of words is um is kind of",
      "offset": 2712.4,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "like um a term that people use when you",
      "offset": 2715.079,
      "duration": 4.201
    },
    {
      "lang": "en",
      "text": "are just averaging up things so this is",
      "offset": 2717.76,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "just a bag of words basically there's a",
      "offset": 2719.28,
      "duration": 4.44
    },
    {
      "lang": "en",
      "text": "word stored on every one of these eight",
      "offset": 2721.359,
      "duration": 3.96
    },
    {
      "lang": "en",
      "text": "locations and we're doing a bag of words",
      "offset": 2723.72,
      "duration": 3.72
    },
    {
      "lang": "en",
      "text": "we're just averaging",
      "offset": 2725.319,
      "duration": 3.561
    },
    {
      "lang": "en",
      "text": "so in the beginning we're going to say",
      "offset": 2727.44,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "that it's just initialized at Zero and",
      "offset": 2728.88,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "then I'm doing a for Loop here so we're",
      "offset": 2730.72,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "not being efficient yet that's coming",
      "offset": 2732.16,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "but for now we're just iterating over",
      "offset": 2734.48,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "all the batch Dimensions independently",
      "offset": 2736,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "iterating over time and then the",
      "offset": 2738,
      "duration": 7.319
    },
    {
      "lang": "en",
      "text": "previous uh tokens are at this uh batch",
      "offset": 2740.88,
      "duration": 6.959
    },
    {
      "lang": "en",
      "text": "Dimension and then everything up to and",
      "offset": 2745.319,
      "duration": 5.961
    },
    {
      "lang": "en",
      "text": "including the teeth token okay so when",
      "offset": 2747.839,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "we slice out X in this way X prev",
      "offset": 2751.28,
      "duration": 7.2
    },
    {
      "lang": "en",
      "text": "Becomes of shape um how many T elements",
      "offset": 2754.319,
      "duration": 5.8
    },
    {
      "lang": "en",
      "text": "there were in the past and then of",
      "offset": 2758.48,
      "duration": 4.119
    },
    {
      "lang": "en",
      "text": "course C so all the two-dimensional",
      "offset": 2760.119,
      "duration": 5.361
    },
    {
      "lang": "en",
      "text": "information from these little tokens so",
      "offset": 2762.599,
      "duration": 5.48
    },
    {
      "lang": "en",
      "text": "that's the previous uh sort of chunk of",
      "offset": 2765.48,
      "duration": 6.56
    },
    {
      "lang": "en",
      "text": "um tokens from my current sequence and",
      "offset": 2768.079,
      "duration": 5.721
    },
    {
      "lang": "en",
      "text": "then I'm just doing the average or the",
      "offset": 2772.04,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "mean over the zero Dimension so I'm",
      "offset": 2773.8,
      "duration": 5.279
    },
    {
      "lang": "en",
      "text": "averaging out the time here and I'm just",
      "offset": 2775.88,
      "duration": 5.439
    },
    {
      "lang": "en",
      "text": "going to get a little c one dimensional",
      "offset": 2779.079,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "Vector which I'm going to store in X bag",
      "offset": 2781.319,
      "duration": 6.201
    },
    {
      "lang": "en",
      "text": "of words so I can run this and and uh",
      "offset": 2783.88,
      "duration": 6.32
    },
    {
      "lang": "en",
      "text": "this is not going to be very informative",
      "offset": 2787.52,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "because let's see so this is X of Zer so",
      "offset": 2790.2,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "this is the zeroth batch element and",
      "offset": 2792.72,
      "duration": 7.32
    },
    {
      "lang": "en",
      "text": "then expo at zero now you see how the at",
      "offset": 2795.4,
      "duration": 7
    },
    {
      "lang": "en",
      "text": "the first location here you see that the",
      "offset": 2800.04,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "two are equal and that's because it's",
      "offset": 2802.4,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "we're just doing an average of this one",
      "offset": 2805,
      "duration": 4.52
    },
    {
      "lang": "en",
      "text": "token but here this one is now an",
      "offset": 2806.72,
      "duration": 6.399
    },
    {
      "lang": "en",
      "text": "average of these two and now this one is",
      "offset": 2809.52,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "an average of these",
      "offset": 2813.119,
      "duration": 4.761
    },
    {
      "lang": "en",
      "text": "three and so on",
      "offset": 2814.96,
      "duration": 6.28
    },
    {
      "lang": "en",
      "text": "so uh and this last one is the average",
      "offset": 2817.88,
      "duration": 5.16
    },
    {
      "lang": "en",
      "text": "of all of these elements so vertical",
      "offset": 2821.24,
      "duration": 4.04
    },
    {
      "lang": "en",
      "text": "average just averaging up all the tokens",
      "offset": 2823.04,
      "duration": 4.36
    },
    {
      "lang": "en",
      "text": "now gives this outcome",
      "offset": 2825.28,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "here so this is all well and good uh but",
      "offset": 2827.4,
      "duration": 5.56
    },
    {
      "lang": "en",
      "text": "this is very inefficient now the trick",
      "offset": 2830.319,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "is that we can be very very efficient",
      "offset": 2832.96,
      "duration": 3.48
    },
    {
      "lang": "en",
      "text": "about doing this using matrix",
      "offset": 2834.4,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "multiplication so that's the",
      "offset": 2836.44,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "mathematical trick and let me show you",
      "offset": 2838.16,
      "duration": 3.32
    },
    {
      "lang": "en",
      "text": "what I mean let's work with the toy",
      "offset": 2839.64,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "example here let me run it and I'll",
      "offset": 2841.48,
      "duration": 6.2
    },
    {
      "lang": "en",
      "text": "explain I have a simple Matrix here that",
      "offset": 2844.44,
      "duration": 6.6
    },
    {
      "lang": "en",
      "text": "is a 3X3 of all ones a matrix B of just",
      "offset": 2847.68,
      "duration": 5.96
    },
    {
      "lang": "en",
      "text": "random numbers and it's a 3x2 and a",
      "offset": 2851.04,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "matrix C which will be 3x3 multip 3x2",
      "offset": 2853.64,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "which will give out a 3x2 so here we're",
      "offset": 2856.88,
      "duration": 6.92
    },
    {
      "lang": "en",
      "text": "just using um matrix multiplication so a",
      "offset": 2859.88,
      "duration": 6.32
    },
    {
      "lang": "en",
      "text": "multiply B gives us",
      "offset": 2863.8,
      "duration": 7.6
    },
    {
      "lang": "en",
      "text": "C okay so how are these numbers in C um",
      "offset": 2866.2,
      "duration": 7.84
    },
    {
      "lang": "en",
      "text": "achieved right so this number in the top",
      "offset": 2871.4,
      "duration": 6.36
    },
    {
      "lang": "en",
      "text": "left is the first row of a dot product",
      "offset": 2874.04,
      "duration": 6.76
    },
    {
      "lang": "en",
      "text": "with the First Column of B and since all",
      "offset": 2877.76,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "the the row of a right now is all just",
      "offset": 2880.8,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "ones then the do product here with with",
      "offset": 2882.88,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "this column of B is just going to do a",
      "offset": 2885.839,
      "duration": 5.881
    },
    {
      "lang": "en",
      "text": "sum of these of this column so 2 + 6 + 6",
      "offset": 2887.88,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "is",
      "offset": 2891.72,
      "duration": 3.8
    },
    {
      "lang": "en",
      "text": "14 the element here in the output of C",
      "offset": 2892.599,
      "duration": 4.921
    },
    {
      "lang": "en",
      "text": "is also the first column here the first",
      "offset": 2895.52,
      "duration": 4.68
    },
    {
      "lang": "en",
      "text": "row of a multiplied now with the second",
      "offset": 2897.52,
      "duration": 7.64
    },
    {
      "lang": "en",
      "text": "column of B so 7 + 4 + 5 is 16 now you",
      "offset": 2900.2,
      "duration": 6.159
    },
    {
      "lang": "en",
      "text": "see that there's repeating elements here",
      "offset": 2905.16,
      "duration": 3.56
    },
    {
      "lang": "en",
      "text": "so this 14 again is because this row is",
      "offset": 2906.359,
      "duration": 4.121
    },
    {
      "lang": "en",
      "text": "again all ones and it's multiplying the",
      "offset": 2908.72,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "First Column of B so we get 14 and this",
      "offset": 2910.48,
      "duration": 5.32
    },
    {
      "lang": "en",
      "text": "one is and so on so this last number",
      "offset": 2913.44,
      "duration": 5.919
    },
    {
      "lang": "en",
      "text": "here is the last row do product last",
      "offset": 2915.8,
      "duration": 6.559
    },
    {
      "lang": "en",
      "text": "column now the trick here is uh the",
      "offset": 2919.359,
      "duration": 5.401
    },
    {
      "lang": "en",
      "text": "following this is just a boring number",
      "offset": 2922.359,
      "duration": 5.72
    },
    {
      "lang": "en",
      "text": "of um it's just a boring array of all",
      "offset": 2924.76,
      "duration": 5.96
    },
    {
      "lang": "en",
      "text": "ones but torch has this function called",
      "offset": 2928.079,
      "duration": 6.121
    },
    {
      "lang": "en",
      "text": "Trail which is short for a",
      "offset": 2930.72,
      "duration": 6.079
    },
    {
      "lang": "en",
      "text": "triangular uh something like that and",
      "offset": 2934.2,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "you can wrap it in torch up once and it",
      "offset": 2936.799,
      "duration": 3.56
    },
    {
      "lang": "en",
      "text": "will just return the lower triangular",
      "offset": 2938.359,
      "duration": 5.401
    },
    {
      "lang": "en",
      "text": "portion of this",
      "offset": 2940.359,
      "duration": 6.2
    },
    {
      "lang": "en",
      "text": "okay so now it will basically zero out",
      "offset": 2943.76,
      "duration": 4.359
    },
    {
      "lang": "en",
      "text": "uh these guys here so we just get the",
      "offset": 2946.559,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "lower triangular part well what happens",
      "offset": 2948.119,
      "duration": 5.521
    },
    {
      "lang": "en",
      "text": "if we do",
      "offset": 2950.4,
      "duration": 3.24
    },
    {
      "lang": "en",
      "text": "that so now we'll have a like this and B",
      "offset": 2954.04,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "like this and now what are we getting",
      "offset": 2957.599,
      "duration": 4.441
    },
    {
      "lang": "en",
      "text": "here in C well what is this number well",
      "offset": 2958.92,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "this is the first row times the First",
      "offset": 2962.04,
      "duration": 6.6
    },
    {
      "lang": "en",
      "text": "Column and because this is zeros",
      "offset": 2964.52,
      "duration": 6.12
    },
    {
      "lang": "en",
      "text": "uh these elements here are now ignored",
      "offset": 2968.64,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "so we just get a two and then this",
      "offset": 2970.64,
      "duration": 4.76
    },
    {
      "lang": "en",
      "text": "number here is the first row times the",
      "offset": 2972.88,
      "duration": 4.36
    },
    {
      "lang": "en",
      "text": "second column and because these are",
      "offset": 2975.4,
      "duration": 3.88
    },
    {
      "lang": "en",
      "text": "zeros they get ignored and it's just",
      "offset": 2977.24,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "seven this seven multiplies this one but",
      "offset": 2979.28,
      "duration": 4.68
    },
    {
      "lang": "en",
      "text": "look what happened here because this is",
      "offset": 2982.52,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "one and then zeros we what ended up",
      "offset": 2983.96,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "happening is we're just plucking out the",
      "offset": 2986.76,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "row of this row of B and that's what we",
      "offset": 2988.28,
      "duration": 9.079
    },
    {
      "lang": "en",
      "text": "got now here we have one 1 Z so here 110",
      "offset": 2991.24,
      "duration": 7.76
    },
    {
      "lang": "en",
      "text": "do product with these two columns will",
      "offset": 2997.359,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "now give us 2 + 6 which is 8 and 7 + 4",
      "offset": 2999,
      "duration": 6.4
    },
    {
      "lang": "en",
      "text": "which is 11 and because this is 111 we",
      "offset": 3002,
      "duration": 5.799
    },
    {
      "lang": "en",
      "text": "ended up with the addition of all of",
      "offset": 3005.4,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "them and so basically depending on how",
      "offset": 3007.799,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "many ones and zeros we have here we are",
      "offset": 3010.2,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "basically doing a sum currently of a",
      "offset": 3012.839,
      "duration": 6.041
    },
    {
      "lang": "en",
      "text": "variable number of these rows and that",
      "offset": 3016.68,
      "duration": 4.04
    },
    {
      "lang": "en",
      "text": "gets deposited into",
      "offset": 3018.88,
      "duration": 4.28
    },
    {
      "lang": "en",
      "text": "C So currently we're doing sums because",
      "offset": 3020.72,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "these are ones but we can also do",
      "offset": 3023.16,
      "duration": 4.439
    },
    {
      "lang": "en",
      "text": "average right and you can start to see",
      "offset": 3025.119,
      "duration": 4.841
    },
    {
      "lang": "en",
      "text": "how we could do average uh of the rows",
      "offset": 3027.599,
      "duration": 5.041
    },
    {
      "lang": "en",
      "text": "of B uh sort of in an incremental",
      "offset": 3029.96,
      "duration": 5.24
    },
    {
      "lang": "en",
      "text": "fashion because we don't have to we can",
      "offset": 3032.64,
      "duration": 4.919
    },
    {
      "lang": "en",
      "text": "basically normalize these rows so that",
      "offset": 3035.2,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "they sum to one and then we're going to",
      "offset": 3037.559,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "get an average so if we took a and then",
      "offset": 3039.04,
      "duration": 4.519
    },
    {
      "lang": "en",
      "text": "we did aals",
      "offset": 3041.96,
      "duration": 9.92
    },
    {
      "lang": "en",
      "text": "aide torch. sum in the um of a in the um",
      "offset": 3043.559,
      "duration": 11.76
    },
    {
      "lang": "en",
      "text": "oneth Dimension and then let's keep them",
      "offset": 3051.88,
      "duration": 5.679
    },
    {
      "lang": "en",
      "text": "as true so so therefore the broadcasting",
      "offset": 3055.319,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "will work out so if I rerun this you see",
      "offset": 3057.559,
      "duration": 6.441
    },
    {
      "lang": "en",
      "text": "now that these rows now sum to one so",
      "offset": 3060.839,
      "duration": 6.28
    },
    {
      "lang": "en",
      "text": "this row is one this row is 0. 5.5 Z and",
      "offset": 3064,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "here we get 1/3 and now when we do a",
      "offset": 3067.119,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "multiply B what are we getting here we",
      "offset": 3069.92,
      "duration": 5.879
    },
    {
      "lang": "en",
      "text": "are just getting the first row first row",
      "offset": 3072.799,
      "duration": 5.361
    },
    {
      "lang": "en",
      "text": "here now we are getting the average of",
      "offset": 3075.799,
      "duration": 4.361
    },
    {
      "lang": "en",
      "text": "the first two",
      "offset": 3078.16,
      "duration": 5.24
    },
    {
      "lang": "en",
      "text": "rows okay so 2 and six average is four",
      "offset": 3080.16,
      "duration": 4.919
    },
    {
      "lang": "en",
      "text": "and four and seven average is",
      "offset": 3083.4,
      "duration": 4.439
    },
    {
      "lang": "en",
      "text": "5.5 and on the bottom here we are now",
      "offset": 3085.079,
      "duration": 6.401
    },
    {
      "lang": "en",
      "text": "getting the average of these three rows",
      "offset": 3087.839,
      "duration": 5.96
    },
    {
      "lang": "en",
      "text": "so the average of all of elements of B",
      "offset": 3091.48,
      "duration": 5.48
    },
    {
      "lang": "en",
      "text": "are now deposited here and so you can",
      "offset": 3093.799,
      "duration": 6.201
    },
    {
      "lang": "en",
      "text": "see that by manipulating these uh",
      "offset": 3096.96,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "elements of this multiplying Matrix and",
      "offset": 3100,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "then multiplying it with any given",
      "offset": 3102.4,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "Matrix we can do these averages in this",
      "offset": 3104.48,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "incremental fashion because we just get",
      "offset": 3107.44,
      "duration": 5.679
    },
    {
      "lang": "en",
      "text": "um and we can manipulate that based on",
      "offset": 3110.4,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "the elements of a okay so that's very",
      "offset": 3113.119,
      "duration": 4.361
    },
    {
      "lang": "en",
      "text": "convenient so let's let's swing back up",
      "offset": 3115.28,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "here and see how we can vectorize this",
      "offset": 3117.48,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "and make it much more efficient using",
      "offset": 3119.2,
      "duration": 4.44
    },
    {
      "lang": "en",
      "text": "what we've learned so in",
      "offset": 3120.839,
      "duration": 4.921
    },
    {
      "lang": "en",
      "text": "particular we are going to produce an",
      "offset": 3123.64,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "array a but here I'm going to call it we",
      "offset": 3125.76,
      "duration": 5.96
    },
    {
      "lang": "en",
      "text": "short for weights but this is our",
      "offset": 3128.28,
      "duration": 6.6
    },
    {
      "lang": "en",
      "text": "a and this is how much of every row we",
      "offset": 3131.72,
      "duration": 5.48
    },
    {
      "lang": "en",
      "text": "want to average up and it's going to be",
      "offset": 3134.88,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "an average because you can see that",
      "offset": 3137.2,
      "duration": 2.84
    },
    {
      "lang": "en",
      "text": "these rows sum to",
      "offset": 3138.319,
      "duration": 5.24
    },
    {
      "lang": "en",
      "text": "one so this is our a and then our B in",
      "offset": 3140.04,
      "duration": 7.759
    },
    {
      "lang": "en",
      "text": "this example of course is X",
      "offset": 3143.559,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "so what's going to happen here now is",
      "offset": 3147.799,
      "duration": 4.121
    },
    {
      "lang": "en",
      "text": "that we are going to have an expo",
      "offset": 3149.319,
      "duration": 7.24
    },
    {
      "lang": "en",
      "text": "2 and this Expo 2 is going to be way",
      "offset": 3151.92,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "multiplying",
      "offset": 3156.559,
      "duration": 6.121
    },
    {
      "lang": "en",
      "text": "RX so let's think this true way is T BYT",
      "offset": 3158.4,
      "duration": 6.12
    },
    {
      "lang": "en",
      "text": "and this is Matrix multiplying in",
      "offset": 3162.68,
      "duration": 4.52
    },
    {
      "lang": "en",
      "text": "pytorch a b by T by",
      "offset": 3164.52,
      "duration": 6.4
    },
    {
      "lang": "en",
      "text": "C and it's giving us uh different what",
      "offset": 3167.2,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "shape so pytorch will come here and it",
      "offset": 3170.92,
      "duration": 3.879
    },
    {
      "lang": "en",
      "text": "will see that these shapes are not the",
      "offset": 3172.96,
      "duration": 4.44
    },
    {
      "lang": "en",
      "text": "same so it will create a batch Dimension",
      "offset": 3174.799,
      "duration": 5.32
    },
    {
      "lang": "en",
      "text": "here and this is a batched matrix",
      "offset": 3177.4,
      "duration": 4.679
    },
    {
      "lang": "en",
      "text": "multiply and so it will apply this",
      "offset": 3180.119,
      "duration": 3.881
    },
    {
      "lang": "en",
      "text": "matrix multiplication in all the batch",
      "offset": 3182.079,
      "duration": 5.961
    },
    {
      "lang": "en",
      "text": "elements um in parallel and individually",
      "offset": 3184,
      "duration": 5.799
    },
    {
      "lang": "en",
      "text": "and then for each batch element there",
      "offset": 3188.04,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "will be a t BYT multiplying T by C",
      "offset": 3189.799,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "exactly as we had",
      "offset": 3192.76,
      "duration": 7.88
    },
    {
      "lang": "en",
      "text": "below so this will now create B by T by",
      "offset": 3195.799,
      "duration": 8.8
    },
    {
      "lang": "en",
      "text": "C and Expo 2 will now become identical",
      "offset": 3200.64,
      "duration": 7.199
    },
    {
      "lang": "en",
      "text": "to Expo",
      "offset": 3204.599,
      "duration": 3.24
    },
    {
      "lang": "en",
      "text": "so we can see that torch. all close of",
      "offset": 3208.119,
      "duration": 8.281
    },
    {
      "lang": "en",
      "text": "xbo and xbo 2 should be true",
      "offset": 3212.319,
      "duration": 6.52
    },
    {
      "lang": "en",
      "text": "now so this kind of like convinces us",
      "offset": 3216.4,
      "duration": 6.8
    },
    {
      "lang": "en",
      "text": "that uh these are in fact um the same so",
      "offset": 3218.839,
      "duration": 8.28
    },
    {
      "lang": "en",
      "text": "xbo and xbo 2 if I just print",
      "offset": 3223.2,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "them uh okay we're not going to be able",
      "offset": 3227.119,
      "duration": 4.601
    },
    {
      "lang": "en",
      "text": "to okay we're not going to be able to",
      "offset": 3229.68,
      "duration": 4.52
    },
    {
      "lang": "en",
      "text": "just stare it down but",
      "offset": 3231.72,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "um well let me try Expo basically just",
      "offset": 3234.2,
      "duration": 4.04
    },
    {
      "lang": "en",
      "text": "at the zeroth element and Expo two at",
      "offset": 3236.68,
      "duration": 3.159
    },
    {
      "lang": "en",
      "text": "the zeroth element so just the first",
      "offset": 3238.24,
      "duration": 4.2
    },
    {
      "lang": "en",
      "text": "batch and we should see that this and",
      "offset": 3239.839,
      "duration": 4.76
    },
    {
      "lang": "en",
      "text": "that should be identical which they",
      "offset": 3242.44,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "are right so what happened here the",
      "offset": 3244.599,
      "duration": 4.921
    },
    {
      "lang": "en",
      "text": "trick is we were able to use batched",
      "offset": 3247.44,
      "duration": 5.32
    },
    {
      "lang": "en",
      "text": "Matrix multiply to do this uh",
      "offset": 3249.52,
      "duration": 6.2
    },
    {
      "lang": "en",
      "text": "aggregation really and it's a weighted",
      "offset": 3252.76,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "aggregation and the weights are",
      "offset": 3255.72,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "specified in this um T BYT array and",
      "offset": 3257.72,
      "duration": 6.72
    },
    {
      "lang": "en",
      "text": "we're basically doing weighted sums and",
      "offset": 3261.48,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "uh these weighted sums are are U",
      "offset": 3264.44,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "according to uh the weights inside here",
      "offset": 3266.44,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "they take on sort of this triangular",
      "offset": 3268.839,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "form and so that means that a token at",
      "offset": 3271.16,
      "duration": 5.08
    },
    {
      "lang": "en",
      "text": "the teth dimension will only get uh sort",
      "offset": 3273.72,
      "duration": 5.879
    },
    {
      "lang": "en",
      "text": "of um information from the um tokens",
      "offset": 3276.24,
      "duration": 5.24
    },
    {
      "lang": "en",
      "text": "perceiving it so that's exactly what we",
      "offset": 3279.599,
      "duration": 3.96
    },
    {
      "lang": "en",
      "text": "want and finally I would like to rewrite",
      "offset": 3281.48,
      "duration": 4.68
    },
    {
      "lang": "en",
      "text": "it in one more way and we're going to",
      "offset": 3283.559,
      "duration": 5.081
    },
    {
      "lang": "en",
      "text": "see why that's useful so this is the",
      "offset": 3286.16,
      "duration": 4.76
    },
    {
      "lang": "en",
      "text": "third version and it's also identical to",
      "offset": 3288.64,
      "duration": 4.6
    },
    {
      "lang": "en",
      "text": "the first and second but let me talk",
      "offset": 3290.92,
      "duration": 3.8
    },
    {
      "lang": "en",
      "text": "through it it uses",
      "offset": 3293.24,
      "duration": 7.119
    },
    {
      "lang": "en",
      "text": "softmax so Trill here is this Matrix",
      "offset": 3294.72,
      "duration": 7.04
    },
    {
      "lang": "en",
      "text": "lower triangular",
      "offset": 3300.359,
      "duration": 5.041
    },
    {
      "lang": "en",
      "text": "ones way begins as all",
      "offset": 3301.76,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "zero okay so if I just print way in the",
      "offset": 3305.4,
      "duration": 6.28
    },
    {
      "lang": "en",
      "text": "beginning it's all zero then I",
      "offset": 3307.92,
      "duration": 7.28
    },
    {
      "lang": "en",
      "text": "used masked fill so what this is doing",
      "offset": 3311.68,
      "duration": 6.679
    },
    {
      "lang": "en",
      "text": "is we. masked fill it's all zeros and",
      "offset": 3315.2,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "I'm saying for all the elements where",
      "offset": 3318.359,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "Trill is equal equal Z make them be",
      "offset": 3320.72,
      "duration": 5.639
    },
    {
      "lang": "en",
      "text": "negative Infinity so all the elements",
      "offset": 3323.799,
      "duration": 4.681
    },
    {
      "lang": "en",
      "text": "where Trill is zero will become negative",
      "offset": 3326.359,
      "duration": 5.96
    },
    {
      "lang": "en",
      "text": "Infinity now so this is what we get and",
      "offset": 3328.48,
      "duration": 7.8
    },
    {
      "lang": "en",
      "text": "then the final line here is",
      "offset": 3332.319,
      "duration": 6.601
    },
    {
      "lang": "en",
      "text": "softmax so if I take a softmax along",
      "offset": 3336.28,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "every single so dim is negative one so",
      "offset": 3338.92,
      "duration": 5.48
    },
    {
      "lang": "en",
      "text": "along every single row if I do softmax",
      "offset": 3340.92,
      "duration": 5.72
    },
    {
      "lang": "en",
      "text": "what is that going to",
      "offset": 3344.4,
      "duration": 7.56
    },
    {
      "lang": "en",
      "text": "do well softmax is um is also like a",
      "offset": 3346.64,
      "duration": 8.28
    },
    {
      "lang": "en",
      "text": "normalization operation right and so",
      "offset": 3351.96,
      "duration": 6.079
    },
    {
      "lang": "en",
      "text": "spoiler alert you get the exact same",
      "offset": 3354.92,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "Matrix let me bring back to",
      "offset": 3358.039,
      "duration": 4.76
    },
    {
      "lang": "en",
      "text": "softmax and recall that in softmax we're",
      "offset": 3360.28,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "going to exponentiate every single one",
      "offset": 3362.799,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "of these and then we're going to divide",
      "offset": 3364.52,
      "duration": 5.48
    },
    {
      "lang": "en",
      "text": "by the sum and so if we exponentiate",
      "offset": 3366.799,
      "duration": 4.52
    },
    {
      "lang": "en",
      "text": "every single element here we're going to",
      "offset": 3370,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "get a one and here we're going to get uh",
      "offset": 3371.319,
      "duration": 5.8
    },
    {
      "lang": "en",
      "text": "basically zero 0 z0 Z everywhere else",
      "offset": 3374,
      "duration": 5.359
    },
    {
      "lang": "en",
      "text": "and then when we normalize we just get",
      "offset": 3377.119,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "one here we're going to get one one and",
      "offset": 3379.359,
      "duration": 5.281
    },
    {
      "lang": "en",
      "text": "then zeros and then softmax will again",
      "offset": 3381.839,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "divide and this will give us 5.5 and so",
      "offset": 3384.64,
      "duration": 6.12
    },
    {
      "lang": "en",
      "text": "on and so this is also the uh the same",
      "offset": 3387.359,
      "duration": 6.321
    },
    {
      "lang": "en",
      "text": "way to produce uh this mask now the",
      "offset": 3390.76,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "reason that this is a bit more",
      "offset": 3393.68,
      "duration": 2.52
    },
    {
      "lang": "en",
      "text": "interesting and the reason we're going",
      "offset": 3394.76,
      "duration": 3.16
    },
    {
      "lang": "en",
      "text": "to end up using it in self",
      "offset": 3396.2,
      "duration": 5.599
    },
    {
      "lang": "en",
      "text": "attention is that these weights here",
      "offset": 3397.92,
      "duration": 6.76
    },
    {
      "lang": "en",
      "text": "begin uh with zero and you can think of",
      "offset": 3401.799,
      "duration": 4.841
    },
    {
      "lang": "en",
      "text": "this as like an interaction strength or",
      "offset": 3404.68,
      "duration": 4.52
    },
    {
      "lang": "en",
      "text": "like an affinity so basically it's",
      "offset": 3406.64,
      "duration": 5.64
    },
    {
      "lang": "en",
      "text": "telling us how much of each uh token",
      "offset": 3409.2,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "from the past do we want to Aggregate",
      "offset": 3412.28,
      "duration": 4.92
    },
    {
      "lang": "en",
      "text": "and average up",
      "offset": 3414.88,
      "duration": 5.08
    },
    {
      "lang": "en",
      "text": "and then this line is saying tokens from",
      "offset": 3417.2,
      "duration": 5.359
    },
    {
      "lang": "en",
      "text": "the past cannot communicate by setting",
      "offset": 3419.96,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "them to negative Infinity we're saying",
      "offset": 3422.559,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "that we will not aggregate anything from",
      "offset": 3424.44,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "those",
      "offset": 3426.319,
      "duration": 3.401
    },
    {
      "lang": "en",
      "text": "tokens and so basically this then goes",
      "offset": 3427.4,
      "duration": 3.959
    },
    {
      "lang": "en",
      "text": "through softmax and through the weighted",
      "offset": 3429.72,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "and this is the aggregation through",
      "offset": 3431.359,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "matrix",
      "offset": 3432.68,
      "duration": 3.48
    },
    {
      "lang": "en",
      "text": "multiplication and so what this is now",
      "offset": 3434.079,
      "duration": 5.081
    },
    {
      "lang": "en",
      "text": "is you can think of these as um these",
      "offset": 3436.16,
      "duration": 5.399
    },
    {
      "lang": "en",
      "text": "zeros are currently just set by us to be",
      "offset": 3439.16,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "zero but a quick preview is that these",
      "offset": 3441.559,
      "duration": 5.881
    },
    {
      "lang": "en",
      "text": "affinities between the tokens are not",
      "offset": 3445.16,
      "duration": 4.04
    },
    {
      "lang": "en",
      "text": "going to be just constant at zero",
      "offset": 3447.44,
      "duration": 3.96
    },
    {
      "lang": "en",
      "text": "they're going to be data dependent these",
      "offset": 3449.2,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "tokens are going to start looking at",
      "offset": 3451.4,
      "duration": 3.48
    },
    {
      "lang": "en",
      "text": "each other and some tokens will find",
      "offset": 3452.799,
      "duration": 4.841
    },
    {
      "lang": "en",
      "text": "other tokens more or less interesting",
      "offset": 3454.88,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "and depending on what their values are",
      "offset": 3457.64,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "they're going to find each other",
      "offset": 3459.599,
      "duration": 3.321
    },
    {
      "lang": "en",
      "text": "interesting to different amounts and I'm",
      "offset": 3461,
      "duration": 4.28
    },
    {
      "lang": "en",
      "text": "going to call those affinities I think",
      "offset": 3462.92,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "and then here we are saying the future",
      "offset": 3465.28,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "cannot communicate with the past we're",
      "offset": 3467.079,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "we're going to clamp them and then when",
      "offset": 3469.119,
      "duration": 4.521
    },
    {
      "lang": "en",
      "text": "we normalize and sum we're going to",
      "offset": 3471.559,
      "duration": 4.601
    },
    {
      "lang": "en",
      "text": "aggregate uh sort of their values",
      "offset": 3473.64,
      "duration": 3.8
    },
    {
      "lang": "en",
      "text": "depending on how interesting they find",
      "offset": 3476.16,
      "duration": 3.32
    },
    {
      "lang": "en",
      "text": "each other and so that's the preview for",
      "offset": 3477.44,
      "duration": 5.679
    },
    {
      "lang": "en",
      "text": "self attention and basically long story",
      "offset": 3479.48,
      "duration": 5.599
    },
    {
      "lang": "en",
      "text": "short from this entire section is that",
      "offset": 3483.119,
      "duration": 4.601
    },
    {
      "lang": "en",
      "text": "you can do weighted aggregations of your",
      "offset": 3485.079,
      "duration": 3.561
    },
    {
      "lang": "en",
      "text": "past",
      "offset": 3487.72,
      "duration": 4.28
    },
    {
      "lang": "en",
      "text": "Elements by having by using matrix",
      "offset": 3488.64,
      "duration": 5.919
    },
    {
      "lang": "en",
      "text": "multiplication of a lower triangular",
      "offset": 3492,
      "duration": 5.119
    },
    {
      "lang": "en",
      "text": "fashion and then the elements here in",
      "offset": 3494.559,
      "duration": 4.201
    },
    {
      "lang": "en",
      "text": "the lower triangular part are telling",
      "offset": 3497.119,
      "duration": 4.521
    },
    {
      "lang": "en",
      "text": "you how much of each element uh fuses",
      "offset": 3498.76,
      "duration": 5.24
    },
    {
      "lang": "en",
      "text": "into this position so we're going to use",
      "offset": 3501.64,
      "duration": 3.88
    },
    {
      "lang": "en",
      "text": "this trick now to develop the self",
      "offset": 3504,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "attention block block so first let's get",
      "offset": 3505.52,
      "duration": 4.92
    },
    {
      "lang": "en",
      "text": "some quick preliminaries out of the way",
      "offset": 3507.68,
      "duration": 4.2
    },
    {
      "lang": "en",
      "text": "first the thing I'm kind of bothered by",
      "offset": 3510.44,
      "duration": 2.84
    },
    {
      "lang": "en",
      "text": "is that you see how we're passing in",
      "offset": 3511.88,
      "duration": 3.239
    },
    {
      "lang": "en",
      "text": "vocap size into the Constructor there's",
      "offset": 3513.28,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "no need to do that because vocap size is",
      "offset": 3515.119,
      "duration": 3.72
    },
    {
      "lang": "en",
      "text": "already defined uh up top as a global",
      "offset": 3516.72,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "variable so there's no need to pass this",
      "offset": 3518.839,
      "duration": 2.76
    },
    {
      "lang": "en",
      "text": "stuff",
      "offset": 3520.559,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "around next what I want to do is I don't",
      "offset": 3521.599,
      "duration": 4.921
    },
    {
      "lang": "en",
      "text": "want to actually create I want to create",
      "offset": 3524.559,
      "duration": 3.401
    },
    {
      "lang": "en",
      "text": "like a level of indirection here where",
      "offset": 3526.52,
      "duration": 3.16
    },
    {
      "lang": "en",
      "text": "we don't directly go to the embedding",
      "offset": 3527.96,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "for the um logits but instead we go",
      "offset": 3529.68,
      "duration": 4.76
    },
    {
      "lang": "en",
      "text": "through this intermediate phase because",
      "offset": 3532.839,
      "duration": 4.361
    },
    {
      "lang": "en",
      "text": "we're going to start making that bigger",
      "offset": 3534.44,
      "duration": 5.399
    },
    {
      "lang": "en",
      "text": "so let me introduce a new variable n",
      "offset": 3537.2,
      "duration": 5.48
    },
    {
      "lang": "en",
      "text": "embed it shorted for number of embedding",
      "offset": 3539.839,
      "duration": 4.681
    },
    {
      "lang": "en",
      "text": "Dimensions so",
      "offset": 3542.68,
      "duration": 6.8
    },
    {
      "lang": "en",
      "text": "nbed here will be say 32 that was a",
      "offset": 3544.52,
      "duration": 6.559
    },
    {
      "lang": "en",
      "text": "suggestion from GitHub co-pilot by the",
      "offset": 3549.48,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "way um it also suest 32 which is a good",
      "offset": 3551.079,
      "duration": 5.641
    },
    {
      "lang": "en",
      "text": "number so this is an embedding table and",
      "offset": 3554.44,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "only 32 dimensional",
      "offset": 3556.72,
      "duration": 4.52
    },
    {
      "lang": "en",
      "text": "embeddings so then here this is not",
      "offset": 3558.92,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "going to give us logits directly instead",
      "offset": 3561.24,
      "duration": 3.559
    },
    {
      "lang": "en",
      "text": "this is going to give us token",
      "offset": 3563.559,
      "duration": 3.641
    },
    {
      "lang": "en",
      "text": "embeddings that's I'm going to call it",
      "offset": 3564.799,
      "duration": 4.201
    },
    {
      "lang": "en",
      "text": "and then to go from the token Tings to",
      "offset": 3567.2,
      "duration": 3.56
    },
    {
      "lang": "en",
      "text": "the logits we're going to need a linear",
      "offset": 3569,
      "duration": 5.079
    },
    {
      "lang": "en",
      "text": "layer so self. LM head let's call it",
      "offset": 3570.76,
      "duration": 5.72
    },
    {
      "lang": "en",
      "text": "short for language modeling head is n",
      "offset": 3574.079,
      "duration": 5.801
    },
    {
      "lang": "en",
      "text": "and linear from n ined up to vocap size",
      "offset": 3576.48,
      "duration": 4.839
    },
    {
      "lang": "en",
      "text": "and then when we swing over here we're",
      "offset": 3579.88,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "actually going to get the loits by",
      "offset": 3581.319,
      "duration": 4.921
    },
    {
      "lang": "en",
      "text": "exactly what the co-pilot says now we",
      "offset": 3583.559,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "have to be careful here because this C",
      "offset": 3586.24,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "and this C are not equal um this is nmed",
      "offset": 3588.44,
      "duration": 7.08
    },
    {
      "lang": "en",
      "text": "C and this is vocap size so let's just",
      "offset": 3592.48,
      "duration": 5.48
    },
    {
      "lang": "en",
      "text": "say that n ined is equal to",
      "offset": 3595.52,
      "duration": 5.64
    },
    {
      "lang": "en",
      "text": "C and then this just creates one spous",
      "offset": 3597.96,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "layer of interaction through a linear",
      "offset": 3601.16,
      "duration": 5.919
    },
    {
      "lang": "en",
      "text": "layer but uh this should basically",
      "offset": 3602.839,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "run so we see that this runs and uh this",
      "offset": 3611.359,
      "duration": 5.641
    },
    {
      "lang": "en",
      "text": "currently looks kind of spous but uh",
      "offset": 3615.16,
      "duration": 3.879
    },
    {
      "lang": "en",
      "text": "we're going to build on top of this now",
      "offset": 3617,
      "duration": 5.079
    },
    {
      "lang": "en",
      "text": "next up so far we've taken these indices",
      "offset": 3619.039,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "and we've encoded them based on the",
      "offset": 3622.079,
      "duration": 6.081
    },
    {
      "lang": "en",
      "text": "identity of the uh tokens in inside idx",
      "offset": 3623.68,
      "duration": 6.399
    },
    {
      "lang": "en",
      "text": "the next thing that people very often do",
      "offset": 3628.16,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "is that we're not just encoding the",
      "offset": 3630.079,
      "duration": 3.72
    },
    {
      "lang": "en",
      "text": "identity of these tokens but also their",
      "offset": 3631.599,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "position so we're going to have a second",
      "offset": 3633.799,
      "duration": 4.601
    },
    {
      "lang": "en",
      "text": "position uh embedding table here so",
      "offset": 3635.92,
      "duration": 5.56
    },
    {
      "lang": "en",
      "text": "self. position embedding table is an an",
      "offset": 3638.4,
      "duration": 5.719
    },
    {
      "lang": "en",
      "text": "embedding of block size by an embed and",
      "offset": 3641.48,
      "duration": 4.76
    },
    {
      "lang": "en",
      "text": "so each position from zero to block size",
      "offset": 3644.119,
      "duration": 3.521
    },
    {
      "lang": "en",
      "text": "minus one will also get its own",
      "offset": 3646.24,
      "duration": 4.599
    },
    {
      "lang": "en",
      "text": "embedding vector and then here first let",
      "offset": 3647.64,
      "duration": 6.719
    },
    {
      "lang": "en",
      "text": "me decode B BYT from idx do",
      "offset": 3650.839,
      "duration": 5.681
    },
    {
      "lang": "en",
      "text": "shape and then here we're also going to",
      "offset": 3654.359,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "have a pause embedding which is the",
      "offset": 3656.52,
      "duration": 3.519
    },
    {
      "lang": "en",
      "text": "positional embedding and these are this",
      "offset": 3658.359,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "is to arrange so this will be basically",
      "offset": 3660.039,
      "duration": 6.201
    },
    {
      "lang": "en",
      "text": "just integers from Z to T minus one and",
      "offset": 3663,
      "duration": 5.079
    },
    {
      "lang": "en",
      "text": "all of those integers from 0 to T minus",
      "offset": 3666.24,
      "duration": 3.64
    },
    {
      "lang": "en",
      "text": "one get embedded through the table to",
      "offset": 3668.079,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "create a t by",
      "offset": 3669.88,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "C and then here this gets renamed to",
      "offset": 3671.28,
      "duration": 7.44
    },
    {
      "lang": "en",
      "text": "just say x and x will be the addition of",
      "offset": 3674.76,
      "duration": 6.12
    },
    {
      "lang": "en",
      "text": "the token embeddings with the positional",
      "offset": 3678.72,
      "duration": 4.2
    },
    {
      "lang": "en",
      "text": "embeddings and here the broadcasting",
      "offset": 3680.88,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "note will work out so B by T by C plus T",
      "offset": 3682.92,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "by C",
      "offset": 3685.44,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "this gets right aligned a new dimension",
      "offset": 3686.44,
      "duration": 3.8
    },
    {
      "lang": "en",
      "text": "of one gets added and it gets",
      "offset": 3688.64,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "broadcasted across",
      "offset": 3690.24,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "batch so at this point x holds not just",
      "offset": 3691.76,
      "duration": 5.799
    },
    {
      "lang": "en",
      "text": "the token identities but the positions",
      "offset": 3694.88,
      "duration": 5.08
    },
    {
      "lang": "en",
      "text": "at which these tokens occur and this is",
      "offset": 3697.559,
      "duration": 4.121
    },
    {
      "lang": "en",
      "text": "currently not that useful because of",
      "offset": 3699.96,
      "duration": 3.399
    },
    {
      "lang": "en",
      "text": "course we just have a simple byr model",
      "offset": 3701.68,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "so it doesn't matter if you're in the",
      "offset": 3703.359,
      "duration": 2.921
    },
    {
      "lang": "en",
      "text": "fifth position the second position or",
      "offset": 3704.4,
      "duration": 3.959
    },
    {
      "lang": "en",
      "text": "wherever it's all translation invariant",
      "offset": 3706.28,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "at this stage uh so this information",
      "offset": 3708.359,
      "duration": 4.521
    },
    {
      "lang": "en",
      "text": "currently wouldn't help uh but as we",
      "offset": 3710.44,
      "duration": 3.96
    },
    {
      "lang": "en",
      "text": "work on the self attention block we'll",
      "offset": 3712.88,
      "duration": 4.959
    },
    {
      "lang": "en",
      "text": "see that this starts to matter",
      "offset": 3714.4,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "okay so now we get the Crux of self",
      "offset": 3719.88,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "attention so this is probably the most",
      "offset": 3721.48,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "important part of this video to",
      "offset": 3723.24,
      "duration": 4.039
    },
    {
      "lang": "en",
      "text": "understand we're going to implement a",
      "offset": 3725.4,
      "duration": 3.32
    },
    {
      "lang": "en",
      "text": "small self attention for a single",
      "offset": 3727.279,
      "duration": 4.121
    },
    {
      "lang": "en",
      "text": "individual head as they're called so we",
      "offset": 3728.72,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "start off with where we were so all of",
      "offset": 3731.4,
      "duration": 5.24
    },
    {
      "lang": "en",
      "text": "this code is familiar so right now I'm",
      "offset": 3733.599,
      "duration": 4.361
    },
    {
      "lang": "en",
      "text": "working with an example where I Chang",
      "offset": 3736.64,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "the number of channels from 2 to 32 so",
      "offset": 3737.96,
      "duration": 6.159
    },
    {
      "lang": "en",
      "text": "we have a 4x8 arrangement of tokens and",
      "offset": 3740.24,
      "duration": 6.4
    },
    {
      "lang": "en",
      "text": "each to and the information each token",
      "offset": 3744.119,
      "duration": 4.761
    },
    {
      "lang": "en",
      "text": "is currently 32 dimensional but we just",
      "offset": 3746.64,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "are working with random",
      "offset": 3748.88,
      "duration": 5.479
    },
    {
      "lang": "en",
      "text": "numbers now we saw here that the code as",
      "offset": 3750.319,
      "duration": 7.321
    },
    {
      "lang": "en",
      "text": "we had it before does a uh simple weight",
      "offset": 3754.359,
      "duration": 6.801
    },
    {
      "lang": "en",
      "text": "simple average of all the past tokens",
      "offset": 3757.64,
      "duration": 5.679
    },
    {
      "lang": "en",
      "text": "and the current token so it's just the",
      "offset": 3761.16,
      "duration": 3.32
    },
    {
      "lang": "en",
      "text": "previous information and current",
      "offset": 3763.319,
      "duration": 2.561
    },
    {
      "lang": "en",
      "text": "information is just being mixed together",
      "offset": 3764.48,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "in an average and that's what this code",
      "offset": 3765.88,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "currently achieves and it Doo by",
      "offset": 3768.319,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "creating this lower triangular structure",
      "offset": 3770.2,
      "duration": 5.48
    },
    {
      "lang": "en",
      "text": "which allows us to mask out this uh we",
      "offset": 3772.4,
      "duration": 6.719
    },
    {
      "lang": "en",
      "text": "uh Matrix that we create so we mask it",
      "offset": 3775.68,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "out and then we normalize it and",
      "offset": 3779.119,
      "duration": 4.841
    },
    {
      "lang": "en",
      "text": "currently when we initialize the",
      "offset": 3781.2,
      "duration": 4.599
    },
    {
      "lang": "en",
      "text": "affinities between all the different",
      "offset": 3783.96,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "sort of tokens or nodes I'm going to use",
      "offset": 3785.799,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "those terms",
      "offset": 3788.599,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "interchangeably so when we initialize",
      "offset": 3789.64,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "the affinities between all the different",
      "offset": 3791.799,
      "duration": 4.28
    },
    {
      "lang": "en",
      "text": "tokens to be zero then we see that way",
      "offset": 3793.079,
      "duration": 5.401
    },
    {
      "lang": "en",
      "text": "gives us this um structure where every",
      "offset": 3796.079,
      "duration": 6.321
    },
    {
      "lang": "en",
      "text": "single row has these um uniform numbers",
      "offset": 3798.48,
      "duration": 6.639
    },
    {
      "lang": "en",
      "text": "and so that's what that's what then uh",
      "offset": 3802.4,
      "duration": 4.679
    },
    {
      "lang": "en",
      "text": "in this Matrix multiply makes it so that",
      "offset": 3805.119,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "we're doing a simple",
      "offset": 3807.079,
      "duration": 5.081
    },
    {
      "lang": "en",
      "text": "average now we don't actually want this",
      "offset": 3808.72,
      "duration": 7.52
    },
    {
      "lang": "en",
      "text": "to be all uniform because different uh",
      "offset": 3812.16,
      "duration": 6.439
    },
    {
      "lang": "en",
      "text": "tokens will find different other tokens",
      "offset": 3816.24,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "more or less interesting and we want",
      "offset": 3818.599,
      "duration": 3.641
    },
    {
      "lang": "en",
      "text": "that to be data dependent so for example",
      "offset": 3820.24,
      "duration": 4.359
    },
    {
      "lang": "en",
      "text": "if I'm a vowel then maybe I'm looking",
      "offset": 3822.24,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "for consonants in my past and maybe I",
      "offset": 3824.599,
      "duration": 3.72
    },
    {
      "lang": "en",
      "text": "want to know what those consonants are",
      "offset": 3826.799,
      "duration": 3.48
    },
    {
      "lang": "en",
      "text": "and I want that information to flow to",
      "offset": 3828.319,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "me and so I want to now gather",
      "offset": 3830.279,
      "duration": 4.52
    },
    {
      "lang": "en",
      "text": "information from the past but I want to",
      "offset": 3832.88,
      "duration": 3.719
    },
    {
      "lang": "en",
      "text": "do it in the data dependent way and this",
      "offset": 3834.799,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "is the problem that self attention",
      "offset": 3836.599,
      "duration": 4.041
    },
    {
      "lang": "en",
      "text": "solves now the way self attention solves",
      "offset": 3838.16,
      "duration": 5.72
    },
    {
      "lang": "en",
      "text": "this is the following every single node",
      "offset": 3840.64,
      "duration": 5.959
    },
    {
      "lang": "en",
      "text": "or every single token at each position",
      "offset": 3843.88,
      "duration": 5.959
    },
    {
      "lang": "en",
      "text": "will emit two vectors it will emit a",
      "offset": 3846.599,
      "duration": 6.041
    },
    {
      "lang": "en",
      "text": "query and it will emit a",
      "offset": 3849.839,
      "duration": 5.641
    },
    {
      "lang": "en",
      "text": "key now the query Vector roughly",
      "offset": 3852.64,
      "duration": 5.959
    },
    {
      "lang": "en",
      "text": "speaking is what am I looking for and",
      "offset": 3855.48,
      "duration": 5.119
    },
    {
      "lang": "en",
      "text": "the key Vector roughly speaking is what",
      "offset": 3858.599,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "do I",
      "offset": 3860.599,
      "duration": 3.48
    },
    {
      "lang": "en",
      "text": "contain and then the way we get",
      "offset": 3861.799,
      "duration": 5.24
    },
    {
      "lang": "en",
      "text": "affinities between these uh tokens now",
      "offset": 3864.079,
      "duration": 5.641
    },
    {
      "lang": "en",
      "text": "in a sequence is we basically just do a",
      "offset": 3867.039,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "do product between the keys and the",
      "offset": 3869.72,
      "duration": 5.599
    },
    {
      "lang": "en",
      "text": "queries so my query dot products with",
      "offset": 3871.839,
      "duration": 6.121
    },
    {
      "lang": "en",
      "text": "all the keys of all the other tokens and",
      "offset": 3875.319,
      "duration": 6.601
    },
    {
      "lang": "en",
      "text": "that dot product now becomes",
      "offset": 3877.96,
      "duration": 7.639
    },
    {
      "lang": "en",
      "text": "wayy and so um if the key and the query",
      "offset": 3881.92,
      "duration": 5.8
    },
    {
      "lang": "en",
      "text": "are sort of aligned they will interact",
      "offset": 3885.599,
      "duration": 4.601
    },
    {
      "lang": "en",
      "text": "to a very high amount and then I will",
      "offset": 3887.72,
      "duration": 4.92
    },
    {
      "lang": "en",
      "text": "get to learn more about that specific",
      "offset": 3890.2,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "token as opposed to any other token in",
      "offset": 3892.64,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "the sequence",
      "offset": 3895.16,
      "duration": 3.879
    },
    {
      "lang": "en",
      "text": "so let's implement this",
      "offset": 3896.16,
      "duration": 2.879
    },
    {
      "lang": "en",
      "text": "now we're going to implement a",
      "offset": 3900.559,
      "duration": 6.441
    },
    {
      "lang": "en",
      "text": "single what's called head of self",
      "offset": 3903.72,
      "duration": 5.839
    },
    {
      "lang": "en",
      "text": "attention so this is just one head",
      "offset": 3907,
      "duration": 3.96
    },
    {
      "lang": "en",
      "text": "there's a hyper parameter involved with",
      "offset": 3909.559,
      "duration": 4.04
    },
    {
      "lang": "en",
      "text": "these heads which is the head size and",
      "offset": 3910.96,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "then here I'm initializing linear",
      "offset": 3913.599,
      "duration": 4.52
    },
    {
      "lang": "en",
      "text": "modules and I'm using bias equals false",
      "offset": 3915.96,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "so these are just going to apply a",
      "offset": 3918.119,
      "duration": 3.641
    },
    {
      "lang": "en",
      "text": "matrix multiply with some fixed",
      "offset": 3919.4,
      "duration": 6.719
    },
    {
      "lang": "en",
      "text": "weights and now let me produce a key and",
      "offset": 3921.76,
      "duration": 8.16
    },
    {
      "lang": "en",
      "text": "q k and Q by forwarding these modules on",
      "offset": 3926.119,
      "duration": 6.641
    },
    {
      "lang": "en",
      "text": "X so the size of this will now",
      "offset": 3929.92,
      "duration": 6.96
    },
    {
      "lang": "en",
      "text": "become B by T by 16 because that is the",
      "offset": 3932.76,
      "duration": 8.92
    },
    {
      "lang": "en",
      "text": "head size and the same here B by T by",
      "offset": 3936.88,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "16 so this being the head size so you",
      "offset": 3944.76,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "see here that when I forward this linear",
      "offset": 3947.96,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "on top of my X all the tokens in all the",
      "offset": 3949.88,
      "duration": 5.479
    },
    {
      "lang": "en",
      "text": "positions in the B BYT Arrangement all",
      "offset": 3952.96,
      "duration": 4.04
    },
    {
      "lang": "en",
      "text": "of them them in parallel and",
      "offset": 3955.359,
      "duration": 4.281
    },
    {
      "lang": "en",
      "text": "independently produce a key and a query",
      "offset": 3957,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "so no communication has happened",
      "offset": 3959.64,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "yet but the communication comes now all",
      "offset": 3961.559,
      "duration": 5.961
    },
    {
      "lang": "en",
      "text": "the queries will do product with all the",
      "offset": 3964.2,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "keys so basically what we want is we",
      "offset": 3967.52,
      "duration": 4.599
    },
    {
      "lang": "en",
      "text": "want way now or the affinities between",
      "offset": 3969.88,
      "duration": 6.84
    },
    {
      "lang": "en",
      "text": "these to be query multiplying key but we",
      "offset": 3972.119,
      "duration": 6.121
    },
    {
      "lang": "en",
      "text": "have to be careful with uh we can't",
      "offset": 3976.72,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "Matrix multiply this we actually need to",
      "offset": 3978.24,
      "duration": 4.76
    },
    {
      "lang": "en",
      "text": "transpose uh K but we have to be also",
      "offset": 3980.079,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "careful because these are when you have",
      "offset": 3983,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "The Bash Dimension so in particular we",
      "offset": 3985.599,
      "duration": 4.841
    },
    {
      "lang": "en",
      "text": "want to transpose uh the last two",
      "offset": 3987.64,
      "duration": 6.32
    },
    {
      "lang": "en",
      "text": "dimensions dimension1 and dimension -2",
      "offset": 3990.44,
      "duration": 5.96
    },
    {
      "lang": "en",
      "text": "so",
      "offset": 3993.96,
      "duration": 2.44
    },
    {
      "lang": "en",
      "text": "-21 and so this Matrix multiply now will",
      "offset": 3996.48,
      "duration": 7.72
    },
    {
      "lang": "en",
      "text": "basically do the following B by T by",
      "offset": 4000.16,
      "duration": 8.919
    },
    {
      "lang": "en",
      "text": "16 Matrix multiplies B by 16 by T to",
      "offset": 4004.2,
      "duration": 9.359
    },
    {
      "lang": "en",
      "text": "give us B by T by",
      "offset": 4009.079,
      "duration": 6.96
    },
    {
      "lang": "en",
      "text": "T right",
      "offset": 4013.559,
      "duration": 5.24
    },
    {
      "lang": "en",
      "text": "so for every row of B we're now going to",
      "offset": 4016.039,
      "duration": 5.08
    },
    {
      "lang": "en",
      "text": "have a t Square Matrix giving us the",
      "offset": 4018.799,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "affinities and these are now the way so",
      "offset": 4021.119,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "they're not zeros they are now coming",
      "offset": 4024.079,
      "duration": 4.121
    },
    {
      "lang": "en",
      "text": "from this dot product between the keys",
      "offset": 4026.319,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "and the queries so this can now run I",
      "offset": 4028.2,
      "duration": 5.599
    },
    {
      "lang": "en",
      "text": "can I can run this and the weighted",
      "offset": 4031.279,
      "duration": 5.121
    },
    {
      "lang": "en",
      "text": "aggregation now is a function in a data",
      "offset": 4033.799,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "Bandon manner between the keys and",
      "offset": 4036.4,
      "duration": 4.28
    },
    {
      "lang": "en",
      "text": "queries of these nodes so just",
      "offset": 4038.2,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "inspecting what happened",
      "offset": 4040.68,
      "duration": 5.72
    },
    {
      "lang": "en",
      "text": "here the way takes on this form",
      "offset": 4042.839,
      "duration": 6.161
    },
    {
      "lang": "en",
      "text": "and you see that before way was uh just",
      "offset": 4046.4,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "a constant so it was applied in the same",
      "offset": 4049,
      "duration": 4.359
    },
    {
      "lang": "en",
      "text": "way to all the batch elements but now",
      "offset": 4051.119,
      "duration": 3.641
    },
    {
      "lang": "en",
      "text": "every single batch elements will have",
      "offset": 4053.359,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "different sort of we because uh every",
      "offset": 4054.76,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "single batch element contains different",
      "offset": 4057.839,
      "duration": 4.121
    },
    {
      "lang": "en",
      "text": "uh tokens at different positions and so",
      "offset": 4059.4,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "this is not data dependent so when we",
      "offset": 4061.96,
      "duration": 5.159
    },
    {
      "lang": "en",
      "text": "look at just the zeroth uh Row for",
      "offset": 4064.599,
      "duration": 5.081
    },
    {
      "lang": "en",
      "text": "example in the input these are the",
      "offset": 4067.119,
      "duration": 4.44
    },
    {
      "lang": "en",
      "text": "weights that came out and so you can see",
      "offset": 4069.68,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "now that they're not just exactly",
      "offset": 4071.559,
      "duration": 4.361
    },
    {
      "lang": "en",
      "text": "uniform um and in particular as an",
      "offset": 4073.44,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "example here for the last row this was",
      "offset": 4075.92,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "the eighth token and the eighth token",
      "offset": 4078.079,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "knows what content it has and it knows",
      "offset": 4080.24,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "at what position it's in and now the E",
      "offset": 4082.24,
      "duration": 6.319
    },
    {
      "lang": "en",
      "text": "token based on that uh creates a query",
      "offset": 4084.88,
      "duration": 5.64
    },
    {
      "lang": "en",
      "text": "hey I'm looking for this kind of stuff",
      "offset": 4088.559,
      "duration": 4.201
    },
    {
      "lang": "en",
      "text": "um I'm a vowel I'm on the E position I'm",
      "offset": 4090.52,
      "duration": 4.12
    },
    {
      "lang": "en",
      "text": "looking for any consonant at positions",
      "offset": 4092.76,
      "duration": 5.399
    },
    {
      "lang": "en",
      "text": "up to four and then all the nodes get to",
      "offset": 4094.64,
      "duration": 5.96
    },
    {
      "lang": "en",
      "text": "emit keys and maybe one of the channels",
      "offset": 4098.159,
      "duration": 4.841
    },
    {
      "lang": "en",
      "text": "could be I am a I am a consonant and I",
      "offset": 4100.6,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "am in a position up to four and that",
      "offset": 4103,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "that key would have a high number in",
      "offset": 4105.6,
      "duration": 4.28
    },
    {
      "lang": "en",
      "text": "that specific Channel and that's how the",
      "offset": 4107.799,
      "duration": 3.88
    },
    {
      "lang": "en",
      "text": "query and the key when they do product",
      "offset": 4109.88,
      "duration": 3.16
    },
    {
      "lang": "en",
      "text": "they can find each other and create a",
      "offset": 4111.679,
      "duration": 3.721
    },
    {
      "lang": "en",
      "text": "high affinity and when they have a high",
      "offset": 4113.04,
      "duration": 5.119
    },
    {
      "lang": "en",
      "text": "Affinity like say uh this token was",
      "offset": 4115.4,
      "duration": 5.919
    },
    {
      "lang": "en",
      "text": "pretty interesting to uh to this eighth",
      "offset": 4118.159,
      "duration": 5.64
    },
    {
      "lang": "en",
      "text": "token when they have a high Affinity",
      "offset": 4121.319,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "then through the softmax I will end up",
      "offset": 4123.799,
      "duration": 3.641
    },
    {
      "lang": "en",
      "text": "aggregating a lot of its information",
      "offset": 4125.719,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "into my position and so I'll get to",
      "offset": 4127.44,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "learn a lot about",
      "offset": 4129.88,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "it now just this we're looking at way",
      "offset": 4131.759,
      "duration": 7.641
    },
    {
      "lang": "en",
      "text": "after this has already happened um let",
      "offset": 4135.88,
      "duration": 5.24
    },
    {
      "lang": "en",
      "text": "me erase this operation as well so let",
      "offset": 4139.4,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "me erase the masking and the softmax",
      "offset": 4141.12,
      "duration": 3.4
    },
    {
      "lang": "en",
      "text": "just to show you the under the hood",
      "offset": 4143.319,
      "duration": 3.88
    },
    {
      "lang": "en",
      "text": "internals and how that works so without",
      "offset": 4144.52,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "the masking in the softmax Whey comes",
      "offset": 4147.199,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "out like this right this is the outputs",
      "offset": 4149.52,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "of the do products um and these are the",
      "offset": 4151.679,
      "duration": 4.281
    },
    {
      "lang": "en",
      "text": "raw outputs and they take on values from",
      "offset": 4154.319,
      "duration": 4.44
    },
    {
      "lang": "en",
      "text": "negative you know two to positive two",
      "offset": 4155.96,
      "duration": 5.879
    },
    {
      "lang": "en",
      "text": "Etc so that's the raw interactions and",
      "offset": 4158.759,
      "duration": 5.801
    },
    {
      "lang": "en",
      "text": "raw affinities between all the nodes but",
      "offset": 4161.839,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "now if I'm going if I'm a fifth node I",
      "offset": 4164.56,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "will not want to aggregate anything from",
      "offset": 4166.719,
      "duration": 3.64
    },
    {
      "lang": "en",
      "text": "the sixth node seventh node and the",
      "offset": 4168.799,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "eighth node so actually we use the upper",
      "offset": 4170.359,
      "duration": 5.241
    },
    {
      "lang": "en",
      "text": "triangular masking so those are not",
      "offset": 4172.88,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "allowed to",
      "offset": 4175.6,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "communicate and now we actually want to",
      "offset": 4177.359,
      "duration": 5.4
    },
    {
      "lang": "en",
      "text": "have a nice uh distribution uh so we",
      "offset": 4180.08,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "don't want to aggregate negative .11 of",
      "offset": 4182.759,
      "duration": 4.361
    },
    {
      "lang": "en",
      "text": "this node that's crazy so instead we",
      "offset": 4185.12,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "exponentiate and normalize and now we",
      "offset": 4187.12,
      "duration": 4.44
    },
    {
      "lang": "en",
      "text": "get a nice distribution that sums to one",
      "offset": 4189.359,
      "duration": 3.48
    },
    {
      "lang": "en",
      "text": "and this is telling us now in the data",
      "offset": 4191.56,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "dependent manner how much of information",
      "offset": 4192.839,
      "duration": 4.121
    },
    {
      "lang": "en",
      "text": "to aggregate from any of these tokens in",
      "offset": 4194.84,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "the",
      "offset": 4196.96,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "past so that's way and it's not zeros",
      "offset": 4198.6,
      "duration": 5.639
    },
    {
      "lang": "en",
      "text": "anymore but but it's calculated in this",
      "offset": 4201.96,
      "duration": 6.04
    },
    {
      "lang": "en",
      "text": "way now there's one more uh part to a",
      "offset": 4204.239,
      "duration": 6.321
    },
    {
      "lang": "en",
      "text": "single self attention head and that is",
      "offset": 4208,
      "duration": 4.36
    },
    {
      "lang": "en",
      "text": "that when we do the aggregation we don't",
      "offset": 4210.56,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "actually aggregate the tokens exactly we",
      "offset": 4212.36,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "aggregate we produce one more value here",
      "offset": 4215.04,
      "duration": 5.119
    },
    {
      "lang": "en",
      "text": "and we call that the",
      "offset": 4217.48,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "value so in the same way that we",
      "offset": 4220.159,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "produced p and query we're also going to",
      "offset": 4222.12,
      "duration": 3.96
    },
    {
      "lang": "en",
      "text": "create a value",
      "offset": 4223.92,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "and",
      "offset": 4226.08,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "then here we don't",
      "offset": 4226.88,
      "duration": 7.359
    },
    {
      "lang": "en",
      "text": "aggregate X we calculate a v which is",
      "offset": 4230.239,
      "duration": 6.92
    },
    {
      "lang": "en",
      "text": "just achieved by uh propagating this",
      "offset": 4234.239,
      "duration": 6.401
    },
    {
      "lang": "en",
      "text": "linear on top of X again and then we",
      "offset": 4237.159,
      "duration": 7.201
    },
    {
      "lang": "en",
      "text": "output way multiplied by V so V is the",
      "offset": 4240.64,
      "duration": 5.8
    },
    {
      "lang": "en",
      "text": "elements that we aggregate or the the",
      "offset": 4244.36,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "vectors that we aggregate instead of the",
      "offset": 4246.44,
      "duration": 2.44
    },
    {
      "lang": "en",
      "text": "raw",
      "offset": 4247.88,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "X and now of course uh this will make it",
      "offset": 4248.88,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "so that the output here of this single",
      "offset": 4251.719,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "head will be 16 dimensional because that",
      "offset": 4253.44,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "is the head",
      "offset": 4255.4,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "size so you can think of X as kind of",
      "offset": 4257.36,
      "duration": 4.359
    },
    {
      "lang": "en",
      "text": "like private information to this token",
      "offset": 4259.8,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "if you if you think about it that way so",
      "offset": 4261.719,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "X is kind of private to this token so",
      "offset": 4263.88,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "I'm a fifth token at some and I have",
      "offset": 4266.04,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "some identity and uh my information is",
      "offset": 4268.04,
      "duration": 6.04
    },
    {
      "lang": "en",
      "text": "kept in Vector X and now for the",
      "offset": 4271.48,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "purposes of the single head here's what",
      "offset": 4274.08,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "I'm interested in here's what I have and",
      "offset": 4276.199,
      "duration": 5.641
    },
    {
      "lang": "en",
      "text": "if you find me interesting here's what I",
      "offset": 4280,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "will communicate to you and that's",
      "offset": 4281.84,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "stored in v and so V is the thing that",
      "offset": 4283.52,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "gets aggregated for the purposes of this",
      "offset": 4286.239,
      "duration": 4.521
    },
    {
      "lang": "en",
      "text": "single head between the different",
      "offset": 4288.4,
      "duration": 5.96
    },
    {
      "lang": "en",
      "text": "notes and that's uh basically the self",
      "offset": 4290.76,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "attention mechanism this is this is what",
      "offset": 4294.36,
      "duration": 4.76
    },
    {
      "lang": "en",
      "text": "it does there are a few notes that I",
      "offset": 4296.84,
      "duration": 4.92
    },
    {
      "lang": "en",
      "text": "would make like to make about attention",
      "offset": 4299.12,
      "duration": 5.16
    },
    {
      "lang": "en",
      "text": "number one attention is a communication",
      "offset": 4301.76,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "mechanism you can really think about it",
      "offset": 4304.28,
      "duration": 3.879
    },
    {
      "lang": "en",
      "text": "as a communication mechanism where you",
      "offset": 4306.239,
      "duration": 3.881
    },
    {
      "lang": "en",
      "text": "have a number of nodes in a directed",
      "offset": 4308.159,
      "duration": 4.04
    },
    {
      "lang": "en",
      "text": "graph where basically you have edges",
      "offset": 4310.12,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "pointed between noes like",
      "offset": 4312.199,
      "duration": 4.601
    },
    {
      "lang": "en",
      "text": "this and what happens is every node has",
      "offset": 4313.88,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "some Vector of information and it gets",
      "offset": 4316.8,
      "duration": 4.52
    },
    {
      "lang": "en",
      "text": "to aggregate information via a weighted",
      "offset": 4318.84,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "sum from all of the nodes that point to",
      "offset": 4321.32,
      "duration": 4.919
    },
    {
      "lang": "en",
      "text": "it and this is done in a data dependent",
      "offset": 4323.8,
      "duration": 4.52
    },
    {
      "lang": "en",
      "text": "manner so depending on whatever data is",
      "offset": 4326.239,
      "duration": 3.241
    },
    {
      "lang": "en",
      "text": "actually stored that you should not at",
      "offset": 4328.32,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "any point in time now our graph doesn't",
      "offset": 4329.48,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "look like this our graph has a different",
      "offset": 4333.36,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "structure we have eight nodes because",
      "offset": 4335,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "the block size is eight and there's",
      "offset": 4337.199,
      "duration": 3.121
    },
    {
      "lang": "en",
      "text": "always eight to",
      "offset": 4338.76,
      "duration": 4.36
    },
    {
      "lang": "en",
      "text": "tokens and uh the first node is only",
      "offset": 4340.32,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "pointed to by itself the second node is",
      "offset": 4343.12,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "pointed to by the first node and itself",
      "offset": 4345.52,
      "duration": 4.28
    },
    {
      "lang": "en",
      "text": "all the way up to the eighth node which",
      "offset": 4347.84,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "is pointed to by all the previous nodes",
      "offset": 4349.8,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "and itself and so that's the structure",
      "offset": 4352.08,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "that our directed graph has or happens",
      "offset": 4354.92,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "happens to have in Auto regressive sort",
      "offset": 4357.08,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "of scenario like language modeling but",
      "offset": 4358.92,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "in principle attention can be applied to",
      "offset": 4361.4,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "any arbitrary directed graph and it's",
      "offset": 4362.84,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "just a communication mechanism between",
      "offset": 4364.44,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "the nodes the second note is that notice",
      "offset": 4366.04,
      "duration": 5.24
    },
    {
      "lang": "en",
      "text": "that there is no notion of space so",
      "offset": 4368.84,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "attention simply acts over like a set of",
      "offset": 4371.28,
      "duration": 5.24
    },
    {
      "lang": "en",
      "text": "vectors in this graph and so by default",
      "offset": 4373.719,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "these nodes have no idea where they are",
      "offset": 4376.52,
      "duration": 3.159
    },
    {
      "lang": "en",
      "text": "positioned in the space and that's why",
      "offset": 4378.04,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "we need to encode them positionally and",
      "offset": 4379.679,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "sort of give them some information that",
      "offset": 4382.199,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "is anchored to a specific position so",
      "offset": 4383.76,
      "duration": 4.919
    },
    {
      "lang": "en",
      "text": "that they sort of know where they are",
      "offset": 4385.8,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "and this is different than for example",
      "offset": 4388.679,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "from convolution because if you're run",
      "offset": 4389.96,
      "duration": 3.239
    },
    {
      "lang": "en",
      "text": "for example a convolution operation over",
      "offset": 4391.719,
      "duration": 4.121
    },
    {
      "lang": "en",
      "text": "some input there's a very specific sort",
      "offset": 4393.199,
      "duration": 5.241
    },
    {
      "lang": "en",
      "text": "of layout of the information in space",
      "offset": 4395.84,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "and the convolutional filters sort of",
      "offset": 4398.44,
      "duration": 5.279
    },
    {
      "lang": "en",
      "text": "act in space and so it's it's not like",
      "offset": 4400.32,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "an attention in ATT ention is just a set",
      "offset": 4403.719,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "of vectors out there in space they",
      "offset": 4406,
      "duration": 3.8
    },
    {
      "lang": "en",
      "text": "communicate and if you want them to have",
      "offset": 4407.88,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "a notion of space you need to",
      "offset": 4409.8,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "specifically add it which is what we've",
      "offset": 4411.32,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "done when we calculated the um relative",
      "offset": 4413.32,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "the positional encode encodings and",
      "offset": 4416.12,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "added that information to the vectors",
      "offset": 4418.44,
      "duration": 3.4
    },
    {
      "lang": "en",
      "text": "the next thing that I hope is very clear",
      "offset": 4420.28,
      "duration": 3.48
    },
    {
      "lang": "en",
      "text": "is that the elements across the batch",
      "offset": 4421.84,
      "duration": 4.04
    },
    {
      "lang": "en",
      "text": "Dimension which are independent examples",
      "offset": 4423.76,
      "duration": 3.56
    },
    {
      "lang": "en",
      "text": "never talk to each other they're always",
      "offset": 4425.88,
      "duration": 3.279
    },
    {
      "lang": "en",
      "text": "processed independently and this is a",
      "offset": 4427.32,
      "duration": 3.72
    },
    {
      "lang": "en",
      "text": "batched matrix multiply that applies",
      "offset": 4429.159,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "basically a matrix multiplication uh",
      "offset": 4431.04,
      "duration": 3.4
    },
    {
      "lang": "en",
      "text": "kind of in parallel across the batch",
      "offset": 4433.08,
      "duration": 3.079
    },
    {
      "lang": "en",
      "text": "dimension so maybe it would be more",
      "offset": 4434.44,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "accurate to say that in this analogy of",
      "offset": 4436.159,
      "duration": 4.681
    },
    {
      "lang": "en",
      "text": "a directed graph we really have because",
      "offset": 4438.28,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "the back size is four we really have",
      "offset": 4440.84,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "four separate pools of eight nodes and",
      "offset": 4443.08,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "those eight nodes only talk to each",
      "offset": 4445.8,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "other but in total there's like 32 nodes",
      "offset": 4447,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "that are being processed uh but there's",
      "offset": 4448.92,
      "duration": 4.68
    },
    {
      "lang": "en",
      "text": "um sort of four separate pools of eight",
      "offset": 4451,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "you can look at it that way the next",
      "offset": 4453.6,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "note is that here in the case of",
      "offset": 4455.8,
      "duration": 4.68
    },
    {
      "lang": "en",
      "text": "language modeling uh we have this",
      "offset": 4458,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "specific uh structure of directed graph",
      "offset": 4460.48,
      "duration": 4.12
    },
    {
      "lang": "en",
      "text": "where the future tokens will not",
      "offset": 4462.719,
      "duration": 4.841
    },
    {
      "lang": "en",
      "text": "communicate to the Past tokens but this",
      "offset": 4464.6,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "doesn't necessarily have to be the",
      "offset": 4467.56,
      "duration": 3.119
    },
    {
      "lang": "en",
      "text": "constraint in the general case and in",
      "offset": 4468.6,
      "duration": 4.2
    },
    {
      "lang": "en",
      "text": "fact in many cases you may want to have",
      "offset": 4470.679,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "all of the uh noes talk to each other uh",
      "offset": 4472.8,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "fully so as an example if you're doing",
      "offset": 4475.56,
      "duration": 3.4
    },
    {
      "lang": "en",
      "text": "sentiment analysis or something like",
      "offset": 4477.52,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "that with a Transformer you might have a",
      "offset": 4478.96,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "number of tokens and you may want to",
      "offset": 4480.96,
      "duration": 4.36
    },
    {
      "lang": "en",
      "text": "have them all talk to each other fully",
      "offset": 4482.88,
      "duration": 3.72
    },
    {
      "lang": "en",
      "text": "because later you are predicting for",
      "offset": 4485.32,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "example the sentiment of the sentence",
      "offset": 4486.6,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "and so it's okay for these NOS to talk",
      "offset": 4489,
      "duration": 4.6
    },
    {
      "lang": "en",
      "text": "to each other and so in those cases you",
      "offset": 4490.679,
      "duration": 5.121
    },
    {
      "lang": "en",
      "text": "will use an encoder block of self",
      "offset": 4493.6,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "attention and uh all it means that it's",
      "offset": 4495.8,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "an encoder block is that you will delete",
      "offset": 4498.239,
      "duration": 4.681
    },
    {
      "lang": "en",
      "text": "this line of code allowing all the noes",
      "offset": 4500.8,
      "duration": 4.04
    },
    {
      "lang": "en",
      "text": "to completely talk to each other what",
      "offset": 4502.92,
      "duration": 3.239
    },
    {
      "lang": "en",
      "text": "we're implementing here is sometimes",
      "offset": 4504.84,
      "duration": 4.68
    },
    {
      "lang": "en",
      "text": "called a decoder block and it's called a",
      "offset": 4506.159,
      "duration": 6.761
    },
    {
      "lang": "en",
      "text": "decoder because it is sort of like a",
      "offset": 4509.52,
      "duration": 5.48
    },
    {
      "lang": "en",
      "text": "decoding language and it's got this",
      "offset": 4512.92,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "autor regressive format where you have",
      "offset": 4515,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "to mask with the Triangular Matrix so",
      "offset": 4517,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "that uh nodes from the future never talk",
      "offset": 4519.719,
      "duration": 4.601
    },
    {
      "lang": "en",
      "text": "to the Past because they would give away",
      "offset": 4522,
      "duration": 3.56
    },
    {
      "lang": "en",
      "text": "the answer",
      "offset": 4524.32,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "and so basically in encoder blocks you",
      "offset": 4525.56,
      "duration": 3.8
    },
    {
      "lang": "en",
      "text": "would delete this allow all the noes to",
      "offset": 4527.44,
      "duration": 4.04
    },
    {
      "lang": "en",
      "text": "talk in decoder blocks this will always",
      "offset": 4529.36,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "be present so that you have this",
      "offset": 4531.48,
      "duration": 3.88
    },
    {
      "lang": "en",
      "text": "triangular structure uh but both are",
      "offset": 4533.36,
      "duration": 3.56
    },
    {
      "lang": "en",
      "text": "allowed and attention doesn't care",
      "offset": 4535.36,
      "duration": 2.799
    },
    {
      "lang": "en",
      "text": "attention supports arbitrary",
      "offset": 4536.92,
      "duration": 3.279
    },
    {
      "lang": "en",
      "text": "connectivity between nodes the next",
      "offset": 4538.159,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "thing I wanted to comment on is you keep",
      "offset": 4540.199,
      "duration": 3.641
    },
    {
      "lang": "en",
      "text": "me you keep hearing me say attention",
      "offset": 4541.84,
      "duration": 3.96
    },
    {
      "lang": "en",
      "text": "self attention Etc there's actually also",
      "offset": 4543.84,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "something called cross attention what is",
      "offset": 4545.8,
      "duration": 2.08
    },
    {
      "lang": "en",
      "text": "the",
      "offset": 4547.199,
      "duration": 2.321
    },
    {
      "lang": "en",
      "text": "difference",
      "offset": 4547.88,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "so basically the reason this attention",
      "offset": 4549.52,
      "duration": 6.119
    },
    {
      "lang": "en",
      "text": "is self attention is because because the",
      "offset": 4552.84,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "keys queries and the values are all",
      "offset": 4555.639,
      "duration": 5.56
    },
    {
      "lang": "en",
      "text": "coming from the same Source from X so",
      "offset": 4557.96,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "the same Source X produces Keys queries",
      "offset": 4561.199,
      "duration": 4.761
    },
    {
      "lang": "en",
      "text": "and values so these nodes are self",
      "offset": 4563.4,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "attending but in principle attention is",
      "offset": 4565.96,
      "duration": 4.36
    },
    {
      "lang": "en",
      "text": "much more General than that so for",
      "offset": 4568.52,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "example an encoder decoder Transformers",
      "offset": 4570.32,
      "duration": 4.68
    },
    {
      "lang": "en",
      "text": "uh you can have a case where the queries",
      "offset": 4572.84,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "are produced from X but the keys and the",
      "offset": 4575,
      "duration": 3.88
    },
    {
      "lang": "en",
      "text": "values come from a whole separate",
      "offset": 4577.32,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "external source and sometimes from uh",
      "offset": 4578.88,
      "duration": 4.92
    },
    {
      "lang": "en",
      "text": "encoder blocks that encode some context",
      "offset": 4581.48,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "that we'd like to condition on",
      "offset": 4583.8,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "and so the keys and the values will",
      "offset": 4585.56,
      "duration": 2.76
    },
    {
      "lang": "en",
      "text": "actually come from a whole separate",
      "offset": 4586.84,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "Source those are nodes on the side and",
      "offset": 4588.32,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "here we're just producing queries and",
      "offset": 4591,
      "duration": 3.32
    },
    {
      "lang": "en",
      "text": "we're reading off information from the",
      "offset": 4592.88,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "side so cross attention is used when",
      "offset": 4594.32,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "there's a separate source of nodes we'd",
      "offset": 4597.36,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "like to pull information from into our",
      "offset": 4600.56,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "nodes and it's self attention if we just",
      "offset": 4602.88,
      "duration": 3.64
    },
    {
      "lang": "en",
      "text": "have nodes that would like to look at",
      "offset": 4605.199,
      "duration": 3.401
    },
    {
      "lang": "en",
      "text": "each other and talk to each other so",
      "offset": 4606.52,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "this attention here happens to be self",
      "offset": 4608.6,
      "duration": 7.079
    },
    {
      "lang": "en",
      "text": "attention but in principle um attention",
      "offset": 4611.8,
      "duration": 5.72
    },
    {
      "lang": "en",
      "text": "is a lot more General okay and the last",
      "offset": 4615.679,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "note at this stage is if we come to the",
      "offset": 4617.52,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "attention is all need paper here we've",
      "offset": 4619.6,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "already implemented attention so given",
      "offset": 4621.76,
      "duration": 4.68
    },
    {
      "lang": "en",
      "text": "query key and value we've U multiplied",
      "offset": 4623.6,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "the query and a key we've soft maxed it",
      "offset": 4626.44,
      "duration": 5.279
    },
    {
      "lang": "en",
      "text": "and then we are aggregating the values",
      "offset": 4629.04,
      "duration": 3.48
    },
    {
      "lang": "en",
      "text": "there's one more thing that we're",
      "offset": 4631.719,
      "duration": 2.201
    },
    {
      "lang": "en",
      "text": "missing here which is the dividing by",
      "offset": 4632.52,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "one / square root of the head size the",
      "offset": 4633.92,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "DK here is the head size why are they",
      "offset": 4636.679,
      "duration": 4.761
    },
    {
      "lang": "en",
      "text": "doing this finds this important so they",
      "offset": 4638.92,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "call it the scaled attention and it's",
      "offset": 4641.44,
      "duration": 4.36
    },
    {
      "lang": "en",
      "text": "kind of like an important normalization",
      "offset": 4644.44,
      "duration": 2.48
    },
    {
      "lang": "en",
      "text": "to basically",
      "offset": 4645.8,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "have the problem is if you have unit gsh",
      "offset": 4646.92,
      "duration": 5.4
    },
    {
      "lang": "en",
      "text": "and inputs so zero mean unit variance K",
      "offset": 4649.719,
      "duration": 4.44
    },
    {
      "lang": "en",
      "text": "and Q are unit gashin then if you just",
      "offset": 4652.32,
      "duration": 4.68
    },
    {
      "lang": "en",
      "text": "do we naively then you see that your we",
      "offset": 4654.159,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "actually will be uh the variance will be",
      "offset": 4657,
      "duration": 3.56
    },
    {
      "lang": "en",
      "text": "on the order of head size which in our",
      "offset": 4658.88,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "case is 16 but if you multiply by one",
      "offset": 4660.56,
      "duration": 5.32
    },
    {
      "lang": "en",
      "text": "over head size square root so this is",
      "offset": 4663.84,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "square root and this is one",
      "offset": 4665.88,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "over then the variance of we will be one",
      "offset": 4667.44,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "so it will be",
      "offset": 4670.6,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "preserved now why is this important",
      "offset": 4672,
      "duration": 4.12
    },
    {
      "lang": "en",
      "text": "you'll not notice that way",
      "offset": 4674.44,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "here will feed into",
      "offset": 4676.12,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "softmax and so it's really important",
      "offset": 4678.679,
      "duration": 4.441
    },
    {
      "lang": "en",
      "text": "especially at initialization that we be",
      "offset": 4680.76,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "fairly diffuse so in our case here we",
      "offset": 4683.12,
      "duration": 6.96
    },
    {
      "lang": "en",
      "text": "sort of locked out here and we had a",
      "offset": 4686.6,
      "duration": 7.079
    },
    {
      "lang": "en",
      "text": "fairly diffuse numbers here so um like",
      "offset": 4690.08,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "this now the problem is that because of",
      "offset": 4693.679,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "softmax if weight takes on very positive",
      "offset": 4695.84,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "and very negative numbers inside it",
      "offset": 4698.159,
      "duration": 4.52
    },
    {
      "lang": "en",
      "text": "softmax will actually converge towards",
      "offset": 4700.159,
      "duration": 5.08
    },
    {
      "lang": "en",
      "text": "one hot vectors and so I can illustrate",
      "offset": 4702.679,
      "duration": 6.921
    },
    {
      "lang": "en",
      "text": "that here um say we are applying softmax",
      "offset": 4705.239,
      "duration": 6.041
    },
    {
      "lang": "en",
      "text": "to a tensor of values that are very",
      "offset": 4709.6,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "close to zero then we're going to get a",
      "offset": 4711.28,
      "duration": 3.24
    },
    {
      "lang": "en",
      "text": "diffuse thing out of",
      "offset": 4713.04,
      "duration": 3.639
    },
    {
      "lang": "en",
      "text": "softmax but the moment I take the exact",
      "offset": 4714.52,
      "duration": 4.04
    },
    {
      "lang": "en",
      "text": "same thing and I start sharpening it",
      "offset": 4716.679,
      "duration": 3.401
    },
    {
      "lang": "en",
      "text": "making it bigger by multiplying these",
      "offset": 4718.56,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "numbers by eight for example you'll see",
      "offset": 4720.08,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "that the softmax will start to sharpen",
      "offset": 4722.159,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "and in fact it will sharpen towards the",
      "offset": 4724.239,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "max so it will sharpen towards whatever",
      "offset": 4726.08,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "number here is the highest and so um",
      "offset": 4728.159,
      "duration": 4.441
    },
    {
      "lang": "en",
      "text": "basically we don't want these values to",
      "offset": 4731.44,
      "duration": 2.36
    },
    {
      "lang": "en",
      "text": "be too extreme especially at",
      "offset": 4732.6,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "initialization otherwise softmax will be",
      "offset": 4733.8,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "way too peaky and um you're basically",
      "offset": 4735.8,
      "duration": 5.399
    },
    {
      "lang": "en",
      "text": "aggregating um information from like a",
      "offset": 4738.6,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "single node every node just agregates",
      "offset": 4741.199,
      "duration": 3.48
    },
    {
      "lang": "en",
      "text": "information from a single other node",
      "offset": 4743.08,
      "duration": 3.32
    },
    {
      "lang": "en",
      "text": "that's not what we want especially at",
      "offset": 4744.679,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "initialization and so the scaling is",
      "offset": 4746.4,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "used just to control the variance at",
      "offset": 4748.84,
      "duration": 4.2
    },
    {
      "lang": "en",
      "text": "initialization okay so having said all",
      "offset": 4751.04,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "that let's now take our self attention",
      "offset": 4753.04,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "knowledge and let's uh take it for a",
      "offset": 4755.12,
      "duration": 4.84
    },
    {
      "lang": "en",
      "text": "spin so here in the code I created this",
      "offset": 4757.12,
      "duration": 5.119
    },
    {
      "lang": "en",
      "text": "head module and it implements a single",
      "offset": 4759.96,
      "duration": 4.84
    },
    {
      "lang": "en",
      "text": "head of self attention so you give it a",
      "offset": 4762.239,
      "duration": 4.601
    },
    {
      "lang": "en",
      "text": "head size and then here it creates the",
      "offset": 4764.8,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "key query and the value linear layers",
      "offset": 4766.84,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "typically people don't use biases in",
      "offset": 4769.28,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "these uh so those are the linear",
      "offset": 4771.239,
      "duration": 3.48
    },
    {
      "lang": "en",
      "text": "projections that we're going to apply to",
      "offset": 4773.28,
      "duration": 4.28
    },
    {
      "lang": "en",
      "text": "all of our nodes now here I'm creating",
      "offset": 4774.719,
      "duration": 5.281
    },
    {
      "lang": "en",
      "text": "this Trill variable Trill is not a",
      "offset": 4777.56,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "parameter of the module so in sort of",
      "offset": 4780,
      "duration": 3.8
    },
    {
      "lang": "en",
      "text": "pytorch naming conventions uh this is",
      "offset": 4781.8,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "called a buffer it's not a parameter and",
      "offset": 4783.8,
      "duration": 3.64
    },
    {
      "lang": "en",
      "text": "you have to call it you have to assign",
      "offset": 4786.12,
      "duration": 3.519
    },
    {
      "lang": "en",
      "text": "it to the module using a register buffer",
      "offset": 4787.44,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "so that creates the trill uh the triang",
      "offset": 4789.639,
      "duration": 5.401
    },
    {
      "lang": "en",
      "text": "lower triangular Matrix and we're given",
      "offset": 4792.4,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "the input X this should look very",
      "offset": 4795.04,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "familiar now we calculate the keys the",
      "offset": 4796.48,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "queries we C calculate the attention",
      "offset": 4798.56,
      "duration": 5.32
    },
    {
      "lang": "en",
      "text": "scores inside way uh we normalize it so",
      "offset": 4800.719,
      "duration": 5.721
    },
    {
      "lang": "en",
      "text": "we're using scaled attention here then",
      "offset": 4803.88,
      "duration": 4.279
    },
    {
      "lang": "en",
      "text": "we make sure that uh future doesn't",
      "offset": 4806.44,
      "duration": 3.56
    },
    {
      "lang": "en",
      "text": "communicate with the past so this makes",
      "offset": 4808.159,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "it a decoder block and then softmax and",
      "offset": 4810,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "then aggregate the value and",
      "offset": 4813.36,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "output then here in the language model",
      "offset": 4815.52,
      "duration": 4.84
    },
    {
      "lang": "en",
      "text": "I'm creating a head in the Constructor",
      "offset": 4817.6,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "and I'm calling it self attention head",
      "offset": 4820.36,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "and the head size I'm going to keep as",
      "offset": 4822.48,
      "duration": 5.08
    },
    {
      "lang": "en",
      "text": "the same and embed just for",
      "offset": 4824.76,
      "duration": 6.32
    },
    {
      "lang": "en",
      "text": "now and then here once we've encoded the",
      "offset": 4827.56,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "information with the token embeddings",
      "offset": 4831.08,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "and the position embeddings we're simply",
      "offset": 4832.92,
      "duration": 3.48
    },
    {
      "lang": "en",
      "text": "going to feed it into the self attention",
      "offset": 4834.76,
      "duration": 3.8
    },
    {
      "lang": "en",
      "text": "head and then the output of that is",
      "offset": 4836.4,
      "duration": 5.799
    },
    {
      "lang": "en",
      "text": "going to go into uh the decoder language",
      "offset": 4838.56,
      "duration": 6.04
    },
    {
      "lang": "en",
      "text": "modeling head and create the logits so",
      "offset": 4842.199,
      "duration": 4.281
    },
    {
      "lang": "en",
      "text": "this the sort of the simplest way to",
      "offset": 4844.6,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "plug in a self attention component uh",
      "offset": 4846.48,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "into our Network right now I had to make",
      "offset": 4849,
      "duration": 6.199
    },
    {
      "lang": "en",
      "text": "one more change which is that here in",
      "offset": 4851.6,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "the generate uh we have to make sure",
      "offset": 4855.199,
      "duration": 5.801
    },
    {
      "lang": "en",
      "text": "that our idx that we feed into the model",
      "offset": 4857.44,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "because now we're using positional",
      "offset": 4861,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "embeddings we can never have more than",
      "offset": 4862.08,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "block size coming in because if idx is",
      "offset": 4864.44,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "more than block size then our position",
      "offset": 4867.52,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "embedding table is going to run out of",
      "offset": 4869.48,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "scope because it only has embeddings for",
      "offset": 4871.12,
      "duration": 3.96
    },
    {
      "lang": "en",
      "text": "up to block size and so therefore I",
      "offset": 4872.679,
      "duration": 4.52
    },
    {
      "lang": "en",
      "text": "added some uh code here to crop the",
      "offset": 4875.08,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "context that we're going to feed into",
      "offset": 4877.199,
      "duration": 6.321
    },
    {
      "lang": "en",
      "text": "self um so that uh we never pass in more",
      "offset": 4880.12,
      "duration": 5.4
    },
    {
      "lang": "en",
      "text": "than block siiz elements",
      "offset": 4883.52,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "so those are the changes and let's Now",
      "offset": 4885.52,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "train the network okay so I also came up",
      "offset": 4887.04,
      "duration": 3.88
    },
    {
      "lang": "en",
      "text": "to the script here and I decreased the",
      "offset": 4889.12,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "learning rate because uh the self",
      "offset": 4890.92,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "attention can't tolerate very very high",
      "offset": 4892.48,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "learning rates and then I also increased",
      "offset": 4894.28,
      "duration": 3.64
    },
    {
      "lang": "en",
      "text": "number of iterations because the",
      "offset": 4896.96,
      "duration": 2.64
    },
    {
      "lang": "en",
      "text": "learning rate is lower and then I",
      "offset": 4897.92,
      "duration": 3.4
    },
    {
      "lang": "en",
      "text": "trained it and previously we were only",
      "offset": 4899.6,
      "duration": 4.119
    },
    {
      "lang": "en",
      "text": "able to get to up to 2.5 and now we are",
      "offset": 4901.32,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "down to 2.4 so we definitely see a",
      "offset": 4903.719,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "little bit of an improvement from 2.5 to",
      "offset": 4906.199,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "2.4 roughly uh but the text is still not",
      "offset": 4908.12,
      "duration": 5.48
    },
    {
      "lang": "en",
      "text": "amazing so clearly the self attention",
      "offset": 4911,
      "duration": 5.56
    },
    {
      "lang": "en",
      "text": "head is doing some useful communication",
      "offset": 4913.6,
      "duration": 5.8
    },
    {
      "lang": "en",
      "text": "but um we still have a long way to go",
      "offset": 4916.56,
      "duration": 4.44
    },
    {
      "lang": "en",
      "text": "okay so now we've implemented the scale.",
      "offset": 4919.4,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "product attention now next up and the",
      "offset": 4921,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "attention is all you need paper there's",
      "offset": 4922.92,
      "duration": 4.12
    },
    {
      "lang": "en",
      "text": "something called multi-head attention",
      "offset": 4925.159,
      "duration": 3.961
    },
    {
      "lang": "en",
      "text": "and what is multi-head attention it's",
      "offset": 4927.04,
      "duration": 4.04
    },
    {
      "lang": "en",
      "text": "just applying multiple attentions in",
      "offset": 4929.12,
      "duration": 4.84
    },
    {
      "lang": "en",
      "text": "parallel and concatenating their results",
      "offset": 4931.08,
      "duration": 4.52
    },
    {
      "lang": "en",
      "text": "so they have a little bit of diagram",
      "offset": 4933.96,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "here I don't know if this is super clear",
      "offset": 4935.6,
      "duration": 5.079
    },
    {
      "lang": "en",
      "text": "it's really just multiple attentions in",
      "offset": 4938.28,
      "duration": 5.24
    },
    {
      "lang": "en",
      "text": "parallel so let's Implement that fairly",
      "offset": 4940.679,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "straightforward",
      "offset": 4943.52,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "if we want a multi-head attention then",
      "offset": 4945.32,
      "duration": 3.56
    },
    {
      "lang": "en",
      "text": "we want multiple heads of self attention",
      "offset": 4947.36,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "running in parallel so in pytorch we can",
      "offset": 4948.88,
      "duration": 6.359
    },
    {
      "lang": "en",
      "text": "do this by simply creating multiple",
      "offset": 4952.56,
      "duration": 5.72
    },
    {
      "lang": "en",
      "text": "heads so however heads how however many",
      "offset": 4955.239,
      "duration": 4.681
    },
    {
      "lang": "en",
      "text": "heads you want and then what is the head",
      "offset": 4958.28,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "size of each and then we run all of them",
      "offset": 4959.92,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "in parallel into a list and simply",
      "offset": 4963.32,
      "duration": 5.399
    },
    {
      "lang": "en",
      "text": "concatenate all of the outputs and we're",
      "offset": 4966,
      "duration": 4.679
    },
    {
      "lang": "en",
      "text": "concatenating over the channel",
      "offset": 4968.719,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "Dimension so the way this looks now is",
      "offset": 4970.679,
      "duration": 5.56
    },
    {
      "lang": "en",
      "text": "we don't have just a single ATT",
      "offset": 4973.28,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "that uh has a hit size of 32 because",
      "offset": 4976.239,
      "duration": 4.361
    },
    {
      "lang": "en",
      "text": "remember n Ed is",
      "offset": 4979.04,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "32 instead of having one Communication",
      "offset": 4980.6,
      "duration": 5.599
    },
    {
      "lang": "en",
      "text": "channel we now have four communication",
      "offset": 4983.76,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "channels in parallel and each one of",
      "offset": 4986.199,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "these communication channels typically",
      "offset": 4988.76,
      "duration": 6.04
    },
    {
      "lang": "en",
      "text": "will be uh smaller uh correspondingly so",
      "offset": 4990.92,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "because we have four communication",
      "offset": 4994.8,
      "duration": 3.399
    },
    {
      "lang": "en",
      "text": "channels we want eight dimensional self",
      "offset": 4995.96,
      "duration": 4.52
    },
    {
      "lang": "en",
      "text": "attention and so from each Communication",
      "offset": 4998.199,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "channel we're going to together eight",
      "offset": 5000.48,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "dimensional vectors and then we have",
      "offset": 5002.04,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "four of them and that concatenates to",
      "offset": 5003.92,
      "duration": 4.2
    },
    {
      "lang": "en",
      "text": "give us 32 which is the original and",
      "offset": 5005.8,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "embed and so this is kind of similar to",
      "offset": 5008.12,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "um if you're familiar with convolutions",
      "offset": 5010.84,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "this is kind of like a group convolution",
      "offset": 5012.199,
      "duration": 3.96
    },
    {
      "lang": "en",
      "text": "uh because basically instead of having",
      "offset": 5014.36,
      "duration": 3.64
    },
    {
      "lang": "en",
      "text": "one large convolution we do convolution",
      "offset": 5016.159,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "in groups and uh that's multi-headed",
      "offset": 5018,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "self",
      "offset": 5020.8,
      "duration": 3.8
    },
    {
      "lang": "en",
      "text": "attention and so then here we just use",
      "offset": 5021.6,
      "duration": 6.2
    },
    {
      "lang": "en",
      "text": "essay heads self attention heads instead",
      "offset": 5024.6,
      "duration": 6.599
    },
    {
      "lang": "en",
      "text": "now I actually ran it and uh scrolling",
      "offset": 5027.8,
      "duration": 5.72
    },
    {
      "lang": "en",
      "text": "down I ran the same thing and then we",
      "offset": 5031.199,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "now get this down to 2.28 roughly and",
      "offset": 5033.52,
      "duration": 5.32
    },
    {
      "lang": "en",
      "text": "the output is still the generation is",
      "offset": 5037.199,
      "duration": 3.281
    },
    {
      "lang": "en",
      "text": "still not amazing but clearly the",
      "offset": 5038.84,
      "duration": 3.24
    },
    {
      "lang": "en",
      "text": "validation loss is improving because we",
      "offset": 5040.48,
      "duration": 5.32
    },
    {
      "lang": "en",
      "text": "were at 2.4 just now and so it helps to",
      "offset": 5042.08,
      "duration": 5.159
    },
    {
      "lang": "en",
      "text": "have multiple communication channels",
      "offset": 5045.8,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "because obviously these tokens have a",
      "offset": 5047.239,
      "duration": 4.201
    },
    {
      "lang": "en",
      "text": "lot to talk about they want to find the",
      "offset": 5049.239,
      "duration": 4.041
    },
    {
      "lang": "en",
      "text": "consonants the vowels they want to find",
      "offset": 5051.44,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "the vowels just from certain positions",
      "offset": 5053.28,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "uh they want to find any kinds of",
      "offset": 5055.36,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "different things and so it helps to",
      "offset": 5057.28,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "create multiple independent channels of",
      "offset": 5059.28,
      "duration": 3.24
    },
    {
      "lang": "en",
      "text": "communication gather lots of different",
      "offset": 5060.719,
      "duration": 4.601
    },
    {
      "lang": "en",
      "text": "types of data and then uh decode the",
      "offset": 5062.52,
      "duration": 4.76
    },
    {
      "lang": "en",
      "text": "output now going back to the paper for a",
      "offset": 5065.32,
      "duration": 3.64
    },
    {
      "lang": "en",
      "text": "second of course I didn't explain this",
      "offset": 5067.28,
      "duration": 3.56
    },
    {
      "lang": "en",
      "text": "figure in full detail but we are",
      "offset": 5068.96,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "starting to see some components of what",
      "offset": 5070.84,
      "duration": 2.799
    },
    {
      "lang": "en",
      "text": "we've already implemented we have the",
      "offset": 5072.08,
      "duration": 3.4
    },
    {
      "lang": "en",
      "text": "positional encodings the token encodings",
      "offset": 5073.639,
      "duration": 4.201
    },
    {
      "lang": "en",
      "text": "that add we have the masked multi-headed",
      "offset": 5075.48,
      "duration": 5.56
    },
    {
      "lang": "en",
      "text": "attention implemented now here's another",
      "offset": 5077.84,
      "duration": 4.92
    },
    {
      "lang": "en",
      "text": "multi-headed attention which is a cross",
      "offset": 5081.04,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "attention to an encoder which we haven't",
      "offset": 5082.76,
      "duration": 3.72
    },
    {
      "lang": "en",
      "text": "we're not going to implement in this",
      "offset": 5085.199,
      "duration": 3.241
    },
    {
      "lang": "en",
      "text": "case I'm going to come back to that",
      "offset": 5086.48,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "later but I want you to notice that",
      "offset": 5088.44,
      "duration": 3.799
    },
    {
      "lang": "en",
      "text": "there's a feed forward part here and",
      "offset": 5090.4,
      "duration": 3.56
    },
    {
      "lang": "en",
      "text": "then this is grouped into a block that",
      "offset": 5092.239,
      "duration": 3.801
    },
    {
      "lang": "en",
      "text": "gets repeat it again and again now the",
      "offset": 5093.96,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "feedforward part here is just a simple",
      "offset": 5096.04,
      "duration": 4.599
    },
    {
      "lang": "en",
      "text": "uh multi-layer perceptron",
      "offset": 5097.96,
      "duration": 6.36
    },
    {
      "lang": "en",
      "text": "um so the multi-headed so here position",
      "offset": 5100.639,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "wise feed forward networks is just a",
      "offset": 5104.32,
      "duration": 4.6
    },
    {
      "lang": "en",
      "text": "simple little MLP so I want to start",
      "offset": 5106.159,
      "duration": 4.52
    },
    {
      "lang": "en",
      "text": "basically in a similar fashion also",
      "offset": 5108.92,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "adding computation into the network and",
      "offset": 5110.679,
      "duration": 5.721
    },
    {
      "lang": "en",
      "text": "this computation is on a per node level",
      "offset": 5113.4,
      "duration": 5.239
    },
    {
      "lang": "en",
      "text": "so I've already implemented it and you",
      "offset": 5116.4,
      "duration": 4.2
    },
    {
      "lang": "en",
      "text": "can see the diff highlighted on the left",
      "offset": 5118.639,
      "duration": 4.281
    },
    {
      "lang": "en",
      "text": "here when I've added or changed things",
      "offset": 5120.6,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "now before we had the self multi-headed",
      "offset": 5122.92,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "self attention that did the",
      "offset": 5125.08,
      "duration": 3.88
    },
    {
      "lang": "en",
      "text": "communication but we went way too fast",
      "offset": 5126.44,
      "duration": 5.279
    },
    {
      "lang": "en",
      "text": "to calculate the logits so the tokens",
      "offset": 5128.96,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "looked at each other but didn't really",
      "offset": 5131.719,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "have a lot of time to think on what they",
      "offset": 5132.88,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "found from the other tokens and so what",
      "offset": 5135.239,
      "duration": 5.281
    },
    {
      "lang": "en",
      "text": "I've implemented here is a little feet",
      "offset": 5138.8,
      "duration": 4.12
    },
    {
      "lang": "en",
      "text": "forward single layer and this little",
      "offset": 5140.52,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "layer is just a linear followed by a Rel",
      "offset": 5142.92,
      "duration": 5.239
    },
    {
      "lang": "en",
      "text": "nonlinearity and that's that's it so",
      "offset": 5145.159,
      "duration": 5.721
    },
    {
      "lang": "en",
      "text": "it's just a little layer and then I call",
      "offset": 5148.159,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "it feed",
      "offset": 5150.88,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "forward um and embed",
      "offset": 5152.32,
      "duration": 3.799
    },
    {
      "lang": "en",
      "text": "and then this feed forward is just",
      "offset": 5154.96,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "called sequentially right after the self",
      "offset": 5156.119,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "attention so we self attend then we feed",
      "offset": 5158,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "forward and you'll notice that the feet",
      "offset": 5161,
      "duration": 3.88
    },
    {
      "lang": "en",
      "text": "forward here when it's applying linear",
      "offset": 5162.8,
      "duration": 3.96
    },
    {
      "lang": "en",
      "text": "this is on a per token level all the",
      "offset": 5164.88,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "tokens do this independently so the self",
      "offset": 5166.76,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "attention is the communication and then",
      "offset": 5169.6,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "once they've gathered all the data now",
      "offset": 5171.639,
      "duration": 3.48
    },
    {
      "lang": "en",
      "text": "they need to think on that data",
      "offset": 5173.28,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "individually and so that's what feed",
      "offset": 5175.119,
      "duration": 3.721
    },
    {
      "lang": "en",
      "text": "forward is doing and that's why I've",
      "offset": 5176.88,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "added it here now when I train this the",
      "offset": 5178.84,
      "duration": 4.279
    },
    {
      "lang": "en",
      "text": "validation LW actually continues to go",
      "offset": 5181.6,
      "duration": 4.92
    },
    {
      "lang": "en",
      "text": "down now to 2. 24 which is down from",
      "offset": 5183.119,
      "duration": 5.681
    },
    {
      "lang": "en",
      "text": "2.28 uh the output still look kind of",
      "offset": 5186.52,
      "duration": 4.52
    },
    {
      "lang": "en",
      "text": "terrible but at least we've improved the",
      "offset": 5188.8,
      "duration": 5.48
    },
    {
      "lang": "en",
      "text": "situation and so as a preview we're",
      "offset": 5191.04,
      "duration": 6.159
    },
    {
      "lang": "en",
      "text": "going to now start to intersperse the",
      "offset": 5194.28,
      "duration": 5.56
    },
    {
      "lang": "en",
      "text": "communication with the computation and",
      "offset": 5197.199,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "that's also what the Transformer does",
      "offset": 5199.84,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "when it has blocks that communicate and",
      "offset": 5202.199,
      "duration": 4.601
    },
    {
      "lang": "en",
      "text": "then compute and it groups them and",
      "offset": 5204.08,
      "duration": 5.559
    },
    {
      "lang": "en",
      "text": "replicates them okay so let me show you",
      "offset": 5206.8,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "what we'd like to do we'd like to do",
      "offset": 5209.639,
      "duration": 4.121
    },
    {
      "lang": "en",
      "text": "something like this we have a block and",
      "offset": 5211.84,
      "duration": 3.399
    },
    {
      "lang": "en",
      "text": "this block is is basically this part",
      "offset": 5213.76,
      "duration": 3.8
    },
    {
      "lang": "en",
      "text": "here except for the cross",
      "offset": 5215.239,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "attention now the block basically",
      "offset": 5217.56,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "intersperses communication and then",
      "offset": 5219.48,
      "duration": 4.04
    },
    {
      "lang": "en",
      "text": "computation the computation the",
      "offset": 5221.56,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "communication is done using multi-headed",
      "offset": 5223.52,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "selfelf attention and then the",
      "offset": 5225.159,
      "duration": 3.56
    },
    {
      "lang": "en",
      "text": "computation is done using a feed forward",
      "offset": 5227.04,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "Network on all the tokens",
      "offset": 5228.719,
      "duration": 5.48
    },
    {
      "lang": "en",
      "text": "independently now what I've added here",
      "offset": 5231.6,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "also is you'll",
      "offset": 5234.199,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "notice this takes the number of",
      "offset": 5236.159,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "embeddings in the embedding Dimension",
      "offset": 5238.04,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "and number of heads that we would like",
      "offset": 5239.52,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "which is kind of like group size in",
      "offset": 5241.08,
      "duration": 3.639
    },
    {
      "lang": "en",
      "text": "group convolution and and I'm saying",
      "offset": 5242.56,
      "duration": 4.2
    },
    {
      "lang": "en",
      "text": "that number of heads we'd like is four",
      "offset": 5244.719,
      "duration": 4.761
    },
    {
      "lang": "en",
      "text": "and so because this is 32 we calculate",
      "offset": 5246.76,
      "duration": 4.359
    },
    {
      "lang": "en",
      "text": "that because this is 32 the number of",
      "offset": 5249.48,
      "duration": 5.239
    },
    {
      "lang": "en",
      "text": "heads should be four um the head size",
      "offset": 5251.119,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "should be eight so that everything sort",
      "offset": 5254.719,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "of works out Channel wise um so this is",
      "offset": 5256.239,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "how the Transformer structures uh sort",
      "offset": 5259.28,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "of the uh the sizes typically so the",
      "offset": 5261.239,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "head size will become eight and then",
      "offset": 5264.48,
      "duration": 3.32
    },
    {
      "lang": "en",
      "text": "this is how we want to intersperse them",
      "offset": 5265.96,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "and then here I'm trying to create",
      "offset": 5267.8,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "blocks which is just a sequential",
      "offset": 5269.56,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "application of block block block so that",
      "offset": 5271.8,
      "duration": 3.879
    },
    {
      "lang": "en",
      "text": "we're interspersing communication feed",
      "offset": 5273.88,
      "duration": 3.88
    },
    {
      "lang": "en",
      "text": "forward many many times and then finally",
      "offset": 5275.679,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "we decode now I actually tried to run",
      "offset": 5277.76,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "this and the problem is this doesn't",
      "offset": 5281.119,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "actually give a very good uh answer and",
      "offset": 5282.8,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "very good result and the reason for that",
      "offset": 5285.84,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "is we're start starting to actually get",
      "offset": 5287.6,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "like a pretty deep neural net and deep",
      "offset": 5289.199,
      "duration": 4.201
    },
    {
      "lang": "en",
      "text": "neural Nets uh suffer from optimization",
      "offset": 5291.36,
      "duration": 3.16
    },
    {
      "lang": "en",
      "text": "issues and I think that's what we're",
      "offset": 5293.4,
      "duration": 2.68
    },
    {
      "lang": "en",
      "text": "kind of like slightly starting to run",
      "offset": 5294.52,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "into so we need one more idea that we",
      "offset": 5296.08,
      "duration": 5.119
    },
    {
      "lang": "en",
      "text": "can borrow from the um Transformer paper",
      "offset": 5298.36,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "to resolve those difficulties now there",
      "offset": 5301.199,
      "duration": 4.201
    },
    {
      "lang": "en",
      "text": "are two optimizations that dramatically",
      "offset": 5303.159,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "help with the depth of these networks",
      "offset": 5305.4,
      "duration": 3.96
    },
    {
      "lang": "en",
      "text": "and make sure that the networks remain",
      "offset": 5307.32,
      "duration": 3.72
    },
    {
      "lang": "en",
      "text": "optimizable let's talk about the first",
      "offset": 5309.36,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "one the first one in this diagram is you",
      "offset": 5311.04,
      "duration": 5.639
    },
    {
      "lang": "en",
      "text": "see this Arrow here and then this arrow",
      "offset": 5313.44,
      "duration": 4.759
    },
    {
      "lang": "en",
      "text": "and this Arrow those are skip",
      "offset": 5316.679,
      "duration": 3.801
    },
    {
      "lang": "en",
      "text": "connections or sometimes called residual",
      "offset": 5318.199,
      "duration": 5.121
    },
    {
      "lang": "en",
      "text": "connections they come from this paper uh",
      "offset": 5320.48,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "the presidual learning for image",
      "offset": 5323.32,
      "duration": 3.319
    },
    {
      "lang": "en",
      "text": "recognition from about",
      "offset": 5324.56,
      "duration": 6.44
    },
    {
      "lang": "en",
      "text": "2015 uh that introduced the concept now",
      "offset": 5326.639,
      "duration": 6.641
    },
    {
      "lang": "en",
      "text": "these are basically what it means is you",
      "offset": 5331,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "transform data but then you have a skip",
      "offset": 5333.28,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "connection with addition from the",
      "offset": 5335.239,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "previous features now the way I like to",
      "offset": 5337.84,
      "duration": 5.48
    },
    {
      "lang": "en",
      "text": "visualize it uh that I prefer is the",
      "offset": 5340.199,
      "duration": 5.121
    },
    {
      "lang": "en",
      "text": "following here the computation happens",
      "offset": 5343.32,
      "duration": 5.08
    },
    {
      "lang": "en",
      "text": "from the top to bottom and basically you",
      "offset": 5345.32,
      "duration": 5.919
    },
    {
      "lang": "en",
      "text": "have this uh residual pathway and you",
      "offset": 5348.4,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "are free to Fork off from the residual",
      "offset": 5351.239,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "pathway perform some computation and",
      "offset": 5353.04,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "then project back to the residual",
      "offset": 5355,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "pathway via addition and so you go from",
      "offset": 5356.4,
      "duration": 6.52
    },
    {
      "lang": "en",
      "text": "the the uh inputs to the targets only",
      "offset": 5359.639,
      "duration": 6.281
    },
    {
      "lang": "en",
      "text": "via plus and plus plus and the reason",
      "offset": 5362.92,
      "duration": 4.36
    },
    {
      "lang": "en",
      "text": "this is useful is because during back",
      "offset": 5365.92,
      "duration": 3.64
    },
    {
      "lang": "en",
      "text": "propagation remember from our microG",
      "offset": 5367.28,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "grad video earlier addition distributes",
      "offset": 5369.56,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "gradients equally to both of its",
      "offset": 5372.28,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "branches that that fed as the input and",
      "offset": 5374.36,
      "duration": 5.96
    },
    {
      "lang": "en",
      "text": "so the supervision or the gradients from",
      "offset": 5377.32,
      "duration": 6.12
    },
    {
      "lang": "en",
      "text": "the loss basically hop through every",
      "offset": 5380.32,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "addition node all the way to the input",
      "offset": 5383.44,
      "duration": 6.88
    },
    {
      "lang": "en",
      "text": "and then also Fork off into the residual",
      "offset": 5386.8,
      "duration": 5.439
    },
    {
      "lang": "en",
      "text": "blocks but basically you have this",
      "offset": 5390.32,
      "duration": 3.399
    },
    {
      "lang": "en",
      "text": "gradient Super Highway that goes",
      "offset": 5392.239,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "directly from the supervision all the",
      "offset": 5393.719,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "way to the input unimpeded and then",
      "offset": 5395.84,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "these viral blocks are usually",
      "offset": 5398.44,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "initialized in the beginning so they",
      "offset": 5399.84,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "contribute very very little if anything",
      "offset": 5401.48,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "to the residual pathway they they are",
      "offset": 5403.199,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "initialized that way so in the beginning",
      "offset": 5405.239,
      "duration": 4.121
    },
    {
      "lang": "en",
      "text": "they are sort of almost kind of like not",
      "offset": 5407.6,
      "duration": 4.039
    },
    {
      "lang": "en",
      "text": "there but then during the optimization",
      "offset": 5409.36,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "they come online over time and they uh",
      "offset": 5411.639,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "start to contribute but at least at the",
      "offset": 5414.8,
      "duration": 4.2
    },
    {
      "lang": "en",
      "text": "initialization you can go from directly",
      "offset": 5417.159,
      "duration": 4.201
    },
    {
      "lang": "en",
      "text": "supervision to the input gradient is",
      "offset": 5419,
      "duration": 4.199
    },
    {
      "lang": "en",
      "text": "unimpeded and just flows and then the",
      "offset": 5421.36,
      "duration": 3.319
    },
    {
      "lang": "en",
      "text": "blocks over time",
      "offset": 5423.199,
      "duration": 4.201
    },
    {
      "lang": "en",
      "text": "kick in and so that dramatically helps",
      "offset": 5424.679,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "with the optimization so let's implement",
      "offset": 5427.4,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "this so coming back to our block here",
      "offset": 5429.32,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "basically what we want to do is we want",
      "offset": 5431.48,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "to do xal",
      "offset": 5433.719,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "X+ self attention and xal X+ self. feed",
      "offset": 5435.48,
      "duration": 7.88
    },
    {
      "lang": "en",
      "text": "forward so this is X and then we Fork",
      "offset": 5439.719,
      "duration": 5.4
    },
    {
      "lang": "en",
      "text": "off and do some communication and come",
      "offset": 5443.36,
      "duration": 3.56
    },
    {
      "lang": "en",
      "text": "back and we Fork off and we do some",
      "offset": 5445.119,
      "duration": 4.201
    },
    {
      "lang": "en",
      "text": "computation and come back so those are",
      "offset": 5446.92,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "residual connections and then swinging",
      "offset": 5449.32,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "back up here we also have to introd use",
      "offset": 5451.92,
      "duration": 5.719
    },
    {
      "lang": "en",
      "text": "this projection so nn.",
      "offset": 5454.28,
      "duration": 6.6
    },
    {
      "lang": "en",
      "text": "linear and uh this is going to be",
      "offset": 5457.639,
      "duration": 5.641
    },
    {
      "lang": "en",
      "text": "from after we concatenate this this is",
      "offset": 5460.88,
      "duration": 4.92
    },
    {
      "lang": "en",
      "text": "the prze and embed so this is the output",
      "offset": 5463.28,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "of the self tension itself but then we",
      "offset": 5465.8,
      "duration": 5.439
    },
    {
      "lang": "en",
      "text": "actually want the uh to apply the",
      "offset": 5468.32,
      "duration": 4.919
    },
    {
      "lang": "en",
      "text": "projection and that's the",
      "offset": 5471.239,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "result so the projection is just a",
      "offset": 5473.239,
      "duration": 3.48
    },
    {
      "lang": "en",
      "text": "linear transformation of the outcome of",
      "offset": 5475.159,
      "duration": 2.681
    },
    {
      "lang": "en",
      "text": "this",
      "offset": 5476.719,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "layer so that's the projection back into",
      "offset": 5477.84,
      "duration": 4.6
    },
    {
      "lang": "en",
      "text": "the virual pathway and then here in a",
      "offset": 5480.159,
      "duration": 3.801
    },
    {
      "lang": "en",
      "text": "feet forward it's going to be the same",
      "offset": 5482.44,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "same thing I could have a a self doot",
      "offset": 5483.96,
      "duration": 4.279
    },
    {
      "lang": "en",
      "text": "projection here as well but let me just",
      "offset": 5486.119,
      "duration": 6.481
    },
    {
      "lang": "en",
      "text": "simplify it and let me uh couple it",
      "offset": 5488.239,
      "duration": 6.641
    },
    {
      "lang": "en",
      "text": "inside the same sequential container and",
      "offset": 5492.6,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "so this is the projection layer going",
      "offset": 5494.88,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "back into the residual",
      "offset": 5496.6,
      "duration": 3.639
    },
    {
      "lang": "en",
      "text": "pathway and",
      "offset": 5498.32,
      "duration": 4.68
    },
    {
      "lang": "en",
      "text": "so that's uh well that's it so now we",
      "offset": 5500.239,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "can train this so I implemented one more",
      "offset": 5503,
      "duration": 4.44
    },
    {
      "lang": "en",
      "text": "small change when you look into the",
      "offset": 5504.719,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "paper again you see that the",
      "offset": 5507.44,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "dimensionality of input and output is",
      "offset": 5509.52,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "512 for them and they're saying that the",
      "offset": 5511.44,
      "duration": 3.88
    },
    {
      "lang": "en",
      "text": "inner layer here in the feet forward has",
      "offset": 5513.36,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "dimensionality of 248 so there's a",
      "offset": 5515.32,
      "duration": 5.16
    },
    {
      "lang": "en",
      "text": "multiplier of four and so the inner",
      "offset": 5517.52,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "layer of the feet forward Network should",
      "offset": 5520.48,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "be multiplied by four in terms of",
      "offset": 5522.88,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "Channel sizes so I came here and I",
      "offset": 5524.159,
      "duration": 4.441
    },
    {
      "lang": "en",
      "text": "multiplied four times embed here for the",
      "offset": 5526,
      "duration": 4.6
    },
    {
      "lang": "en",
      "text": "feed forward and then from four times",
      "offset": 5528.6,
      "duration": 4.68
    },
    {
      "lang": "en",
      "text": "nmed coming back down to nmed when we go",
      "offset": 5530.6,
      "duration": 4.92
    },
    {
      "lang": "en",
      "text": "back to the pro uh to the projection so",
      "offset": 5533.28,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "adding a bit of computation here and",
      "offset": 5535.52,
      "duration": 3.639
    },
    {
      "lang": "en",
      "text": "growing that layer that is in the",
      "offset": 5537.04,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "residual block on the side of the",
      "offset": 5539.159,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "residual",
      "offset": 5541.119,
      "duration": 3.241
    },
    {
      "lang": "en",
      "text": "pathway and then I train this and we",
      "offset": 5542.119,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "actually get down all the way to uh 2.08",
      "offset": 5544.36,
      "duration": 4.68
    },
    {
      "lang": "en",
      "text": "validation loss and we also see that",
      "offset": 5547.119,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "network is starting to get big enough",
      "offset": 5549.04,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "that our train loss is getting ahead of",
      "offset": 5550.719,
      "duration": 3.161
    },
    {
      "lang": "en",
      "text": "validation loss so we're starting to see",
      "offset": 5552.239,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "like a little bit of",
      "offset": 5553.88,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "overfitting and um our our",
      "offset": 5555.239,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "um uh Generations here are still not",
      "offset": 5558.88,
      "duration": 4.04
    },
    {
      "lang": "en",
      "text": "amazing but at least you see that we can",
      "offset": 5561.239,
      "duration": 5.521
    },
    {
      "lang": "en",
      "text": "see like is here this now grief syn like",
      "offset": 5562.92,
      "duration": 6.04
    },
    {
      "lang": "en",
      "text": "this starts to almost look like English",
      "offset": 5566.76,
      "duration": 4.2
    },
    {
      "lang": "en",
      "text": "so um yeah we're starting to really get",
      "offset": 5568.96,
      "duration": 3.96
    },
    {
      "lang": "en",
      "text": "there okay and the second Innovation",
      "offset": 5570.96,
      "duration": 3.719
    },
    {
      "lang": "en",
      "text": "that is very helpful for optimizing very",
      "offset": 5572.92,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "deep neural networks is right here so we",
      "offset": 5574.679,
      "duration": 3.56
    },
    {
      "lang": "en",
      "text": "have this addition now that's the",
      "offset": 5577.08,
      "duration": 3.4
    },
    {
      "lang": "en",
      "text": "residual part but this Norm is referring",
      "offset": 5578.239,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "to something called layer Norm so layer",
      "offset": 5580.48,
      "duration": 4.12
    },
    {
      "lang": "en",
      "text": "Norm is implemented in pytorch it's a",
      "offset": 5583.119,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "paper that came out a while back here",
      "offset": 5584.6,
      "duration": 7
    },
    {
      "lang": "en",
      "text": "um and layer Norm is very very similar",
      "offset": 5589.199,
      "duration": 5.401
    },
    {
      "lang": "en",
      "text": "to bash Norm so remember back to our",
      "offset": 5591.6,
      "duration": 4.76
    },
    {
      "lang": "en",
      "text": "make more series part three we",
      "offset": 5594.6,
      "duration": 3.119
    },
    {
      "lang": "en",
      "text": "implemented bash",
      "offset": 5596.36,
      "duration": 3.48
    },
    {
      "lang": "en",
      "text": "normalization and uh bash normalization",
      "offset": 5597.719,
      "duration": 5.241
    },
    {
      "lang": "en",
      "text": "basically just made sure that um Across",
      "offset": 5599.84,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "The Bash dimension any individual neuron",
      "offset": 5602.96,
      "duration": 7.48
    },
    {
      "lang": "en",
      "text": "had unit uh Gan um distribution so it",
      "offset": 5605.92,
      "duration": 6.36
    },
    {
      "lang": "en",
      "text": "was zero mean and unit standard",
      "offset": 5610.44,
      "duration": 5.4
    },
    {
      "lang": "en",
      "text": "deviation one standard deviation output",
      "offset": 5612.28,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "so what I did here is I'm copy pasting",
      "offset": 5615.84,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "the bashor 1D that we developed in our",
      "offset": 5617.8,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "make more series and see here we can",
      "offset": 5619.36,
      "duration": 5.48
    },
    {
      "lang": "en",
      "text": "initialize for example this module and",
      "offset": 5622.44,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "we can have a batch of 32 100",
      "offset": 5624.84,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "dimensional vectors feeding through the",
      "offset": 5627.159,
      "duration": 5.281
    },
    {
      "lang": "en",
      "text": "bachor layer so what this does is it",
      "offset": 5628.679,
      "duration": 5.96
    },
    {
      "lang": "en",
      "text": "guarantees that when we look at just the",
      "offset": 5632.44,
      "duration": 5.88
    },
    {
      "lang": "en",
      "text": "zeroth column it's a zero mean one",
      "offset": 5634.639,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "standard deviation so it's normalizing",
      "offset": 5638.32,
      "duration": 5.799
    },
    {
      "lang": "en",
      "text": "every single column of this uh input now",
      "offset": 5640.719,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "the rows are not uh going to be",
      "offset": 5644.119,
      "duration": 3.881
    },
    {
      "lang": "en",
      "text": "normalized by default because we're just",
      "offset": 5646.239,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "normalizing columns so let's now",
      "offset": 5648,
      "duration": 4.44
    },
    {
      "lang": "en",
      "text": "Implement layer Norm uh it's very",
      "offset": 5650.159,
      "duration": 5.441
    },
    {
      "lang": "en",
      "text": "complicated look we come here we change",
      "offset": 5652.44,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "this from zero to one so we don't",
      "offset": 5655.6,
      "duration": 5.32
    },
    {
      "lang": "en",
      "text": "normalize The Columns we normalize the",
      "offset": 5658.28,
      "duration": 5.08
    },
    {
      "lang": "en",
      "text": "rows and now we've implemented layer",
      "offset": 5660.92,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "Norm",
      "offset": 5663.36,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "so now the columns are not going to be",
      "offset": 5665.08,
      "duration": 6.4
    },
    {
      "lang": "en",
      "text": "normalized um but the rows are going to",
      "offset": 5668.159,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "be normalized for every individual",
      "offset": 5671.48,
      "duration": 3.639
    },
    {
      "lang": "en",
      "text": "example it's 100 dimensional Vector is",
      "offset": 5673.199,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "normalized uh in this way and because",
      "offset": 5675.119,
      "duration": 5.161
    },
    {
      "lang": "en",
      "text": "our computation Now does not span across",
      "offset": 5678.08,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "examples we can delete all of this",
      "offset": 5680.28,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "buffers stuff uh because uh we can",
      "offset": 5683.28,
      "duration": 5.399
    },
    {
      "lang": "en",
      "text": "always apply this operation and don't",
      "offset": 5685.8,
      "duration": 5.08
    },
    {
      "lang": "en",
      "text": "need to maintain any running buffers so",
      "offset": 5688.679,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "we don't need the",
      "offset": 5690.88,
      "duration": 3.4
    },
    {
      "lang": "en",
      "text": "buffers uh we",
      "offset": 5692.28,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "don't There's no distinction between",
      "offset": 5694.28,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "training and test",
      "offset": 5696.28,
      "duration": 4.52
    },
    {
      "lang": "en",
      "text": "time uh and we don't need these running",
      "offset": 5698.199,
      "duration": 5.561
    },
    {
      "lang": "en",
      "text": "buffers we do keep gamma and beta we",
      "offset": 5700.8,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "don't need the momentum we don't care if",
      "offset": 5703.76,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "it's training or not and this is now a",
      "offset": 5705.52,
      "duration": 4.28
    },
    {
      "lang": "en",
      "text": "layer",
      "offset": 5708.8,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "norm and it normalizes the rows instead",
      "offset": 5709.8,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "of the columns and this here is",
      "offset": 5712.96,
      "duration": 6.8
    },
    {
      "lang": "en",
      "text": "identical to basically this here so",
      "offset": 5715.96,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "let's now Implement layer Norm in our",
      "offset": 5719.76,
      "duration": 3.56
    },
    {
      "lang": "en",
      "text": "Transformer before I incorporate the",
      "offset": 5721.56,
      "duration": 3.96
    },
    {
      "lang": "en",
      "text": "layer Norm I just wanted to note that as",
      "offset": 5723.32,
      "duration": 3.879
    },
    {
      "lang": "en",
      "text": "I said very few details about the",
      "offset": 5725.52,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "Transformer have changed in the last 5",
      "offset": 5727.199,
      "duration": 3.081
    },
    {
      "lang": "en",
      "text": "years but this is actually something",
      "offset": 5728.719,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "that slightly departs from the original",
      "offset": 5730.28,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "paper you see that the ADD and Norm is",
      "offset": 5731.679,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "applied after the",
      "offset": 5734.44,
      "duration": 5.56
    },
    {
      "lang": "en",
      "text": "transformation but um in now it is a bit",
      "offset": 5736.56,
      "duration": 5.559
    },
    {
      "lang": "en",
      "text": "more uh basically common to apply the",
      "offset": 5740,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "layer Norm before the transformation so",
      "offset": 5742.119,
      "duration": 4.761
    },
    {
      "lang": "en",
      "text": "there's a reshuffling of the layer Norms",
      "offset": 5744.56,
      "duration": 3.72
    },
    {
      "lang": "en",
      "text": "uh so this is called the prorm",
      "offset": 5746.88,
      "duration": 2.6
    },
    {
      "lang": "en",
      "text": "formulation and that's the one that",
      "offset": 5748.28,
      "duration": 2.52
    },
    {
      "lang": "en",
      "text": "we're going to implement as well so",
      "offset": 5749.48,
      "duration": 3.8
    },
    {
      "lang": "en",
      "text": "select deviation from the original paper",
      "offset": 5750.8,
      "duration": 4.359
    },
    {
      "lang": "en",
      "text": "basically we need two layer Norms layer",
      "offset": 5753.28,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "Norm one is uh NN do layer norm and we",
      "offset": 5755.159,
      "duration": 5.841
    },
    {
      "lang": "en",
      "text": "tell it how many um what is the",
      "offset": 5759.28,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "embedding Dimension and we need the",
      "offset": 5761,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "second layer norm and then here the",
      "offset": 5763.52,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "layer Norms are applied immediately on X",
      "offset": 5766.6,
      "duration": 6.44
    },
    {
      "lang": "en",
      "text": "so self. layer Norm one applied on X and",
      "offset": 5769.36,
      "duration": 6.4
    },
    {
      "lang": "en",
      "text": "self. layer Norm two applied on X before",
      "offset": 5773.04,
      "duration": 5.079
    },
    {
      "lang": "en",
      "text": "it goes into self attention and feed",
      "offset": 5775.76,
      "duration": 4.919
    },
    {
      "lang": "en",
      "text": "forward and uh the size of the layer",
      "offset": 5778.119,
      "duration": 5.481
    },
    {
      "lang": "en",
      "text": "Norm here is an ed so 32 so when the",
      "offset": 5780.679,
      "duration": 5.761
    },
    {
      "lang": "en",
      "text": "layer Norm is normalizing our features",
      "offset": 5783.6,
      "duration": 6.76
    },
    {
      "lang": "en",
      "text": "it is uh the normalization here uh",
      "offset": 5786.44,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "happens the mean and the variance are",
      "offset": 5790.36,
      "duration": 4.52
    },
    {
      "lang": "en",
      "text": "taken over 32 numbers so the batch and",
      "offset": 5792.36,
      "duration": 5.16
    },
    {
      "lang": "en",
      "text": "the time act as batch Dimensions both of",
      "offset": 5794.88,
      "duration": 5.759
    },
    {
      "lang": "en",
      "text": "them so this is kind of like a per token",
      "offset": 5797.52,
      "duration": 5.32
    },
    {
      "lang": "en",
      "text": "um transformation that just normalizes",
      "offset": 5800.639,
      "duration": 5.56
    },
    {
      "lang": "en",
      "text": "the features and makes them a unit mean",
      "offset": 5802.84,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "uh unit Gan at",
      "offset": 5806.199,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "initialization but of course because",
      "offset": 5808.36,
      "duration": 4.04
    },
    {
      "lang": "en",
      "text": "these layer Norms inside it have these",
      "offset": 5810.04,
      "duration": 4.199
    },
    {
      "lang": "en",
      "text": "gamma and beta training",
      "offset": 5812.4,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "parameters uh the layer Norm will U",
      "offset": 5814.239,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "eventually create outputs that might not",
      "offset": 5817.28,
      "duration": 4.6
    },
    {
      "lang": "en",
      "text": "be unit gion but the optimization will",
      "offset": 5819.44,
      "duration": 5.719
    },
    {
      "lang": "en",
      "text": "determine that so for now this is the uh",
      "offset": 5821.88,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "this is incorporating the layer norms",
      "offset": 5825.159,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "and let's train them on okay so I let it",
      "offset": 5826.84,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "run and we see that we get down to 2.06",
      "offset": 5829.159,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "which is better than the previous 2.08",
      "offset": 5832.04,
      "duration": 3.639
    },
    {
      "lang": "en",
      "text": "so a slight Improvement by adding the",
      "offset": 5834.159,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "layer norms and I'd expect that they",
      "offset": 5835.679,
      "duration": 4.04
    },
    {
      "lang": "en",
      "text": "help uh even more if we had bigger and",
      "offset": 5837.679,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "deeper Network one more thing I forgot",
      "offset": 5839.719,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "to add is that there should be a layer",
      "offset": 5841.92,
      "duration": 4.92
    },
    {
      "lang": "en",
      "text": "Norm here also typically as at the end",
      "offset": 5843.719,
      "duration": 5.081
    },
    {
      "lang": "en",
      "text": "of the Transformer and right before the",
      "offset": 5846.84,
      "duration": 4.839
    },
    {
      "lang": "en",
      "text": "final uh linear layer that decodes into",
      "offset": 5848.8,
      "duration": 6.2
    },
    {
      "lang": "en",
      "text": "vocabulary so I added that as well so at",
      "offset": 5851.679,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "this stage we actually have a pretty",
      "offset": 5855,
      "duration": 3.119
    },
    {
      "lang": "en",
      "text": "complete uh Transformer according to the",
      "offset": 5856.239,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "original paper and it's a decoder only",
      "offset": 5858.119,
      "duration": 4.12
    },
    {
      "lang": "en",
      "text": "Transformer I'll I'll talk about that in",
      "offset": 5860.239,
      "duration": 4.36
    },
    {
      "lang": "en",
      "text": "a second uh but at this stage uh the",
      "offset": 5862.239,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "major pieces are in place so we can try",
      "offset": 5864.599,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "to scale this up and see how well we can",
      "offset": 5866.239,
      "duration": 3.96
    },
    {
      "lang": "en",
      "text": "push this number now in order to scale",
      "offset": 5867.96,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "out the model I had to perform some",
      "offset": 5870.199,
      "duration": 4.361
    },
    {
      "lang": "en",
      "text": "cosmetic changes here to make it nicer",
      "offset": 5871.96,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "so I introduced this variable called n",
      "offset": 5874.56,
      "duration": 3.4
    },
    {
      "lang": "en",
      "text": "layer which just specifies how many",
      "offset": 5876.44,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "layers of the blocks we're going to have",
      "offset": 5877.96,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "I created a bunch of blocks and we have",
      "offset": 5881.08,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "a new variable number of heads as well I",
      "offset": 5882.52,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "pulled out the layer Norm here and uh so",
      "offset": 5885.56,
      "duration": 4.44
    },
    {
      "lang": "en",
      "text": "this is identical now one thing that I",
      "offset": 5887.719,
      "duration": 5.48
    },
    {
      "lang": "en",
      "text": "did briefly change is I added a Dropout",
      "offset": 5890,
      "duration": 5.56
    },
    {
      "lang": "en",
      "text": "so Dropout is something that you can add",
      "offset": 5893.199,
      "duration": 4.761
    },
    {
      "lang": "en",
      "text": "right before the residual connection",
      "offset": 5895.56,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "back right before the connection back",
      "offset": 5897.96,
      "duration": 4.44
    },
    {
      "lang": "en",
      "text": "into the residual pathway so we can drop",
      "offset": 5899.88,
      "duration": 6.279
    },
    {
      "lang": "en",
      "text": "out that as l layer here we can drop out",
      "offset": 5902.4,
      "duration": 5.48
    },
    {
      "lang": "en",
      "text": "uh here at the end of the multi-headed",
      "offset": 5906.159,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "exension as well and we can also drop",
      "offset": 5907.88,
      "duration": 6.4
    },
    {
      "lang": "en",
      "text": "out here uh when we calculate the um",
      "offset": 5910.4,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "basically affinities and after the",
      "offset": 5914.28,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "softmax we can drop out some of those so",
      "offset": 5916,
      "duration": 4.119
    },
    {
      "lang": "en",
      "text": "we can randomly prevent some of the",
      "offset": 5918.28,
      "duration": 2.839
    },
    {
      "lang": "en",
      "text": "nodes from",
      "offset": 5920.119,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "communicating and so Dropout uh comes",
      "offset": 5921.119,
      "duration": 7.921
    },
    {
      "lang": "en",
      "text": "from this paper from 2014 or so and",
      "offset": 5923.8,
      "duration": 7
    },
    {
      "lang": "en",
      "text": "basically it takes your neural",
      "offset": 5929.04,
      "duration": 4.44
    },
    {
      "lang": "en",
      "text": "nut and it randomly every forward",
      "offset": 5930.8,
      "duration": 5.64
    },
    {
      "lang": "en",
      "text": "backward pass shuts off some subset of",
      "offset": 5933.48,
      "duration": 6.04
    },
    {
      "lang": "en",
      "text": "uh neurons so randomly drops them to",
      "offset": 5936.44,
      "duration": 6.199
    },
    {
      "lang": "en",
      "text": "zero and trains without them and what",
      "offset": 5939.52,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "this does effectively is because the",
      "offset": 5942.639,
      "duration": 3.881
    },
    {
      "lang": "en",
      "text": "mask of what's being dropped out is",
      "offset": 5944.88,
      "duration": 3.08
    },
    {
      "lang": "en",
      "text": "changed every single forward backward",
      "offset": 5946.52,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "pass it ends up kind of uh training an",
      "offset": 5947.96,
      "duration": 5.639
    },
    {
      "lang": "en",
      "text": "ensemble of sub networks and then at",
      "offset": 5951,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "test time everything is fully enabled",
      "offset": 5953.599,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "and kind of all of those sub networks",
      "offset": 5955.159,
      "duration": 3.641
    },
    {
      "lang": "en",
      "text": "are merged into a single Ensemble if you",
      "offset": 5956.719,
      "duration": 3.4
    },
    {
      "lang": "en",
      "text": "can if you want to think about it that",
      "offset": 5958.8,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "way so I would read the paper to get the",
      "offset": 5960.119,
      "duration": 4.361
    },
    {
      "lang": "en",
      "text": "full detail for now we're just going to",
      "offset": 5962.4,
      "duration": 3.56
    },
    {
      "lang": "en",
      "text": "stay on the level of this is a",
      "offset": 5964.48,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "regularization technique and I added it",
      "offset": 5965.96,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "because I'm about to scale up the model",
      "offset": 5968.639,
      "duration": 3.721
    },
    {
      "lang": "en",
      "text": "quite a bit and I was concerned about",
      "offset": 5970.119,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "overfitting so now when we scroll up to",
      "offset": 5972.36,
      "duration": 4.6
    },
    {
      "lang": "en",
      "text": "the top uh we'll see that I changed a",
      "offset": 5974.76,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "number of hyper parameters here about",
      "offset": 5976.96,
      "duration": 3.8
    },
    {
      "lang": "en",
      "text": "our neural nut so I made the batch size",
      "offset": 5978.599,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "be much larger now it's 64 I changed the",
      "offset": 5980.76,
      "duration": 5.359
    },
    {
      "lang": "en",
      "text": "block size to be 256 so previously it",
      "offset": 5983.639,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "was just eight eight characters of",
      "offset": 5986.119,
      "duration": 4.201
    },
    {
      "lang": "en",
      "text": "context now it is 256 characters of",
      "offset": 5987.639,
      "duration": 6.52
    },
    {
      "lang": "en",
      "text": "context to predict the 257th",
      "offset": 5990.32,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "uh I brought down the learning rate a",
      "offset": 5994.159,
      "duration": 3.161
    },
    {
      "lang": "en",
      "text": "little bit because the neural net is now",
      "offset": 5995.84,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "much bigger so I brought down the",
      "offset": 5997.32,
      "duration": 3.72
    },
    {
      "lang": "en",
      "text": "learning rate the embedding Dimension is",
      "offset": 5998.8,
      "duration": 6.56
    },
    {
      "lang": "en",
      "text": "now 384 and there are six heads so 384",
      "offset": 6001.04,
      "duration": 7.159
    },
    {
      "lang": "en",
      "text": "divide 6 means that every head is 64",
      "offset": 6005.36,
      "duration": 6.279
    },
    {
      "lang": "en",
      "text": "dimensional as it as a standard and then",
      "offset": 6008.199,
      "duration": 5.761
    },
    {
      "lang": "en",
      "text": "there's going to be six layers of that",
      "offset": 6011.639,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "and the Dropout will be at 02 so every",
      "offset": 6013.96,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "forward backward pass 20% of all of",
      "offset": 6015.88,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "these um intermediate calculations are",
      "offset": 6018.199,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "disabled and dropped to zero",
      "offset": 6021.56,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "and then I already trained this and I",
      "offset": 6024.119,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "ran it so uh drum roll how well does it",
      "offset": 6025.56,
      "duration": 6.079
    },
    {
      "lang": "en",
      "text": "perform so let me just scroll up",
      "offset": 6028.52,
      "duration": 6.32
    },
    {
      "lang": "en",
      "text": "here we get a validation loss of",
      "offset": 6031.639,
      "duration": 5.441
    },
    {
      "lang": "en",
      "text": "1.48 which is actually quite a bit of an",
      "offset": 6034.84,
      "duration": 3.799
    },
    {
      "lang": "en",
      "text": "improvement on what we had before which",
      "offset": 6037.08,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "I think was 2.07 so it went from 2.07",
      "offset": 6038.639,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "all the way down to 1.48 just by scaling",
      "offset": 6041.639,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "up this neural nut with the code that we",
      "offset": 6043.679,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "have and this of course ran for a lot",
      "offset": 6045.4,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "longer this maybe trained for I want to",
      "offset": 6047.679,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "say about 15 minutes on my a100 GPU so",
      "offset": 6049.719,
      "duration": 5.241
    },
    {
      "lang": "en",
      "text": "that's a pretty a GPU and if you don't",
      "offset": 6052.88,
      "duration": 3.239
    },
    {
      "lang": "en",
      "text": "have a GPU you're not going to be able",
      "offset": 6054.96,
      "duration": 4.04
    },
    {
      "lang": "en",
      "text": "to reproduce this uh on a CPU this would",
      "offset": 6056.119,
      "duration": 5.361
    },
    {
      "lang": "en",
      "text": "be um I would not run this on a CPU or",
      "offset": 6059,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "MacBook or something like that you'll",
      "offset": 6061.48,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "have to Brak down the number of uh",
      "offset": 6063,
      "duration": 3.8
    },
    {
      "lang": "en",
      "text": "layers and the embedding Dimension and",
      "offset": 6064.76,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "so on uh but in about 15 minutes we can",
      "offset": 6066.8,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "get this kind of a result and um I'm",
      "offset": 6069.76,
      "duration": 5.359
    },
    {
      "lang": "en",
      "text": "printing some of the Shakespeare here",
      "offset": 6072.96,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "but what I did also is I printed 10,000",
      "offset": 6075.119,
      "duration": 3.801
    },
    {
      "lang": "en",
      "text": "characters so a lot more and I wrote",
      "offset": 6077.44,
      "duration": 3.88
    },
    {
      "lang": "en",
      "text": "them to a file and so here we see some",
      "offset": 6078.92,
      "duration": 5.4
    },
    {
      "lang": "en",
      "text": "of the outputs",
      "offset": 6081.32,
      "duration": 5.64
    },
    {
      "lang": "en",
      "text": "so it's a lot more recognizable as the",
      "offset": 6084.32,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "input text file so the input text file",
      "offset": 6086.96,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "just for reference looked like this so",
      "offset": 6089.119,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "there's always like someone speaking in",
      "offset": 6091.92,
      "duration": 5.799
    },
    {
      "lang": "en",
      "text": "this manner and uh our predictions now",
      "offset": 6093.599,
      "duration": 6.64
    },
    {
      "lang": "en",
      "text": "take on that form except of course",
      "offset": 6097.719,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "they're they're nonsensical when you",
      "offset": 6100.239,
      "duration": 3.4
    },
    {
      "lang": "en",
      "text": "actually read them",
      "offset": 6101.639,
      "duration": 6.121
    },
    {
      "lang": "en",
      "text": "so it is every crimp tap be a house oh",
      "offset": 6103.639,
      "duration": 5.161
    },
    {
      "lang": "en",
      "text": "those",
      "offset": 6107.76,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "prepation we give",
      "offset": 6108.8,
      "duration": 6.439
    },
    {
      "lang": "en",
      "text": "heed um you know",
      "offset": 6111.199,
      "duration": 4.04
    },
    {
      "lang": "en",
      "text": "Oho sent me you mighty",
      "offset": 6116.159,
      "duration": 3.56
    },
    {
      "lang": "en",
      "text": "Lord anyway so you can read through this",
      "offset": 6119.8,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "um it's nonsensical of course but this",
      "offset": 6122.36,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "is just a Transformer trained on a",
      "offset": 6124.679,
      "duration": 4.44
    },
    {
      "lang": "en",
      "text": "character level for 1 million characters",
      "offset": 6126.52,
      "duration": 4.04
    },
    {
      "lang": "en",
      "text": "that come from Shakespeare so there's",
      "offset": 6129.119,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "sort of like blabbers on in Shakespeare",
      "offset": 6130.56,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "like manner but it doesn't of course",
      "offset": 6132.719,
      "duration": 5.281
    },
    {
      "lang": "en",
      "text": "make sense at this scale uh but I think",
      "offset": 6134.719,
      "duration": 4.44
    },
    {
      "lang": "en",
      "text": "I think still a pretty good",
      "offset": 6138,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "demonstration of what's",
      "offset": 6139.159,
      "duration": 5.321
    },
    {
      "lang": "en",
      "text": "possible so now",
      "offset": 6140.8,
      "duration": 5.56
    },
    {
      "lang": "en",
      "text": "I think uh that kind of like concludes",
      "offset": 6144.48,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "the programming section of this video we",
      "offset": 6146.36,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "basically kind of uh did a pretty good",
      "offset": 6148.719,
      "duration": 4.121
    },
    {
      "lang": "en",
      "text": "job and um of implementing this",
      "offset": 6150.44,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "Transformer uh but the picture doesn't",
      "offset": 6152.84,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "exactly match up to what we've done so",
      "offset": 6155.32,
      "duration": 3.56
    },
    {
      "lang": "en",
      "text": "what's going on with all these digital",
      "offset": 6157.48,
      "duration": 3.88
    },
    {
      "lang": "en",
      "text": "Parts here so let me finish explaining",
      "offset": 6158.88,
      "duration": 4.2
    },
    {
      "lang": "en",
      "text": "this architecture and why it looks so",
      "offset": 6161.36,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "funky basically what's happening here is",
      "offset": 6163.08,
      "duration": 4.84
    },
    {
      "lang": "en",
      "text": "what we implemented here is a decoder",
      "offset": 6165.84,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "only Transformer so there's no component",
      "offset": 6167.92,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "here this part is called the encoder and",
      "offset": 6170.84,
      "duration": 4.92
    },
    {
      "lang": "en",
      "text": "there's no cross attention block here",
      "offset": 6172.92,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "our block only has a self attention and",
      "offset": 6175.76,
      "duration": 4.6
    },
    {
      "lang": "en",
      "text": "the feet forward so it is missing this",
      "offset": 6178.119,
      "duration": 5.161
    },
    {
      "lang": "en",
      "text": "third in between piece here this piece",
      "offset": 6180.36,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "does cross attention so we don't have it",
      "offset": 6183.28,
      "duration": 3.8
    },
    {
      "lang": "en",
      "text": "and we don't have the encoder we just",
      "offset": 6185.48,
      "duration": 3.4
    },
    {
      "lang": "en",
      "text": "have the decoder and the reason we have",
      "offset": 6187.08,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "a decoder only uh is because we are just",
      "offset": 6188.88,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "uh generating text and it's",
      "offset": 6192.119,
      "duration": 3.08
    },
    {
      "lang": "en",
      "text": "unconditioned on anything we're just",
      "offset": 6193.44,
      "duration": 3.4
    },
    {
      "lang": "en",
      "text": "we're just blabbering on according to a",
      "offset": 6195.199,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "given data set what makes it a decoder",
      "offset": 6196.84,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "is that we are using the Triangular mask",
      "offset": 6199.679,
      "duration": 4.681
    },
    {
      "lang": "en",
      "text": "in our uh trans former so it has this",
      "offset": 6201.96,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "Auto regressive property where we can",
      "offset": 6204.36,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "just uh go and sample from it so the",
      "offset": 6206.199,
      "duration": 4.04
    },
    {
      "lang": "en",
      "text": "fact that it's using the Triangular",
      "offset": 6208.92,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "triangular mask to mask out the",
      "offset": 6210.239,
      "duration": 4.521
    },
    {
      "lang": "en",
      "text": "attention makes it a decoder and it can",
      "offset": 6212.44,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "be used for language modeling now the",
      "offset": 6214.76,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "reason that the original paper had an",
      "offset": 6217.4,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "incoder decoder architecture is because",
      "offset": 6219.08,
      "duration": 4.36
    },
    {
      "lang": "en",
      "text": "it is a machine translation paper so it",
      "offset": 6221.32,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "is concerned with a different setting in",
      "offset": 6223.44,
      "duration": 6.239
    },
    {
      "lang": "en",
      "text": "particular it expects some uh tokens",
      "offset": 6225.88,
      "duration": 6.359
    },
    {
      "lang": "en",
      "text": "that encode say for example French and",
      "offset": 6229.679,
      "duration": 4.361
    },
    {
      "lang": "en",
      "text": "then it is expecting to decode the",
      "offset": 6232.239,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "translation in English so so you",
      "offset": 6234.04,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "typically these here are special tokens",
      "offset": 6236.8,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "so you are expected to read in this and",
      "offset": 6239.48,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "condition on it and then you start off",
      "offset": 6242.08,
      "duration": 3.639
    },
    {
      "lang": "en",
      "text": "the generation with a special token",
      "offset": 6244.199,
      "duration": 3.801
    },
    {
      "lang": "en",
      "text": "called start so this is a special new",
      "offset": 6245.719,
      "duration": 5.121
    },
    {
      "lang": "en",
      "text": "token um that you introduce and always",
      "offset": 6248,
      "duration": 4.92
    },
    {
      "lang": "en",
      "text": "place in the beginning and then the",
      "offset": 6250.84,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "network is expected to Output neural",
      "offset": 6252.92,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "networks are awesome and then a special",
      "offset": 6255.719,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "end token to finish the",
      "offset": 6257.88,
      "duration": 5.279
    },
    {
      "lang": "en",
      "text": "generation so this part here will be",
      "offset": 6260.119,
      "duration": 5.401
    },
    {
      "lang": "en",
      "text": "decoded exactly as we we've done it",
      "offset": 6263.159,
      "duration": 4.04
    },
    {
      "lang": "en",
      "text": "neural networks are awesome will be",
      "offset": 6265.52,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "identical to what we did but unlike what",
      "offset": 6267.199,
      "duration": 5.4
    },
    {
      "lang": "en",
      "text": "we did they wanton to condition the",
      "offset": 6269.92,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "generation on some additional",
      "offset": 6272.599,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "information and in that case this",
      "offset": 6274.92,
      "duration": 3.319
    },
    {
      "lang": "en",
      "text": "additional information is the French",
      "offset": 6276.679,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "sentence that they should be",
      "offset": 6278.239,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "translating so what they do now is they",
      "offset": 6279.679,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "bring in the encoder now the encoder",
      "offset": 6282.88,
      "duration": 5.64
    },
    {
      "lang": "en",
      "text": "reads this part here so we're only going",
      "offset": 6285.599,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "to take the part of French and we're",
      "offset": 6288.52,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "going to uh create tokens from it",
      "offset": 6290.56,
      "duration": 4.2
    },
    {
      "lang": "en",
      "text": "exactly as we've seen in our video and",
      "offset": 6292.76,
      "duration": 4.52
    },
    {
      "lang": "en",
      "text": "we're going to put a Transformer on it",
      "offset": 6294.76,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "but there's going to be no triangular",
      "offset": 6297.28,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "mask and so all the tokens are allowed",
      "offset": 6298.92,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "to talk to each other as much as they",
      "offset": 6300.719,
      "duration": 3.641
    },
    {
      "lang": "en",
      "text": "want and they're just encoding",
      "offset": 6302.119,
      "duration": 5.161
    },
    {
      "lang": "en",
      "text": "whatever's the content of this French uh",
      "offset": 6304.36,
      "duration": 6.6
    },
    {
      "lang": "en",
      "text": "sentence once they've encoded it they",
      "offset": 6307.28,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "they basically come out in the top here",
      "offset": 6310.96,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "and then what happens here is in our",
      "offset": 6313.44,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "decoder which does the uh language",
      "offset": 6314.96,
      "duration": 5.08
    },
    {
      "lang": "en",
      "text": "modeling there's an additional",
      "offset": 6317.76,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "connection here to the outputs of the",
      "offset": 6320.04,
      "duration": 3.48
    },
    {
      "lang": "en",
      "text": "encoder",
      "offset": 6322,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "and that is brought in through a cross",
      "offset": 6323.52,
      "duration": 4.52
    },
    {
      "lang": "en",
      "text": "attention so the queries are still",
      "offset": 6326,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "generated from X but now the keys and",
      "offset": 6328.04,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "the values are coming from the side the",
      "offset": 6330.239,
      "duration": 4.041
    },
    {
      "lang": "en",
      "text": "keys and the values are coming from the",
      "offset": 6332.52,
      "duration": 4.36
    },
    {
      "lang": "en",
      "text": "top generated by the nodes that came",
      "offset": 6334.28,
      "duration": 5.72
    },
    {
      "lang": "en",
      "text": "outside of the de the encoder and those",
      "offset": 6336.88,
      "duration": 5.319
    },
    {
      "lang": "en",
      "text": "tops the keys and the values there the",
      "offset": 6340,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "top of it feed in on a side into every",
      "offset": 6342.199,
      "duration": 5.561
    },
    {
      "lang": "en",
      "text": "single block of the decoder and so",
      "offset": 6345.36,
      "duration": 3.879
    },
    {
      "lang": "en",
      "text": "that's why there's an additional cross",
      "offset": 6347.76,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "attention and really what it's doing is",
      "offset": 6349.239,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "it's conditioning the decoding",
      "offset": 6351.36,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "not just on the past of this current",
      "offset": 6353.639,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "decoding but also on having seen the",
      "offset": 6355.92,
      "duration": 8.12
    },
    {
      "lang": "en",
      "text": "full fully encoded French um prompt sort",
      "offset": 6359.639,
      "duration": 6.96
    },
    {
      "lang": "en",
      "text": "of and so it's an encoder decoder model",
      "offset": 6364.04,
      "duration": 3.4
    },
    {
      "lang": "en",
      "text": "which is why we have those two",
      "offset": 6366.599,
      "duration": 3.241
    },
    {
      "lang": "en",
      "text": "Transformers an additional block and so",
      "offset": 6367.44,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "on so we did not do this because we have",
      "offset": 6369.84,
      "duration": 4.04
    },
    {
      "lang": "en",
      "text": "no we have nothing to encode there's no",
      "offset": 6372,
      "duration": 3.48
    },
    {
      "lang": "en",
      "text": "conditioning we just have a text file",
      "offset": 6373.88,
      "duration": 3.08
    },
    {
      "lang": "en",
      "text": "and we just want to imitate it and",
      "offset": 6375.48,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "that's why we are using a decoder only",
      "offset": 6376.96,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "Transformer exactly as done in",
      "offset": 6379,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "GPT okay okay so now I wanted to do a",
      "offset": 6381.76,
      "duration": 4.359
    },
    {
      "lang": "en",
      "text": "very brief walkthrough of nanog GPT",
      "offset": 6384.04,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "which you can find in my GitHub and uh",
      "offset": 6386.119,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "nanog GPT is basically two files of",
      "offset": 6388.599,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "Interest there's train.py and model.py",
      "offset": 6390.44,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "train.py is all the boilerplate code for",
      "offset": 6393.719,
      "duration": 4.52
    },
    {
      "lang": "en",
      "text": "training the network it is basically all",
      "offset": 6395.88,
      "duration": 4.44
    },
    {
      "lang": "en",
      "text": "the stuff that we had here it's the",
      "offset": 6398.239,
      "duration": 4.521
    },
    {
      "lang": "en",
      "text": "training loop it's just that it's a lot",
      "offset": 6400.32,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "more complicated because we're saving",
      "offset": 6402.76,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "and loading checkpoints and pre-trained",
      "offset": 6404.48,
      "duration": 3.8
    },
    {
      "lang": "en",
      "text": "weights and we are uh decaying the",
      "offset": 6406.119,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "learning rate and compiling the model",
      "offset": 6408.28,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "and using distributed training across",
      "offset": 6410.119,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "multiple nodes or GP use so the training",
      "offset": 6411.639,
      "duration": 4.681
    },
    {
      "lang": "en",
      "text": "Pi gets a little bit more hairy",
      "offset": 6414.52,
      "duration": 5.159
    },
    {
      "lang": "en",
      "text": "complicated uh there's more options Etc",
      "offset": 6416.32,
      "duration": 5.64
    },
    {
      "lang": "en",
      "text": "but the model.py should look very very",
      "offset": 6419.679,
      "duration": 4.681
    },
    {
      "lang": "en",
      "text": "um similar to what we've done here in",
      "offset": 6421.96,
      "duration": 6.12
    },
    {
      "lang": "en",
      "text": "fact the model is is almost identical so",
      "offset": 6424.36,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "first here we have the causal self",
      "offset": 6428.08,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "attention block and all of this should",
      "offset": 6429.8,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "look very very recognizable to you we're",
      "offset": 6431.92,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "producing queries Keys values we're",
      "offset": 6433.719,
      "duration": 4.681
    },
    {
      "lang": "en",
      "text": "doing Dot products we're masking",
      "offset": 6436.159,
      "duration": 4.44
    },
    {
      "lang": "en",
      "text": "applying soft Maxs optionally dropping",
      "offset": 6438.4,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "out and here we are pulling the wi the",
      "offset": 6440.599,
      "duration": 5.361
    },
    {
      "lang": "en",
      "text": "values what is different here is that in",
      "offset": 6443.36,
      "duration": 6.68
    },
    {
      "lang": "en",
      "text": "our code I have separated out the",
      "offset": 6445.96,
      "duration": 5.88
    },
    {
      "lang": "en",
      "text": "multi-headed detention into just a",
      "offset": 6450.04,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "single individual head and then here I",
      "offset": 6451.84,
      "duration": 4.359
    },
    {
      "lang": "en",
      "text": "have multiple heads and I explicitly",
      "offset": 6454.599,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "concatenate them whereas here uh all of",
      "offset": 6456.199,
      "duration": 5.121
    },
    {
      "lang": "en",
      "text": "it is implemented in a batched manner",
      "offset": 6459.48,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "inside a single causal self attention",
      "offset": 6461.32,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "and so we don't just have a b and a T",
      "offset": 6463.88,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "and A C Dimension we also end up with a",
      "offset": 6465.4,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "fourth dimension which is the heads and",
      "offset": 6467.56,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "so it just gets a lot more sort of hairy",
      "offset": 6470.119,
      "duration": 4.04
    },
    {
      "lang": "en",
      "text": "because we have four dimensional array",
      "offset": 6472.36,
      "duration": 5.08
    },
    {
      "lang": "en",
      "text": "um tensors now but it is um equivalent",
      "offset": 6474.159,
      "duration": 5.08
    },
    {
      "lang": "en",
      "text": "mathematically so the exact same thing",
      "offset": 6477.44,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "is happening as what we have it's just",
      "offset": 6479.239,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "it's a bit more efficient because all",
      "offset": 6481.44,
      "duration": 3.08
    },
    {
      "lang": "en",
      "text": "the heads are now treated as a batch",
      "offset": 6482.84,
      "duration": 2.879
    },
    {
      "lang": "en",
      "text": "Dimension as",
      "offset": 6484.52,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "well then we have the multier perceptron",
      "offset": 6485.719,
      "duration": 4.841
    },
    {
      "lang": "en",
      "text": "it's using the Galu nonlinearity which",
      "offset": 6488.199,
      "duration": 5.241
    },
    {
      "lang": "en",
      "text": "is defined here except instead of Ru and",
      "offset": 6490.56,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "this is done just because opening I used",
      "offset": 6493.44,
      "duration": 3.64
    },
    {
      "lang": "en",
      "text": "it and I want to be able to load their",
      "offset": 6494.96,
      "duration": 4.04
    },
    {
      "lang": "en",
      "text": "checkpoints uh the blocks of the",
      "offset": 6497.08,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "Transformer are identical to communicate",
      "offset": 6499,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "in the compute phase as we saw and then",
      "offset": 6501.08,
      "duration": 4.2
    },
    {
      "lang": "en",
      "text": "the GPT will be identical we have the",
      "offset": 6503.48,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "position encodings token encodings the",
      "offset": 6505.28,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "blocks the layer Norm at the end uh the",
      "offset": 6507.719,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "final linear layer and this should look",
      "offset": 6510.32,
      "duration": 5.279
    },
    {
      "lang": "en",
      "text": "all very recognizable and there's a bit",
      "offset": 6513.239,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "more here because I'm loading",
      "offset": 6515.599,
      "duration": 3.241
    },
    {
      "lang": "en",
      "text": "checkpoints and stuff like that I'm",
      "offset": 6516.84,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "separating out the parameters into those",
      "offset": 6518.84,
      "duration": 3.16
    },
    {
      "lang": "en",
      "text": "that should be weight decayed and those",
      "offset": 6520.599,
      "duration": 2.161
    },
    {
      "lang": "en",
      "text": "that",
      "offset": 6522,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "shouldn't um but the generate function",
      "offset": 6522.76,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "should also be very very similar so a",
      "offset": 6524.96,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "few details are different but you should",
      "offset": 6527.159,
      "duration": 4.121
    },
    {
      "lang": "en",
      "text": "definitely be able to look at this uh",
      "offset": 6528.8,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "file and be able to understand little",
      "offset": 6531.28,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "the pieces now so let's now bring things",
      "offset": 6532.88,
      "duration": 4.759
    },
    {
      "lang": "en",
      "text": "back to chat GPT what would it look like",
      "offset": 6535.04,
      "duration": 4.52
    },
    {
      "lang": "en",
      "text": "if we wanted to train chat GPT ourselves",
      "offset": 6537.639,
      "duration": 3.161
    },
    {
      "lang": "en",
      "text": "and how does it relate to what we",
      "offset": 6539.56,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "learned today well to train in chat GPT",
      "offset": 6540.8,
      "duration": 4.839
    },
    {
      "lang": "en",
      "text": "there are roughly two stages first is",
      "offset": 6543.56,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "the pre-training stage and then the",
      "offset": 6545.639,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "fine-tuning stage in the pre-training",
      "offset": 6547.48,
      "duration": 4.759
    },
    {
      "lang": "en",
      "text": "stage uh we are training on a large",
      "offset": 6549.88,
      "duration": 4.839
    },
    {
      "lang": "en",
      "text": "chunk of internet and just trying to get",
      "offset": 6552.239,
      "duration": 5.041
    },
    {
      "lang": "en",
      "text": "a first decoder only Transformer to",
      "offset": 6554.719,
      "duration": 5.601
    },
    {
      "lang": "en",
      "text": "babble text so it's very very similar to",
      "offset": 6557.28,
      "duration": 5.8
    },
    {
      "lang": "en",
      "text": "what we've done ourselves except we've",
      "offset": 6560.32,
      "duration": 4.359
    },
    {
      "lang": "en",
      "text": "done like a tiny little baby",
      "offset": 6563.08,
      "duration": 5.8
    },
    {
      "lang": "en",
      "text": "pre-training step um and so in our case",
      "offset": 6564.679,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "uh this is how you print a number of",
      "offset": 6568.88,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "parameters I printed it and it's about",
      "offset": 6570.679,
      "duration": 4.601
    },
    {
      "lang": "en",
      "text": "10 million so this Transformer that I",
      "offset": 6572.719,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "created here to create little",
      "offset": 6575.28,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "Shakespeare um Transformer was about 10",
      "offset": 6577.04,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "million parameters our data set is",
      "offset": 6580.32,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "roughly 1 million uh characters so",
      "offset": 6582.96,
      "duration": 4.44
    },
    {
      "lang": "en",
      "text": "roughly 1 million tokens but you have to",
      "offset": 6585.199,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "remember that opening I is different",
      "offset": 6587.4,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "vocabulary they're not on the Character",
      "offset": 6588.88,
      "duration": 4.759
    },
    {
      "lang": "en",
      "text": "level they use these um subword chunks",
      "offset": 6590.44,
      "duration": 5.159
    },
    {
      "lang": "en",
      "text": "of words and so they have a vocabulary",
      "offset": 6593.639,
      "duration": 4.761
    },
    {
      "lang": "en",
      "text": "of 50,000 roughly elements and so their",
      "offset": 6595.599,
      "duration": 5.961
    },
    {
      "lang": "en",
      "text": "sequences are a bit more condensed so",
      "offset": 6598.4,
      "duration": 4.92
    },
    {
      "lang": "en",
      "text": "our data set the Shakespeare data set",
      "offset": 6601.56,
      "duration": 3.96
    },
    {
      "lang": "en",
      "text": "would be probably around 300,000 uh",
      "offset": 6603.32,
      "duration": 5.96
    },
    {
      "lang": "en",
      "text": "tokens in the open AI vocabulary roughly",
      "offset": 6605.52,
      "duration": 6.04
    },
    {
      "lang": "en",
      "text": "so we trained about 10 million parameter",
      "offset": 6609.28,
      "duration": 5.24
    },
    {
      "lang": "en",
      "text": "model on roughly 300,000 tokens now when",
      "offset": 6611.56,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "you go to the gpt3",
      "offset": 6614.52,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "paper and you look at the Transformers",
      "offset": 6616.76,
      "duration": 5.56
    },
    {
      "lang": "en",
      "text": "that they trained they trained a number",
      "offset": 6620.04,
      "duration": 4.44
    },
    {
      "lang": "en",
      "text": "of trans Transformers of different sizes",
      "offset": 6622.32,
      "duration": 4.68
    },
    {
      "lang": "en",
      "text": "but the biggest Transformer here has 175",
      "offset": 6624.48,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "billion parameters uh so ours is again",
      "offset": 6627,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "10 million they used this number of",
      "offset": 6629.36,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "layers in the Transformer this is the",
      "offset": 6631.8,
      "duration": 5.16
    },
    {
      "lang": "en",
      "text": "nmed this is the number of heads and",
      "offset": 6634.239,
      "duration": 5.521
    },
    {
      "lang": "en",
      "text": "this is the head size and then this is",
      "offset": 6636.96,
      "duration": 6.199
    },
    {
      "lang": "en",
      "text": "the batch size uh so ours was",
      "offset": 6639.76,
      "duration": 6.399
    },
    {
      "lang": "en",
      "text": "65 and the learning rate is similar now",
      "offset": 6643.159,
      "duration": 4.761
    },
    {
      "lang": "en",
      "text": "when they train this Transformer they",
      "offset": 6646.159,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "trained on 300 billion tokens so again",
      "offset": 6647.92,
      "duration": 5.239
    },
    {
      "lang": "en",
      "text": "remember ours is about 300,000",
      "offset": 6651.159,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "so this is uh about a millionfold",
      "offset": 6653.159,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "increase and this number would not be",
      "offset": 6656.04,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "even that large by today's standards",
      "offset": 6657.88,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "you'd be going up uh 1 trillion and",
      "offset": 6659.32,
      "duration": 5.359
    },
    {
      "lang": "en",
      "text": "above so they are training a",
      "offset": 6661.719,
      "duration": 4.681
    },
    {
      "lang": "en",
      "text": "significantly larger",
      "offset": 6664.679,
      "duration": 5.321
    },
    {
      "lang": "en",
      "text": "model on uh a good chunk of the internet",
      "offset": 6666.4,
      "duration": 5.679
    },
    {
      "lang": "en",
      "text": "and that is the pre-training stage but",
      "offset": 6670,
      "duration": 3.639
    },
    {
      "lang": "en",
      "text": "otherwise these hyper parameters should",
      "offset": 6672.079,
      "duration": 3.321
    },
    {
      "lang": "en",
      "text": "be fairly recognizable to you and the",
      "offset": 6673.639,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "architecture is actually like nearly",
      "offset": 6675.4,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "identical to what we implemented",
      "offset": 6677.159,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "ourselves but of course it's a massive",
      "offset": 6678.4,
      "duration": 4.52
    },
    {
      "lang": "en",
      "text": "infrastructure challenge to train this",
      "offset": 6680.52,
      "duration": 3.96
    },
    {
      "lang": "en",
      "text": "you're talking about typically thousands",
      "offset": 6682.92,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "of gpus having to you know talk to each",
      "offset": 6684.48,
      "duration": 5.48
    },
    {
      "lang": "en",
      "text": "other to train models of this size so",
      "offset": 6687.159,
      "duration": 4.841
    },
    {
      "lang": "en",
      "text": "that's just a pre-training stage now",
      "offset": 6689.96,
      "duration": 3.32
    },
    {
      "lang": "en",
      "text": "after you complete the pre-training",
      "offset": 6692,
      "duration": 3.96
    },
    {
      "lang": "en",
      "text": "stage uh you don't get something that",
      "offset": 6693.28,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "responds to your questions with answers",
      "offset": 6695.96,
      "duration": 4.679
    },
    {
      "lang": "en",
      "text": "and is not helpful and Etc you get a",
      "offset": 6698,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "document",
      "offset": 6700.639,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "completer right so it babbles but it",
      "offset": 6701.599,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "doesn't Babble Shakespeare it babbles",
      "offset": 6704.56,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "internet it will create arbitrary news",
      "offset": 6706.239,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "articles and documents and it will try",
      "offset": 6708.239,
      "duration": 3.281
    },
    {
      "lang": "en",
      "text": "to complete documents because that's",
      "offset": 6710,
      "duration": 2.679
    },
    {
      "lang": "en",
      "text": "what it's trained for it's trying to",
      "offset": 6711.52,
      "duration": 3.079
    },
    {
      "lang": "en",
      "text": "complete the sequence so when you give",
      "offset": 6712.679,
      "duration": 3.721
    },
    {
      "lang": "en",
      "text": "it a question it would just uh",
      "offset": 6714.599,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "potentially just give you more questions",
      "offset": 6716.4,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "it would follow with more questions it",
      "offset": 6718.76,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "will do whatever it looks like the some",
      "offset": 6720.4,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "close document would do in the training",
      "offset": 6722.679,
      "duration": 4.52
    },
    {
      "lang": "en",
      "text": "data on the internet and so who knows",
      "offset": 6725.28,
      "duration": 3.16
    },
    {
      "lang": "en",
      "text": "you're getting kind of like undefined",
      "offset": 6727.199,
      "duration": 3.96
    },
    {
      "lang": "en",
      "text": "Behavior it might basically answer with",
      "offset": 6728.44,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "to questions with other questions it",
      "offset": 6731.159,
      "duration": 3.881
    },
    {
      "lang": "en",
      "text": "might ignore your question it might just",
      "offset": 6733.159,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "try to complete some news article it's",
      "offset": 6735.04,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "totally unineed as we say so the second",
      "offset": 6737.079,
      "duration": 5.241
    },
    {
      "lang": "en",
      "text": "fine-tuning stage is to actually align",
      "offset": 6740.56,
      "duration": 4.76
    },
    {
      "lang": "en",
      "text": "it to be an assistant and uh this is the",
      "offset": 6742.32,
      "duration": 6.279
    },
    {
      "lang": "en",
      "text": "second stage and so this chat GPT block",
      "offset": 6745.32,
      "duration": 5.08
    },
    {
      "lang": "en",
      "text": "post from openi talks a little bit about",
      "offset": 6748.599,
      "duration": 5.64
    },
    {
      "lang": "en",
      "text": "how the stage is achieved we basically",
      "offset": 6750.4,
      "duration": 6.36
    },
    {
      "lang": "en",
      "text": "um there's roughly three steps to to",
      "offset": 6754.239,
      "duration": 4.761
    },
    {
      "lang": "en",
      "text": "this stage uh so what they do here is",
      "offset": 6756.76,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "they start to collect training data that",
      "offset": 6759,
      "duration": 3.88
    },
    {
      "lang": "en",
      "text": "looks specifically like what an",
      "offset": 6761.159,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "assistant would do so these are",
      "offset": 6762.88,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "documents that have to format where the",
      "offset": 6764.52,
      "duration": 3.32
    },
    {
      "lang": "en",
      "text": "question is on top and then an answer is",
      "offset": 6766,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "below and they have a large number of",
      "offset": 6767.84,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "these but probably not on the order of",
      "offset": 6770.159,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "the internet uh this is probably on the",
      "offset": 6771.92,
      "duration": 6.199
    },
    {
      "lang": "en",
      "text": "of maybe thousands of examples and so",
      "offset": 6773.84,
      "duration": 6.879
    },
    {
      "lang": "en",
      "text": "they they then fine-tune the model to",
      "offset": 6778.119,
      "duration": 5.321
    },
    {
      "lang": "en",
      "text": "basically only focus on documents that",
      "offset": 6780.719,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "look like that and so you're starting to",
      "offset": 6783.44,
      "duration": 3.96
    },
    {
      "lang": "en",
      "text": "slowly align it so it's going to expect",
      "offset": 6785.199,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "a question at the top and it's going to",
      "offset": 6787.4,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "expect to complete the answer and uh",
      "offset": 6788.8,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "these very very large models are very",
      "offset": 6791.56,
      "duration": 3.4
    },
    {
      "lang": "en",
      "text": "sample efficient during their",
      "offset": 6793.599,
      "duration": 3.241
    },
    {
      "lang": "en",
      "text": "fine-tuning so this actually somehow",
      "offset": 6794.96,
      "duration": 4.119
    },
    {
      "lang": "en",
      "text": "works but that's just step one that's",
      "offset": 6796.84,
      "duration": 4.12
    },
    {
      "lang": "en",
      "text": "just fine tuning so then they actually",
      "offset": 6799.079,
      "duration": 4.12
    },
    {
      "lang": "en",
      "text": "have more steps where okay the second",
      "offset": 6800.96,
      "duration": 4.52
    },
    {
      "lang": "en",
      "text": "step is you let the model respond and",
      "offset": 6803.199,
      "duration": 3.801
    },
    {
      "lang": "en",
      "text": "then different Raiders look at the",
      "offset": 6805.48,
      "duration": 3.719
    },
    {
      "lang": "en",
      "text": "different responses and rank them for",
      "offset": 6807,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "their preference as to which one is",
      "offset": 6809.199,
      "duration": 3.321
    },
    {
      "lang": "en",
      "text": "better than the other they use that to",
      "offset": 6810.679,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "train a reward model so they can predict",
      "offset": 6812.52,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "uh basically using a different network",
      "offset": 6815.079,
      "duration": 4.841
    },
    {
      "lang": "en",
      "text": "how much of any candidate",
      "offset": 6817.079,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "response would be desirable and then",
      "offset": 6819.92,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "once they have a reward model they run",
      "offset": 6823.159,
      "duration": 4.361
    },
    {
      "lang": "en",
      "text": "po which is a form of polic policy",
      "offset": 6825.04,
      "duration": 4.599
    },
    {
      "lang": "en",
      "text": "gradient um reinforcement learning",
      "offset": 6827.52,
      "duration": 5.48
    },
    {
      "lang": "en",
      "text": "Optimizer to uh fine-tune this sampling",
      "offset": 6829.639,
      "duration": 6.121
    },
    {
      "lang": "en",
      "text": "policy uh so that the answers that the",
      "offset": 6833,
      "duration": 6.119
    },
    {
      "lang": "en",
      "text": "GP chat GPT now generates are expected",
      "offset": 6835.76,
      "duration": 6.399
    },
    {
      "lang": "en",
      "text": "to score a high reward according to the",
      "offset": 6839.119,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "reward model and so basically there's a",
      "offset": 6842.159,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "whole aligning stage here or fine-tuning",
      "offset": 6844.639,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "stage it's got multiple steps in between",
      "offset": 6847.119,
      "duration": 4.681
    },
    {
      "lang": "en",
      "text": "there as well and it takes the model",
      "offset": 6849.199,
      "duration": 5.281
    },
    {
      "lang": "en",
      "text": "from being a document completer to a",
      "offset": 6851.8,
      "duration": 5.16
    },
    {
      "lang": "en",
      "text": "question answerer and that's like a",
      "offset": 6854.48,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "whole separate stage a lot of this data",
      "offset": 6856.96,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "is not available publicly it is internal",
      "offset": 6859.199,
      "duration": 4.841
    },
    {
      "lang": "en",
      "text": "to open AI and uh it's much harder to",
      "offset": 6861.44,
      "duration": 5.88
    },
    {
      "lang": "en",
      "text": "replicate this stage um and so that's",
      "offset": 6864.04,
      "duration": 5.559
    },
    {
      "lang": "en",
      "text": "roughly what would give you a chat GPT",
      "offset": 6867.32,
      "duration": 3.879
    },
    {
      "lang": "en",
      "text": "and nanog GPT focuses on the",
      "offset": 6869.599,
      "duration": 3.321
    },
    {
      "lang": "en",
      "text": "pre-training stage okay and that's",
      "offset": 6871.199,
      "duration": 3.801
    },
    {
      "lang": "en",
      "text": "everything that I wanted to cover today",
      "offset": 6872.92,
      "duration": 5.759
    },
    {
      "lang": "en",
      "text": "so we trained to summarize a decoder",
      "offset": 6875,
      "duration": 6.8
    },
    {
      "lang": "en",
      "text": "only Transformer following this famous",
      "offset": 6878.679,
      "duration": 5.241
    },
    {
      "lang": "en",
      "text": "paper attention is all you need from",
      "offset": 6881.8,
      "duration": 5.48
    },
    {
      "lang": "en",
      "text": "2017 and so that's basically a GPT we",
      "offset": 6883.92,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "trained it on Tiny Shakespeare and got",
      "offset": 6887.28,
      "duration": 5.48
    },
    {
      "lang": "en",
      "text": "sensible results",
      "offset": 6890.4,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "all of the training code is",
      "offset": 6892.76,
      "duration": 4.839
    },
    {
      "lang": "en",
      "text": "roughly 200 lines of code I will be",
      "offset": 6894.639,
      "duration": 7.121
    },
    {
      "lang": "en",
      "text": "releasing this um code base so also it",
      "offset": 6897.599,
      "duration": 6.6
    },
    {
      "lang": "en",
      "text": "comes with all the git log commits along",
      "offset": 6901.76,
      "duration": 4.04
    },
    {
      "lang": "en",
      "text": "the way as we built it",
      "offset": 6904.199,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "up in addition to this code I'm going to",
      "offset": 6905.8,
      "duration": 5.16
    },
    {
      "lang": "en",
      "text": "release the um notebook of course the",
      "offset": 6908.119,
      "duration": 5.361
    },
    {
      "lang": "en",
      "text": "Google collab and I hope that gave you a",
      "offset": 6910.96,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "sense for how you can train um these",
      "offset": 6913.48,
      "duration": 5.8
    },
    {
      "lang": "en",
      "text": "models like say gpt3 that will be um",
      "offset": 6916.48,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "architecturally basically identical to",
      "offset": 6919.28,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "what we have but they are somewhere",
      "offset": 6920.96,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "between 10,000 and 1 million times",
      "offset": 6922.48,
      "duration": 5.119
    },
    {
      "lang": "en",
      "text": "bigger depending on how you count and so",
      "offset": 6924.4,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "uh that's all I have for now uh we did",
      "offset": 6927.599,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "not talk about any of the fine-tuning",
      "offset": 6930.56,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "stages that would typically go on top of",
      "offset": 6932,
      "duration": 3.48
    },
    {
      "lang": "en",
      "text": "this so if you're interested in",
      "offset": 6933.92,
      "duration": 2.64
    },
    {
      "lang": "en",
      "text": "something that's not just language",
      "offset": 6935.48,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "modeling but you actually want to you",
      "offset": 6936.56,
      "duration": 4.28
    },
    {
      "lang": "en",
      "text": "know say perform tasks um or you want",
      "offset": 6938.28,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "them to be aligned in a specific way or",
      "offset": 6940.84,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "you want um to detect sentiment or",
      "offset": 6943.159,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "anything like that basically anytime you",
      "offset": 6945.56,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "don't want something that's just a",
      "offset": 6947.239,
      "duration": 3.4
    },
    {
      "lang": "en",
      "text": "document completer you have to complete",
      "offset": 6948.52,
      "duration": 4.04
    },
    {
      "lang": "en",
      "text": "further stages of fine tuning which did",
      "offset": 6950.639,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "not cover uh and that could be simple",
      "offset": 6952.56,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "supervised fine tuning or it can be",
      "offset": 6955.199,
      "duration": 3.161
    },
    {
      "lang": "en",
      "text": "something more fancy like we see in chat",
      "offset": 6957.04,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "jpt where we actually train a reward",
      "offset": 6958.36,
      "duration": 4.68
    },
    {
      "lang": "en",
      "text": "model and then do rounds of Po to uh",
      "offset": 6960.48,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "align it with respect to the reward",
      "offset": 6963.04,
      "duration": 3.32
    },
    {
      "lang": "en",
      "text": "model so there's a lot more that can be",
      "offset": 6964.4,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "done on top of it I think for now we're",
      "offset": 6966.36,
      "duration": 3.879
    },
    {
      "lang": "en",
      "text": "starting to get to about two hours Mark",
      "offset": 6968.32,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "uh so I'm going to um kind of finish",
      "offset": 6970.239,
      "duration": 5.681
    },
    {
      "lang": "en",
      "text": "here uh I hope you enjoyed the lecture",
      "offset": 6973.28,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "uh and uh yeah go forth and transform",
      "offset": 6975.92,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "see you later",
      "offset": 6978.48,
      "duration": 3.28
    }
  ],
  "cleanText": null,
  "dumpedAt": "2025-07-21T18:43:24.675Z"
}