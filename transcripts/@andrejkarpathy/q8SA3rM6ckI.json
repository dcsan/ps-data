{
  "episodeId": "q8SA3rM6ckI",
  "channelSlug": "@andrejkarpathy",
  "title": "Building makemore Part 4: Becoming a Backprop Ninja",
  "publishedAt": "2022-10-11T17:56:19.000Z",
  "rawLines": [
    {
      "lang": "en",
      "text": "hi everyone so today we are once again",
      "offset": 0,
      "duration": 4.2
    },
    {
      "lang": "en",
      "text": "continuing our implementation of make",
      "offset": 2.399,
      "duration": 4.621
    },
    {
      "lang": "en",
      "text": "more now so far we've come up to here",
      "offset": 4.2,
      "duration": 5.64
    },
    {
      "lang": "en",
      "text": "montalia perceptrons and our neural net",
      "offset": 7.02,
      "duration": 4.38
    },
    {
      "lang": "en",
      "text": "looked like this and we were",
      "offset": 9.84,
      "duration": 2.64
    },
    {
      "lang": "en",
      "text": "implementing this over the last few",
      "offset": 11.4,
      "duration": 1.98
    },
    {
      "lang": "en",
      "text": "lectures",
      "offset": 12.48,
      "duration": 2.76
    },
    {
      "lang": "en",
      "text": "now I'm sure everyone is very excited to",
      "offset": 13.38,
      "duration": 3.3
    },
    {
      "lang": "en",
      "text": "go into recurring neural networks and",
      "offset": 15.24,
      "duration": 3.119
    },
    {
      "lang": "en",
      "text": "all of their variants and how they work",
      "offset": 16.68,
      "duration": 3.66
    },
    {
      "lang": "en",
      "text": "and the diagrams look cool and it's very",
      "offset": 18.359,
      "duration": 3.18
    },
    {
      "lang": "en",
      "text": "exciting and interesting and we're going",
      "offset": 20.34,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "to get a better result but unfortunately",
      "offset": 21.539,
      "duration": 3.541
    },
    {
      "lang": "en",
      "text": "I think we have to remain here for one",
      "offset": 23.46,
      "duration": 5.159
    },
    {
      "lang": "en",
      "text": "more lecture and the reason for that is",
      "offset": 25.08,
      "duration": 5.459
    },
    {
      "lang": "en",
      "text": "we've already trained this multilio",
      "offset": 28.619,
      "duration": 3.24
    },
    {
      "lang": "en",
      "text": "perceptron right and we are getting",
      "offset": 30.539,
      "duration": 3.061
    },
    {
      "lang": "en",
      "text": "pretty good loss and I think we have a",
      "offset": 31.859,
      "duration": 2.941
    },
    {
      "lang": "en",
      "text": "pretty decent understanding of the",
      "offset": 33.6,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "architecture and how it works but the",
      "offset": 34.8,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "line of code here that I take an issue",
      "offset": 37.68,
      "duration": 4.62
    },
    {
      "lang": "en",
      "text": "with is here lost up backward that is we",
      "offset": 39.12,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "are taking a pytorch auto grad and using",
      "offset": 42.3,
      "duration": 4.62
    },
    {
      "lang": "en",
      "text": "it to calculate all of our gradients",
      "offset": 45.12,
      "duration": 3.779
    },
    {
      "lang": "en",
      "text": "along the way and I would like to remove",
      "offset": 46.92,
      "duration": 3.959
    },
    {
      "lang": "en",
      "text": "the use of lost at backward and I would",
      "offset": 48.899,
      "duration": 3.301
    },
    {
      "lang": "en",
      "text": "like us to write our backward pass",
      "offset": 50.879,
      "duration": 4.261
    },
    {
      "lang": "en",
      "text": "manually on the level of tensors and I",
      "offset": 52.2,
      "duration": 4.26
    },
    {
      "lang": "en",
      "text": "think that this is a very useful",
      "offset": 55.14,
      "duration": 3.72
    },
    {
      "lang": "en",
      "text": "exercise for the following reasons",
      "offset": 56.46,
      "duration": 3.9
    },
    {
      "lang": "en",
      "text": "I actually have an entire blog post on",
      "offset": 58.86,
      "duration": 3.66
    },
    {
      "lang": "en",
      "text": "this topic but I'd like to call back",
      "offset": 60.36,
      "duration": 5.1
    },
    {
      "lang": "en",
      "text": "propagation a leaky abstraction",
      "offset": 62.52,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "and what I mean by that is back",
      "offset": 65.46,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "propagation does doesn't just make your",
      "offset": 67.32,
      "duration": 4.14
    },
    {
      "lang": "en",
      "text": "neural networks just work magically it's",
      "offset": 69.06,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "not the case they can just Stack Up",
      "offset": 71.46,
      "duration": 2.699
    },
    {
      "lang": "en",
      "text": "arbitrary Lego blocks of differentiable",
      "offset": 72.659,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "functions and just cross your fingers",
      "offset": 74.159,
      "duration": 3.541
    },
    {
      "lang": "en",
      "text": "and back propagate and everything is",
      "offset": 76.26,
      "duration": 3.48
    },
    {
      "lang": "en",
      "text": "great things don't just work",
      "offset": 77.7,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "automatically it is a leaky abstraction",
      "offset": 79.74,
      "duration": 4.019
    },
    {
      "lang": "en",
      "text": "in the sense that you can shoot yourself",
      "offset": 82.02,
      "duration": 3.419
    },
    {
      "lang": "en",
      "text": "in the foot if you do not understanding",
      "offset": 83.759,
      "duration": 4.741
    },
    {
      "lang": "en",
      "text": "its internals it will magically not work",
      "offset": 85.439,
      "duration": 6.121
    },
    {
      "lang": "en",
      "text": "or not work optimally and you will need",
      "offset": 88.5,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "to understand how it works under the",
      "offset": 91.56,
      "duration": 3.419
    },
    {
      "lang": "en",
      "text": "hood if you're hoping to debug it and if",
      "offset": 92.82,
      "duration": 3.42
    },
    {
      "lang": "en",
      "text": "you are hoping to address it in your",
      "offset": 94.979,
      "duration": 2.82
    },
    {
      "lang": "en",
      "text": "neural nut",
      "offset": 96.24,
      "duration": 3.72
    },
    {
      "lang": "en",
      "text": "um so this blog post here from a while",
      "offset": 97.799,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "ago goes into some of those examples so",
      "offset": 99.96,
      "duration": 3.9
    },
    {
      "lang": "en",
      "text": "for example we've already covered them",
      "offset": 102.36,
      "duration": 3.66
    },
    {
      "lang": "en",
      "text": "some of them already for example the",
      "offset": 103.86,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "flat tails of these functions and how",
      "offset": 106.02,
      "duration": 5.459
    },
    {
      "lang": "en",
      "text": "you do not want to saturate them too",
      "offset": 108.9,
      "duration": 4.62
    },
    {
      "lang": "en",
      "text": "much because your gradients will die the",
      "offset": 111.479,
      "duration": 3.661
    },
    {
      "lang": "en",
      "text": "case of dead neurons which I've already",
      "offset": 113.52,
      "duration": 2.879
    },
    {
      "lang": "en",
      "text": "covered as well",
      "offset": 115.14,
      "duration": 3.479
    },
    {
      "lang": "en",
      "text": "the case of exploding or Vanishing",
      "offset": 116.399,
      "duration": 4.141
    },
    {
      "lang": "en",
      "text": "gradients in the case of repair neural",
      "offset": 118.619,
      "duration": 4.201
    },
    {
      "lang": "en",
      "text": "networks which we are about to cover",
      "offset": 120.54,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "and then also you will often come across",
      "offset": 122.82,
      "duration": 4.74
    },
    {
      "lang": "en",
      "text": "some examples in the wild",
      "offset": 125.579,
      "duration": 4.501
    },
    {
      "lang": "en",
      "text": "this is a snippet that I found uh in a",
      "offset": 127.56,
      "duration": 4.2
    },
    {
      "lang": "en",
      "text": "random code base on the internet where",
      "offset": 130.08,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "they actually have like a very subtle",
      "offset": 131.76,
      "duration": 3.72
    },
    {
      "lang": "en",
      "text": "but pretty major bug in their",
      "offset": 133.44,
      "duration": 4.68
    },
    {
      "lang": "en",
      "text": "implementation and the bug points at the",
      "offset": 135.48,
      "duration": 4.619
    },
    {
      "lang": "en",
      "text": "fact that the author of this code does",
      "offset": 138.12,
      "duration": 3.18
    },
    {
      "lang": "en",
      "text": "not actually understand by propagation",
      "offset": 140.099,
      "duration": 3.061
    },
    {
      "lang": "en",
      "text": "so they're trying to do here is they're",
      "offset": 141.3,
      "duration": 3.96
    },
    {
      "lang": "en",
      "text": "trying to clip the loss at a certain",
      "offset": 143.16,
      "duration": 4.26
    },
    {
      "lang": "en",
      "text": "maximum value but actually what they're",
      "offset": 145.26,
      "duration": 3.3
    },
    {
      "lang": "en",
      "text": "trying to do is they're trying to",
      "offset": 147.42,
      "duration": 2.76
    },
    {
      "lang": "en",
      "text": "collect the gradients to have a maximum",
      "offset": 148.56,
      "duration": 3.48
    },
    {
      "lang": "en",
      "text": "value instead of trying to clip the loss",
      "offset": 150.18,
      "duration": 4.02
    },
    {
      "lang": "en",
      "text": "at a maximum value and",
      "offset": 152.04,
      "duration": 4.14
    },
    {
      "lang": "en",
      "text": "um indirectly they're basically causing",
      "offset": 154.2,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "some of the outliers to be actually",
      "offset": 156.18,
      "duration": 5.58
    },
    {
      "lang": "en",
      "text": "ignored because when you clip a loss of",
      "offset": 158.28,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "an outlier you are setting its gradient",
      "offset": 161.76,
      "duration": 5.1
    },
    {
      "lang": "en",
      "text": "to zero and so have a look through this",
      "offset": 163.8,
      "duration": 5.159
    },
    {
      "lang": "en",
      "text": "and read through it but there's",
      "offset": 166.86,
      "duration": 3.42
    },
    {
      "lang": "en",
      "text": "basically a bunch of subtle issues that",
      "offset": 168.959,
      "duration": 2.521
    },
    {
      "lang": "en",
      "text": "you're going to avoid if you actually",
      "offset": 170.28,
      "duration": 3.239
    },
    {
      "lang": "en",
      "text": "know what you're doing and that's why I",
      "offset": 171.48,
      "duration": 3.78
    },
    {
      "lang": "en",
      "text": "don't think it's the case that because",
      "offset": 173.519,
      "duration": 3.421
    },
    {
      "lang": "en",
      "text": "pytorch or other Frameworks offer",
      "offset": 175.26,
      "duration": 3.78
    },
    {
      "lang": "en",
      "text": "autograd it is okay for us to ignore how",
      "offset": 176.94,
      "duration": 3.659
    },
    {
      "lang": "en",
      "text": "it works",
      "offset": 179.04,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "now we've actually already covered",
      "offset": 180.599,
      "duration": 4.201
    },
    {
      "lang": "en",
      "text": "covered autograd and we wrote micrograd",
      "offset": 182.64,
      "duration": 4.679
    },
    {
      "lang": "en",
      "text": "but micrograd was an autograd engine",
      "offset": 184.8,
      "duration": 4.38
    },
    {
      "lang": "en",
      "text": "only on the level of individual scalars",
      "offset": 187.319,
      "duration": 3.78
    },
    {
      "lang": "en",
      "text": "so the atoms were single individual",
      "offset": 189.18,
      "duration": 4.44
    },
    {
      "lang": "en",
      "text": "numbers and uh you know I don't think",
      "offset": 191.099,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "it's enough and I'd like us to basically",
      "offset": 193.62,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "think about back propagation on level of",
      "offset": 194.94,
      "duration": 4.2
    },
    {
      "lang": "en",
      "text": "tensors as well and so in a summary I",
      "offset": 196.62,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "think it's a good exercise I think it is",
      "offset": 199.14,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "very very valuable you're going to",
      "offset": 201.659,
      "duration": 3.421
    },
    {
      "lang": "en",
      "text": "become better at debugging neural",
      "offset": 203.22,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "networks and making sure that you",
      "offset": 205.08,
      "duration": 3.78
    },
    {
      "lang": "en",
      "text": "understand what you're doing it is going",
      "offset": 207.06,
      "duration": 3.179
    },
    {
      "lang": "en",
      "text": "to make everything fully explicit so",
      "offset": 208.86,
      "duration": 2.4
    },
    {
      "lang": "en",
      "text": "you're not going to be nervous about",
      "offset": 210.239,
      "duration": 3.121
    },
    {
      "lang": "en",
      "text": "what is hidden away from you and",
      "offset": 211.26,
      "duration": 3.42
    },
    {
      "lang": "en",
      "text": "basically in general we're going to",
      "offset": 213.36,
      "duration": 4.2
    },
    {
      "lang": "en",
      "text": "emerge stronger and so let's get into it",
      "offset": 214.68,
      "duration": 5.4
    },
    {
      "lang": "en",
      "text": "a bit of a fun historical note here is",
      "offset": 217.56,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "that today writing your backward pass by",
      "offset": 220.08,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "hand and manually is not recommended and",
      "offset": 222.12,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "no one does it except for the purposes",
      "offset": 223.92,
      "duration": 4.379
    },
    {
      "lang": "en",
      "text": "of exercise but about 10 years ago in",
      "offset": 225.72,
      "duration": 4.14
    },
    {
      "lang": "en",
      "text": "deep learning this was fairly standard",
      "offset": 228.299,
      "duration": 3.961
    },
    {
      "lang": "en",
      "text": "and in fact pervasive so at the time",
      "offset": 229.86,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "everyone used to write their own",
      "offset": 232.26,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "backward pass by hand manually including",
      "offset": 233.459,
      "duration": 4.021
    },
    {
      "lang": "en",
      "text": "myself and it's just what you would do",
      "offset": 235.26,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "so we used to ride backward pass by hand",
      "offset": 237.48,
      "duration": 4.2
    },
    {
      "lang": "en",
      "text": "and now everyone just calls lost that",
      "offset": 239.58,
      "duration": 4.86
    },
    {
      "lang": "en",
      "text": "backward uh we've lost something I want",
      "offset": 241.68,
      "duration": 5.46
    },
    {
      "lang": "en",
      "text": "to give you a few examples of this so",
      "offset": 244.44,
      "duration": 6.659
    },
    {
      "lang": "en",
      "text": "here's a 2006 paper from Jeff Hinton and",
      "offset": 247.14,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "Russell selectinov in science that was",
      "offset": 251.099,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "influential at the time and this was",
      "offset": 253.62,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "training some architectures called",
      "offset": 255.42,
      "duration": 4.379
    },
    {
      "lang": "en",
      "text": "restricted bolstery machines and",
      "offset": 257.94,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "basically it's an auto encoder trained",
      "offset": 259.799,
      "duration": 6.361
    },
    {
      "lang": "en",
      "text": "here and this is from roughly 2010 I had",
      "offset": 262.019,
      "duration": 5.521
    },
    {
      "lang": "en",
      "text": "a library for training researchable",
      "offset": 266.16,
      "duration": 4.14
    },
    {
      "lang": "en",
      "text": "machines and this was at the time",
      "offset": 267.54,
      "duration": 4.86
    },
    {
      "lang": "en",
      "text": "written in Matlab so python was not used",
      "offset": 270.3,
      "duration": 3.899
    },
    {
      "lang": "en",
      "text": "for deep learning pervasively it was all",
      "offset": 272.4,
      "duration": 4.26
    },
    {
      "lang": "en",
      "text": "Matlab and Matlab was this a scientific",
      "offset": 274.199,
      "duration": 5.161
    },
    {
      "lang": "en",
      "text": "Computing package that everyone would",
      "offset": 276.66,
      "duration": 4.92
    },
    {
      "lang": "en",
      "text": "use so we would write Matlab which is",
      "offset": 279.36,
      "duration": 4.86
    },
    {
      "lang": "en",
      "text": "barely a programming language as well",
      "offset": 281.58,
      "duration": 4.619
    },
    {
      "lang": "en",
      "text": "but I've had a very convenient tensor",
      "offset": 284.22,
      "duration": 3.9
    },
    {
      "lang": "en",
      "text": "class and was this a Computing",
      "offset": 286.199,
      "duration": 3.301
    },
    {
      "lang": "en",
      "text": "environment and you would run here it",
      "offset": 288.12,
      "duration": 3.48
    },
    {
      "lang": "en",
      "text": "would all run on a CPU of course but you",
      "offset": 289.5,
      "duration": 3.66
    },
    {
      "lang": "en",
      "text": "would have very nice plots to go with it",
      "offset": 291.6,
      "duration": 3.3
    },
    {
      "lang": "en",
      "text": "and a built-in debugger and it was",
      "offset": 293.16,
      "duration": 4.68
    },
    {
      "lang": "en",
      "text": "pretty nice now the code in this package",
      "offset": 294.9,
      "duration": 5.82
    },
    {
      "lang": "en",
      "text": "in 2010 that I wrote for fitting",
      "offset": 297.84,
      "duration": 5.16
    },
    {
      "lang": "en",
      "text": "research multiple machines to a large",
      "offset": 300.72,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "extent is recognizable but I wanted to",
      "offset": 303,
      "duration": 4.02
    },
    {
      "lang": "en",
      "text": "show you how you would well I'm creating",
      "offset": 305.04,
      "duration": 4.2
    },
    {
      "lang": "en",
      "text": "the data in the XY batches I'm",
      "offset": 307.02,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "initializing the neural nut so it's got",
      "offset": 309.24,
      "duration": 4.019
    },
    {
      "lang": "en",
      "text": "weights and biases just like we're used",
      "offset": 311.82,
      "duration": 3.42
    },
    {
      "lang": "en",
      "text": "to and then this is the training Loop",
      "offset": 313.259,
      "duration": 3.961
    },
    {
      "lang": "en",
      "text": "where we actually do the forward pass",
      "offset": 315.24,
      "duration": 4.739
    },
    {
      "lang": "en",
      "text": "and then here at this time they didn't",
      "offset": 317.22,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "even necessarily use back propagation to",
      "offset": 319.979,
      "duration": 3.541
    },
    {
      "lang": "en",
      "text": "train neural networks so this in",
      "offset": 321.54,
      "duration": 3.719
    },
    {
      "lang": "en",
      "text": "particular implements contrastive",
      "offset": 323.52,
      "duration": 4.619
    },
    {
      "lang": "en",
      "text": "Divergence which estimates a gradient",
      "offset": 325.259,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "and then here we take that gradient and",
      "offset": 328.139,
      "duration": 4.441
    },
    {
      "lang": "en",
      "text": "use it for a parameter update along the",
      "offset": 330.539,
      "duration": 4.021
    },
    {
      "lang": "en",
      "text": "lines that we're used to",
      "offset": 332.58,
      "duration": 4.02
    },
    {
      "lang": "en",
      "text": "um yeah here",
      "offset": 334.56,
      "duration": 3.479
    },
    {
      "lang": "en",
      "text": "but you can see that basically people",
      "offset": 336.6,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "are meddling with these gradients uh",
      "offset": 338.039,
      "duration": 3.901
    },
    {
      "lang": "en",
      "text": "directly and inline and themselves uh it",
      "offset": 339.6,
      "duration": 3.78
    },
    {
      "lang": "en",
      "text": "wasn't that common to use an auto grad",
      "offset": 341.94,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "engine here's one more example from a",
      "offset": 343.38,
      "duration": 4.44
    },
    {
      "lang": "en",
      "text": "paper of mine from 2014",
      "offset": 345.3,
      "duration": 4.619
    },
    {
      "lang": "en",
      "text": "um called the fragmented embeddings",
      "offset": 347.82,
      "duration": 3.54
    },
    {
      "lang": "en",
      "text": "and here what I was doing is I was",
      "offset": 349.919,
      "duration": 3.421
    },
    {
      "lang": "en",
      "text": "aligning images and text",
      "offset": 351.36,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "um and so it's kind of like a clip if",
      "offset": 353.34,
      "duration": 3.54
    },
    {
      "lang": "en",
      "text": "you're familiar with it but instead of",
      "offset": 355.199,
      "duration": 3.06
    },
    {
      "lang": "en",
      "text": "working on the level of entire images",
      "offset": 356.88,
      "duration": 3.3
    },
    {
      "lang": "en",
      "text": "and entire sentences it was working on",
      "offset": 358.259,
      "duration": 3.541
    },
    {
      "lang": "en",
      "text": "the level of individual objects and",
      "offset": 360.18,
      "duration": 3.54
    },
    {
      "lang": "en",
      "text": "little pieces of sentences and I was",
      "offset": 361.8,
      "duration": 3.72
    },
    {
      "lang": "en",
      "text": "embedding them and then calculating very",
      "offset": 363.72,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "much like a clip-like loss and I dig up",
      "offset": 365.52,
      "duration": 4.86
    },
    {
      "lang": "en",
      "text": "the code from 2014 of how I implemented",
      "offset": 368.039,
      "duration": 5.22
    },
    {
      "lang": "en",
      "text": "this and it was already in numpy and",
      "offset": 370.38,
      "duration": 4.14
    },
    {
      "lang": "en",
      "text": "python",
      "offset": 373.259,
      "duration": 3.241
    },
    {
      "lang": "en",
      "text": "and here I'm planting the cost function",
      "offset": 374.52,
      "duration": 4.679
    },
    {
      "lang": "en",
      "text": "and it was standard to implement not",
      "offset": 376.5,
      "duration": 4.44
    },
    {
      "lang": "en",
      "text": "just the cost but also the backward pass",
      "offset": 379.199,
      "duration": 4.5
    },
    {
      "lang": "en",
      "text": "manually so here I'm calculating the",
      "offset": 380.94,
      "duration": 5.099
    },
    {
      "lang": "en",
      "text": "image embeddings sentence embeddings the",
      "offset": 383.699,
      "duration": 4.921
    },
    {
      "lang": "en",
      "text": "loss function I calculate this course",
      "offset": 386.039,
      "duration": 5.22
    },
    {
      "lang": "en",
      "text": "this is the loss function and then once",
      "offset": 388.62,
      "duration": 3.96
    },
    {
      "lang": "en",
      "text": "I have the loss function I do the",
      "offset": 391.259,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "backward pass right here so I backward",
      "offset": 392.58,
      "duration": 3.959
    },
    {
      "lang": "en",
      "text": "through the loss function and through",
      "offset": 394.62,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "the neural nut and I append",
      "offset": 396.539,
      "duration": 4.741
    },
    {
      "lang": "en",
      "text": "regularization so everything was done by",
      "offset": 398.22,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "hand manually and you were just right",
      "offset": 401.28,
      "duration": 3.18
    },
    {
      "lang": "en",
      "text": "out the backward pass and then you would",
      "offset": 402.78,
      "duration": 3.72
    },
    {
      "lang": "en",
      "text": "use a gradient Checker to make sure that",
      "offset": 404.46,
      "duration": 3.48
    },
    {
      "lang": "en",
      "text": "your numerical estimate of the gradient",
      "offset": 406.5,
      "duration": 2.94
    },
    {
      "lang": "en",
      "text": "agrees with the one you calculated",
      "offset": 407.94,
      "duration": 3.96
    },
    {
      "lang": "en",
      "text": "during back propagation so this was very",
      "offset": 409.44,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "standard for a long time but today of",
      "offset": 411.9,
      "duration": 3.239
    },
    {
      "lang": "en",
      "text": "course it is standard to use an auto",
      "offset": 413.52,
      "duration": 3.299
    },
    {
      "lang": "en",
      "text": "grad engine",
      "offset": 415.139,
      "duration": 3.421
    },
    {
      "lang": "en",
      "text": "um but it was definitely useful and I",
      "offset": 416.819,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "think people sort of understood how",
      "offset": 418.56,
      "duration": 2.579
    },
    {
      "lang": "en",
      "text": "these neural networks work on a very",
      "offset": 419.819,
      "duration": 3.301
    },
    {
      "lang": "en",
      "text": "intuitive level and so I think it's a",
      "offset": 421.139,
      "duration": 3.541
    },
    {
      "lang": "en",
      "text": "good exercise again and this is where we",
      "offset": 423.12,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "want to be okay so just as a reminder",
      "offset": 424.68,
      "duration": 3.54
    },
    {
      "lang": "en",
      "text": "from our previous lecture this is The",
      "offset": 426.479,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "jupyter Notebook that we implemented at",
      "offset": 428.22,
      "duration": 3.539
    },
    {
      "lang": "en",
      "text": "the time and",
      "offset": 429.84,
      "duration": 3.54
    },
    {
      "lang": "en",
      "text": "we're going to keep everything the same",
      "offset": 431.759,
      "duration": 3.241
    },
    {
      "lang": "en",
      "text": "so we're still going to have a two layer",
      "offset": 433.38,
      "duration": 3.24
    },
    {
      "lang": "en",
      "text": "multiplayer perceptron with a batch",
      "offset": 435,
      "duration": 3.66
    },
    {
      "lang": "en",
      "text": "normalization layer so the forward pass",
      "offset": 436.62,
      "duration": 3.66
    },
    {
      "lang": "en",
      "text": "will be basically identical to this",
      "offset": 438.66,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "lecture but here we're going to get rid",
      "offset": 440.28,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "of lost and backward and instead we're",
      "offset": 442.02,
      "duration": 2.64
    },
    {
      "lang": "en",
      "text": "going to write the backward pass",
      "offset": 443.639,
      "duration": 2.4
    },
    {
      "lang": "en",
      "text": "manually",
      "offset": 444.66,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "now here's the starter code for this",
      "offset": 446.039,
      "duration": 3.481
    },
    {
      "lang": "en",
      "text": "lecture we are becoming a back prop",
      "offset": 447.66,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "ninja in this notebook",
      "offset": 449.52,
      "duration": 4.5
    },
    {
      "lang": "en",
      "text": "and the first few cells here are",
      "offset": 451.5,
      "duration": 4.5
    },
    {
      "lang": "en",
      "text": "identical to what we are used to so we",
      "offset": 454.02,
      "duration": 3.959
    },
    {
      "lang": "en",
      "text": "are doing some imports loading the data",
      "offset": 456,
      "duration": 4.139
    },
    {
      "lang": "en",
      "text": "set and processing the data set none of",
      "offset": 457.979,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "this changed",
      "offset": 460.139,
      "duration": 3.18
    },
    {
      "lang": "en",
      "text": "now here I'm introducing a utility",
      "offset": 461.58,
      "duration": 3.119
    },
    {
      "lang": "en",
      "text": "function that we're going to use later",
      "offset": 463.319,
      "duration": 3.301
    },
    {
      "lang": "en",
      "text": "to compare the gradients so in",
      "offset": 464.699,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "particular we are going to have the",
      "offset": 466.62,
      "duration": 2.519
    },
    {
      "lang": "en",
      "text": "gradients that we estimate manually",
      "offset": 467.699,
      "duration": 3.06
    },
    {
      "lang": "en",
      "text": "ourselves and we're going to have",
      "offset": 469.139,
      "duration": 4.141
    },
    {
      "lang": "en",
      "text": "gradients that Pi torch calculates and",
      "offset": 470.759,
      "duration": 3.72
    },
    {
      "lang": "en",
      "text": "we're going to be checking for",
      "offset": 473.28,
      "duration": 2.639
    },
    {
      "lang": "en",
      "text": "correctness assuming of course that",
      "offset": 474.479,
      "duration": 4.44
    },
    {
      "lang": "en",
      "text": "pytorch is correct",
      "offset": 475.919,
      "duration": 4.381
    },
    {
      "lang": "en",
      "text": "um then here we have the initialization",
      "offset": 478.919,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "that we are quite used to so we have our",
      "offset": 480.3,
      "duration": 4.86
    },
    {
      "lang": "en",
      "text": "embedding table for the characters the",
      "offset": 483,
      "duration": 3.78
    },
    {
      "lang": "en",
      "text": "first layer second layer and the batch",
      "offset": 485.16,
      "duration": 3.18
    },
    {
      "lang": "en",
      "text": "normalization in between",
      "offset": 486.78,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "and here's where we create all the",
      "offset": 488.34,
      "duration": 3.54
    },
    {
      "lang": "en",
      "text": "parameters now you will note that I",
      "offset": 489.78,
      "duration": 3.72
    },
    {
      "lang": "en",
      "text": "changed the initialization a little bit",
      "offset": 491.88,
      "duration": 4.2
    },
    {
      "lang": "en",
      "text": "uh to be small numbers so normally you",
      "offset": 493.5,
      "duration": 4.74
    },
    {
      "lang": "en",
      "text": "would set the biases to be all zero here",
      "offset": 496.08,
      "duration": 4.019
    },
    {
      "lang": "en",
      "text": "I am setting them to be small random",
      "offset": 498.24,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "numbers and I'm doing this because",
      "offset": 500.099,
      "duration": 4.44
    },
    {
      "lang": "en",
      "text": "if your variables are initialized to",
      "offset": 502.8,
      "duration": 3.42
    },
    {
      "lang": "en",
      "text": "exactly zero sometimes what can happen",
      "offset": 504.539,
      "duration": 3.541
    },
    {
      "lang": "en",
      "text": "is that can mask an incorrect",
      "offset": 506.22,
      "duration": 4.14
    },
    {
      "lang": "en",
      "text": "implementation of a gradient",
      "offset": 508.08,
      "duration": 4.44
    },
    {
      "lang": "en",
      "text": "um because uh when everything is zero it",
      "offset": 510.36,
      "duration": 3.66
    },
    {
      "lang": "en",
      "text": "sort of like simplifies and gives you a",
      "offset": 512.52,
      "duration": 2.759
    },
    {
      "lang": "en",
      "text": "much simpler expression of the gradient",
      "offset": 514.02,
      "duration": 3.42
    },
    {
      "lang": "en",
      "text": "than you would otherwise get and so by",
      "offset": 515.279,
      "duration": 3.961
    },
    {
      "lang": "en",
      "text": "making it small numbers I'm trying to",
      "offset": 517.44,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "unmask those potential errors in these",
      "offset": 519.24,
      "duration": 3.9
    },
    {
      "lang": "en",
      "text": "calculations",
      "offset": 521.76,
      "duration": 4.74
    },
    {
      "lang": "en",
      "text": "you also notice that I'm using uh B1 in",
      "offset": 523.14,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "the first layer I'm using a bias despite",
      "offset": 526.5,
      "duration": 4.44
    },
    {
      "lang": "en",
      "text": "batch normalization right afterwards",
      "offset": 528.42,
      "duration": 4.38
    },
    {
      "lang": "en",
      "text": "um so this would typically not be what",
      "offset": 530.94,
      "duration": 3.24
    },
    {
      "lang": "en",
      "text": "you do because we talked about the fact",
      "offset": 532.8,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "that you don't need the bias but I'm",
      "offset": 534.18,
      "duration": 3.719
    },
    {
      "lang": "en",
      "text": "doing this here just for fun",
      "offset": 535.92,
      "duration": 3.06
    },
    {
      "lang": "en",
      "text": "um because we're going to have a",
      "offset": 537.899,
      "duration": 2.461
    },
    {
      "lang": "en",
      "text": "gradient with respect to it and we can",
      "offset": 538.98,
      "duration": 2.82
    },
    {
      "lang": "en",
      "text": "check that we are still calculating it",
      "offset": 540.36,
      "duration": 3.18
    },
    {
      "lang": "en",
      "text": "correctly even though this bias is",
      "offset": 541.8,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "asparious",
      "offset": 543.54,
      "duration": 3.72
    },
    {
      "lang": "en",
      "text": "so here I'm calculating a single batch",
      "offset": 545.16,
      "duration": 5.16
    },
    {
      "lang": "en",
      "text": "and then here I'm doing a forward pass",
      "offset": 547.26,
      "duration": 4.44
    },
    {
      "lang": "en",
      "text": "now you'll notice that the forward pass",
      "offset": 550.32,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "is significantly expanded from what we",
      "offset": 551.7,
      "duration": 3.9
    },
    {
      "lang": "en",
      "text": "are used to here the forward pass was",
      "offset": 553.68,
      "duration": 2.94
    },
    {
      "lang": "en",
      "text": "just",
      "offset": 555.6,
      "duration": 2.28
    },
    {
      "lang": "en",
      "text": "um here",
      "offset": 556.62,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "now the reason that the forward pass is",
      "offset": 557.88,
      "duration": 4.2
    },
    {
      "lang": "en",
      "text": "longer is for two reasons number one",
      "offset": 559.62,
      "duration": 4.38
    },
    {
      "lang": "en",
      "text": "here we just had an F dot cross entropy",
      "offset": 562.08,
      "duration": 4.439
    },
    {
      "lang": "en",
      "text": "but here I am bringing back a explicit",
      "offset": 564,
      "duration": 4.38
    },
    {
      "lang": "en",
      "text": "implementation of the loss function",
      "offset": 566.519,
      "duration": 3.301
    },
    {
      "lang": "en",
      "text": "and number two",
      "offset": 568.38,
      "duration": 3.66
    },
    {
      "lang": "en",
      "text": "I've broken up the implementation into",
      "offset": 569.82,
      "duration": 5.519
    },
    {
      "lang": "en",
      "text": "manageable chunks so we have a lot a lot",
      "offset": 572.04,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "more intermediate tensors along the way",
      "offset": 575.339,
      "duration": 3.481
    },
    {
      "lang": "en",
      "text": "in the forward pass and that's because",
      "offset": 577.08,
      "duration": 3.42
    },
    {
      "lang": "en",
      "text": "we are about to go backwards and",
      "offset": 578.82,
      "duration": 4.019
    },
    {
      "lang": "en",
      "text": "calculate the gradients in this back",
      "offset": 580.5,
      "duration": 5.22
    },
    {
      "lang": "en",
      "text": "propagation from the bottom to the top",
      "offset": 582.839,
      "duration": 5.581
    },
    {
      "lang": "en",
      "text": "so we're going to go upwards and just",
      "offset": 585.72,
      "duration": 4.2
    },
    {
      "lang": "en",
      "text": "like we have for example the lock props",
      "offset": 588.42,
      "duration": 3.539
    },
    {
      "lang": "en",
      "text": "tensor in a forward pass in the backward",
      "offset": 589.92,
      "duration": 3.78
    },
    {
      "lang": "en",
      "text": "pass we're going to have a d-lock probes",
      "offset": 591.959,
      "duration": 3.241
    },
    {
      "lang": "en",
      "text": "which is going to store the derivative",
      "offset": 593.7,
      "duration": 2.819
    },
    {
      "lang": "en",
      "text": "of the loss with respect to the lock",
      "offset": 595.2,
      "duration": 3.48
    },
    {
      "lang": "en",
      "text": "props tensor and so we're going to be",
      "offset": 596.519,
      "duration": 4.38
    },
    {
      "lang": "en",
      "text": "prepending D to every one of these",
      "offset": 598.68,
      "duration": 4.26
    },
    {
      "lang": "en",
      "text": "tensors and calculating it along the way",
      "offset": 600.899,
      "duration": 4.021
    },
    {
      "lang": "en",
      "text": "of this back propagation",
      "offset": 602.94,
      "duration": 4.38
    },
    {
      "lang": "en",
      "text": "so as an example we have a b and raw",
      "offset": 604.92,
      "duration": 4.2
    },
    {
      "lang": "en",
      "text": "here we're going to be calculating a DB",
      "offset": 607.32,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "in raw so here I'm telling pytorch that",
      "offset": 609.12,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "we want to retain the grad of all these",
      "offset": 612.6,
      "duration": 4.14
    },
    {
      "lang": "en",
      "text": "intermediate values because here in",
      "offset": 614.64,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "exercise one we're going to calculate",
      "offset": 616.74,
      "duration": 3.539
    },
    {
      "lang": "en",
      "text": "the backward pass so we're going to",
      "offset": 618.48,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "calculate all these D values D variables",
      "offset": 620.279,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "and use the CNP function I've introduced",
      "offset": 622.56,
      "duration": 4.2
    },
    {
      "lang": "en",
      "text": "above to check our correctness with",
      "offset": 625.08,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "respect to what pi torch is telling us",
      "offset": 626.76,
      "duration": 4.68
    },
    {
      "lang": "en",
      "text": "this is going to be exercise one uh",
      "offset": 629.399,
      "duration": 3.421
    },
    {
      "lang": "en",
      "text": "where we sort of back propagate through",
      "offset": 631.44,
      "duration": 3.24
    },
    {
      "lang": "en",
      "text": "this entire graph",
      "offset": 632.82,
      "duration": 3.42
    },
    {
      "lang": "en",
      "text": "now just to give you a very quick",
      "offset": 634.68,
      "duration": 2.82
    },
    {
      "lang": "en",
      "text": "preview of what's going to happen in",
      "offset": 636.24,
      "duration": 4.14
    },
    {
      "lang": "en",
      "text": "exercise two and below here we have",
      "offset": 637.5,
      "duration": 5.82
    },
    {
      "lang": "en",
      "text": "fully broken up the loss and back",
      "offset": 640.38,
      "duration": 5.22
    },
    {
      "lang": "en",
      "text": "propagated through it manually in all",
      "offset": 643.32,
      "duration": 4.5
    },
    {
      "lang": "en",
      "text": "the little Atomic pieces that make it up",
      "offset": 645.6,
      "duration": 3.54
    },
    {
      "lang": "en",
      "text": "but here we're going to collapse the",
      "offset": 647.82,
      "duration": 3.079
    },
    {
      "lang": "en",
      "text": "laws into a single cross-entropy call",
      "offset": 649.14,
      "duration": 4.199
    },
    {
      "lang": "en",
      "text": "and instead we're going to analytically",
      "offset": 650.899,
      "duration": 6.041
    },
    {
      "lang": "en",
      "text": "derive using math and paper and pencil",
      "offset": 653.339,
      "duration": 6.18
    },
    {
      "lang": "en",
      "text": "the gradient of the loss with respect to",
      "offset": 656.94,
      "duration": 4.2
    },
    {
      "lang": "en",
      "text": "the logits and instead of back",
      "offset": 659.519,
      "duration": 2.82
    },
    {
      "lang": "en",
      "text": "propagating through all of its little",
      "offset": 661.14,
      "duration": 3.3
    },
    {
      "lang": "en",
      "text": "chunks one at a time we're just going to",
      "offset": 662.339,
      "duration": 3.541
    },
    {
      "lang": "en",
      "text": "analytically derive what that gradient",
      "offset": 664.44,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "is and we're going to implement that",
      "offset": 665.88,
      "duration": 3.3
    },
    {
      "lang": "en",
      "text": "which is much more efficient as we'll",
      "offset": 667.44,
      "duration": 3.24
    },
    {
      "lang": "en",
      "text": "see in the in a bit",
      "offset": 669.18,
      "duration": 3.48
    },
    {
      "lang": "en",
      "text": "then we're going to do the exact same",
      "offset": 670.68,
      "duration": 3.96
    },
    {
      "lang": "en",
      "text": "thing for patch normalization so instead",
      "offset": 672.66,
      "duration": 3.48
    },
    {
      "lang": "en",
      "text": "of breaking up bass drum into all the",
      "offset": 674.64,
      "duration": 3.96
    },
    {
      "lang": "en",
      "text": "old tiny components we're going to use",
      "offset": 676.14,
      "duration": 4.68
    },
    {
      "lang": "en",
      "text": "uh pen and paper and Mathematics and",
      "offset": 678.6,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "calculus to derive the gradient through",
      "offset": 680.82,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "the bachelor Bachelor layer so we're",
      "offset": 682.92,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "going to calculate the backward",
      "offset": 685.38,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "passthrough bathroom layer in a much",
      "offset": 687,
      "duration": 3.24
    },
    {
      "lang": "en",
      "text": "more efficient expression instead of",
      "offset": 688.5,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "backward propagating through all of its",
      "offset": 690.24,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "little pieces independently",
      "offset": 691.62,
      "duration": 4.86
    },
    {
      "lang": "en",
      "text": "so there's going to be exercise three",
      "offset": 693.6,
      "duration": 4.44
    },
    {
      "lang": "en",
      "text": "and then in exercise four we're going to",
      "offset": 696.48,
      "duration": 3.539
    },
    {
      "lang": "en",
      "text": "put it all together and this is the full",
      "offset": 698.04,
      "duration": 4.68
    },
    {
      "lang": "en",
      "text": "code of training this two layer MLP and",
      "offset": 700.019,
      "duration": 4.38
    },
    {
      "lang": "en",
      "text": "we're going to basically insert our",
      "offset": 702.72,
      "duration": 3.42
    },
    {
      "lang": "en",
      "text": "manual back prop and we're going to take",
      "offset": 704.399,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "out lost it backward and you will",
      "offset": 706.14,
      "duration": 4.62
    },
    {
      "lang": "en",
      "text": "basically see that you can get all the",
      "offset": 708.48,
      "duration": 5.099
    },
    {
      "lang": "en",
      "text": "same results using fully your own code",
      "offset": 710.76,
      "duration": 5.22
    },
    {
      "lang": "en",
      "text": "and the only thing we're using from",
      "offset": 713.579,
      "duration": 5.7
    },
    {
      "lang": "en",
      "text": "pytorch is the torch.tensor to make the",
      "offset": 715.98,
      "duration": 5.7
    },
    {
      "lang": "en",
      "text": "calculations efficient but otherwise you",
      "offset": 719.279,
      "duration": 4.021
    },
    {
      "lang": "en",
      "text": "will understand fully what it means to",
      "offset": 721.68,
      "duration": 3.06
    },
    {
      "lang": "en",
      "text": "forward and backward and neural net and",
      "offset": 723.3,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "train it and I think that'll be awesome",
      "offset": 724.74,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "so let's get to it",
      "offset": 726.42,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "okay so I read all the cells of this",
      "offset": 728.1,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "notebook all the way up to here and I'm",
      "offset": 730.019,
      "duration": 4.681
    },
    {
      "lang": "en",
      "text": "going to erase this and I'm going to",
      "offset": 733.14,
      "duration": 2.759
    },
    {
      "lang": "en",
      "text": "start implementing backward pass",
      "offset": 734.7,
      "duration": 4.02
    },
    {
      "lang": "en",
      "text": "starting with d lock problems so we want",
      "offset": 735.899,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "to understand what should go here to",
      "offset": 738.72,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "calculate the gradient of the loss with",
      "offset": 740.459,
      "duration": 3.301
    },
    {
      "lang": "en",
      "text": "respect to all the elements of the log",
      "offset": 742.079,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "props tensor",
      "offset": 743.76,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "now I'm going to give away the answer",
      "offset": 745.079,
      "duration": 3.301
    },
    {
      "lang": "en",
      "text": "here but I wanted to put a quick note",
      "offset": 746.88,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "here that I think would be most",
      "offset": 748.38,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "pedagogically useful for you is to",
      "offset": 750.24,
      "duration": 4.26
    },
    {
      "lang": "en",
      "text": "actually go into the description of this",
      "offset": 752.459,
      "duration": 4.201
    },
    {
      "lang": "en",
      "text": "video and find the link to this Jupiter",
      "offset": 754.5,
      "duration": 3.899
    },
    {
      "lang": "en",
      "text": "notebook you can find it both on GitHub",
      "offset": 756.66,
      "duration": 3.48
    },
    {
      "lang": "en",
      "text": "but you can also find Google collab with",
      "offset": 758.399,
      "duration": 2.94
    },
    {
      "lang": "en",
      "text": "it so you don't have to install anything",
      "offset": 760.14,
      "duration": 2.999
    },
    {
      "lang": "en",
      "text": "you'll just go to a website on Google",
      "offset": 761.339,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "collab and you can try to implement",
      "offset": 763.139,
      "duration": 4.82
    },
    {
      "lang": "en",
      "text": "these derivatives or gradients yourself",
      "offset": 765.18,
      "duration": 5.339
    },
    {
      "lang": "en",
      "text": "and then if you are not able to come to",
      "offset": 767.959,
      "duration": 5.32
    },
    {
      "lang": "en",
      "text": "my video and see me do it and so work in",
      "offset": 770.519,
      "duration": 4.921
    },
    {
      "lang": "en",
      "text": "Tandem and try it first yourself and",
      "offset": 773.279,
      "duration": 4.68
    },
    {
      "lang": "en",
      "text": "then see me give away the answer and I",
      "offset": 775.44,
      "duration": 3.66
    },
    {
      "lang": "en",
      "text": "think that'll be most valuable to you",
      "offset": 777.959,
      "duration": 2.281
    },
    {
      "lang": "en",
      "text": "and that's how I recommend you go",
      "offset": 779.1,
      "duration": 2.34
    },
    {
      "lang": "en",
      "text": "through this lecture",
      "offset": 780.24,
      "duration": 3.3
    },
    {
      "lang": "en",
      "text": "so we are starting here with d-log props",
      "offset": 781.44,
      "duration": 5.519
    },
    {
      "lang": "en",
      "text": "now d-lock props will hold the",
      "offset": 783.54,
      "duration": 5.099
    },
    {
      "lang": "en",
      "text": "derivative of the loss with respect to",
      "offset": 786.959,
      "duration": 4.38
    },
    {
      "lang": "en",
      "text": "all the elements of log props",
      "offset": 788.639,
      "duration": 5.041
    },
    {
      "lang": "en",
      "text": "what is inside log blobs the shape of",
      "offset": 791.339,
      "duration": 6.841
    },
    {
      "lang": "en",
      "text": "this is 32 by 27. so it's not going to",
      "offset": 793.68,
      "duration": 5.94
    },
    {
      "lang": "en",
      "text": "surprise you that D log props should",
      "offset": 798.18,
      "duration": 3.719
    },
    {
      "lang": "en",
      "text": "also be an array of size 32 by 27",
      "offset": 799.62,
      "duration": 4.14
    },
    {
      "lang": "en",
      "text": "because we want the derivative loss with",
      "offset": 801.899,
      "duration": 4.141
    },
    {
      "lang": "en",
      "text": "respect to all of its elements so the",
      "offset": 803.76,
      "duration": 3.78
    },
    {
      "lang": "en",
      "text": "sizes of those are always going to be",
      "offset": 806.04,
      "duration": 3.479
    },
    {
      "lang": "en",
      "text": "equal",
      "offset": 807.54,
      "duration": 5.58
    },
    {
      "lang": "en",
      "text": "now how how does log props influence the",
      "offset": 809.519,
      "duration": 7.38
    },
    {
      "lang": "en",
      "text": "loss okay loss is negative block probes",
      "offset": 813.12,
      "duration": 7.56
    },
    {
      "lang": "en",
      "text": "indexed with range of N and YB and then",
      "offset": 816.899,
      "duration": 5.961
    },
    {
      "lang": "en",
      "text": "the mean of that now just as a reminder",
      "offset": 820.68,
      "duration": 7.14
    },
    {
      "lang": "en",
      "text": "YB is just a basically an array of all",
      "offset": 822.86,
      "duration": 8.68
    },
    {
      "lang": "en",
      "text": "the correct indices",
      "offset": 827.82,
      "duration": 5.16
    },
    {
      "lang": "en",
      "text": "um so what we're doing here is we're",
      "offset": 831.54,
      "duration": 3.18
    },
    {
      "lang": "en",
      "text": "taking the lock props array of size 32",
      "offset": 832.98,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "by 27.",
      "offset": 834.72,
      "duration": 4.2
    },
    {
      "lang": "en",
      "text": "right",
      "offset": 837.3,
      "duration": 3.659
    },
    {
      "lang": "en",
      "text": "and then we are going in every single",
      "offset": 838.92,
      "duration": 4.2
    },
    {
      "lang": "en",
      "text": "row and in each row we are plugging",
      "offset": 840.959,
      "duration": 5.341
    },
    {
      "lang": "en",
      "text": "plucking out the index eight and then 14",
      "offset": 843.12,
      "duration": 4.86
    },
    {
      "lang": "en",
      "text": "and 15 and so on so we're going down the",
      "offset": 846.3,
      "duration": 4.62
    },
    {
      "lang": "en",
      "text": "rows that's the iterator range of N and",
      "offset": 847.98,
      "duration": 4.919
    },
    {
      "lang": "en",
      "text": "then we are always plucking out the",
      "offset": 850.92,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "index of the column specified by this",
      "offset": 852.899,
      "duration": 4.861
    },
    {
      "lang": "en",
      "text": "tensor YB so in the zeroth row we are",
      "offset": 855.24,
      "duration": 4.92
    },
    {
      "lang": "en",
      "text": "taking the eighth column in the first",
      "offset": 857.76,
      "duration": 5.579
    },
    {
      "lang": "en",
      "text": "row we're taking the 14th column Etc and",
      "offset": 860.16,
      "duration": 6.359
    },
    {
      "lang": "en",
      "text": "so log props at this plugs out",
      "offset": 863.339,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "all those",
      "offset": 866.519,
      "duration": 4.38
    },
    {
      "lang": "en",
      "text": "log probabilities of the correct next",
      "offset": 868.459,
      "duration": 4.301
    },
    {
      "lang": "en",
      "text": "character in a sequence",
      "offset": 870.899,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "so that's what that does and the shape",
      "offset": 872.76,
      "duration": 3.78
    },
    {
      "lang": "en",
      "text": "of this or the size of it is of course",
      "offset": 874.74,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "32 because our batch size is 32.",
      "offset": 876.54,
      "duration": 6.72
    },
    {
      "lang": "en",
      "text": "so these elements get plugged out and",
      "offset": 880.5,
      "duration": 5.1
    },
    {
      "lang": "en",
      "text": "then their mean and the negative of that",
      "offset": 883.26,
      "duration": 3.96
    },
    {
      "lang": "en",
      "text": "becomes loss",
      "offset": 885.6,
      "duration": 4.14
    },
    {
      "lang": "en",
      "text": "so I always like to work with simpler",
      "offset": 887.22,
      "duration": 5.1
    },
    {
      "lang": "en",
      "text": "examples to understand the numerical",
      "offset": 889.74,
      "duration": 5.459
    },
    {
      "lang": "en",
      "text": "form of derivative what's going on here",
      "offset": 892.32,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "is once we've plucked out these examples",
      "offset": 895.199,
      "duration": 5.521
    },
    {
      "lang": "en",
      "text": "um we're taking the mean and then the",
      "offset": 898.8,
      "duration": 4.02
    },
    {
      "lang": "en",
      "text": "negative so the loss basically",
      "offset": 900.72,
      "duration": 4.2
    },
    {
      "lang": "en",
      "text": "I can write it this way is the negative",
      "offset": 902.82,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "of say a plus b plus c",
      "offset": 904.92,
      "duration": 4.44
    },
    {
      "lang": "en",
      "text": "and the mean of those three numbers",
      "offset": 907.86,
      "duration": 3.419
    },
    {
      "lang": "en",
      "text": "would be say negative would divide three",
      "offset": 909.36,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "that would be how we achieve the mean of",
      "offset": 911.279,
      "duration": 3.961
    },
    {
      "lang": "en",
      "text": "three numbers ABC although we actually",
      "offset": 913.199,
      "duration": 3.781
    },
    {
      "lang": "en",
      "text": "have 32 numbers here",
      "offset": 915.24,
      "duration": 5.099
    },
    {
      "lang": "en",
      "text": "and so what is basically the loss by say",
      "offset": 916.98,
      "duration": 5.46
    },
    {
      "lang": "en",
      "text": "like d a right",
      "offset": 920.339,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "well if we simplify this expression",
      "offset": 922.44,
      "duration": 3.72
    },
    {
      "lang": "en",
      "text": "mathematically this is negative one over",
      "offset": 924.66,
      "duration": 3.66
    },
    {
      "lang": "en",
      "text": "three of A and negative plus negative",
      "offset": 926.16,
      "duration": 4.22
    },
    {
      "lang": "en",
      "text": "one over three of B",
      "offset": 928.32,
      "duration": 5.459
    },
    {
      "lang": "en",
      "text": "plus negative 1 over 3 of c and so what",
      "offset": 930.38,
      "duration": 5.319
    },
    {
      "lang": "en",
      "text": "is D loss by D A it's just negative one",
      "offset": 933.779,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "over three",
      "offset": 935.699,
      "duration": 2.82
    },
    {
      "lang": "en",
      "text": "and so you can see that if we don't just",
      "offset": 936.779,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "have a b and c but we have 32 numbers",
      "offset": 938.519,
      "duration": 5.281
    },
    {
      "lang": "en",
      "text": "then D loss by D",
      "offset": 940.699,
      "duration": 4.541
    },
    {
      "lang": "en",
      "text": "um you know every one of those numbers",
      "offset": 943.8,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "is going to be one over N More generally",
      "offset": 945.24,
      "duration": 5.58
    },
    {
      "lang": "en",
      "text": "because n is the um the size of the",
      "offset": 947.399,
      "duration": 5.641
    },
    {
      "lang": "en",
      "text": "batch 32 in this case",
      "offset": 950.82,
      "duration": 4.98
    },
    {
      "lang": "en",
      "text": "so D loss by",
      "offset": 953.04,
      "duration": 6.78
    },
    {
      "lang": "en",
      "text": "um D Lock probs is negative 1 over n",
      "offset": 955.8,
      "duration": 6.3
    },
    {
      "lang": "en",
      "text": "in all these places",
      "offset": 959.82,
      "duration": 4.259
    },
    {
      "lang": "en",
      "text": "now what about the other elements inside",
      "offset": 962.1,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "lock problems because lock props is",
      "offset": 964.079,
      "duration": 3.721
    },
    {
      "lang": "en",
      "text": "large array you see that lock problems",
      "offset": 965.94,
      "duration": 5.22
    },
    {
      "lang": "en",
      "text": "at shape is 32 by 27. but only 32 of",
      "offset": 967.8,
      "duration": 5.94
    },
    {
      "lang": "en",
      "text": "them participate in the loss calculation",
      "offset": 971.16,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "so what's the derivative of all the",
      "offset": 973.74,
      "duration": 4.74
    },
    {
      "lang": "en",
      "text": "other most of the elements that do not",
      "offset": 975.959,
      "duration": 4.38
    },
    {
      "lang": "en",
      "text": "get plucked out here",
      "offset": 978.48,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "while their loss intuitively is zero",
      "offset": 980.339,
      "duration": 3.961
    },
    {
      "lang": "en",
      "text": "sorry they're gradient intuitively is",
      "offset": 982.079,
      "duration": 3.721
    },
    {
      "lang": "en",
      "text": "zero and that's because they did not",
      "offset": 984.3,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "participate in the loss",
      "offset": 985.8,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "so most of these numbers inside this",
      "offset": 987.42,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "tensor does not feed into the loss and",
      "offset": 989.639,
      "duration": 3.961
    },
    {
      "lang": "en",
      "text": "so if we were to change these numbers",
      "offset": 992.22,
      "duration": 3.9
    },
    {
      "lang": "en",
      "text": "then the loss doesn't change which is",
      "offset": 993.6,
      "duration": 5.159
    },
    {
      "lang": "en",
      "text": "the equivalent of way of saying that the",
      "offset": 996.12,
      "duration": 3.719
    },
    {
      "lang": "en",
      "text": "derivative of the loss with respect to",
      "offset": 998.759,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "them is zero they don't impact it",
      "offset": 999.839,
      "duration": 5.521
    },
    {
      "lang": "en",
      "text": "so here's a way to implement this",
      "offset": 1003.32,
      "duration": 4.199
    },
    {
      "lang": "en",
      "text": "derivative then we start out with",
      "offset": 1005.36,
      "duration": 5.46
    },
    {
      "lang": "en",
      "text": "torch.zeros of shape 32 by 27 or let's",
      "offset": 1007.519,
      "duration": 5.221
    },
    {
      "lang": "en",
      "text": "just say instead of doing this because",
      "offset": 1010.82,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "we don't want to hard code numbers let's",
      "offset": 1012.74,
      "duration": 4.26
    },
    {
      "lang": "en",
      "text": "do torch.zeros like",
      "offset": 1014.42,
      "duration": 4.62
    },
    {
      "lang": "en",
      "text": "block probs so basically this is going",
      "offset": 1017,
      "duration": 3.66
    },
    {
      "lang": "en",
      "text": "to create an array of zeros exactly in",
      "offset": 1019.04,
      "duration": 3.539
    },
    {
      "lang": "en",
      "text": "the shape of log probs",
      "offset": 1020.66,
      "duration": 4.44
    },
    {
      "lang": "en",
      "text": "and then we need to set the derivative",
      "offset": 1022.579,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "of negative 1 over n inside exactly",
      "offset": 1025.1,
      "duration": 4.859
    },
    {
      "lang": "en",
      "text": "these locations so here's what we can do",
      "offset": 1027.14,
      "duration": 5.699
    },
    {
      "lang": "en",
      "text": "the lock props indexed in The Identical",
      "offset": 1029.959,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "way",
      "offset": 1032.839,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "will be just set to negative one over",
      "offset": 1034.28,
      "duration": 5.519
    },
    {
      "lang": "en",
      "text": "zero divide n",
      "offset": 1036.439,
      "duration": 6.12
    },
    {
      "lang": "en",
      "text": "right just like we derived here",
      "offset": 1039.799,
      "duration": 6.061
    },
    {
      "lang": "en",
      "text": "so now let me erase all this reasoning",
      "offset": 1042.559,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "and then this is the candidate",
      "offset": 1045.86,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "derivative for D log props let's",
      "offset": 1047.12,
      "duration": 4.679
    },
    {
      "lang": "en",
      "text": "uncomment the first line and check that",
      "offset": 1049.94,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "this is correct",
      "offset": 1051.799,
      "duration": 7.921
    },
    {
      "lang": "en",
      "text": "okay so CMP ran and let's go back to CMP",
      "offset": 1054.26,
      "duration": 6.96
    },
    {
      "lang": "en",
      "text": "and you see that what it's doing is it's",
      "offset": 1059.72,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "calculating if",
      "offset": 1061.22,
      "duration": 4.86
    },
    {
      "lang": "en",
      "text": "the calculated value by us which is DT",
      "offset": 1062.84,
      "duration": 5.339
    },
    {
      "lang": "en",
      "text": "is exactly equal to T dot grad as",
      "offset": 1066.08,
      "duration": 4.979
    },
    {
      "lang": "en",
      "text": "calculated by pi torch and then this is",
      "offset": 1068.179,
      "duration": 4.5
    },
    {
      "lang": "en",
      "text": "making sure that all the elements are",
      "offset": 1071.059,
      "duration": 3.901
    },
    {
      "lang": "en",
      "text": "exactly equal and then converting this",
      "offset": 1072.679,
      "duration": 4.5
    },
    {
      "lang": "en",
      "text": "to a single Boolean value because we",
      "offset": 1074.96,
      "duration": 3.48
    },
    {
      "lang": "en",
      "text": "don't want the Boolean tensor we just",
      "offset": 1077.179,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "want to Boolean value",
      "offset": 1078.44,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "and then here we are making sure that",
      "offset": 1080.059,
      "duration": 4.261
    },
    {
      "lang": "en",
      "text": "okay if they're not exactly equal maybe",
      "offset": 1082.76,
      "duration": 3.24
    },
    {
      "lang": "en",
      "text": "they are approximately equal because of",
      "offset": 1084.32,
      "duration": 3.239
    },
    {
      "lang": "en",
      "text": "some floating Point issues but they're",
      "offset": 1086,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "very very close",
      "offset": 1087.559,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "so here we are using torch.allclose",
      "offset": 1089.12,
      "duration": 3.9
    },
    {
      "lang": "en",
      "text": "which has a little bit of a wiggle",
      "offset": 1090.919,
      "duration": 4.38
    },
    {
      "lang": "en",
      "text": "available because sometimes you can get",
      "offset": 1093.02,
      "duration": 4.74
    },
    {
      "lang": "en",
      "text": "very very close but if you use a",
      "offset": 1095.299,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "slightly different calculation because a",
      "offset": 1097.76,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "floating Point arithmetic you can get a",
      "offset": 1099.38,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "slightly different result so this is",
      "offset": 1102.559,
      "duration": 3.301
    },
    {
      "lang": "en",
      "text": "checking if you get an approximately",
      "offset": 1104.419,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "close result",
      "offset": 1105.86,
      "duration": 2.939
    },
    {
      "lang": "en",
      "text": "and then here we are checking the",
      "offset": 1107.299,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "maximum uh basically the value that has",
      "offset": 1108.799,
      "duration": 5.401
    },
    {
      "lang": "en",
      "text": "the highest difference and what is the",
      "offset": 1111.62,
      "duration": 4.26
    },
    {
      "lang": "en",
      "text": "difference in the absolute value",
      "offset": 1114.2,
      "duration": 3.66
    },
    {
      "lang": "en",
      "text": "difference between those two and so we",
      "offset": 1115.88,
      "duration": 3.48
    },
    {
      "lang": "en",
      "text": "are printing whether we have an exact",
      "offset": 1117.86,
      "duration": 4.199
    },
    {
      "lang": "en",
      "text": "equality an approximate equality and",
      "offset": 1119.36,
      "duration": 5.64
    },
    {
      "lang": "en",
      "text": "what is the largest difference",
      "offset": 1122.059,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "and so here",
      "offset": 1125,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "we see that we actually have exact",
      "offset": 1126.86,
      "duration": 3.42
    },
    {
      "lang": "en",
      "text": "equality and so therefore of course we",
      "offset": 1128.36,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "also have an approximate equality and",
      "offset": 1130.28,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "the maximum difference is exactly zero",
      "offset": 1132.44,
      "duration": 5.22
    },
    {
      "lang": "en",
      "text": "so basically our d-log props is exactly",
      "offset": 1134.6,
      "duration": 6.26
    },
    {
      "lang": "en",
      "text": "equal to what pytors calculated to be",
      "offset": 1137.66,
      "duration": 6.12
    },
    {
      "lang": "en",
      "text": "lockprops.grad in its back propagation",
      "offset": 1140.86,
      "duration": 5.679
    },
    {
      "lang": "en",
      "text": "so so far we're working pretty well okay",
      "offset": 1143.78,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "so let's now continue our back",
      "offset": 1146.539,
      "duration": 2.161
    },
    {
      "lang": "en",
      "text": "propagation",
      "offset": 1147.62,
      "duration": 2.939
    },
    {
      "lang": "en",
      "text": "we have that lock props depends on",
      "offset": 1148.7,
      "duration": 3.78
    },
    {
      "lang": "en",
      "text": "probes through a log",
      "offset": 1150.559,
      "duration": 3.721
    },
    {
      "lang": "en",
      "text": "so all the elements of probes are being",
      "offset": 1152.48,
      "duration": 5.1
    },
    {
      "lang": "en",
      "text": "element wise applied log to",
      "offset": 1154.28,
      "duration": 5.46
    },
    {
      "lang": "en",
      "text": "now if we want deep props then then",
      "offset": 1157.58,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "remember your micrograph training",
      "offset": 1159.74,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "we have like a log node it takes in",
      "offset": 1162.14,
      "duration": 5.64
    },
    {
      "lang": "en",
      "text": "probs and creates log probs and the",
      "offset": 1164.539,
      "duration": 5.461
    },
    {
      "lang": "en",
      "text": "props will be the local derivative of",
      "offset": 1167.78,
      "duration": 5.639
    },
    {
      "lang": "en",
      "text": "that individual Operation Log times the",
      "offset": 1170,
      "duration": 4.86
    },
    {
      "lang": "en",
      "text": "derivative loss with respect to its",
      "offset": 1173.419,
      "duration": 4.14
    },
    {
      "lang": "en",
      "text": "output which in this case is D log props",
      "offset": 1174.86,
      "duration": 4.74
    },
    {
      "lang": "en",
      "text": "so what is the local derivative of this",
      "offset": 1177.559,
      "duration": 3.961
    },
    {
      "lang": "en",
      "text": "operation well we are taking log element",
      "offset": 1179.6,
      "duration": 4.199
    },
    {
      "lang": "en",
      "text": "wise and we can come here and we can see",
      "offset": 1181.52,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "well from alpha is your friend that d by",
      "offset": 1183.799,
      "duration": 4.141
    },
    {
      "lang": "en",
      "text": "DX of log of x is just simply one of our",
      "offset": 1185.84,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "X",
      "offset": 1187.94,
      "duration": 3.72
    },
    {
      "lang": "en",
      "text": "so therefore in this case X is problems",
      "offset": 1188.96,
      "duration": 5.94
    },
    {
      "lang": "en",
      "text": "so we have d by DX is one over X which",
      "offset": 1191.66,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "is one of our probes and then this is",
      "offset": 1194.9,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "the local derivative and then times we",
      "offset": 1196.94,
      "duration": 3.239
    },
    {
      "lang": "en",
      "text": "want to chain it",
      "offset": 1198.98,
      "duration": 2.699
    },
    {
      "lang": "en",
      "text": "so this is chain rule",
      "offset": 1200.179,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "times do log props",
      "offset": 1201.679,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "let me uncomment this and let me run the",
      "offset": 1203.539,
      "duration": 5.341
    },
    {
      "lang": "en",
      "text": "cell in place and we see that the",
      "offset": 1206.24,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "derivative of props as we calculated",
      "offset": 1208.88,
      "duration": 4.02
    },
    {
      "lang": "en",
      "text": "here is exactly correct",
      "offset": 1210.32,
      "duration": 5.4
    },
    {
      "lang": "en",
      "text": "and so notice here how this works probes",
      "offset": 1212.9,
      "duration": 5.399
    },
    {
      "lang": "en",
      "text": "that are props is going to be inverted",
      "offset": 1215.72,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "and then element was multiplied here",
      "offset": 1218.299,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "so if your probes is very very close to",
      "offset": 1220.76,
      "duration": 4.26
    },
    {
      "lang": "en",
      "text": "one that means you are your network is",
      "offset": 1223.1,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "currently predicting the character",
      "offset": 1225.02,
      "duration": 3.18
    },
    {
      "lang": "en",
      "text": "correctly then this will become one over",
      "offset": 1226.1,
      "duration": 4.62
    },
    {
      "lang": "en",
      "text": "one and D log probes just gets passed",
      "offset": 1228.2,
      "duration": 3.42
    },
    {
      "lang": "en",
      "text": "through",
      "offset": 1230.72,
      "duration": 2.339
    },
    {
      "lang": "en",
      "text": "but if your probabilities are",
      "offset": 1231.62,
      "duration": 3.78
    },
    {
      "lang": "en",
      "text": "incorrectly assigned so if the correct",
      "offset": 1233.059,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "character here is getting a very low",
      "offset": 1235.4,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "probability then 1.0 dividing by it will",
      "offset": 1237.38,
      "duration": 5.88
    },
    {
      "lang": "en",
      "text": "boost this",
      "offset": 1241.64,
      "duration": 4.02
    },
    {
      "lang": "en",
      "text": "and then multiply by the log props so",
      "offset": 1243.26,
      "duration": 3.539
    },
    {
      "lang": "en",
      "text": "basically what this line is doing",
      "offset": 1245.66,
      "duration": 3.72
    },
    {
      "lang": "en",
      "text": "intuitively is it's taking the examples",
      "offset": 1246.799,
      "duration": 4.141
    },
    {
      "lang": "en",
      "text": "that have a very low probability",
      "offset": 1249.38,
      "duration": 3.48
    },
    {
      "lang": "en",
      "text": "currently assigned and it's boosting",
      "offset": 1250.94,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "their gradient uh you can you can look",
      "offset": 1252.86,
      "duration": 6.42
    },
    {
      "lang": "en",
      "text": "at it that way next up is Count some imp",
      "offset": 1255.02,
      "duration": 7.74
    },
    {
      "lang": "en",
      "text": "so we want the river of this now let me",
      "offset": 1259.28,
      "duration": 5.82
    },
    {
      "lang": "en",
      "text": "just pause here and kind of introduce",
      "offset": 1262.76,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "What's Happening Here in general because",
      "offset": 1265.1,
      "duration": 3.54
    },
    {
      "lang": "en",
      "text": "I know it's a little bit confusing we",
      "offset": 1266.84,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "have the locusts that come out of the",
      "offset": 1268.64,
      "duration": 3.3
    },
    {
      "lang": "en",
      "text": "neural nut here what I'm doing is I'm",
      "offset": 1269.84,
      "duration": 5.16
    },
    {
      "lang": "en",
      "text": "finding the maximum in each row and I'm",
      "offset": 1271.94,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "subtracting it for the purposes of",
      "offset": 1275,
      "duration": 3.24
    },
    {
      "lang": "en",
      "text": "numerical stability and we talked about",
      "offset": 1276.5,
      "duration": 4.02
    },
    {
      "lang": "en",
      "text": "how if you do not do this you run",
      "offset": 1278.24,
      "duration": 4.02
    },
    {
      "lang": "en",
      "text": "numerical issues if some of the logits",
      "offset": 1280.52,
      "duration": 4.019
    },
    {
      "lang": "en",
      "text": "take on two large values because we end",
      "offset": 1282.26,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "up exponentiating them",
      "offset": 1284.539,
      "duration": 3.721
    },
    {
      "lang": "en",
      "text": "so this is done just for safety",
      "offset": 1286.34,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "numerically then here's the",
      "offset": 1288.26,
      "duration": 4.34
    },
    {
      "lang": "en",
      "text": "exponentiation of all the sort of like",
      "offset": 1290.659,
      "duration": 5.161
    },
    {
      "lang": "en",
      "text": "logits to create our accounts and then",
      "offset": 1292.6,
      "duration": 5.439
    },
    {
      "lang": "en",
      "text": "we want to take the some of these counts",
      "offset": 1295.82,
      "duration": 4.44
    },
    {
      "lang": "en",
      "text": "and normalize so that all of the probes",
      "offset": 1298.039,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "sum to one",
      "offset": 1300.26,
      "duration": 3.299
    },
    {
      "lang": "en",
      "text": "now here instead of using one over count",
      "offset": 1301.64,
      "duration": 4.38
    },
    {
      "lang": "en",
      "text": "sum I use uh raised to the power of",
      "offset": 1303.559,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "negative one mathematically they are",
      "offset": 1306.02,
      "duration": 3.3
    },
    {
      "lang": "en",
      "text": "identical I just found that there's",
      "offset": 1307.64,
      "duration": 2.82
    },
    {
      "lang": "en",
      "text": "something wrong with the pytorch",
      "offset": 1309.32,
      "duration": 2.7
    },
    {
      "lang": "en",
      "text": "implementation of the backward pass of",
      "offset": 1310.46,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "division",
      "offset": 1312.02,
      "duration": 3.96
    },
    {
      "lang": "en",
      "text": "um and it gives like a real result but",
      "offset": 1313.46,
      "duration": 4.68
    },
    {
      "lang": "en",
      "text": "that doesn't happen for star star native",
      "offset": 1315.98,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "one that's why I'm using this formula",
      "offset": 1318.14,
      "duration": 3.779
    },
    {
      "lang": "en",
      "text": "instead but basically all that's",
      "offset": 1319.82,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "happening here is we got the logits",
      "offset": 1321.919,
      "duration": 3.541
    },
    {
      "lang": "en",
      "text": "we're going to exponentiate all of them",
      "offset": 1324.14,
      "duration": 3.539
    },
    {
      "lang": "en",
      "text": "and want to normalize the counts to",
      "offset": 1325.46,
      "duration": 4.26
    },
    {
      "lang": "en",
      "text": "create our probabilities it's just that",
      "offset": 1327.679,
      "duration": 4.86
    },
    {
      "lang": "en",
      "text": "it's happening across multiple lines",
      "offset": 1329.72,
      "duration": 4.5
    },
    {
      "lang": "en",
      "text": "so now",
      "offset": 1332.539,
      "duration": 4.681
    },
    {
      "lang": "en",
      "text": "here",
      "offset": 1334.22,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "we want to First Take the derivative we",
      "offset": 1337.7,
      "duration": 4.14
    },
    {
      "lang": "en",
      "text": "want to back propagate into account",
      "offset": 1340.7,
      "duration": 3.719
    },
    {
      "lang": "en",
      "text": "sumiv and then into counts as well",
      "offset": 1341.84,
      "duration": 6.719
    },
    {
      "lang": "en",
      "text": "so what should be the count sum M now we",
      "offset": 1344.419,
      "duration": 5.521
    },
    {
      "lang": "en",
      "text": "actually have to be careful here because",
      "offset": 1348.559,
      "duration": 3.661
    },
    {
      "lang": "en",
      "text": "we have to scrutinize and be careful",
      "offset": 1349.94,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "with the shapes so counts that shape and",
      "offset": 1352.22,
      "duration": 7.02
    },
    {
      "lang": "en",
      "text": "then count some inverse shape",
      "offset": 1355.94,
      "duration": 4.68
    },
    {
      "lang": "en",
      "text": "are different",
      "offset": 1359.24,
      "duration": 4.2
    },
    {
      "lang": "en",
      "text": "so in particular counts as 32 by 27 but",
      "offset": 1360.62,
      "duration": 6.539
    },
    {
      "lang": "en",
      "text": "this count sum m is 32 by 1. and so in",
      "offset": 1363.44,
      "duration": 5.7
    },
    {
      "lang": "en",
      "text": "this multiplication here we also have an",
      "offset": 1367.159,
      "duration": 4.861
    },
    {
      "lang": "en",
      "text": "implicit broadcasting that pytorch will",
      "offset": 1369.14,
      "duration": 4.62
    },
    {
      "lang": "en",
      "text": "do because it needs to take this column",
      "offset": 1372.02,
      "duration": 3.659
    },
    {
      "lang": "en",
      "text": "tensor of 32 numbers and replicate it",
      "offset": 1373.76,
      "duration": 4.74
    },
    {
      "lang": "en",
      "text": "horizontally 27 times to align these two",
      "offset": 1375.679,
      "duration": 4.74
    },
    {
      "lang": "en",
      "text": "tensors so it can do an element twice",
      "offset": 1378.5,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "multiply",
      "offset": 1380.419,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "so really what this looks like is the",
      "offset": 1381.62,
      "duration": 4.679
    },
    {
      "lang": "en",
      "text": "following using a toy example again",
      "offset": 1383.539,
      "duration": 4.741
    },
    {
      "lang": "en",
      "text": "what we really have here is just props",
      "offset": 1386.299,
      "duration": 3.961
    },
    {
      "lang": "en",
      "text": "is counts times conservative so it's a C",
      "offset": 1388.28,
      "duration": 3.48
    },
    {
      "lang": "en",
      "text": "equals a times B",
      "offset": 1390.26,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "but a is 3 by 3 and b is just three by",
      "offset": 1391.76,
      "duration": 5.82
    },
    {
      "lang": "en",
      "text": "one a column tensor and so pytorch",
      "offset": 1395.059,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "internally replicated this elements of B",
      "offset": 1397.58,
      "duration": 4.86
    },
    {
      "lang": "en",
      "text": "and it did that across all the columns",
      "offset": 1399.86,
      "duration": 4.679
    },
    {
      "lang": "en",
      "text": "so for example B1 which is the first",
      "offset": 1402.44,
      "duration": 3.96
    },
    {
      "lang": "en",
      "text": "element of B would be replicated here",
      "offset": 1404.539,
      "duration": 3.241
    },
    {
      "lang": "en",
      "text": "across all the columns in this",
      "offset": 1406.4,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "multiplication",
      "offset": 1407.78,
      "duration": 3.24
    },
    {
      "lang": "en",
      "text": "and now we're trying to back propagate",
      "offset": 1409.28,
      "duration": 4.92
    },
    {
      "lang": "en",
      "text": "through this operation to count some m",
      "offset": 1411.02,
      "duration": 4.86
    },
    {
      "lang": "en",
      "text": "so when we're calculating this",
      "offset": 1414.2,
      "duration": 3.18
    },
    {
      "lang": "en",
      "text": "derivative",
      "offset": 1415.88,
      "duration": 3.48
    },
    {
      "lang": "en",
      "text": "it's important to realize that these two",
      "offset": 1417.38,
      "duration": 4.38
    },
    {
      "lang": "en",
      "text": "this looks like a single operation but",
      "offset": 1419.36,
      "duration": 4.679
    },
    {
      "lang": "en",
      "text": "actually is two operations applied",
      "offset": 1421.76,
      "duration": 4.5
    },
    {
      "lang": "en",
      "text": "sequentially the first operation that",
      "offset": 1424.039,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "pytorch did is it took this column",
      "offset": 1426.26,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "tensor and replicated it across all the",
      "offset": 1428.36,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "um across all the columns basically 27",
      "offset": 1432.02,
      "duration": 3.899
    },
    {
      "lang": "en",
      "text": "times so that's the first operation it's",
      "offset": 1434.12,
      "duration": 3.419
    },
    {
      "lang": "en",
      "text": "a replication and then the second",
      "offset": 1435.919,
      "duration": 3.961
    },
    {
      "lang": "en",
      "text": "operation is the multiplication so let's",
      "offset": 1437.539,
      "duration": 3.541
    },
    {
      "lang": "en",
      "text": "first background through the",
      "offset": 1439.88,
      "duration": 2.7
    },
    {
      "lang": "en",
      "text": "multiplication",
      "offset": 1441.08,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "if these two arrays are of the same size",
      "offset": 1442.58,
      "duration": 5.7
    },
    {
      "lang": "en",
      "text": "and we just have a and b of both of them",
      "offset": 1445.64,
      "duration": 5.519
    },
    {
      "lang": "en",
      "text": "three by three then how do we mult how",
      "offset": 1448.28,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "do we back propagate through a",
      "offset": 1451.159,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "multiplication so if we just have",
      "offset": 1452.12,
      "duration": 4.02
    },
    {
      "lang": "en",
      "text": "scalars and not tensors then if you have",
      "offset": 1454.039,
      "duration": 5.281
    },
    {
      "lang": "en",
      "text": "C equals a times B then what is uh the",
      "offset": 1456.14,
      "duration": 5.399
    },
    {
      "lang": "en",
      "text": "order of the of C with respect to B well",
      "offset": 1459.32,
      "duration": 4.14
    },
    {
      "lang": "en",
      "text": "it's just a and so that's the local",
      "offset": 1461.539,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "derivative",
      "offset": 1463.46,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "so here in our case undoing the",
      "offset": 1464.659,
      "duration": 4.38
    },
    {
      "lang": "en",
      "text": "multiplication and back propagating",
      "offset": 1467.059,
      "duration": 3.48
    },
    {
      "lang": "en",
      "text": "through just the multiplication itself",
      "offset": 1469.039,
      "duration": 3.961
    },
    {
      "lang": "en",
      "text": "which is element wise is going to be the",
      "offset": 1470.539,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "local derivative which in this case is",
      "offset": 1473,
      "duration": 7.26
    },
    {
      "lang": "en",
      "text": "simply counts because counts is the a",
      "offset": 1476.059,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "so this is the local derivative and then",
      "offset": 1480.26,
      "duration": 5.88
    },
    {
      "lang": "en",
      "text": "times because the chain rule D props",
      "offset": 1482.059,
      "duration": 6.541
    },
    {
      "lang": "en",
      "text": "so this here is the derivative or the",
      "offset": 1486.14,
      "duration": 4.62
    },
    {
      "lang": "en",
      "text": "gradient but with respect to replicated",
      "offset": 1488.6,
      "duration": 3.48
    },
    {
      "lang": "en",
      "text": "B",
      "offset": 1490.76,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "but we don't have a replicated B we just",
      "offset": 1492.08,
      "duration": 4.38
    },
    {
      "lang": "en",
      "text": "have a single B column so how do we now",
      "offset": 1494.12,
      "duration": 5.22
    },
    {
      "lang": "en",
      "text": "back propagate through the replication",
      "offset": 1496.46,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "and intuitively this B1 is the same",
      "offset": 1499.34,
      "duration": 4.68
    },
    {
      "lang": "en",
      "text": "variable and it's just reused multiple",
      "offset": 1502.22,
      "duration": 2.699
    },
    {
      "lang": "en",
      "text": "times",
      "offset": 1504.02,
      "duration": 3.3
    },
    {
      "lang": "en",
      "text": "and so you can look at it",
      "offset": 1504.919,
      "duration": 4.441
    },
    {
      "lang": "en",
      "text": "as being equivalent to a case we've",
      "offset": 1507.32,
      "duration": 3.66
    },
    {
      "lang": "en",
      "text": "encountered in micrograd",
      "offset": 1509.36,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "and so here I'm just pulling out a",
      "offset": 1510.98,
      "duration": 3.96
    },
    {
      "lang": "en",
      "text": "random graph we used in micrograd we had",
      "offset": 1512.72,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "an example where a single node",
      "offset": 1514.94,
      "duration": 4.92
    },
    {
      "lang": "en",
      "text": "has its output feeding into two branches",
      "offset": 1517.76,
      "duration": 4.98
    },
    {
      "lang": "en",
      "text": "of basically the graph until the last",
      "offset": 1519.86,
      "duration": 5.16
    },
    {
      "lang": "en",
      "text": "function and we're talking about how the",
      "offset": 1522.74,
      "duration": 3.539
    },
    {
      "lang": "en",
      "text": "correct thing to do in the backward pass",
      "offset": 1525.02,
      "duration": 4.019
    },
    {
      "lang": "en",
      "text": "is we need to sum all the gradients that",
      "offset": 1526.279,
      "duration": 5.581
    },
    {
      "lang": "en",
      "text": "arrive at any one node so across these",
      "offset": 1529.039,
      "duration": 4.38
    },
    {
      "lang": "en",
      "text": "different branches the gradients would",
      "offset": 1531.86,
      "duration": 2.939
    },
    {
      "lang": "en",
      "text": "sum",
      "offset": 1533.419,
      "duration": 3.901
    },
    {
      "lang": "en",
      "text": "so if a node is used multiple times the",
      "offset": 1534.799,
      "duration": 4.98
    },
    {
      "lang": "en",
      "text": "gradients for all of its uses sum during",
      "offset": 1537.32,
      "duration": 4.14
    },
    {
      "lang": "en",
      "text": "back propagation",
      "offset": 1539.779,
      "duration": 4.26
    },
    {
      "lang": "en",
      "text": "so here B1 is used multiple times in all",
      "offset": 1541.46,
      "duration": 4.199
    },
    {
      "lang": "en",
      "text": "these columns and therefore the right",
      "offset": 1544.039,
      "duration": 4.341
    },
    {
      "lang": "en",
      "text": "thing to do here is to sum",
      "offset": 1545.659,
      "duration": 5.821
    },
    {
      "lang": "en",
      "text": "horizontally across all the rows so I'm",
      "offset": 1548.38,
      "duration": 4.36
    },
    {
      "lang": "en",
      "text": "going to sum in",
      "offset": 1551.48,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "Dimension one but we want to retain this",
      "offset": 1552.74,
      "duration": 5.58
    },
    {
      "lang": "en",
      "text": "Dimension so that the uh so that counts",
      "offset": 1555.799,
      "duration": 4.5
    },
    {
      "lang": "en",
      "text": "some end and its gradient are going to",
      "offset": 1558.32,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "be exactly the same shape so we want to",
      "offset": 1560.299,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "make sure that we keep them as true so",
      "offset": 1562.4,
      "duration": 4.74
    },
    {
      "lang": "en",
      "text": "we don't lose this dimension and this",
      "offset": 1564.86,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "will make the count sum M be exactly",
      "offset": 1567.14,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "shape 32 by 1.",
      "offset": 1568.94,
      "duration": 5.82
    },
    {
      "lang": "en",
      "text": "so revealing this comparison as well and",
      "offset": 1571.46,
      "duration": 5.819
    },
    {
      "lang": "en",
      "text": "running this we see that we get an exact",
      "offset": 1574.76,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "match",
      "offset": 1577.279,
      "duration": 4.981
    },
    {
      "lang": "en",
      "text": "so this derivative is exactly correct",
      "offset": 1578.36,
      "duration": 6.059
    },
    {
      "lang": "en",
      "text": "and let me erase",
      "offset": 1582.26,
      "duration": 4.62
    },
    {
      "lang": "en",
      "text": "this now let's also back propagate into",
      "offset": 1584.419,
      "duration": 5.161
    },
    {
      "lang": "en",
      "text": "counts which is the other variable here",
      "offset": 1586.88,
      "duration": 5.279
    },
    {
      "lang": "en",
      "text": "to create probes so from props to count",
      "offset": 1589.58,
      "duration": 4.199
    },
    {
      "lang": "en",
      "text": "some INF we just did that let's go into",
      "offset": 1592.159,
      "duration": 3.241
    },
    {
      "lang": "en",
      "text": "counts as well",
      "offset": 1593.779,
      "duration": 5.601
    },
    {
      "lang": "en",
      "text": "so decounts will be",
      "offset": 1595.4,
      "duration": 3.98
    },
    {
      "lang": "en",
      "text": "the chances are a so DC by d a is just B",
      "offset": 1599.779,
      "duration": 7.741
    },
    {
      "lang": "en",
      "text": "so therefore it's count summative",
      "offset": 1603.62,
      "duration": 7.62
    },
    {
      "lang": "en",
      "text": "um and then times chain rule the props",
      "offset": 1607.52,
      "duration": 6.96
    },
    {
      "lang": "en",
      "text": "now councilman is three two by One D",
      "offset": 1611.24,
      "duration": 6.66
    },
    {
      "lang": "en",
      "text": "probs is 32 by 27.",
      "offset": 1614.48,
      "duration": 4.679
    },
    {
      "lang": "en",
      "text": "so",
      "offset": 1617.9,
      "duration": 4.44
    },
    {
      "lang": "en",
      "text": "um those will broadcast fine and will",
      "offset": 1619.159,
      "duration": 5.281
    },
    {
      "lang": "en",
      "text": "give us decounts there's no additional",
      "offset": 1622.34,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "summation required here",
      "offset": 1624.44,
      "duration": 4.44
    },
    {
      "lang": "en",
      "text": "um there will be a broadcasting that",
      "offset": 1626.659,
      "duration": 4.441
    },
    {
      "lang": "en",
      "text": "happens in this multiply here because",
      "offset": 1628.88,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "count some M needs to be replicated",
      "offset": 1631.1,
      "duration": 5.4
    },
    {
      "lang": "en",
      "text": "again to correctly multiply D props but",
      "offset": 1632.72,
      "duration": 5.939
    },
    {
      "lang": "en",
      "text": "that's going to give the correct result",
      "offset": 1636.5,
      "duration": 4.38
    },
    {
      "lang": "en",
      "text": "so as far as the single operation is",
      "offset": 1638.659,
      "duration": 4.62
    },
    {
      "lang": "en",
      "text": "concerned so we back probably go from",
      "offset": 1640.88,
      "duration": 4.5
    },
    {
      "lang": "en",
      "text": "props to counts but we can't actually",
      "offset": 1643.279,
      "duration": 6.121
    },
    {
      "lang": "en",
      "text": "check the derivative counts uh I have it",
      "offset": 1645.38,
      "duration": 6.06
    },
    {
      "lang": "en",
      "text": "much later on and the reason for that is",
      "offset": 1649.4,
      "duration": 4.98
    },
    {
      "lang": "en",
      "text": "because count sum in depends on counts",
      "offset": 1651.44,
      "duration": 4.739
    },
    {
      "lang": "en",
      "text": "and so there's a second Branch here that",
      "offset": 1654.38,
      "duration": 3.96
    },
    {
      "lang": "en",
      "text": "we have to finish because can't summon",
      "offset": 1656.179,
      "duration": 4.141
    },
    {
      "lang": "en",
      "text": "back propagates into account sum and",
      "offset": 1658.34,
      "duration": 3.78
    },
    {
      "lang": "en",
      "text": "count sum will buy properly into counts",
      "offset": 1660.32,
      "duration": 4.2
    },
    {
      "lang": "en",
      "text": "and so counts is a node that is being",
      "offset": 1662.12,
      "duration": 4.74
    },
    {
      "lang": "en",
      "text": "used twice it's used right here in two",
      "offset": 1664.52,
      "duration": 3.72
    },
    {
      "lang": "en",
      "text": "props and it goes through this other",
      "offset": 1666.86,
      "duration": 3.48
    },
    {
      "lang": "en",
      "text": "Branch through count summative",
      "offset": 1668.24,
      "duration": 4.38
    },
    {
      "lang": "en",
      "text": "so even though we've calculated the",
      "offset": 1670.34,
      "duration": 3.78
    },
    {
      "lang": "en",
      "text": "first contribution of it we still have",
      "offset": 1672.62,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "to calculate the second contribution of",
      "offset": 1674.12,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "it later",
      "offset": 1675.74,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "okay so we're continuing with this",
      "offset": 1677,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "Branch we have the derivative for count",
      "offset": 1678.62,
      "duration": 3.72
    },
    {
      "lang": "en",
      "text": "sum if now we want the derivative of",
      "offset": 1680.6,
      "duration": 5.34
    },
    {
      "lang": "en",
      "text": "count sum so D count sum equals what is",
      "offset": 1682.34,
      "duration": 4.98
    },
    {
      "lang": "en",
      "text": "the local derivative of this operation",
      "offset": 1685.94,
      "duration": 3.78
    },
    {
      "lang": "en",
      "text": "so this is basically an element wise one",
      "offset": 1687.32,
      "duration": 4.62
    },
    {
      "lang": "en",
      "text": "over counts sum",
      "offset": 1689.72,
      "duration": 3.9
    },
    {
      "lang": "en",
      "text": "so count sum raised to the power of",
      "offset": 1691.94,
      "duration": 3.479
    },
    {
      "lang": "en",
      "text": "negative one is the same as one over",
      "offset": 1693.62,
      "duration": 4.2
    },
    {
      "lang": "en",
      "text": "count sum if we go to all from alpha we",
      "offset": 1695.419,
      "duration": 4.86
    },
    {
      "lang": "en",
      "text": "see that x to the negative one D by D by",
      "offset": 1697.82,
      "duration": 5.339
    },
    {
      "lang": "en",
      "text": "D by DX of it is basically Negative X to",
      "offset": 1700.279,
      "duration": 5.701
    },
    {
      "lang": "en",
      "text": "the negative 2. right one negative one",
      "offset": 1703.159,
      "duration": 4.38
    },
    {
      "lang": "en",
      "text": "over squared is the same as Negative X",
      "offset": 1705.98,
      "duration": 3.24
    },
    {
      "lang": "en",
      "text": "to the negative two",
      "offset": 1707.539,
      "duration": 5.101
    },
    {
      "lang": "en",
      "text": "so D count sum here will be local",
      "offset": 1709.22,
      "duration": 6.42
    },
    {
      "lang": "en",
      "text": "derivative is going to be negative",
      "offset": 1712.64,
      "duration": 3.539
    },
    {
      "lang": "en",
      "text": "um",
      "offset": 1715.64,
      "duration": 3.779
    },
    {
      "lang": "en",
      "text": "counts sum to the negative two that's",
      "offset": 1716.179,
      "duration": 5.701
    },
    {
      "lang": "en",
      "text": "the local derivative times chain rule",
      "offset": 1719.419,
      "duration": 6.921
    },
    {
      "lang": "en",
      "text": "which is D count sum in",
      "offset": 1721.88,
      "duration": 4.46
    },
    {
      "lang": "en",
      "text": "so that's D count sum",
      "offset": 1726.74,
      "duration": 4.919
    },
    {
      "lang": "en",
      "text": "let's uncomment this and check that I am",
      "offset": 1729.26,
      "duration": 6.12
    },
    {
      "lang": "en",
      "text": "correct okay so we have perfect equality",
      "offset": 1731.659,
      "duration": 6.421
    },
    {
      "lang": "en",
      "text": "and there's no sketchiness going on here",
      "offset": 1735.38,
      "duration": 4.38
    },
    {
      "lang": "en",
      "text": "with any shapes because these are of the",
      "offset": 1738.08,
      "duration": 4.26
    },
    {
      "lang": "en",
      "text": "same shape okay next up we want to back",
      "offset": 1739.76,
      "duration": 4.38
    },
    {
      "lang": "en",
      "text": "propagate through this line we have that",
      "offset": 1742.34,
      "duration": 4.98
    },
    {
      "lang": "en",
      "text": "count sum it's count.sum along the rows",
      "offset": 1744.14,
      "duration": 5.279
    },
    {
      "lang": "en",
      "text": "so I wrote out",
      "offset": 1747.32,
      "duration": 4.44
    },
    {
      "lang": "en",
      "text": "um some help here we have to keep in",
      "offset": 1749.419,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "mind that counts of course is 32 by 27",
      "offset": 1751.76,
      "duration": 5.519
    },
    {
      "lang": "en",
      "text": "and count sum is 32 by 1. so in this",
      "offset": 1753.74,
      "duration": 6.179
    },
    {
      "lang": "en",
      "text": "back propagation we need to take this",
      "offset": 1757.279,
      "duration": 5.221
    },
    {
      "lang": "en",
      "text": "column of derivatives and transform it",
      "offset": 1759.919,
      "duration": 4.62
    },
    {
      "lang": "en",
      "text": "into a array of derivatives",
      "offset": 1762.5,
      "duration": 4.2
    },
    {
      "lang": "en",
      "text": "two-dimensional array",
      "offset": 1764.539,
      "duration": 4.441
    },
    {
      "lang": "en",
      "text": "so what is this operation doing we're",
      "offset": 1766.7,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "taking in some kind of an input like say",
      "offset": 1768.98,
      "duration": 3.78
    },
    {
      "lang": "en",
      "text": "a three by three Matrix a and we are",
      "offset": 1771.02,
      "duration": 4.98
    },
    {
      "lang": "en",
      "text": "summing up the rows into a column tells",
      "offset": 1772.76,
      "duration": 7.139
    },
    {
      "lang": "en",
      "text": "her B1 b2b3 that is basically this",
      "offset": 1776,
      "duration": 5.88
    },
    {
      "lang": "en",
      "text": "so now we have the derivatives of the",
      "offset": 1779.899,
      "duration": 4.38
    },
    {
      "lang": "en",
      "text": "loss with respect to B all the elements",
      "offset": 1781.88,
      "duration": 3.24
    },
    {
      "lang": "en",
      "text": "of B",
      "offset": 1784.279,
      "duration": 2.76
    },
    {
      "lang": "en",
      "text": "and now we want to derivative loss with",
      "offset": 1785.12,
      "duration": 4.62
    },
    {
      "lang": "en",
      "text": "respect to all these little A's",
      "offset": 1787.039,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "so how do the B's depend on the ace is",
      "offset": 1789.74,
      "duration": 4.38
    },
    {
      "lang": "en",
      "text": "basically what we're after what is the",
      "offset": 1792.799,
      "duration": 3.541
    },
    {
      "lang": "en",
      "text": "local derivative of this operation",
      "offset": 1794.12,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "well we can see here that B1 only",
      "offset": 1796.34,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "depends on these elements here the",
      "offset": 1798.2,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "derivative of B1 with respect to all of",
      "offset": 1801.62,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "these elements down here is zero but for",
      "offset": 1803.24,
      "duration": 6.059
    },
    {
      "lang": "en",
      "text": "these elements here like a11 a12 Etc the",
      "offset": 1806.419,
      "duration": 6.721
    },
    {
      "lang": "en",
      "text": "local derivative is one right so DB 1 by",
      "offset": 1809.299,
      "duration": 6.901
    },
    {
      "lang": "en",
      "text": "D A 1 1 for example is one so it's one",
      "offset": 1813.14,
      "duration": 4.86
    },
    {
      "lang": "en",
      "text": "one and one",
      "offset": 1816.2,
      "duration": 3.719
    },
    {
      "lang": "en",
      "text": "so when we have the derivative of loss",
      "offset": 1818,
      "duration": 3.48
    },
    {
      "lang": "en",
      "text": "with respect to B1",
      "offset": 1819.919,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "did a local derivative of B1 with",
      "offset": 1821.48,
      "duration": 4.14
    },
    {
      "lang": "en",
      "text": "respect to these inputs is zeros here",
      "offset": 1823.52,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "but it's one on these guys",
      "offset": 1825.62,
      "duration": 4.26
    },
    {
      "lang": "en",
      "text": "so in the chain rule",
      "offset": 1827.84,
      "duration": 4.579
    },
    {
      "lang": "en",
      "text": "we have the local derivative uh times",
      "offset": 1829.88,
      "duration": 5.94
    },
    {
      "lang": "en",
      "text": "sort of the derivative of B1 and so",
      "offset": 1832.419,
      "duration": 5.441
    },
    {
      "lang": "en",
      "text": "because the local derivative is one on",
      "offset": 1835.82,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "these three elements the look of them",
      "offset": 1837.86,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "are multiplying the derivative of B1",
      "offset": 1839.899,
      "duration": 5.701
    },
    {
      "lang": "en",
      "text": "will just be the derivative of B1 and so",
      "offset": 1841.7,
      "duration": 5.88
    },
    {
      "lang": "en",
      "text": "you can look at it as a router basically",
      "offset": 1845.6,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "an addition is a router of gradient",
      "offset": 1847.58,
      "duration": 4.44
    },
    {
      "lang": "en",
      "text": "whatever gradient comes from above it",
      "offset": 1850.399,
      "duration": 3.421
    },
    {
      "lang": "en",
      "text": "just gets routed equally to all the",
      "offset": 1852.02,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "elements that participate in that",
      "offset": 1853.82,
      "duration": 2.28
    },
    {
      "lang": "en",
      "text": "addition",
      "offset": 1855.14,
      "duration": 3.18
    },
    {
      "lang": "en",
      "text": "so in this case the derivative of B1",
      "offset": 1856.1,
      "duration": 4.74
    },
    {
      "lang": "en",
      "text": "will just flow equally to the derivative",
      "offset": 1858.32,
      "duration": 4.979
    },
    {
      "lang": "en",
      "text": "of a11 a12 and a13",
      "offset": 1860.84,
      "duration": 4.26
    },
    {
      "lang": "en",
      "text": ". so if we have a derivative of all the",
      "offset": 1863.299,
      "duration": 4.441
    },
    {
      "lang": "en",
      "text": "elements of B and in this column tensor",
      "offset": 1865.1,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "which is D counts sum that we've",
      "offset": 1867.74,
      "duration": 4.2
    },
    {
      "lang": "en",
      "text": "calculated just now",
      "offset": 1870.38,
      "duration": 3.96
    },
    {
      "lang": "en",
      "text": "we basically see that what that amounts",
      "offset": 1871.94,
      "duration": 5.219
    },
    {
      "lang": "en",
      "text": "to is all of these are now flowing to",
      "offset": 1874.34,
      "duration": 4.98
    },
    {
      "lang": "en",
      "text": "all these elements of a and they're",
      "offset": 1877.159,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "doing that horizontally",
      "offset": 1879.32,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "so basically what we want is we want to",
      "offset": 1881,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "take the decount sum of size 30 by 1 and",
      "offset": 1882.919,
      "duration": 5.301
    },
    {
      "lang": "en",
      "text": "we just want to replicate it 27 times",
      "offset": 1886.039,
      "duration": 6.301
    },
    {
      "lang": "en",
      "text": "horizontally to create 32 by 27 array",
      "offset": 1888.22,
      "duration": 5.62
    },
    {
      "lang": "en",
      "text": "so there's many ways to implement this",
      "offset": 1892.34,
      "duration": 2.76
    },
    {
      "lang": "en",
      "text": "operation you could of course just",
      "offset": 1893.84,
      "duration": 3.78
    },
    {
      "lang": "en",
      "text": "replicate the tensor but I think maybe",
      "offset": 1895.1,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "one clean one is that the counts is",
      "offset": 1897.62,
      "duration": 6.12
    },
    {
      "lang": "en",
      "text": "simply torch dot once like",
      "offset": 1900.38,
      "duration": 5.58
    },
    {
      "lang": "en",
      "text": "so just an two-dimensional arrays of",
      "offset": 1903.74,
      "duration": 5.939
    },
    {
      "lang": "en",
      "text": "ones in the shape of counts so 32 by 27",
      "offset": 1905.96,
      "duration": 7.68
    },
    {
      "lang": "en",
      "text": "times D counts sum so this way we're",
      "offset": 1909.679,
      "duration": 6.961
    },
    {
      "lang": "en",
      "text": "letting the broadcasting here basically",
      "offset": 1913.64,
      "duration": 4.5
    },
    {
      "lang": "en",
      "text": "implement the replication you can look",
      "offset": 1916.64,
      "duration": 2.759
    },
    {
      "lang": "en",
      "text": "at it that way",
      "offset": 1918.14,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "but then we have to also be careful",
      "offset": 1919.399,
      "duration": 5.64
    },
    {
      "lang": "en",
      "text": "because decounts was already calculated",
      "offset": 1922.46,
      "duration": 5.64
    },
    {
      "lang": "en",
      "text": "we calculated earlier here and that was",
      "offset": 1925.039,
      "duration": 4.5
    },
    {
      "lang": "en",
      "text": "just the first branch and we're now",
      "offset": 1928.1,
      "duration": 3.059
    },
    {
      "lang": "en",
      "text": "finishing the second Branch so we need",
      "offset": 1929.539,
      "duration": 3.961
    },
    {
      "lang": "en",
      "text": "to make sure that these gradients add so",
      "offset": 1931.159,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "plus equals",
      "offset": 1933.5,
      "duration": 2.82
    },
    {
      "lang": "en",
      "text": "and then here",
      "offset": 1934.76,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "um let's comment out the comparison and",
      "offset": 1936.32,
      "duration": 6.9
    },
    {
      "lang": "en",
      "text": "let's make sure crossing fingers that we",
      "offset": 1940.52,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "have the correct result so pytorch",
      "offset": 1943.22,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "agrees with us on this gradient as well",
      "offset": 1945.32,
      "duration": 4.5
    },
    {
      "lang": "en",
      "text": "okay hopefully we're getting a hang of",
      "offset": 1948.02,
      "duration": 4.92
    },
    {
      "lang": "en",
      "text": "this now counts as an element-wise X of",
      "offset": 1949.82,
      "duration": 6.839
    },
    {
      "lang": "en",
      "text": "Norm legits so now we want D Norm logits",
      "offset": 1952.94,
      "duration": 5.16
    },
    {
      "lang": "en",
      "text": "and because it's an element price",
      "offset": 1956.659,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "operation everything is very simple what",
      "offset": 1958.1,
      "duration": 3.42
    },
    {
      "lang": "en",
      "text": "is the local derivative of e to the X",
      "offset": 1960.02,
      "duration": 5.159
    },
    {
      "lang": "en",
      "text": "it's famously just e to the x so this is",
      "offset": 1961.52,
      "duration": 6.379
    },
    {
      "lang": "en",
      "text": "the local derivative",
      "offset": 1965.179,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "that is the local derivative now we",
      "offset": 1968.48,
      "duration": 3.179
    },
    {
      "lang": "en",
      "text": "already calculated it and it's inside",
      "offset": 1970.279,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "counts so we may as well potentially",
      "offset": 1971.659,
      "duration": 3.661
    },
    {
      "lang": "en",
      "text": "just reuse counts that is the local",
      "offset": 1973.64,
      "duration": 2.58
    },
    {
      "lang": "en",
      "text": "derivative",
      "offset": 1975.32,
      "duration": 5.42
    },
    {
      "lang": "en",
      "text": "times uh D counts",
      "offset": 1976.22,
      "duration": 4.52
    },
    {
      "lang": "en",
      "text": "funny as that looks constant decount is",
      "offset": 1981.919,
      "duration": 5.101
    },
    {
      "lang": "en",
      "text": "derivative on the normal objects and now",
      "offset": 1984.32,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "let's erase this and let's verify and it",
      "offset": 1987.02,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "looks good",
      "offset": 1990.08,
      "duration": 2.3
    },
    {
      "lang": "en",
      "text": "so that's uh normal agents",
      "offset": 1992.419,
      "duration": 5.1
    },
    {
      "lang": "en",
      "text": "okay so we are here on this line now the",
      "offset": 1994.88,
      "duration": 4.019
    },
    {
      "lang": "en",
      "text": "normal objects",
      "offset": 1997.519,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "we have that and we're trying to",
      "offset": 1998.899,
      "duration": 3.78
    },
    {
      "lang": "en",
      "text": "calculate the logits and deloget Maxes",
      "offset": 2000.519,
      "duration": 4.861
    },
    {
      "lang": "en",
      "text": "so back propagating through this line",
      "offset": 2002.679,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "now we have to be careful here because",
      "offset": 2005.38,
      "duration": 3.72
    },
    {
      "lang": "en",
      "text": "the shapes again are not the same and so",
      "offset": 2006.76,
      "duration": 4.139
    },
    {
      "lang": "en",
      "text": "there's an implicit broadcasting",
      "offset": 2009.1,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "Happening Here",
      "offset": 2010.899,
      "duration": 3.66
    },
    {
      "lang": "en",
      "text": "so normal jits has this shape 32 by 27",
      "offset": 2012.1,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "logist does as well but logit Maxis is",
      "offset": 2014.559,
      "duration": 5.881
    },
    {
      "lang": "en",
      "text": "only 32 by one so there's a broadcasting",
      "offset": 2017.62,
      "duration": 5.22
    },
    {
      "lang": "en",
      "text": "here in the minus",
      "offset": 2020.44,
      "duration": 5.459
    },
    {
      "lang": "en",
      "text": "now here I try to sort of write out a",
      "offset": 2022.84,
      "duration": 5.219
    },
    {
      "lang": "en",
      "text": "two example again we basically have that",
      "offset": 2025.899,
      "duration": 4.861
    },
    {
      "lang": "en",
      "text": "this is our C equals a minus B",
      "offset": 2028.059,
      "duration": 4.441
    },
    {
      "lang": "en",
      "text": "and we see that because of the shape",
      "offset": 2030.76,
      "duration": 3.24
    },
    {
      "lang": "en",
      "text": "these are three by three but this one is",
      "offset": 2032.5,
      "duration": 2.76
    },
    {
      "lang": "en",
      "text": "just a column",
      "offset": 2034,
      "duration": 3.72
    },
    {
      "lang": "en",
      "text": "and so for example every element of C we",
      "offset": 2035.26,
      "duration": 5.279
    },
    {
      "lang": "en",
      "text": "have to look at how it uh came to be and",
      "offset": 2037.72,
      "duration": 4.14
    },
    {
      "lang": "en",
      "text": "every element of C is just the",
      "offset": 2040.539,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "corresponding element of a minus uh",
      "offset": 2041.86,
      "duration": 6.239
    },
    {
      "lang": "en",
      "text": "basically that associated b",
      "offset": 2044.86,
      "duration": 5.64
    },
    {
      "lang": "en",
      "text": "so it's very clear now that the",
      "offset": 2048.099,
      "duration": 4.981
    },
    {
      "lang": "en",
      "text": "derivatives of every one of these c's",
      "offset": 2050.5,
      "duration": 6.06
    },
    {
      "lang": "en",
      "text": "with respect to their inputs are one for",
      "offset": 2053.08,
      "duration": 5.22
    },
    {
      "lang": "en",
      "text": "the corresponding a",
      "offset": 2056.56,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "and it's a negative one for the",
      "offset": 2058.3,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "corresponding B",
      "offset": 2060.639,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "and so therefore",
      "offset": 2062.139,
      "duration": 2.881
    },
    {
      "lang": "en",
      "text": "um",
      "offset": 2064,
      "duration": 3.8
    },
    {
      "lang": "en",
      "text": "the derivatives on the C will flow",
      "offset": 2065.02,
      "duration": 5.879
    },
    {
      "lang": "en",
      "text": "equally to the corresponding Ace and",
      "offset": 2067.8,
      "duration": 5.319
    },
    {
      "lang": "en",
      "text": "then also to the corresponding base but",
      "offset": 2070.899,
      "duration": 4.141
    },
    {
      "lang": "en",
      "text": "then in addition to that the B's are",
      "offset": 2073.119,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "broadcast so we'll have to do the",
      "offset": 2075.04,
      "duration": 4.379
    },
    {
      "lang": "en",
      "text": "additional sum just like we did before",
      "offset": 2076.48,
      "duration": 4.619
    },
    {
      "lang": "en",
      "text": "and of course the derivatives for B's",
      "offset": 2079.419,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "will undergo a minus because the local",
      "offset": 2081.099,
      "duration": 5.161
    },
    {
      "lang": "en",
      "text": "derivative here is uh negative one",
      "offset": 2083.5,
      "duration": 6.96
    },
    {
      "lang": "en",
      "text": "so DC three two by D B3 is negative one",
      "offset": 2086.26,
      "duration": 6.119
    },
    {
      "lang": "en",
      "text": "so let's just Implement that basically",
      "offset": 2090.46,
      "duration": 5.639
    },
    {
      "lang": "en",
      "text": "delugits will be uh exactly copying the",
      "offset": 2092.379,
      "duration": 6.121
    },
    {
      "lang": "en",
      "text": "derivative on normal objects",
      "offset": 2096.099,
      "duration": 3.621
    },
    {
      "lang": "en",
      "text": "so",
      "offset": 2098.5,
      "duration": 4.74
    },
    {
      "lang": "en",
      "text": "delugits equals the norm logits and I'll",
      "offset": 2099.72,
      "duration": 5.5
    },
    {
      "lang": "en",
      "text": "do a DOT clone for safety so we're just",
      "offset": 2103.24,
      "duration": 3.48
    },
    {
      "lang": "en",
      "text": "making a copy",
      "offset": 2105.22,
      "duration": 4.379
    },
    {
      "lang": "en",
      "text": "and then we have that the loaded Maxis",
      "offset": 2106.72,
      "duration": 7.2
    },
    {
      "lang": "en",
      "text": "will be the negative of the non-legits",
      "offset": 2109.599,
      "duration": 6.061
    },
    {
      "lang": "en",
      "text": "because of the negative sign",
      "offset": 2113.92,
      "duration": 3.86
    },
    {
      "lang": "en",
      "text": "and then we have to be careful because",
      "offset": 2115.66,
      "duration": 5.22
    },
    {
      "lang": "en",
      "text": "logic Maxis is a column",
      "offset": 2117.78,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "and so just like we saw before because",
      "offset": 2120.88,
      "duration": 5.94
    },
    {
      "lang": "en",
      "text": "we keep replicating the same elements",
      "offset": 2123.7,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "across all the columns",
      "offset": 2126.82,
      "duration": 4.259
    },
    {
      "lang": "en",
      "text": "then in the backward pass because we",
      "offset": 2128.98,
      "duration": 4.02
    },
    {
      "lang": "en",
      "text": "keep reusing this these are all just",
      "offset": 2131.079,
      "duration": 4.141
    },
    {
      "lang": "en",
      "text": "like separate branches of use of that",
      "offset": 2133,
      "duration": 4.38
    },
    {
      "lang": "en",
      "text": "one variable and so therefore we have to",
      "offset": 2135.22,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "do a Sum along one would keep them",
      "offset": 2137.38,
      "duration": 4.68
    },
    {
      "lang": "en",
      "text": "equals true so that we don't destroy",
      "offset": 2139.78,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "this dimension",
      "offset": 2142.06,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "and then the logic Maxes will be the",
      "offset": 2143.38,
      "duration": 4.02
    },
    {
      "lang": "en",
      "text": "same shape now we have to be careful",
      "offset": 2145.18,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "because this deloaches is not the final",
      "offset": 2147.4,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "deloaches and that's because not only do",
      "offset": 2149.26,
      "duration": 5.339
    },
    {
      "lang": "en",
      "text": "we get gradient signal into logits",
      "offset": 2152.68,
      "duration": 4.2
    },
    {
      "lang": "en",
      "text": "through here but the logic Maxes as a",
      "offset": 2154.599,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "function of logits and that's a second",
      "offset": 2156.88,
      "duration": 4.38
    },
    {
      "lang": "en",
      "text": "Branch into logits so this is not yet",
      "offset": 2158.92,
      "duration": 4.439
    },
    {
      "lang": "en",
      "text": "our final derivative for logits we will",
      "offset": 2161.26,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "come back later for the second branch",
      "offset": 2163.359,
      "duration": 4.201
    },
    {
      "lang": "en",
      "text": "for now the logic Maxis is the final",
      "offset": 2165.579,
      "duration": 4.741
    },
    {
      "lang": "en",
      "text": "derivative so let me uncomment this CMP",
      "offset": 2167.56,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "here and let's just run this",
      "offset": 2170.32,
      "duration": 5.519
    },
    {
      "lang": "en",
      "text": "and logit Maxes hit by torch agrees with",
      "offset": 2172.359,
      "duration": 4.201
    },
    {
      "lang": "en",
      "text": "us",
      "offset": 2175.839,
      "duration": 3.481
    },
    {
      "lang": "en",
      "text": "so that was the derivative into through",
      "offset": 2176.56,
      "duration": 4.5
    },
    {
      "lang": "en",
      "text": "this line",
      "offset": 2179.32,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "now before we move on I want to pause",
      "offset": 2181.06,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "here briefly and I want to look at these",
      "offset": 2182.92,
      "duration": 3.179
    },
    {
      "lang": "en",
      "text": "logic Maxes and especially their",
      "offset": 2184.42,
      "duration": 2.939
    },
    {
      "lang": "en",
      "text": "gradients",
      "offset": 2186.099,
      "duration": 2.701
    },
    {
      "lang": "en",
      "text": "we've talked previously in the previous",
      "offset": 2187.359,
      "duration": 3.781
    },
    {
      "lang": "en",
      "text": "lecture that the only reason we're doing",
      "offset": 2188.8,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "this is for the numerical stability of",
      "offset": 2191.14,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "the softmax that we are implementing",
      "offset": 2193.119,
      "duration": 4.5
    },
    {
      "lang": "en",
      "text": "here and we talked about how if you take",
      "offset": 2194.74,
      "duration": 4.44
    },
    {
      "lang": "en",
      "text": "these logents for any one of these",
      "offset": 2197.619,
      "duration": 3.661
    },
    {
      "lang": "en",
      "text": "examples so one row of this logit's",
      "offset": 2199.18,
      "duration": 4.98
    },
    {
      "lang": "en",
      "text": "tensor if you add or subtract any value",
      "offset": 2201.28,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "equally to all the elements then the",
      "offset": 2204.16,
      "duration": 5.16
    },
    {
      "lang": "en",
      "text": "value of the probes will be unchanged",
      "offset": 2207.28,
      "duration": 4.02
    },
    {
      "lang": "en",
      "text": "you're not changing soft Max the only",
      "offset": 2209.32,
      "duration": 3.72
    },
    {
      "lang": "en",
      "text": "thing that this is doing is it's making",
      "offset": 2211.3,
      "duration": 4.38
    },
    {
      "lang": "en",
      "text": "sure that X doesn't overflow and the",
      "offset": 2213.04,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "reason we're using a Max is because then",
      "offset": 2215.68,
      "duration": 3.24
    },
    {
      "lang": "en",
      "text": "we are guaranteed that each row of",
      "offset": 2217.359,
      "duration": 4.621
    },
    {
      "lang": "en",
      "text": "logits the highest number is zero and so",
      "offset": 2218.92,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "this will be safe",
      "offset": 2221.98,
      "duration": 3.3
    },
    {
      "lang": "en",
      "text": "and so",
      "offset": 2223.72,
      "duration": 2.34
    },
    {
      "lang": "en",
      "text": "um",
      "offset": 2225.28,
      "duration": 4.02
    },
    {
      "lang": "en",
      "text": "basically what that has repercussions",
      "offset": 2226.06,
      "duration": 5.519
    },
    {
      "lang": "en",
      "text": "if it is the case that changing logit",
      "offset": 2229.3,
      "duration": 4.68
    },
    {
      "lang": "en",
      "text": "Maxis does not change the props and",
      "offset": 2231.579,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "therefore there's not change the loss",
      "offset": 2233.98,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "then the gradient on logic masses should",
      "offset": 2235.42,
      "duration": 4.679
    },
    {
      "lang": "en",
      "text": "be zero right because saying those two",
      "offset": 2237.579,
      "duration": 4.201
    },
    {
      "lang": "en",
      "text": "things is the same",
      "offset": 2240.099,
      "duration": 3.721
    },
    {
      "lang": "en",
      "text": "so indeed we hope that this is very very",
      "offset": 2241.78,
      "duration": 3.42
    },
    {
      "lang": "en",
      "text": "small numbers so indeed we hope this is",
      "offset": 2243.82,
      "duration": 4.5
    },
    {
      "lang": "en",
      "text": "zero now because of floating Point uh",
      "offset": 2245.2,
      "duration": 4.919
    },
    {
      "lang": "en",
      "text": "sort of wonkiness",
      "offset": 2248.32,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "um this doesn't come out exactly zero",
      "offset": 2250.119,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "only in some of the rows it does but we",
      "offset": 2251.68,
      "duration": 3.899
    },
    {
      "lang": "en",
      "text": "get extremely small values like one e",
      "offset": 2253.96,
      "duration": 3.659
    },
    {
      "lang": "en",
      "text": "negative nine or ten and so this is",
      "offset": 2255.579,
      "duration": 4.141
    },
    {
      "lang": "en",
      "text": "telling us that the values of loaded",
      "offset": 2257.619,
      "duration": 4.441
    },
    {
      "lang": "en",
      "text": "Maxes are not impacting the loss as they",
      "offset": 2259.72,
      "duration": 3.48
    },
    {
      "lang": "en",
      "text": "shouldn't",
      "offset": 2262.06,
      "duration": 2.76
    },
    {
      "lang": "en",
      "text": "it feels kind of weird to back propagate",
      "offset": 2263.2,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "through this branch honestly because",
      "offset": 2264.82,
      "duration": 5.58
    },
    {
      "lang": "en",
      "text": "if you have any implementation of like f",
      "offset": 2268,
      "duration": 4.2
    },
    {
      "lang": "en",
      "text": "dot cross entropy and pytorch and you",
      "offset": 2270.4,
      "duration": 3.719
    },
    {
      "lang": "en",
      "text": "you block together all these elements",
      "offset": 2272.2,
      "duration": 2.76
    },
    {
      "lang": "en",
      "text": "and you're not doing the back",
      "offset": 2274.119,
      "duration": 3.24
    },
    {
      "lang": "en",
      "text": "propagation piece by piece then you",
      "offset": 2274.96,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "would probably assume that the",
      "offset": 2277.359,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "derivative through here is exactly zero",
      "offset": 2279.28,
      "duration": 4.68
    },
    {
      "lang": "en",
      "text": "uh so you would be sort of",
      "offset": 2281.2,
      "duration": 6.3
    },
    {
      "lang": "en",
      "text": "um skipping this branch because it's",
      "offset": 2283.96,
      "duration": 5.639
    },
    {
      "lang": "en",
      "text": "only done for numerical stability but",
      "offset": 2287.5,
      "duration": 3.42
    },
    {
      "lang": "en",
      "text": "it's interesting to see that even if you",
      "offset": 2289.599,
      "duration": 3.661
    },
    {
      "lang": "en",
      "text": "break up everything into the full atoms",
      "offset": 2290.92,
      "duration": 3.9
    },
    {
      "lang": "en",
      "text": "and you still do the computation as",
      "offset": 2293.26,
      "duration": 2.76
    },
    {
      "lang": "en",
      "text": "you'd like with respect to numerical",
      "offset": 2294.82,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "stability uh the correct thing happens",
      "offset": 2296.02,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "and you still get a very very small",
      "offset": 2297.94,
      "duration": 3.78
    },
    {
      "lang": "en",
      "text": "gradients here",
      "offset": 2300.099,
      "duration": 3.301
    },
    {
      "lang": "en",
      "text": "um basically reflecting the fact that",
      "offset": 2301.72,
      "duration": 4.379
    },
    {
      "lang": "en",
      "text": "the values of these do not matter with",
      "offset": 2303.4,
      "duration": 4.5
    },
    {
      "lang": "en",
      "text": "respect to the final loss",
      "offset": 2306.099,
      "duration": 3.121
    },
    {
      "lang": "en",
      "text": "okay so let's now continue back",
      "offset": 2307.9,
      "duration": 3.719
    },
    {
      "lang": "en",
      "text": "propagation through this line here we've",
      "offset": 2309.22,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "just calculated the logit Maxis and now",
      "offset": 2311.619,
      "duration": 3.541
    },
    {
      "lang": "en",
      "text": "we want to back prop into logits through",
      "offset": 2313.3,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "this second branch",
      "offset": 2315.16,
      "duration": 3.78
    },
    {
      "lang": "en",
      "text": "now here of course we took legits and we",
      "offset": 2316.66,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "took the max along all the rows and then",
      "offset": 2318.94,
      "duration": 4.98
    },
    {
      "lang": "en",
      "text": "we looked at its values here now the way",
      "offset": 2321.46,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "this works is that in pytorch",
      "offset": 2323.92,
      "duration": 5.64
    },
    {
      "lang": "en",
      "text": "this thing here",
      "offset": 2327.3,
      "duration": 4.779
    },
    {
      "lang": "en",
      "text": "the max returns both the values and it",
      "offset": 2329.56,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "Returns the indices at which those",
      "offset": 2332.079,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "values to count the maximum value",
      "offset": 2333.4,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "now in the forward pass we only used",
      "offset": 2335.68,
      "duration": 4.5
    },
    {
      "lang": "en",
      "text": "values because that's all we needed but",
      "offset": 2337.72,
      "duration": 3.78
    },
    {
      "lang": "en",
      "text": "in the backward pass it's extremely",
      "offset": 2340.18,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "useful to know about where those maximum",
      "offset": 2341.5,
      "duration": 5.099
    },
    {
      "lang": "en",
      "text": "values occurred and we have the indices",
      "offset": 2344.02,
      "duration": 4.62
    },
    {
      "lang": "en",
      "text": "at which they occurred and this will of",
      "offset": 2346.599,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "course helps us to help us do the back",
      "offset": 2348.64,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "propagation because what should the",
      "offset": 2350.2,
      "duration": 4.5
    },
    {
      "lang": "en",
      "text": "backward pass be here in this case we",
      "offset": 2352.24,
      "duration": 4.02
    },
    {
      "lang": "en",
      "text": "have the largest tensor which is 32 by",
      "offset": 2354.7,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "27 and in each row we find the maximum",
      "offset": 2356.26,
      "duration": 4.2
    },
    {
      "lang": "en",
      "text": "value and then that value gets plucked",
      "offset": 2358.54,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "out into loaded Maxis and so intuitively",
      "offset": 2360.46,
      "duration": 7.44
    },
    {
      "lang": "en",
      "text": "um basically the derivative flowing",
      "offset": 2364.54,
      "duration": 6.559
    },
    {
      "lang": "en",
      "text": "through here then should be one",
      "offset": 2367.9,
      "duration": 6.12
    },
    {
      "lang": "en",
      "text": "times the look of derivatives is 1 for",
      "offset": 2371.099,
      "duration": 4.421
    },
    {
      "lang": "en",
      "text": "the appropriate entry that was plucked",
      "offset": 2374.02,
      "duration": 2.339
    },
    {
      "lang": "en",
      "text": "out",
      "offset": 2375.52,
      "duration": 3.9
    },
    {
      "lang": "en",
      "text": "and then times the global derivative of",
      "offset": 2376.359,
      "duration": 4.5
    },
    {
      "lang": "en",
      "text": "the logic axis",
      "offset": 2379.42,
      "duration": 3.3
    },
    {
      "lang": "en",
      "text": "so really what we're doing here if you",
      "offset": 2380.859,
      "duration": 3.301
    },
    {
      "lang": "en",
      "text": "think through it is we need to take the",
      "offset": 2382.72,
      "duration": 3.78
    },
    {
      "lang": "en",
      "text": "deloachet Maxis and we need to scatter",
      "offset": 2384.16,
      "duration": 5.939
    },
    {
      "lang": "en",
      "text": "it to the correct positions in these",
      "offset": 2386.5,
      "duration": 6.119
    },
    {
      "lang": "en",
      "text": "logits from where the maximum values",
      "offset": 2390.099,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "came",
      "offset": 2392.619,
      "duration": 2.341
    },
    {
      "lang": "en",
      "text": "and so",
      "offset": 2393.46,
      "duration": 2.7
    },
    {
      "lang": "en",
      "text": "um",
      "offset": 2394.96,
      "duration": 3.3
    },
    {
      "lang": "en",
      "text": "I came up with one line of code sort of",
      "offset": 2396.16,
      "duration": 3.78
    },
    {
      "lang": "en",
      "text": "that does that let me just erase a bunch",
      "offset": 2398.26,
      "duration": 4.02
    },
    {
      "lang": "en",
      "text": "of stuff here so the line of uh you",
      "offset": 2399.94,
      "duration": 3.659
    },
    {
      "lang": "en",
      "text": "could do it kind of very similar to what",
      "offset": 2402.28,
      "duration": 3.18
    },
    {
      "lang": "en",
      "text": "we've done here where we create a zeros",
      "offset": 2403.599,
      "duration": 3.961
    },
    {
      "lang": "en",
      "text": "and then we populate uh the correct",
      "offset": 2405.46,
      "duration": 4.86
    },
    {
      "lang": "en",
      "text": "elements uh so we use the indices here",
      "offset": 2407.56,
      "duration": 5.519
    },
    {
      "lang": "en",
      "text": "and we would set them to be one but you",
      "offset": 2410.32,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "can also use one hot",
      "offset": 2413.079,
      "duration": 5.401
    },
    {
      "lang": "en",
      "text": "so F dot one hot and then I'm taking the",
      "offset": 2415.359,
      "duration": 5.641
    },
    {
      "lang": "en",
      "text": "lowest of Max over the First Dimension",
      "offset": 2418.48,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "dot indices and I'm telling uh pytorch",
      "offset": 2421,
      "duration": 6.599
    },
    {
      "lang": "en",
      "text": "that the dimension of every one of these",
      "offset": 2424.24,
      "duration": 4.859
    },
    {
      "lang": "en",
      "text": "tensors should be",
      "offset": 2427.599,
      "duration": 2.221
    },
    {
      "lang": "en",
      "text": "um",
      "offset": 2429.099,
      "duration": 4.74
    },
    {
      "lang": "en",
      "text": "27 and so what this is going to do",
      "offset": 2429.82,
      "duration": 7.799
    },
    {
      "lang": "en",
      "text": "is okay I apologize this is crazy filthy",
      "offset": 2433.839,
      "duration": 5.701
    },
    {
      "lang": "en",
      "text": "that I am sure of this",
      "offset": 2437.619,
      "duration": 4.021
    },
    {
      "lang": "en",
      "text": "it's really just a an array of where the",
      "offset": 2439.54,
      "duration": 4.86
    },
    {
      "lang": "en",
      "text": "Maxes came from in each row and that",
      "offset": 2441.64,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "element is one and the all the other",
      "offset": 2444.4,
      "duration": 3.48
    },
    {
      "lang": "en",
      "text": "elements are zero so it's a one-half",
      "offset": 2445.96,
      "duration": 4.68
    },
    {
      "lang": "en",
      "text": "Vector in each row and these indices are",
      "offset": 2447.88,
      "duration": 5.16
    },
    {
      "lang": "en",
      "text": "now populating a single one in the",
      "offset": 2450.64,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "proper place",
      "offset": 2453.04,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "and then what I'm doing here is I'm",
      "offset": 2454.48,
      "duration": 4.139
    },
    {
      "lang": "en",
      "text": "multiplying by the logit Maxis and keep",
      "offset": 2456.04,
      "duration": 5.16
    },
    {
      "lang": "en",
      "text": "in mind that this is a column",
      "offset": 2458.619,
      "duration": 7.081
    },
    {
      "lang": "en",
      "text": "of 32 by 1. and so when I'm doing this",
      "offset": 2461.2,
      "duration": 7.26
    },
    {
      "lang": "en",
      "text": "times the logic Maxis the logic Maxes",
      "offset": 2465.7,
      "duration": 5.1
    },
    {
      "lang": "en",
      "text": "will broadcast and that column will you",
      "offset": 2468.46,
      "duration": 3.659
    },
    {
      "lang": "en",
      "text": "know get replicated and in an element",
      "offset": 2470.8,
      "duration": 4.2
    },
    {
      "lang": "en",
      "text": "wise multiply will ensure that each of",
      "offset": 2472.119,
      "duration": 5.22
    },
    {
      "lang": "en",
      "text": "these just gets routed to whichever one",
      "offset": 2475,
      "duration": 4.26
    },
    {
      "lang": "en",
      "text": "of these bits is turned on",
      "offset": 2477.339,
      "duration": 3.901
    },
    {
      "lang": "en",
      "text": "and so that's another way to implement",
      "offset": 2479.26,
      "duration": 4.2
    },
    {
      "lang": "en",
      "text": "uh this kind of a this kind of a",
      "offset": 2481.24,
      "duration": 5.46
    },
    {
      "lang": "en",
      "text": "operation and both of these can be used",
      "offset": 2483.46,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "I just thought I would show an",
      "offset": 2486.7,
      "duration": 3.659
    },
    {
      "lang": "en",
      "text": "equivalent way to do it and I'm using",
      "offset": 2488.02,
      "duration": 3.48
    },
    {
      "lang": "en",
      "text": "plus equals because we already",
      "offset": 2490.359,
      "duration": 3.421
    },
    {
      "lang": "en",
      "text": "calculated the logits here and this is",
      "offset": 2491.5,
      "duration": 3.66
    },
    {
      "lang": "en",
      "text": "not the second branch",
      "offset": 2493.78,
      "duration": 3.48
    },
    {
      "lang": "en",
      "text": "so let's",
      "offset": 2495.16,
      "duration": 4.26
    },
    {
      "lang": "en",
      "text": "look at logits and make sure that this",
      "offset": 2497.26,
      "duration": 3.42
    },
    {
      "lang": "en",
      "text": "is correct",
      "offset": 2499.42,
      "duration": 3.24
    },
    {
      "lang": "en",
      "text": "and we see that we have exactly the",
      "offset": 2500.68,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "correct answer",
      "offset": 2502.66,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "next up we want to continue with logits",
      "offset": 2504.52,
      "duration": 4.62
    },
    {
      "lang": "en",
      "text": "here that is an outcome of a matrix",
      "offset": 2506.98,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "multiplication and a bias offset in this",
      "offset": 2509.14,
      "duration": 3.959
    },
    {
      "lang": "en",
      "text": "linear layer",
      "offset": 2511.54,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "so I've printed out the shapes of all",
      "offset": 2513.099,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "these intermediate tensors we see that",
      "offset": 2516.339,
      "duration": 4.621
    },
    {
      "lang": "en",
      "text": "logits is of course 32 by 27 as we've",
      "offset": 2518.619,
      "duration": 3.301
    },
    {
      "lang": "en",
      "text": "just seen",
      "offset": 2520.96,
      "duration": 4.86
    },
    {
      "lang": "en",
      "text": "then the H here is 32 by 64. so these",
      "offset": 2521.92,
      "duration": 6.419
    },
    {
      "lang": "en",
      "text": "are 64 dimensional hidden States and",
      "offset": 2525.82,
      "duration": 4.92
    },
    {
      "lang": "en",
      "text": "then this W Matrix projects those 64",
      "offset": 2528.339,
      "duration": 4.5
    },
    {
      "lang": "en",
      "text": "dimensional vectors into 27 dimensions",
      "offset": 2530.74,
      "duration": 5.099
    },
    {
      "lang": "en",
      "text": "and then there's a 27 dimensional offset",
      "offset": 2532.839,
      "duration": 5.701
    },
    {
      "lang": "en",
      "text": "which is a one-dimensional vector",
      "offset": 2535.839,
      "duration": 4.5
    },
    {
      "lang": "en",
      "text": "now we should note that this plus here",
      "offset": 2538.54,
      "duration": 4.68
    },
    {
      "lang": "en",
      "text": "actually broadcasts because H multiplied",
      "offset": 2540.339,
      "duration": 7.201
    },
    {
      "lang": "en",
      "text": "by by W2 will give us a 32 by 27. and so",
      "offset": 2543.22,
      "duration": 7.92
    },
    {
      "lang": "en",
      "text": "then this plus B2 is a 27 dimensional",
      "offset": 2547.54,
      "duration": 4.68
    },
    {
      "lang": "en",
      "text": "lecture here",
      "offset": 2551.14,
      "duration": 2.699
    },
    {
      "lang": "en",
      "text": "now in the rules of broadcasting what's",
      "offset": 2552.22,
      "duration": 3.66
    },
    {
      "lang": "en",
      "text": "going to happen with this bias Vector is",
      "offset": 2553.839,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "that this one-dimensional Vector of 27",
      "offset": 2555.88,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "will get aligned with a padded dimension",
      "offset": 2557.92,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "of one on the left and it will basically",
      "offset": 2561.16,
      "duration": 4.679
    },
    {
      "lang": "en",
      "text": "become a row vector and then it will get",
      "offset": 2563.68,
      "duration": 5.159
    },
    {
      "lang": "en",
      "text": "replicated vertically 32 times to make",
      "offset": 2565.839,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "it 32 by 27 and then there's an",
      "offset": 2568.839,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "element-wise multiply",
      "offset": 2570.64,
      "duration": 3.959
    },
    {
      "lang": "en",
      "text": "now",
      "offset": 2572.92,
      "duration": 3.54
    },
    {
      "lang": "en",
      "text": "the question is how do we back propagate",
      "offset": 2574.599,
      "duration": 4.621
    },
    {
      "lang": "en",
      "text": "from logits to the hidden States the",
      "offset": 2576.46,
      "duration": 5.639
    },
    {
      "lang": "en",
      "text": "weight Matrix W2 and the bias B2",
      "offset": 2579.22,
      "duration": 4.619
    },
    {
      "lang": "en",
      "text": "and you might think that we need to go",
      "offset": 2582.099,
      "duration": 5.581
    },
    {
      "lang": "en",
      "text": "to some Matrix calculus and then we have",
      "offset": 2583.839,
      "duration": 5.881
    },
    {
      "lang": "en",
      "text": "to look up the derivative for a matrix",
      "offset": 2587.68,
      "duration": 3.96
    },
    {
      "lang": "en",
      "text": "multiplication but actually you don't",
      "offset": 2589.72,
      "duration": 3.18
    },
    {
      "lang": "en",
      "text": "have to do any of that and you can go",
      "offset": 2591.64,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "back to First principles and derive this",
      "offset": 2592.9,
      "duration": 4.14
    },
    {
      "lang": "en",
      "text": "yourself on a piece of paper and",
      "offset": 2594.64,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "specifically what I like to do and I",
      "offset": 2597.04,
      "duration": 3.72
    },
    {
      "lang": "en",
      "text": "what I find works well for me is you",
      "offset": 2598.72,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "find a specific small example that you",
      "offset": 2600.76,
      "duration": 4.44
    },
    {
      "lang": "en",
      "text": "then fully write out and then in the",
      "offset": 2603.28,
      "duration": 3.78
    },
    {
      "lang": "en",
      "text": "process of analyzing how that individual",
      "offset": 2605.2,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "small example works you will understand",
      "offset": 2607.06,
      "duration": 3.48
    },
    {
      "lang": "en",
      "text": "the broader pattern and you'll be able",
      "offset": 2608.8,
      "duration": 3.559
    },
    {
      "lang": "en",
      "text": "to generalize and write out the full",
      "offset": 2610.54,
      "duration": 4.98
    },
    {
      "lang": "en",
      "text": "general formula for what how these",
      "offset": 2612.359,
      "duration": 5.021
    },
    {
      "lang": "en",
      "text": "derivatives flow in an expression like",
      "offset": 2615.52,
      "duration": 3.72
    },
    {
      "lang": "en",
      "text": "this so let's try that out",
      "offset": 2617.38,
      "duration": 3.9
    },
    {
      "lang": "en",
      "text": "so pardon the low budget production here",
      "offset": 2619.24,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "but what I've done here is I'm writing",
      "offset": 2621.28,
      "duration": 4.02
    },
    {
      "lang": "en",
      "text": "it out on a piece of paper really what",
      "offset": 2623.56,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "we are interested in is we have a",
      "offset": 2625.3,
      "duration": 5.4
    },
    {
      "lang": "en",
      "text": "multiply B plus C and that creates a d",
      "offset": 2626.92,
      "duration": 6.3
    },
    {
      "lang": "en",
      "text": "and we have the derivative of the loss",
      "offset": 2630.7,
      "duration": 4.139
    },
    {
      "lang": "en",
      "text": "with respect to D and we'd like to know",
      "offset": 2633.22,
      "duration": 2.76
    },
    {
      "lang": "en",
      "text": "what the derivative of the losses with",
      "offset": 2634.839,
      "duration": 3.061
    },
    {
      "lang": "en",
      "text": "respect to a b and c",
      "offset": 2635.98,
      "duration": 4.02
    },
    {
      "lang": "en",
      "text": "now these here are little",
      "offset": 2637.9,
      "duration": 3.48
    },
    {
      "lang": "en",
      "text": "two-dimensional examples of a matrix",
      "offset": 2640,
      "duration": 3.96
    },
    {
      "lang": "en",
      "text": "multiplication Two by Two Times a two by",
      "offset": 2641.38,
      "duration": 3.479
    },
    {
      "lang": "en",
      "text": "two",
      "offset": 2643.96,
      "duration": 4.02
    },
    {
      "lang": "en",
      "text": "plus a 2 a vector of just two elements",
      "offset": 2644.859,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "C1 and C2 gives me a two by two",
      "offset": 2647.98,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "now notice here that I have a bias",
      "offset": 2650.859,
      "duration": 6.181
    },
    {
      "lang": "en",
      "text": "Vector here called C and the bisex",
      "offset": 2654.22,
      "duration": 5.099
    },
    {
      "lang": "en",
      "text": "vector is C1 and C2 but as I described",
      "offset": 2657.04,
      "duration": 4.5
    },
    {
      "lang": "en",
      "text": "over here that bias Vector will become a",
      "offset": 2659.319,
      "duration": 3.961
    },
    {
      "lang": "en",
      "text": "row Vector in the broadcasting and will",
      "offset": 2661.54,
      "duration": 3.42
    },
    {
      "lang": "en",
      "text": "replicate vertically so that's what's",
      "offset": 2663.28,
      "duration": 4.02
    },
    {
      "lang": "en",
      "text": "happening here as well C1 C2 is",
      "offset": 2664.96,
      "duration": 4.74
    },
    {
      "lang": "en",
      "text": "replicated vertically and we see how we",
      "offset": 2667.3,
      "duration": 5.88
    },
    {
      "lang": "en",
      "text": "have two rows of C1 C2 as a result",
      "offset": 2669.7,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "so now when I say write it out I just",
      "offset": 2673.18,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "mean like this basically break up this",
      "offset": 2675.22,
      "duration": 4.859
    },
    {
      "lang": "en",
      "text": "matrix multiplication into the actual",
      "offset": 2677.74,
      "duration": 3.72
    },
    {
      "lang": "en",
      "text": "thing that that's going on under the",
      "offset": 2680.079,
      "duration": 4.02
    },
    {
      "lang": "en",
      "text": "hood so as a result of matrix",
      "offset": 2681.46,
      "duration": 5.399
    },
    {
      "lang": "en",
      "text": "multiplication and how it works d11 is",
      "offset": 2684.099,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "the result of a DOT product between the",
      "offset": 2686.859,
      "duration": 4.141
    },
    {
      "lang": "en",
      "text": "first row of a and the First Column of B",
      "offset": 2688.66,
      "duration": 9.06
    },
    {
      "lang": "en",
      "text": "so a11 b11 plus a12 B21 plus C1",
      "offset": 2691,
      "duration": 8.94
    },
    {
      "lang": "en",
      "text": "and so on so forth for all the other",
      "offset": 2697.72,
      "duration": 4.5
    },
    {
      "lang": "en",
      "text": "elements of D and once you actually",
      "offset": 2699.94,
      "duration": 4.02
    },
    {
      "lang": "en",
      "text": "write it out it becomes obvious this is",
      "offset": 2702.22,
      "duration": 3.899
    },
    {
      "lang": "en",
      "text": "just a bunch of multipliers and",
      "offset": 2703.96,
      "duration": 5.1
    },
    {
      "lang": "en",
      "text": "um adds and we know from micrograd how",
      "offset": 2706.119,
      "duration": 5.581
    },
    {
      "lang": "en",
      "text": "to differentiate multiplies and adds and",
      "offset": 2709.06,
      "duration": 3.96
    },
    {
      "lang": "en",
      "text": "so this is not scary anymore it's not",
      "offset": 2711.7,
      "duration": 3.72
    },
    {
      "lang": "en",
      "text": "just matrix multiplication it's just uh",
      "offset": 2713.02,
      "duration": 4.38
    },
    {
      "lang": "en",
      "text": "tedious unfortunately but this is",
      "offset": 2715.42,
      "duration": 4.98
    },
    {
      "lang": "en",
      "text": "completely tractable we have DL by D for",
      "offset": 2717.4,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "all of these and we want DL by uh all",
      "offset": 2720.4,
      "duration": 5.1
    },
    {
      "lang": "en",
      "text": "these little other variables so how do",
      "offset": 2723.4,
      "duration": 3.3
    },
    {
      "lang": "en",
      "text": "we achieve that and how do we actually",
      "offset": 2725.5,
      "duration": 3.66
    },
    {
      "lang": "en",
      "text": "get the gradients okay so the low budget",
      "offset": 2726.7,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "production continues here",
      "offset": 2729.16,
      "duration": 3.78
    },
    {
      "lang": "en",
      "text": "so let's for example derive the",
      "offset": 2730.78,
      "duration": 3.48
    },
    {
      "lang": "en",
      "text": "derivative of the loss with respect to",
      "offset": 2732.94,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "a11",
      "offset": 2734.26,
      "duration": 4.62
    },
    {
      "lang": "en",
      "text": "we see here that a11 occurs twice in our",
      "offset": 2736.06,
      "duration": 4.44
    },
    {
      "lang": "en",
      "text": "simple expression right here right here",
      "offset": 2738.88,
      "duration": 4.38
    },
    {
      "lang": "en",
      "text": "and influences d11 and D12",
      "offset": 2740.5,
      "duration": 5.64
    },
    {
      "lang": "en",
      "text": ". so this is so what is DL by d a one",
      "offset": 2743.26,
      "duration": 8.04
    },
    {
      "lang": "en",
      "text": "one well it's DL by d11 times the local",
      "offset": 2746.14,
      "duration": 7.679
    },
    {
      "lang": "en",
      "text": "derivative of d11 which in this case is",
      "offset": 2751.3,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "just b11 because that's what's",
      "offset": 2753.819,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "multiplying a11 here",
      "offset": 2755.619,
      "duration": 4.861
    },
    {
      "lang": "en",
      "text": "so uh and likewise here the local",
      "offset": 2757.9,
      "duration": 4.98
    },
    {
      "lang": "en",
      "text": "derivative of D12 with respect to a11 is",
      "offset": 2760.48,
      "duration": 5.099
    },
    {
      "lang": "en",
      "text": "just B12 and so B12 well in the chain",
      "offset": 2762.88,
      "duration": 5.88
    },
    {
      "lang": "en",
      "text": "rule therefore multiply the L by d 1 2.",
      "offset": 2765.579,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "and then because a11 is used both to",
      "offset": 2768.76,
      "duration": 6.3
    },
    {
      "lang": "en",
      "text": "produce d11 and D12 we need to add up",
      "offset": 2771.819,
      "duration": 6.421
    },
    {
      "lang": "en",
      "text": "the contributions of both of those sort",
      "offset": 2775.06,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "of chains that are running in parallel",
      "offset": 2778.24,
      "duration": 4.5
    },
    {
      "lang": "en",
      "text": "and that's why we get a plus just adding",
      "offset": 2780.099,
      "duration": 4.141
    },
    {
      "lang": "en",
      "text": "up those two",
      "offset": 2782.74,
      "duration": 3.48
    },
    {
      "lang": "en",
      "text": "um those two contributions and that",
      "offset": 2784.24,
      "duration": 4.92
    },
    {
      "lang": "en",
      "text": "gives us DL by d a one one we can do the",
      "offset": 2786.22,
      "duration": 4.859
    },
    {
      "lang": "en",
      "text": "exact same analysis for the other one",
      "offset": 2789.16,
      "duration": 5.1
    },
    {
      "lang": "en",
      "text": "for all the other elements of a and when",
      "offset": 2791.079,
      "duration": 4.981
    },
    {
      "lang": "en",
      "text": "you simply write it out it's just super",
      "offset": 2794.26,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "simple",
      "offset": 2796.06,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "um taking of gradients on you know",
      "offset": 2797.26,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "expressions like this",
      "offset": 2800.14,
      "duration": 4.199
    },
    {
      "lang": "en",
      "text": "you find that",
      "offset": 2802.06,
      "duration": 4.98
    },
    {
      "lang": "en",
      "text": "this Matrix DL by D A that we're after",
      "offset": 2804.339,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "right if we just arrange all the all of",
      "offset": 2807.04,
      "duration": 5.46
    },
    {
      "lang": "en",
      "text": "them in the same shape as a takes so a",
      "offset": 2809.859,
      "duration": 5.341
    },
    {
      "lang": "en",
      "text": "is just too much Matrix so d l by D A",
      "offset": 2812.5,
      "duration": 7.46
    },
    {
      "lang": "en",
      "text": "here will be also just the same shape",
      "offset": 2815.2,
      "duration": 8.1
    },
    {
      "lang": "en",
      "text": "tester with the derivatives now so deal",
      "offset": 2819.96,
      "duration": 5.139
    },
    {
      "lang": "en",
      "text": "by D a11 Etc",
      "offset": 2823.3,
      "duration": 3.42
    },
    {
      "lang": "en",
      "text": "and we see that actually we can express",
      "offset": 2825.099,
      "duration": 4.621
    },
    {
      "lang": "en",
      "text": "what we've written out here as a matrix",
      "offset": 2826.72,
      "duration": 3.96
    },
    {
      "lang": "en",
      "text": "multiplied",
      "offset": 2829.72,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "and so it just so happens that D all by",
      "offset": 2830.68,
      "duration": 4.86
    },
    {
      "lang": "en",
      "text": "that all of these formulas that we've",
      "offset": 2833.56,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "derived here by taking gradients can",
      "offset": 2835.54,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "actually be expressed as a matrix",
      "offset": 2837.88,
      "duration": 3.42
    },
    {
      "lang": "en",
      "text": "multiplication and in particular we see",
      "offset": 2839.14,
      "duration": 3.78
    },
    {
      "lang": "en",
      "text": "that it is the matrix multiplication of",
      "offset": 2841.3,
      "duration": 4.26
    },
    {
      "lang": "en",
      "text": "these two array matrices",
      "offset": 2842.92,
      "duration": 7.26
    },
    {
      "lang": "en",
      "text": "so it is the um DL by D and then Matrix",
      "offset": 2845.56,
      "duration": 7.38
    },
    {
      "lang": "en",
      "text": "multiplying B but B transpose actually",
      "offset": 2850.18,
      "duration": 7.26
    },
    {
      "lang": "en",
      "text": "so you see that B21 and b12 have changed",
      "offset": 2852.94,
      "duration": 5.639
    },
    {
      "lang": "en",
      "text": "place",
      "offset": 2857.44,
      "duration": 3.72
    },
    {
      "lang": "en",
      "text": "whereas before we had of course b11 B12",
      "offset": 2858.579,
      "duration": 6.481
    },
    {
      "lang": "en",
      "text": "B2 on B22 so you see that this other",
      "offset": 2861.16,
      "duration": 6.3
    },
    {
      "lang": "en",
      "text": "Matrix B is transposed",
      "offset": 2865.06,
      "duration": 4.2
    },
    {
      "lang": "en",
      "text": "and so basically what we have long story",
      "offset": 2867.46,
      "duration": 3.18
    },
    {
      "lang": "en",
      "text": "short just by doing very simple",
      "offset": 2869.26,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "reasoning here by breaking up the",
      "offset": 2870.64,
      "duration": 3.719
    },
    {
      "lang": "en",
      "text": "expression in the case of a very simple",
      "offset": 2872.859,
      "duration": 6.121
    },
    {
      "lang": "en",
      "text": "example is that DL by d a is which is",
      "offset": 2874.359,
      "duration": 7.681
    },
    {
      "lang": "en",
      "text": "this is simply equal to DL by DD Matrix",
      "offset": 2878.98,
      "duration": 6.68
    },
    {
      "lang": "en",
      "text": "multiplied with B transpose",
      "offset": 2882.04,
      "duration": 3.62
    },
    {
      "lang": "en",
      "text": "so that is what we have so far now we",
      "offset": 2885.819,
      "duration": 4.921
    },
    {
      "lang": "en",
      "text": "also want the derivative with respect to",
      "offset": 2888.4,
      "duration": 5.16
    },
    {
      "lang": "en",
      "text": "um B and C now",
      "offset": 2890.74,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "for B I'm not actually doing the full",
      "offset": 2893.56,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "derivation because honestly it's um it's",
      "offset": 2895.54,
      "duration": 5.1
    },
    {
      "lang": "en",
      "text": "not deep it's just uh annoying it's",
      "offset": 2898.359,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "exhausting you can actually do this",
      "offset": 2900.64,
      "duration": 3.959
    },
    {
      "lang": "en",
      "text": "analysis yourself you'll also find that",
      "offset": 2902.44,
      "duration": 3.78
    },
    {
      "lang": "en",
      "text": "if you take this these expressions and",
      "offset": 2904.599,
      "duration": 3.24
    },
    {
      "lang": "en",
      "text": "you differentiate with respect to b",
      "offset": 2906.22,
      "duration": 4.74
    },
    {
      "lang": "en",
      "text": "instead of a you will find that DL by DB",
      "offset": 2907.839,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "is also a matrix multiplication in this",
      "offset": 2910.96,
      "duration": 4.44
    },
    {
      "lang": "en",
      "text": "case you have to take the Matrix a and",
      "offset": 2913.359,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "transpose it and Matrix multiply that",
      "offset": 2915.4,
      "duration": 4.26
    },
    {
      "lang": "en",
      "text": "with bl by DD",
      "offset": 2917.68,
      "duration": 4.74
    },
    {
      "lang": "en",
      "text": "and that's what gives you a deal by DB",
      "offset": 2919.66,
      "duration": 6.6
    },
    {
      "lang": "en",
      "text": "and then here for the offsets C1 and C2",
      "offset": 2922.42,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "if you again just differentiate with",
      "offset": 2926.26,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "respect to C1 you will find an",
      "offset": 2927.94,
      "duration": 4.139
    },
    {
      "lang": "en",
      "text": "expression like this",
      "offset": 2930.579,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "and C2 an expression like this",
      "offset": 2932.079,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "and basically you'll find the DL by DC",
      "offset": 2935.14,
      "duration": 4.199
    },
    {
      "lang": "en",
      "text": "is simply because they're just",
      "offset": 2937.119,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "offsetting these Expressions you just",
      "offset": 2939.339,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "have to take the deal by DD Matrix",
      "offset": 2941.44,
      "duration": 6.12
    },
    {
      "lang": "en",
      "text": "of the derivatives of D and you just",
      "offset": 2944.859,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "have to sum across the columns and that",
      "offset": 2947.56,
      "duration": 5.88
    },
    {
      "lang": "en",
      "text": "gives you the derivatives for C",
      "offset": 2951.099,
      "duration": 4.74
    },
    {
      "lang": "en",
      "text": "so long story short",
      "offset": 2953.44,
      "duration": 4.86
    },
    {
      "lang": "en",
      "text": "the backward Paths of a matrix multiply",
      "offset": 2955.839,
      "duration": 4.201
    },
    {
      "lang": "en",
      "text": "is a matrix multiply",
      "offset": 2958.3,
      "duration": 3.96
    },
    {
      "lang": "en",
      "text": "and instead of just like we had D equals",
      "offset": 2960.04,
      "duration": 5.7
    },
    {
      "lang": "en",
      "text": "a times B plus C in the scalar case uh",
      "offset": 2962.26,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "we sort of like arrive at something very",
      "offset": 2965.74,
      "duration": 3.66
    },
    {
      "lang": "en",
      "text": "very similar but now uh with a matrix",
      "offset": 2967.3,
      "duration": 3.96
    },
    {
      "lang": "en",
      "text": "multiplication instead of a scalar",
      "offset": 2969.4,
      "duration": 3.179
    },
    {
      "lang": "en",
      "text": "multiplication",
      "offset": 2971.26,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "so the derivative of D with respect to a",
      "offset": 2972.579,
      "duration": 5.24
    },
    {
      "lang": "en",
      "text": "is",
      "offset": 2976.06,
      "duration": 5.16
    },
    {
      "lang": "en",
      "text": "DL by DD Matrix multiplied B trespose",
      "offset": 2977.819,
      "duration": 6.641
    },
    {
      "lang": "en",
      "text": "and here it's a transpose multiply deal",
      "offset": 2981.22,
      "duration": 5.22
    },
    {
      "lang": "en",
      "text": "by DD but in both cases it's a matrix",
      "offset": 2984.46,
      "duration": 5.1
    },
    {
      "lang": "en",
      "text": "multiplication with the derivative and",
      "offset": 2986.44,
      "duration": 6.72
    },
    {
      "lang": "en",
      "text": "the other term in the multiplication",
      "offset": 2989.56,
      "duration": 6.299
    },
    {
      "lang": "en",
      "text": "and for C it is a sum",
      "offset": 2993.16,
      "duration": 5.699
    },
    {
      "lang": "en",
      "text": "now I'll tell you a secret I can never",
      "offset": 2995.859,
      "duration": 4.381
    },
    {
      "lang": "en",
      "text": "remember the formulas that we just",
      "offset": 2998.859,
      "duration": 2.76
    },
    {
      "lang": "en",
      "text": "arrived for back proper gain information",
      "offset": 3000.24,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "multiplication and I can back propagate",
      "offset": 3001.619,
      "duration": 4.2
    },
    {
      "lang": "en",
      "text": "through these Expressions just fine and",
      "offset": 3003.839,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "the reason this works is because the",
      "offset": 3005.819,
      "duration": 3.181
    },
    {
      "lang": "en",
      "text": "dimensions have to work out",
      "offset": 3007.44,
      "duration": 4.2
    },
    {
      "lang": "en",
      "text": "uh so let me give you an example say I",
      "offset": 3009,
      "duration": 4.68
    },
    {
      "lang": "en",
      "text": "want to create DH",
      "offset": 3011.64,
      "duration": 5.34
    },
    {
      "lang": "en",
      "text": "then what should the H be number one I",
      "offset": 3013.68,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "have to know that the shape of DH must",
      "offset": 3016.98,
      "duration": 4.379
    },
    {
      "lang": "en",
      "text": "be the same as the shape of H",
      "offset": 3019.2,
      "duration": 5.1
    },
    {
      "lang": "en",
      "text": "and the shape of H is 32 by 64. and then",
      "offset": 3021.359,
      "duration": 4.921
    },
    {
      "lang": "en",
      "text": "the other piece of information I know is",
      "offset": 3024.3,
      "duration": 4.5
    },
    {
      "lang": "en",
      "text": "that DH must be some kind of matrix",
      "offset": 3026.28,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "multiplication of the logits with W2",
      "offset": 3028.8,
      "duration": 8.819
    },
    {
      "lang": "en",
      "text": "and delojits is 32 by 27 and W2 is a 64",
      "offset": 3032.52,
      "duration": 7.86
    },
    {
      "lang": "en",
      "text": "by 27. there is only a single way to",
      "offset": 3037.619,
      "duration": 6.361
    },
    {
      "lang": "en",
      "text": "make the shape work out in this case and",
      "offset": 3040.38,
      "duration": 5.58
    },
    {
      "lang": "en",
      "text": "it is indeed the correct result in",
      "offset": 3043.98,
      "duration": 4.859
    },
    {
      "lang": "en",
      "text": "particular here H needs to be 32 by 64.",
      "offset": 3045.96,
      "duration": 4.92
    },
    {
      "lang": "en",
      "text": "the only way to achieve that is to take",
      "offset": 3048.839,
      "duration": 4.141
    },
    {
      "lang": "en",
      "text": "a deluges",
      "offset": 3050.88,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "and Matrix multiply it with you see how",
      "offset": 3052.98,
      "duration": 4.5
    },
    {
      "lang": "en",
      "text": "I have to take W2 but I have to",
      "offset": 3055.92,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "transpose it to make the dimensions work",
      "offset": 3057.48,
      "duration": 2.24
    },
    {
      "lang": "en",
      "text": "out",
      "offset": 3058.92,
      "duration": 3.899
    },
    {
      "lang": "en",
      "text": "so w to transpose and it's the only way",
      "offset": 3059.72,
      "duration": 5.02
    },
    {
      "lang": "en",
      "text": "to make these to Matrix multiply those",
      "offset": 3062.819,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "two pieces to make the shapes work out",
      "offset": 3064.74,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "and that turns out to be the correct",
      "offset": 3066.66,
      "duration": 5.159
    },
    {
      "lang": "en",
      "text": "formula so if we come here we want DH",
      "offset": 3068.099,
      "duration": 7.26
    },
    {
      "lang": "en",
      "text": "which is d a and we see that d a is DL",
      "offset": 3071.819,
      "duration": 6.181
    },
    {
      "lang": "en",
      "text": "by DD Matrix multiply B transpose",
      "offset": 3075.359,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "so that's Delo just multiply and B is W2",
      "offset": 3078,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "so W2 transpose which is exactly what we",
      "offset": 3081.359,
      "duration": 4.921
    },
    {
      "lang": "en",
      "text": "have here so there's no need to remember",
      "offset": 3084,
      "duration": 6.079
    },
    {
      "lang": "en",
      "text": "these formulas similarly now if I want",
      "offset": 3086.28,
      "duration": 7.2
    },
    {
      "lang": "en",
      "text": "dw2 well I know that it must be a matrix",
      "offset": 3090.079,
      "duration": 7.361
    },
    {
      "lang": "en",
      "text": "multiplication of D logits and H",
      "offset": 3093.48,
      "duration": 5.82
    },
    {
      "lang": "en",
      "text": "and maybe there's a few transpose like",
      "offset": 3097.44,
      "duration": 3.24
    },
    {
      "lang": "en",
      "text": "there's one transpose in there as well",
      "offset": 3099.3,
      "duration": 3.059
    },
    {
      "lang": "en",
      "text": "and I don't know which way it is so I",
      "offset": 3100.68,
      "duration": 4.139
    },
    {
      "lang": "en",
      "text": "have to come to W2 and I see that its",
      "offset": 3102.359,
      "duration": 4.921
    },
    {
      "lang": "en",
      "text": "shape is 64 by 27",
      "offset": 3104.819,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "and that has to come from some interest",
      "offset": 3107.28,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "multiplication of these two",
      "offset": 3109.14,
      "duration": 6.719
    },
    {
      "lang": "en",
      "text": "and so to get a 64 by 27 I need to take",
      "offset": 3111.119,
      "duration": 5.7
    },
    {
      "lang": "en",
      "text": "um",
      "offset": 3115.859,
      "duration": 3.421
    },
    {
      "lang": "en",
      "text": "H I need to transpose it",
      "offset": 3116.819,
      "duration": 4.921
    },
    {
      "lang": "en",
      "text": "and then I need to Matrix multiply it",
      "offset": 3119.28,
      "duration": 4.86
    },
    {
      "lang": "en",
      "text": "um so that will become 64 by 32 and then",
      "offset": 3121.74,
      "duration": 3.54
    },
    {
      "lang": "en",
      "text": "I need to make sure to multiply with the",
      "offset": 3124.14,
      "duration": 3.419
    },
    {
      "lang": "en",
      "text": "32 by 27 and that's going to give me a",
      "offset": 3125.28,
      "duration": 4.44
    },
    {
      "lang": "en",
      "text": "64 by 27. so I need to make sure it's",
      "offset": 3127.559,
      "duration": 3.721
    },
    {
      "lang": "en",
      "text": "multiplied this with the logist that",
      "offset": 3129.72,
      "duration": 3.54
    },
    {
      "lang": "en",
      "text": "shape just like that that's the only way",
      "offset": 3131.28,
      "duration": 4.38
    },
    {
      "lang": "en",
      "text": "to make the dimensions work out and just",
      "offset": 3133.26,
      "duration": 4.68
    },
    {
      "lang": "en",
      "text": "use matrix multiplication and if we come",
      "offset": 3135.66,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "here we see that that's exactly what's",
      "offset": 3137.94,
      "duration": 5.06
    },
    {
      "lang": "en",
      "text": "here so a transpose a for us is H",
      "offset": 3139.98,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "multiplied with deloaches",
      "offset": 3143,
      "duration": 7.359
    },
    {
      "lang": "en",
      "text": "so that's W2 and then db2",
      "offset": 3145.98,
      "duration": 7.5
    },
    {
      "lang": "en",
      "text": "is just the um",
      "offset": 3150.359,
      "duration": 5.521
    },
    {
      "lang": "en",
      "text": "vertical sum and actually in the same",
      "offset": 3153.48,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "way there's only one way to make the",
      "offset": 3155.88,
      "duration": 3.06
    },
    {
      "lang": "en",
      "text": "shapes work out I don't have to remember",
      "offset": 3157.319,
      "duration": 3.661
    },
    {
      "lang": "en",
      "text": "that it's a vertical Sum along the zero",
      "offset": 3158.94,
      "duration": 3.78
    },
    {
      "lang": "en",
      "text": "axis because that's the only way that",
      "offset": 3160.98,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "this makes sense because B2 shape is 27",
      "offset": 3162.72,
      "duration": 7.74
    },
    {
      "lang": "en",
      "text": "so in order to get a um delugits",
      "offset": 3165.3,
      "duration": 9.18
    },
    {
      "lang": "en",
      "text": "here is 30 by 27 so knowing that it's",
      "offset": 3170.46,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "just sum over deloaches in some",
      "offset": 3174.48,
      "duration": 3.98
    },
    {
      "lang": "en",
      "text": "Direction",
      "offset": 3176.22,
      "duration": 2.24
    },
    {
      "lang": "en",
      "text": "that direction must be zero because I",
      "offset": 3179.819,
      "duration": 4.861
    },
    {
      "lang": "en",
      "text": "need to eliminate this Dimension so it's",
      "offset": 3182.099,
      "duration": 3.901
    },
    {
      "lang": "en",
      "text": "this",
      "offset": 3184.68,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "so this is so let's kind of like the",
      "offset": 3186,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "hacky way let me copy paste and delete",
      "offset": 3188.52,
      "duration": 5.099
    },
    {
      "lang": "en",
      "text": "that and let me swing over here and this",
      "offset": 3190.559,
      "duration": 4.381
    },
    {
      "lang": "en",
      "text": "is our backward pass for the linear",
      "offset": 3193.619,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "layer uh hopefully",
      "offset": 3194.94,
      "duration": 4.44
    },
    {
      "lang": "en",
      "text": "so now let's uncomment",
      "offset": 3197.22,
      "duration": 4.52
    },
    {
      "lang": "en",
      "text": "these three and we're checking that we",
      "offset": 3199.38,
      "duration": 4.92
    },
    {
      "lang": "en",
      "text": "got all the three derivatives correct",
      "offset": 3201.74,
      "duration": 5.14
    },
    {
      "lang": "en",
      "text": "and run",
      "offset": 3204.3,
      "duration": 6.059
    },
    {
      "lang": "en",
      "text": "and we see that h wh and B2 are all",
      "offset": 3206.88,
      "duration": 6.12
    },
    {
      "lang": "en",
      "text": "exactly correct so we back propagated",
      "offset": 3210.359,
      "duration": 5.841
    },
    {
      "lang": "en",
      "text": "through a linear layer",
      "offset": 3213,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "now next up we have derivative for the h",
      "offset": 3216.26,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "already and we need to back propagate",
      "offset": 3219.359,
      "duration": 4.5
    },
    {
      "lang": "en",
      "text": "through 10h into h preact",
      "offset": 3221.46,
      "duration": 5.7
    },
    {
      "lang": "en",
      "text": "so we want to derive DH preact",
      "offset": 3223.859,
      "duration": 4.681
    },
    {
      "lang": "en",
      "text": "and here we have to back propagate",
      "offset": 3227.16,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "through a 10 H and we've already done",
      "offset": 3228.54,
      "duration": 3.72
    },
    {
      "lang": "en",
      "text": "this in micrograd and we remember that",
      "offset": 3230.28,
      "duration": 3.96
    },
    {
      "lang": "en",
      "text": "10h has a very simple backward formula",
      "offset": 3232.26,
      "duration": 4.44
    },
    {
      "lang": "en",
      "text": "now unfortunately if I just put in D by",
      "offset": 3234.24,
      "duration": 4.859
    },
    {
      "lang": "en",
      "text": "DX of 10 h of X into both from alpha it",
      "offset": 3236.7,
      "duration": 4.2
    },
    {
      "lang": "en",
      "text": "lets us down it tells us that it's a",
      "offset": 3239.099,
      "duration": 4.201
    },
    {
      "lang": "en",
      "text": "hyperbolic secant function squared of X",
      "offset": 3240.9,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "it's not exactly helpful but luckily",
      "offset": 3243.3,
      "duration": 4.98
    },
    {
      "lang": "en",
      "text": "Google image search does not let us down",
      "offset": 3246.42,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "and it gives us the simpler formula and",
      "offset": 3248.28,
      "duration": 4.14
    },
    {
      "lang": "en",
      "text": "in particular if you have that a is",
      "offset": 3250.74,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "equal to 10 h of Z then d a by DZ by",
      "offset": 3252.42,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "propagating through 10 H is just one",
      "offset": 3256.02,
      "duration": 5.099
    },
    {
      "lang": "en",
      "text": "minus a square and take note that 1",
      "offset": 3257.94,
      "duration": 5.7
    },
    {
      "lang": "en",
      "text": "minus a square a here is the output of",
      "offset": 3261.119,
      "duration": 5.94
    },
    {
      "lang": "en",
      "text": "the 10h not the input to the 10h Z so",
      "offset": 3263.64,
      "duration": 6.179
    },
    {
      "lang": "en",
      "text": "the D A by DZ is here formulated in",
      "offset": 3267.059,
      "duration": 4.921
    },
    {
      "lang": "en",
      "text": "terms of the output of that 10h",
      "offset": 3269.819,
      "duration": 4.201
    },
    {
      "lang": "en",
      "text": "and here also in Google image search we",
      "offset": 3271.98,
      "duration": 3.96
    },
    {
      "lang": "en",
      "text": "have the full derivation if you want to",
      "offset": 3274.02,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "actually take the actual definition of",
      "offset": 3275.94,
      "duration": 4.02
    },
    {
      "lang": "en",
      "text": "10h and work through the math to figure",
      "offset": 3278.099,
      "duration": 4.441
    },
    {
      "lang": "en",
      "text": "out 1 minus standard square of Z",
      "offset": 3279.96,
      "duration": 5.34
    },
    {
      "lang": "en",
      "text": "so 1 minus a square is the local",
      "offset": 3282.54,
      "duration": 6.66
    },
    {
      "lang": "en",
      "text": "derivative in our case that is 1 minus",
      "offset": 3285.3,
      "duration": 7.019
    },
    {
      "lang": "en",
      "text": "uh the output of 10 H squared which here",
      "offset": 3289.2,
      "duration": 4.379
    },
    {
      "lang": "en",
      "text": "is H",
      "offset": 3292.319,
      "duration": 3.901
    },
    {
      "lang": "en",
      "text": "so it's h squared and that is the local",
      "offset": 3293.579,
      "duration": 5.401
    },
    {
      "lang": "en",
      "text": "derivative and then times the chain rule",
      "offset": 3296.22,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "DH",
      "offset": 3298.98,
      "duration": 3.48
    },
    {
      "lang": "en",
      "text": "so that is going to be our candidate",
      "offset": 3300.78,
      "duration": 4.68
    },
    {
      "lang": "en",
      "text": "implementation so if we come here",
      "offset": 3302.46,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "and then uncomment this let's hope for",
      "offset": 3305.46,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "the best",
      "offset": 3308.7,
      "duration": 3.659
    },
    {
      "lang": "en",
      "text": "and we have the right answer",
      "offset": 3309.78,
      "duration": 5.339
    },
    {
      "lang": "en",
      "text": "okay next up we have DH preact and we",
      "offset": 3312.359,
      "duration": 4.74
    },
    {
      "lang": "en",
      "text": "want to back propagate into the gain the",
      "offset": 3315.119,
      "duration": 4.021
    },
    {
      "lang": "en",
      "text": "B and raw and the B and bias",
      "offset": 3317.099,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "so here this is the bathroom parameters",
      "offset": 3319.14,
      "duration": 4.14
    },
    {
      "lang": "en",
      "text": "being gained in bias inside the bash",
      "offset": 3321.66,
      "duration": 3.959
    },
    {
      "lang": "en",
      "text": "term that take the B and raw that is",
      "offset": 3323.28,
      "duration": 4.92
    },
    {
      "lang": "en",
      "text": "exact unit caution and then scale it and",
      "offset": 3325.619,
      "duration": 3.781
    },
    {
      "lang": "en",
      "text": "shift it",
      "offset": 3328.2,
      "duration": 2.76
    },
    {
      "lang": "en",
      "text": "and these are the parameters of The",
      "offset": 3329.4,
      "duration": 4.38
    },
    {
      "lang": "en",
      "text": "Bachelor now here we have a",
      "offset": 3330.96,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "multiplication but it's worth noting",
      "offset": 3333.78,
      "duration": 2.94
    },
    {
      "lang": "en",
      "text": "that this multiply is very very",
      "offset": 3335.52,
      "duration": 3.18
    },
    {
      "lang": "en",
      "text": "different from this Matrix multiply here",
      "offset": 3336.72,
      "duration": 4.74
    },
    {
      "lang": "en",
      "text": "Matrix multiply are DOT products between",
      "offset": 3338.7,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "rows and Columns of these matrices",
      "offset": 3341.46,
      "duration": 4.02
    },
    {
      "lang": "en",
      "text": "involved this is an element twice",
      "offset": 3343.26,
      "duration": 3.66
    },
    {
      "lang": "en",
      "text": "multiply so things are quite a bit",
      "offset": 3345.48,
      "duration": 2.46
    },
    {
      "lang": "en",
      "text": "simpler",
      "offset": 3346.92,
      "duration": 2.639
    },
    {
      "lang": "en",
      "text": "now we do have to be careful with some",
      "offset": 3347.94,
      "duration": 3.3
    },
    {
      "lang": "en",
      "text": "of the broadcasting happening in this",
      "offset": 3349.559,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "line of code though so you see how BN",
      "offset": 3351.24,
      "duration": 6.96
    },
    {
      "lang": "en",
      "text": "gain and B and bias are 1 by 64. but H",
      "offset": 3353.64,
      "duration": 8.699
    },
    {
      "lang": "en",
      "text": "preact and B and raw are 32 by 64.",
      "offset": 3358.2,
      "duration": 5.82
    },
    {
      "lang": "en",
      "text": "so we have to be careful with that and",
      "offset": 3362.339,
      "duration": 3.181
    },
    {
      "lang": "en",
      "text": "make sure that all the shapes work out",
      "offset": 3364.02,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "fine and that the broadcasting is",
      "offset": 3365.52,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "correctly back propagated",
      "offset": 3366.9,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "so in particular let's start with the B",
      "offset": 3368.52,
      "duration": 6.42
    },
    {
      "lang": "en",
      "text": "and Gain so DB and gain should be",
      "offset": 3370.26,
      "duration": 6.78
    },
    {
      "lang": "en",
      "text": "and here this is again elementorized",
      "offset": 3374.94,
      "duration": 4.26
    },
    {
      "lang": "en",
      "text": "multiply and whenever we have a times b",
      "offset": 3377.04,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "equals c we saw that the local",
      "offset": 3379.2,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "derivative here is just if this is a the",
      "offset": 3381.359,
      "duration": 4.5
    },
    {
      "lang": "en",
      "text": "local derivative is just the B the other",
      "offset": 3383.52,
      "duration": 4.38
    },
    {
      "lang": "en",
      "text": "one so the local derivative is just B",
      "offset": 3385.859,
      "duration": 5.7
    },
    {
      "lang": "en",
      "text": "and raw and then times chain rule",
      "offset": 3387.9,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "so DH preact",
      "offset": 3391.559,
      "duration": 7.321
    },
    {
      "lang": "en",
      "text": "so this is the candidate gradient now",
      "offset": 3394.14,
      "duration": 6.54
    },
    {
      "lang": "en",
      "text": "again we have to be careful because B",
      "offset": 3398.88,
      "duration": 5.82
    },
    {
      "lang": "en",
      "text": "and Gain Is of size 1 by 64. but this",
      "offset": 3400.68,
      "duration": 7.62
    },
    {
      "lang": "en",
      "text": "here would be 32 by 64.",
      "offset": 3404.7,
      "duration": 4.919
    },
    {
      "lang": "en",
      "text": "and so",
      "offset": 3408.3,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "um the correct thing to do in this case",
      "offset": 3409.619,
      "duration": 4.2
    },
    {
      "lang": "en",
      "text": "of course is that b and gain here is a",
      "offset": 3411.3,
      "duration": 4.44
    },
    {
      "lang": "en",
      "text": "rule Vector of 64 numbers it gets",
      "offset": 3413.819,
      "duration": 4.221
    },
    {
      "lang": "en",
      "text": "replicated vertically in this operation",
      "offset": 3415.74,
      "duration": 4.619
    },
    {
      "lang": "en",
      "text": "and so therefore the correct thing to do",
      "offset": 3418.04,
      "duration": 5.46
    },
    {
      "lang": "en",
      "text": "is to sum because it's being replicated",
      "offset": 3420.359,
      "duration": 5.7
    },
    {
      "lang": "en",
      "text": "and therefore all the gradients in each",
      "offset": 3423.5,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "of the rows that are now flowing",
      "offset": 3426.059,
      "duration": 4.201
    },
    {
      "lang": "en",
      "text": "backwards need to sum up to that same",
      "offset": 3427.98,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "tensor DB and Gain so we have to sum",
      "offset": 3430.26,
      "duration": 6.599
    },
    {
      "lang": "en",
      "text": "across all the zero all the examples",
      "offset": 3433.26,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "basically",
      "offset": 3436.859,
      "duration": 2.341
    },
    {
      "lang": "en",
      "text": "which is the direction in which this",
      "offset": 3437.819,
      "duration": 2.581
    },
    {
      "lang": "en",
      "text": "gets replicated",
      "offset": 3439.2,
      "duration": 2.58
    },
    {
      "lang": "en",
      "text": "and now we have to be also careful",
      "offset": 3440.4,
      "duration": 3.3
    },
    {
      "lang": "en",
      "text": "because we",
      "offset": 3441.78,
      "duration": 4.86
    },
    {
      "lang": "en",
      "text": "um being gain is of shape 1 by 64. so in",
      "offset": 3443.7,
      "duration": 5.34
    },
    {
      "lang": "en",
      "text": "fact I need to keep them as true",
      "offset": 3446.64,
      "duration": 5.34
    },
    {
      "lang": "en",
      "text": "otherwise I would just get 64.",
      "offset": 3449.04,
      "duration": 5.34
    },
    {
      "lang": "en",
      "text": "now I don't actually really remember why",
      "offset": 3451.98,
      "duration": 4.859
    },
    {
      "lang": "en",
      "text": "the being gain and the BN bias I made",
      "offset": 3454.38,
      "duration": 5.64
    },
    {
      "lang": "en",
      "text": "them be 1 by 64.",
      "offset": 3456.839,
      "duration": 4.441
    },
    {
      "lang": "en",
      "text": "um",
      "offset": 3460.02,
      "duration": 4.02
    },
    {
      "lang": "en",
      "text": "but the biases B1 and B2 I just made",
      "offset": 3461.28,
      "duration": 4.5
    },
    {
      "lang": "en",
      "text": "them be one-dimensional vectors they're",
      "offset": 3464.04,
      "duration": 3.66
    },
    {
      "lang": "en",
      "text": "not two-dimensional tensors so I can't",
      "offset": 3465.78,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "recall exactly why I left the gain and",
      "offset": 3467.7,
      "duration": 5.58
    },
    {
      "lang": "en",
      "text": "the bias as two-dimensional but it",
      "offset": 3471.3,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "doesn't really matter as long as you are",
      "offset": 3473.28,
      "duration": 2.64
    },
    {
      "lang": "en",
      "text": "consistent and you're keeping it the",
      "offset": 3474.66,
      "duration": 1.919
    },
    {
      "lang": "en",
      "text": "same",
      "offset": 3475.92,
      "duration": 2.1
    },
    {
      "lang": "en",
      "text": "so in this case we want to keep the",
      "offset": 3476.579,
      "duration": 4.681
    },
    {
      "lang": "en",
      "text": "dimension so that the tensor shapes work",
      "offset": 3478.02,
      "duration": 7.74
    },
    {
      "lang": "en",
      "text": "next up we have B and raw so DB and raw",
      "offset": 3481.26,
      "duration": 8.4
    },
    {
      "lang": "en",
      "text": "will be BN gain",
      "offset": 3485.76,
      "duration": 5.78
    },
    {
      "lang": "en",
      "text": "multiplying",
      "offset": 3489.66,
      "duration": 5.82
    },
    {
      "lang": "en",
      "text": "dhreact that's our chain rule now what",
      "offset": 3491.54,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "about the",
      "offset": 3495.48,
      "duration": 2.52
    },
    {
      "lang": "en",
      "text": "um",
      "offset": 3497.22,
      "duration": 2.94
    },
    {
      "lang": "en",
      "text": "dimensions of this we have to be careful",
      "offset": 3498,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "right so DH preact is 32 by 64. B and",
      "offset": 3500.16,
      "duration": 7.14
    },
    {
      "lang": "en",
      "text": "gain is 1 by 64. so it will just get",
      "offset": 3504.48,
      "duration": 5.119
    },
    {
      "lang": "en",
      "text": "replicated and to create this",
      "offset": 3507.3,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "multiplication which is the correct",
      "offset": 3509.599,
      "duration": 4.181
    },
    {
      "lang": "en",
      "text": "thing because in a forward pass it also",
      "offset": 3511.859,
      "duration": 3.96
    },
    {
      "lang": "en",
      "text": "gets replicated in just the same way",
      "offset": 3513.78,
      "duration": 3.539
    },
    {
      "lang": "en",
      "text": "so in fact we don't need the brackets",
      "offset": 3515.819,
      "duration": 2.641
    },
    {
      "lang": "en",
      "text": "here we're done",
      "offset": 3517.319,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "and the shapes are already correct",
      "offset": 3518.46,
      "duration": 4.92
    },
    {
      "lang": "en",
      "text": "and finally for the bias",
      "offset": 3520.92,
      "duration": 5.159
    },
    {
      "lang": "en",
      "text": "very similar this bias here is very very",
      "offset": 3523.38,
      "duration": 4.14
    },
    {
      "lang": "en",
      "text": "similar to the bias we saw when you",
      "offset": 3526.079,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "layer in the linear layer and we see",
      "offset": 3527.52,
      "duration": 4.44
    },
    {
      "lang": "en",
      "text": "that the gradients from each preact will",
      "offset": 3529.44,
      "duration": 4.619
    },
    {
      "lang": "en",
      "text": "simply flow into the biases and add up",
      "offset": 3531.96,
      "duration": 3.96
    },
    {
      "lang": "en",
      "text": "because these are just these are just",
      "offset": 3534.059,
      "duration": 2.941
    },
    {
      "lang": "en",
      "text": "offsets",
      "offset": 3535.92,
      "duration": 3.3
    },
    {
      "lang": "en",
      "text": "and so basically we want this to be DH",
      "offset": 3537,
      "duration": 4.859
    },
    {
      "lang": "en",
      "text": "preact but it needs to Sum along the",
      "offset": 3539.22,
      "duration": 5.16
    },
    {
      "lang": "en",
      "text": "right Dimension and in this case similar",
      "offset": 3541.859,
      "duration": 4.381
    },
    {
      "lang": "en",
      "text": "to the gain we need to sum across the",
      "offset": 3544.38,
      "duration": 4.62
    },
    {
      "lang": "en",
      "text": "zeroth dimension the examples because of",
      "offset": 3546.24,
      "duration": 4.2
    },
    {
      "lang": "en",
      "text": "the way that the bias gets replicated",
      "offset": 3549,
      "duration": 2.7
    },
    {
      "lang": "en",
      "text": "vertically",
      "offset": 3550.44,
      "duration": 4.02
    },
    {
      "lang": "en",
      "text": "and we also want to have keep them as",
      "offset": 3551.7,
      "duration": 3.659
    },
    {
      "lang": "en",
      "text": "true",
      "offset": 3554.46,
      "duration": 3.18
    },
    {
      "lang": "en",
      "text": "and so this will basically take this and",
      "offset": 3555.359,
      "duration": 5.281
    },
    {
      "lang": "en",
      "text": "sum it up and give us a 1 by 64.",
      "offset": 3557.64,
      "duration": 5.4
    },
    {
      "lang": "en",
      "text": "so this is the candidate implementation",
      "offset": 3560.64,
      "duration": 4.679
    },
    {
      "lang": "en",
      "text": "it makes all the shapes work",
      "offset": 3563.04,
      "duration": 5.94
    },
    {
      "lang": "en",
      "text": "let me bring it up down here and then",
      "offset": 3565.319,
      "duration": 6.721
    },
    {
      "lang": "en",
      "text": "let me uncomment these three lines",
      "offset": 3568.98,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "to check that we are getting the correct",
      "offset": 3572.04,
      "duration": 4.38
    },
    {
      "lang": "en",
      "text": "result for all the three tensors and",
      "offset": 3573.54,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "indeed we see that all of that got back",
      "offset": 3576.42,
      "duration": 4.139
    },
    {
      "lang": "en",
      "text": "propagated correctly so now we get to",
      "offset": 3578.339,
      "duration": 4.621
    },
    {
      "lang": "en",
      "text": "the batch Norm layer we see how here",
      "offset": 3580.559,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "being gay and being bias are the",
      "offset": 3582.96,
      "duration": 3.659
    },
    {
      "lang": "en",
      "text": "parameters so the back propagation ends",
      "offset": 3584.64,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "but B and raw now is the output of the",
      "offset": 3586.619,
      "duration": 5.161
    },
    {
      "lang": "en",
      "text": "standardization",
      "offset": 3590.16,
      "duration": 3.3
    },
    {
      "lang": "en",
      "text": "so here what I'm doing of course is I'm",
      "offset": 3591.78,
      "duration": 2.76
    },
    {
      "lang": "en",
      "text": "breaking up the batch form into",
      "offset": 3593.46,
      "duration": 2.52
    },
    {
      "lang": "en",
      "text": "manageable pieces so we can back",
      "offset": 3594.54,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "propagate through each line individually",
      "offset": 3595.98,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "but basically what's happening is BN",
      "offset": 3597.66,
      "duration": 6.06
    },
    {
      "lang": "en",
      "text": "mean I is the sum",
      "offset": 3600.54,
      "duration": 6.42
    },
    {
      "lang": "en",
      "text": "so this is the B and mean I I apologize",
      "offset": 3603.72,
      "duration": 6.42
    },
    {
      "lang": "en",
      "text": "for the variable naming B and diff is x",
      "offset": 3606.96,
      "duration": 4.5
    },
    {
      "lang": "en",
      "text": "minus mu",
      "offset": 3610.14,
      "duration": 4.86
    },
    {
      "lang": "en",
      "text": "B and div 2 is x minus mu squared here",
      "offset": 3611.46,
      "duration": 5.159
    },
    {
      "lang": "en",
      "text": "inside the variance",
      "offset": 3615,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "B and VAR is the variance so uh Sigma",
      "offset": 3616.619,
      "duration": 6.18
    },
    {
      "lang": "en",
      "text": "Square this is B and bar and it's",
      "offset": 3620.52,
      "duration": 4.74
    },
    {
      "lang": "en",
      "text": "basically the sum of squares",
      "offset": 3622.799,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "so this is the x minus mu squared and",
      "offset": 3625.26,
      "duration": 5.46
    },
    {
      "lang": "en",
      "text": "then the sum now you'll notice one",
      "offset": 3628.559,
      "duration": 3.54
    },
    {
      "lang": "en",
      "text": "departure here",
      "offset": 3630.72,
      "duration": 4.26
    },
    {
      "lang": "en",
      "text": "here it is normalized as 1 over m",
      "offset": 3632.099,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "uh which is number of examples here I'm",
      "offset": 3634.98,
      "duration": 4.859
    },
    {
      "lang": "en",
      "text": "normalizing as one over n minus 1",
      "offset": 3637.859,
      "duration": 4.141
    },
    {
      "lang": "en",
      "text": "instead of N and this is deliberate and",
      "offset": 3639.839,
      "duration": 3.78
    },
    {
      "lang": "en",
      "text": "I'll come back to that in a bit when we",
      "offset": 3642,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "are at this line it is something called",
      "offset": 3643.619,
      "duration": 4.2
    },
    {
      "lang": "en",
      "text": "the bezels correction",
      "offset": 3645.599,
      "duration": 5.48
    },
    {
      "lang": "en",
      "text": "but this is how I want it in our case",
      "offset": 3647.819,
      "duration": 5.821
    },
    {
      "lang": "en",
      "text": "bienvar inv then becomes basically",
      "offset": 3651.079,
      "duration": 5.621
    },
    {
      "lang": "en",
      "text": "bienvar plus Epsilon Epsilon is one",
      "offset": 3653.64,
      "duration": 5.159
    },
    {
      "lang": "en",
      "text": "negative five and then it's one over",
      "offset": 3656.7,
      "duration": 3.24
    },
    {
      "lang": "en",
      "text": "square root",
      "offset": 3658.799,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "is the same as raising to the power of",
      "offset": 3659.94,
      "duration": 5.7
    },
    {
      "lang": "en",
      "text": "negative 0.5 right because 0.5 is square",
      "offset": 3662.4,
      "duration": 5.399
    },
    {
      "lang": "en",
      "text": "root and then negative makes it one over",
      "offset": 3665.64,
      "duration": 3.24
    },
    {
      "lang": "en",
      "text": "square root",
      "offset": 3667.799,
      "duration": 4.441
    },
    {
      "lang": "en",
      "text": "so BM Bar M is a one over this uh",
      "offset": 3668.88,
      "duration": 5.459
    },
    {
      "lang": "en",
      "text": "denominator here and then we can see",
      "offset": 3672.24,
      "duration": 4.26
    },
    {
      "lang": "en",
      "text": "that b and raw which is the X hat here",
      "offset": 3674.339,
      "duration": 5.361
    },
    {
      "lang": "en",
      "text": "is equal to the BN diff the numerator",
      "offset": 3676.5,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "multiplied by the",
      "offset": 3679.7,
      "duration": 5.08
    },
    {
      "lang": "en",
      "text": "um BN bar in",
      "offset": 3682.26,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "and this line here that creates pre-h",
      "offset": 3684.78,
      "duration": 4.26
    },
    {
      "lang": "en",
      "text": "pre-act was the last piece we've already",
      "offset": 3687.299,
      "duration": 4.26
    },
    {
      "lang": "en",
      "text": "back propagated through it",
      "offset": 3689.04,
      "duration": 4.98
    },
    {
      "lang": "en",
      "text": "so now what we want to do is we are here",
      "offset": 3691.559,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "and we have B and raw and we have to",
      "offset": 3694.02,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "first back propagate into B and diff and",
      "offset": 3695.88,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "B and Bar M",
      "offset": 3698.819,
      "duration": 4.681
    },
    {
      "lang": "en",
      "text": "so now we're here and we have DB and raw",
      "offset": 3700.68,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "and we need to back propagate through",
      "offset": 3703.5,
      "duration": 2.839
    },
    {
      "lang": "en",
      "text": "this line",
      "offset": 3705.24,
      "duration": 4.02
    },
    {
      "lang": "en",
      "text": "now I've written out the shapes here and",
      "offset": 3706.339,
      "duration": 6.881
    },
    {
      "lang": "en",
      "text": "indeed bien VAR m is a shape 1 by 64. so",
      "offset": 3709.26,
      "duration": 6.18
    },
    {
      "lang": "en",
      "text": "there is a broadcasting happening here",
      "offset": 3713.22,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "that we have to be careful with but it",
      "offset": 3715.44,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "is just an element-wise simple",
      "offset": 3717.54,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "multiplication by now we should be",
      "offset": 3718.799,
      "duration": 3.78
    },
    {
      "lang": "en",
      "text": "pretty comfortable with that to get DB",
      "offset": 3720.42,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "and diff we know that this is just B and",
      "offset": 3722.579,
      "duration": 3.961
    },
    {
      "lang": "en",
      "text": "varm",
      "offset": 3725.46,
      "duration": 2.7
    },
    {
      "lang": "en",
      "text": "multiplied with",
      "offset": 3726.54,
      "duration": 4.759
    },
    {
      "lang": "en",
      "text": "DP and raw",
      "offset": 3728.16,
      "duration": 3.139
    },
    {
      "lang": "en",
      "text": "and conversely to get dbmring",
      "offset": 3731.579,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "we need to take the end if",
      "offset": 3735,
      "duration": 6.799
    },
    {
      "lang": "en",
      "text": "and multiply that by DB and raw",
      "offset": 3737.819,
      "duration": 3.98
    },
    {
      "lang": "en",
      "text": "so this is the candidate but of course",
      "offset": 3742.38,
      "duration": 4.14
    },
    {
      "lang": "en",
      "text": "we need to make sure that broadcasting",
      "offset": 3744.66,
      "duration": 5.1
    },
    {
      "lang": "en",
      "text": "is obeyed so in particular B and VAR M",
      "offset": 3746.52,
      "duration": 5.22
    },
    {
      "lang": "en",
      "text": "multiplying with DB and raw",
      "offset": 3749.76,
      "duration": 5.88
    },
    {
      "lang": "en",
      "text": "will be okay and give us 32 by 64 as we",
      "offset": 3751.74,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "expect",
      "offset": 3755.64,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "but dbm VAR inv would be taking a 32 by",
      "offset": 3756.78,
      "duration": 5.579
    },
    {
      "lang": "en",
      "text": "64.",
      "offset": 3760.68,
      "duration": 4.74
    },
    {
      "lang": "en",
      "text": "multiplying it by 32 by 64. so this is a",
      "offset": 3762.359,
      "duration": 7.321
    },
    {
      "lang": "en",
      "text": "32 by 64. but of course DB this uh B and",
      "offset": 3765.42,
      "duration": 7.32
    },
    {
      "lang": "en",
      "text": "VAR in is only 1 by 64. so the second",
      "offset": 3769.68,
      "duration": 5.939
    },
    {
      "lang": "en",
      "text": "line here needs a sum across the",
      "offset": 3772.74,
      "duration": 5.099
    },
    {
      "lang": "en",
      "text": "examples and because there's this",
      "offset": 3775.619,
      "duration": 4.381
    },
    {
      "lang": "en",
      "text": "Dimension here we need to make sure that",
      "offset": 3777.839,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "keep them is true",
      "offset": 3780,
      "duration": 4.92
    },
    {
      "lang": "en",
      "text": "so this is the candidate",
      "offset": 3782.16,
      "duration": 5.34
    },
    {
      "lang": "en",
      "text": "let's erase this and let's swing down",
      "offset": 3784.92,
      "duration": 4.139
    },
    {
      "lang": "en",
      "text": "here",
      "offset": 3787.5,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "and implement it and then let's comment",
      "offset": 3789.059,
      "duration": 7.141
    },
    {
      "lang": "en",
      "text": "out dbm barif and DB and diff",
      "offset": 3791.579,
      "duration": 7.26
    },
    {
      "lang": "en",
      "text": "now we'll actually notice that DB and",
      "offset": 3796.2,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "diff by the way is going to be incorrect",
      "offset": 3798.839,
      "duration": 5.541
    },
    {
      "lang": "en",
      "text": "so when I run this",
      "offset": 3802.2,
      "duration": 5.639
    },
    {
      "lang": "en",
      "text": "BMR m is correct B and diff is not",
      "offset": 3804.38,
      "duration": 5.739
    },
    {
      "lang": "en",
      "text": "correct and this is actually expected",
      "offset": 3807.839,
      "duration": 6.301
    },
    {
      "lang": "en",
      "text": "because we're not done with b and diff",
      "offset": 3810.119,
      "duration": 5.881
    },
    {
      "lang": "en",
      "text": "so in particular when we slide here we",
      "offset": 3814.14,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "see here that b and raw as a function of",
      "offset": 3816,
      "duration": 4.98
    },
    {
      "lang": "en",
      "text": "B and diff but actually B and far of is",
      "offset": 3817.74,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "a function of B of R which is a function",
      "offset": 3820.98,
      "duration": 3.54
    },
    {
      "lang": "en",
      "text": "of B and df2 which is a function of B",
      "offset": 3822.54,
      "duration": 2.819
    },
    {
      "lang": "en",
      "text": "and diff",
      "offset": 3824.52,
      "duration": 4.26
    },
    {
      "lang": "en",
      "text": "so it comes here so bdn diff",
      "offset": 3825.359,
      "duration": 5.401
    },
    {
      "lang": "en",
      "text": "um these variable names are crazy I'm",
      "offset": 3828.78,
      "duration": 4.62
    },
    {
      "lang": "en",
      "text": "sorry it branches out into two branches",
      "offset": 3830.76,
      "duration": 4.92
    },
    {
      "lang": "en",
      "text": "and we've only done one branch of it we",
      "offset": 3833.4,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "have to continue our back propagation",
      "offset": 3835.68,
      "duration": 2.879
    },
    {
      "lang": "en",
      "text": "and eventually come back to B and diff",
      "offset": 3837,
      "duration": 3.119
    },
    {
      "lang": "en",
      "text": "and then we'll be able to do a plus",
      "offset": 3838.559,
      "duration": 4.02
    },
    {
      "lang": "en",
      "text": "equals and get the actual card gradient",
      "offset": 3840.119,
      "duration": 5.041
    },
    {
      "lang": "en",
      "text": "for now it is good to verify that CMP",
      "offset": 3842.579,
      "duration": 4.681
    },
    {
      "lang": "en",
      "text": "also works it doesn't just lie to us and",
      "offset": 3845.16,
      "duration": 3.419
    },
    {
      "lang": "en",
      "text": "tell us that everything is always",
      "offset": 3847.26,
      "duration": 4.14
    },
    {
      "lang": "en",
      "text": "correct it can in fact detect when your",
      "offset": 3848.579,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "gradient is not correct so it's that's",
      "offset": 3851.4,
      "duration": 4.38
    },
    {
      "lang": "en",
      "text": "good to see as well okay so now we have",
      "offset": 3853.859,
      "duration": 3.48
    },
    {
      "lang": "en",
      "text": "the derivative here and we're trying to",
      "offset": 3855.78,
      "duration": 3.18
    },
    {
      "lang": "en",
      "text": "back propagate through this line",
      "offset": 3857.339,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "and because we're raising to a power of",
      "offset": 3858.96,
      "duration": 4.44
    },
    {
      "lang": "en",
      "text": "negative 0.5 I brought up the power rule",
      "offset": 3861.18,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "and we see that basically we have that",
      "offset": 3863.4,
      "duration": 4.919
    },
    {
      "lang": "en",
      "text": "the BM bar will now be we bring down the",
      "offset": 3865.26,
      "duration": 6.299
    },
    {
      "lang": "en",
      "text": "exponent so negative 0.5 times",
      "offset": 3868.319,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "uh X which is this",
      "offset": 3871.559,
      "duration": 5.101
    },
    {
      "lang": "en",
      "text": "and now raised to the power of negative",
      "offset": 3874.799,
      "duration": 4.861
    },
    {
      "lang": "en",
      "text": "0.5 minus 1 which is negative 1.5",
      "offset": 3876.66,
      "duration": 5.939
    },
    {
      "lang": "en",
      "text": "now we would have to also apply a small",
      "offset": 3879.66,
      "duration": 5.58
    },
    {
      "lang": "en",
      "text": "chain rule here in our head because we",
      "offset": 3882.599,
      "duration": 5.401
    },
    {
      "lang": "en",
      "text": "need to take further the derivative of B",
      "offset": 3885.24,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "and VAR with respect to this expression",
      "offset": 3888,
      "duration": 3.9
    },
    {
      "lang": "en",
      "text": "here inside the bracket but because this",
      "offset": 3889.799,
      "duration": 3.661
    },
    {
      "lang": "en",
      "text": "is an elementalized operation and",
      "offset": 3891.9,
      "duration": 3.06
    },
    {
      "lang": "en",
      "text": "everything is fairly simple that's just",
      "offset": 3893.46,
      "duration": 4.26
    },
    {
      "lang": "en",
      "text": "one and so there's nothing to do there",
      "offset": 3894.96,
      "duration": 5.339
    },
    {
      "lang": "en",
      "text": "so this is the local derivative and then",
      "offset": 3897.72,
      "duration": 4.2
    },
    {
      "lang": "en",
      "text": "times the global derivative to create",
      "offset": 3900.299,
      "duration": 4.02
    },
    {
      "lang": "en",
      "text": "the chain rule this is just times the BM",
      "offset": 3901.92,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "bar have",
      "offset": 3904.319,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "so this is our candidate let me bring",
      "offset": 3905.52,
      "duration": 4.92
    },
    {
      "lang": "en",
      "text": "this down",
      "offset": 3908.4,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "and uncommon to the check",
      "offset": 3910.44,
      "duration": 3.56
    },
    {
      "lang": "en",
      "text": "and we see that we have the correct",
      "offset": 3914.46,
      "duration": 2.94
    },
    {
      "lang": "en",
      "text": "result",
      "offset": 3916.26,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "now before we propagate through the next",
      "offset": 3917.4,
      "duration": 3.48
    },
    {
      "lang": "en",
      "text": "line I want to briefly talk about the",
      "offset": 3919.14,
      "duration": 3.179
    },
    {
      "lang": "en",
      "text": "note here where I'm using the bezels",
      "offset": 3920.88,
      "duration": 3.719
    },
    {
      "lang": "en",
      "text": "correction dividing by n minus 1 instead",
      "offset": 3922.319,
      "duration": 4.861
    },
    {
      "lang": "en",
      "text": "of dividing by n when I normalize here",
      "offset": 3924.599,
      "duration": 5.101
    },
    {
      "lang": "en",
      "text": "the sum of squares",
      "offset": 3927.18,
      "duration": 4.139
    },
    {
      "lang": "en",
      "text": "now you'll notice that this is departure",
      "offset": 3929.7,
      "duration": 3.78
    },
    {
      "lang": "en",
      "text": "from the paper which uses one over n",
      "offset": 3931.319,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "instead not one over n minus one their m",
      "offset": 3933.48,
      "duration": 4.619
    },
    {
      "lang": "en",
      "text": "is RN",
      "offset": 3936.599,
      "duration": 2.52
    },
    {
      "lang": "en",
      "text": "and",
      "offset": 3938.099,
      "duration": 2.52
    },
    {
      "lang": "en",
      "text": "um so it turns out that there are two",
      "offset": 3939.119,
      "duration": 4.261
    },
    {
      "lang": "en",
      "text": "ways of estimating variance of an array",
      "offset": 3940.619,
      "duration": 6.18
    },
    {
      "lang": "en",
      "text": "one is the biased estimate which is one",
      "offset": 3943.38,
      "duration": 5.64
    },
    {
      "lang": "en",
      "text": "over n and the other one is the unbiased",
      "offset": 3946.799,
      "duration": 4.5
    },
    {
      "lang": "en",
      "text": "estimate which is one over n minus one",
      "offset": 3949.02,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "now confusingly in the paper this is uh",
      "offset": 3951.299,
      "duration": 5.101
    },
    {
      "lang": "en",
      "text": "not very clearly described and also it's",
      "offset": 3954.059,
      "duration": 4.74
    },
    {
      "lang": "en",
      "text": "a detail that kind of matters I think",
      "offset": 3956.4,
      "duration": 4.02
    },
    {
      "lang": "en",
      "text": "um they are using the biased version",
      "offset": 3958.799,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "training time but later when they are",
      "offset": 3960.42,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "talking about the inference they are",
      "offset": 3962.64,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "mentioning that when they do the",
      "offset": 3964.74,
      "duration": 3.78
    },
    {
      "lang": "en",
      "text": "inference they are using the unbiased",
      "offset": 3966,
      "duration": 4.5
    },
    {
      "lang": "en",
      "text": "estimate which is the n minus one",
      "offset": 3968.52,
      "duration": 3.9
    },
    {
      "lang": "en",
      "text": "version in",
      "offset": 3970.5,
      "duration": 2.46
    },
    {
      "lang": "en",
      "text": "um",
      "offset": 3972.42,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "basically for inference",
      "offset": 3972.96,
      "duration": 5.399
    },
    {
      "lang": "en",
      "text": "and to calibrate the running mean and",
      "offset": 3975.54,
      "duration": 5.16
    },
    {
      "lang": "en",
      "text": "the running variance basically and so",
      "offset": 3978.359,
      "duration": 4.2
    },
    {
      "lang": "en",
      "text": "they they actually introduce a trained",
      "offset": 3980.7,
      "duration": 3.78
    },
    {
      "lang": "en",
      "text": "test mismatch where in training they use",
      "offset": 3982.559,
      "duration": 3.78
    },
    {
      "lang": "en",
      "text": "the biased version and in the in test",
      "offset": 3984.48,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "time they use the unbiased version I",
      "offset": 3986.339,
      "duration": 4.2
    },
    {
      "lang": "en",
      "text": "find this extremely confusing you can",
      "offset": 3988.559,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "read more about the bezels correction",
      "offset": 3990.539,
      "duration": 4.861
    },
    {
      "lang": "en",
      "text": "and why uh dividing by n minus one gives",
      "offset": 3992.16,
      "duration": 5.1
    },
    {
      "lang": "en",
      "text": "you a better estimate of the variance in",
      "offset": 3995.4,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "a case where you have population size or",
      "offset": 3997.26,
      "duration": 3.74
    },
    {
      "lang": "en",
      "text": "samples for the population",
      "offset": 3999.48,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "that are very small and that is indeed",
      "offset": 4001,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "the case for us because we are dealing",
      "offset": 4004.28,
      "duration": 3.779
    },
    {
      "lang": "en",
      "text": "with many patches and these mini matches",
      "offset": 4006.2,
      "duration": 4.139
    },
    {
      "lang": "en",
      "text": "are a small sample of a larger",
      "offset": 4008.059,
      "duration": 3.961
    },
    {
      "lang": "en",
      "text": "population which is the entire training",
      "offset": 4010.339,
      "duration": 4.921
    },
    {
      "lang": "en",
      "text": "set and so it just turns out that if you",
      "offset": 4012.02,
      "duration": 5.16
    },
    {
      "lang": "en",
      "text": "just estimate it using one over n that",
      "offset": 4015.26,
      "duration": 3.66
    },
    {
      "lang": "en",
      "text": "actually almost always underestimates",
      "offset": 4017.18,
      "duration": 3.78
    },
    {
      "lang": "en",
      "text": "the variance and it is a biased",
      "offset": 4018.92,
      "duration": 3.899
    },
    {
      "lang": "en",
      "text": "estimator and it is advised that you use",
      "offset": 4020.96,
      "duration": 3.659
    },
    {
      "lang": "en",
      "text": "the unbiased version and divide by n",
      "offset": 4022.819,
      "duration": 3.48
    },
    {
      "lang": "en",
      "text": "minus one and you can go through this",
      "offset": 4024.619,
      "duration": 3.661
    },
    {
      "lang": "en",
      "text": "article here that I liked that actually",
      "offset": 4026.299,
      "duration": 3.661
    },
    {
      "lang": "en",
      "text": "describes the full reasoning and I'll",
      "offset": 4028.28,
      "duration": 3.96
    },
    {
      "lang": "en",
      "text": "link it in the video description",
      "offset": 4029.96,
      "duration": 3.899
    },
    {
      "lang": "en",
      "text": "now when you calculate the torture",
      "offset": 4032.24,
      "duration": 3.059
    },
    {
      "lang": "en",
      "text": "variance",
      "offset": 4033.859,
      "duration": 2.641
    },
    {
      "lang": "en",
      "text": "you'll notice that they take the",
      "offset": 4035.299,
      "duration": 3.06
    },
    {
      "lang": "en",
      "text": "unbiased flag whether or not you want to",
      "offset": 4036.5,
      "duration": 4.68
    },
    {
      "lang": "en",
      "text": "divide by n or n minus one confusingly",
      "offset": 4038.359,
      "duration": 6.121
    },
    {
      "lang": "en",
      "text": "they do not mention what the default is",
      "offset": 4041.18,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "for unbiased but I believe unbiased by",
      "offset": 4044.48,
      "duration": 4.92
    },
    {
      "lang": "en",
      "text": "default is true I'm not sure why the",
      "offset": 4046.94,
      "duration": 4.379
    },
    {
      "lang": "en",
      "text": "docs here don't cite that",
      "offset": 4049.4,
      "duration": 4.02
    },
    {
      "lang": "en",
      "text": "now in The Bachelor",
      "offset": 4051.319,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "1D the documentation again is kind of",
      "offset": 4053.42,
      "duration": 4.919
    },
    {
      "lang": "en",
      "text": "wrong and confusing it says that the",
      "offset": 4055.88,
      "duration": 4.02
    },
    {
      "lang": "en",
      "text": "standard deviation is calculated via the",
      "offset": 4058.339,
      "duration": 3.061
    },
    {
      "lang": "en",
      "text": "biased estimator",
      "offset": 4059.9,
      "duration": 3.24
    },
    {
      "lang": "en",
      "text": "but this is actually not exactly right",
      "offset": 4061.4,
      "duration": 3.24
    },
    {
      "lang": "en",
      "text": "and people have pointed out that it is",
      "offset": 4063.14,
      "duration": 3.659
    },
    {
      "lang": "en",
      "text": "not right in a number of issues since",
      "offset": 4064.64,
      "duration": 4.74
    },
    {
      "lang": "en",
      "text": "then because actually the rabbit hole is",
      "offset": 4066.799,
      "duration": 5.221
    },
    {
      "lang": "en",
      "text": "deeper and they follow the paper exactly",
      "offset": 4069.38,
      "duration": 4.919
    },
    {
      "lang": "en",
      "text": "and they use the biased version for",
      "offset": 4072.02,
      "duration": 4.14
    },
    {
      "lang": "en",
      "text": "training but when they're estimating the",
      "offset": 4074.299,
      "duration": 4.02
    },
    {
      "lang": "en",
      "text": "running standard deviation we are using",
      "offset": 4076.16,
      "duration": 4.199
    },
    {
      "lang": "en",
      "text": "the unbiased version so again there's",
      "offset": 4078.319,
      "duration": 4.381
    },
    {
      "lang": "en",
      "text": "the train test mismatch so long story",
      "offset": 4080.359,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "short I'm not a fan of trained test",
      "offset": 4082.7,
      "duration": 4.74
    },
    {
      "lang": "en",
      "text": "discrepancies I basically kind of",
      "offset": 4085.16,
      "duration": 3.379
    },
    {
      "lang": "en",
      "text": "consider",
      "offset": 4087.44,
      "duration": 3.54
    },
    {
      "lang": "en",
      "text": "the fact that we use the bias version",
      "offset": 4088.539,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "the training time and the unbiased test",
      "offset": 4090.98,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "time I basically consider this to be a",
      "offset": 4093.02,
      "duration": 3.299
    },
    {
      "lang": "en",
      "text": "bug and I don't think that there's a",
      "offset": 4094.579,
      "duration": 3.66
    },
    {
      "lang": "en",
      "text": "good reason for that it's not really",
      "offset": 4096.319,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "they don't really go into the detail of",
      "offset": 4098.239,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "the reasoning behind it in this paper so",
      "offset": 4099.92,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "that's why I basically prefer to use the",
      "offset": 4102.319,
      "duration": 3.98
    },
    {
      "lang": "en",
      "text": "bestless correction in my own work",
      "offset": 4104.48,
      "duration": 4.739
    },
    {
      "lang": "en",
      "text": "unfortunately Bastion does not take a",
      "offset": 4106.299,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "keyword argument that tells you whether",
      "offset": 4109.219,
      "duration": 3.96
    },
    {
      "lang": "en",
      "text": "or not you want to use the unbiased",
      "offset": 4110.54,
      "duration": 4.139
    },
    {
      "lang": "en",
      "text": "version of the bias version in both",
      "offset": 4113.179,
      "duration": 3.421
    },
    {
      "lang": "en",
      "text": "train and test and so therefore anyone",
      "offset": 4114.679,
      "duration": 4.02
    },
    {
      "lang": "en",
      "text": "using batch normalization basically in",
      "offset": 4116.6,
      "duration": 4.619
    },
    {
      "lang": "en",
      "text": "my view has a bit of a bug in the code",
      "offset": 4118.699,
      "duration": 3.301
    },
    {
      "lang": "en",
      "text": "um",
      "offset": 4121.219,
      "duration": 3.241
    },
    {
      "lang": "en",
      "text": "and this turns out to be much less of a",
      "offset": 4122,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "problem if your batch mini batch sizes",
      "offset": 4124.46,
      "duration": 4.2
    },
    {
      "lang": "en",
      "text": "are a bit larger but still I just might",
      "offset": 4126.799,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "kind of uh unpardable so maybe someone",
      "offset": 4128.66,
      "duration": 4.98
    },
    {
      "lang": "en",
      "text": "can explain why this is okay but for now",
      "offset": 4131.359,
      "duration": 3.9
    },
    {
      "lang": "en",
      "text": "I prefer to use the unbiased version",
      "offset": 4133.64,
      "duration": 4.199
    },
    {
      "lang": "en",
      "text": "consistently both during training and at",
      "offset": 4135.259,
      "duration": 4.92
    },
    {
      "lang": "en",
      "text": "this time and that's why I'm using one",
      "offset": 4137.839,
      "duration": 4.02
    },
    {
      "lang": "en",
      "text": "over n minus one here",
      "offset": 4140.179,
      "duration": 2.881
    },
    {
      "lang": "en",
      "text": "okay so let's now actually back",
      "offset": 4141.859,
      "duration": 3.781
    },
    {
      "lang": "en",
      "text": "propagate through this line",
      "offset": 4143.06,
      "duration": 4.259
    },
    {
      "lang": "en",
      "text": "so",
      "offset": 4145.64,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "the first thing that I always like to do",
      "offset": 4147.319,
      "duration": 3.181
    },
    {
      "lang": "en",
      "text": "is I like to scrutinize the shapes first",
      "offset": 4148.64,
      "duration": 4.26
    },
    {
      "lang": "en",
      "text": "so in particular here looking at the",
      "offset": 4150.5,
      "duration": 4.44
    },
    {
      "lang": "en",
      "text": "shapes of what's involved I see that b",
      "offset": 4152.9,
      "duration": 6.06
    },
    {
      "lang": "en",
      "text": "and VAR shape is 1 by 64. so it's a row",
      "offset": 4154.94,
      "duration": 6.66
    },
    {
      "lang": "en",
      "text": "vector and BND if two dot shape is 32 by",
      "offset": 4158.96,
      "duration": 3.779
    },
    {
      "lang": "en",
      "text": "64.",
      "offset": 4161.6,
      "duration": 3.48
    },
    {
      "lang": "en",
      "text": "so clearly here we're doing a sum over",
      "offset": 4162.739,
      "duration": 6.12
    },
    {
      "lang": "en",
      "text": "the zeroth axis to squash the first",
      "offset": 4165.08,
      "duration": 7.619
    },
    {
      "lang": "en",
      "text": "dimension of of the shapes here using a",
      "offset": 4168.859,
      "duration": 6.181
    },
    {
      "lang": "en",
      "text": "sum so that right away actually hints to",
      "offset": 4172.699,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "me that there will be some kind of a",
      "offset": 4175.04,
      "duration": 3.719
    },
    {
      "lang": "en",
      "text": "replication or broadcasting in the",
      "offset": 4176.779,
      "duration": 4.02
    },
    {
      "lang": "en",
      "text": "backward pass and maybe you're noticing",
      "offset": 4178.759,
      "duration": 3.901
    },
    {
      "lang": "en",
      "text": "the pattern here but basically anytime",
      "offset": 4180.799,
      "duration": 4.621
    },
    {
      "lang": "en",
      "text": "you have a sum in the forward pass that",
      "offset": 4182.66,
      "duration": 5.099
    },
    {
      "lang": "en",
      "text": "turns into a replication or broadcasting",
      "offset": 4185.42,
      "duration": 3.72
    },
    {
      "lang": "en",
      "text": "in the backward pass along the same",
      "offset": 4187.759,
      "duration": 4.381
    },
    {
      "lang": "en",
      "text": "Dimension and conversely when we have a",
      "offset": 4189.14,
      "duration": 5.7
    },
    {
      "lang": "en",
      "text": "replication or a broadcasting in the",
      "offset": 4192.14,
      "duration": 4.98
    },
    {
      "lang": "en",
      "text": "forward pass that indicates a variable",
      "offset": 4194.84,
      "duration": 5.1
    },
    {
      "lang": "en",
      "text": "reuse and so in the backward pass that",
      "offset": 4197.12,
      "duration": 4.68
    },
    {
      "lang": "en",
      "text": "turns into a sum over the exact same",
      "offset": 4199.94,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "dimension",
      "offset": 4201.8,
      "duration": 2.64
    },
    {
      "lang": "en",
      "text": "and so hopefully you're noticing that",
      "offset": 4202.82,
      "duration": 3.24
    },
    {
      "lang": "en",
      "text": "Duality that those two are kind of like",
      "offset": 4204.44,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "the opposite of each other in the",
      "offset": 4206.06,
      "duration": 3.06
    },
    {
      "lang": "en",
      "text": "forward and backward pass",
      "offset": 4207.44,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "now once we understand the shapes the",
      "offset": 4209.12,
      "duration": 3.78
    },
    {
      "lang": "en",
      "text": "next thing I like to do always is I like",
      "offset": 4211.52,
      "duration": 3.48
    },
    {
      "lang": "en",
      "text": "to look at a toy example in my head to",
      "offset": 4212.9,
      "duration": 3.779
    },
    {
      "lang": "en",
      "text": "sort of just like understand roughly how",
      "offset": 4215,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "uh the variable the variable",
      "offset": 4216.679,
      "duration": 3.301
    },
    {
      "lang": "en",
      "text": "dependencies go in the mathematical",
      "offset": 4218.36,
      "duration": 3.06
    },
    {
      "lang": "en",
      "text": "formula",
      "offset": 4219.98,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "so here we have a two-dimensional array",
      "offset": 4221.42,
      "duration": 5.279
    },
    {
      "lang": "en",
      "text": "of the end of two which we are scaling",
      "offset": 4224.3,
      "duration": 5.58
    },
    {
      "lang": "en",
      "text": "by a constant and then we are summing uh",
      "offset": 4226.699,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "vertically over the columns so if we",
      "offset": 4229.88,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "have a two by two Matrix a and then we",
      "offset": 4232.219,
      "duration": 3.96
    },
    {
      "lang": "en",
      "text": "sum over the columns and scale we would",
      "offset": 4233.96,
      "duration": 5.82
    },
    {
      "lang": "en",
      "text": "get a row Vector B1 B2 and B1 depends on",
      "offset": 4236.179,
      "duration": 6.181
    },
    {
      "lang": "en",
      "text": "a in this way whereas just sum they're",
      "offset": 4239.78,
      "duration": 5.879
    },
    {
      "lang": "en",
      "text": "scaled of a and B2 in this way where",
      "offset": 4242.36,
      "duration": 6.54
    },
    {
      "lang": "en",
      "text": "it's the second column sump and scale",
      "offset": 4245.659,
      "duration": 6.421
    },
    {
      "lang": "en",
      "text": "and so looking at this basically",
      "offset": 4248.9,
      "duration": 4.62
    },
    {
      "lang": "en",
      "text": "what we want to do now is we have the",
      "offset": 4252.08,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "derivatives on B1 and B2 and we want to",
      "offset": 4253.52,
      "duration": 4.74
    },
    {
      "lang": "en",
      "text": "back propagate them into Ace and so it's",
      "offset": 4255.679,
      "duration": 3.961
    },
    {
      "lang": "en",
      "text": "clear that just differentiating in your",
      "offset": 4258.26,
      "duration": 3.66
    },
    {
      "lang": "en",
      "text": "head the local derivative here is one",
      "offset": 4259.64,
      "duration": 5.7
    },
    {
      "lang": "en",
      "text": "over n minus 1 times uh one",
      "offset": 4261.92,
      "duration": 7.44
    },
    {
      "lang": "en",
      "text": "uh for each one of these A's and um",
      "offset": 4265.34,
      "duration": 6.12
    },
    {
      "lang": "en",
      "text": "basically the derivative of B1 has to",
      "offset": 4269.36,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "flow through The Columns of a",
      "offset": 4271.46,
      "duration": 5.1
    },
    {
      "lang": "en",
      "text": "scaled by one over n minus one",
      "offset": 4273.92,
      "duration": 4.68
    },
    {
      "lang": "en",
      "text": "and that's roughly What's Happening Here",
      "offset": 4276.56,
      "duration": 4.92
    },
    {
      "lang": "en",
      "text": "so intuitively the derivative flow tells",
      "offset": 4278.6,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "us that DB and diff2",
      "offset": 4281.48,
      "duration": 6.3
    },
    {
      "lang": "en",
      "text": "will be the local derivative of this",
      "offset": 4284.84,
      "duration": 4.98
    },
    {
      "lang": "en",
      "text": "operation and there are many ways to do",
      "offset": 4287.78,
      "duration": 3.3
    },
    {
      "lang": "en",
      "text": "this by the way but I like to do",
      "offset": 4289.82,
      "duration": 4.02
    },
    {
      "lang": "en",
      "text": "something like this torch dot once like",
      "offset": 4291.08,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "of bndf2 so I'll create a large array",
      "offset": 4293.84,
      "duration": 5.819
    },
    {
      "lang": "en",
      "text": "two-dimensional of ones",
      "offset": 4297.32,
      "duration": 4.859
    },
    {
      "lang": "en",
      "text": "and then I will scale it so 1.0 divided",
      "offset": 4299.659,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "by n minus 1.",
      "offset": 4302.179,
      "duration": 4.741
    },
    {
      "lang": "en",
      "text": "so this is a array of",
      "offset": 4304.699,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "um one over n minus one and that's sort",
      "offset": 4306.92,
      "duration": 3.779
    },
    {
      "lang": "en",
      "text": "of like the local derivative",
      "offset": 4309.26,
      "duration": 4.14
    },
    {
      "lang": "en",
      "text": "and now for the chain rule I will simply",
      "offset": 4310.699,
      "duration": 7.341
    },
    {
      "lang": "en",
      "text": "just multiply it by dbm bar",
      "offset": 4313.4,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "and notice here what's going to happen",
      "offset": 4318.38,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "this is 32 by 64 and this is just 1 by",
      "offset": 4320,
      "duration": 6.12
    },
    {
      "lang": "en",
      "text": "64. so I'm letting the broadcasting do",
      "offset": 4322.94,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "the replication because internally in",
      "offset": 4326.12,
      "duration": 5.4
    },
    {
      "lang": "en",
      "text": "pytorch basically dbnbar which is 1 by",
      "offset": 4328.46,
      "duration": 4.62
    },
    {
      "lang": "en",
      "text": "64 row vector",
      "offset": 4331.52,
      "duration": 3.9
    },
    {
      "lang": "en",
      "text": "well in this multiplication get",
      "offset": 4333.08,
      "duration": 5.099
    },
    {
      "lang": "en",
      "text": "um copied vertically until the two are",
      "offset": 4335.42,
      "duration": 4.14
    },
    {
      "lang": "en",
      "text": "of the same shape and then there will be",
      "offset": 4338.179,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "an element wise multiply and so that uh",
      "offset": 4339.56,
      "duration": 4.139
    },
    {
      "lang": "en",
      "text": "so that the broadcasting is basically",
      "offset": 4342.02,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "doing the replication",
      "offset": 4343.699,
      "duration": 3.421
    },
    {
      "lang": "en",
      "text": "and I will end up with the derivatives",
      "offset": 4345.14,
      "duration": 4.92
    },
    {
      "lang": "en",
      "text": "of DB and diff2 here",
      "offset": 4347.12,
      "duration": 5.34
    },
    {
      "lang": "en",
      "text": "so this is the candidate solution let's",
      "offset": 4350.06,
      "duration": 3.9
    },
    {
      "lang": "en",
      "text": "bring it down here",
      "offset": 4352.46,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "let's uncomment this line where we check",
      "offset": 4353.96,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "it and let's hope for the best",
      "offset": 4356.78,
      "duration": 4.62
    },
    {
      "lang": "en",
      "text": "and indeed we see that this is the",
      "offset": 4359.48,
      "duration": 3.9
    },
    {
      "lang": "en",
      "text": "correct formula next up let's",
      "offset": 4361.4,
      "duration": 4.2
    },
    {
      "lang": "en",
      "text": "differentiate here and to be in this",
      "offset": 4363.38,
      "duration": 4.62
    },
    {
      "lang": "en",
      "text": "so here we have that b and diff is",
      "offset": 4365.6,
      "duration": 5.34
    },
    {
      "lang": "en",
      "text": "element y squared to create B and F2",
      "offset": 4368,
      "duration": 4.739
    },
    {
      "lang": "en",
      "text": "so this is a relatively simple",
      "offset": 4370.94,
      "duration": 3.66
    },
    {
      "lang": "en",
      "text": "derivative because it's a simple element",
      "offset": 4372.739,
      "duration": 3.541
    },
    {
      "lang": "en",
      "text": "wise operation so it's kind of like the",
      "offset": 4374.6,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "scalar case and we have that DB and div",
      "offset": 4376.28,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "should be if this is x squared then the",
      "offset": 4379.4,
      "duration": 5.279
    },
    {
      "lang": "en",
      "text": "derivative of this is 2x right so it's",
      "offset": 4382.28,
      "duration": 5.34
    },
    {
      "lang": "en",
      "text": "simply 2 times B and if that's the local",
      "offset": 4384.679,
      "duration": 4.201
    },
    {
      "lang": "en",
      "text": "derivative",
      "offset": 4387.62,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "and then times chain Rule and the shape",
      "offset": 4388.88,
      "duration": 4.2
    },
    {
      "lang": "en",
      "text": "of these is the same they are of the",
      "offset": 4391.219,
      "duration": 4.621
    },
    {
      "lang": "en",
      "text": "same shape so times this",
      "offset": 4393.08,
      "duration": 4.619
    },
    {
      "lang": "en",
      "text": "so that's the backward pass for this",
      "offset": 4395.84,
      "duration": 4.74
    },
    {
      "lang": "en",
      "text": "variable let me bring that down here",
      "offset": 4397.699,
      "duration": 4.381
    },
    {
      "lang": "en",
      "text": "and now we have to be careful because we",
      "offset": 4400.58,
      "duration": 3.9
    },
    {
      "lang": "en",
      "text": "already calculated dbm depth right so",
      "offset": 4402.08,
      "duration": 5.82
    },
    {
      "lang": "en",
      "text": "this is just the end of the other uh you",
      "offset": 4404.48,
      "duration": 5.64
    },
    {
      "lang": "en",
      "text": "know other Branch coming back to B and",
      "offset": 4407.9,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "diff",
      "offset": 4410.12,
      "duration": 2.579
    },
    {
      "lang": "en",
      "text": "because B and diff was already back",
      "offset": 4410.9,
      "duration": 3.96
    },
    {
      "lang": "en",
      "text": "propagated to way over here",
      "offset": 4412.699,
      "duration": 5.161
    },
    {
      "lang": "en",
      "text": "from being raw so we now completed the",
      "offset": 4414.86,
      "duration": 4.859
    },
    {
      "lang": "en",
      "text": "second branch and so that's why I have",
      "offset": 4417.86,
      "duration": 4.68
    },
    {
      "lang": "en",
      "text": "to do plus equals and if you recall we",
      "offset": 4419.719,
      "duration": 4.261
    },
    {
      "lang": "en",
      "text": "had an incorrect derivative for being",
      "offset": 4422.54,
      "duration": 3.72
    },
    {
      "lang": "en",
      "text": "diff before and I'm hoping that once we",
      "offset": 4423.98,
      "duration": 4.92
    },
    {
      "lang": "en",
      "text": "append this last missing piece we have",
      "offset": 4426.26,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "the exact correctness so let's run",
      "offset": 4428.9,
      "duration": 6.299
    },
    {
      "lang": "en",
      "text": "ambient to be in div now actually shows",
      "offset": 4431.62,
      "duration": 6.22
    },
    {
      "lang": "en",
      "text": "the exact correct derivative",
      "offset": 4435.199,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "um so that's comforting okay so let's",
      "offset": 4437.84,
      "duration": 3.54
    },
    {
      "lang": "en",
      "text": "now back propagate through this line",
      "offset": 4440,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "here",
      "offset": 4441.38,
      "duration": 3.54
    },
    {
      "lang": "en",
      "text": "um the first thing we do of course is we",
      "offset": 4443.12,
      "duration": 3.9
    },
    {
      "lang": "en",
      "text": "check the shapes and I wrote them out",
      "offset": 4444.92,
      "duration": 3.9
    },
    {
      "lang": "en",
      "text": "here and basically the shape of this is",
      "offset": 4447.02,
      "duration": 5.46
    },
    {
      "lang": "en",
      "text": "32 by 64. hpbn is the same shape",
      "offset": 4448.82,
      "duration": 6.6
    },
    {
      "lang": "en",
      "text": "but B and mean I is a row Vector 1 by",
      "offset": 4452.48,
      "duration": 5.34
    },
    {
      "lang": "en",
      "text": "64. so this minus here will actually do",
      "offset": 4455.42,
      "duration": 4.2
    },
    {
      "lang": "en",
      "text": "broadcasting and so we have to be",
      "offset": 4457.82,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "careful with that and as a hint to us",
      "offset": 4459.62,
      "duration": 4.14
    },
    {
      "lang": "en",
      "text": "again because of The Duality a",
      "offset": 4461.9,
      "duration": 4.02
    },
    {
      "lang": "en",
      "text": "broadcasting and the forward pass means",
      "offset": 4463.76,
      "duration": 3.899
    },
    {
      "lang": "en",
      "text": "a variable reuse and therefore there",
      "offset": 4465.92,
      "duration": 4.2
    },
    {
      "lang": "en",
      "text": "will be a sum in the backward pass",
      "offset": 4467.659,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "so let's write out the backward pass",
      "offset": 4470.12,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "here now",
      "offset": 4471.5,
      "duration": 2.82
    },
    {
      "lang": "en",
      "text": "um",
      "offset": 4473.12,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "back propagate into the hpbn",
      "offset": 4474.32,
      "duration": 4.74
    },
    {
      "lang": "en",
      "text": "because this is these are the same shape",
      "offset": 4477.199,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "then the local derivative for each one",
      "offset": 4479.06,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "of the elements here is just one for the",
      "offset": 4481.04,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "corresponding element in here",
      "offset": 4483.14,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "so basically what this means is that the",
      "offset": 4485.12,
      "duration": 5.579
    },
    {
      "lang": "en",
      "text": "gradient just simply copies it's just a",
      "offset": 4487.699,
      "duration": 5.101
    },
    {
      "lang": "en",
      "text": "variable assignment it's quality so I'm",
      "offset": 4490.699,
      "duration": 4.201
    },
    {
      "lang": "en",
      "text": "just going to clone this tensor just for",
      "offset": 4492.8,
      "duration": 5.939
    },
    {
      "lang": "en",
      "text": "safety to create an exact copy of DB and",
      "offset": 4494.9,
      "duration": 5.339
    },
    {
      "lang": "en",
      "text": "div",
      "offset": 4498.739,
      "duration": 3.061
    },
    {
      "lang": "en",
      "text": "and then here to back propagate into",
      "offset": 4500.239,
      "duration": 6.261
    },
    {
      "lang": "en",
      "text": "this one what I'm inclined to do here is",
      "offset": 4501.8,
      "duration": 4.7
    },
    {
      "lang": "en",
      "text": "will basically be",
      "offset": 4507.219,
      "duration": 5.081
    },
    {
      "lang": "en",
      "text": "uh what is the local derivative well",
      "offset": 4509.84,
      "duration": 6.54
    },
    {
      "lang": "en",
      "text": "it's negative torch.1's like",
      "offset": 4512.3,
      "duration": 7.02
    },
    {
      "lang": "en",
      "text": "of the shape of uh B and diff",
      "offset": 4516.38,
      "duration": 5.359
    },
    {
      "lang": "en",
      "text": "right",
      "offset": 4519.32,
      "duration": 2.419
    },
    {
      "lang": "en",
      "text": "and then times",
      "offset": 4522.199,
      "duration": 5.161
    },
    {
      "lang": "en",
      "text": "the um",
      "offset": 4524.9,
      "duration": 6.36
    },
    {
      "lang": "en",
      "text": "the derivative here dbf",
      "offset": 4527.36,
      "duration": 3.9
    },
    {
      "lang": "en",
      "text": "and this here is the back propagation",
      "offset": 4532.88,
      "duration": 4.92
    },
    {
      "lang": "en",
      "text": "for the replicated B and mean I",
      "offset": 4534.739,
      "duration": 4.92
    },
    {
      "lang": "en",
      "text": "so I still have to back propagate",
      "offset": 4537.8,
      "duration": 4.26
    },
    {
      "lang": "en",
      "text": "through the uh replication in the",
      "offset": 4539.659,
      "duration": 4.141
    },
    {
      "lang": "en",
      "text": "broadcasting and I do that by doing a",
      "offset": 4542.06,
      "duration": 3.54
    },
    {
      "lang": "en",
      "text": "sum so I'm going to take this whole",
      "offset": 4543.8,
      "duration": 4.14
    },
    {
      "lang": "en",
      "text": "thing and I'm going to do a sum over the",
      "offset": 4545.6,
      "duration": 4.139
    },
    {
      "lang": "en",
      "text": "zeroth dimension which was the",
      "offset": 4547.94,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "replication",
      "offset": 4549.739,
      "duration": 2.601
    },
    {
      "lang": "en",
      "text": "so if you scrutinize this by the way",
      "offset": 4553.52,
      "duration": 4.139
    },
    {
      "lang": "en",
      "text": "you'll notice that this is the same",
      "offset": 4555.92,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "shape as that and so what I'm doing uh",
      "offset": 4557.659,
      "duration": 3.661
    },
    {
      "lang": "en",
      "text": "what I'm doing here doesn't actually",
      "offset": 4560,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "make that much sense because it's just a",
      "offset": 4561.32,
      "duration": 5.16
    },
    {
      "lang": "en",
      "text": "array of ones multiplying DP and diff so",
      "offset": 4563.44,
      "duration": 6.759
    },
    {
      "lang": "en",
      "text": "in fact I can just do this",
      "offset": 4566.48,
      "duration": 5.699
    },
    {
      "lang": "en",
      "text": "um and that is equivalent",
      "offset": 4570.199,
      "duration": 4.98
    },
    {
      "lang": "en",
      "text": "so this is the candidate backward pass",
      "offset": 4572.179,
      "duration": 5.821
    },
    {
      "lang": "en",
      "text": "let me copy it here and then let me",
      "offset": 4575.179,
      "duration": 7.321
    },
    {
      "lang": "en",
      "text": "comment out this one and this one",
      "offset": 4578,
      "duration": 6.06
    },
    {
      "lang": "en",
      "text": "enter",
      "offset": 4582.5,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "and it's wrong",
      "offset": 4584.06,
      "duration": 3.08
    },
    {
      "lang": "en",
      "text": "damn",
      "offset": 4587.239,
      "duration": 3.96
    },
    {
      "lang": "en",
      "text": "actually sorry this is supposed to be",
      "offset": 4589.46,
      "duration": 3.9
    },
    {
      "lang": "en",
      "text": "wrong and it's supposed to be wrong",
      "offset": 4591.199,
      "duration": 3.421
    },
    {
      "lang": "en",
      "text": "because",
      "offset": 4593.36,
      "duration": 3.299
    },
    {
      "lang": "en",
      "text": "we are back propagating from a b and",
      "offset": 4594.62,
      "duration": 5.22
    },
    {
      "lang": "en",
      "text": "diff into hpbn and but we're not done",
      "offset": 4596.659,
      "duration": 6.721
    },
    {
      "lang": "en",
      "text": "because B and mean I depends on hpbn and",
      "offset": 4599.84,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "there will be a second portion of that",
      "offset": 4603.38,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "derivative coming from this second",
      "offset": 4604.88,
      "duration": 3.54
    },
    {
      "lang": "en",
      "text": "Branch so we're not done yet and we",
      "offset": 4606.26,
      "duration": 3.78
    },
    {
      "lang": "en",
      "text": "expect it to be incorrect so there you",
      "offset": 4608.42,
      "duration": 2.34
    },
    {
      "lang": "en",
      "text": "go",
      "offset": 4610.04,
      "duration": 3.06
    },
    {
      "lang": "en",
      "text": "uh so let's now back propagate from uh B",
      "offset": 4610.76,
      "duration": 5.58
    },
    {
      "lang": "en",
      "text": "and mean I into hpbn",
      "offset": 4613.1,
      "duration": 4.2
    },
    {
      "lang": "en",
      "text": "um",
      "offset": 4616.34,
      "duration": 2.52
    },
    {
      "lang": "en",
      "text": "and so here again we have to be careful",
      "offset": 4617.3,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "because there's a broadcasting along",
      "offset": 4618.86,
      "duration": 4.379
    },
    {
      "lang": "en",
      "text": "um or there's a Sum along the zeroth",
      "offset": 4621.38,
      "duration": 3.18
    },
    {
      "lang": "en",
      "text": "dimension so this will turn into",
      "offset": 4623.239,
      "duration": 3.301
    },
    {
      "lang": "en",
      "text": "broadcasting in the backward pass now",
      "offset": 4624.56,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "and I'm going to go a little bit faster",
      "offset": 4626.54,
      "duration": 3.78
    },
    {
      "lang": "en",
      "text": "on this line because it is very similar",
      "offset": 4628.64,
      "duration": 3.78
    },
    {
      "lang": "en",
      "text": "to the line that we had before and",
      "offset": 4630.32,
      "duration": 4.5
    },
    {
      "lang": "en",
      "text": "multiplies in the past in fact",
      "offset": 4632.42,
      "duration": 6.12
    },
    {
      "lang": "en",
      "text": "so the hpbn",
      "offset": 4634.82,
      "duration": 5.24
    },
    {
      "lang": "en",
      "text": "will be",
      "offset": 4638.54,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "the gradient will be scaled by 1 over n",
      "offset": 4640.06,
      "duration": 5.38
    },
    {
      "lang": "en",
      "text": "and then basically this gradient here on",
      "offset": 4642.62,
      "duration": 4.68
    },
    {
      "lang": "en",
      "text": "dbn mean I",
      "offset": 4645.44,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "is going to be scaled by 1 over n and",
      "offset": 4647.3,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "then it's going to flow across all the",
      "offset": 4650.239,
      "duration": 5.46
    },
    {
      "lang": "en",
      "text": "columns and deposit itself into the hpvn",
      "offset": 4652.1,
      "duration": 6.36
    },
    {
      "lang": "en",
      "text": "so what we want is this thing scaled by",
      "offset": 4655.699,
      "duration": 3.901
    },
    {
      "lang": "en",
      "text": "1 over n",
      "offset": 4658.46,
      "duration": 4.759
    },
    {
      "lang": "en",
      "text": "only put the constant up front here",
      "offset": 4659.6,
      "duration": 3.619
    },
    {
      "lang": "en",
      "text": "um",
      "offset": 4663.98,
      "duration": 3.54
    },
    {
      "lang": "en",
      "text": "so scale down the gradient and now we",
      "offset": 4665.3,
      "duration": 6.18
    },
    {
      "lang": "en",
      "text": "need to replicate it across all the um",
      "offset": 4667.52,
      "duration": 7.62
    },
    {
      "lang": "en",
      "text": "across all the rows here so we I like to",
      "offset": 4671.48,
      "duration": 9.36
    },
    {
      "lang": "en",
      "text": "do that by torch.lunslike of basically",
      "offset": 4675.14,
      "duration": 8.7
    },
    {
      "lang": "en",
      "text": "um hpbn",
      "offset": 4680.84,
      "duration": 4.98
    },
    {
      "lang": "en",
      "text": "and I will let the broadcasting do the",
      "offset": 4683.84,
      "duration": 5.46
    },
    {
      "lang": "en",
      "text": "work of replication",
      "offset": 4685.82,
      "duration": 6.02
    },
    {
      "lang": "en",
      "text": "so",
      "offset": 4689.3,
      "duration": 2.54
    },
    {
      "lang": "en",
      "text": "like that",
      "offset": 4694.94,
      "duration": 6.36
    },
    {
      "lang": "en",
      "text": "so this is uh the hppn and hopefully",
      "offset": 4696.32,
      "duration": 8.419
    },
    {
      "lang": "en",
      "text": "we can plus equals that",
      "offset": 4701.3,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "so this here is broadcasting",
      "offset": 4707.12,
      "duration": 5.16
    },
    {
      "lang": "en",
      "text": "um and then this is the scaling so this",
      "offset": 4710.12,
      "duration": 3.42
    },
    {
      "lang": "en",
      "text": "should be current",
      "offset": 4712.28,
      "duration": 2.78
    },
    {
      "lang": "en",
      "text": "okay",
      "offset": 4713.54,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "so that completes the back propagation",
      "offset": 4715.06,
      "duration": 3.88
    },
    {
      "lang": "en",
      "text": "of the bathroom layer and we are now",
      "offset": 4717.14,
      "duration": 3.42
    },
    {
      "lang": "en",
      "text": "here let's back propagate through the",
      "offset": 4718.94,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "linear layer one here now because",
      "offset": 4720.56,
      "duration": 3.78
    },
    {
      "lang": "en",
      "text": "everything is getting a little",
      "offset": 4723.26,
      "duration": 3.72
    },
    {
      "lang": "en",
      "text": "vertically crazy I copy pasted the line",
      "offset": 4724.34,
      "duration": 4.02
    },
    {
      "lang": "en",
      "text": "here and let's just back properly",
      "offset": 4726.98,
      "duration": 3.179
    },
    {
      "lang": "en",
      "text": "through this one line",
      "offset": 4728.36,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "so first of course we inspect the shapes",
      "offset": 4730.159,
      "duration": 6.121
    },
    {
      "lang": "en",
      "text": "and we see that this is 32 by 64. MCAT",
      "offset": 4732.199,
      "duration": 6.661
    },
    {
      "lang": "en",
      "text": "is 32 by 30.",
      "offset": 4736.28,
      "duration": 8.34
    },
    {
      "lang": "en",
      "text": "W1 is 30 30 by 64 and B1 is just 64. so",
      "offset": 4738.86,
      "duration": 7.799
    },
    {
      "lang": "en",
      "text": "as I mentioned back propagating through",
      "offset": 4744.62,
      "duration": 3.78
    },
    {
      "lang": "en",
      "text": "linear layers is fairly easy just by",
      "offset": 4746.659,
      "duration": 4.441
    },
    {
      "lang": "en",
      "text": "matching the shapes so let's do that we",
      "offset": 4748.4,
      "duration": 5.7
    },
    {
      "lang": "en",
      "text": "have that dmcat",
      "offset": 4751.1,
      "duration": 4.5
    },
    {
      "lang": "en",
      "text": "should be",
      "offset": 4754.1,
      "duration": 4.38
    },
    {
      "lang": "en",
      "text": "um some matrix multiplication of dhbn",
      "offset": 4755.6,
      "duration": 6.059
    },
    {
      "lang": "en",
      "text": "with uh W1 and one transpose thrown in",
      "offset": 4758.48,
      "duration": 9.719
    },
    {
      "lang": "en",
      "text": "there so to make uh MCAT be 32 by 30",
      "offset": 4761.659,
      "duration": 10.5
    },
    {
      "lang": "en",
      "text": "I need to take dhpn",
      "offset": 4768.199,
      "duration": 8.101
    },
    {
      "lang": "en",
      "text": "32 by 64 and multiply it by w1.",
      "offset": 4772.159,
      "duration": 6.621
    },
    {
      "lang": "en",
      "text": "transpose",
      "offset": 4776.3,
      "duration": 2.48
    },
    {
      "lang": "en",
      "text": "to get the only one I need to end up",
      "offset": 4779.9,
      "duration": 5.7
    },
    {
      "lang": "en",
      "text": "with 30 by 64.",
      "offset": 4783.38,
      "duration": 5.4
    },
    {
      "lang": "en",
      "text": "so to get that I need to take uh MCAT",
      "offset": 4785.6,
      "duration": 5.7
    },
    {
      "lang": "en",
      "text": "transpose",
      "offset": 4788.78,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "and multiply that by",
      "offset": 4791.3,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "uh dhpion",
      "offset": 4793.34,
      "duration": 3.96
    },
    {
      "lang": "en",
      "text": "and finally to get DB1",
      "offset": 4798.38,
      "duration": 6.42
    },
    {
      "lang": "en",
      "text": "this is a addition and we saw that",
      "offset": 4801.5,
      "duration": 5.46
    },
    {
      "lang": "en",
      "text": "basically I need to just sum the",
      "offset": 4804.8,
      "duration": 4.76
    },
    {
      "lang": "en",
      "text": "elements in dhpbn along some Dimension",
      "offset": 4806.96,
      "duration": 5.1
    },
    {
      "lang": "en",
      "text": "and to make the dimensions work out I",
      "offset": 4809.56,
      "duration": 4.659
    },
    {
      "lang": "en",
      "text": "need to Sum along the zeroth axis here",
      "offset": 4812.06,
      "duration": 5.46
    },
    {
      "lang": "en",
      "text": "to eliminate this Dimension and we do",
      "offset": 4814.219,
      "duration": 4.861
    },
    {
      "lang": "en",
      "text": "not keep dims",
      "offset": 4817.52,
      "duration": 3.54
    },
    {
      "lang": "en",
      "text": "uh so that we want to just get a single",
      "offset": 4819.08,
      "duration": 4.139
    },
    {
      "lang": "en",
      "text": "one-dimensional lecture of 64.",
      "offset": 4821.06,
      "duration": 6.3
    },
    {
      "lang": "en",
      "text": "so these are the claimed derivatives",
      "offset": 4823.219,
      "duration": 6.321
    },
    {
      "lang": "en",
      "text": "let me put that here and let me",
      "offset": 4827.36,
      "duration": 5.16
    },
    {
      "lang": "en",
      "text": "uncomment three lines and cross our",
      "offset": 4829.54,
      "duration": 4.54
    },
    {
      "lang": "en",
      "text": "fingers",
      "offset": 4832.52,
      "duration": 3.54
    },
    {
      "lang": "en",
      "text": "everything is great okay so we now",
      "offset": 4834.08,
      "duration": 3.9
    },
    {
      "lang": "en",
      "text": "continue almost there we have the",
      "offset": 4836.06,
      "duration": 3.72
    },
    {
      "lang": "en",
      "text": "derivative of MCAT and we want to",
      "offset": 4837.98,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "derivative we want to back propagate",
      "offset": 4839.78,
      "duration": 3.419
    },
    {
      "lang": "en",
      "text": "into m",
      "offset": 4841.58,
      "duration": 4.68
    },
    {
      "lang": "en",
      "text": "so I again copied this line over here",
      "offset": 4843.199,
      "duration": 5.46
    },
    {
      "lang": "en",
      "text": "so this is the forward pass and then",
      "offset": 4846.26,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "this is the shapes so remember that the",
      "offset": 4848.659,
      "duration": 4.741
    },
    {
      "lang": "en",
      "text": "shape here was 32 by 30 and the original",
      "offset": 4851.3,
      "duration": 5.82
    },
    {
      "lang": "en",
      "text": "shape of M plus 32 by 3 by 10. so this",
      "offset": 4853.4,
      "duration": 5.22
    },
    {
      "lang": "en",
      "text": "layer in the forward pass as you recall",
      "offset": 4857.12,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "did the concatenation of these three",
      "offset": 4858.62,
      "duration": 5.579
    },
    {
      "lang": "en",
      "text": "10-dimensional character vectors",
      "offset": 4861.28,
      "duration": 5.439
    },
    {
      "lang": "en",
      "text": "and so now we just want to undo that",
      "offset": 4864.199,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "so this is actually relatively",
      "offset": 4866.719,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "straightforward operation because uh the",
      "offset": 4868.52,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "backward pass of the what is the view",
      "offset": 4871.04,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "view is just a representation of the",
      "offset": 4872.6,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "array it's just a logical form of how",
      "offset": 4875.12,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "you interpret the array so let's just",
      "offset": 4877.4,
      "duration": 4.5
    },
    {
      "lang": "en",
      "text": "reinterpret it to be what it was before",
      "offset": 4878.96,
      "duration": 6.66
    },
    {
      "lang": "en",
      "text": "so in other words the end is not uh 32",
      "offset": 4881.9,
      "duration": 7.56
    },
    {
      "lang": "en",
      "text": "by 30. it is basically dmcat",
      "offset": 4885.62,
      "duration": 8.579
    },
    {
      "lang": "en",
      "text": "but if you view it as the original shape",
      "offset": 4889.46,
      "duration": 7.739
    },
    {
      "lang": "en",
      "text": "so just m dot shape",
      "offset": 4894.199,
      "duration": 5.341
    },
    {
      "lang": "en",
      "text": "uh you can you can pass in tuples into",
      "offset": 4897.199,
      "duration": 3.301
    },
    {
      "lang": "en",
      "text": "view",
      "offset": 4899.54,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "and so this should just be okay",
      "offset": 4900.5,
      "duration": 4.04
    },
    {
      "lang": "en",
      "text": "we just re-represent that view and then",
      "offset": 4904.64,
      "duration": 5.34
    },
    {
      "lang": "en",
      "text": "we uncomment this line here and",
      "offset": 4907.1,
      "duration": 3.96
    },
    {
      "lang": "en",
      "text": "hopefully",
      "offset": 4909.98,
      "duration": 5.16
    },
    {
      "lang": "en",
      "text": "yeah so the derivative of M is correct",
      "offset": 4911.06,
      "duration": 5.099
    },
    {
      "lang": "en",
      "text": "so in this case we just have to",
      "offset": 4915.14,
      "duration": 2.22
    },
    {
      "lang": "en",
      "text": "re-represent the shape of those",
      "offset": 4916.159,
      "duration": 3.721
    },
    {
      "lang": "en",
      "text": "derivatives into the original View",
      "offset": 4917.36,
      "duration": 4.379
    },
    {
      "lang": "en",
      "text": "so now we are at the final line and the",
      "offset": 4919.88,
      "duration": 3.06
    },
    {
      "lang": "en",
      "text": "only thing that's left to back propagate",
      "offset": 4921.739,
      "duration": 3.801
    },
    {
      "lang": "en",
      "text": "through is this indexing operation here",
      "offset": 4922.94,
      "duration": 6.54
    },
    {
      "lang": "en",
      "text": "MSC at xB so as I did before I copy",
      "offset": 4925.54,
      "duration": 6.04
    },
    {
      "lang": "en",
      "text": "pasted this line here and let's look at",
      "offset": 4929.48,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "the shapes of everything that's involved",
      "offset": 4931.58,
      "duration": 3.659
    },
    {
      "lang": "en",
      "text": "and remind ourselves how this worked",
      "offset": 4932.84,
      "duration": 6.78
    },
    {
      "lang": "en",
      "text": "so m.shape was 32 by 3 by 10.",
      "offset": 4935.239,
      "duration": 7.021
    },
    {
      "lang": "en",
      "text": "it says 32 examples and then we have",
      "offset": 4939.62,
      "duration": 4.86
    },
    {
      "lang": "en",
      "text": "three characters each one of them has a",
      "offset": 4942.26,
      "duration": 4.38
    },
    {
      "lang": "en",
      "text": "10 dimensional embedding",
      "offset": 4944.48,
      "duration": 4.44
    },
    {
      "lang": "en",
      "text": "and this was achieved by taking the",
      "offset": 4946.64,
      "duration": 4.74
    },
    {
      "lang": "en",
      "text": "lookup table C which have 27 possible",
      "offset": 4948.92,
      "duration": 3.72
    },
    {
      "lang": "en",
      "text": "characters",
      "offset": 4951.38,
      "duration": 3.48
    },
    {
      "lang": "en",
      "text": "each of them 10 dimensional and we",
      "offset": 4952.64,
      "duration": 3.26
    },
    {
      "lang": "en",
      "text": "looked up",
      "offset": 4954.86,
      "duration": 4.14
    },
    {
      "lang": "en",
      "text": "at the rows that were specified inside",
      "offset": 4955.9,
      "duration": 5.14
    },
    {
      "lang": "en",
      "text": "this tensor xB",
      "offset": 4959,
      "duration": 4.739
    },
    {
      "lang": "en",
      "text": "so XB is 32 by 3 and it's basically",
      "offset": 4961.04,
      "duration": 4.619
    },
    {
      "lang": "en",
      "text": "giving us for each example the Identity",
      "offset": 4963.739,
      "duration": 5.46
    },
    {
      "lang": "en",
      "text": "or the index of which character is part",
      "offset": 4965.659,
      "duration": 5.161
    },
    {
      "lang": "en",
      "text": "of that example",
      "offset": 4969.199,
      "duration": 3.721
    },
    {
      "lang": "en",
      "text": "and so here I'm showing the first five",
      "offset": 4970.82,
      "duration": 6.3
    },
    {
      "lang": "en",
      "text": "rows of three of this tensor xB",
      "offset": 4972.92,
      "duration": 5.88
    },
    {
      "lang": "en",
      "text": "and so we can see that for example here",
      "offset": 4977.12,
      "duration": 3.539
    },
    {
      "lang": "en",
      "text": "it was the first example in this batch",
      "offset": 4978.8,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "is that the first character and the",
      "offset": 4980.659,
      "duration": 3.54
    },
    {
      "lang": "en",
      "text": "first character and the fourth character",
      "offset": 4982.88,
      "duration": 3.42
    },
    {
      "lang": "en",
      "text": "comes into the neural net",
      "offset": 4984.199,
      "duration": 4.201
    },
    {
      "lang": "en",
      "text": "and then we want to predict the next",
      "offset": 4986.3,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "character in a sequence after the",
      "offset": 4988.4,
      "duration": 3.9
    },
    {
      "lang": "en",
      "text": "character is one one four",
      "offset": 4990.14,
      "duration": 4.44
    },
    {
      "lang": "en",
      "text": "so basically What's Happening Here is",
      "offset": 4992.3,
      "duration": 5.7
    },
    {
      "lang": "en",
      "text": "there are integers inside XB and each",
      "offset": 4994.58,
      "duration": 4.98
    },
    {
      "lang": "en",
      "text": "one of these integers is specifying",
      "offset": 4998,
      "duration": 4.5
    },
    {
      "lang": "en",
      "text": "which row of C we want to pluck out",
      "offset": 4999.56,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "right and then we arrange those rows",
      "offset": 5002.5,
      "duration": 6.179
    },
    {
      "lang": "en",
      "text": "that we've plucked out into 32 by 3 by",
      "offset": 5005.56,
      "duration": 5.34
    },
    {
      "lang": "en",
      "text": "10 tensor and we just package them in we",
      "offset": 5008.679,
      "duration": 4.98
    },
    {
      "lang": "en",
      "text": "just package them into the sensor",
      "offset": 5010.9,
      "duration": 4.2
    },
    {
      "lang": "en",
      "text": "and now what's happening is that we have",
      "offset": 5013.659,
      "duration": 2.641
    },
    {
      "lang": "en",
      "text": "D amp",
      "offset": 5015.1,
      "duration": 3.9
    },
    {
      "lang": "en",
      "text": "so for every one of these uh basically",
      "offset": 5016.3,
      "duration": 5.34
    },
    {
      "lang": "en",
      "text": "plucked out rows we have their gradients",
      "offset": 5019,
      "duration": 3.96
    },
    {
      "lang": "en",
      "text": "now",
      "offset": 5021.64,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "but they're arranged inside this 32 by 3",
      "offset": 5022.96,
      "duration": 5.1
    },
    {
      "lang": "en",
      "text": "by 10 tensor so all we have to do now is",
      "offset": 5025.239,
      "duration": 4.521
    },
    {
      "lang": "en",
      "text": "we just need to Route this gradient",
      "offset": 5028.06,
      "duration": 4.2
    },
    {
      "lang": "en",
      "text": "backwards through this assignment so we",
      "offset": 5029.76,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "need to find which row of C that every",
      "offset": 5032.26,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "one of these",
      "offset": 5034.96,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "um 10 dimensional embeddings come from",
      "offset": 5036.1,
      "duration": 7.02
    },
    {
      "lang": "en",
      "text": "and then we need to deposit them into DC",
      "offset": 5039.04,
      "duration": 7.02
    },
    {
      "lang": "en",
      "text": "so we just need to undo the indexing and",
      "offset": 5043.12,
      "duration": 5.34
    },
    {
      "lang": "en",
      "text": "of course if any of these rows of C was",
      "offset": 5046.06,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "used multiple times which almost",
      "offset": 5048.46,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "certainly is the case like the row one",
      "offset": 5050.14,
      "duration": 3.66
    },
    {
      "lang": "en",
      "text": "and one was used multiple times then we",
      "offset": 5051.82,
      "duration": 3.78
    },
    {
      "lang": "en",
      "text": "have to remember that the gradients that",
      "offset": 5053.8,
      "duration": 4.26
    },
    {
      "lang": "en",
      "text": "arrive there have to add",
      "offset": 5055.6,
      "duration": 4.139
    },
    {
      "lang": "en",
      "text": "so for each occurrence we have to have",
      "offset": 5058.06,
      "duration": 3.3
    },
    {
      "lang": "en",
      "text": "an addition",
      "offset": 5059.739,
      "duration": 3.781
    },
    {
      "lang": "en",
      "text": "so let's now write this out and I don't",
      "offset": 5061.36,
      "duration": 3.48
    },
    {
      "lang": "en",
      "text": "actually know if like a much better way",
      "offset": 5063.52,
      "duration": 2.94
    },
    {
      "lang": "en",
      "text": "to do this than a for Loop unfortunately",
      "offset": 5064.84,
      "duration": 3.18
    },
    {
      "lang": "en",
      "text": "in Python",
      "offset": 5066.46,
      "duration": 3.3
    },
    {
      "lang": "en",
      "text": "um so maybe someone can come up with a",
      "offset": 5068.02,
      "duration": 4.199
    },
    {
      "lang": "en",
      "text": "vectorized efficient operation but for",
      "offset": 5069.76,
      "duration": 4.919
    },
    {
      "lang": "en",
      "text": "now let's just use for loops so let me",
      "offset": 5072.219,
      "duration": 4.921
    },
    {
      "lang": "en",
      "text": "create a torch.zeros like",
      "offset": 5074.679,
      "duration": 6.301
    },
    {
      "lang": "en",
      "text": "C to initialize uh just uh 27 by 10",
      "offset": 5077.14,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "tensor of all zeros",
      "offset": 5080.98,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "and then honestly 4K in range XB dot",
      "offset": 5083.14,
      "duration": 6.3
    },
    {
      "lang": "en",
      "text": "shape at zero",
      "offset": 5086.98,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "maybe someone has a better way to do",
      "offset": 5089.44,
      "duration": 3.779
    },
    {
      "lang": "en",
      "text": "this but for J and range",
      "offset": 5091.06,
      "duration": 4.679
    },
    {
      "lang": "en",
      "text": "be that shape at one",
      "offset": 5093.219,
      "duration": 5.101
    },
    {
      "lang": "en",
      "text": "this is going to iterate over all the",
      "offset": 5095.739,
      "duration": 6.061
    },
    {
      "lang": "en",
      "text": "um all the elements of XB all these",
      "offset": 5098.32,
      "duration": 4.859
    },
    {
      "lang": "en",
      "text": "integers",
      "offset": 5101.8,
      "duration": 3.54
    },
    {
      "lang": "en",
      "text": "and then let's get the index at this",
      "offset": 5103.179,
      "duration": 3.661
    },
    {
      "lang": "en",
      "text": "position",
      "offset": 5105.34,
      "duration": 6.3
    },
    {
      "lang": "en",
      "text": "so the index is basically x b at KJ",
      "offset": 5106.84,
      "duration": 7.5
    },
    {
      "lang": "en",
      "text": "so that an example of that like is 11 or",
      "offset": 5111.64,
      "duration": 4.38
    },
    {
      "lang": "en",
      "text": "14 and so on",
      "offset": 5114.34,
      "duration": 5.1
    },
    {
      "lang": "en",
      "text": "and now in the forward pass we took",
      "offset": 5116.02,
      "duration": 7.699
    },
    {
      "lang": "en",
      "text": "and we basically took um",
      "offset": 5119.44,
      "duration": 4.279
    },
    {
      "lang": "en",
      "text": "the row of C at index and we deposited",
      "offset": 5124.239,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "it into M at K of J",
      "offset": 5127.96,
      "duration": 4.44
    },
    {
      "lang": "en",
      "text": "that's what happened that's where they",
      "offset": 5130.719,
      "duration": 3.421
    },
    {
      "lang": "en",
      "text": "are packaged so now we need to go",
      "offset": 5132.4,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "backwards and we just need to route",
      "offset": 5134.14,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "DM at the position KJ",
      "offset": 5136.56,
      "duration": 6.099
    },
    {
      "lang": "en",
      "text": "we now have these derivatives",
      "offset": 5139.9,
      "duration": 4.86
    },
    {
      "lang": "en",
      "text": "for each position and it's 10",
      "offset": 5142.659,
      "duration": 3.181
    },
    {
      "lang": "en",
      "text": "dimensional",
      "offset": 5144.76,
      "duration": 2.52
    },
    {
      "lang": "en",
      "text": "and you just need to go into the correct",
      "offset": 5145.84,
      "duration": 3.3
    },
    {
      "lang": "en",
      "text": "row of C",
      "offset": 5147.28,
      "duration": 7.2
    },
    {
      "lang": "en",
      "text": "so DC rather at IX is this but plus",
      "offset": 5149.14,
      "duration": 6.539
    },
    {
      "lang": "en",
      "text": "equals",
      "offset": 5154.48,
      "duration": 2.4
    },
    {
      "lang": "en",
      "text": "because there could be multiple",
      "offset": 5155.679,
      "duration": 3.301
    },
    {
      "lang": "en",
      "text": "occurrences uh like the same row could",
      "offset": 5156.88,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "have been used many many times and so",
      "offset": 5158.98,
      "duration": 5.58
    },
    {
      "lang": "en",
      "text": "all of those derivatives will just go",
      "offset": 5160.96,
      "duration": 5.219
    },
    {
      "lang": "en",
      "text": "backwards through the indexing and they",
      "offset": 5164.56,
      "duration": 3.42
    },
    {
      "lang": "en",
      "text": "will add",
      "offset": 5166.179,
      "duration": 6.361
    },
    {
      "lang": "en",
      "text": "so this is my candidate solution",
      "offset": 5167.98,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "let's copy it here",
      "offset": 5172.659,
      "duration": 3.5
    },
    {
      "lang": "en",
      "text": "let's uncomment this and cross our",
      "offset": 5176.38,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "fingers",
      "offset": 5179.38,
      "duration": 2.6
    },
    {
      "lang": "en",
      "text": "hey",
      "offset": 5180.94,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "so that's it we've back propagated",
      "offset": 5181.98,
      "duration": 3.699
    },
    {
      "lang": "en",
      "text": "through",
      "offset": 5184.3,
      "duration": 3.72
    },
    {
      "lang": "en",
      "text": "this entire Beast",
      "offset": 5185.679,
      "duration": 5.401
    },
    {
      "lang": "en",
      "text": "so there we go totally makes sense",
      "offset": 5188.02,
      "duration": 5.58
    },
    {
      "lang": "en",
      "text": "so now we come to exercise two it",
      "offset": 5191.08,
      "duration": 3.9
    },
    {
      "lang": "en",
      "text": "basically turns out that in this first",
      "offset": 5193.6,
      "duration": 3.18
    },
    {
      "lang": "en",
      "text": "exercise we were doing way too much work",
      "offset": 5194.98,
      "duration": 4.02
    },
    {
      "lang": "en",
      "text": "uh we were back propagating way too much",
      "offset": 5196.78,
      "duration": 3.959
    },
    {
      "lang": "en",
      "text": "and it was all good practice and so on",
      "offset": 5199,
      "duration": 3.239
    },
    {
      "lang": "en",
      "text": "but it's not what you would do in",
      "offset": 5200.739,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "practice and the reason for that is for",
      "offset": 5202.239,
      "duration": 4.92
    },
    {
      "lang": "en",
      "text": "example here I separated out this loss",
      "offset": 5204.34,
      "duration": 4.92
    },
    {
      "lang": "en",
      "text": "calculation over multiple lines and I",
      "offset": 5207.159,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "broke it up all all to like its smallest",
      "offset": 5209.26,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "atomic pieces and we back propagated",
      "offset": 5211.719,
      "duration": 3.48
    },
    {
      "lang": "en",
      "text": "through all of those individually",
      "offset": 5213.34,
      "duration": 3.42
    },
    {
      "lang": "en",
      "text": "but it turns out that if you just look",
      "offset": 5215.199,
      "duration": 3.181
    },
    {
      "lang": "en",
      "text": "at the mathematical expression for the",
      "offset": 5216.76,
      "duration": 3.24
    },
    {
      "lang": "en",
      "text": "loss",
      "offset": 5218.38,
      "duration": 3.779
    },
    {
      "lang": "en",
      "text": "um then actually you can do the",
      "offset": 5220,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "differentiation on pen and paper and a",
      "offset": 5222.159,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "lot of terms cancel and simplify and the",
      "offset": 5224.32,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "mathematical expression you end up with",
      "offset": 5226.48,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "can be significantly shorter and easier",
      "offset": 5227.679,
      "duration": 3.96
    },
    {
      "lang": "en",
      "text": "to implement than back propagating",
      "offset": 5230.08,
      "duration": 2.639
    },
    {
      "lang": "en",
      "text": "through all the little pieces of",
      "offset": 5231.639,
      "duration": 2.341
    },
    {
      "lang": "en",
      "text": "everything you've done",
      "offset": 5232.719,
      "duration": 3.42
    },
    {
      "lang": "en",
      "text": "so before we had this complicated",
      "offset": 5233.98,
      "duration": 4.44
    },
    {
      "lang": "en",
      "text": "forward paths going from logits to the",
      "offset": 5236.139,
      "duration": 3.121
    },
    {
      "lang": "en",
      "text": "loss",
      "offset": 5238.42,
      "duration": 2.7
    },
    {
      "lang": "en",
      "text": "but in pytorch everything can just be",
      "offset": 5239.26,
      "duration": 3.72
    },
    {
      "lang": "en",
      "text": "glued together into a single call at",
      "offset": 5241.12,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "that cross entropy you just pass in",
      "offset": 5242.98,
      "duration": 3.42
    },
    {
      "lang": "en",
      "text": "logits and the labels and you get the",
      "offset": 5244.719,
      "duration": 4.261
    },
    {
      "lang": "en",
      "text": "exact same loss as I verify here so our",
      "offset": 5246.4,
      "duration": 4.92
    },
    {
      "lang": "en",
      "text": "previous loss and the fast loss coming",
      "offset": 5248.98,
      "duration": 4.5
    },
    {
      "lang": "en",
      "text": "from the chunk of operations as a single",
      "offset": 5251.32,
      "duration": 5.399
    },
    {
      "lang": "en",
      "text": "mathematical expression is the same but",
      "offset": 5253.48,
      "duration": 5.1
    },
    {
      "lang": "en",
      "text": "it's much much faster in a forward pass",
      "offset": 5256.719,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "it's also much much faster in backward",
      "offset": 5258.58,
      "duration": 3.9
    },
    {
      "lang": "en",
      "text": "pass and the reason for that is if you",
      "offset": 5260.56,
      "duration": 3.179
    },
    {
      "lang": "en",
      "text": "just look at the mathematical form of",
      "offset": 5262.48,
      "duration": 3.06
    },
    {
      "lang": "en",
      "text": "this and differentiate again you will",
      "offset": 5263.739,
      "duration": 3.121
    },
    {
      "lang": "en",
      "text": "end up with a very small and short",
      "offset": 5265.54,
      "duration": 3.42
    },
    {
      "lang": "en",
      "text": "expression so that's what we want to do",
      "offset": 5266.86,
      "duration": 4.74
    },
    {
      "lang": "en",
      "text": "here we want to in a single operation or",
      "offset": 5268.96,
      "duration": 5.46
    },
    {
      "lang": "en",
      "text": "in a single go or like very quickly go",
      "offset": 5271.6,
      "duration": 4.86
    },
    {
      "lang": "en",
      "text": "directly to delojits",
      "offset": 5274.42,
      "duration": 4.739
    },
    {
      "lang": "en",
      "text": "and we need to implement the logits as a",
      "offset": 5276.46,
      "duration": 5.94
    },
    {
      "lang": "en",
      "text": "function of logits and yb's",
      "offset": 5279.159,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "but it will be significantly shorter",
      "offset": 5282.4,
      "duration": 4.38
    },
    {
      "lang": "en",
      "text": "than whatever we did here where to get",
      "offset": 5284.199,
      "duration": 4.5
    },
    {
      "lang": "en",
      "text": "to deluggets we had to go all the way",
      "offset": 5286.78,
      "duration": 3.24
    },
    {
      "lang": "en",
      "text": "here",
      "offset": 5288.699,
      "duration": 3.781
    },
    {
      "lang": "en",
      "text": "so all of this work can be skipped in a",
      "offset": 5290.02,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "much much simpler mathematical",
      "offset": 5292.48,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "expression that you can Implement here",
      "offset": 5293.86,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "so you can give it a shot yourself",
      "offset": 5296.32,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "basically look at what exactly is the",
      "offset": 5298.659,
      "duration": 4.861
    },
    {
      "lang": "en",
      "text": "mathematical expression of loss and",
      "offset": 5301.84,
      "duration": 4.44
    },
    {
      "lang": "en",
      "text": "differentiate with respect to the logits",
      "offset": 5303.52,
      "duration": 6.179
    },
    {
      "lang": "en",
      "text": "so let me show you a hint you can of",
      "offset": 5306.28,
      "duration": 5.7
    },
    {
      "lang": "en",
      "text": "course try it fully yourself but if not",
      "offset": 5309.699,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "I can give you some hint of how to get",
      "offset": 5311.98,
      "duration": 4.46
    },
    {
      "lang": "en",
      "text": "started mathematically",
      "offset": 5313.54,
      "duration": 2.9
    },
    {
      "lang": "en",
      "text": "so basically What's Happening Here is we",
      "offset": 5316.6,
      "duration": 4.44
    },
    {
      "lang": "en",
      "text": "have logits then there's a softmax that",
      "offset": 5318.699,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "takes the logits and gives you",
      "offset": 5321.04,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "probabilities then we are using the",
      "offset": 5322.06,
      "duration": 4.679
    },
    {
      "lang": "en",
      "text": "identity of the correct next character",
      "offset": 5324.88,
      "duration": 5.16
    },
    {
      "lang": "en",
      "text": "to pluck out a row of probabilities take",
      "offset": 5326.739,
      "duration": 5.041
    },
    {
      "lang": "en",
      "text": "the negative log of it to get our",
      "offset": 5330.04,
      "duration": 4.02
    },
    {
      "lang": "en",
      "text": "negative block probability and then we",
      "offset": 5331.78,
      "duration": 5.1
    },
    {
      "lang": "en",
      "text": "average up all the log probabilities or",
      "offset": 5334.06,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "negative block probabilities to get our",
      "offset": 5336.88,
      "duration": 2.94
    },
    {
      "lang": "en",
      "text": "loss",
      "offset": 5338.139,
      "duration": 3.481
    },
    {
      "lang": "en",
      "text": "so basically what we have is for a",
      "offset": 5339.82,
      "duration": 4.2
    },
    {
      "lang": "en",
      "text": "single individual example rather we have",
      "offset": 5341.62,
      "duration": 4.44
    },
    {
      "lang": "en",
      "text": "that loss is equal to negative log",
      "offset": 5344.02,
      "duration": 5.4
    },
    {
      "lang": "en",
      "text": "probability uh where P here is kind of",
      "offset": 5346.06,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "like thought of as a vector of all the",
      "offset": 5349.42,
      "duration": 4.739
    },
    {
      "lang": "en",
      "text": "probabilities so at the Y position where",
      "offset": 5351.1,
      "duration": 5.4
    },
    {
      "lang": "en",
      "text": "Y is the label",
      "offset": 5354.159,
      "duration": 5.461
    },
    {
      "lang": "en",
      "text": "and we have that P here of course is the",
      "offset": 5356.5,
      "duration": 6.96
    },
    {
      "lang": "en",
      "text": "softmax so the ith component of P of",
      "offset": 5359.62,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "this probability Vector is just the",
      "offset": 5363.46,
      "duration": 4.86
    },
    {
      "lang": "en",
      "text": "softmax function so raising all the",
      "offset": 5365.38,
      "duration": 6.54
    },
    {
      "lang": "en",
      "text": "logits uh basically to the power of E",
      "offset": 5368.32,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "and normalizing so everything comes to",
      "offset": 5371.92,
      "duration": 3.48
    },
    {
      "lang": "en",
      "text": "1.",
      "offset": 5374.32,
      "duration": 4.02
    },
    {
      "lang": "en",
      "text": "now if you write out P of Y here you can",
      "offset": 5375.4,
      "duration": 4.86
    },
    {
      "lang": "en",
      "text": "just write out the soft Max and then",
      "offset": 5378.34,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "basically what we're interested in is",
      "offset": 5380.26,
      "duration": 2.82
    },
    {
      "lang": "en",
      "text": "we're interested in the derivative of",
      "offset": 5381.34,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "the loss with respect to the I logit",
      "offset": 5383.08,
      "duration": 7.98
    },
    {
      "lang": "en",
      "text": "and so basically it's a d by DLI of this",
      "offset": 5387.82,
      "duration": 4.919
    },
    {
      "lang": "en",
      "text": "expression here",
      "offset": 5391.06,
      "duration": 3.24
    },
    {
      "lang": "en",
      "text": "where we have L indexed with the",
      "offset": 5392.739,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "specific label Y and on the bottom we",
      "offset": 5394.3,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "have a sum over J of e to the L J and",
      "offset": 5396.58,
      "duration": 4.38
    },
    {
      "lang": "en",
      "text": "the negative block of all that so",
      "offset": 5398.86,
      "duration": 3.779
    },
    {
      "lang": "en",
      "text": "potentially give it a shot pen and paper",
      "offset": 5400.96,
      "duration": 3.179
    },
    {
      "lang": "en",
      "text": "and see if you can actually derive the",
      "offset": 5402.639,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "expression for the loss by DLI and then",
      "offset": 5404.139,
      "duration": 5.401
    },
    {
      "lang": "en",
      "text": "we're going to implement it here okay so",
      "offset": 5407.199,
      "duration": 4.5
    },
    {
      "lang": "en",
      "text": "I'm going to give away the result here",
      "offset": 5409.54,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "so this is some of the math I did to",
      "offset": 5411.699,
      "duration": 5.701
    },
    {
      "lang": "en",
      "text": "derive the gradients analytically and so",
      "offset": 5413.38,
      "duration": 5.64
    },
    {
      "lang": "en",
      "text": "we see here that I'm just applying the",
      "offset": 5417.4,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "rules of calculus from your first or",
      "offset": 5419.02,
      "duration": 3.119
    },
    {
      "lang": "en",
      "text": "second year of bachelor's degree if you",
      "offset": 5420.52,
      "duration": 4.139
    },
    {
      "lang": "en",
      "text": "took it and we see that the expression",
      "offset": 5422.139,
      "duration": 4.201
    },
    {
      "lang": "en",
      "text": "is actually simplify quite a bit you",
      "offset": 5424.659,
      "duration": 3.301
    },
    {
      "lang": "en",
      "text": "have to separate out the analysis in the",
      "offset": 5426.34,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "case where the ith index that you're",
      "offset": 5427.96,
      "duration": 4.199
    },
    {
      "lang": "en",
      "text": "interested in inside logits is either",
      "offset": 5430.179,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "equal to the label or it's not equal to",
      "offset": 5432.159,
      "duration": 3.54
    },
    {
      "lang": "en",
      "text": "the label and then the expression",
      "offset": 5434.26,
      "duration": 3.3
    },
    {
      "lang": "en",
      "text": "simplify and cancel in a slightly",
      "offset": 5435.699,
      "duration": 4.02
    },
    {
      "lang": "en",
      "text": "different way and what we end up with is",
      "offset": 5437.56,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "something very very simple",
      "offset": 5439.719,
      "duration": 3.301
    },
    {
      "lang": "en",
      "text": "and we either end up with basically",
      "offset": 5441.159,
      "duration": 5.06
    },
    {
      "lang": "en",
      "text": "pirai where p is again this Vector of",
      "offset": 5443.02,
      "duration": 6.36
    },
    {
      "lang": "en",
      "text": "probabilities after a soft Max or P at I",
      "offset": 5446.219,
      "duration": 5.381
    },
    {
      "lang": "en",
      "text": "minus 1 where we just simply subtract a",
      "offset": 5449.38,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "one but in any case we just need to",
      "offset": 5451.6,
      "duration": 4.74
    },
    {
      "lang": "en",
      "text": "calculate the soft Max p e and then in",
      "offset": 5453.699,
      "duration": 4.5
    },
    {
      "lang": "en",
      "text": "the correct Dimension we need to",
      "offset": 5456.34,
      "duration": 4.14
    },
    {
      "lang": "en",
      "text": "subtract one and that's the gradient the",
      "offset": 5458.199,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "form that it takes analytically so let's",
      "offset": 5460.48,
      "duration": 4.199
    },
    {
      "lang": "en",
      "text": "implement this basically and we have to",
      "offset": 5463,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "keep in mind that this is only done for",
      "offset": 5464.679,
      "duration": 3.54
    },
    {
      "lang": "en",
      "text": "a single example but here we are working",
      "offset": 5466,
      "duration": 3.78
    },
    {
      "lang": "en",
      "text": "with batches of examples",
      "offset": 5468.219,
      "duration": 4.141
    },
    {
      "lang": "en",
      "text": "so we have to be careful of that and",
      "offset": 5469.78,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "then the loss for a batch is the average",
      "offset": 5472.36,
      "duration": 4.74
    },
    {
      "lang": "en",
      "text": "loss over all the examples so in other",
      "offset": 5474.82,
      "duration": 3.66
    },
    {
      "lang": "en",
      "text": "words is the example for all the",
      "offset": 5477.1,
      "duration": 3.539
    },
    {
      "lang": "en",
      "text": "individual examples is the loss for each",
      "offset": 5478.48,
      "duration": 4.259
    },
    {
      "lang": "en",
      "text": "individual example summed up and then",
      "offset": 5480.639,
      "duration": 4.02
    },
    {
      "lang": "en",
      "text": "divided by n and we have to back",
      "offset": 5482.739,
      "duration": 3.541
    },
    {
      "lang": "en",
      "text": "propagate through that as well and be",
      "offset": 5484.659,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "careful with it",
      "offset": 5486.28,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "so deluggets is going to be of that soft",
      "offset": 5488.02,
      "duration": 4.679
    },
    {
      "lang": "en",
      "text": "Max",
      "offset": 5490.6,
      "duration": 4.44
    },
    {
      "lang": "en",
      "text": "uh pytorch has a softmax function that",
      "offset": 5492.699,
      "duration": 4.201
    },
    {
      "lang": "en",
      "text": "you can call and we want to apply the",
      "offset": 5495.04,
      "duration": 4.38
    },
    {
      "lang": "en",
      "text": "softmax on the logits and we want to go",
      "offset": 5496.9,
      "duration": 5.7
    },
    {
      "lang": "en",
      "text": "in the dimension that is one so",
      "offset": 5499.42,
      "duration": 4.62
    },
    {
      "lang": "en",
      "text": "basically we want to do the softmax",
      "offset": 5502.6,
      "duration": 4.86
    },
    {
      "lang": "en",
      "text": "along the rows of these logits",
      "offset": 5504.04,
      "duration": 5.34
    },
    {
      "lang": "en",
      "text": "then at the correct positions we need to",
      "offset": 5507.46,
      "duration": 5.46
    },
    {
      "lang": "en",
      "text": "subtract a 1. so delugits at iterating",
      "offset": 5509.38,
      "duration": 5.1
    },
    {
      "lang": "en",
      "text": "over all the rows",
      "offset": 5512.92,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "and indexing into the columns",
      "offset": 5514.48,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "provided by the correct labels inside YB",
      "offset": 5517,
      "duration": 6.06
    },
    {
      "lang": "en",
      "text": "we need to subtract one",
      "offset": 5520,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "and then finally it's the average loss",
      "offset": 5523.06,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "that is the loss and in the average",
      "offset": 5525.52,
      "duration": 3.96
    },
    {
      "lang": "en",
      "text": "there's a one over n of all the losses",
      "offset": 5527.139,
      "duration": 4.861
    },
    {
      "lang": "en",
      "text": "added up and so we need to also",
      "offset": 5529.48,
      "duration": 4.92
    },
    {
      "lang": "en",
      "text": "propagate through that division",
      "offset": 5532,
      "duration": 4.5
    },
    {
      "lang": "en",
      "text": "so the gradient has to be scaled down by",
      "offset": 5534.4,
      "duration": 5.46
    },
    {
      "lang": "en",
      "text": "by n as well because of the mean",
      "offset": 5536.5,
      "duration": 5.699
    },
    {
      "lang": "en",
      "text": "but this otherwise should be the result",
      "offset": 5539.86,
      "duration": 4.92
    },
    {
      "lang": "en",
      "text": "so now if we verify this",
      "offset": 5542.199,
      "duration": 4.681
    },
    {
      "lang": "en",
      "text": "we see that we don't get an exact match",
      "offset": 5544.78,
      "duration": 5.22
    },
    {
      "lang": "en",
      "text": "but at the same time the maximum",
      "offset": 5546.88,
      "duration": 6.72
    },
    {
      "lang": "en",
      "text": "difference from logits from pytorch and",
      "offset": 5550,
      "duration": 7.32
    },
    {
      "lang": "en",
      "text": "RD logits here is uh on the order of 5e",
      "offset": 5553.6,
      "duration": 5.46
    },
    {
      "lang": "en",
      "text": "negative 9. so it's a tiny tiny number",
      "offset": 5557.32,
      "duration": 4.26
    },
    {
      "lang": "en",
      "text": "so because of floating point wantiness",
      "offset": 5559.06,
      "duration": 5.099
    },
    {
      "lang": "en",
      "text": "we don't get the exact bitwise result",
      "offset": 5561.58,
      "duration": 5.94
    },
    {
      "lang": "en",
      "text": "but we basically get the correct answer",
      "offset": 5564.159,
      "duration": 4.921
    },
    {
      "lang": "en",
      "text": "approximately",
      "offset": 5567.52,
      "duration": 3.54
    },
    {
      "lang": "en",
      "text": "now I'd like to pause here briefly",
      "offset": 5569.08,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "before we move on to the next exercise",
      "offset": 5571.06,
      "duration": 3.42
    },
    {
      "lang": "en",
      "text": "because I'd like us to get an intuitive",
      "offset": 5572.44,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "sense of what the logits is because it",
      "offset": 5574.48,
      "duration": 3.8
    },
    {
      "lang": "en",
      "text": "has a beautiful and very simple",
      "offset": 5576.76,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "explanation honestly",
      "offset": 5578.28,
      "duration": 5.02
    },
    {
      "lang": "en",
      "text": "um so here I'm taking the logits and I'm",
      "offset": 5580.6,
      "duration": 4.619
    },
    {
      "lang": "en",
      "text": "visualizing it and we can see that we",
      "offset": 5583.3,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "have a batch of 32 examples of 27",
      "offset": 5585.219,
      "duration": 3.241
    },
    {
      "lang": "en",
      "text": "characters",
      "offset": 5587.38,
      "duration": 3.42
    },
    {
      "lang": "en",
      "text": "and what is the logits intuitively right",
      "offset": 5588.46,
      "duration": 5.4
    },
    {
      "lang": "en",
      "text": "the logits is the probabilities that the",
      "offset": 5590.8,
      "duration": 4.74
    },
    {
      "lang": "en",
      "text": "properties Matrix in the forward pass",
      "offset": 5593.86,
      "duration": 3.779
    },
    {
      "lang": "en",
      "text": "but then here these black squares are",
      "offset": 5595.54,
      "duration": 3.54
    },
    {
      "lang": "en",
      "text": "the positions of the correct indices",
      "offset": 5597.639,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "where we subtracted a one",
      "offset": 5599.08,
      "duration": 5.46
    },
    {
      "lang": "en",
      "text": "and so uh what is this doing right these",
      "offset": 5601.719,
      "duration": 6.121
    },
    {
      "lang": "en",
      "text": "are the derivatives on the logits and so",
      "offset": 5604.54,
      "duration": 7.199
    },
    {
      "lang": "en",
      "text": "let's look at just the first row here",
      "offset": 5607.84,
      "duration": 5.46
    },
    {
      "lang": "en",
      "text": "so that's what I'm doing here I'm",
      "offset": 5611.739,
      "duration": 2.881
    },
    {
      "lang": "en",
      "text": "clocking the probabilities of these",
      "offset": 5613.3,
      "duration": 2.82
    },
    {
      "lang": "en",
      "text": "logits and then I'm taking just the",
      "offset": 5614.62,
      "duration": 3.66
    },
    {
      "lang": "en",
      "text": "first row and this is the probability",
      "offset": 5616.12,
      "duration": 5.22
    },
    {
      "lang": "en",
      "text": "row and then the logits of the first row",
      "offset": 5618.28,
      "duration": 5.64
    },
    {
      "lang": "en",
      "text": "and multiplying by n just for us so that",
      "offset": 5621.34,
      "duration": 4.98
    },
    {
      "lang": "en",
      "text": "we don't have the scaling by n in here",
      "offset": 5623.92,
      "duration": 4.62
    },
    {
      "lang": "en",
      "text": "and everything is more interpretable we",
      "offset": 5626.32,
      "duration": 3.72
    },
    {
      "lang": "en",
      "text": "see that it's exactly equal to the",
      "offset": 5628.54,
      "duration": 3.72
    },
    {
      "lang": "en",
      "text": "probability of course but then the",
      "offset": 5630.04,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "position of the correct index has a",
      "offset": 5632.26,
      "duration": 4.14
    },
    {
      "lang": "en",
      "text": "minus equals one so minus one on that",
      "offset": 5633.639,
      "duration": 3.721
    },
    {
      "lang": "en",
      "text": "position",
      "offset": 5636.4,
      "duration": 2.819
    },
    {
      "lang": "en",
      "text": "and so notice that",
      "offset": 5637.36,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "um if you take Delo Jets at zero and you",
      "offset": 5639.219,
      "duration": 4.201
    },
    {
      "lang": "en",
      "text": "sum it",
      "offset": 5641.679,
      "duration": 4.621
    },
    {
      "lang": "en",
      "text": "it actually sums to zero and so you",
      "offset": 5643.42,
      "duration": 5.46
    },
    {
      "lang": "en",
      "text": "should think of these uh gradients here",
      "offset": 5646.3,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "at each cell as like a force",
      "offset": 5648.88,
      "duration": 6.6
    },
    {
      "lang": "en",
      "text": "um we are going to be basically pulling",
      "offset": 5652.54,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "down on the probabilities of the",
      "offset": 5655.48,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "incorrect characters and we're going to",
      "offset": 5657.82,
      "duration": 4.859
    },
    {
      "lang": "en",
      "text": "be pulling up on the probability at the",
      "offset": 5659.56,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "correct index and that's what's",
      "offset": 5662.679,
      "duration": 6.421
    },
    {
      "lang": "en",
      "text": "basically happening in each row and thus",
      "offset": 5664.6,
      "duration": 7.079
    },
    {
      "lang": "en",
      "text": "the amount of push and pull is exactly",
      "offset": 5669.1,
      "duration": 5.579
    },
    {
      "lang": "en",
      "text": "equalized because the sum is zero so the",
      "offset": 5671.679,
      "duration": 4.5
    },
    {
      "lang": "en",
      "text": "amount to which we pull down in the",
      "offset": 5674.679,
      "duration": 2.881
    },
    {
      "lang": "en",
      "text": "probabilities and the demand that we",
      "offset": 5676.179,
      "duration": 3.301
    },
    {
      "lang": "en",
      "text": "push up on the probability of the",
      "offset": 5677.56,
      "duration": 3.659
    },
    {
      "lang": "en",
      "text": "correct character is equal",
      "offset": 5679.48,
      "duration": 3.719
    },
    {
      "lang": "en",
      "text": "so sort of the the repulsion and the",
      "offset": 5681.219,
      "duration": 4.44
    },
    {
      "lang": "en",
      "text": "attraction are equal and think of the",
      "offset": 5683.199,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "neural app now as a like a massive uh",
      "offset": 5685.659,
      "duration": 4.681
    },
    {
      "lang": "en",
      "text": "pulley system or something like that",
      "offset": 5688.199,
      "duration": 4.121
    },
    {
      "lang": "en",
      "text": "we're up here on top of the logits and",
      "offset": 5690.34,
      "duration": 4.02
    },
    {
      "lang": "en",
      "text": "we're pulling up we're pulling down the",
      "offset": 5692.32,
      "duration": 3.48
    },
    {
      "lang": "en",
      "text": "properties of Incorrect and pulling up",
      "offset": 5694.36,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "the property of the correct and in this",
      "offset": 5695.8,
      "duration": 3.54
    },
    {
      "lang": "en",
      "text": "complicated pulley system because",
      "offset": 5697.96,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "everything is mathematically uh just",
      "offset": 5699.34,
      "duration": 3.96
    },
    {
      "lang": "en",
      "text": "determined just think of it as sort of",
      "offset": 5701.32,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "like this tension translating to this",
      "offset": 5703.3,
      "duration": 4.02
    },
    {
      "lang": "en",
      "text": "complicating pulling mechanism and then",
      "offset": 5705.4,
      "duration": 4.14
    },
    {
      "lang": "en",
      "text": "eventually we get a tug on the weights",
      "offset": 5707.32,
      "duration": 4.14
    },
    {
      "lang": "en",
      "text": "and the biases and basically in each",
      "offset": 5709.54,
      "duration": 3.96
    },
    {
      "lang": "en",
      "text": "update we just kind of like tug in the",
      "offset": 5711.46,
      "duration": 3.66
    },
    {
      "lang": "en",
      "text": "direction that we like for each of these",
      "offset": 5713.5,
      "duration": 3.78
    },
    {
      "lang": "en",
      "text": "elements and the parameters are slowly",
      "offset": 5715.12,
      "duration": 4.019
    },
    {
      "lang": "en",
      "text": "given in to the tug and that's what",
      "offset": 5717.28,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "training in neural net kind of like",
      "offset": 5719.139,
      "duration": 2.941
    },
    {
      "lang": "en",
      "text": "looks like on a high level",
      "offset": 5720.639,
      "duration": 4.02
    },
    {
      "lang": "en",
      "text": "and so I think the the forces of push",
      "offset": 5722.08,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "and pull in these gradients are actually",
      "offset": 5724.659,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "uh very intuitive here we're pushing and",
      "offset": 5726.88,
      "duration": 4.14
    },
    {
      "lang": "en",
      "text": "pulling on the correct answer and the",
      "offset": 5729.46,
      "duration": 3.779
    },
    {
      "lang": "en",
      "text": "incorrect answers and the amount of",
      "offset": 5731.02,
      "duration": 3.54
    },
    {
      "lang": "en",
      "text": "force that we're applying is actually",
      "offset": 5733.239,
      "duration": 3.9
    },
    {
      "lang": "en",
      "text": "proportional to uh the probabilities",
      "offset": 5734.56,
      "duration": 4.98
    },
    {
      "lang": "en",
      "text": "that came out in the forward pass",
      "offset": 5737.139,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "and so for example if our probabilities",
      "offset": 5739.54,
      "duration": 4.02
    },
    {
      "lang": "en",
      "text": "came out exactly correct so they would",
      "offset": 5741.219,
      "duration": 4.381
    },
    {
      "lang": "en",
      "text": "have had zero everywhere except for one",
      "offset": 5743.56,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "at the correct uh position then the the",
      "offset": 5745.6,
      "duration": 5.7
    },
    {
      "lang": "en",
      "text": "logits would be all a row of zeros for",
      "offset": 5748.6,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "that example there would be no push and",
      "offset": 5751.3,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "pull so the amount to which your",
      "offset": 5752.92,
      "duration": 5.219
    },
    {
      "lang": "en",
      "text": "prediction is incorrect is exactly the",
      "offset": 5755.62,
      "duration": 3.78
    },
    {
      "lang": "en",
      "text": "amount by which you're going to get a",
      "offset": 5758.139,
      "duration": 3.781
    },
    {
      "lang": "en",
      "text": "pull or a push in that dimension",
      "offset": 5759.4,
      "duration": 4.68
    },
    {
      "lang": "en",
      "text": "so if you have for example a very",
      "offset": 5761.92,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "confidently mispredicted element here",
      "offset": 5764.08,
      "duration": 3.24
    },
    {
      "lang": "en",
      "text": "then",
      "offset": 5765.76,
      "duration": 2.52
    },
    {
      "lang": "en",
      "text": "um what's going to happen is that",
      "offset": 5767.32,
      "duration": 3.3
    },
    {
      "lang": "en",
      "text": "element is going to be pulled down very",
      "offset": 5768.28,
      "duration": 4.379
    },
    {
      "lang": "en",
      "text": "heavily and the correct answer is going",
      "offset": 5770.62,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "to be pulled up to the same amount",
      "offset": 5772.659,
      "duration": 3.961
    },
    {
      "lang": "en",
      "text": "and the other characters are not going",
      "offset": 5774.94,
      "duration": 4.38
    },
    {
      "lang": "en",
      "text": "to be influenced too much",
      "offset": 5776.62,
      "duration": 4.44
    },
    {
      "lang": "en",
      "text": "so the amounts to which you mispredict",
      "offset": 5779.32,
      "duration": 4.02
    },
    {
      "lang": "en",
      "text": "is then proportional to the strength of",
      "offset": 5781.06,
      "duration": 4.5
    },
    {
      "lang": "en",
      "text": "the pole and that's happening",
      "offset": 5783.34,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "independently in all the dimensions of",
      "offset": 5785.56,
      "duration": 3.96
    },
    {
      "lang": "en",
      "text": "this of this tensor and it's sort of",
      "offset": 5787.179,
      "duration": 3.721
    },
    {
      "lang": "en",
      "text": "very intuitive and varies to think",
      "offset": 5789.52,
      "duration": 3.42
    },
    {
      "lang": "en",
      "text": "through and that's basically the magic",
      "offset": 5790.9,
      "duration": 3.72
    },
    {
      "lang": "en",
      "text": "of the cross-entropy loss and what it's",
      "offset": 5792.94,
      "duration": 3.66
    },
    {
      "lang": "en",
      "text": "doing dynamically in the backward pass",
      "offset": 5794.62,
      "duration": 4.019
    },
    {
      "lang": "en",
      "text": "of the neural net so now we get to",
      "offset": 5796.6,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "exercise number three which is a very",
      "offset": 5798.639,
      "duration": 3.901
    },
    {
      "lang": "en",
      "text": "fun exercise",
      "offset": 5801.159,
      "duration": 2.701
    },
    {
      "lang": "en",
      "text": "um depending on your definition of fun",
      "offset": 5802.54,
      "duration": 2.94
    },
    {
      "lang": "en",
      "text": "and we are going to do for batch",
      "offset": 5803.86,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "normalization exactly what we did for",
      "offset": 5805.48,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "cross entropy loss in exercise number",
      "offset": 5807.219,
      "duration": 3.96
    },
    {
      "lang": "en",
      "text": "two that is we are going to consider it",
      "offset": 5809.08,
      "duration": 3.72
    },
    {
      "lang": "en",
      "text": "as a glued single mathematical",
      "offset": 5811.179,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "expression and back propagate through it",
      "offset": 5812.8,
      "duration": 3.78
    },
    {
      "lang": "en",
      "text": "in a very efficient manner because we",
      "offset": 5814.78,
      "duration": 3.419
    },
    {
      "lang": "en",
      "text": "are going to derive a much simpler",
      "offset": 5816.58,
      "duration": 3.3
    },
    {
      "lang": "en",
      "text": "formula for the backward path of batch",
      "offset": 5818.199,
      "duration": 2.821
    },
    {
      "lang": "en",
      "text": "normalization",
      "offset": 5819.88,
      "duration": 2.759
    },
    {
      "lang": "en",
      "text": "and we're going to do that using pen and",
      "offset": 5821.02,
      "duration": 2.46
    },
    {
      "lang": "en",
      "text": "paper",
      "offset": 5822.639,
      "duration": 2.58
    },
    {
      "lang": "en",
      "text": "so previously we've broken up",
      "offset": 5823.48,
      "duration": 3.3
    },
    {
      "lang": "en",
      "text": "bastionalization into all of the little",
      "offset": 5825.219,
      "duration": 3.241
    },
    {
      "lang": "en",
      "text": "intermediate pieces and all the atomic",
      "offset": 5826.78,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "operations inside it and then we back",
      "offset": 5828.46,
      "duration": 4.98
    },
    {
      "lang": "en",
      "text": "propagate it through it one by one",
      "offset": 5830.38,
      "duration": 5.4
    },
    {
      "lang": "en",
      "text": "now we just have a single sort of",
      "offset": 5833.44,
      "duration": 5.34
    },
    {
      "lang": "en",
      "text": "forward pass of a batch form and it's",
      "offset": 5835.78,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "all glued together",
      "offset": 5838.78,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "and we see that we get the exact same",
      "offset": 5840.1,
      "duration": 2.94
    },
    {
      "lang": "en",
      "text": "result as before",
      "offset": 5841.78,
      "duration": 3.48
    },
    {
      "lang": "en",
      "text": "now for the backward pass we'd like to",
      "offset": 5843.04,
      "duration": 4.44
    },
    {
      "lang": "en",
      "text": "also Implement a single formula",
      "offset": 5845.26,
      "duration": 4.26
    },
    {
      "lang": "en",
      "text": "basically for back propagating through",
      "offset": 5847.48,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "this entire operation that is the",
      "offset": 5849.52,
      "duration": 2.699
    },
    {
      "lang": "en",
      "text": "bachelorization",
      "offset": 5850.84,
      "duration": 3.42
    },
    {
      "lang": "en",
      "text": "so in the forward pass previously we",
      "offset": 5852.219,
      "duration": 5.221
    },
    {
      "lang": "en",
      "text": "took hpvn the hidden states of the",
      "offset": 5854.26,
      "duration": 5.64
    },
    {
      "lang": "en",
      "text": "pre-batch realization and created H",
      "offset": 5857.44,
      "duration": 4.92
    },
    {
      "lang": "en",
      "text": "preact which is the hidden States just",
      "offset": 5859.9,
      "duration": 4.14
    },
    {
      "lang": "en",
      "text": "before the activation",
      "offset": 5862.36,
      "duration": 4.26
    },
    {
      "lang": "en",
      "text": "in the bachelorization paper each pbn is",
      "offset": 5864.04,
      "duration": 5.58
    },
    {
      "lang": "en",
      "text": "X and each preact is y",
      "offset": 5866.62,
      "duration": 4.5
    },
    {
      "lang": "en",
      "text": "so in the backward pass what we'd like",
      "offset": 5869.62,
      "duration": 4.5
    },
    {
      "lang": "en",
      "text": "to do now is we have DH preact and we'd",
      "offset": 5871.12,
      "duration": 5.46
    },
    {
      "lang": "en",
      "text": "like to produce d h previous",
      "offset": 5874.12,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "and we'd like to do that in a very",
      "offset": 5876.58,
      "duration": 3.48
    },
    {
      "lang": "en",
      "text": "efficient manner so that's the name of",
      "offset": 5877.96,
      "duration": 4.62
    },
    {
      "lang": "en",
      "text": "the game calculate the H previan given",
      "offset": 5880.06,
      "duration": 5.159
    },
    {
      "lang": "en",
      "text": "DH preact and for the purposes of this",
      "offset": 5882.58,
      "duration": 4.68
    },
    {
      "lang": "en",
      "text": "exercise we're going to ignore gamma and",
      "offset": 5885.219,
      "duration": 4.44
    },
    {
      "lang": "en",
      "text": "beta and their derivatives because they",
      "offset": 5887.26,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "take on a very simple form in a very",
      "offset": 5889.659,
      "duration": 4.621
    },
    {
      "lang": "en",
      "text": "similar way to what we did up above",
      "offset": 5891.58,
      "duration": 6.42
    },
    {
      "lang": "en",
      "text": "so let's calculate this given that right",
      "offset": 5894.28,
      "duration": 4.68
    },
    {
      "lang": "en",
      "text": "here",
      "offset": 5898,
      "duration": 2.699
    },
    {
      "lang": "en",
      "text": "so to help you a little bit like I did",
      "offset": 5898.96,
      "duration": 4.62
    },
    {
      "lang": "en",
      "text": "before I started off the implementation",
      "offset": 5900.699,
      "duration": 6.061
    },
    {
      "lang": "en",
      "text": "here on pen and paper and I took two",
      "offset": 5903.58,
      "duration": 4.44
    },
    {
      "lang": "en",
      "text": "sheets of paper to derive the",
      "offset": 5906.76,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "mathematical formulas for the backward",
      "offset": 5908.02,
      "duration": 2.46
    },
    {
      "lang": "en",
      "text": "pass",
      "offset": 5909.76,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "and basically to set up the problem uh",
      "offset": 5910.48,
      "duration": 4.92
    },
    {
      "lang": "en",
      "text": "just write out the MU Sigma Square",
      "offset": 5913.12,
      "duration": 6.18
    },
    {
      "lang": "en",
      "text": "variance x i hat and Y I exactly as in",
      "offset": 5915.4,
      "duration": 5.279
    },
    {
      "lang": "en",
      "text": "the paper except for the bezel",
      "offset": 5919.3,
      "duration": 2.52
    },
    {
      "lang": "en",
      "text": "correction",
      "offset": 5920.679,
      "duration": 2.281
    },
    {
      "lang": "en",
      "text": "and then",
      "offset": 5921.82,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "in a backward pass we have the",
      "offset": 5922.96,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "derivative of the loss with respect to",
      "offset": 5924.82,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "all the elements of Y and remember that",
      "offset": 5926.08,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "Y is a vector there's there's multiple",
      "offset": 5928.659,
      "duration": 3.421
    },
    {
      "lang": "en",
      "text": "numbers here",
      "offset": 5930.639,
      "duration": 3.961
    },
    {
      "lang": "en",
      "text": "so we have all the derivatives with",
      "offset": 5932.08,
      "duration": 4.86
    },
    {
      "lang": "en",
      "text": "respect to all the Y's",
      "offset": 5934.6,
      "duration": 4.68
    },
    {
      "lang": "en",
      "text": "and then there's a demo and a beta and",
      "offset": 5936.94,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "this is kind of like the compute graph",
      "offset": 5939.28,
      "duration": 3.899
    },
    {
      "lang": "en",
      "text": "the gamma and the beta there's the X hat",
      "offset": 5941.02,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "and then the MU and the sigma squared",
      "offset": 5943.179,
      "duration": 7.321
    },
    {
      "lang": "en",
      "text": "and the X so we have DL by DYI and we",
      "offset": 5946.54,
      "duration": 6.9
    },
    {
      "lang": "en",
      "text": "won't DL by d x i for all the I's in",
      "offset": 5950.5,
      "duration": 4.739
    },
    {
      "lang": "en",
      "text": "these vectors",
      "offset": 5953.44,
      "duration": 4.14
    },
    {
      "lang": "en",
      "text": "so this is the compute graph and you",
      "offset": 5955.239,
      "duration": 4.741
    },
    {
      "lang": "en",
      "text": "have to be careful because I'm trying to",
      "offset": 5957.58,
      "duration": 4.74
    },
    {
      "lang": "en",
      "text": "note here that these are vectors so",
      "offset": 5959.98,
      "duration": 5.46
    },
    {
      "lang": "en",
      "text": "there's many nodes here inside x x hat",
      "offset": 5962.32,
      "duration": 6.72
    },
    {
      "lang": "en",
      "text": "and Y but mu and sigma sorry Sigma",
      "offset": 5965.44,
      "duration": 5.4
    },
    {
      "lang": "en",
      "text": "Square are just individual scalars",
      "offset": 5969.04,
      "duration": 4.26
    },
    {
      "lang": "en",
      "text": "single numbers so you have to be careful",
      "offset": 5970.84,
      "duration": 3.66
    },
    {
      "lang": "en",
      "text": "with that you have to imagine there's",
      "offset": 5973.3,
      "duration": 2.58
    },
    {
      "lang": "en",
      "text": "multiple nodes here or you're going to",
      "offset": 5974.5,
      "duration": 3.96
    },
    {
      "lang": "en",
      "text": "get your math wrong",
      "offset": 5975.88,
      "duration": 4.98
    },
    {
      "lang": "en",
      "text": "um so as an example I would suggest that",
      "offset": 5978.46,
      "duration": 4.739
    },
    {
      "lang": "en",
      "text": "you go in the following order one two",
      "offset": 5980.86,
      "duration": 3.9
    },
    {
      "lang": "en",
      "text": "three four in terms of the back",
      "offset": 5983.199,
      "duration": 3.661
    },
    {
      "lang": "en",
      "text": "propagation so back propagating to X hat",
      "offset": 5984.76,
      "duration": 4.919
    },
    {
      "lang": "en",
      "text": "then into Sigma Square then into mu and",
      "offset": 5986.86,
      "duration": 5.22
    },
    {
      "lang": "en",
      "text": "then into X",
      "offset": 5989.679,
      "duration": 4.381
    },
    {
      "lang": "en",
      "text": "um just like in a topological sort in",
      "offset": 5992.08,
      "duration": 3.78
    },
    {
      "lang": "en",
      "text": "micrograd we would go from right to left",
      "offset": 5994.06,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "you're doing the exact same thing except",
      "offset": 5995.86,
      "duration": 3.9
    },
    {
      "lang": "en",
      "text": "you're doing it with symbols and on a",
      "offset": 5997.659,
      "duration": 3.901
    },
    {
      "lang": "en",
      "text": "piece of paper",
      "offset": 5999.76,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "so for number one uh I'm not giving away",
      "offset": 6001.56,
      "duration": 8.4
    },
    {
      "lang": "en",
      "text": "too much if you want DL of d x i hat",
      "offset": 6005.28,
      "duration": 7.439
    },
    {
      "lang": "en",
      "text": "then we just take DL by DYI and multiply",
      "offset": 6009.96,
      "duration": 5.219
    },
    {
      "lang": "en",
      "text": "it by gamma because of this expression",
      "offset": 6012.719,
      "duration": 5.041
    },
    {
      "lang": "en",
      "text": "here where any individual Yi is just",
      "offset": 6015.179,
      "duration": 6.421
    },
    {
      "lang": "en",
      "text": "gamma times x i hat plus beta so it",
      "offset": 6017.76,
      "duration": 6.12
    },
    {
      "lang": "en",
      "text": "doesn't help you too much there but this",
      "offset": 6021.6,
      "duration": 3.78
    },
    {
      "lang": "en",
      "text": "gives you basically the derivatives for",
      "offset": 6023.88,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "all the X hats and so now try to go",
      "offset": 6025.38,
      "duration": 5.819
    },
    {
      "lang": "en",
      "text": "through this computational graph and",
      "offset": 6028.92,
      "duration": 6.42
    },
    {
      "lang": "en",
      "text": "derive what is DL by D Sigma Square",
      "offset": 6031.199,
      "duration": 6.901
    },
    {
      "lang": "en",
      "text": "and then what is DL by B mu and then one",
      "offset": 6035.34,
      "duration": 4.5
    },
    {
      "lang": "en",
      "text": "is D L by DX",
      "offset": 6038.1,
      "duration": 4.2
    },
    {
      "lang": "en",
      "text": "eventually so give it a go and I'm going",
      "offset": 6039.84,
      "duration": 4.26
    },
    {
      "lang": "en",
      "text": "to be revealing the answer one piece at",
      "offset": 6042.3,
      "duration": 4.5
    },
    {
      "lang": "en",
      "text": "a time okay so to get DL by D Sigma",
      "offset": 6044.1,
      "duration": 4.619
    },
    {
      "lang": "en",
      "text": "Square we have to remember again like I",
      "offset": 6046.8,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "mentioned that there are many excess X",
      "offset": 6048.719,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "hats here",
      "offset": 6051.12,
      "duration": 3.119
    },
    {
      "lang": "en",
      "text": "and remember that Sigma square is just a",
      "offset": 6052.32,
      "duration": 3.54
    },
    {
      "lang": "en",
      "text": "single individual number here",
      "offset": 6054.239,
      "duration": 5.341
    },
    {
      "lang": "en",
      "text": "so when we look at the expression",
      "offset": 6055.86,
      "duration": 5.94
    },
    {
      "lang": "en",
      "text": "for the L by D Sigma Square",
      "offset": 6059.58,
      "duration": 3.48
    },
    {
      "lang": "en",
      "text": "we have that we have to actually",
      "offset": 6061.8,
      "duration": 6.3
    },
    {
      "lang": "en",
      "text": "consider all the possible paths that um",
      "offset": 6063.06,
      "duration": 7.619
    },
    {
      "lang": "en",
      "text": "we basically have that there's many X",
      "offset": 6068.1,
      "duration": 4.98
    },
    {
      "lang": "en",
      "text": "hats and they all feed off from they all",
      "offset": 6070.679,
      "duration": 4.741
    },
    {
      "lang": "en",
      "text": "depend on Sigma Square so Sigma square",
      "offset": 6073.08,
      "duration": 4.139
    },
    {
      "lang": "en",
      "text": "has a large fan out there's lots of",
      "offset": 6075.42,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "arrows coming out from Sigma square into",
      "offset": 6077.219,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "all the X hats",
      "offset": 6079.02,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "and then there's a back propagating",
      "offset": 6080.58,
      "duration": 4.26
    },
    {
      "lang": "en",
      "text": "signal from each X hat into Sigma square",
      "offset": 6082.38,
      "duration": 4.44
    },
    {
      "lang": "en",
      "text": "and that's why we actually need to sum",
      "offset": 6084.84,
      "duration": 5.1
    },
    {
      "lang": "en",
      "text": "over all those I's from I equal to 1 to",
      "offset": 6086.82,
      "duration": 4.02
    },
    {
      "lang": "en",
      "text": "m",
      "offset": 6089.94,
      "duration": 5.64
    },
    {
      "lang": "en",
      "text": "of the DL by d x i hat which is the",
      "offset": 6090.84,
      "duration": 6.12
    },
    {
      "lang": "en",
      "text": "global gradient",
      "offset": 6095.58,
      "duration": 4.5
    },
    {
      "lang": "en",
      "text": "times the x i Hat by D Sigma Square",
      "offset": 6096.96,
      "duration": 5.159
    },
    {
      "lang": "en",
      "text": "which is the local gradient",
      "offset": 6100.08,
      "duration": 4.5
    },
    {
      "lang": "en",
      "text": "of this operation here",
      "offset": 6102.119,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "and then mathematically I'm just working",
      "offset": 6104.58,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "it out here and I'm simplifying and you",
      "offset": 6106.44,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "get a certain expression for DL by D",
      "offset": 6108.42,
      "duration": 4.199
    },
    {
      "lang": "en",
      "text": "Sigma square and we're going to be using",
      "offset": 6111,
      "duration": 2.94
    },
    {
      "lang": "en",
      "text": "this expression when we back propagate",
      "offset": 6112.619,
      "duration": 4.141
    },
    {
      "lang": "en",
      "text": "into mu and then eventually into X so",
      "offset": 6113.94,
      "duration": 4.14
    },
    {
      "lang": "en",
      "text": "now let's continue our back propagation",
      "offset": 6116.76,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "into mu so what is D L by D mu now again",
      "offset": 6118.08,
      "duration": 6.659
    },
    {
      "lang": "en",
      "text": "be careful that mu influences X hat and",
      "offset": 6121.8,
      "duration": 5.399
    },
    {
      "lang": "en",
      "text": "X hat is actually lots of values so for",
      "offset": 6124.739,
      "duration": 4.681
    },
    {
      "lang": "en",
      "text": "example if our mini batch size is 32 as",
      "offset": 6127.199,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "it is in our example that we were",
      "offset": 6129.42,
      "duration": 4.02
    },
    {
      "lang": "en",
      "text": "working on then this is 32 numbers and",
      "offset": 6130.8,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "32 arrows going back to mu and then mu",
      "offset": 6133.44,
      "duration": 4.62
    },
    {
      "lang": "en",
      "text": "going to Sigma square is just a single",
      "offset": 6136.56,
      "duration": 3.059
    },
    {
      "lang": "en",
      "text": "Arrow because Sigma square is a scalar",
      "offset": 6138.06,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "so in total there are 33 arrows",
      "offset": 6139.619,
      "duration": 5.58
    },
    {
      "lang": "en",
      "text": "emanating from you and then all of them",
      "offset": 6142.38,
      "duration": 5.1
    },
    {
      "lang": "en",
      "text": "have gradients coming into mu and they",
      "offset": 6145.199,
      "duration": 4.141
    },
    {
      "lang": "en",
      "text": "all need to be summed up",
      "offset": 6147.48,
      "duration": 4.259
    },
    {
      "lang": "en",
      "text": "and so that's why when we look at the",
      "offset": 6149.34,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "expression for DL by D mu I am summing",
      "offset": 6151.739,
      "duration": 5.701
    },
    {
      "lang": "en",
      "text": "up over all the gradients of DL by d x i",
      "offset": 6154.38,
      "duration": 5.94
    },
    {
      "lang": "en",
      "text": "hat times the x i Hat by being mu",
      "offset": 6157.44,
      "duration": 5.58
    },
    {
      "lang": "en",
      "text": "uh so that's the that's this arrow and",
      "offset": 6160.32,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "that's 32 arrows here and then plus the",
      "offset": 6163.02,
      "duration": 4.199
    },
    {
      "lang": "en",
      "text": "one Arrow from here which is the L by",
      "offset": 6165.36,
      "duration": 4.2
    },
    {
      "lang": "en",
      "text": "the sigma Square Times the sigma squared",
      "offset": 6167.219,
      "duration": 3.301
    },
    {
      "lang": "en",
      "text": "by D mu",
      "offset": 6169.56,
      "duration": 2.76
    },
    {
      "lang": "en",
      "text": "so now we have to work out that",
      "offset": 6170.52,
      "duration": 3.48
    },
    {
      "lang": "en",
      "text": "expression and let me just reveal the",
      "offset": 6172.32,
      "duration": 2.94
    },
    {
      "lang": "en",
      "text": "rest of it",
      "offset": 6174,
      "duration": 4.139
    },
    {
      "lang": "en",
      "text": "uh simplifying here is not complicated",
      "offset": 6175.26,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "the first term and you just get an",
      "offset": 6178.139,
      "duration": 3.06
    },
    {
      "lang": "en",
      "text": "expression here",
      "offset": 6180.06,
      "duration": 2.34
    },
    {
      "lang": "en",
      "text": "for the second term though there's",
      "offset": 6181.199,
      "duration": 2.161
    },
    {
      "lang": "en",
      "text": "something really interesting that",
      "offset": 6182.4,
      "duration": 1.98
    },
    {
      "lang": "en",
      "text": "happens",
      "offset": 6183.36,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "when we look at the sigma squared by D",
      "offset": 6184.38,
      "duration": 4.04
    },
    {
      "lang": "en",
      "text": "mu and we simplify",
      "offset": 6186.36,
      "duration": 5.4
    },
    {
      "lang": "en",
      "text": "at one point if we assume that in a",
      "offset": 6188.42,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "special case where mu is actually the",
      "offset": 6191.76,
      "duration": 5.359
    },
    {
      "lang": "en",
      "text": "average of X I's as it is in this case",
      "offset": 6194.34,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "then if we plug that in then actually",
      "offset": 6197.119,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "the gradient vanishes and becomes",
      "offset": 6200.34,
      "duration": 4.44
    },
    {
      "lang": "en",
      "text": "exactly zero and that makes the entire",
      "offset": 6202.08,
      "duration": 4.38
    },
    {
      "lang": "en",
      "text": "second term cancel",
      "offset": 6204.78,
      "duration": 4.62
    },
    {
      "lang": "en",
      "text": "and so these uh if you just have a",
      "offset": 6206.46,
      "duration": 4.199
    },
    {
      "lang": "en",
      "text": "mathematical expression like this and",
      "offset": 6209.4,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "you look at D Sigma Square by D mu you",
      "offset": 6210.659,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "would get some mathematical formula for",
      "offset": 6213,
      "duration": 4.679
    },
    {
      "lang": "en",
      "text": "how mu impacts Sigma Square",
      "offset": 6215.699,
      "duration": 4.201
    },
    {
      "lang": "en",
      "text": "but if it is the special case that Nu is",
      "offset": 6217.679,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "actually equal to the average as it is",
      "offset": 6219.9,
      "duration": 3.9
    },
    {
      "lang": "en",
      "text": "in the case of pastoralization that",
      "offset": 6222,
      "duration": 3.3
    },
    {
      "lang": "en",
      "text": "gradient will actually vanish and become",
      "offset": 6223.8,
      "duration": 4.2
    },
    {
      "lang": "en",
      "text": "zero so the whole term cancels and we",
      "offset": 6225.3,
      "duration": 3.98
    },
    {
      "lang": "en",
      "text": "just get a fairly straightforward",
      "offset": 6228,
      "duration": 4.86
    },
    {
      "lang": "en",
      "text": "expression here for DL by D mu okay and",
      "offset": 6229.28,
      "duration": 5.439
    },
    {
      "lang": "en",
      "text": "now we get to the craziest part which is",
      "offset": 6232.86,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "uh deriving DL by dxi which is",
      "offset": 6234.719,
      "duration": 4.381
    },
    {
      "lang": "en",
      "text": "ultimately what we're after",
      "offset": 6237.659,
      "duration": 3.301
    },
    {
      "lang": "en",
      "text": "now let's count",
      "offset": 6239.1,
      "duration": 4.26
    },
    {
      "lang": "en",
      "text": "first of all how many numbers are there",
      "offset": 6240.96,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "inside X as I mentioned there are 32",
      "offset": 6243.36,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "numbers there are 32 Little X I's and",
      "offset": 6245.28,
      "duration": 3.899
    },
    {
      "lang": "en",
      "text": "let's count the number of arrows",
      "offset": 6248.159,
      "duration": 2.881
    },
    {
      "lang": "en",
      "text": "emanating from each x i",
      "offset": 6249.179,
      "duration": 4.141
    },
    {
      "lang": "en",
      "text": "there's an arrow going to Mu an arrow",
      "offset": 6251.04,
      "duration": 3.72
    },
    {
      "lang": "en",
      "text": "going to Sigma Square",
      "offset": 6253.32,
      "duration": 3.299
    },
    {
      "lang": "en",
      "text": "and then there's an arrow going to X hat",
      "offset": 6254.76,
      "duration": 4.439
    },
    {
      "lang": "en",
      "text": "but this Arrow here let's scrutinize",
      "offset": 6256.619,
      "duration": 3.661
    },
    {
      "lang": "en",
      "text": "that a little bit",
      "offset": 6259.199,
      "duration": 4.261
    },
    {
      "lang": "en",
      "text": "each x i hat is just a function of x i",
      "offset": 6260.28,
      "duration": 7.02
    },
    {
      "lang": "en",
      "text": "and all the other scalars so x i hat",
      "offset": 6263.46,
      "duration": 5.699
    },
    {
      "lang": "en",
      "text": "only depends on x i and none of the",
      "offset": 6267.3,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "other X's",
      "offset": 6269.159,
      "duration": 3.48
    },
    {
      "lang": "en",
      "text": "and so therefore there are actually in",
      "offset": 6270.3,
      "duration": 4.439
    },
    {
      "lang": "en",
      "text": "this single Arrow there are 32 arrows",
      "offset": 6272.639,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "but those 32 arrows are going exactly",
      "offset": 6274.739,
      "duration": 4.261
    },
    {
      "lang": "en",
      "text": "parallel they don't interfere and",
      "offset": 6277.199,
      "duration": 3.721
    },
    {
      "lang": "en",
      "text": "they're just going parallel between x",
      "offset": 6279,
      "duration": 3.96
    },
    {
      "lang": "en",
      "text": "and x hat you can look at it that way",
      "offset": 6280.92,
      "duration": 3.54
    },
    {
      "lang": "en",
      "text": "and so how many arrows are emanating",
      "offset": 6282.96,
      "duration": 4.199
    },
    {
      "lang": "en",
      "text": "from each x i there are three arrows mu",
      "offset": 6284.46,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "Sigma squared and the associated X hat",
      "offset": 6287.159,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "and so in back propagation we now need",
      "offset": 6290.94,
      "duration": 4.62
    },
    {
      "lang": "en",
      "text": "to apply the chain rule and we need to",
      "offset": 6293.159,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "add up those three contributions",
      "offset": 6295.56,
      "duration": 4.26
    },
    {
      "lang": "en",
      "text": "so here's what that looks like if I just",
      "offset": 6297.96,
      "duration": 4.14
    },
    {
      "lang": "en",
      "text": "write that out",
      "offset": 6299.82,
      "duration": 4.859
    },
    {
      "lang": "en",
      "text": "we have uh we're going through we're",
      "offset": 6302.1,
      "duration": 4.68
    },
    {
      "lang": "en",
      "text": "chaining through mu Sigma square and",
      "offset": 6304.679,
      "duration": 4.5
    },
    {
      "lang": "en",
      "text": "through X hat and those three terms are",
      "offset": 6306.78,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "just here",
      "offset": 6309.179,
      "duration": 4.201
    },
    {
      "lang": "en",
      "text": "now we already have three of these we",
      "offset": 6310.38,
      "duration": 5.1
    },
    {
      "lang": "en",
      "text": "have d l by d x i hat",
      "offset": 6313.38,
      "duration": 4.5
    },
    {
      "lang": "en",
      "text": "we have DL by D mu which we derived here",
      "offset": 6315.48,
      "duration": 4.44
    },
    {
      "lang": "en",
      "text": "and we have DL by D Sigma Square which",
      "offset": 6317.88,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "we derived here but we need three other",
      "offset": 6319.92,
      "duration": 3.719
    },
    {
      "lang": "en",
      "text": "terms here",
      "offset": 6322.44,
      "duration": 4.38
    },
    {
      "lang": "en",
      "text": "the this one this one and this one so I",
      "offset": 6323.639,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "invite you to try to derive them it's",
      "offset": 6326.82,
      "duration": 3.06
    },
    {
      "lang": "en",
      "text": "not that complicated you're just looking",
      "offset": 6328.44,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "at these Expressions here and",
      "offset": 6329.88,
      "duration": 5.1
    },
    {
      "lang": "en",
      "text": "differentiating with respect to x i",
      "offset": 6331.56,
      "duration": 7.34
    },
    {
      "lang": "en",
      "text": "so give it a shot but here's the result",
      "offset": 6334.98,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "or at least what I got",
      "offset": 6339.42,
      "duration": 3.42
    },
    {
      "lang": "en",
      "text": "um",
      "offset": 6341.699,
      "duration": 2.401
    },
    {
      "lang": "en",
      "text": "yeah I'm just I'm just differentiating",
      "offset": 6342.84,
      "duration": 2.7
    },
    {
      "lang": "en",
      "text": "with respect to x i for all these",
      "offset": 6344.1,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "expressions and honestly I don't think",
      "offset": 6345.54,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "there's anything too tricky here it's",
      "offset": 6347.1,
      "duration": 3.42
    },
    {
      "lang": "en",
      "text": "basic calculus",
      "offset": 6348.42,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "now it gets a little bit more tricky is",
      "offset": 6350.52,
      "duration": 3.179
    },
    {
      "lang": "en",
      "text": "we are now going to plug everything",
      "offset": 6352.26,
      "duration": 3.54
    },
    {
      "lang": "en",
      "text": "together so all of these terms",
      "offset": 6353.699,
      "duration": 3.721
    },
    {
      "lang": "en",
      "text": "multiplied with all of these terms and",
      "offset": 6355.8,
      "duration": 3.78
    },
    {
      "lang": "en",
      "text": "add it up according to this formula and",
      "offset": 6357.42,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "that gets a little bit hairy so what",
      "offset": 6359.58,
      "duration": 4.5
    },
    {
      "lang": "en",
      "text": "ends up happening is",
      "offset": 6361.5,
      "duration": 3.78
    },
    {
      "lang": "en",
      "text": "uh",
      "offset": 6364.08,
      "duration": 4.38
    },
    {
      "lang": "en",
      "text": "you get a large expression and the thing",
      "offset": 6365.28,
      "duration": 4.62
    },
    {
      "lang": "en",
      "text": "to be very careful with here of course",
      "offset": 6368.46,
      "duration": 4.02
    },
    {
      "lang": "en",
      "text": "is we are working with a DL by dxi for",
      "offset": 6369.9,
      "duration": 5.819
    },
    {
      "lang": "en",
      "text": "specific I here but when we are plugging",
      "offset": 6372.48,
      "duration": 4.679
    },
    {
      "lang": "en",
      "text": "in some of these terms",
      "offset": 6375.719,
      "duration": 3.121
    },
    {
      "lang": "en",
      "text": "like say",
      "offset": 6377.159,
      "duration": 2.341
    },
    {
      "lang": "en",
      "text": "um",
      "offset": 6378.84,
      "duration": 3.42
    },
    {
      "lang": "en",
      "text": "this term here deal by D signal squared",
      "offset": 6379.5,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "you see how the L by D Sigma squared I",
      "offset": 6382.26,
      "duration": 4.02
    },
    {
      "lang": "en",
      "text": "end up with an expression and I'm",
      "offset": 6384.78,
      "duration": 4.439
    },
    {
      "lang": "en",
      "text": "iterating over little I's here but I",
      "offset": 6386.28,
      "duration": 5.58
    },
    {
      "lang": "en",
      "text": "can't use I as the variable when I plug",
      "offset": 6389.219,
      "duration": 4.681
    },
    {
      "lang": "en",
      "text": "in here because this is a different I",
      "offset": 6391.86,
      "duration": 3.299
    },
    {
      "lang": "en",
      "text": "from this eye",
      "offset": 6393.9,
      "duration": 3.48
    },
    {
      "lang": "en",
      "text": "this I here is just a place or like a",
      "offset": 6395.159,
      "duration": 4.261
    },
    {
      "lang": "en",
      "text": "local variable for for a for Loop in",
      "offset": 6397.38,
      "duration": 4.5
    },
    {
      "lang": "en",
      "text": "here so here when I plug that in you",
      "offset": 6399.42,
      "duration": 4.5
    },
    {
      "lang": "en",
      "text": "notice that I rename the I to a j",
      "offset": 6401.88,
      "duration": 3.779
    },
    {
      "lang": "en",
      "text": "because I need to make sure that this J",
      "offset": 6403.92,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "is not that this J is not this I this J",
      "offset": 6405.659,
      "duration": 4.741
    },
    {
      "lang": "en",
      "text": "is like like a little local iterator",
      "offset": 6408.719,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "over 32 terms and so you have to be",
      "offset": 6410.4,
      "duration": 3.96
    },
    {
      "lang": "en",
      "text": "careful with that when you're plugging",
      "offset": 6413.28,
      "duration": 2.82
    },
    {
      "lang": "en",
      "text": "in the expressions from here to here you",
      "offset": 6414.36,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "may have to rename eyes into J's and you",
      "offset": 6416.1,
      "duration": 4.019
    },
    {
      "lang": "en",
      "text": "have to be very careful what is actually",
      "offset": 6418.44,
      "duration": 5.759
    },
    {
      "lang": "en",
      "text": "an I with respect to the L by t x i",
      "offset": 6420.119,
      "duration": 7.08
    },
    {
      "lang": "en",
      "text": "so some of these are J's some of these",
      "offset": 6424.199,
      "duration": 4.5
    },
    {
      "lang": "en",
      "text": "are I's",
      "offset": 6427.199,
      "duration": 4.261
    },
    {
      "lang": "en",
      "text": "and then we simplify this expression",
      "offset": 6428.699,
      "duration": 5.101
    },
    {
      "lang": "en",
      "text": "and I guess like the big thing to notice",
      "offset": 6431.46,
      "duration": 4.38
    },
    {
      "lang": "en",
      "text": "here is a bunch of terms just kind of",
      "offset": 6433.8,
      "duration": 3.06
    },
    {
      "lang": "en",
      "text": "come out to the front and you can",
      "offset": 6435.84,
      "duration": 2.879
    },
    {
      "lang": "en",
      "text": "refactor them there's a sigma squared",
      "offset": 6436.86,
      "duration": 3.06
    },
    {
      "lang": "en",
      "text": "plus Epsilon raised to the power of",
      "offset": 6438.719,
      "duration": 3.241
    },
    {
      "lang": "en",
      "text": "negative three over two uh this Sigma",
      "offset": 6439.92,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "squared plus Epsilon can be actually",
      "offset": 6441.96,
      "duration": 3.96
    },
    {
      "lang": "en",
      "text": "separated out into three terms each of",
      "offset": 6443.76,
      "duration": 4.62
    },
    {
      "lang": "en",
      "text": "them are Sigma squared plus Epsilon to",
      "offset": 6445.92,
      "duration": 4.38
    },
    {
      "lang": "en",
      "text": "the negative one over two so the three",
      "offset": 6448.38,
      "duration": 5.279
    },
    {
      "lang": "en",
      "text": "of them multiplied is equal to this and",
      "offset": 6450.3,
      "duration": 4.859
    },
    {
      "lang": "en",
      "text": "then those three terms can go different",
      "offset": 6453.659,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "places because of the multiplication so",
      "offset": 6455.159,
      "duration": 3.901
    },
    {
      "lang": "en",
      "text": "one of them actually comes out to the",
      "offset": 6457.739,
      "duration": 4.861
    },
    {
      "lang": "en",
      "text": "front and will end up here outside one",
      "offset": 6459.06,
      "duration": 6.36
    },
    {
      "lang": "en",
      "text": "of them joins up with this term and one",
      "offset": 6462.6,
      "duration": 5.099
    },
    {
      "lang": "en",
      "text": "of them joins up with this other term",
      "offset": 6465.42,
      "duration": 3.66
    },
    {
      "lang": "en",
      "text": "and then when you simplify the",
      "offset": 6467.699,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "expression you'll notice that some of",
      "offset": 6469.08,
      "duration": 3.3
    },
    {
      "lang": "en",
      "text": "these terms that are coming out are just",
      "offset": 6471.06,
      "duration": 2.94
    },
    {
      "lang": "en",
      "text": "the x i hats",
      "offset": 6472.38,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "so you can simplify just by rewriting",
      "offset": 6474,
      "duration": 3.119
    },
    {
      "lang": "en",
      "text": "that",
      "offset": 6476.219,
      "duration": 2.761
    },
    {
      "lang": "en",
      "text": "and what we end up with at the end is a",
      "offset": 6477.119,
      "duration": 3.241
    },
    {
      "lang": "en",
      "text": "fairly simple mathematical expression",
      "offset": 6478.98,
      "duration": 3.3
    },
    {
      "lang": "en",
      "text": "over here that I cannot simplify further",
      "offset": 6480.36,
      "duration": 4.74
    },
    {
      "lang": "en",
      "text": "but basically you'll notice that it only",
      "offset": 6482.28,
      "duration": 4.68
    },
    {
      "lang": "en",
      "text": "uses the stuff we have and it derives",
      "offset": 6485.1,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "the thing we need so we have the L by d",
      "offset": 6486.96,
      "duration": 6.06
    },
    {
      "lang": "en",
      "text": "y for all the I's and those are used",
      "offset": 6490.38,
      "duration": 5.339
    },
    {
      "lang": "en",
      "text": "plenty of times here and also in",
      "offset": 6493.02,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "addition what we're using is these x i",
      "offset": 6495.719,
      "duration": 3.541
    },
    {
      "lang": "en",
      "text": "hats and XJ hats and they just come from",
      "offset": 6497.34,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "the forward pass",
      "offset": 6499.26,
      "duration": 3.66
    },
    {
      "lang": "en",
      "text": "and otherwise this is a simple",
      "offset": 6500.94,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "expression and it gives us DL by d x i",
      "offset": 6502.92,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "for all the I's and that's ultimately",
      "offset": 6505.739,
      "duration": 3.541
    },
    {
      "lang": "en",
      "text": "what we're interested in",
      "offset": 6507.96,
      "duration": 4.86
    },
    {
      "lang": "en",
      "text": "so that's the end of Bachelor backward",
      "offset": 6509.28,
      "duration": 5.7
    },
    {
      "lang": "en",
      "text": "pass analytically let's now implement",
      "offset": 6512.82,
      "duration": 4.02
    },
    {
      "lang": "en",
      "text": "this final result",
      "offset": 6514.98,
      "duration": 3.48
    },
    {
      "lang": "en",
      "text": "okay so I implemented the expression",
      "offset": 6516.84,
      "duration": 4.74
    },
    {
      "lang": "en",
      "text": "into a single line of code here and you",
      "offset": 6518.46,
      "duration": 5.1
    },
    {
      "lang": "en",
      "text": "can see that the max diff is Tiny so",
      "offset": 6521.58,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "this is the correct implementation of",
      "offset": 6523.56,
      "duration": 4.92
    },
    {
      "lang": "en",
      "text": "this formula now I'll just uh",
      "offset": 6524.94,
      "duration": 5.699
    },
    {
      "lang": "en",
      "text": "basically tell you that getting this",
      "offset": 6528.48,
      "duration": 3.9
    },
    {
      "lang": "en",
      "text": "formula here from this mathematical",
      "offset": 6530.639,
      "duration": 4.02
    },
    {
      "lang": "en",
      "text": "expression was not trivial and there's a",
      "offset": 6532.38,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "lot going on packed into this one",
      "offset": 6534.659,
      "duration": 3.721
    },
    {
      "lang": "en",
      "text": "formula and this is a whole exercise by",
      "offset": 6536.219,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "itself because you have to consider the",
      "offset": 6538.38,
      "duration": 4.68
    },
    {
      "lang": "en",
      "text": "fact that this formula here is just for",
      "offset": 6540.78,
      "duration": 4.439
    },
    {
      "lang": "en",
      "text": "a single neuron and a batch of 32",
      "offset": 6543.06,
      "duration": 4.38
    },
    {
      "lang": "en",
      "text": "examples but what I'm doing here is I'm",
      "offset": 6545.219,
      "duration": 4.92
    },
    {
      "lang": "en",
      "text": "actually we actually have 64 neurons and",
      "offset": 6547.44,
      "duration": 4.44
    },
    {
      "lang": "en",
      "text": "so this expression has to in parallel",
      "offset": 6550.139,
      "duration": 4.141
    },
    {
      "lang": "en",
      "text": "evaluate the bathroom backward pass for",
      "offset": 6551.88,
      "duration": 4.259
    },
    {
      "lang": "en",
      "text": "all of those 64 neurons in parallel",
      "offset": 6554.28,
      "duration": 4.14
    },
    {
      "lang": "en",
      "text": "independently so this has to happen",
      "offset": 6556.139,
      "duration": 4.141
    },
    {
      "lang": "en",
      "text": "basically in every single",
      "offset": 6558.42,
      "duration": 2.4
    },
    {
      "lang": "en",
      "text": "um",
      "offset": 6560.28,
      "duration": 4.14
    },
    {
      "lang": "en",
      "text": "column of the inputs here",
      "offset": 6560.82,
      "duration": 5.64
    },
    {
      "lang": "en",
      "text": "and in addition to that you see how",
      "offset": 6564.42,
      "duration": 3.779
    },
    {
      "lang": "en",
      "text": "there are a bunch of sums here and we",
      "offset": 6566.46,
      "duration": 2.94
    },
    {
      "lang": "en",
      "text": "need to make sure that when I do those",
      "offset": 6568.199,
      "duration": 3.42
    },
    {
      "lang": "en",
      "text": "sums that they broadcast correctly onto",
      "offset": 6569.4,
      "duration": 3.96
    },
    {
      "lang": "en",
      "text": "everything else that's here",
      "offset": 6571.619,
      "duration": 3.661
    },
    {
      "lang": "en",
      "text": "and so getting this expression is just",
      "offset": 6573.36,
      "duration": 3.42
    },
    {
      "lang": "en",
      "text": "like highly non-trivial and I invite you",
      "offset": 6575.28,
      "duration": 2.7
    },
    {
      "lang": "en",
      "text": "to basically look through it and step",
      "offset": 6576.78,
      "duration": 2.58
    },
    {
      "lang": "en",
      "text": "through it and it's a whole exercise to",
      "offset": 6577.98,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "make sure that this this checks out but",
      "offset": 6579.36,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "once all the shapes are green and once",
      "offset": 6583.1,
      "duration": 3.46
    },
    {
      "lang": "en",
      "text": "you convince yourself that it's correct",
      "offset": 6585.36,
      "duration": 3.42
    },
    {
      "lang": "en",
      "text": "you can also verify that Patrick's gets",
      "offset": 6586.56,
      "duration": 4.02
    },
    {
      "lang": "en",
      "text": "the exact same answer as well and so",
      "offset": 6588.78,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "that gives you a lot of peace of mind",
      "offset": 6590.58,
      "duration": 3.3
    },
    {
      "lang": "en",
      "text": "that this mathematical formula is",
      "offset": 6591.9,
      "duration": 3.96
    },
    {
      "lang": "en",
      "text": "correctly implemented here and",
      "offset": 6593.88,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "broadcasted correctly and replicated in",
      "offset": 6595.86,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "parallel for all of the 64 neurons",
      "offset": 6597.96,
      "duration": 5.58
    },
    {
      "lang": "en",
      "text": "inside this bastrum layer okay and",
      "offset": 6600.5,
      "duration": 5.26
    },
    {
      "lang": "en",
      "text": "finally exercise number four asks you to",
      "offset": 6603.54,
      "duration": 4.74
    },
    {
      "lang": "en",
      "text": "put it all together and uh here we have",
      "offset": 6605.76,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "a redefinition of the entire problem so",
      "offset": 6608.28,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "you see that we reinitialize the neural",
      "offset": 6610.32,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "nut from scratch and everything and then",
      "offset": 6611.88,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "here instead of calling loss that",
      "offset": 6613.92,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "backward we want to have the manual back",
      "offset": 6615.96,
      "duration": 4.14
    },
    {
      "lang": "en",
      "text": "propagation here as we derived It Up",
      "offset": 6618.239,
      "duration": 4.861
    },
    {
      "lang": "en",
      "text": "Above so go up copy paste all the chunks",
      "offset": 6620.1,
      "duration": 4.98
    },
    {
      "lang": "en",
      "text": "of code that we've already derived put",
      "offset": 6623.1,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "them here and drive your own gradients",
      "offset": 6625.08,
      "duration": 3.72
    },
    {
      "lang": "en",
      "text": "and then optimize this neural nut",
      "offset": 6626.94,
      "duration": 4.199
    },
    {
      "lang": "en",
      "text": "basically using your own gradients all",
      "offset": 6628.8,
      "duration": 4.26
    },
    {
      "lang": "en",
      "text": "the way to the calibration of The",
      "offset": 6631.139,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "Bachelor and the evaluation of the loss",
      "offset": 6633.06,
      "duration": 3.78
    },
    {
      "lang": "en",
      "text": "and I was able to achieve quite a good",
      "offset": 6634.98,
      "duration": 3.42
    },
    {
      "lang": "en",
      "text": "loss basically the same loss you would",
      "offset": 6636.84,
      "duration": 3.72
    },
    {
      "lang": "en",
      "text": "achieve before and that shouldn't be",
      "offset": 6638.4,
      "duration": 3.54
    },
    {
      "lang": "en",
      "text": "surprising because all we've done is",
      "offset": 6640.56,
      "duration": 3.78
    },
    {
      "lang": "en",
      "text": "we've really gotten to Lost That",
      "offset": 6641.94,
      "duration": 3.9
    },
    {
      "lang": "en",
      "text": "backward and we've pulled out all the",
      "offset": 6644.34,
      "duration": 2.339
    },
    {
      "lang": "en",
      "text": "code",
      "offset": 6645.84,
      "duration": 3.299
    },
    {
      "lang": "en",
      "text": "and inserted it here but those gradients",
      "offset": 6646.679,
      "duration": 3.781
    },
    {
      "lang": "en",
      "text": "are identical and everything is",
      "offset": 6649.139,
      "duration": 3.181
    },
    {
      "lang": "en",
      "text": "identical and the results are identical",
      "offset": 6650.46,
      "duration": 4.02
    },
    {
      "lang": "en",
      "text": "it's just that we have full visibility",
      "offset": 6652.32,
      "duration": 4.14
    },
    {
      "lang": "en",
      "text": "on exactly what goes on under the hood",
      "offset": 6654.48,
      "duration": 4.02
    },
    {
      "lang": "en",
      "text": "I'll plot that backward in this specific",
      "offset": 6656.46,
      "duration": 5.58
    },
    {
      "lang": "en",
      "text": "case and this is all of our code this is",
      "offset": 6658.5,
      "duration": 5.82
    },
    {
      "lang": "en",
      "text": "the full backward pass using basically",
      "offset": 6662.04,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "the simplified backward pass for the",
      "offset": 6664.32,
      "duration": 3.899
    },
    {
      "lang": "en",
      "text": "cross entropy loss and the mass",
      "offset": 6666.119,
      "duration": 4.261
    },
    {
      "lang": "en",
      "text": "generalization so back propagating",
      "offset": 6668.219,
      "duration": 4.98
    },
    {
      "lang": "en",
      "text": "through cross entropy the second layer",
      "offset": 6670.38,
      "duration": 5.22
    },
    {
      "lang": "en",
      "text": "the 10 H nonlinearity the batch",
      "offset": 6673.199,
      "duration": 3.781
    },
    {
      "lang": "en",
      "text": "normalization",
      "offset": 6675.6,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "uh through the first layer and through",
      "offset": 6676.98,
      "duration": 4.199
    },
    {
      "lang": "en",
      "text": "the embedding and so you see that this",
      "offset": 6679.199,
      "duration": 4.02
    },
    {
      "lang": "en",
      "text": "is only maybe what is this 20 lines of",
      "offset": 6681.179,
      "duration": 3.96
    },
    {
      "lang": "en",
      "text": "code or something like that and that's",
      "offset": 6683.219,
      "duration": 4.261
    },
    {
      "lang": "en",
      "text": "what gives us gradients and now we can",
      "offset": 6685.139,
      "duration": 4.921
    },
    {
      "lang": "en",
      "text": "potentially erase losses backward so the",
      "offset": 6687.48,
      "duration": 4.5
    },
    {
      "lang": "en",
      "text": "way I have the code set up is you should",
      "offset": 6690.06,
      "duration": 3.78
    },
    {
      "lang": "en",
      "text": "be able to run this entire cell once you",
      "offset": 6691.98,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "fill this in and this will run for only",
      "offset": 6693.84,
      "duration": 4.14
    },
    {
      "lang": "en",
      "text": "100 iterations and then break",
      "offset": 6696.06,
      "duration": 3.48
    },
    {
      "lang": "en",
      "text": "and it breaks because it gives you an",
      "offset": 6697.98,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "opportunity to check your gradients",
      "offset": 6699.54,
      "duration": 3.54
    },
    {
      "lang": "en",
      "text": "against pytorch",
      "offset": 6701.1,
      "duration": 5.22
    },
    {
      "lang": "en",
      "text": "so here our gradients we see are not",
      "offset": 6703.08,
      "duration": 5.88
    },
    {
      "lang": "en",
      "text": "exactly equal they are approximately",
      "offset": 6706.32,
      "duration": 4.68
    },
    {
      "lang": "en",
      "text": "equal and the differences are tiny",
      "offset": 6708.96,
      "duration": 4.02
    },
    {
      "lang": "en",
      "text": "wanting negative 9 or so and I don't",
      "offset": 6711,
      "duration": 3.239
    },
    {
      "lang": "en",
      "text": "exactly know where they're coming from",
      "offset": 6712.98,
      "duration": 3.06
    },
    {
      "lang": "en",
      "text": "to be honest",
      "offset": 6714.239,
      "duration": 3.061
    },
    {
      "lang": "en",
      "text": "um so once we have some confidence that",
      "offset": 6716.04,
      "duration": 3.42
    },
    {
      "lang": "en",
      "text": "the gradients are basically correct we",
      "offset": 6717.3,
      "duration": 4.02
    },
    {
      "lang": "en",
      "text": "can take out the gradient tracking",
      "offset": 6719.46,
      "duration": 6.3
    },
    {
      "lang": "en",
      "text": "we can disable this breaking statement",
      "offset": 6721.32,
      "duration": 6.419
    },
    {
      "lang": "en",
      "text": "and then we can",
      "offset": 6725.76,
      "duration": 4.859
    },
    {
      "lang": "en",
      "text": "basically disable lost of backward we",
      "offset": 6727.739,
      "duration": 5.46
    },
    {
      "lang": "en",
      "text": "don't need it anymore it feels amazing",
      "offset": 6730.619,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "to say that",
      "offset": 6733.199,
      "duration": 3.48
    },
    {
      "lang": "en",
      "text": "and then here when we are doing the",
      "offset": 6734.46,
      "duration": 3.96
    },
    {
      "lang": "en",
      "text": "update we're not going to use P dot grad",
      "offset": 6736.679,
      "duration": 5.101
    },
    {
      "lang": "en",
      "text": "this is the old way of pytorch we don't",
      "offset": 6738.42,
      "duration": 4.5
    },
    {
      "lang": "en",
      "text": "have that anymore because we're not",
      "offset": 6741.78,
      "duration": 3.66
    },
    {
      "lang": "en",
      "text": "doing backward we are going to use this",
      "offset": 6742.92,
      "duration": 4.739
    },
    {
      "lang": "en",
      "text": "update where we you see that I'm",
      "offset": 6745.44,
      "duration": 3.66
    },
    {
      "lang": "en",
      "text": "iterating over",
      "offset": 6747.659,
      "duration": 3.241
    },
    {
      "lang": "en",
      "text": "I've arranged the grads to be in the",
      "offset": 6749.1,
      "duration": 3.48
    },
    {
      "lang": "en",
      "text": "same order as the parameters and I'm",
      "offset": 6750.9,
      "duration": 3.779
    },
    {
      "lang": "en",
      "text": "zipping them up the gradients and the",
      "offset": 6752.58,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "parameters into p and grad and then here",
      "offset": 6754.679,
      "duration": 4.141
    },
    {
      "lang": "en",
      "text": "I'm going to step with just the grad",
      "offset": 6757.139,
      "duration": 3.721
    },
    {
      "lang": "en",
      "text": "that we derived manually",
      "offset": 6758.82,
      "duration": 4.919
    },
    {
      "lang": "en",
      "text": "so the last piece",
      "offset": 6760.86,
      "duration": 5.7
    },
    {
      "lang": "en",
      "text": "um is that none of this now requires",
      "offset": 6763.739,
      "duration": 5.94
    },
    {
      "lang": "en",
      "text": "gradients from pytorch and so one thing",
      "offset": 6766.56,
      "duration": 4.86
    },
    {
      "lang": "en",
      "text": "you can do here",
      "offset": 6769.679,
      "duration": 2.641
    },
    {
      "lang": "en",
      "text": "um",
      "offset": 6771.42,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "is you can do with no grad and offset",
      "offset": 6772.32,
      "duration": 5.819
    },
    {
      "lang": "en",
      "text": "this whole code block",
      "offset": 6776.219,
      "duration": 3.181
    },
    {
      "lang": "en",
      "text": "and really what you're saying is you're",
      "offset": 6778.139,
      "duration": 2.461
    },
    {
      "lang": "en",
      "text": "telling Pat George that hey I'm not",
      "offset": 6779.4,
      "duration": 2.64
    },
    {
      "lang": "en",
      "text": "going to call backward on any of this",
      "offset": 6780.6,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "and this allows pytorch to be a bit more",
      "offset": 6782.04,
      "duration": 3.42
    },
    {
      "lang": "en",
      "text": "efficient with all of it",
      "offset": 6783.96,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "and then we should be able to just uh",
      "offset": 6785.46,
      "duration": 4.44
    },
    {
      "lang": "en",
      "text": "run this",
      "offset": 6787.8,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "and",
      "offset": 6789.9,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "it's running",
      "offset": 6791.88,
      "duration": 4.98
    },
    {
      "lang": "en",
      "text": "and you see that losses backward is",
      "offset": 6793.98,
      "duration": 4.02
    },
    {
      "lang": "en",
      "text": "commented out",
      "offset": 6796.86,
      "duration": 3.299
    },
    {
      "lang": "en",
      "text": "and we're optimizing",
      "offset": 6798,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "so we're going to leave this run and uh",
      "offset": 6800.159,
      "duration": 5.161
    },
    {
      "lang": "en",
      "text": "hopefully we get a good result",
      "offset": 6803.52,
      "duration": 3.54
    },
    {
      "lang": "en",
      "text": "okay so I allowed the neural net to",
      "offset": 6805.32,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "finish optimization",
      "offset": 6807.06,
      "duration": 4.139
    },
    {
      "lang": "en",
      "text": "then here I calibrate the bachelor",
      "offset": 6808.679,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "parameters because I did not keep track",
      "offset": 6811.199,
      "duration": 4.741
    },
    {
      "lang": "en",
      "text": "of the running mean and very variants in",
      "offset": 6813.239,
      "duration": 4.021
    },
    {
      "lang": "en",
      "text": "their training Loop",
      "offset": 6815.94,
      "duration": 3.9
    },
    {
      "lang": "en",
      "text": "then here I ran the loss and you see",
      "offset": 6817.26,
      "duration": 3.72
    },
    {
      "lang": "en",
      "text": "that we actually obtained a pretty good",
      "offset": 6819.84,
      "duration": 2.94
    },
    {
      "lang": "en",
      "text": "loss very similar to what we've achieved",
      "offset": 6820.98,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "before",
      "offset": 6822.78,
      "duration": 2.7
    },
    {
      "lang": "en",
      "text": "and then here I'm sampling from the",
      "offset": 6823.98,
      "duration": 3.659
    },
    {
      "lang": "en",
      "text": "model and we see some of the name like",
      "offset": 6825.48,
      "duration": 4.259
    },
    {
      "lang": "en",
      "text": "gibberish that we're sort of used to so",
      "offset": 6827.639,
      "duration": 4.681
    },
    {
      "lang": "en",
      "text": "basically the model worked and samples",
      "offset": 6829.739,
      "duration": 5.041
    },
    {
      "lang": "en",
      "text": "uh pretty decent results compared to",
      "offset": 6832.32,
      "duration": 4.02
    },
    {
      "lang": "en",
      "text": "what we were used to so everything is",
      "offset": 6834.78,
      "duration": 3.66
    },
    {
      "lang": "en",
      "text": "the same but of course the big deal is",
      "offset": 6836.34,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "that we did not use lots of backward we",
      "offset": 6838.44,
      "duration": 3.779
    },
    {
      "lang": "en",
      "text": "did not use package Auto grad and we",
      "offset": 6840.42,
      "duration": 3.9
    },
    {
      "lang": "en",
      "text": "estimated our gradients ourselves by",
      "offset": 6842.219,
      "duration": 2.881
    },
    {
      "lang": "en",
      "text": "hand",
      "offset": 6844.32,
      "duration": 2.339
    },
    {
      "lang": "en",
      "text": "and so hopefully you're looking at this",
      "offset": 6845.1,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "the backward pass of this neural net and",
      "offset": 6846.659,
      "duration": 3.48
    },
    {
      "lang": "en",
      "text": "you're thinking to yourself actually",
      "offset": 6848.699,
      "duration": 3.781
    },
    {
      "lang": "en",
      "text": "that's not too complicated",
      "offset": 6850.139,
      "duration": 2.941
    },
    {
      "lang": "en",
      "text": "um",
      "offset": 6852.48,
      "duration": 2.699
    },
    {
      "lang": "en",
      "text": "each one of these layers is like three",
      "offset": 6853.08,
      "duration": 4.2
    },
    {
      "lang": "en",
      "text": "lines of code or something like that and",
      "offset": 6855.179,
      "duration": 3.54
    },
    {
      "lang": "en",
      "text": "most of it is fairly straightforward",
      "offset": 6857.28,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "potentially with the notable exception",
      "offset": 6858.719,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "of the batch normalization backward pass",
      "offset": 6860.639,
      "duration": 4.681
    },
    {
      "lang": "en",
      "text": "otherwise it's pretty good okay and",
      "offset": 6862.8,
      "duration": 3.899
    },
    {
      "lang": "en",
      "text": "that's everything I wanted to cover for",
      "offset": 6865.32,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "this lecture so hopefully you found this",
      "offset": 6866.699,
      "duration": 4.381
    },
    {
      "lang": "en",
      "text": "interesting and what I liked about it",
      "offset": 6869.159,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "honestly is that it gave us a very nice",
      "offset": 6871.08,
      "duration": 3.78
    },
    {
      "lang": "en",
      "text": "diversity of layers to back propagate",
      "offset": 6873,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "through and",
      "offset": 6874.86,
      "duration": 3.48
    },
    {
      "lang": "en",
      "text": "um I think it gives a pretty nice and",
      "offset": 6876.6,
      "duration": 3.18
    },
    {
      "lang": "en",
      "text": "comprehensive sense of how these",
      "offset": 6878.34,
      "duration": 3.24
    },
    {
      "lang": "en",
      "text": "backward passes are implemented and how",
      "offset": 6879.78,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "they work and you'd be able to derive",
      "offset": 6881.58,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "them yourself but of course in practice",
      "offset": 6883.86,
      "duration": 2.94
    },
    {
      "lang": "en",
      "text": "you probably don't want to and you want",
      "offset": 6885.179,
      "duration": 3.96
    },
    {
      "lang": "en",
      "text": "to use the pythonograd but hopefully you",
      "offset": 6886.8,
      "duration": 4.2
    },
    {
      "lang": "en",
      "text": "have some intuition about how gradients",
      "offset": 6889.139,
      "duration": 3.54
    },
    {
      "lang": "en",
      "text": "flow backwards through the neural net",
      "offset": 6891,
      "duration": 4.139
    },
    {
      "lang": "en",
      "text": "starting at the loss and how they flow",
      "offset": 6892.679,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "through all the variables and all the",
      "offset": 6895.139,
      "duration": 3.06
    },
    {
      "lang": "en",
      "text": "intermediate results",
      "offset": 6896.52,
      "duration": 4.199
    },
    {
      "lang": "en",
      "text": "and if you understood a good chunk of it",
      "offset": 6898.199,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "and if you have a sense of that then you",
      "offset": 6900.719,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "can count yourself as one of these buff",
      "offset": 6902.28,
      "duration": 3.78
    },
    {
      "lang": "en",
      "text": "doji's on the left instead of the uh",
      "offset": 6903.719,
      "duration": 5.4
    },
    {
      "lang": "en",
      "text": "those on the right here now in the next",
      "offset": 6906.06,
      "duration": 4.5
    },
    {
      "lang": "en",
      "text": "lecture we're actually going to go to",
      "offset": 6909.119,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "recurrent neural nuts lstms and all the",
      "offset": 6910.56,
      "duration": 5.7
    },
    {
      "lang": "en",
      "text": "other variants of RNs and we're going to",
      "offset": 6913.44,
      "duration": 4.38
    },
    {
      "lang": "en",
      "text": "start to complexify the architecture and",
      "offset": 6916.26,
      "duration": 3.419
    },
    {
      "lang": "en",
      "text": "start to achieve better uh log",
      "offset": 6917.82,
      "duration": 3.72
    },
    {
      "lang": "en",
      "text": "likelihoods and so I'm really looking",
      "offset": 6919.679,
      "duration": 5.601
    },
    {
      "lang": "en",
      "text": "forward to that and I'll see you then",
      "offset": 6921.54,
      "duration": 3.74
    }
  ],
  "cleanText": null,
  "dumpedAt": "2025-07-21T18:43:24.896Z"
}