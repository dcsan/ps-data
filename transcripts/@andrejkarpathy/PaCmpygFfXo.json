{
  "episodeId": "PaCmpygFfXo",
  "channelSlug": "@andrejkarpathy",
  "title": "The spelled-out intro to language modeling: building makemore",
  "publishedAt": "2022-09-07T19:14:47.000Z",
  "rawLines": [
    {
      "lang": "en",
      "text": "hi everyone hope you're well",
      "offset": 0.24,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "and next up what i'd like to do is i'd",
      "offset": 2.24,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "like to build out make more",
      "offset": 4.08,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "like micrograd before it make more is a",
      "offset": 6.16,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "repository that i have on my github",
      "offset": 8.559,
      "duration": 2.801
    },
    {
      "lang": "en",
      "text": "webpage",
      "offset": 10.24,
      "duration": 2.399
    },
    {
      "lang": "en",
      "text": "you can look at it",
      "offset": 11.36,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "but just like with micrograd i'm going",
      "offset": 12.639,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "to build it out step by step and i'm",
      "offset": 14.4,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "going to spell everything out so we're",
      "offset": 16.32,
      "duration": 2.719
    },
    {
      "lang": "en",
      "text": "going to build it out slowly and",
      "offset": 17.92,
      "duration": 2.32
    },
    {
      "lang": "en",
      "text": "together",
      "offset": 19.039,
      "duration": 3.121
    },
    {
      "lang": "en",
      "text": "now what is make more",
      "offset": 20.24,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "make more as the name suggests",
      "offset": 22.16,
      "duration": 5.359
    },
    {
      "lang": "en",
      "text": "makes more of things that you give it",
      "offset": 24.56,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "so here's an example",
      "offset": 27.519,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "names.txt is an example dataset to make",
      "offset": 29.039,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "more",
      "offset": 31.519,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "and when you look at names.txt you'll",
      "offset": 32.48,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "find that it's a very large data set of",
      "offset": 34.399,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "names",
      "offset": 36.8,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "so",
      "offset": 38.16,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "here's lots of different types of names",
      "offset": 40.16,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "in fact i believe there are 32 000 names",
      "offset": 41.68,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "that i've sort of found randomly on the",
      "offset": 44.16,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "government website",
      "offset": 46.079,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "and if you train make more on this data",
      "offset": 47.84,
      "duration": 5.359
    },
    {
      "lang": "en",
      "text": "set it will learn to make more of things",
      "offset": 50.079,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "like this",
      "offset": 53.199,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "and in particular in this case that will",
      "offset": 55.28,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "mean more things that sound name-like",
      "offset": 57.6,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "but are actually unique names",
      "offset": 60.32,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "and maybe if you have a baby and you're",
      "offset": 62.32,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "trying to assign name maybe you're",
      "offset": 63.84,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "looking for a cool new sounding unique",
      "offset": 65.28,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "name make more might help you",
      "offset": 67.04,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "so here are some example generations",
      "offset": 69.52,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "from the neural network",
      "offset": 71.28,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "once we train it on our data set",
      "offset": 73.36,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "so here's some example",
      "offset": 76.159,
      "duration": 3.521
    },
    {
      "lang": "en",
      "text": "unique names that it will generate",
      "offset": 77.759,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "dontel",
      "offset": 79.68,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "irot",
      "offset": 81.68,
      "duration": 2.799
    },
    {
      "lang": "en",
      "text": "zhendi",
      "offset": 83.439,
      "duration": 3.521
    },
    {
      "lang": "en",
      "text": "and so on and so all these are sound",
      "offset": 84.479,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "name like but they're not of course",
      "offset": 86.96,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "names",
      "offset": 88.88,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "so under the hood make more is a",
      "offset": 90.64,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "character level language model so what",
      "offset": 92.88,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "that means is that it is treating every",
      "offset": 95.2,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "single line here as an example and",
      "offset": 97.28,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "within each example it's treating them",
      "offset": 99.68,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "all as sequences of individual",
      "offset": 102.079,
      "duration": 6.801
    },
    {
      "lang": "en",
      "text": "characters so r e e s e is this example",
      "offset": 104,
      "duration": 6.56
    },
    {
      "lang": "en",
      "text": "and that's the sequence of characters",
      "offset": 108.88,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "and that's the level on which we are",
      "offset": 110.56,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "building out make more and what it means",
      "offset": 111.84,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "to be a character level language model",
      "offset": 114.799,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "then is that it's just uh sort of",
      "offset": 116.479,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "modeling those sequences of characters",
      "offset": 118.399,
      "duration": 2.961
    },
    {
      "lang": "en",
      "text": "and it knows how to predict the next",
      "offset": 119.92,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "character in the sequence",
      "offset": 121.36,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "now we're actually going to implement a",
      "offset": 123.6,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "large number of character level language",
      "offset": 125.28,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "models in terms of the neural networks",
      "offset": 127.439,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "that are involved in predicting the next",
      "offset": 129.2,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "character in a sequence so very simple",
      "offset": 130.879,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "bi-gram and back of work models",
      "offset": 133.44,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "multilingual perceptrons recurrent",
      "offset": 135.28,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "neural networks all the way to modern",
      "offset": 137.12,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "transformers in fact the transformer",
      "offset": 139.599,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "that we will build will be basically the",
      "offset": 141.68,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "equivalent transformer to gpt2 if you",
      "offset": 143.68,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "have heard of gpt uh so that's kind of a",
      "offset": 146.239,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "big deal it's a modern network and by",
      "offset": 148.64,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "the end of the series you will actually",
      "offset": 150.879,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "understand how that works um on the",
      "offset": 152.56,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "level of characters now to give you a",
      "offset": 154.64,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "sense of the extensions here uh after",
      "offset": 156.8,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "characters we will probably spend some",
      "offset": 159.84,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "time on the word level so that we can",
      "offset": 161.44,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "generate documents of words not just",
      "offset": 163.28,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "little you know segments of characters",
      "offset": 165.04,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "but we can generate entire large much",
      "offset": 167.519,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "larger documents",
      "offset": 169.44,
      "duration": 2.879
    },
    {
      "lang": "en",
      "text": "and then we're probably going to go into",
      "offset": 170.959,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "images and image text",
      "offset": 172.319,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "networks such as dolly stable diffusion",
      "offset": 174.64,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "and so on but for now we have to start",
      "offset": 177.04,
      "duration": 5.119
    },
    {
      "lang": "en",
      "text": "here character level language modeling",
      "offset": 180,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "let's go",
      "offset": 182.159,
      "duration": 2.641
    },
    {
      "lang": "en",
      "text": "so like before we are starting with a",
      "offset": 183.36,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "completely blank jupiter notebook page",
      "offset": 184.8,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "the first thing is i would like to",
      "offset": 186.959,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "basically load up the dataset names.txt",
      "offset": 188.56,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "so we're going to open up names.txt for",
      "offset": 191.76,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "reading",
      "offset": 193.84,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "and we're going to read in everything",
      "offset": 195.36,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "into a massive string",
      "offset": 197.36,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "and then because it's a massive string",
      "offset": 199.76,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "we'd only like the individual words and",
      "offset": 201.36,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "put them in the list",
      "offset": 203.12,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "so let's call split lines",
      "offset": 204.48,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "on that string",
      "offset": 206.48,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "to get all of our words as a python list",
      "offset": 207.76,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "of strings",
      "offset": 210.239,
      "duration": 3.121
    },
    {
      "lang": "en",
      "text": "so basically we can look at for example",
      "offset": 212,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "the first 10 words",
      "offset": 213.36,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "and we have that it's a list of emma",
      "offset": 215.76,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "olivia eva and so on",
      "offset": 219.04,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "and if we look at",
      "offset": 221.44,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "the top of the page here that is indeed",
      "offset": 223.68,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "what we see",
      "offset": 225.519,
      "duration": 2.64
    },
    {
      "lang": "en",
      "text": "um",
      "offset": 227.04,
      "duration": 2.559
    },
    {
      "lang": "en",
      "text": "so that's good",
      "offset": 228.159,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "this list actually makes me feel that",
      "offset": 229.599,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "this is probably sorted by frequency",
      "offset": 232.239,
      "duration": 5.041
    },
    {
      "lang": "en",
      "text": "but okay so",
      "offset": 235.599,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "these are the words now we'd like to",
      "offset": 237.28,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "actually like learn a little bit more",
      "offset": 238.959,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "about this data set let's look at the",
      "offset": 240.56,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "total number of words we expect this to",
      "offset": 242.319,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "be roughly 32 000",
      "offset": 243.92,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "and then what is the for example",
      "offset": 246.4,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "shortest word",
      "offset": 247.68,
      "duration": 3.279
    },
    {
      "lang": "en",
      "text": "so min of",
      "offset": 249.12,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "length of each word for w inwards",
      "offset": 250.959,
      "duration": 6.081
    },
    {
      "lang": "en",
      "text": "so the shortest word will be length",
      "offset": 253.599,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "two",
      "offset": 257.04,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "and max of one w for w in words so the",
      "offset": 258.239,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "longest word will be",
      "offset": 261.199,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "15 characters",
      "offset": 263.12,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "so let's now think through our very",
      "offset": 264.56,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "first language model",
      "offset": 265.84,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "as i mentioned a character level",
      "offset": 267.36,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "language model is predicting the next",
      "offset": 268.72,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "character in a sequence given already",
      "offset": 270.72,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "some concrete sequence of characters",
      "offset": 273.44,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "before it",
      "offset": 275.199,
      "duration": 2.801
    },
    {
      "lang": "en",
      "text": "now we have to realize here is that",
      "offset": 276.56,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "every single word here like isabella is",
      "offset": 278,
      "duration": 5.919
    },
    {
      "lang": "en",
      "text": "actually quite a few examples packed in",
      "offset": 280.96,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "to that single word",
      "offset": 283.919,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "because what is an existence of a word",
      "offset": 285.52,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "like isabella in the data set telling us",
      "offset": 287.28,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "really it's saying that",
      "offset": 288.96,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "the character i is a very likely",
      "offset": 291.12,
      "duration": 5.359
    },
    {
      "lang": "en",
      "text": "character to come first in the sequence",
      "offset": 293.6,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "of a name",
      "offset": 296.479,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "the character s is likely to come",
      "offset": 298.56,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "after i",
      "offset": 301.68,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "the character a is likely to come after",
      "offset": 304.32,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "is",
      "offset": 306.4,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "the character b is very likely to come",
      "offset": 307.6,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "after isa and so on all the way to a",
      "offset": 309.36,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "following isabel",
      "offset": 312.479,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "and then there's one more example",
      "offset": 314.4,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "actually packed in here",
      "offset": 315.68,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "and that is that",
      "offset": 317.28,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "after there's isabella",
      "offset": 319.039,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "the word is very likely to end",
      "offset": 321.36,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "so that's one more sort of explicit",
      "offset": 323.759,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "piece of information that we have here",
      "offset": 325.44,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "that we have to be careful with",
      "offset": 327.36,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "and so there's a lot backed into a",
      "offset": 329.6,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "single individual word in terms of the",
      "offset": 331.68,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "statistical structure of what's likely",
      "offset": 333.68,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "to follow in these character sequences",
      "offset": 335.36,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "and then of course we don't have just an",
      "offset": 338,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "individual word we actually have 32 000",
      "offset": 339.199,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "of these and so there's a lot of",
      "offset": 341.52,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "structure here to model",
      "offset": 342.8,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "now in the beginning what i'd like to",
      "offset": 344.8,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "start with is i'd like to start with",
      "offset": 346.16,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "building a bi-gram language model",
      "offset": 348.08,
      "duration": 4.959
    },
    {
      "lang": "en",
      "text": "now in the bigram language model we're",
      "offset": 351.199,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "always working with just",
      "offset": 353.039,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "two characters at a time",
      "offset": 354.639,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "so we're only looking at one character",
      "offset": 356.72,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "that we are given and we're trying to",
      "offset": 359.28,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "predict the next character in the",
      "offset": 360.88,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "sequence",
      "offset": 362.479,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "so um what characters are likely to",
      "offset": 363.84,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "follow are what characters are likely to",
      "offset": 366.319,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "follow a and so on and we're just",
      "offset": 368.319,
      "duration": 3.521
    },
    {
      "lang": "en",
      "text": "modeling that kind of a little local",
      "offset": 370.16,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "structure",
      "offset": 371.84,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "and we're forgetting the fact that we",
      "offset": 372.88,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "may have a lot more information we're",
      "offset": 374.96,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "always just looking at the previous",
      "offset": 376.96,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "character to predict the next one so",
      "offset": 378.319,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "it's a very simple and weak language",
      "offset": 380.319,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "model but i think it's a great place to",
      "offset": 381.68,
      "duration": 2.4
    },
    {
      "lang": "en",
      "text": "start",
      "offset": 383.199,
      "duration": 2.481
    },
    {
      "lang": "en",
      "text": "so now let's begin by looking at these",
      "offset": 384.08,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "bi-grams in our data set and what they",
      "offset": 385.68,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "look like and these bi-grams again are",
      "offset": 387.44,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "just two characters in a row",
      "offset": 389.039,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "so for w in words",
      "offset": 390.88,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "each w here is an individual word a",
      "offset": 393.12,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "string",
      "offset": 395.199,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "we want to iterate uh for",
      "offset": 396.319,
      "duration": 5.361
    },
    {
      "lang": "en",
      "text": "we're going to iterate this word",
      "offset": 399.6,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "with consecutive characters so two",
      "offset": 401.68,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "characters at a time sliding it through",
      "offset": 404,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "the word now a interesting nice way cute",
      "offset": 405.919,
      "duration": 5.521
    },
    {
      "lang": "en",
      "text": "way to do this in python by the way is",
      "offset": 409.36,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "doing something like this for character",
      "offset": 411.44,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "one character two in zip off",
      "offset": 413.199,
      "duration": 6.801
    },
    {
      "lang": "en",
      "text": "w and w at one",
      "offset": 416.319,
      "duration": 5.361
    },
    {
      "lang": "en",
      "text": "one column",
      "offset": 420,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "print",
      "offset": 421.68,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "character one character two",
      "offset": 422.8,
      "duration": 3.119
    },
    {
      "lang": "en",
      "text": "and let's not do all the words let's",
      "offset": 424.56,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "just do the first three words and i'm",
      "offset": 425.919,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "going to show you in a second how this",
      "offset": 427.44,
      "duration": 2.479
    },
    {
      "lang": "en",
      "text": "works",
      "offset": 429.039,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "but for now basically as an example",
      "offset": 429.919,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "let's just do the very first word alone",
      "offset": 431.919,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "emma",
      "offset": 433.68,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "you see how we have a emma and this will",
      "offset": 435.28,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "just print e m m m a",
      "offset": 438.319,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "and the reason this works is because w",
      "offset": 440.96,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "is the string emma w at one column is",
      "offset": 443.759,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "the string mma",
      "offset": 446.8,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "and zip",
      "offset": 448.639,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "takes two iterators and it pairs them up",
      "offset": 449.68,
      "duration": 5.359
    },
    {
      "lang": "en",
      "text": "and then creates an iterator over the",
      "offset": 453.36,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "tuples of their consecutive entries",
      "offset": 455.039,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "and if any one of these lists is shorter",
      "offset": 457.36,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "than the other then it will just",
      "offset": 459.44,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "halt and return",
      "offset": 461.84,
      "duration": 7.12
    },
    {
      "lang": "en",
      "text": "so basically that's why we return em mmm",
      "offset": 463.68,
      "duration": 6.239
    },
    {
      "lang": "en",
      "text": "ma",
      "offset": 468.96,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "but then because this iterator second",
      "offset": 469.919,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "one here runs out of elements zip just",
      "offset": 472.08,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "ends and that's why we only get these",
      "offset": 475.12,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "tuples so pretty cute",
      "offset": 476.879,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "so these are the consecutive elements in",
      "offset": 479.68,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "the first word now we have to be careful",
      "offset": 481.84,
      "duration": 3.039
    },
    {
      "lang": "en",
      "text": "because we actually have more",
      "offset": 483.919,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "information here than just these three",
      "offset": 484.879,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "examples as i mentioned we know that e",
      "offset": 487.28,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "is the is very likely to come first and",
      "offset": 489.919,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "we know that a in this case is coming",
      "offset": 492.319,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "last",
      "offset": 494.4,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "so one way to do this is basically we're",
      "offset": 495.599,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "going to create",
      "offset": 497.759,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "a special array here all",
      "offset": 499.28,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "characters",
      "offset": 501.52,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "and um we're going to hallucinate a",
      "offset": 503.039,
      "duration": 5.361
    },
    {
      "lang": "en",
      "text": "special start token here",
      "offset": 505.28,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "i'm going to",
      "offset": 508.4,
      "duration": 3.999
    },
    {
      "lang": "en",
      "text": "call it like special start",
      "offset": 509.84,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "so this is a list of one element",
      "offset": 512.399,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "plus",
      "offset": 514.56,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "w",
      "offset": 516.479,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "and then plus a special end character",
      "offset": 517.76,
      "duration": 4.959
    },
    {
      "lang": "en",
      "text": "and the reason i'm wrapping the list of",
      "offset": 521.279,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "w here is because w is a string emma",
      "offset": 522.719,
      "duration": 5.921
    },
    {
      "lang": "en",
      "text": "list of w will just have the individual",
      "offset": 525.92,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "characters in the list",
      "offset": 528.64,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "and then",
      "offset": 530.8,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "doing this again now but not iterating",
      "offset": 531.839,
      "duration": 6.241
    },
    {
      "lang": "en",
      "text": "over w's but over the characters",
      "offset": 534.72,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "will give us something like this",
      "offset": 538.08,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "so e is likely so this is a bigram of",
      "offset": 540.16,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "the start character and e and this is a",
      "offset": 542.72,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "bigram of the",
      "offset": 545.12,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "a and the special end character",
      "offset": 546.399,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "and now we can look at for example what",
      "offset": 549.04,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "this looks like for",
      "offset": 550.48,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "olivia or eva",
      "offset": 552,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "and indeed we can actually",
      "offset": 554.48,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "potentially do this for the entire data",
      "offset": 556.16,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "set but we won't print that that's going",
      "offset": 557.6,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "to be too much",
      "offset": 559.36,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "but these are the individual character",
      "offset": 560.64,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "diagrams and we can print them",
      "offset": 562.48,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "now in order to learn the statistics",
      "offset": 564.959,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "about which characters are likely to",
      "offset": 566.48,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "follow other characters the simplest way",
      "offset": 568.32,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "in the bigram language models is to",
      "offset": 570.56,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "simply do it by counting",
      "offset": 572.72,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "so we're basically just going to count",
      "offset": 574.399,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "how often any one of these combinations",
      "offset": 576.399,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "occurs in the training set",
      "offset": 578.399,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "in these words",
      "offset": 580.24,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "so we're going to need some kind of a",
      "offset": 581.76,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "dictionary that's going to maintain some",
      "offset": 583.12,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "counts for every one of these diagrams",
      "offset": 584.88,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "so let's use a dictionary b",
      "offset": 587.279,
      "duration": 5.601
    },
    {
      "lang": "en",
      "text": "and this will map these bi-grams so",
      "offset": 589.76,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "bi-gram is a tuple of character one",
      "offset": 592.88,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "character two",
      "offset": 594.48,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "and then b at bi-gram",
      "offset": 596.16,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "will be b dot get of bi-gram",
      "offset": 598.72,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "which is basically the same as b at",
      "offset": 601.04,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "bigram",
      "offset": 603.2,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "but in the case that bigram is not in",
      "offset": 604.64,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "the dictionary b we would like to by",
      "offset": 607.04,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "default return to zero",
      "offset": 609.36,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "plus one",
      "offset": 611.839,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "so this will basically add up all the",
      "offset": 613.04,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "bigrams and count how often they occur",
      "offset": 615.519,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "let's get rid of printing",
      "offset": 618.24,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "or rather",
      "offset": 620.24,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "let's keep the printing and let's just",
      "offset": 622.399,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "inspect what b is in this case",
      "offset": 624,
      "duration": 5.279
    },
    {
      "lang": "en",
      "text": "and we see that many bi-grams occur just",
      "offset": 627.12,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "a single time this one allegedly",
      "offset": 629.279,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "occurred three times",
      "offset": 631.12,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "so a was an ending character three times",
      "offset": 632.959,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "and that's true for all of these words",
      "offset": 635.36,
      "duration": 6.56
    },
    {
      "lang": "en",
      "text": "all of emma olivia and eva and with a",
      "offset": 637.92,
      "duration": 8.68
    },
    {
      "lang": "en",
      "text": "so that's why this occurred three times",
      "offset": 641.92,
      "duration": 4.68
    },
    {
      "lang": "en",
      "text": "now let's do it for all the words",
      "offset": 646.64,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "oops i should not have printed",
      "offset": 651.279,
      "duration": 5.441
    },
    {
      "lang": "en",
      "text": "i'm going to erase that",
      "offset": 655.04,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "let's kill this",
      "offset": 656.72,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "let's just run",
      "offset": 658.72,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "and now b will have the statistics of",
      "offset": 660.56,
      "duration": 3.519
    },
    {
      "lang": "en",
      "text": "the entire data set",
      "offset": 662.32,
      "duration": 3.519
    },
    {
      "lang": "en",
      "text": "so these are the counts across all the",
      "offset": 664.079,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "words of the individual pie grams",
      "offset": 665.839,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "and we could for example look at some of",
      "offset": 668.32,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "the most common ones and least common",
      "offset": 669.92,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "ones",
      "offset": 671.519,
      "duration": 3.521
    },
    {
      "lang": "en",
      "text": "this kind of grows in python but the way",
      "offset": 673.36,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "to do this the simplest way i like is we",
      "offset": 675.04,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "just use b dot items",
      "offset": 677.44,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "b dot items returns",
      "offset": 679.519,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "the tuples of",
      "offset": 681.76,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "key value in this case the keys are",
      "offset": 684.32,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "the character diagrams and the values",
      "offset": 687.04,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "are the counts",
      "offset": 689.2,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "and so then what we want to do is we",
      "offset": 690.88,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "want to do",
      "offset": 692.399,
      "duration": 2.401
    },
    {
      "lang": "en",
      "text": "sorted of this",
      "offset": 695.68,
      "duration": 6.719
    },
    {
      "lang": "en",
      "text": "but by default sort is on the first",
      "offset": 698.399,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "on the first item of a tuple but we want",
      "offset": 703.519,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "to sort by the values which are the",
      "offset": 705.92,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "second element of a tuple that is the",
      "offset": 707.519,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "key value",
      "offset": 709.279,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "so we want to use the key",
      "offset": 710.639,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "equals lambda",
      "offset": 712.959,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "that takes the key value",
      "offset": 715.279,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "and returns",
      "offset": 717.6,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "the key value at the one not at zero but",
      "offset": 718.88,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "at one which is the count so we want to",
      "offset": 722.24,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "sort by the count",
      "offset": 724.56,
      "duration": 5.839
    },
    {
      "lang": "en",
      "text": "of these elements",
      "offset": 727.12,
      "duration": 5.519
    },
    {
      "lang": "en",
      "text": "and actually we wanted to go backwards",
      "offset": 730.399,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "so here we have is",
      "offset": 732.639,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "the bi-gram q and r occurs only a single",
      "offset": 734.639,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "time",
      "offset": 737.2,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "dz occurred only a single time",
      "offset": 738.399,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "and when we sort this the other way",
      "offset": 740.639,
      "duration": 2.721
    },
    {
      "lang": "en",
      "text": "around",
      "offset": 741.839,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "we're going to see the most likely",
      "offset": 743.36,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "bigrams so we see that n was",
      "offset": 745.279,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "very often an ending character",
      "offset": 748.32,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "many many times and apparently n almost",
      "offset": 750.639,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "always follows an a",
      "offset": 752.959,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "and that's a very likely combination as",
      "offset": 754.399,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "well",
      "offset": 756.16,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "so",
      "offset": 758.639,
      "duration": 3.281
    },
    {
      "lang": "en",
      "text": "this is kind of the individual counts",
      "offset": 759.839,
      "duration": 5.041
    },
    {
      "lang": "en",
      "text": "that we achieve over the entire data set",
      "offset": 761.92,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "now it's actually going to be",
      "offset": 764.88,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "significantly more convenient for us to",
      "offset": 766.079,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "keep this information in a",
      "offset": 768.32,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "two-dimensional array instead of a",
      "offset": 769.76,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "python dictionary",
      "offset": 771.68,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "so",
      "offset": 773.6,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "we're going to store this information",
      "offset": 774.48,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "in a 2d array",
      "offset": 776.72,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "and",
      "offset": 778.48,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "the rows are going to be the first",
      "offset": 780,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "character of the bigram and the columns",
      "offset": 781.44,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "are going to be the second character and",
      "offset": 783.44,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "each entry in this two-dimensional array",
      "offset": 785.279,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "will tell us how often that first",
      "offset": 786.88,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "character files the second character in",
      "offset": 788.88,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "the data set",
      "offset": 791.279,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "so in particular the array",
      "offset": 792.639,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "representation that we're going to use",
      "offset": 794.639,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "or the library is that of pytorch",
      "offset": 796.079,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "and pytorch is a deep",
      "offset": 798.8,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "learning neural network framework but",
      "offset": 800.24,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "part of it is also this torch.tensor",
      "offset": 802.399,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "which allows us to create",
      "offset": 805.2,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "multi-dimensional arrays and manipulate",
      "offset": 806.16,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "them very efficiently",
      "offset": 808,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "so",
      "offset": 809.76,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "let's import pytorch which you can do by",
      "offset": 810.72,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "import torch",
      "offset": 812.72,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "and then we can create",
      "offset": 814.72,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "arrays",
      "offset": 816.32,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "so let's create a array of zeros",
      "offset": 817.44,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "and we give it a",
      "offset": 820.639,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "size of this array let's create a three",
      "offset": 822.88,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "by five array as an example",
      "offset": 824.56,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "and",
      "offset": 827.04,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "this is a three by five array of zeros",
      "offset": 828.56,
      "duration": 5.279
    },
    {
      "lang": "en",
      "text": "and by default you'll notice a.d type",
      "offset": 831.519,
      "duration": 5.041
    },
    {
      "lang": "en",
      "text": "which is short for data type is float32",
      "offset": 833.839,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "so these are single precision floating",
      "offset": 836.56,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "point numbers",
      "offset": 838.16,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "because we are going to represent counts",
      "offset": 839.44,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "let's actually use d type as torch dot",
      "offset": 841.76,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "and 32",
      "offset": 844.32,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "so these are",
      "offset": 846,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "32-bit integers",
      "offset": 847.76,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "so now you see that we have integer data",
      "offset": 850.079,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "inside this tensor",
      "offset": 852.8,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "now tensors allow us to really",
      "offset": 854.56,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "manipulate all the individual entries",
      "offset": 857.36,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "and do it very efficiently",
      "offset": 858.959,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "so for example if we want to change this",
      "offset": 860.72,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "bit",
      "offset": 862.88,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "we have to index into the tensor and in",
      "offset": 863.68,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "particular here this is the first row",
      "offset": 865.92,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "and the",
      "offset": 869.68,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "because it's zero indexed so this is row",
      "offset": 871.44,
      "duration": 6.399
    },
    {
      "lang": "en",
      "text": "index one and column index zero one two",
      "offset": 874.16,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "three",
      "offset": 877.839,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "so a at one comma three we can set that",
      "offset": 878.72,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "to one",
      "offset": 881.839,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "and then a we'll have a 1 over there",
      "offset": 883.6,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "we can of course also do things like",
      "offset": 887.04,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "this so now a will be 2 over there",
      "offset": 888.72,
      "duration": 5.119
    },
    {
      "lang": "en",
      "text": "or 3.",
      "offset": 892.48,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "and also we can for example say a 0 0 is",
      "offset": 893.839,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "5",
      "offset": 896.079,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "and then a will have a 5 over here",
      "offset": 897.199,
      "duration": 5.041
    },
    {
      "lang": "en",
      "text": "so that's how we can index into the",
      "offset": 900.079,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "arrays now of course the array that we",
      "offset": 902.24,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "are interested in is much much bigger so",
      "offset": 904.32,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "for our purposes we have 26 letters of",
      "offset": 906.48,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "the alphabet",
      "offset": 908.8,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "and then we have two special characters",
      "offset": 909.92,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "s and e",
      "offset": 912.399,
      "duration": 5.761
    },
    {
      "lang": "en",
      "text": "so uh we want 26 plus 2 or 28 by 28",
      "offset": 914,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "array",
      "offset": 918.16,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "and let's call it the capital n because",
      "offset": 919.199,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "it's going to represent sort of the",
      "offset": 921.12,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "counts",
      "offset": 922.56,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "let me erase this stuff",
      "offset": 924.48,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "so that's the array that starts at zeros",
      "offset": 926.72,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "28 by 28",
      "offset": 928.72,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "and now let's copy paste this",
      "offset": 930.399,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "here",
      "offset": 933.68,
      "duration": 3.279
    },
    {
      "lang": "en",
      "text": "but instead of having a dictionary b",
      "offset": 934.56,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "which we're going to erase we now have",
      "offset": 936.959,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "an n",
      "offset": 939.12,
      "duration": 3.279
    },
    {
      "lang": "en",
      "text": "now the problem here is that we have",
      "offset": 940.959,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "these characters which are strings but",
      "offset": 942.399,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "we have to now",
      "offset": 944.16,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "um basically index into a",
      "offset": 945.519,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "um array and we have to index using",
      "offset": 948.079,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "integers so we need some kind of a",
      "offset": 950.48,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "lookup table from characters to integers",
      "offset": 952.079,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "so let's construct such a character",
      "offset": 955.199,
      "duration": 2.801
    },
    {
      "lang": "en",
      "text": "array",
      "offset": 956.8,
      "duration": 2.479
    },
    {
      "lang": "en",
      "text": "and the way we're going to do this is",
      "offset": 958,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "we're going to take all the words which",
      "offset": 959.279,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "is a list of strings",
      "offset": 961.12,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "we're going to concatenate all of it",
      "offset": 962.72,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "into a massive string so this is just",
      "offset": 964.32,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "simply the entire data set as a single",
      "offset": 966,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "string",
      "offset": 967.6,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "we're going to pass this to the set",
      "offset": 969.199,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "constructor which takes this massive",
      "offset": 970.8,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "string",
      "offset": 973.519,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "and throws out duplicates because sets",
      "offset": 974.399,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "do not allow duplicates",
      "offset": 976.959,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "so set of this will just be the set of",
      "offset": 978.88,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "all the lowercase characters",
      "offset": 981.12,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "and there should be a total of 26 of",
      "offset": 984.16,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "them",
      "offset": 986,
      "duration": 2.24
    },
    {
      "lang": "en",
      "text": "and now we actually don't want a set we",
      "offset": 988.56,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "want a list",
      "offset": 989.92,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "but we don't want a list sorted in some",
      "offset": 992.639,
      "duration": 3.521
    },
    {
      "lang": "en",
      "text": "weird arbitrary way we want it to be",
      "offset": 994.56,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "sorted",
      "offset": 996.16,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "from a to z",
      "offset": 997.6,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "so sorted list",
      "offset": 999.759,
      "duration": 5.281
    },
    {
      "lang": "en",
      "text": "so those are our characters",
      "offset": 1001.839,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "now what we want is this lookup table as",
      "offset": 1005.6,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "i mentioned so let's create a special",
      "offset": 1007.279,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "s2i i will call it",
      "offset": 1009.92,
      "duration": 5.839
    },
    {
      "lang": "en",
      "text": "um s is string or character and this",
      "offset": 1012.959,
      "duration": 5.841
    },
    {
      "lang": "en",
      "text": "will be an s2i mapping",
      "offset": 1015.759,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "for",
      "offset": 1018.8,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "is in enumerate of these characters",
      "offset": 1019.92,
      "duration": 6.96
    },
    {
      "lang": "en",
      "text": "so enumerate basically gives us this",
      "offset": 1024.24,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "iterator over the integer index and the",
      "offset": 1026.88,
      "duration": 5.439
    },
    {
      "lang": "en",
      "text": "actual element of the list and then we",
      "offset": 1030,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "are mapping the character to the integer",
      "offset": 1032.319,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "so s2i",
      "offset": 1035.199,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "is a mapping from a to 0 b to 1 etc all",
      "offset": 1036.72,
      "duration": 6.8
    },
    {
      "lang": "en",
      "text": "the way from z to 25",
      "offset": 1039.919,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "and that's going to be useful here but",
      "offset": 1044.079,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "we actually also have to specifically",
      "offset": 1045.439,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "set that s will be 26",
      "offset": 1047.199,
      "duration": 6.401
    },
    {
      "lang": "en",
      "text": "and s to i at e will be 27 right because",
      "offset": 1049.76,
      "duration": 6.159
    },
    {
      "lang": "en",
      "text": "z was 25.",
      "offset": 1053.6,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "so those are the lookups and now we can",
      "offset": 1055.919,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "come here and we can map",
      "offset": 1058.08,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "both character 1 and character 2 to",
      "offset": 1059.84,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "their integers",
      "offset": 1061.52,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "so this will be s2i at character 1",
      "offset": 1062.72,
      "duration": 6.64
    },
    {
      "lang": "en",
      "text": "and ix2 will be s2i of character 2.",
      "offset": 1065.28,
      "duration": 6.8
    },
    {
      "lang": "en",
      "text": "and now we should be able to",
      "offset": 1069.36,
      "duration": 6.319
    },
    {
      "lang": "en",
      "text": "do this line but using our array so n at",
      "offset": 1072.08,
      "duration": 6.8
    },
    {
      "lang": "en",
      "text": "x1 ix2 this is the two-dimensional array",
      "offset": 1075.679,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "indexing i've shown you before",
      "offset": 1078.88,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "and honestly just plus equals one",
      "offset": 1080.559,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "because everything starts at",
      "offset": 1082.96,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "zero",
      "offset": 1084.799,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "so this should",
      "offset": 1086.16,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "work",
      "offset": 1087.919,
      "duration": 5.041
    },
    {
      "lang": "en",
      "text": "and give us a large 28 by 28 array",
      "offset": 1088.88,
      "duration": 6.159
    },
    {
      "lang": "en",
      "text": "of all these counts so",
      "offset": 1092.96,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "if we print n",
      "offset": 1095.039,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "this is the array but of course it looks",
      "offset": 1096.88,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "ugly so let's erase this ugly mess and",
      "offset": 1099.12,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "let's try to visualize it a bit more",
      "offset": 1101.84,
      "duration": 2.959
    },
    {
      "lang": "en",
      "text": "nicer",
      "offset": 1103.679,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "so for that we're going to use a library",
      "offset": 1104.799,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "called matplotlib",
      "offset": 1106.88,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "so matplotlib allows us to create",
      "offset": 1108.799,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "figures so we can do things like plt",
      "offset": 1110.32,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "item show of the counter array",
      "offset": 1112.48,
      "duration": 6.559
    },
    {
      "lang": "en",
      "text": "so this is the 28x28 array",
      "offset": 1116.08,
      "duration": 5.839
    },
    {
      "lang": "en",
      "text": "and this is structure but even this i",
      "offset": 1119.039,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "would say is still pretty ugly",
      "offset": 1121.919,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "so we're going to try to create a much",
      "offset": 1123.84,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "nicer visualization of it and i wrote a",
      "offset": 1125.679,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "bunch of code for that",
      "offset": 1127.52,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "the first thing we're going to need is",
      "offset": 1129.679,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "we're going to need to invert",
      "offset": 1131.84,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "this array here this dictionary so s2i",
      "offset": 1133.84,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "is mapping from s to i",
      "offset": 1137.36,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "and in i2s we're going to reverse this",
      "offset": 1139.76,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "dictionary so iterator of all the items",
      "offset": 1142.24,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "and just reverse that array",
      "offset": 1144.48,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "so i2s",
      "offset": 1146.559,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "maps inversely from 0 to a 1 to b etc",
      "offset": 1148.32,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "so we'll need that",
      "offset": 1152.559,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "and then here's the code that i came up",
      "offset": 1154.16,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "with to try to make this a little bit",
      "offset": 1156.16,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "nicer",
      "offset": 1157.6,
      "duration": 2.4
    },
    {
      "lang": "en",
      "text": "we create a figure",
      "offset": 1160.48,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "we plot",
      "offset": 1162,
      "duration": 2.4
    },
    {
      "lang": "en",
      "text": "n",
      "offset": 1163.36,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "and then we do and then we visualize a",
      "offset": 1164.4,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "bunch of things later let me just run it",
      "offset": 1166.08,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "so you get a sense of what this is",
      "offset": 1168,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "okay",
      "offset": 1171.919,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "so you see here that we have",
      "offset": 1172.96,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "the array spaced out",
      "offset": 1175.28,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "and every one of these is basically like",
      "offset": 1177.12,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "b follows g zero times",
      "offset": 1179.84,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "b follows h 41 times",
      "offset": 1182.32,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "um so a follows j 175 times",
      "offset": 1184.64,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "and so what you can see that i'm doing",
      "offset": 1187.84,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "here is first i show that entire array",
      "offset": 1189.28,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "and then i iterate over all the",
      "offset": 1192.88,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "individual little cells here",
      "offset": 1194.24,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "and i create a character string here",
      "offset": 1196.72,
      "duration": 5.839
    },
    {
      "lang": "en",
      "text": "which is the inverse mapping i2s of the",
      "offset": 1199.28,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "integer i and the integer j so those are",
      "offset": 1202.559,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "the bi-grams in a character",
      "offset": 1205.2,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "representation",
      "offset": 1206.72,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "and then i plot just the diagram text",
      "offset": 1208.559,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "and then i plot the number of times that",
      "offset": 1212,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "this bigram occurs",
      "offset": 1214.159,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "now the reason that there's a dot item",
      "offset": 1216,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "here is because when you index into",
      "offset": 1217.52,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "these arrays these are torch tensors",
      "offset": 1220,
      "duration": 5.919
    },
    {
      "lang": "en",
      "text": "you see that we still get a tensor back",
      "offset": 1222.96,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "so the type of this thing you'd think it",
      "offset": 1225.919,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "would be just an integer 149 but it's",
      "offset": 1228.08,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "actually a torch.tensor",
      "offset": 1229.919,
      "duration": 2.961
    },
    {
      "lang": "en",
      "text": "and so",
      "offset": 1231.919,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "if you do dot item then it will pop out",
      "offset": 1232.88,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "that in individual integer",
      "offset": 1235.36,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "so it will just be 149.",
      "offset": 1238.4,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "so that's what's happening there and",
      "offset": 1240.64,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "these are just some options to make it",
      "offset": 1242.48,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "look nice",
      "offset": 1243.76,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "so what is the structure of this array",
      "offset": 1245.2,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "we have all these counts and we see that",
      "offset": 1249.12,
      "duration": 3.039
    },
    {
      "lang": "en",
      "text": "some of them occur often and some of",
      "offset": 1250.48,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "them do not occur often",
      "offset": 1252.159,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "now if you scrutinize this carefully you",
      "offset": 1253.919,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "will notice that we're not actually",
      "offset": 1256,
      "duration": 2.559
    },
    {
      "lang": "en",
      "text": "being very clever",
      "offset": 1257.12,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "that's because when you come over here",
      "offset": 1258.559,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "you'll notice that for example we have",
      "offset": 1260.559,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "an entire row of completely zeros and",
      "offset": 1262.24,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "that's because the end character",
      "offset": 1264.72,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "is never possibly going to be the first",
      "offset": 1266.96,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "character of a bi-gram because we're",
      "offset": 1268.64,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "always placing these end tokens all at",
      "offset": 1270.159,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "the end of the diagram",
      "offset": 1272.4,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "similarly we have entire columns zeros",
      "offset": 1274.24,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "here because the s",
      "offset": 1276.799,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "character will never possibly be the",
      "offset": 1279.6,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "second element of a bigram because we",
      "offset": 1281.36,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "always start with s and we end with e",
      "offset": 1283.679,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "and we only have the words in between",
      "offset": 1285.6,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "so we have an entire column of zeros an",
      "offset": 1287.6,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "entire row of zeros and in this little",
      "offset": 1290.08,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "two by two matrix here as well the only",
      "offset": 1292.24,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "one that can possibly happen is if s",
      "offset": 1294.32,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "directly follows e",
      "offset": 1296.799,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "that can be non-zero if we have a word",
      "offset": 1298.48,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "that has no letters so in that case",
      "offset": 1301.36,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "there's no letters in the word it's an",
      "offset": 1303.6,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "empty word and we just have s follows e",
      "offset": 1304.88,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "but the other ones are just not possible",
      "offset": 1307.44,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "and so we're basically wasting space and",
      "offset": 1310.08,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "not only that but the s and the e are",
      "offset": 1311.679,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "getting very crowded here",
      "offset": 1313.679,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "i was using these brackets because",
      "offset": 1315.52,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "there's convention and natural language",
      "offset": 1317.44,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "processing to use these kinds of",
      "offset": 1318.96,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "brackets to denote special tokens",
      "offset": 1320.32,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "but we're going to use something else",
      "offset": 1323.12,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "so let's fix all this and make it",
      "offset": 1325.12,
      "duration": 3.039
    },
    {
      "lang": "en",
      "text": "prettier",
      "offset": 1326.88,
      "duration": 2.64
    },
    {
      "lang": "en",
      "text": "we're not actually going to have two",
      "offset": 1328.159,
      "duration": 2.961
    },
    {
      "lang": "en",
      "text": "special tokens we're only going to have",
      "offset": 1329.52,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "one special token",
      "offset": 1331.12,
      "duration": 2.559
    },
    {
      "lang": "en",
      "text": "so",
      "offset": 1332.96,
      "duration": 2.48
    },
    {
      "lang": "en",
      "text": "we're going to have n by n",
      "offset": 1333.679,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "array of 27 by 27 instead",
      "offset": 1335.44,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "instead of having two",
      "offset": 1338.799,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "we will just have one and i will call it",
      "offset": 1340.4,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "a dot",
      "offset": 1342.4,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "okay",
      "offset": 1344.64,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "let me swing this over here",
      "offset": 1347.28,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "now one more thing that i would like to",
      "offset": 1350.4,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "do is i would actually like to make this",
      "offset": 1351.679,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "special character half position zero",
      "offset": 1353.52,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "and i would like to offset all the other",
      "offset": 1356.159,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "letters off i find that a little bit",
      "offset": 1357.76,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "more",
      "offset": 1359.76,
      "duration": 2.799
    },
    {
      "lang": "en",
      "text": "pleasing",
      "offset": 1360.48,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "so",
      "offset": 1362.559,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "we need a plus one here so that the",
      "offset": 1364.64,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "first character which is a will start at",
      "offset": 1366.4,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "one",
      "offset": 1368.48,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "so s2i",
      "offset": 1369.76,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "will now be a starts at one and dot is 0",
      "offset": 1371.2,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "and",
      "offset": 1375.84,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "i2s of course we're not changing this",
      "offset": 1376.72,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "because i2s just creates a reverse",
      "offset": 1378.88,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "mapping and this will work fine so 1 is",
      "offset": 1380.48,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "a 2 is b",
      "offset": 1382.88,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "0 is dot",
      "offset": 1384.32,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "so we've reversed that here",
      "offset": 1386.48,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "we have",
      "offset": 1389.039,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "a dot and a dot",
      "offset": 1390.24,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "this should work fine",
      "offset": 1392.88,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "make sure i start at zeros",
      "offset": 1394.72,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "count",
      "offset": 1397.84,
      "duration": 2.719
    },
    {
      "lang": "en",
      "text": "and then here we don't go up to 28 we go",
      "offset": 1398.72,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "up to 27",
      "offset": 1400.559,
      "duration": 5.681
    },
    {
      "lang": "en",
      "text": "and this should just work",
      "offset": 1402.559,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "okay",
      "offset": 1410.799,
      "duration": 2.801
    },
    {
      "lang": "en",
      "text": "so we see that dot never happened it's",
      "offset": 1411.679,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "at zero because we don't have empty",
      "offset": 1413.6,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "words",
      "offset": 1415.36,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "then this row here now is just uh very",
      "offset": 1416.4,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "simply the um",
      "offset": 1418.96,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "counts for all the first letters so",
      "offset": 1420.72,
      "duration": 7.04
    },
    {
      "lang": "en",
      "text": "uh j starts a word h starts a word i",
      "offset": 1424.88,
      "duration": 5.279
    },
    {
      "lang": "en",
      "text": "starts a word etc and then these are all",
      "offset": 1427.76,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "the ending",
      "offset": 1430.159,
      "duration": 2.801
    },
    {
      "lang": "en",
      "text": "characters",
      "offset": 1431.76,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "and in between we have the structure of",
      "offset": 1432.96,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "what characters follow each other",
      "offset": 1434.64,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "so this is the counts array of our",
      "offset": 1436.96,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "entire",
      "offset": 1439.52,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "data set so this array actually has all",
      "offset": 1440.72,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "the information necessary for us to",
      "offset": 1443.279,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "actually sample from this bigram",
      "offset": 1444.96,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "uh character level language model",
      "offset": 1447.44,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "and um roughly speaking what we're going",
      "offset": 1449.6,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "to do is we're just going to start",
      "offset": 1451.76,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "following these probabilities and these",
      "offset": 1453.2,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "counts and we're going to start sampling",
      "offset": 1454.88,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "from the from the model",
      "offset": 1456.72,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "so in the beginning of course",
      "offset": 1458.72,
      "duration": 4.959
    },
    {
      "lang": "en",
      "text": "we start with the dot the start token",
      "offset": 1460.48,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "dot",
      "offset": 1463.679,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "so to sample the first character of a",
      "offset": 1464.559,
      "duration": 5.921
    },
    {
      "lang": "en",
      "text": "name we're looking at this row here",
      "offset": 1467.44,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "so we see that we have the counts and",
      "offset": 1470.48,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "those concepts terminally are telling us",
      "offset": 1472.64,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "how often any one of these characters is",
      "offset": 1474.559,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "to start a word",
      "offset": 1477.2,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "so if we take this n",
      "offset": 1479.52,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "and we grab the first row",
      "offset": 1481.84,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "we can do that by using just indexing as",
      "offset": 1484.72,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "zero",
      "offset": 1487.44,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "and then using this notation column for",
      "offset": 1488.32,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "the rest of that row",
      "offset": 1491.6,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "so n zero colon",
      "offset": 1493.6,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "is indexing into the zeroth",
      "offset": 1496.48,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "row and then it's grabbing all the",
      "offset": 1498.799,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "columns",
      "offset": 1500.559,
      "duration": 2.961
    },
    {
      "lang": "en",
      "text": "and so this will give us a",
      "offset": 1501.919,
      "duration": 3.281
    },
    {
      "lang": "en",
      "text": "one-dimensional array",
      "offset": 1503.52,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "of the first row so zero four four ten",
      "offset": 1505.2,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "you know zero four four ten one three oh",
      "offset": 1508.32,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "six one five four two etc it's just the",
      "offset": 1510.96,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "first row the shape of this",
      "offset": 1513.2,
      "duration": 6.64
    },
    {
      "lang": "en",
      "text": "is 27 it's just the row of 27",
      "offset": 1515.679,
      "duration": 5.441
    },
    {
      "lang": "en",
      "text": "and the other way that you can do this",
      "offset": 1519.84,
      "duration": 2.56
    },
    {
      "lang": "en",
      "text": "also is you just you don't need to",
      "offset": 1521.12,
      "duration": 2.559
    },
    {
      "lang": "en",
      "text": "actually give this",
      "offset": 1522.4,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "you just grab the zeroth row like this",
      "offset": 1523.679,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "this is equivalent",
      "offset": 1526.08,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "now these are the counts",
      "offset": 1528.08,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "and now what we'd like to do is we'd",
      "offset": 1529.919,
      "duration": 5.041
    },
    {
      "lang": "en",
      "text": "like to basically um sample from this",
      "offset": 1531.76,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "since these are the raw counts we",
      "offset": 1534.96,
      "duration": 2.319
    },
    {
      "lang": "en",
      "text": "actually have to convert this to",
      "offset": 1536.08,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "probabilities",
      "offset": 1537.279,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "so we create a probability vector",
      "offset": 1539.12,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "so we'll take n of zero",
      "offset": 1542.799,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "and we'll actually convert this to float",
      "offset": 1544.96,
      "duration": 4.959
    },
    {
      "lang": "en",
      "text": "first",
      "offset": 1548.159,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "okay so these integers are converted to",
      "offset": 1549.919,
      "duration": 2.801
    },
    {
      "lang": "en",
      "text": "float",
      "offset": 1551.76,
      "duration": 2.799
    },
    {
      "lang": "en",
      "text": "floating point numbers and the reason",
      "offset": 1552.72,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "we're creating floats is because we're",
      "offset": 1554.559,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "about to normalize these counts",
      "offset": 1556.24,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "so to create a probability distribution",
      "offset": 1558.799,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "here we want to divide",
      "offset": 1560.799,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "we basically want to do p p p divide p",
      "offset": 1563.76,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "that sum",
      "offset": 1565.919,
      "duration": 2.401
    },
    {
      "lang": "en",
      "text": "and now we get a vector of smaller",
      "offset": 1569.6,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "numbers and these are now probabilities",
      "offset": 1571.44,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "so of course because we divided by the",
      "offset": 1573.679,
      "duration": 5.041
    },
    {
      "lang": "en",
      "text": "sum the sum of p now is 1.",
      "offset": 1575.2,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "so this is a nice proper probability",
      "offset": 1578.72,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "distribution it sums to 1 and this is",
      "offset": 1580.4,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "giving us the probability for any single",
      "offset": 1582.559,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "character to be the first",
      "offset": 1584.32,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "character of a word",
      "offset": 1586,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "so now we can try to sample from this",
      "offset": 1587.919,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "distribution to sample from these",
      "offset": 1589.6,
      "duration": 3.4
    },
    {
      "lang": "en",
      "text": "distributions we're going to use",
      "offset": 1591.52,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "storch.multinomial which i've pulled up",
      "offset": 1593,
      "duration": 3.08
    },
    {
      "lang": "en",
      "text": "here",
      "offset": 1594.88,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "so torch.multinomial returns uh",
      "offset": 1596.08,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "samples from the multinomial probability",
      "offset": 1600.48,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "distribution which is a complicated way",
      "offset": 1602.24,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "of saying you give me probabilities and",
      "offset": 1604.32,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "i will give you integers which are",
      "offset": 1606.4,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "sampled",
      "offset": 1608.4,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "according to the property distribution",
      "offset": 1609.44,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "so this is the signature of the method",
      "offset": 1611.52,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "and to make everything deterministic",
      "offset": 1613.12,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "we're going to use a generator object in",
      "offset": 1614.72,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "pytorch",
      "offset": 1617.36,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "so this makes everything deterministic",
      "offset": 1619.2,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "so when you run this on your computer",
      "offset": 1620.88,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "you're going to the exact get the exact",
      "offset": 1622.48,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "same results that i'm getting here on my",
      "offset": 1624.08,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "computer",
      "offset": 1625.84,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "so let me show you how this works",
      "offset": 1627.2,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "here's the deterministic way of creating",
      "offset": 1632.64,
      "duration": 5.519
    },
    {
      "lang": "en",
      "text": "a torch generator object",
      "offset": 1635.36,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "seeding it with some number that we can",
      "offset": 1638.159,
      "duration": 3.041
    },
    {
      "lang": "en",
      "text": "agree on",
      "offset": 1640,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "so that seeds a generator gets gives us",
      "offset": 1641.2,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "an object g",
      "offset": 1643.6,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "and then we can pass that g",
      "offset": 1644.799,
      "duration": 3.521
    },
    {
      "lang": "en",
      "text": "to a function",
      "offset": 1646.48,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "that creates um",
      "offset": 1648.32,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "here random numbers twerk.rand creates",
      "offset": 1650.159,
      "duration": 5.041
    },
    {
      "lang": "en",
      "text": "random numbers three of them",
      "offset": 1652.64,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "and it's using this generator object to",
      "offset": 1655.2,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "as a source of randomness",
      "offset": 1657.52,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "so",
      "offset": 1660.399,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "without normalizing it",
      "offset": 1661.84,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "i can just print",
      "offset": 1664.24,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "this is sort of like numbers between 0",
      "offset": 1666.48,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "and 1 that are random according to this",
      "offset": 1668.399,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "thing and whenever i run it again",
      "offset": 1670.72,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "i'm always going to get the same result",
      "offset": 1673.039,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "because i keep using the same generator",
      "offset": 1674.559,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "object which i'm seeing here",
      "offset": 1676.24,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "and then if i divide",
      "offset": 1678.72,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "to normalize i'm going to get a nice",
      "offset": 1681.6,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "probability distribution of just three",
      "offset": 1684,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "elements",
      "offset": 1685.919,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "and then we can use torsion multinomial",
      "offset": 1687.52,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "to draw samples from it so this is what",
      "offset": 1689.279,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "that looks like",
      "offset": 1691.6,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "tertiary multinomial we'll take the",
      "offset": 1693.6,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "torch tensor",
      "offset": 1696.72,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "of probability distributions",
      "offset": 1698.32,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "then we can ask for a number of samples",
      "offset": 1700.96,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "let's say 20.",
      "offset": 1702.48,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "replacement equals true means that when",
      "offset": 1704.48,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "we draw an element",
      "offset": 1706.96,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "we will uh we can draw it and then we",
      "offset": 1708.799,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "can put it back into the list of",
      "offset": 1710.88,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "eligible indices to draw again",
      "offset": 1712.96,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "and we have to specify replacement as",
      "offset": 1715.76,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "true because by default uh for some",
      "offset": 1717.44,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "reason it's false",
      "offset": 1719.76,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "and i think",
      "offset": 1721.52,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "you know it's just something to be",
      "offset": 1722.96,
      "duration": 2.719
    },
    {
      "lang": "en",
      "text": "careful with",
      "offset": 1724.24,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "and the generator is passed in here so",
      "offset": 1725.679,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "we're going to always get deterministic",
      "offset": 1727.6,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "results the same results so if i run",
      "offset": 1729.279,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "these two",
      "offset": 1731.84,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "we're going to get a bunch of samples",
      "offset": 1733.679,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "from this distribution",
      "offset": 1735.279,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "now you'll notice here that the",
      "offset": 1737.12,
      "duration": 3.279
    },
    {
      "lang": "en",
      "text": "probability for the",
      "offset": 1738.88,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "first element in this tensor is 60",
      "offset": 1740.399,
      "duration": 7.841
    },
    {
      "lang": "en",
      "text": "so in these 20 samples we'd expect 60 of",
      "offset": 1744.48,
      "duration": 6.079
    },
    {
      "lang": "en",
      "text": "them to be zero",
      "offset": 1748.24,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "we'd expect thirty percent of them to be",
      "offset": 1750.559,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "one",
      "offset": 1752.799,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "and because the the element index two",
      "offset": 1754.159,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "has only ten percent probability very",
      "offset": 1757.44,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "few of these samples should be two and",
      "offset": 1759.679,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "indeed we only have a small number of",
      "offset": 1762.24,
      "duration": 3.039
    },
    {
      "lang": "en",
      "text": "twos",
      "offset": 1764.08,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "and we can sample as many as we'd like",
      "offset": 1765.279,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "and the more we sample the more",
      "offset": 1768.88,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "these numbers should um roughly have the",
      "offset": 1771.039,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "distribution here",
      "offset": 1773.279,
      "duration": 5.041
    },
    {
      "lang": "en",
      "text": "so we should have lots of zeros",
      "offset": 1775.84,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "half as many um",
      "offset": 1778.32,
      "duration": 6.56
    },
    {
      "lang": "en",
      "text": "ones and we should have um three times",
      "offset": 1781.6,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "as few",
      "offset": 1784.88,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "oh sorry s few ones and three times as",
      "offset": 1786.08,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "few uh",
      "offset": 1789.039,
      "duration": 2.64
    },
    {
      "lang": "en",
      "text": "twos",
      "offset": 1790.48,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "so you see that we have very few twos we",
      "offset": 1791.679,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "have some ones and most of them are zero",
      "offset": 1793.44,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "so that's what torsion multinomial is",
      "offset": 1795.679,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "doing",
      "offset": 1797.6,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "for us here",
      "offset": 1798.799,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "we are interested in this row we've",
      "offset": 1801.039,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "created this",
      "offset": 1802.559,
      "duration": 2.561
    },
    {
      "lang": "en",
      "text": "p here",
      "offset": 1805.6,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "and now we can sample from it",
      "offset": 1806.799,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "so if we use the same",
      "offset": 1809.6,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "seed",
      "offset": 1811.36,
      "duration": 3.039
    },
    {
      "lang": "en",
      "text": "and then we sample from this",
      "offset": 1812.799,
      "duration": 5.281
    },
    {
      "lang": "en",
      "text": "distribution let's just get one sample",
      "offset": 1814.399,
      "duration": 8.241
    },
    {
      "lang": "en",
      "text": "then we see that the sample is say 13.",
      "offset": 1818.08,
      "duration": 7.12
    },
    {
      "lang": "en",
      "text": "so this will be the index",
      "offset": 1822.64,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "and let's you see how it's a tensor that",
      "offset": 1825.2,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "wraps 13 we again have to use that item",
      "offset": 1827.36,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "to pop out that integer",
      "offset": 1830.399,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "and now index would be just the number",
      "offset": 1832.88,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "13.",
      "offset": 1835.279,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "and of course the um we can do",
      "offset": 1837.36,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "we can map the i2s of ix to figure out",
      "offset": 1840.159,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "exactly which character",
      "offset": 1843.52,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "we're sampling here we're sampling m",
      "offset": 1845.039,
      "duration": 5.041
    },
    {
      "lang": "en",
      "text": "so we're saying that the first character",
      "offset": 1848,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "is",
      "offset": 1850.08,
      "duration": 2.959
    },
    {
      "lang": "en",
      "text": "in our generation",
      "offset": 1851.12,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "and just looking at the road here",
      "offset": 1853.039,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "m was drawn and you we can see that m",
      "offset": 1855.12,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "actually starts a large number of words",
      "offset": 1857.2,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "uh m",
      "offset": 1859.679,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "started 2 500 words out of 32 000 words",
      "offset": 1861.039,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "so almost",
      "offset": 1864.64,
      "duration": 3.279
    },
    {
      "lang": "en",
      "text": "a bit less than 10 percent of the words",
      "offset": 1866.24,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "start with them so this was actually a",
      "offset": 1867.919,
      "duration": 5.281
    },
    {
      "lang": "en",
      "text": "fairly likely character to draw",
      "offset": 1869.76,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "um",
      "offset": 1873.2,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "so that would be the first character of",
      "offset": 1875.12,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "our work and now we can continue to",
      "offset": 1876.48,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "sample more characters because now we",
      "offset": 1878.08,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "know that m started",
      "offset": 1880.48,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "m is already sampled",
      "offset": 1882.88,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "so now to draw the next character we",
      "offset": 1884.64,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "will come back here and we will look for",
      "offset": 1886.88,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "the row",
      "offset": 1889.519,
      "duration": 3.121
    },
    {
      "lang": "en",
      "text": "that starts with m",
      "offset": 1890.88,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "so you see m",
      "offset": 1892.64,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "and we have a row here",
      "offset": 1894.559,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "so we see that m dot is",
      "offset": 1896.559,
      "duration": 6.641
    },
    {
      "lang": "en",
      "text": "516 m a is this many and b is this many",
      "offset": 1899.44,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "etc so these are the counts for the next",
      "offset": 1903.2,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "row and that's the next character that",
      "offset": 1904.96,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "we are going to now generate",
      "offset": 1906.559,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "so i think we are ready to actually just",
      "offset": 1908.559,
      "duration": 3.041
    },
    {
      "lang": "en",
      "text": "write out the loop because i think",
      "offset": 1910,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "you're starting to get a sense of how",
      "offset": 1911.6,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "this is going to go",
      "offset": 1912.72,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "the um",
      "offset": 1914.48,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "we always begin at",
      "offset": 1916.159,
      "duration": 6.161
    },
    {
      "lang": "en",
      "text": "index 0 because that's the start token",
      "offset": 1917.84,
      "duration": 7.04
    },
    {
      "lang": "en",
      "text": "and then while true",
      "offset": 1922.32,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "we're going to grab the row",
      "offset": 1924.88,
      "duration": 3.519
    },
    {
      "lang": "en",
      "text": "corresponding to index",
      "offset": 1926.08,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "that we're currently on so that's p",
      "offset": 1928.399,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "so that's n array at ix",
      "offset": 1931.12,
      "duration": 7.36
    },
    {
      "lang": "en",
      "text": "converted to float is rp",
      "offset": 1934.399,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "then we normalize",
      "offset": 1939.039,
      "duration": 5.601
    },
    {
      "lang": "en",
      "text": "this p to sum to one",
      "offset": 1941.2,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "i accidentally ran the infinite loop we",
      "offset": 1945.279,
      "duration": 5.441
    },
    {
      "lang": "en",
      "text": "normalize p to something one",
      "offset": 1948.24,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "then we need this generator object",
      "offset": 1950.72,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "now we're going to initialize up here",
      "offset": 1953.76,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "and we're going to draw a single sample",
      "offset": 1955.6,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "from this distribution",
      "offset": 1956.96,
      "duration": 2.959
    },
    {
      "lang": "en",
      "text": "and then this is going to tell us what",
      "offset": 1960.799,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "index is going to be next",
      "offset": 1962.799,
      "duration": 5.921
    },
    {
      "lang": "en",
      "text": "if the index sampled is",
      "offset": 1966.559,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "0",
      "offset": 1968.72,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "then that's now the end token",
      "offset": 1969.679,
      "duration": 5.681
    },
    {
      "lang": "en",
      "text": "so we will break",
      "offset": 1972.64,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "otherwise we are going to print",
      "offset": 1975.36,
      "duration": 5.919
    },
    {
      "lang": "en",
      "text": "s2i of ix",
      "offset": 1977.76,
      "duration": 3.519
    },
    {
      "lang": "en",
      "text": "i2s",
      "offset": 1982.24,
      "duration": 2.559
    },
    {
      "lang": "en",
      "text": "and uh that's pretty much it we're just",
      "offset": 1985.279,
      "duration": 6.801
    },
    {
      "lang": "en",
      "text": "uh this should work okay more",
      "offset": 1987.84,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "so that's that's the name that we've",
      "offset": 1992.08,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "sampled we started with m the next step",
      "offset": 1993.84,
      "duration": 7.28
    },
    {
      "lang": "en",
      "text": "was o then r and then dot",
      "offset": 1996.96,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "and this dot we it here as well",
      "offset": 2001.36,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "so",
      "offset": 2004.799,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "let's now do this a few times",
      "offset": 2006.399,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "so let's actually create an",
      "offset": 2009.919,
      "duration": 6.64
    },
    {
      "lang": "en",
      "text": "out list here",
      "offset": 2013.44,
      "duration": 3.119
    },
    {
      "lang": "en",
      "text": "and instead of printing we're going to",
      "offset": 2017.12,
      "duration": 2.559
    },
    {
      "lang": "en",
      "text": "append",
      "offset": 2018.559,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "so out that append this character",
      "offset": 2019.679,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "and then here let's just print it at the",
      "offset": 2024.399,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "end so let's just join up all the outs",
      "offset": 2026.48,
      "duration": 5.439
    },
    {
      "lang": "en",
      "text": "and we're just going to print more okay",
      "offset": 2029.36,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "now we're always getting the same result",
      "offset": 2031.919,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "because of the generator",
      "offset": 2033.36,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "so if we want to do this a few times we",
      "offset": 2035.039,
      "duration": 5.041
    },
    {
      "lang": "en",
      "text": "can go for i in range",
      "offset": 2037.039,
      "duration": 5.441
    },
    {
      "lang": "en",
      "text": "10 we can sample 10 names",
      "offset": 2040.08,
      "duration": 5.599
    },
    {
      "lang": "en",
      "text": "and we can just do that 10 times",
      "offset": 2042.48,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "and these are the names that we're",
      "offset": 2045.679,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "getting out",
      "offset": 2046.799,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "let's do 20.",
      "offset": 2048.399,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "i'll be honest with you this doesn't",
      "offset": 2054.24,
      "duration": 2.24
    },
    {
      "lang": "en",
      "text": "look right",
      "offset": 2055.44,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "so i started a few minutes to convince",
      "offset": 2056.48,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "myself that it actually is right",
      "offset": 2058.24,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "the reason these samples are so terrible",
      "offset": 2060.48,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "is that bigram language model",
      "offset": 2062.399,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "is actually look just like really",
      "offset": 2064.8,
      "duration": 2.879
    },
    {
      "lang": "en",
      "text": "terrible",
      "offset": 2066.48,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "we can generate a few more here",
      "offset": 2067.679,
      "duration": 3.281
    },
    {
      "lang": "en",
      "text": "and you can see that they're kind of",
      "offset": 2070,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "like their name like a little bit like",
      "offset": 2070.96,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "yanu o'reilly etc but they're just like",
      "offset": 2073.2,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "totally messed up um",
      "offset": 2076.159,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "and i mean the reason that this is so",
      "offset": 2078.56,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "bad like we're generating h as a name",
      "offset": 2080.48,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "but you have to think through",
      "offset": 2082.879,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "it from the model's eyes it doesn't know",
      "offset": 2084.48,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "that this h is the very first h all it",
      "offset": 2087.2,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "knows is that h was previously and now",
      "offset": 2089.76,
      "duration": 6.079
    },
    {
      "lang": "en",
      "text": "how likely is h the last character well",
      "offset": 2092.48,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "it's somewhat",
      "offset": 2095.839,
      "duration": 2.961
    },
    {
      "lang": "en",
      "text": "likely and so it just makes it last",
      "offset": 2097.04,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "character it doesn't know that there",
      "offset": 2098.8,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "were other things before it or there",
      "offset": 2100,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "were not other things before it and so",
      "offset": 2102,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "that's why it's generating all these",
      "offset": 2104.4,
      "duration": 2.08
    },
    {
      "lang": "en",
      "text": "like",
      "offset": 2105.599,
      "duration": 2.48
    },
    {
      "lang": "en",
      "text": "nonsense names",
      "offset": 2106.48,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "another way to do this is",
      "offset": 2108.079,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "to convince yourself that this is",
      "offset": 2111.92,
      "duration": 2.56
    },
    {
      "lang": "en",
      "text": "actually doing something reasonable even",
      "offset": 2112.96,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "though it's so terrible is",
      "offset": 2114.48,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "these little piece here are 27 right",
      "offset": 2117.68,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "like 27.",
      "offset": 2120.96,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "so how about if we did something like",
      "offset": 2123.04,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "this",
      "offset": 2124.32,
      "duration": 3.279
    },
    {
      "lang": "en",
      "text": "instead of p having any structure",
      "offset": 2126.24,
      "duration": 2.64
    },
    {
      "lang": "en",
      "text": "whatsoever",
      "offset": 2127.599,
      "duration": 3.121
    },
    {
      "lang": "en",
      "text": "how about if p was just",
      "offset": 2128.88,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "torch dot once",
      "offset": 2130.72,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "of 27",
      "offset": 2134.88,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "by default this is a float 32 so this is",
      "offset": 2137.2,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "fine divide 27",
      "offset": 2139.359,
      "duration": 5.601
    },
    {
      "lang": "en",
      "text": "so what i'm doing here is this is the",
      "offset": 2142.8,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "uniform distribution which will make",
      "offset": 2144.96,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "everything equally likely",
      "offset": 2147.119,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "and we can sample from that so let's see",
      "offset": 2149.839,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "if that does any better",
      "offset": 2152.32,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "okay so it's",
      "offset": 2154.16,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "this is what you have from a model that",
      "offset": 2155.92,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "is completely untrained where everything",
      "offset": 2157.44,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "is equally likely so it's obviously",
      "offset": 2159.119,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "garbage and then if we have a trained",
      "offset": 2161.44,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "model which is trained on just bi-grams",
      "offset": 2163.52,
      "duration": 5.599
    },
    {
      "lang": "en",
      "text": "this is what we get so you can see that",
      "offset": 2167.04,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "it is more name-like it is actually",
      "offset": 2169.119,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "working it's just um",
      "offset": 2171.04,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "my gram is so terrible and we have to do",
      "offset": 2174.079,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "better now next i would like to fix an",
      "offset": 2175.92,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "inefficiency that we have going on here",
      "offset": 2177.839,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "because what we're doing here is we're",
      "offset": 2180.16,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "always fetching a row of n from the",
      "offset": 2181.92,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "counts matrix up ahead",
      "offset": 2184.32,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "and then we're always doing the same",
      "offset": 2186.4,
      "duration": 2.56
    },
    {
      "lang": "en",
      "text": "things we're converting to float and",
      "offset": 2187.68,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "we're dividing and we're doing this",
      "offset": 2188.96,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "every single iteration of this loop and",
      "offset": 2190.8,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "we just keep renormalizing these rows",
      "offset": 2192.8,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "over and over again and it's extremely",
      "offset": 2194.32,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "inefficient and wasteful so what i'd",
      "offset": 2195.68,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "like to do is i'd like to actually",
      "offset": 2197.68,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "prepare a matrix capital p that will",
      "offset": 2198.96,
      "duration": 5.119
    },
    {
      "lang": "en",
      "text": "just have the probabilities in it so in",
      "offset": 2201.92,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "other words it's going to be the same as",
      "offset": 2204.079,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "the capital n matrix here of counts but",
      "offset": 2205.599,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "every single row will have the row of",
      "offset": 2208.079,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "probabilities uh that is normalized to 1",
      "offset": 2210.079,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "indicating the probability distribution",
      "offset": 2212.88,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "for the next character given the",
      "offset": 2214.4,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "character before it",
      "offset": 2216.56,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "um as defined by which row we're in",
      "offset": 2217.92,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "so basically what we'd like to do is",
      "offset": 2221.52,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "we'd like to just do it up front here",
      "offset": 2223.119,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "and then we would like to just use that",
      "offset": 2224.96,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "row here so here we would like to just",
      "offset": 2226.48,
      "duration": 6.4
    },
    {
      "lang": "en",
      "text": "do p equals p of ix instead",
      "offset": 2229.52,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "okay",
      "offset": 2232.88,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "the other reason i want to do this is",
      "offset": 2234.88,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "not just for efficiency but also i would",
      "offset": 2236,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "like us to practice",
      "offset": 2237.76,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "these n-dimensional tensors and i'd like",
      "offset": 2239.359,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "us to practice their manipulation and",
      "offset": 2241.52,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "especially something that's called",
      "offset": 2243.599,
      "duration": 2.401
    },
    {
      "lang": "en",
      "text": "broadcasting that we'll go into in a",
      "offset": 2244.56,
      "duration": 2.32
    },
    {
      "lang": "en",
      "text": "second",
      "offset": 2246,
      "duration": 2.16
    },
    {
      "lang": "en",
      "text": "we're actually going to have to become",
      "offset": 2246.88,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "very good at these tensor manipulations",
      "offset": 2248.16,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "because if we're going to build out all",
      "offset": 2250.48,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "the way to transformers we're going to",
      "offset": 2252,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "be doing some pretty complicated um",
      "offset": 2253.52,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "array operations for efficiency and we",
      "offset": 2255.359,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "need to really understand that and be",
      "offset": 2257.92,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "very good at it",
      "offset": 2259.44,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "so intuitively what we want to do is we",
      "offset": 2262.079,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "first want to grab the floating point",
      "offset": 2263.76,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "copy of n",
      "offset": 2265.839,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "and i'm mimicking the line here",
      "offset": 2268.16,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "basically",
      "offset": 2269.359,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "and then we want to divide all the rows",
      "offset": 2270.88,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "so that they sum to 1.",
      "offset": 2273.2,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "so we'd like to do something like this p",
      "offset": 2275.599,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "divide p dot sum",
      "offset": 2277.359,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "but",
      "offset": 2280.56,
      "duration": 2.24
    },
    {
      "lang": "en",
      "text": "now we have to be careful",
      "offset": 2281.28,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "because p dot sum actually",
      "offset": 2282.8,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "produces a sum",
      "offset": 2285.28,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "sorry equals and that float copy",
      "offset": 2288,
      "duration": 6.96
    },
    {
      "lang": "en",
      "text": "p dot sum produces a um",
      "offset": 2290.64,
      "duration": 6.32
    },
    {
      "lang": "en",
      "text": "sums up all of the counts of this entire",
      "offset": 2294.96,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "matrix n and gives us a single number of",
      "offset": 2296.96,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "just the summation of everything so",
      "offset": 2299.52,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "that's not the way we want to define",
      "offset": 2301.359,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "divide we want to simultaneously and in",
      "offset": 2302.8,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "parallel divide all the rows",
      "offset": 2305.2,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "by their respective sums",
      "offset": 2307.599,
      "duration": 5.041
    },
    {
      "lang": "en",
      "text": "so what we have to do now is we have to",
      "offset": 2310.56,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "go into documentation for torch.sum",
      "offset": 2312.64,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "and we can scroll down here to a",
      "offset": 2315.76,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "definition that is relevant to us which",
      "offset": 2317.28,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "is where we don't only provide an input",
      "offset": 2318.96,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "array that we want to sum but we also",
      "offset": 2321.68,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "provide the dimension along which we",
      "offset": 2323.839,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "want to sum",
      "offset": 2325.76,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "and in particular we want to sum up",
      "offset": 2327.2,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "over rows",
      "offset": 2329.839,
      "duration": 2.481
    },
    {
      "lang": "en",
      "text": "right",
      "offset": 2331.04,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "now one more argument that i want you to",
      "offset": 2332.32,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "pay attention to here is the keep them",
      "offset": 2333.76,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "is false",
      "offset": 2336.24,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "if keep them is true then the output",
      "offset": 2337.839,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "tensor is of the same size as input",
      "offset": 2340.24,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "except of course the dimension along",
      "offset": 2342.4,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "which is summed which will become just",
      "offset": 2343.76,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "one",
      "offset": 2345.839,
      "duration": 6.161
    },
    {
      "lang": "en",
      "text": "but if you pass in keep them as false",
      "offset": 2347.28,
      "duration": 7.2
    },
    {
      "lang": "en",
      "text": "then this dimension is squeezed out and",
      "offset": 2352,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "so torch.sum not only does the sum and",
      "offset": 2354.48,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "collapses dimension to be of size one",
      "offset": 2356.8,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "but in addition it does what's called a",
      "offset": 2358.8,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "squeeze where it squeezes out it",
      "offset": 2360.4,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "squeezes out that dimension",
      "offset": 2362.16,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "so",
      "offset": 2364.56,
      "duration": 2.48
    },
    {
      "lang": "en",
      "text": "basically what we want here is we",
      "offset": 2365.76,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "instead want to do p dot sum of some",
      "offset": 2367.04,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "axis",
      "offset": 2369.04,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "and in particular notice that p dot",
      "offset": 2370.72,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "shape is 27 by 27",
      "offset": 2372.48,
      "duration": 5.359
    },
    {
      "lang": "en",
      "text": "so when we sum up across axis zero then",
      "offset": 2375.52,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "we would be taking the zeroth dimension",
      "offset": 2377.839,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "and we would be summing across it",
      "offset": 2380,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "so when keep them as true",
      "offset": 2382.56,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "then this thing will not only give us",
      "offset": 2384.96,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "the counts across um",
      "offset": 2386.96,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "along the columns",
      "offset": 2390,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "but notice that basically the shape of",
      "offset": 2391.839,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "this is 1 by 27 we just get a row vector",
      "offset": 2393.599,
      "duration": 5.361
    },
    {
      "lang": "en",
      "text": "and the reason we get a row vector here",
      "offset": 2397.359,
      "duration": 2.881
    },
    {
      "lang": "en",
      "text": "again is because we passed in zero",
      "offset": 2398.96,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "dimension so this zero dimension becomes",
      "offset": 2400.24,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "one and we've done a sum",
      "offset": 2402.48,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "and we get a row and so basically we've",
      "offset": 2404.72,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "done the sum",
      "offset": 2406.64,
      "duration": 2.64
    },
    {
      "lang": "en",
      "text": "this way",
      "offset": 2408.079,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "vertically and arrived at just a single",
      "offset": 2409.28,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "1 by 27",
      "offset": 2411.119,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "vector of counts",
      "offset": 2412.88,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "what happens when you take out keep them",
      "offset": 2415.2,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "is that we just get 27. so it squeezes",
      "offset": 2417.68,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "out that dimension and we just get",
      "offset": 2420.24,
      "duration": 7.599
    },
    {
      "lang": "en",
      "text": "a one-dimensional vector of size 27.",
      "offset": 2423.52,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "now we don't actually want",
      "offset": 2428.56,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "one by 27 row vector because that gives",
      "offset": 2431.28,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "us the counts or the sums across",
      "offset": 2433.68,
      "duration": 5.679
    },
    {
      "lang": "en",
      "text": "the columns",
      "offset": 2437.44,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "we actually want to sum the other way",
      "offset": 2439.359,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "along dimension one and you'll see that",
      "offset": 2441.119,
      "duration": 5.121
    },
    {
      "lang": "en",
      "text": "the shape of this is 27 by one so it's a",
      "offset": 2443.28,
      "duration": 6.559
    },
    {
      "lang": "en",
      "text": "column vector it's a 27 by one",
      "offset": 2446.24,
      "duration": 6.56
    },
    {
      "lang": "en",
      "text": "vector of counts",
      "offset": 2449.839,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "okay",
      "offset": 2452.8,
      "duration": 2.319
    },
    {
      "lang": "en",
      "text": "and that's because what's happened here",
      "offset": 2453.839,
      "duration": 3.121
    },
    {
      "lang": "en",
      "text": "is that we're going horizontally and",
      "offset": 2455.119,
      "duration": 6.321
    },
    {
      "lang": "en",
      "text": "this 27 by 27 matrix becomes a 27 by 1",
      "offset": 2456.96,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "array",
      "offset": 2461.44,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "now you'll notice by the way that um the",
      "offset": 2463.44,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "actual numbers",
      "offset": 2466.319,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "of these counts are identical",
      "offset": 2468.079,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "and that's because this special array of",
      "offset": 2470.48,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "counts here comes from bi-gram",
      "offset": 2472.56,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "statistics and actually it just so",
      "offset": 2473.92,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "happens by chance",
      "offset": 2475.76,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "or because of the way this array is",
      "offset": 2477.68,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "constructed that the sums along the",
      "offset": 2479.2,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "columns or along the rows horizontally",
      "offset": 2481.04,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "or vertically is identical",
      "offset": 2483.44,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "but actually what we want to do in this",
      "offset": 2486.079,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "case is we want to sum across the",
      "offset": 2487.359,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "rows",
      "offset": 2490.079,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "horizontally so what we want here is p",
      "offset": 2491.119,
      "duration": 6.081
    },
    {
      "lang": "en",
      "text": "that sum of one with keep in true",
      "offset": 2493.839,
      "duration": 5.601
    },
    {
      "lang": "en",
      "text": "27 by one column vector",
      "offset": 2497.2,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "and now what we want to do is we want to",
      "offset": 2499.44,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "divide by that",
      "offset": 2500.96,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "now we have to be careful here again is",
      "offset": 2504.56,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "it possible to take",
      "offset": 2506.319,
      "duration": 5.361
    },
    {
      "lang": "en",
      "text": "what's a um p dot shape you see here 27",
      "offset": 2508.8,
      "duration": 6.4
    },
    {
      "lang": "en",
      "text": "by 27 is it possible to take a 27 by 27",
      "offset": 2511.68,
      "duration": 7.52
    },
    {
      "lang": "en",
      "text": "array and divide it by what is a 27 by 1",
      "offset": 2515.2,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "array",
      "offset": 2519.2,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "is that an operation that you can do",
      "offset": 2521.28,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "and whether or not you can perform this",
      "offset": 2523.76,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "operation is determined by what's called",
      "offset": 2525.359,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "broadcasting rules so if you just search",
      "offset": 2526.8,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "broadcasting semantics in torch",
      "offset": 2529.52,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "you'll notice that there's a special",
      "offset": 2531.92,
      "duration": 2.399
    },
    {
      "lang": "en",
      "text": "definition for",
      "offset": 2532.96,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "what's called broadcasting that uh for",
      "offset": 2534.319,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "whether or not um these two uh arrays",
      "offset": 2536.88,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "can be combined in a binary operation",
      "offset": 2540.079,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "like division",
      "offset": 2542.079,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "so the first condition is each tensor",
      "offset": 2543.839,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "has at least one dimension which is the",
      "offset": 2545.52,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "case for us",
      "offset": 2547.2,
      "duration": 2.399
    },
    {
      "lang": "en",
      "text": "and then when iterating over the",
      "offset": 2548.48,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "dimension sizes starting at the trailing",
      "offset": 2549.599,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "dimension",
      "offset": 2551.28,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "the dimension sizes must either be equal",
      "offset": 2552.319,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "one of them is one or one of them does",
      "offset": 2554.56,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "not exist",
      "offset": 2556.24,
      "duration": 2.64
    },
    {
      "lang": "en",
      "text": "okay",
      "offset": 2558,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "so let's do that we need to align the",
      "offset": 2558.88,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "two arrays and their shapes which is",
      "offset": 2561.28,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "very easy because both of these shapes",
      "offset": 2564.24,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "have two elements so they're aligned",
      "offset": 2565.68,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "then we iterate over from the from the",
      "offset": 2568,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "right and going to the left",
      "offset": 2570.16,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "each dimension must be either equal one",
      "offset": 2572.24,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "of them is a one or one of them does not",
      "offset": 2575.44,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "exist so in this case they're not equal",
      "offset": 2577.04,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "but one of them is a one so this is fine",
      "offset": 2579.2,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "and then this dimension they're both",
      "offset": 2581.839,
      "duration": 2.161
    },
    {
      "lang": "en",
      "text": "equal",
      "offset": 2583.2,
      "duration": 2.639
    },
    {
      "lang": "en",
      "text": "so uh this is fine",
      "offset": 2584,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "so all the dimensions are fine and",
      "offset": 2585.839,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "therefore the this operation is",
      "offset": 2588.48,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "broadcastable so that means that this",
      "offset": 2590.839,
      "duration": 3.641
    },
    {
      "lang": "en",
      "text": "operation is allowed",
      "offset": 2592.8,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "and what is it that these arrays do when",
      "offset": 2594.48,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "you divide 27 by 27 by 27 by one",
      "offset": 2596.8,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "what it does is that it takes this",
      "offset": 2599.76,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "dimension one and it stretches it out it",
      "offset": 2601.68,
      "duration": 5.439
    },
    {
      "lang": "en",
      "text": "copies it to match",
      "offset": 2604.16,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "27 here in this case",
      "offset": 2607.119,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "so in our case it takes this column",
      "offset": 2608.96,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "vector which is 27 by 1",
      "offset": 2610.72,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "and it copies it 27 times",
      "offset": 2612.8,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "to make",
      "offset": 2616.64,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "these both be 27 by 27 internally you",
      "offset": 2617.76,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "can think of it that way and so it",
      "offset": 2620.56,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "copies those counts",
      "offset": 2622.16,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "and then it does an element-wise",
      "offset": 2624.16,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "division",
      "offset": 2625.52,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "which is what we want because these",
      "offset": 2627.28,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "counts we want to divide by them on",
      "offset": 2628.72,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "every single one of these columns in",
      "offset": 2630.72,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "this matrix",
      "offset": 2632.96,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "so this actually we expect will",
      "offset": 2634.72,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "normalize",
      "offset": 2636.48,
      "duration": 3.119
    },
    {
      "lang": "en",
      "text": "every single row",
      "offset": 2637.68,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "and we can check that this is true by",
      "offset": 2639.599,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "taking the first row for example and",
      "offset": 2641.599,
      "duration": 5.24
    },
    {
      "lang": "en",
      "text": "taking its sum we expect this to be",
      "offset": 2644.079,
      "duration": 6.161
    },
    {
      "lang": "en",
      "text": "1. because it's not normalized",
      "offset": 2646.839,
      "duration": 6.201
    },
    {
      "lang": "en",
      "text": "and then we expect this now because if",
      "offset": 2650.24,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "we actually correctly normalize all the",
      "offset": 2653.04,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "rows we expect to get the exact same",
      "offset": 2654.72,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "result here so let's run this",
      "offset": 2656.88,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "it's the exact same result",
      "offset": 2659.119,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "this is correct so now i would like to",
      "offset": 2661.76,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "scare you a little bit",
      "offset": 2663.68,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "uh you actually have to like i basically",
      "offset": 2665.2,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "encourage you very strongly to read",
      "offset": 2667.2,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "through broadcasting semantics",
      "offset": 2668.56,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "and i encourage you to treat this with",
      "offset": 2670.4,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "respect and it's not something to play",
      "offset": 2671.76,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "fast and loose with it's something to",
      "offset": 2674.24,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "really respect really understand and",
      "offset": 2675.76,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "look up maybe some tutorials for",
      "offset": 2677.599,
      "duration": 2.881
    },
    {
      "lang": "en",
      "text": "broadcasting and practice it and be",
      "offset": 2678.96,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "careful with it because you can very",
      "offset": 2680.48,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "quickly run into books let me show you",
      "offset": 2682.319,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "what i mean",
      "offset": 2684.16,
      "duration": 2.48
    },
    {
      "lang": "en",
      "text": "you see how here we have p dot sum of",
      "offset": 2687.119,
      "duration": 3.281
    },
    {
      "lang": "en",
      "text": "one keep them as true",
      "offset": 2688.64,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "the shape of this is 27 by one let me",
      "offset": 2690.4,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "take out this line just so we have the n",
      "offset": 2693.2,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "and then we can see the counts",
      "offset": 2695.599,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "we can see that this is a all the counts",
      "offset": 2698.4,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "across all the",
      "offset": 2700.56,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "rows",
      "offset": 2702.48,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "and it's a 27 by one column vector right",
      "offset": 2703.44,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "now suppose that i tried to do the",
      "offset": 2707.04,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "following",
      "offset": 2709.44,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "but i erase keep them just true here",
      "offset": 2710.4,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "what does that do if keep them is not",
      "offset": 2713.839,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "true it's false then remember according",
      "offset": 2715.92,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "to documentation it gets rid of this",
      "offset": 2718.079,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "dimension one it squeezes it out so",
      "offset": 2720.24,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "basically we just get all the same",
      "offset": 2723.04,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "counts the same result except the shape",
      "offset": 2724.8,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "of it is not 27 by 1 it is just 27 the",
      "offset": 2727.2,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "one disappears",
      "offset": 2730,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "but all the counts are the same",
      "offset": 2731.599,
      "duration": 6.321
    },
    {
      "lang": "en",
      "text": "so you'd think that this divide that",
      "offset": 2734.16,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "would uh would work",
      "offset": 2737.92,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "first of all can we even uh write this",
      "offset": 2740,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "and will it is it even is it even",
      "offset": 2742.079,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "expected to run is it broadcastable",
      "offset": 2744.24,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "let's determine if this result is",
      "offset": 2746.24,
      "duration": 2.879
    },
    {
      "lang": "en",
      "text": "broadcastable",
      "offset": 2747.599,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "p.summit one is shape",
      "offset": 2749.119,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "is 27.",
      "offset": 2751.52,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "this is 27 by 27. so 27 by 27",
      "offset": 2752.8,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "broadcasting into 27. so now",
      "offset": 2757.599,
      "duration": 6.081
    },
    {
      "lang": "en",
      "text": "rules of broadcasting number one align",
      "offset": 2761.599,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "all the dimensions on the right done now",
      "offset": 2763.68,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "iteration over all the dimensions",
      "offset": 2766.4,
      "duration": 2.719
    },
    {
      "lang": "en",
      "text": "starting from the right going to the",
      "offset": 2767.76,
      "duration": 2.319
    },
    {
      "lang": "en",
      "text": "left",
      "offset": 2769.119,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "all the dimensions must either be equal",
      "offset": 2770.079,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "one of them must be one or one that does",
      "offset": 2772.96,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "not exist so here they are all equal",
      "offset": 2774.96,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "here the dimension does not exist",
      "offset": 2777.68,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "so internally what broadcasting will do",
      "offset": 2779.92,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "is it will create a one here",
      "offset": 2781.44,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "and then",
      "offset": 2784.24,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "we see that one of them is a one and",
      "offset": 2785.839,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "this will get copied and this will run",
      "offset": 2787.76,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "this will broadcast",
      "offset": 2790.24,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "okay so you'd expect this",
      "offset": 2792.4,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "to work",
      "offset": 2794.8,
      "duration": 2.4
    },
    {
      "lang": "en",
      "text": "because we we are",
      "offset": 2797.28,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "this broadcast and this we can divide",
      "offset": 2801.2,
      "duration": 2.32
    },
    {
      "lang": "en",
      "text": "this",
      "offset": 2802.8,
      "duration": 2.319
    },
    {
      "lang": "en",
      "text": "now if i run this you'd expect it to",
      "offset": 2803.52,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "work but",
      "offset": 2805.119,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "it doesn't",
      "offset": 2806.64,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "uh you actually get garbage you get a",
      "offset": 2807.839,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "wrong dissolve because this is actually",
      "offset": 2809.68,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "a bug",
      "offset": 2811.359,
      "duration": 5.321
    },
    {
      "lang": "en",
      "text": "this keep them equals true",
      "offset": 2812.4,
      "duration": 4.28
    },
    {
      "lang": "en",
      "text": "makes it work",
      "offset": 2817.119,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "this is a bug",
      "offset": 2820.56,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "in both cases we are doing",
      "offset": 2822.8,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "the correct counts we are summing up",
      "offset": 2824.72,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "across the rows",
      "offset": 2827.28,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "but keep them is saving us and making it",
      "offset": 2829.2,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "work so in this case",
      "offset": 2830.88,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "i'd like to encourage you to potentially",
      "offset": 2832.64,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "like pause this video at this point and",
      "offset": 2834.24,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "try to think about why this is buggy and",
      "offset": 2835.76,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "why the keep dim was necessary here",
      "offset": 2838.4,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "okay",
      "offset": 2842.16,
      "duration": 2.24
    },
    {
      "lang": "en",
      "text": "so the reason to do",
      "offset": 2842.96,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "for this is i'm trying to hint it here",
      "offset": 2844.4,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "when i was sort of giving you a bit of a",
      "offset": 2846.559,
      "duration": 2.881
    },
    {
      "lang": "en",
      "text": "hint on how this works",
      "offset": 2847.839,
      "duration": 2.48
    },
    {
      "lang": "en",
      "text": "this",
      "offset": 2849.44,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "27 vector",
      "offset": 2850.319,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "internally inside the broadcasting this",
      "offset": 2852.16,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "becomes a 1 by 27",
      "offset": 2854.4,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "and 1 by 27 is a row vector right",
      "offset": 2856.4,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "and now we are dividing 27 by 27 by 1 by",
      "offset": 2859.52,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "27",
      "offset": 2862,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "and torch will replicate this dimension",
      "offset": 2863.119,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "so basically",
      "offset": 2865.839,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "uh it will take",
      "offset": 2867.599,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "it will take this",
      "offset": 2869.68,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "row vector and it will copy it",
      "offset": 2871.04,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "vertically now",
      "offset": 2873.599,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "27 times so the 27 by 27 lies exactly",
      "offset": 2875.28,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "and element wise divides",
      "offset": 2877.92,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "and so basically what's happening here",
      "offset": 2880.24,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "is",
      "offset": 2882.16,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "we're actually normalizing the columns",
      "offset": 2884.319,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "instead of normalizing the rows",
      "offset": 2886.079,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "so you can check that what's happening",
      "offset": 2889.359,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "here is that p at zero which is the",
      "offset": 2891.359,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "first row of p dot sum",
      "offset": 2893.599,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "is not one it's seven",
      "offset": 2896.24,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "it is the first column as an example",
      "offset": 2898.559,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "that sums to one",
      "offset": 2900.72,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "so",
      "offset": 2903.599,
      "duration": 2.641
    },
    {
      "lang": "en",
      "text": "to summarize where does the issue come",
      "offset": 2904.48,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "from the issue comes from the silent",
      "offset": 2906.24,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "adding of a dimension here because in",
      "offset": 2908.079,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "broadcasting rules you align on the",
      "offset": 2910.079,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "right and go from right to left and if",
      "offset": 2911.92,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "dimension doesn't exist you create it",
      "offset": 2914,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "so that's where the problem happens we",
      "offset": 2916.079,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "still did the counts correctly we did",
      "offset": 2918,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "the counts across the rows and we got",
      "offset": 2919.599,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "the the counts on the right here as a",
      "offset": 2921.68,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "column vector but because the keep",
      "offset": 2924.559,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "things was true this this uh this",
      "offset": 2926.24,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "dimension was discarded and now we just",
      "offset": 2928.319,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "have a vector of 27. and because of",
      "offset": 2929.839,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "broadcasting the way it works this",
      "offset": 2932.4,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "vector of 27 suddenly becomes a row",
      "offset": 2934.079,
      "duration": 2.881
    },
    {
      "lang": "en",
      "text": "vector",
      "offset": 2936,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "and then this row vector gets replicated",
      "offset": 2936.96,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "vertically and that every single point",
      "offset": 2938.88,
      "duration": 6.4
    },
    {
      "lang": "en",
      "text": "we are dividing by the by the count",
      "offset": 2940.96,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "in the opposite direction",
      "offset": 2945.44,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "so uh",
      "offset": 2947.28,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "so this thing just uh doesn't work this",
      "offset": 2948.8,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "needs to be keep things equal true in",
      "offset": 2951.599,
      "duration": 2.561
    },
    {
      "lang": "en",
      "text": "this case",
      "offset": 2952.96,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "so then",
      "offset": 2954.16,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "then we have that p at zero is",
      "offset": 2956.319,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "normalized",
      "offset": 2957.839,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "and conversely the first column you'd",
      "offset": 2959.839,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "expect to potentially not be normalized",
      "offset": 2961.68,
      "duration": 5.919
    },
    {
      "lang": "en",
      "text": "and this is what makes it work",
      "offset": 2964.559,
      "duration": 6.481
    },
    {
      "lang": "en",
      "text": "so pretty subtle and uh hopefully this",
      "offset": 2967.599,
      "duration": 5.441
    },
    {
      "lang": "en",
      "text": "helps to scare you that you should have",
      "offset": 2971.04,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "a respect for broadcasting be careful",
      "offset": 2973.04,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "check your work uh and uh understand how",
      "offset": 2974.96,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "it works under the hood and make sure",
      "offset": 2977.839,
      "duration": 2.561
    },
    {
      "lang": "en",
      "text": "that it's broadcasting in the direction",
      "offset": 2979.04,
      "duration": 2.799
    },
    {
      "lang": "en",
      "text": "that you like otherwise you're going to",
      "offset": 2980.4,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "introduce very subtle bugs very hard to",
      "offset": 2981.839,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "find bugs and uh just be careful one",
      "offset": 2984.079,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "more note on efficiency we don't want to",
      "offset": 2986.72,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "be doing this here because this creates",
      "offset": 2988.72,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "a completely new tensor that we store",
      "offset": 2991.44,
      "duration": 2.879
    },
    {
      "lang": "en",
      "text": "into p",
      "offset": 2993.28,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "we prefer to use in place operations if",
      "offset": 2994.319,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "possible",
      "offset": 2996.4,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "so this would be an in-place operation",
      "offset": 2997.76,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "it has the potential to be faster it",
      "offset": 3000,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "doesn't create new memory",
      "offset": 3001.839,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "under the hood and then let's erase this",
      "offset": 3003.44,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "we don't need it",
      "offset": 3006.16,
      "duration": 2.959
    },
    {
      "lang": "en",
      "text": "and let's",
      "offset": 3007.92,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "also",
      "offset": 3009.119,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "um just do fewer just so i'm not wasting",
      "offset": 3010.88,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "space",
      "offset": 3013.599,
      "duration": 2.321
    },
    {
      "lang": "en",
      "text": "okay so we're actually in a pretty good",
      "offset": 3014.559,
      "duration": 2.401
    },
    {
      "lang": "en",
      "text": "spot now",
      "offset": 3015.92,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "we trained a bigram language model and",
      "offset": 3016.96,
      "duration": 5.119
    },
    {
      "lang": "en",
      "text": "we trained it really just by counting uh",
      "offset": 3019.28,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "how frequently any pairing occurs and",
      "offset": 3022.079,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "then normalizing so that we get a nice",
      "offset": 3024.4,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "property distribution",
      "offset": 3026.319,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "so really these elements of this array p",
      "offset": 3027.839,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "are really the parameters of our biogram",
      "offset": 3031.04,
      "duration": 3.519
    },
    {
      "lang": "en",
      "text": "language model giving us and summarizing",
      "offset": 3032.72,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "the statistics of these bigrams",
      "offset": 3034.559,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "so we train the model and then we know",
      "offset": 3036.88,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "how to sample from a model we just",
      "offset": 3038.64,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "iteratively uh sample the next character",
      "offset": 3040.48,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "and feed it in each time and get a next",
      "offset": 3043.359,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "character",
      "offset": 3045.52,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "now what i'd like to do is i'd like to",
      "offset": 3046.96,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "somehow evaluate the quality of this",
      "offset": 3048.48,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "model we'd like to somehow summarize the",
      "offset": 3050.319,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "quality of this model into a single",
      "offset": 3052.96,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "number how good is it at predicting",
      "offset": 3054.72,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "the training set",
      "offset": 3057.44,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "and as an example so in the training set",
      "offset": 3058.96,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "we can evaluate now the training loss",
      "offset": 3060.96,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "and this training loss is telling us",
      "offset": 3064.16,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "about",
      "offset": 3065.92,
      "duration": 2.24
    },
    {
      "lang": "en",
      "text": "sort of the quality of this model in a",
      "offset": 3066.88,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "single number just like we saw in",
      "offset": 3068.16,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "micrograd",
      "offset": 3070,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "so let's try to think through the",
      "offset": 3071.839,
      "duration": 2.641
    },
    {
      "lang": "en",
      "text": "quality of the model and how we would",
      "offset": 3073.359,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "evaluate it",
      "offset": 3074.48,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "basically what we're going to do is",
      "offset": 3076.96,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "we're going to copy paste this code",
      "offset": 3078.079,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "that we previously used for counting",
      "offset": 3080.559,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "okay",
      "offset": 3082.88,
      "duration": 2.719
    },
    {
      "lang": "en",
      "text": "and let me just print these diagrams",
      "offset": 3084.079,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "first we're gonna use f strings",
      "offset": 3085.599,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "and i'm gonna print character one",
      "offset": 3087.76,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "followed by character two these are the",
      "offset": 3089.68,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "diagrams and then i don't wanna do it",
      "offset": 3090.96,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "for all the words just do the first",
      "offset": 3092.88,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "three words so here we have emma olivia",
      "offset": 3094.16,
      "duration": 5.919
    },
    {
      "lang": "en",
      "text": "and ava bigrams",
      "offset": 3097.599,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "now what we'd like to do is we'd like to",
      "offset": 3100.079,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "basically look at the probability that",
      "offset": 3101.76,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "the model assigns to every one of these",
      "offset": 3104.64,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "diagrams",
      "offset": 3106.72,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "so in other words we can look at the",
      "offset": 3108.079,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "probability which is",
      "offset": 3109.44,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "summarized in the matrix b",
      "offset": 3111.119,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "of i x 1 x 2",
      "offset": 3112.96,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "and then we can print it here",
      "offset": 3116,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "as probability",
      "offset": 3117.92,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "and because these properties are way too",
      "offset": 3120.64,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "large let me present",
      "offset": 3122,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "or call in 0.4 f",
      "offset": 3124.72,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "to like truncate it a bit",
      "offset": 3126.72,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "so what do we have here right we're",
      "offset": 3129.04,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "looking at the probabilities that the",
      "offset": 3130.319,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "model assigns to every one of these",
      "offset": 3131.76,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "bigrams in the dataset",
      "offset": 3133.119,
      "duration": 3.521
    },
    {
      "lang": "en",
      "text": "and so we can see some of them are four",
      "offset": 3135.119,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "percent three percent etc",
      "offset": 3136.64,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "just to have a measuring stick in our",
      "offset": 3138.559,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "mind by the way um we have 27 possible",
      "offset": 3139.92,
      "duration": 5.439
    },
    {
      "lang": "en",
      "text": "characters or tokens and if everything",
      "offset": 3143.28,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "was equally likely then you'd expect all",
      "offset": 3145.359,
      "duration": 3.521
    },
    {
      "lang": "en",
      "text": "these probabilities",
      "offset": 3147.44,
      "duration": 3.119
    },
    {
      "lang": "en",
      "text": "to be",
      "offset": 3148.88,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "four percent roughly",
      "offset": 3150.559,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "so anything above four percent means",
      "offset": 3152.4,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "that we've learned something useful from",
      "offset": 3154.319,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "these bigram statistics and you see that",
      "offset": 3155.92,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "roughly some of these are four percent",
      "offset": 3158.079,
      "duration": 2.801
    },
    {
      "lang": "en",
      "text": "but some of them are as high as 40",
      "offset": 3159.44,
      "duration": 2.399
    },
    {
      "lang": "en",
      "text": "percent",
      "offset": 3160.88,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "35 percent and so on so you see that the",
      "offset": 3161.839,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "model actually assigned a pretty high",
      "offset": 3164.4,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "probability to whatever's in the",
      "offset": 3165.599,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "training set and so that's a good thing",
      "offset": 3167.44,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "um basically if you have a very good",
      "offset": 3170,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "model you'd expect that these",
      "offset": 3171.68,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "probabilities should be near one because",
      "offset": 3173.2,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "that means that your model is correctly",
      "offset": 3174.96,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "predicting what's going to come next",
      "offset": 3177.119,
      "duration": 3.121
    },
    {
      "lang": "en",
      "text": "especially on the training set where you",
      "offset": 3178.64,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "where you trained your model",
      "offset": 3180.24,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "so",
      "offset": 3182.8,
      "duration": 2.48
    },
    {
      "lang": "en",
      "text": "now we'd like to think about how can we",
      "offset": 3183.68,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "summarize these probabilities into a",
      "offset": 3185.28,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "single number that measures the quality",
      "offset": 3187.52,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "of this model",
      "offset": 3189.44,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "now when you look at the literature into",
      "offset": 3191.68,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "maximum likelihood estimation and",
      "offset": 3193.2,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "statistical modeling and so on",
      "offset": 3195.119,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "you'll see that what's typically used",
      "offset": 3197.2,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "here is something called the likelihood",
      "offset": 3198.72,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "and the likelihood is the product of all",
      "offset": 3201.359,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "of these probabilities",
      "offset": 3203.599,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "and so the product of all these",
      "offset": 3205.76,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "probabilities is the likelihood and it's",
      "offset": 3207.28,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "really telling us about the probability",
      "offset": 3209.44,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "of the entire data set assigned uh",
      "offset": 3211.839,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "assigned by the model that we've trained",
      "offset": 3214.96,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "and that is a measure of quality",
      "offset": 3217.599,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "so the product of these",
      "offset": 3219.359,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "should be as high as possible",
      "offset": 3221.2,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "when you are training the model and when",
      "offset": 3223.2,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "you have a good model your pro your",
      "offset": 3224.64,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "product of these probabilities should be",
      "offset": 3226.559,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "very high",
      "offset": 3227.76,
      "duration": 2.559
    },
    {
      "lang": "en",
      "text": "um",
      "offset": 3229.359,
      "duration": 2.161
    },
    {
      "lang": "en",
      "text": "now because the product of these",
      "offset": 3230.319,
      "duration": 3.121
    },
    {
      "lang": "en",
      "text": "probabilities is an unwieldy thing to",
      "offset": 3231.52,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "work with you can see that all of them",
      "offset": 3233.44,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "are between zero and one so your product",
      "offset": 3235.04,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "of these probabilities will be a very",
      "offset": 3236.96,
      "duration": 3.119
    },
    {
      "lang": "en",
      "text": "tiny number",
      "offset": 3238.4,
      "duration": 2.48
    },
    {
      "lang": "en",
      "text": "um",
      "offset": 3240.079,
      "duration": 1.681
    },
    {
      "lang": "en",
      "text": "so",
      "offset": 3240.88,
      "duration": 2.4
    },
    {
      "lang": "en",
      "text": "for convenience what people work with",
      "offset": 3241.76,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "usually is not the likelihood but they",
      "offset": 3243.28,
      "duration": 2.799
    },
    {
      "lang": "en",
      "text": "work with what's called the log",
      "offset": 3244.96,
      "duration": 2.879
    },
    {
      "lang": "en",
      "text": "likelihood",
      "offset": 3246.079,
      "duration": 2.801
    },
    {
      "lang": "en",
      "text": "so",
      "offset": 3247.839,
      "duration": 2.961
    },
    {
      "lang": "en",
      "text": "the product of these is the likelihood",
      "offset": 3248.88,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "to get the log likelihood we just have",
      "offset": 3250.8,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "to take the log of the probability",
      "offset": 3252.72,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "and so the log of the probability here i",
      "offset": 3254.96,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "have the log of x from zero to one",
      "offset": 3257.119,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "the log is a you see here monotonic",
      "offset": 3259.68,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "transformation of the probability",
      "offset": 3261.92,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "where if you pass in one",
      "offset": 3264.559,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "you get zero",
      "offset": 3267.2,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "so probability one gets your log",
      "offset": 3268.72,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "probability of zero",
      "offset": 3270.64,
      "duration": 2.959
    },
    {
      "lang": "en",
      "text": "and then as you go lower and lower",
      "offset": 3272.16,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "probability the log will grow more and",
      "offset": 3273.599,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "more negative until all the way to",
      "offset": 3275.68,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "negative infinity at zero",
      "offset": 3277.359,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "so here we have a log prob which is",
      "offset": 3281.839,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "really just a torch.log of probability",
      "offset": 3284,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "let's print it out to get a sense of",
      "offset": 3286.72,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "what that looks like",
      "offset": 3288.48,
      "duration": 3.119
    },
    {
      "lang": "en",
      "text": "log prob",
      "offset": 3289.92,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "also 0.4 f",
      "offset": 3291.599,
      "duration": 3.041
    },
    {
      "lang": "en",
      "text": "okay",
      "offset": 3294.72,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "so as you can see when we plug in",
      "offset": 3296.559,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "numbers that are very close some of our",
      "offset": 3298.559,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "higher numbers we get closer and closer",
      "offset": 3300.64,
      "duration": 2.719
    },
    {
      "lang": "en",
      "text": "to zero",
      "offset": 3302.319,
      "duration": 2.641
    },
    {
      "lang": "en",
      "text": "and then if we plug in very bad",
      "offset": 3303.359,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "probabilities we get more and more",
      "offset": 3304.96,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "negative number that's bad",
      "offset": 3306.559,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "so",
      "offset": 3309.52,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "and the reason we work with this is for",
      "offset": 3310.799,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "a large extent convenience right",
      "offset": 3312.4,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "because we have mathematically that if",
      "offset": 3315.2,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "you have some product a times b times c",
      "offset": 3316.88,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "of all these probabilities right",
      "offset": 3318.88,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "the likelihood is the product of all",
      "offset": 3321.119,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "these probabilities",
      "offset": 3323.68,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "then the log",
      "offset": 3325.359,
      "duration": 3.041
    },
    {
      "lang": "en",
      "text": "of these",
      "offset": 3327.359,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "is just log of a plus",
      "offset": 3328.4,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "log of b",
      "offset": 3330.799,
      "duration": 2.481
    },
    {
      "lang": "en",
      "text": "plus log of c if you remember your logs",
      "offset": 3333.68,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "from your",
      "offset": 3336.16,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "high school or undergrad and so on",
      "offset": 3337.44,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "so we have that basically",
      "offset": 3339.68,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "the likelihood of the product",
      "offset": 3341.44,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "probabilities the log likelihood is just",
      "offset": 3342.48,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "the sum of the logs of the individual",
      "offset": 3344.64,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "probabilities",
      "offset": 3346.88,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "so",
      "offset": 3348.799,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "log likelihood",
      "offset": 3350,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "starts at zero",
      "offset": 3352.72,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "and then log likelihood here we can just",
      "offset": 3354.559,
      "duration": 5.681
    },
    {
      "lang": "en",
      "text": "accumulate simply",
      "offset": 3357.119,
      "duration": 3.121
    },
    {
      "lang": "en",
      "text": "and in the end we can print this",
      "offset": 3360.319,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "print the log likelihood",
      "offset": 3365.359,
      "duration": 3.281
    },
    {
      "lang": "en",
      "text": "f strings",
      "offset": 3369.52,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "maybe you're familiar with this",
      "offset": 3371.68,
      "duration": 6.639
    },
    {
      "lang": "en",
      "text": "so log likelihood is negative 38.",
      "offset": 3373.839,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "okay",
      "offset": 3379.92,
      "duration": 2.56
    },
    {
      "lang": "en",
      "text": "now",
      "offset": 3381.28,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "we actually want um",
      "offset": 3382.48,
      "duration": 5.359
    },
    {
      "lang": "en",
      "text": "so how high can log likelihood get it",
      "offset": 3385.2,
      "duration": 5.119
    },
    {
      "lang": "en",
      "text": "can go to zero so when all the",
      "offset": 3387.839,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "probabilities are one log likelihood",
      "offset": 3390.319,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "will be zero and then when all the",
      "offset": 3391.92,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "probabilities are lower this will grow",
      "offset": 3393.599,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "more and more negative",
      "offset": 3395.44,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "now we don't actually like this because",
      "offset": 3397.44,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "what we'd like is a loss function and a",
      "offset": 3399.76,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "loss function has the semantics that low",
      "offset": 3401.839,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "is good",
      "offset": 3404.64,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "because we're trying to minimize the",
      "offset": 3406.16,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "loss so we actually need to invert this",
      "offset": 3407.44,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "and that's what gives us something",
      "offset": 3410.319,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "called the negative log likelihood",
      "offset": 3411.76,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "negative log likelihood is just negative",
      "offset": 3415.92,
      "duration": 5.679
    },
    {
      "lang": "en",
      "text": "of the log likelihood",
      "offset": 3418.559,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "these are f strings by the way if you'd",
      "offset": 3423.76,
      "duration": 2.64
    },
    {
      "lang": "en",
      "text": "like to look this up",
      "offset": 3425.04,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "negative log likelihood equals",
      "offset": 3426.4,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "so negative log likelihood now is just",
      "offset": 3429.28,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "negative of it and so the negative log",
      "offset": 3430.88,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "block load is a very nice loss function",
      "offset": 3432.88,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "because um",
      "offset": 3435.599,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "the lowest it can get is zero",
      "offset": 3437.599,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "and the higher it is the worse off the",
      "offset": 3439.68,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "predictions are that you're making",
      "offset": 3442.16,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "and then one more modification to this",
      "offset": 3444.64,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "that sometimes people do is that for",
      "offset": 3446,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "convenience uh they actually like to",
      "offset": 3447.92,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "normalize by they like to make it an",
      "offset": 3449.839,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "average instead of a sum",
      "offset": 3452.16,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "and so uh here",
      "offset": 3454.319,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "let's just keep some counts as well",
      "offset": 3457.04,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "so n plus equals one",
      "offset": 3459.28,
      "duration": 3.519
    },
    {
      "lang": "en",
      "text": "starts at zero",
      "offset": 3461.44,
      "duration": 2.56
    },
    {
      "lang": "en",
      "text": "and then here",
      "offset": 3462.799,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "um we can have sort of like a normalized",
      "offset": 3464,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "log likelihood",
      "offset": 3466.16,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "um",
      "offset": 3467.76,
      "duration": 2.24
    },
    {
      "lang": "en",
      "text": "if we just normalize it by the count",
      "offset": 3470.4,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "then we will sort of get the average",
      "offset": 3472.4,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "log likelihood so this would be",
      "offset": 3474.64,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "usually our loss function here is what",
      "offset": 3476.88,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "this we would this is what we would use",
      "offset": 3479.2,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "uh so our loss function for the training",
      "offset": 3482.079,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "set assigned by the model is 2.4 that's",
      "offset": 3483.92,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "the quality of this model",
      "offset": 3486.559,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "and the lower it is the better off we",
      "offset": 3488.48,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "are and the higher it is the worse off",
      "offset": 3490.4,
      "duration": 2.959
    },
    {
      "lang": "en",
      "text": "we are",
      "offset": 3492,
      "duration": 2.24
    },
    {
      "lang": "en",
      "text": "and",
      "offset": 3493.359,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "the job of our you know training is to",
      "offset": 3494.24,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "find the parameters that minimize the",
      "offset": 3497.04,
      "duration": 5.759
    },
    {
      "lang": "en",
      "text": "negative log likelihood loss",
      "offset": 3499.52,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "and that would be like a high quality",
      "offset": 3502.799,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "model okay so to summarize i actually",
      "offset": 3504.559,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "wrote it out here",
      "offset": 3506.48,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "so our goal is to maximize likelihood",
      "offset": 3508,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "which is the",
      "offset": 3510.799,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "product of all the probabilities",
      "offset": 3511.92,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "assigned by the model",
      "offset": 3514,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "and we want to maximize this likelihood",
      "offset": 3515.52,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "with respect to the model parameters and",
      "offset": 3517.52,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "in our case the model parameters here",
      "offset": 3519.76,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "are defined in the table these numbers",
      "offset": 3521.92,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "the probabilities",
      "offset": 3523.92,
      "duration": 2.48
    },
    {
      "lang": "en",
      "text": "are",
      "offset": 3525.44,
      "duration": 2.399
    },
    {
      "lang": "en",
      "text": "the model parameters sort of in our",
      "offset": 3526.4,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "program language models so far but you",
      "offset": 3527.839,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "have to keep in mind that here we are",
      "offset": 3530.319,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "storing everything in a table format the",
      "offset": 3532,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "probabilities but what's coming up as a",
      "offset": 3533.68,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "brief preview is that these numbers will",
      "offset": 3535.839,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "not be kept explicitly but these numbers",
      "offset": 3538.48,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "will be calculated by a neural network",
      "offset": 3540.72,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "so that's coming up",
      "offset": 3543.04,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "and we want to change and tune the",
      "offset": 3544.48,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "parameters of these neural networks we",
      "offset": 3546.24,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "want to change these parameters to",
      "offset": 3548.16,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "maximize the likelihood the product of",
      "offset": 3549.599,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "the probabilities",
      "offset": 3551.44,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "now maximizing the likelihood is",
      "offset": 3553.28,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "equivalent to maximizing the log",
      "offset": 3555.04,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "likelihood because log is a monotonic",
      "offset": 3556.48,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "function",
      "offset": 3558.64,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "here's the graph of log",
      "offset": 3559.839,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "and basically all it is doing is it's",
      "offset": 3562,
      "duration": 5.119
    },
    {
      "lang": "en",
      "text": "just scaling your um you can look at it",
      "offset": 3564.319,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "as just a scaling of the loss function",
      "offset": 3567.119,
      "duration": 5.361
    },
    {
      "lang": "en",
      "text": "and so the optimization problem here and",
      "offset": 3569.359,
      "duration": 5.041
    },
    {
      "lang": "en",
      "text": "here are actually equivalent because",
      "offset": 3572.48,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "this is just scaling you can look at it",
      "offset": 3574.4,
      "duration": 2.56
    },
    {
      "lang": "en",
      "text": "that way",
      "offset": 3576,
      "duration": 2.64
    },
    {
      "lang": "en",
      "text": "and so these are two identical",
      "offset": 3576.96,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "optimization problems",
      "offset": 3578.64,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "um",
      "offset": 3581.04,
      "duration": 2.079
    },
    {
      "lang": "en",
      "text": "maximizing the log-likelihood is",
      "offset": 3581.839,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "equivalent to minimizing the negative",
      "offset": 3583.119,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "log likelihood and then in practice",
      "offset": 3584.559,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "people actually minimize the average",
      "offset": 3587.04,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "negative log likelihood to get numbers",
      "offset": 3588.799,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "like 2.4",
      "offset": 3590.799,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "and then this summarizes the quality of",
      "offset": 3592.88,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "your model and we'd like to minimize it",
      "offset": 3595.2,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "and make it as small as possible",
      "offset": 3597.359,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "and the lowest it can get is zero",
      "offset": 3599.599,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "and the lower it is",
      "offset": 3602.319,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "the better off your model is because",
      "offset": 3604.16,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "it's signing it's assigning high",
      "offset": 3605.92,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "probabilities to your data now let's",
      "offset": 3607.44,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "estimate the probability over the entire",
      "offset": 3609.839,
      "duration": 2.881
    },
    {
      "lang": "en",
      "text": "training set just to make sure that we",
      "offset": 3611.28,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "get something around 2.4 let's run this",
      "offset": 3612.72,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "over the entire oops",
      "offset": 3615.359,
      "duration": 5.281
    },
    {
      "lang": "en",
      "text": "let's take out the print segment as well",
      "offset": 3617.2,
      "duration": 7.2
    },
    {
      "lang": "en",
      "text": "okay 2.45 or the entire training set",
      "offset": 3620.64,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "now what i'd like to show you is that",
      "offset": 3624.4,
      "duration": 2.159
    },
    {
      "lang": "en",
      "text": "you can actually evaluate the",
      "offset": 3625.68,
      "duration": 2.56
    },
    {
      "lang": "en",
      "text": "probability for any word that you want",
      "offset": 3626.559,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "like for example",
      "offset": 3628.24,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "if we just test a single word andre and",
      "offset": 3630.4,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "bring back the print statement",
      "offset": 3632.88,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "then you see that andre is actually kind",
      "offset": 3635.76,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "of like an unlikely word like on average",
      "offset": 3637.119,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "we take",
      "offset": 3640.64,
      "duration": 1.919
    },
    {
      "lang": "en",
      "text": "three",
      "offset": 3641.68,
      "duration": 3.119
    },
    {
      "lang": "en",
      "text": "log probability to represent it and",
      "offset": 3642.559,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "roughly that's because ej apparently is",
      "offset": 3644.799,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "very uncommon as an example",
      "offset": 3646.48,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "now",
      "offset": 3650,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "think through this um",
      "offset": 3651.2,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "when i take andre and i append q and i",
      "offset": 3653.76,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "test the probability of it under q",
      "offset": 3655.92,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "we actually get",
      "offset": 3660.16,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "infinity",
      "offset": 3661.68,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "and that's because jq has a zero percent",
      "offset": 3662.96,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "probability according to our model so",
      "offset": 3665.599,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "the log likelihood",
      "offset": 3667.68,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "so the log of zero will be negative",
      "offset": 3669.28,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "infinity we get infinite loss",
      "offset": 3671.2,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "so this is kind of undesirable right",
      "offset": 3674.4,
      "duration": 2.48
    },
    {
      "lang": "en",
      "text": "because we plugged in a string that",
      "offset": 3675.68,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "could be like a somewhat reasonable name",
      "offset": 3676.88,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "but basically what this is saying is",
      "offset": 3679.119,
      "duration": 3.281
    },
    {
      "lang": "en",
      "text": "that this model is exactly zero percent",
      "offset": 3680.48,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "likely to uh to predict this",
      "offset": 3682.4,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "name",
      "offset": 3685.44,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "and our loss is infinity on this example",
      "offset": 3686.4,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "and really what the reason for that is",
      "offset": 3689.68,
      "duration": 3.119
    },
    {
      "lang": "en",
      "text": "that j",
      "offset": 3691.44,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "is followed by q",
      "offset": 3692.799,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "uh zero times",
      "offset": 3694.96,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "uh where's q jq is zero and so jq is uh",
      "offset": 3696.88,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "zero percent likely",
      "offset": 3700.64,
      "duration": 2.959
    },
    {
      "lang": "en",
      "text": "so it's actually kind of gross and",
      "offset": 3702.16,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "people don't like this too much to fix",
      "offset": 3703.599,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "this there's a very simple fix that",
      "offset": 3705.599,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "people like to do to sort of like smooth",
      "offset": 3707.04,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "out your model a little bit and it's",
      "offset": 3709.28,
      "duration": 3.279
    },
    {
      "lang": "en",
      "text": "called model smoothing and roughly",
      "offset": 3710.4,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "what's happening is that we will eight",
      "offset": 3712.559,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "we will add some fake counts",
      "offset": 3713.92,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "so",
      "offset": 3716.24,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "imagine adding a count of one to",
      "offset": 3717.119,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "everything",
      "offset": 3719.28,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "so we add a count of one",
      "offset": 3720.88,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "like this",
      "offset": 3723.359,
      "duration": 2.161
    },
    {
      "lang": "en",
      "text": "and then we recalculate the",
      "offset": 3724.559,
      "duration": 3.121
    },
    {
      "lang": "en",
      "text": "probabilities",
      "offset": 3725.52,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "and that's model smoothing and you can",
      "offset": 3727.68,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "add as much as you like you can add five",
      "offset": 3729.2,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "and it will give you a smoother model",
      "offset": 3730.88,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "and the more you add here",
      "offset": 3732.799,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "the more",
      "offset": 3734.72,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "uniform model you're going to have and",
      "offset": 3735.839,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "the less you add",
      "offset": 3737.76,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "the more peaked model you are going to",
      "offset": 3739.599,
      "duration": 2.641
    },
    {
      "lang": "en",
      "text": "have of course",
      "offset": 3741.119,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "so one is like a pretty decent count to",
      "offset": 3742.24,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "add",
      "offset": 3744.88,
      "duration": 2.64
    },
    {
      "lang": "en",
      "text": "and that will ensure that there will be",
      "offset": 3745.68,
      "duration": 5.119
    },
    {
      "lang": "en",
      "text": "no zeros in our probability matrix p",
      "offset": 3747.52,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "and so this will of course change the",
      "offset": 3750.799,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "generations a little bit in this case it",
      "offset": 3752,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "didn't but in principle it could",
      "offset": 3754.16,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "but what that's going to do now is that",
      "offset": 3756.48,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "nothing will be infinity unlikely",
      "offset": 3758.319,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "so now",
      "offset": 3761.119,
      "duration": 2.801
    },
    {
      "lang": "en",
      "text": "our model will predict some other",
      "offset": 3762.319,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "probability and we see that jq now has a",
      "offset": 3763.92,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "very small probability so the model",
      "offset": 3766.079,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "still finds it very surprising that this",
      "offset": 3768.079,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "was a word or a bigram but we don't get",
      "offset": 3769.599,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "negative infinity so it's kind of like a",
      "offset": 3772,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "nice fix that people like to apply",
      "offset": 3774,
      "duration": 2.24
    },
    {
      "lang": "en",
      "text": "sometimes and it's called model",
      "offset": 3775.28,
      "duration": 3.039
    },
    {
      "lang": "en",
      "text": "smoothing okay so we've now trained a",
      "offset": 3776.24,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "respectable bi-gram character level",
      "offset": 3778.319,
      "duration": 5.841
    },
    {
      "lang": "en",
      "text": "language model and we saw that we both",
      "offset": 3780.48,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "sort of trained the model by looking at",
      "offset": 3784.16,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "the counts of all the bigrams and",
      "offset": 3785.92,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "normalizing the rows to get probability",
      "offset": 3788.079,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "distributions",
      "offset": 3790,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "we saw that we can also then use those",
      "offset": 3791.44,
      "duration": 5.119
    },
    {
      "lang": "en",
      "text": "parameters of this model to perform",
      "offset": 3794.16,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "sampling of new words",
      "offset": 3796.559,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "so we sample new names according to",
      "offset": 3799.44,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "those distributions and we also saw that",
      "offset": 3800.96,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "we can evaluate the quality of this",
      "offset": 3802.96,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "model and the quality of this model is",
      "offset": 3804.559,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "summarized in a single number which is",
      "offset": 3806.559,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "the negative log likelihood and the",
      "offset": 3808.319,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "lower this number is the better the",
      "offset": 3810.16,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "model is",
      "offset": 3812.079,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "because it is giving high probabilities",
      "offset": 3813.28,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "to the actual next characters in all the",
      "offset": 3815.44,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "bi-grams in our training set",
      "offset": 3817.68,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "so that's all well and good but we've",
      "offset": 3820.16,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "arrived at this model explicitly by",
      "offset": 3822.4,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "doing something that felt sensible we",
      "offset": 3824.4,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "were just performing counts and then we",
      "offset": 3826.16,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "were normalizing those counts",
      "offset": 3828.88,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "now what i would like to do is i would",
      "offset": 3830.88,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "like to take an alternative approach we",
      "offset": 3832.079,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "will end up in a very very similar",
      "offset": 3834.16,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "position but the approach will look very",
      "offset": 3835.76,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "different because i would like to cast",
      "offset": 3837.44,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "the problem of bi-gram character level",
      "offset": 3839.28,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "language modeling into the neural",
      "offset": 3840.88,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "network framework",
      "offset": 3842.64,
      "duration": 2.959
    },
    {
      "lang": "en",
      "text": "in the neural network framework we're",
      "offset": 3844.16,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "going to approach things slightly",
      "offset": 3845.599,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "differently but again end up in a very",
      "offset": 3847.599,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "similar spot i'll go into that later now",
      "offset": 3849.28,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "our neural network is going to be a",
      "offset": 3852.559,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "still a background character level",
      "offset": 3854.88,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "language model so it receives a single",
      "offset": 3856.24,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "character as an input",
      "offset": 3858.64,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "then there's neural network with some",
      "offset": 3860.319,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "weights or some parameters w",
      "offset": 3861.76,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "and it's going to output the probability",
      "offset": 3864.079,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "distribution over the next character in",
      "offset": 3866.48,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "a sequence it's going to make guesses as",
      "offset": 3868.4,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "to what is likely to follow this",
      "offset": 3870.559,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "character that was input to the model",
      "offset": 3872.64,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "and then in addition to that we're going",
      "offset": 3875.839,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "to be able to evaluate any setting of",
      "offset": 3877.68,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "the parameters of the neural net because",
      "offset": 3879.68,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "we have the loss function",
      "offset": 3881.52,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "the negative log likelihood so we're",
      "offset": 3883.599,
      "duration": 2.881
    },
    {
      "lang": "en",
      "text": "going to take a look at its probability",
      "offset": 3885.2,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "distributions and we're going to use the",
      "offset": 3886.48,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "labels",
      "offset": 3888.24,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "which are basically just the identity of",
      "offset": 3890,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "the next character in that diagram the",
      "offset": 3891.68,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "second character",
      "offset": 3893.359,
      "duration": 3.041
    },
    {
      "lang": "en",
      "text": "so knowing what second character",
      "offset": 3894.64,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "actually comes next in the bigram allows",
      "offset": 3896.4,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "us to then look at what how high of",
      "offset": 3898.4,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "probability the model assigns to that",
      "offset": 3900.799,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "character",
      "offset": 3902.799,
      "duration": 2.24
    },
    {
      "lang": "en",
      "text": "and then we of course want the",
      "offset": 3903.839,
      "duration": 3.121
    },
    {
      "lang": "en",
      "text": "probability to be very high",
      "offset": 3905.039,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "and that is another way of saying that",
      "offset": 3906.96,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "the loss is low",
      "offset": 3908.799,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "so we're going to use gradient-based",
      "offset": 3910.72,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "optimization then to tune the parameters",
      "offset": 3912.48,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "of this network because we have the loss",
      "offset": 3914.319,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "function and we're going to minimize it",
      "offset": 3916.16,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "so we're going to tune the weights so",
      "offset": 3918.319,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "that the neural net is correctly",
      "offset": 3920.24,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "predicting the probabilities for the",
      "offset": 3921.68,
      "duration": 2.639
    },
    {
      "lang": "en",
      "text": "next character",
      "offset": 3922.96,
      "duration": 3.119
    },
    {
      "lang": "en",
      "text": "so let's get started the first thing i",
      "offset": 3924.319,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "want to do is i want to compile the",
      "offset": 3926.079,
      "duration": 3.121
    },
    {
      "lang": "en",
      "text": "training set of this neural network",
      "offset": 3927.52,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "right so",
      "offset": 3929.2,
      "duration": 2.08
    },
    {
      "lang": "en",
      "text": "create",
      "offset": 3930.24,
      "duration": 2.799
    },
    {
      "lang": "en",
      "text": "the training set",
      "offset": 3931.28,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "of all the bigrams",
      "offset": 3933.039,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "okay",
      "offset": 3936.319,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "and",
      "offset": 3937.599,
      "duration": 3.041
    },
    {
      "lang": "en",
      "text": "here",
      "offset": 3939.359,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "i'm going to copy paste this code",
      "offset": 3940.64,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "because this code iterates over all the",
      "offset": 3943.68,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "programs",
      "offset": 3945.119,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "so here we start with the words we",
      "offset": 3947.359,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "iterate over all the bygrams and",
      "offset": 3948.96,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "previously as you recall we did the",
      "offset": 3950.559,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "counts but now we're not going to do",
      "offset": 3952.24,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "counts we're just creating a training",
      "offset": 3954,
      "duration": 2.799
    },
    {
      "lang": "en",
      "text": "set",
      "offset": 3955.76,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "now this training set will be made up of",
      "offset": 3956.799,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "two lists",
      "offset": 3958.64,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "we have the",
      "offset": 3962,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "inputs",
      "offset": 3964.64,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "and the targets",
      "offset": 3966.24,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "the the labels",
      "offset": 3967.68,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "and these bi-grams will denote x y those",
      "offset": 3969.44,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "are the characters right",
      "offset": 3971.44,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "and so we're given the first character",
      "offset": 3973.119,
      "duration": 3.041
    },
    {
      "lang": "en",
      "text": "of the bi-gram and then we're trying to",
      "offset": 3974.799,
      "duration": 2.881
    },
    {
      "lang": "en",
      "text": "predict the next one",
      "offset": 3976.16,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "both of these are going to be integers",
      "offset": 3977.68,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "so here we'll take x's that append is",
      "offset": 3979.359,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "just",
      "offset": 3982.24,
      "duration": 5.359
    },
    {
      "lang": "en",
      "text": "x1 ystat append ix2",
      "offset": 3983.52,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "and then here",
      "offset": 3987.599,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "we actually don't want lists of integers",
      "offset": 3989.28,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "we will create tensors out of these so",
      "offset": 3991.44,
      "duration": 6.679
    },
    {
      "lang": "en",
      "text": "axis is torch.tensor of axis and wise a",
      "offset": 3994.079,
      "duration": 7.361
    },
    {
      "lang": "en",
      "text": "storage.tensor of ys",
      "offset": 3998.119,
      "duration": 4.281
    },
    {
      "lang": "en",
      "text": "and then",
      "offset": 4001.44,
      "duration": 2.24
    },
    {
      "lang": "en",
      "text": "we don't actually want to take all the",
      "offset": 4002.4,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "words just yet because i want everything",
      "offset": 4003.68,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "to be manageable",
      "offset": 4005.28,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "so let's just do the first word which is",
      "offset": 4006.96,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "emma",
      "offset": 4008.88,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "and then it's clear what these x's and",
      "offset": 4011.2,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "y's would be",
      "offset": 4012.88,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "here let me print",
      "offset": 4015.359,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "character 1 character 2 just so you see",
      "offset": 4017.76,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "what's going on here",
      "offset": 4019.599,
      "duration": 5.361
    },
    {
      "lang": "en",
      "text": "so the bigrams of these characters is",
      "offset": 4021.52,
      "duration": 8
    },
    {
      "lang": "en",
      "text": "dot e e m m m a a dot so this single",
      "offset": 4024.96,
      "duration": 6.72
    },
    {
      "lang": "en",
      "text": "word as i mentioned has one two three",
      "offset": 4029.52,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "four five examples for our neural",
      "offset": 4031.68,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "network",
      "offset": 4033.44,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "there are five separate examples in emma",
      "offset": 4034.64,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "and those examples are summarized here",
      "offset": 4037.52,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "when the input to the neural network is",
      "offset": 4039.2,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "integer 0",
      "offset": 4041.2,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "the desired label is integer 5 which",
      "offset": 4043.039,
      "duration": 5.601
    },
    {
      "lang": "en",
      "text": "corresponds to e when the input to the",
      "offset": 4045.92,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "neural network is 5 we want its weights",
      "offset": 4048.64,
      "duration": 4.959
    },
    {
      "lang": "en",
      "text": "to be arranged so that 13 gets a very",
      "offset": 4051.119,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "high probability",
      "offset": 4053.599,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "when 13 is put in we want 13 to have a",
      "offset": 4055.039,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "high probability",
      "offset": 4057.76,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "when 13 is put in we also want 1 to have",
      "offset": 4059.119,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "a high probability",
      "offset": 4061.52,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "when one is input we want zero to have a",
      "offset": 4063.599,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "very high probability so there are five",
      "offset": 4065.839,
      "duration": 5.601
    },
    {
      "lang": "en",
      "text": "separate input examples to a neural nut",
      "offset": 4068.16,
      "duration": 6.32
    },
    {
      "lang": "en",
      "text": "in this data set",
      "offset": 4071.44,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "i wanted to add a tangent of a node of",
      "offset": 4074.96,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "caution to be careful with a lot of the",
      "offset": 4077.52,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "apis of some of these frameworks",
      "offset": 4078.96,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "you saw me silently use torch.tensor",
      "offset": 4081.44,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "with a lowercase t",
      "offset": 4083.92,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "and the output looked right",
      "offset": 4085.839,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "but you should be aware that there's",
      "offset": 4087.839,
      "duration": 3.041
    },
    {
      "lang": "en",
      "text": "actually two ways of constructing a",
      "offset": 4089.52,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "tensor there's a torch.lowercase tensor",
      "offset": 4090.88,
      "duration": 5.439
    },
    {
      "lang": "en",
      "text": "and there's also a torch.capital tensor",
      "offset": 4093.839,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "class which you can also construct",
      "offset": 4096.319,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "so you can actually call both you can",
      "offset": 4098.56,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "also do torch.capital tensor",
      "offset": 4100.319,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "and you get a nexus and wise as well",
      "offset": 4102.799,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "so that's not confusing at all",
      "offset": 4105.359,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "um",
      "offset": 4107.44,
      "duration": 2.399
    },
    {
      "lang": "en",
      "text": "there are threads on what is the",
      "offset": 4108.799,
      "duration": 2.721
    },
    {
      "lang": "en",
      "text": "difference between these two",
      "offset": 4109.839,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "and um",
      "offset": 4111.52,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "unfortunately the docs are just like not",
      "offset": 4113.359,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "clear on the difference and when you",
      "offset": 4115.12,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "look at the the docs of lower case",
      "offset": 4116.319,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "tensor construct tensor with no autograd",
      "offset": 4118.719,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "history by copying data",
      "offset": 4120.88,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "it's just like it doesn't",
      "offset": 4123.52,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "it doesn't make sense so the actual",
      "offset": 4125.6,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "difference as far as i can tell is",
      "offset": 4127.199,
      "duration": 2.801
    },
    {
      "lang": "en",
      "text": "explained eventually in this random",
      "offset": 4128.56,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "thread that you can google",
      "offset": 4130,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "and really it comes down to",
      "offset": 4131.52,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "i believe",
      "offset": 4133.199,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "that um",
      "offset": 4135.04,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "what is this",
      "offset": 4136.56,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "torch.tensor in first d-type the data",
      "offset": 4138.48,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "type automatically while torch.tensor",
      "offset": 4140.64,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "just returns a float tensor",
      "offset": 4142.48,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "i would recommend stick to",
      "offset": 4144.319,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "torch.lowercase tensor",
      "offset": 4145.44,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "so um",
      "offset": 4147.759,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "indeed we see that when i",
      "offset": 4149.52,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "construct this with a capital t the data",
      "offset": 4151.52,
      "duration": 6.56
    },
    {
      "lang": "en",
      "text": "type here of xs is float32",
      "offset": 4153.839,
      "duration": 7.201
    },
    {
      "lang": "en",
      "text": "but towards that lowercase tensor",
      "offset": 4158.08,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "you see how it's now x dot d type is now",
      "offset": 4161.04,
      "duration": 5.679
    },
    {
      "lang": "en",
      "text": "integer",
      "offset": 4164.56,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "so um",
      "offset": 4166.719,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "it's advised that you use lowercase t",
      "offset": 4168.159,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "and you can read more about it if you",
      "offset": 4170.719,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "like in some of these threads but",
      "offset": 4172.159,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "basically",
      "offset": 4174.4,
      "duration": 2.08
    },
    {
      "lang": "en",
      "text": "um",
      "offset": 4175.679,
      "duration": 2.401
    },
    {
      "lang": "en",
      "text": "i'm pointing out some of these things",
      "offset": 4176.48,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "because i want to caution you and i want",
      "offset": 4178.08,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "you to re get used to reading a lot of",
      "offset": 4179.679,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "documentation and reading through a lot",
      "offset": 4181.6,
      "duration": 2.719
    },
    {
      "lang": "en",
      "text": "of",
      "offset": 4183.6,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "q and a's and threads like this",
      "offset": 4184.319,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "and",
      "offset": 4186.96,
      "duration": 2.16
    },
    {
      "lang": "en",
      "text": "you know some of the stuff is",
      "offset": 4188.159,
      "duration": 2.401
    },
    {
      "lang": "en",
      "text": "unfortunately not easy and not very well",
      "offset": 4189.12,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "documented and you have to be careful",
      "offset": 4190.56,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "out there what we want here is integers",
      "offset": 4191.92,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "because that's what makes uh sense",
      "offset": 4194.64,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "um",
      "offset": 4196.96,
      "duration": 2.4
    },
    {
      "lang": "en",
      "text": "and so",
      "offset": 4198,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "lowercase tensor is what we are using",
      "offset": 4199.36,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "okay now we want to think through how",
      "offset": 4201.12,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "we're going to feed in these examples",
      "offset": 4202.56,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "into a neural network",
      "offset": 4204.48,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "now it's not quite as straightforward as",
      "offset": 4206.239,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "plugging it in because these examples",
      "offset": 4209.04,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "right now are integers so there's like a",
      "offset": 4210.96,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "0 5 or 13 it gives us the index of the",
      "offset": 4212.96,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "character and you can't just plug an",
      "offset": 4215.84,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "integer index into a neural net",
      "offset": 4217.52,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "these neural nets right are sort of made",
      "offset": 4219.92,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "up of these neurons",
      "offset": 4222.4,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "and",
      "offset": 4224.48,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "these neurons have weights and as you",
      "offset": 4225.36,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "saw in micrograd these weights act",
      "offset": 4227.28,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "multiplicatively on the inputs w x plus",
      "offset": 4229.28,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "b there's 10 h's and so on and so it",
      "offset": 4232,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "doesn't really make sense to make an",
      "offset": 4234.56,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "input neuron take on integer values that",
      "offset": 4235.679,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "you feed in and then multiply on with",
      "offset": 4237.84,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "weights",
      "offset": 4240.4,
      "duration": 2.4
    },
    {
      "lang": "en",
      "text": "so instead",
      "offset": 4241.6,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "a common way of encoding integers is",
      "offset": 4242.8,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "what's called one hot encoding",
      "offset": 4244.64,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "in one hot encoding",
      "offset": 4246.96,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "we take an integer like 13 and we create",
      "offset": 4248.719,
      "duration": 5.281
    },
    {
      "lang": "en",
      "text": "a vector that is all zeros except for",
      "offset": 4251.28,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "the 13th dimension which we turn to a",
      "offset": 4254,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "one and then that vector can feed into a",
      "offset": 4256.56,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "neural net",
      "offset": 4259.44,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "now conveniently",
      "offset": 4261.12,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "uh pi torch actually has something",
      "offset": 4263.12,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "called the one hot",
      "offset": 4264.64,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "function inside torching and functional",
      "offset": 4267.84,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "it takes a tensor made up of integers",
      "offset": 4270.239,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "um",
      "offset": 4273.84,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "long is a is a as an integer",
      "offset": 4274.96,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "um",
      "offset": 4278.159,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "and it also takes a number of classes um",
      "offset": 4279.12,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "which is how large you want your uh",
      "offset": 4282.48,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "tensor uh your vector to be",
      "offset": 4284.32,
      "duration": 5.72
    },
    {
      "lang": "en",
      "text": "so here let's import",
      "offset": 4287.679,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "torch.n.functional sf this is a common",
      "offset": 4290.04,
      "duration": 4.04
    },
    {
      "lang": "en",
      "text": "way of importing it",
      "offset": 4292.32,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "and then let's do f.1 hot",
      "offset": 4294.08,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "and we feed in the integers that we want",
      "offset": 4296.64,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "to encode so we can actually feed in the",
      "offset": 4298.64,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "entire array of x's",
      "offset": 4301.28,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "and we can tell it that num classes is",
      "offset": 4304,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "27.",
      "offset": 4306.159,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "so it doesn't have to try to guess it it",
      "offset": 4307.679,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "may have guessed that it's only 13 and",
      "offset": 4309.52,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "would give us an incorrect result",
      "offset": 4311.679,
      "duration": 5.761
    },
    {
      "lang": "en",
      "text": "so this is the one hot let's call this x",
      "offset": 4314.56,
      "duration": 6.88
    },
    {
      "lang": "en",
      "text": "inc for x encoded",
      "offset": 4317.44,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "and then we see that x encoded that",
      "offset": 4322,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "shape is 5 by 27",
      "offset": 4323.679,
      "duration": 6.641
    },
    {
      "lang": "en",
      "text": "and uh we can also visualize it plt.i am",
      "offset": 4327.04,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "show of x inc",
      "offset": 4330.32,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "to make it a little bit more clear",
      "offset": 4332.239,
      "duration": 3.121
    },
    {
      "lang": "en",
      "text": "because this is a little messy",
      "offset": 4333.52,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "so we see that we've encoded all the",
      "offset": 4335.36,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "five examples uh into vectors we have",
      "offset": 4337.12,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "five examples so we have five rows and",
      "offset": 4340.64,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "each row here is now an example into a",
      "offset": 4342.8,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "neural nut",
      "offset": 4344.96,
      "duration": 3.279
    },
    {
      "lang": "en",
      "text": "and we see that the appropriate bit is",
      "offset": 4346.239,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "turned on as a one and everything else",
      "offset": 4348.239,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "is zero",
      "offset": 4350.48,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "so um",
      "offset": 4351.84,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "here for example the zeroth bit is",
      "offset": 4353.44,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "turned on the fifth bit is turned on",
      "offset": 4355.76,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "13th bits are turned on for both of",
      "offset": 4358.239,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "these examples and then the first bit",
      "offset": 4360.159,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "here is turned on",
      "offset": 4362.64,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "so that's how we can encode",
      "offset": 4364.56,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "integers into vectors and then these",
      "offset": 4367.04,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "vectors can feed in to neural nets one",
      "offset": 4369.76,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "more issue to be careful with here by",
      "offset": 4372.08,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "the way is",
      "offset": 4373.52,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "let's look at the data type of encoding",
      "offset": 4375.12,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "we always want to be careful with data",
      "offset": 4376.88,
      "duration": 2.48
    },
    {
      "lang": "en",
      "text": "types",
      "offset": 4378.32,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "what would you expect x encoding's data",
      "offset": 4379.36,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "type to be",
      "offset": 4381.679,
      "duration": 2.961
    },
    {
      "lang": "en",
      "text": "when we're plugging numbers into neural",
      "offset": 4382.719,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "nuts we don't want them to be integers",
      "offset": 4384.64,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "we want them to be floating point",
      "offset": 4386.159,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "numbers that can take on various values",
      "offset": 4387.44,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "but the d type here is actually 64-bit",
      "offset": 4390.4,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "integer",
      "offset": 4393.12,
      "duration": 2.559
    },
    {
      "lang": "en",
      "text": "and the reason for that i suspect is",
      "offset": 4394.239,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "that one hot received a 64-bit integer",
      "offset": 4395.679,
      "duration": 6.161
    },
    {
      "lang": "en",
      "text": "here and it returned the same data type",
      "offset": 4398.88,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "and when you look at the signature of",
      "offset": 4401.84,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "one hot it doesn't even take a d type a",
      "offset": 4403.199,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "desired data type of the output tensor",
      "offset": 4405.44,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "and so we can't in a lot of functions in",
      "offset": 4408.4,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "torch we'd be able to do something like",
      "offset": 4410.56,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "d type equal storage.float32",
      "offset": 4412,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "which is what we want but one heart does",
      "offset": 4414.32,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "not support that",
      "offset": 4416.48,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "so instead we're going to want to cast",
      "offset": 4417.92,
      "duration": 5.279
    },
    {
      "lang": "en",
      "text": "this to float like this",
      "offset": 4419.679,
      "duration": 5.121
    },
    {
      "lang": "en",
      "text": "so that these",
      "offset": 4423.199,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "everything is the same",
      "offset": 4424.8,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "everything looks the same but the d-type",
      "offset": 4426.4,
      "duration": 5.839
    },
    {
      "lang": "en",
      "text": "is float32 and floats can feed into",
      "offset": 4428.64,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "neural nets so now let's construct our",
      "offset": 4432.239,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "first neuron",
      "offset": 4434.32,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "this neuron will look at these input",
      "offset": 4436,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "vectors",
      "offset": 4438.56,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "and as you remember from micrograd these",
      "offset": 4440.08,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "neurons basically perform a very simple",
      "offset": 4442.08,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "function w x plus b where w x is a dot",
      "offset": 4443.76,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "product",
      "offset": 4447.28,
      "duration": 2.24
    },
    {
      "lang": "en",
      "text": "right",
      "offset": 4448.239,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "so we can achieve the same thing here",
      "offset": 4449.52,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "let's first define the weights of this",
      "offset": 4452,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "neuron basically what are the initial",
      "offset": 4454.159,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "weights at initialization for this",
      "offset": 4455.679,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "neuron",
      "offset": 4457.679,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "let's initialize them with torch.rendin",
      "offset": 4458.719,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "torch.rendin",
      "offset": 4461.6,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "is um",
      "offset": 4463.28,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "fills a tensor with random numbers",
      "offset": 4464.56,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "drawn from a normal distribution",
      "offset": 4467.04,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "and a normal distribution",
      "offset": 4469.199,
      "duration": 2.641
    },
    {
      "lang": "en",
      "text": "has",
      "offset": 4471.04,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "a probability density function like this",
      "offset": 4471.84,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "and so most of the numbers drawn from",
      "offset": 4474.32,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "this distribution will be around 0",
      "offset": 4475.84,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "but some of them will be as high as",
      "offset": 4479.04,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "almost three and so on and very few",
      "offset": 4480.64,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "numbers will be above three in magnitude",
      "offset": 4482.4,
      "duration": 6.64
    },
    {
      "lang": "en",
      "text": "so we need to take a size as an input",
      "offset": 4486.32,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "here",
      "offset": 4489.04,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "and i'm going to use size as to be 27 by",
      "offset": 4490.4,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "one",
      "offset": 4492.88,
      "duration": 2.64
    },
    {
      "lang": "en",
      "text": "so",
      "offset": 4494.56,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "27 by one and then let's visualize w so",
      "offset": 4495.52,
      "duration": 7.52
    },
    {
      "lang": "en",
      "text": "w is a column vector of 27 numbers",
      "offset": 4498.719,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "and",
      "offset": 4503.04,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "these weights are then multiplied by the",
      "offset": 4503.92,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "inputs",
      "offset": 4506.88,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "so now to perform this multiplication we",
      "offset": 4508.64,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "can take x encoding and we can multiply",
      "offset": 4510.719,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "it with w",
      "offset": 4513.12,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "this is a matrix multiplication operator",
      "offset": 4514.96,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "in pi torch",
      "offset": 4517.52,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "and the output of this operation is five",
      "offset": 4519.84,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "by one",
      "offset": 4522.4,
      "duration": 2.4
    },
    {
      "lang": "en",
      "text": "the reason is five by five is the",
      "offset": 4523.6,
      "duration": 2.24
    },
    {
      "lang": "en",
      "text": "following",
      "offset": 4524.8,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "we took x encoding which is five by",
      "offset": 4525.84,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "twenty seven and we multiplied it by",
      "offset": 4528,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "twenty seven by one",
      "offset": 4530.56,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "and",
      "offset": 4533.52,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "in matrix multiplication",
      "offset": 4534.4,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "you see that the output will become five",
      "offset": 4536.48,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "by one because these 27",
      "offset": 4538.8,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "will multiply and add",
      "offset": 4541.6,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "so basically what we're seeing here outs",
      "offset": 4544.8,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "out of this operation",
      "offset": 4546.88,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "is we are seeing the five",
      "offset": 4548.719,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "activations",
      "offset": 4551.84,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "of this neuron",
      "offset": 4553.679,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "on these five inputs",
      "offset": 4556.32,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "and we've evaluated all of them in",
      "offset": 4558.159,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "parallel we didn't feed in just a single",
      "offset": 4559.76,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "input to the single neuron we fed in",
      "offset": 4561.84,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "simultaneously all the five inputs into",
      "offset": 4563.92,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "the same neuron",
      "offset": 4566.64,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "and in parallel patrol has evaluated",
      "offset": 4568.08,
      "duration": 6.32
    },
    {
      "lang": "en",
      "text": "the wx plus b but here is just the wx",
      "offset": 4571.12,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "there's no bias",
      "offset": 4574.4,
      "duration": 5.279
    },
    {
      "lang": "en",
      "text": "it has value w times x for all of them",
      "offset": 4575.84,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "independently now instead of a single",
      "offset": 4579.679,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "neuron though i would like to have 27",
      "offset": 4581.44,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "neurons and i'll show you in a second",
      "offset": 4583.36,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "why i want 27 neurons",
      "offset": 4585.199,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "so instead of having just a 1 here which",
      "offset": 4587.52,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "is indicating this presence of one",
      "offset": 4589.76,
      "duration": 2.64
    },
    {
      "lang": "en",
      "text": "single neuron",
      "offset": 4591.199,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "we can use 27",
      "offset": 4592.4,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "and then when w is 27 by 27",
      "offset": 4594.64,
      "duration": 7.12
    },
    {
      "lang": "en",
      "text": "this will in parallel evaluate all the",
      "offset": 4598.32,
      "duration": 8.16
    },
    {
      "lang": "en",
      "text": "27 neurons on all the 5 inputs",
      "offset": 4601.76,
      "duration": 6.8
    },
    {
      "lang": "en",
      "text": "giving us a much better much much bigger",
      "offset": 4606.48,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "result so now what we've done is 5 by 27",
      "offset": 4608.56,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "multiplied 27 by 27",
      "offset": 4611.36,
      "duration": 6.4
    },
    {
      "lang": "en",
      "text": "and the output of this is now 5 by 27",
      "offset": 4614,
      "duration": 7.44
    },
    {
      "lang": "en",
      "text": "so we can see that the shape of this",
      "offset": 4617.76,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "is 5 by 27.",
      "offset": 4621.76,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "so what is every element here telling us",
      "offset": 4623.84,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "right",
      "offset": 4626.239,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "it's telling us for every one of 27",
      "offset": 4627.04,
      "duration": 5.679
    },
    {
      "lang": "en",
      "text": "neurons that we created",
      "offset": 4629.199,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "what is the firing rate of those neurons",
      "offset": 4633.199,
      "duration": 6.401
    },
    {
      "lang": "en",
      "text": "on every one of those five examples",
      "offset": 4636.64,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "so",
      "offset": 4639.6,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "the element for example 3 comma 13",
      "offset": 4640.719,
      "duration": 7.52
    },
    {
      "lang": "en",
      "text": "is giving us the firing rate of the 13th",
      "offset": 4645.28,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "neuron",
      "offset": 4648.239,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "looking at the third input",
      "offset": 4649.28,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "and the way this was achieved is by a",
      "offset": 4651.84,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "dot product",
      "offset": 4654.48,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "between the third",
      "offset": 4656.239,
      "duration": 2.641
    },
    {
      "lang": "en",
      "text": "input",
      "offset": 4657.76,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "and the 13th column",
      "offset": 4658.88,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "of this w matrix here",
      "offset": 4661.44,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "okay",
      "offset": 4664.8,
      "duration": 1.68
    },
    {
      "lang": "en",
      "text": "so",
      "offset": 4665.6,
      "duration": 2.639
    },
    {
      "lang": "en",
      "text": "using matrix multiplication we can very",
      "offset": 4666.48,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "efficiently evaluate",
      "offset": 4668.239,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "the dot product between lots of input",
      "offset": 4670.8,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "examples in a batch",
      "offset": 4672.64,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "and lots of neurons where all those",
      "offset": 4674.96,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "neurons have weights in the columns of",
      "offset": 4677.52,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "those w's",
      "offset": 4679.679,
      "duration": 2.721
    },
    {
      "lang": "en",
      "text": "and in matrix multiplication we're just",
      "offset": 4681.04,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "doing those dot products and",
      "offset": 4682.4,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "in parallel just to show you that this",
      "offset": 4684.48,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "is the case we can take x and we can",
      "offset": 4686.239,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "take the third",
      "offset": 4688.48,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "row",
      "offset": 4690.4,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "and we can take the w and take its 13th",
      "offset": 4692.08,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "column",
      "offset": 4694.64,
      "duration": 2.32
    },
    {
      "lang": "en",
      "text": "and then we can do",
      "offset": 4697.36,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "x and get three",
      "offset": 4698.8,
      "duration": 7.84
    },
    {
      "lang": "en",
      "text": "elementwise multiply with w at 13.",
      "offset": 4701.52,
      "duration": 8.08
    },
    {
      "lang": "en",
      "text": "and sum that up that's wx plus b",
      "offset": 4706.64,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "well there's no plus b it's just wx dot",
      "offset": 4709.6,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "product",
      "offset": 4711.92,
      "duration": 2.16
    },
    {
      "lang": "en",
      "text": "and that's",
      "offset": 4712.8,
      "duration": 2.399
    },
    {
      "lang": "en",
      "text": "this number",
      "offset": 4714.08,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "so you see that this is just being done",
      "offset": 4715.199,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "efficiently by the matrix multiplication",
      "offset": 4716.88,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "operation",
      "offset": 4719.52,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "for all the input examples and for all",
      "offset": 4720.4,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "the output neurons of this first layer",
      "offset": 4722.719,
      "duration": 6.561
    },
    {
      "lang": "en",
      "text": "okay so we fed our 27-dimensional inputs",
      "offset": 4726,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "into a first layer of a neural net that",
      "offset": 4729.28,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "has 27 neurons right so we have 27",
      "offset": 4731.12,
      "duration": 6.079
    },
    {
      "lang": "en",
      "text": "inputs and now we have 27 neurons these",
      "offset": 4734.4,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "neurons perform w times x they don't",
      "offset": 4737.199,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "have a bias and they don't have a",
      "offset": 4740,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "non-linearity like 10 h we're going to",
      "offset": 4741.52,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "leave them to be a linear layer",
      "offset": 4743.6,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "in addition to that we're not going to",
      "offset": 4746.48,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "have any other layers this is going to",
      "offset": 4747.92,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "be it it's just going to be",
      "offset": 4749.52,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "the dumbest smallest simplest neural net",
      "offset": 4751.44,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "which is just a single linear layer",
      "offset": 4753.52,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "and now i'd like to explain what i want",
      "offset": 4756.4,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "those 27 outputs to be",
      "offset": 4758.08,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "intuitively what we're trying to produce",
      "offset": 4761.199,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "here for every single input example is",
      "offset": 4762.56,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "we're trying to produce some kind of a",
      "offset": 4764.64,
      "duration": 3.039
    },
    {
      "lang": "en",
      "text": "probability distribution for the next",
      "offset": 4766,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "character in a sequence",
      "offset": 4767.679,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "and there's 27 of them",
      "offset": 4769.28,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "but we have to come up with like precise",
      "offset": 4771.44,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "semantics for exactly how we're going to",
      "offset": 4773.04,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "interpret these 27 numbers that these",
      "offset": 4774.8,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "neurons take on",
      "offset": 4777.679,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "now intuitively",
      "offset": 4779.679,
      "duration": 2.881
    },
    {
      "lang": "en",
      "text": "you see here that these numbers are",
      "offset": 4781.12,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "negative and some of them are positive",
      "offset": 4782.56,
      "duration": 2.56
    },
    {
      "lang": "en",
      "text": "etc",
      "offset": 4784,
      "duration": 2.48
    },
    {
      "lang": "en",
      "text": "and that's because these are coming out",
      "offset": 4785.12,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "of a neural net layer initialized with",
      "offset": 4786.48,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "these",
      "offset": 4788.88,
      "duration": 2.319
    },
    {
      "lang": "en",
      "text": "normal distribution",
      "offset": 4791.36,
      "duration": 2.879
    },
    {
      "lang": "en",
      "text": "parameters",
      "offset": 4792.96,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "but what we want is we want something",
      "offset": 4794.239,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "like we had here",
      "offset": 4795.84,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "like each row here",
      "offset": 4797.199,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "told us the counts and then we",
      "offset": 4798.96,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "normalized the counts to get",
      "offset": 4801.12,
      "duration": 3.119
    },
    {
      "lang": "en",
      "text": "probabilities and we want something",
      "offset": 4802.159,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "similar to come out of the neural net",
      "offset": 4804.239,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "but what we just have right now is just",
      "offset": 4806.4,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "some negative and positive numbers",
      "offset": 4807.84,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "now we want those numbers to somehow",
      "offset": 4810.56,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "represent the probabilities for the next",
      "offset": 4812.239,
      "duration": 3.041
    },
    {
      "lang": "en",
      "text": "character",
      "offset": 4814,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "but you see that probabilities they they",
      "offset": 4815.28,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "have a special structure they um",
      "offset": 4817.6,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "they're positive numbers and they sum to",
      "offset": 4819.92,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "one",
      "offset": 4821.92,
      "duration": 2.64
    },
    {
      "lang": "en",
      "text": "and so that doesn't just come out of a",
      "offset": 4822.8,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "neural net",
      "offset": 4824.56,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "and then they can't be counts",
      "offset": 4825.76,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "because these counts are positive and",
      "offset": 4827.76,
      "duration": 4.959
    },
    {
      "lang": "en",
      "text": "counts are integers",
      "offset": 4831.04,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "so counts are also not really a good",
      "offset": 4832.719,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "thing to output from a neural net",
      "offset": 4834.48,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "so instead what the neural net is going",
      "offset": 4836.639,
      "duration": 3.121
    },
    {
      "lang": "en",
      "text": "to output and how we are going to",
      "offset": 4838.159,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "interpret the um",
      "offset": 4839.76,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "the 27 numbers is that these 27 numbers",
      "offset": 4842.08,
      "duration": 6.4
    },
    {
      "lang": "en",
      "text": "are giving us log counts",
      "offset": 4845.6,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "basically",
      "offset": 4848.48,
      "duration": 1.92
    },
    {
      "lang": "en",
      "text": "um",
      "offset": 4849.679,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "so instead of giving us counts directly",
      "offset": 4850.4,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "like in this table they're giving us log",
      "offset": 4852.88,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "counts",
      "offset": 4854.8,
      "duration": 2.879
    },
    {
      "lang": "en",
      "text": "and to get the counts we're going to",
      "offset": 4856,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "take the log counts and we're going to",
      "offset": 4857.679,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "exponentiate them",
      "offset": 4859.36,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "now",
      "offset": 4861.36,
      "duration": 2.799
    },
    {
      "lang": "en",
      "text": "exponentiation",
      "offset": 4862.32,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "takes the following form",
      "offset": 4864.159,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "it takes numbers",
      "offset": 4867.199,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "that are negative or they are positive",
      "offset": 4868.8,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "it takes the entire real line",
      "offset": 4870.88,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "and then if you plug in negative numbers",
      "offset": 4872.8,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "you're going to get e to the x",
      "offset": 4874.719,
      "duration": 5.841
    },
    {
      "lang": "en",
      "text": "which is uh always below one",
      "offset": 4877.199,
      "duration": 6.161
    },
    {
      "lang": "en",
      "text": "so you're getting numbers lower than one",
      "offset": 4880.56,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "and if you plug in numbers greater than",
      "offset": 4883.36,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "zero you're getting numbers greater than",
      "offset": 4885.28,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "one all the way growing to the infinity",
      "offset": 4887.44,
      "duration": 5.759
    },
    {
      "lang": "en",
      "text": "and this here grows to zero",
      "offset": 4890.8,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "so basically we're going to",
      "offset": 4893.199,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "take these numbers",
      "offset": 4895.12,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "here",
      "offset": 4897.76,
      "duration": 2.24
    },
    {
      "lang": "en",
      "text": "and",
      "offset": 4900.32,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "instead of them being positive and",
      "offset": 4903.28,
      "duration": 2.879
    },
    {
      "lang": "en",
      "text": "negative and all over the place we're",
      "offset": 4904.56,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "going to interpret them as log counts",
      "offset": 4906.159,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "and then we're going to element wise",
      "offset": 4908.719,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "exponentiate these numbers",
      "offset": 4910.159,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "exponentiating them now gives us",
      "offset": 4912.719,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "something like this",
      "offset": 4914.48,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "and you see that these numbers now",
      "offset": 4916.4,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "because they went through an exponent",
      "offset": 4917.6,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "all the negative numbers turned into",
      "offset": 4919.28,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "numbers below 1 like 0.338 and all the",
      "offset": 4920.8,
      "duration": 6.32
    },
    {
      "lang": "en",
      "text": "positive numbers originally turned into",
      "offset": 4924.56,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "even more positive numbers sort of",
      "offset": 4927.12,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "greater than one",
      "offset": 4928.639,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "so like for example",
      "offset": 4930.719,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "seven",
      "offset": 4932.48,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "is some positive number over here",
      "offset": 4934.48,
      "duration": 6.56
    },
    {
      "lang": "en",
      "text": "that is greater than zero",
      "offset": 4938.4,
      "duration": 6.4
    },
    {
      "lang": "en",
      "text": "but exponentiated outputs here",
      "offset": 4941.04,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "basically give us something that we can",
      "offset": 4944.8,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "use and interpret as the equivalent of",
      "offset": 4946.32,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "counts originally so you see these",
      "offset": 4949.04,
      "duration": 7.28
    },
    {
      "lang": "en",
      "text": "counts here 112 7 51 1 etc",
      "offset": 4951.52,
      "duration": 7.04
    },
    {
      "lang": "en",
      "text": "the neural net is kind of now predicting",
      "offset": 4956.32,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "uh",
      "offset": 4958.56,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "counts",
      "offset": 4960.239,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "and these counts are positive numbers",
      "offset": 4961.52,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "they can never be below zero so that",
      "offset": 4963.84,
      "duration": 2.879
    },
    {
      "lang": "en",
      "text": "makes sense",
      "offset": 4965.679,
      "duration": 2.881
    },
    {
      "lang": "en",
      "text": "and uh they can now take on various",
      "offset": 4966.719,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "values",
      "offset": 4968.56,
      "duration": 5.599
    },
    {
      "lang": "en",
      "text": "depending on the settings of w",
      "offset": 4969.679,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "so let me break this down",
      "offset": 4974.159,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "we're going to interpret these to be the",
      "offset": 4976.159,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "log counts",
      "offset": 4978.239,
      "duration": 2.801
    },
    {
      "lang": "en",
      "text": "in other words for this that is often",
      "offset": 4981.199,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "used is so-called logits",
      "offset": 4982.96,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "these are logits log counts",
      "offset": 4985.12,
      "duration": 6.079
    },
    {
      "lang": "en",
      "text": "then these will be sort of the counts",
      "offset": 4988.56,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "largest exponentiated",
      "offset": 4991.199,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "and this is equivalent to the n matrix",
      "offset": 4993.36,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "sort of the n",
      "offset": 4996.239,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "array that we used previously remember",
      "offset": 4998,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "this was the n",
      "offset": 5000.08,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "this is the the array of counts",
      "offset": 5001.6,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "and each row here are the counts for the",
      "offset": 5004.08,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "for the um",
      "offset": 5007.76,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "next character sort of",
      "offset": 5008.96,
      "duration": 3.279
    },
    {
      "lang": "en",
      "text": "so those are the counts and now the",
      "offset": 5012.639,
      "duration": 5.281
    },
    {
      "lang": "en",
      "text": "probabilities are just the counts um",
      "offset": 5014.8,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "normalized",
      "offset": 5017.92,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "and so um",
      "offset": 5019.6,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "i'm not going to find the same but",
      "offset": 5021.6,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "basically i'm not going to scroll all",
      "offset": 5023.04,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "over the place",
      "offset": 5024.8,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "we've already done this we want to",
      "offset": 5026,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "counts that sum",
      "offset": 5028.159,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "along the first dimension and we want to",
      "offset": 5030.08,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "keep them as true",
      "offset": 5032.239,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "we've went over this and this is how we",
      "offset": 5034.719,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "normalize the rows of our counts matrix",
      "offset": 5036.8,
      "duration": 6.399
    },
    {
      "lang": "en",
      "text": "to get our probabilities",
      "offset": 5039.92,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "props",
      "offset": 5043.199,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "so now these are the probabilities",
      "offset": 5044.88,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "and",
      "offset": 5047.92,
      "duration": 2.239
    },
    {
      "lang": "en",
      "text": "these are the counts that we ask",
      "offset": 5048.639,
      "duration": 2.721
    },
    {
      "lang": "en",
      "text": "currently and now when i show the",
      "offset": 5050.159,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "probabilities",
      "offset": 5051.36,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "you see that um",
      "offset": 5053.679,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "every row here",
      "offset": 5055.6,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "of course",
      "offset": 5057.36,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "will sum to 1",
      "offset": 5059.44,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "because they're normalized",
      "offset": 5061.36,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "and the shape of this",
      "offset": 5063.12,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "is 5 by 27",
      "offset": 5065.28,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "and so really what we've achieved is for",
      "offset": 5067.36,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "every one of our five examples",
      "offset": 5069.76,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "we now have a row that came out of a",
      "offset": 5071.679,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "neural net",
      "offset": 5073.76,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "and because of the transformations here",
      "offset": 5075.04,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "we made sure that this output of this",
      "offset": 5077.52,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "neural net now are probabilities or we",
      "offset": 5079.199,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "can interpret to be probabilities",
      "offset": 5081.44,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "so",
      "offset": 5084,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "our wx here gave us logits",
      "offset": 5085.28,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "and then we interpret those to be log",
      "offset": 5088,
      "duration": 2.639
    },
    {
      "lang": "en",
      "text": "counts",
      "offset": 5089.679,
      "duration": 2.721
    },
    {
      "lang": "en",
      "text": "we exponentiate to get something that",
      "offset": 5090.639,
      "duration": 3.281
    },
    {
      "lang": "en",
      "text": "looks like counts",
      "offset": 5092.4,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "and then we normalize those counts to",
      "offset": 5093.92,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "get a probability distribution",
      "offset": 5095.36,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "and all of these are differentiable",
      "offset": 5097.44,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "operations",
      "offset": 5099.04,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "so what we've done now is we're taking",
      "offset": 5100.32,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "inputs we have differentiable operations",
      "offset": 5102.32,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "that we can back propagate through",
      "offset": 5104.639,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "and we're getting out probability",
      "offset": 5106.8,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "distributions",
      "offset": 5108.239,
      "duration": 2.801
    },
    {
      "lang": "en",
      "text": "so",
      "offset": 5109.84,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "for example for the zeroth example that",
      "offset": 5111.04,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "fed in",
      "offset": 5113.36,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "right which was um",
      "offset": 5115.12,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "the zeroth example here was a one-half",
      "offset": 5117.04,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "vector of zero",
      "offset": 5118.8,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "and um",
      "offset": 5120.56,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "it basically corresponded to feeding in",
      "offset": 5122.48,
      "duration": 6.159
    },
    {
      "lang": "en",
      "text": "this example here so we're feeding in a",
      "offset": 5126.56,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "dot into a neural net and the way we fed",
      "offset": 5128.639,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "the dot into a neural net is that we",
      "offset": 5131.04,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "first got its index",
      "offset": 5132.639,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "then we one hot encoded it",
      "offset": 5134.32,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "then it went into the neural net and out",
      "offset": 5136.56,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "came",
      "offset": 5139.04,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "this distribution of probabilities",
      "offset": 5140.56,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "and its shape",
      "offset": 5143.36,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "is 27 there's 27 numbers and we're going",
      "offset": 5146.4,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "to interpret this as the neural nets",
      "offset": 5149.12,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "assignment for how likely every one of",
      "offset": 5151.36,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "these characters um",
      "offset": 5154.4,
      "duration": 5.279
    },
    {
      "lang": "en",
      "text": "the 27 characters are to come next",
      "offset": 5156.56,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "and as we tune the weights w",
      "offset": 5159.679,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "we're going to be of course getting",
      "offset": 5162.32,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "different probabilities out for any",
      "offset": 5163.52,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "character that you input",
      "offset": 5165.36,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "and so now the question is just can we",
      "offset": 5167.12,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "optimize and find a good w",
      "offset": 5168.96,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "such that the probabilities coming out",
      "offset": 5170.96,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "are pretty good and the way we measure",
      "offset": 5172.96,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "pretty good is by the loss function okay",
      "offset": 5175.28,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "so i organized everything into a single",
      "offset": 5177.36,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "summary so that hopefully it's a bit",
      "offset": 5178.96,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "more clear so it starts here",
      "offset": 5180.4,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "with an input data set",
      "offset": 5182.56,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "we have some inputs to the neural net",
      "offset": 5184.4,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "and we have some labels for the correct",
      "offset": 5186.32,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "next character in a sequence these are",
      "offset": 5188.56,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "integers",
      "offset": 5190.8,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "here i'm using uh torch generators now",
      "offset": 5192.719,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "so that you see the same numbers that i",
      "offset": 5195.36,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "see",
      "offset": 5197.44,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "and i'm generating um",
      "offset": 5198.56,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "27 neurons weights",
      "offset": 5200.8,
      "duration": 6.72
    },
    {
      "lang": "en",
      "text": "and each neuron here receives 27 inputs",
      "offset": 5202.8,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "then here we're going to plug in all the",
      "offset": 5208.56,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "input examples x's into a neural net so",
      "offset": 5210.4,
      "duration": 5.279
    },
    {
      "lang": "en",
      "text": "here this is a forward pass",
      "offset": 5212.88,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "first we have to encode all of the",
      "offset": 5215.679,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "inputs into one hot representations",
      "offset": 5217.6,
      "duration": 5.119
    },
    {
      "lang": "en",
      "text": "so we have 27 classes we pass in these",
      "offset": 5220.4,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "integers and",
      "offset": 5222.719,
      "duration": 6.96
    },
    {
      "lang": "en",
      "text": "x inc becomes a array that is 5 by 27",
      "offset": 5224.4,
      "duration": 7.839
    },
    {
      "lang": "en",
      "text": "zeros except for a few ones",
      "offset": 5229.679,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "we then multiply this in the first layer",
      "offset": 5232.239,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "of a neural net to get logits",
      "offset": 5234.239,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "exponentiate the logits to get fake",
      "offset": 5236.719,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "counts sort of",
      "offset": 5238.88,
      "duration": 3.279
    },
    {
      "lang": "en",
      "text": "and normalize these counts to get",
      "offset": 5240.639,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "probabilities",
      "offset": 5242.159,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "so we lock these last two lines by the",
      "offset": 5244.32,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "way here are called the softmax",
      "offset": 5246.48,
      "duration": 6.719
    },
    {
      "lang": "en",
      "text": "which i pulled up here soft max is a",
      "offset": 5250,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "very often used layer in a neural net",
      "offset": 5253.199,
      "duration": 5.681
    },
    {
      "lang": "en",
      "text": "that takes these z's which are logics",
      "offset": 5255.92,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "exponentiates them",
      "offset": 5258.88,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "and divides and normalizes it's a way of",
      "offset": 5260.96,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "taking",
      "offset": 5263.679,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "outputs of a neural net layer and these",
      "offset": 5264.639,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "these outputs can be positive or",
      "offset": 5267.199,
      "duration": 2.561
    },
    {
      "lang": "en",
      "text": "negative",
      "offset": 5268.56,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "and it outputs probability distributions",
      "offset": 5269.76,
      "duration": 5.439
    },
    {
      "lang": "en",
      "text": "it outputs something that is always",
      "offset": 5272.4,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "sums to one and are positive numbers",
      "offset": 5275.199,
      "duration": 3.281
    },
    {
      "lang": "en",
      "text": "just like probabilities",
      "offset": 5276.8,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "um so it's kind of like a normalization",
      "offset": 5278.48,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "function if you want to think of it that",
      "offset": 5280.159,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "way and you can put it on top of any",
      "offset": 5281.44,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "other linear layer inside a neural net",
      "offset": 5283.36,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "and it basically makes a neural net",
      "offset": 5285.6,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "output probabilities that's very often",
      "offset": 5287.12,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "used and we used it as well here",
      "offset": 5289.76,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "so this is the forward pass and that's",
      "offset": 5293.28,
      "duration": 2.879
    },
    {
      "lang": "en",
      "text": "how we made a neural net output",
      "offset": 5294.639,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "probability",
      "offset": 5296.159,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "now",
      "offset": 5297.84,
      "duration": 2.799
    },
    {
      "lang": "en",
      "text": "you'll notice that",
      "offset": 5299.36,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "um",
      "offset": 5300.639,
      "duration": 2.161
    },
    {
      "lang": "en",
      "text": "all of these",
      "offset": 5302.96,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "this entire forward pass is made up of",
      "offset": 5304.239,
      "duration": 3.041
    },
    {
      "lang": "en",
      "text": "differentiable",
      "offset": 5305.92,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "layers everything here we can back",
      "offset": 5307.28,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "propagate through and we saw some of the",
      "offset": 5309.12,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "back propagation in micrograd",
      "offset": 5310.96,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "this is just",
      "offset": 5313.199,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "multiplication and addition all that's",
      "offset": 5314.719,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "happening here is just multiply and then",
      "offset": 5316.8,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "add and we know how to backpropagate",
      "offset": 5318.239,
      "duration": 2.48
    },
    {
      "lang": "en",
      "text": "through them",
      "offset": 5319.76,
      "duration": 2.24
    },
    {
      "lang": "en",
      "text": "exponentiation we know how to",
      "offset": 5320.719,
      "duration": 3.121
    },
    {
      "lang": "en",
      "text": "backpropagate through",
      "offset": 5322,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "and then here we are summing",
      "offset": 5323.84,
      "duration": 5.359
    },
    {
      "lang": "en",
      "text": "and sum is is easily backpropagable as",
      "offset": 5326.48,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "well",
      "offset": 5329.199,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "and division as well so everything here",
      "offset": 5330.08,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "is differentiable operation",
      "offset": 5332.56,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "and we can back propagate through",
      "offset": 5334.56,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "now we achieve these probabilities which",
      "offset": 5337.52,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "are 5 by 27",
      "offset": 5339.76,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "for every single example we have a",
      "offset": 5341.6,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "vector of probabilities that's into one",
      "offset": 5343.92,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "and then here i wrote a bunch of stuff",
      "offset": 5346.32,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "to sort of like break down uh the",
      "offset": 5348.48,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "examples",
      "offset": 5350.4,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "so we have five examples making up emma",
      "offset": 5351.44,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "right",
      "offset": 5354.32,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "and there are five bigrams inside emma",
      "offset": 5356.32,
      "duration": 7.28
    },
    {
      "lang": "en",
      "text": "so bigram example a bigram example1 is",
      "offset": 5360,
      "duration": 6.32
    },
    {
      "lang": "en",
      "text": "that e is the beginning character right",
      "offset": 5363.6,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "after dot",
      "offset": 5366.32,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "and the indexes for these are zero and",
      "offset": 5368.239,
      "duration": 3.041
    },
    {
      "lang": "en",
      "text": "five",
      "offset": 5370.4,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "so then we feed in a zero",
      "offset": 5371.28,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "that's the input of the neural net",
      "offset": 5374.08,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "we get probabilities from the neural net",
      "offset": 5375.92,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "that are 27 numbers",
      "offset": 5378.08,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "and then the label is 5 because e",
      "offset": 5381.28,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "actually comes after dot",
      "offset": 5384.08,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "so that's the label",
      "offset": 5385.84,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "and then",
      "offset": 5387.92,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "we use this label 5 to index into the",
      "offset": 5389.36,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "probability distribution here",
      "offset": 5392.32,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "so",
      "offset": 5394.32,
      "duration": 1.68
    },
    {
      "lang": "en",
      "text": "this",
      "offset": 5395.12,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "index 5 here is 0 1 2 3 4 5. it's this",
      "offset": 5396,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "number here",
      "offset": 5400,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "which is here",
      "offset": 5401.28,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "so that's basically the probability",
      "offset": 5404,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "assigned by the neural net to the actual",
      "offset": 5405.36,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "correct character",
      "offset": 5407.28,
      "duration": 2.879
    },
    {
      "lang": "en",
      "text": "you see that the network currently",
      "offset": 5408.719,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "thinks that this next character that e",
      "offset": 5410.159,
      "duration": 5.121
    },
    {
      "lang": "en",
      "text": "following dot is only one percent likely",
      "offset": 5412.48,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "which is of course not very good right",
      "offset": 5415.28,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "because this actually is a training",
      "offset": 5416.96,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "example and the network thinks this is",
      "offset": 5418.8,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "currently very very unlikely but that's",
      "offset": 5420.8,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "just because we didn't get very lucky in",
      "offset": 5422.8,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "generating a good setting of w so right",
      "offset": 5424.8,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "now this network things it says unlikely",
      "offset": 5427.28,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "and 0.01 is not a good outcome",
      "offset": 5429.28,
      "duration": 5.439
    },
    {
      "lang": "en",
      "text": "so the log likelihood then is very",
      "offset": 5431.92,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "negative",
      "offset": 5434.719,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "and the negative log likelihood is very",
      "offset": 5435.76,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "positive",
      "offset": 5438.239,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "and so four is a very high negative log",
      "offset": 5439.36,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "likelihood and that means we're going to",
      "offset": 5442.4,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "have a high loss",
      "offset": 5444,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "because what is the loss the loss is",
      "offset": 5445.28,
      "duration": 6.32
    },
    {
      "lang": "en",
      "text": "just the average negative log likelihood",
      "offset": 5447.44,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "so the second character is em",
      "offset": 5451.6,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "and you see here that also the network",
      "offset": 5453.6,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "thought that m following e is very",
      "offset": 5455.28,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "unlikely one percent",
      "offset": 5457.76,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "the for m following m i thought it was",
      "offset": 5460.96,
      "duration": 3.279
    },
    {
      "lang": "en",
      "text": "two percent",
      "offset": 5463.04,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "and for a following m it actually",
      "offset": 5464.239,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "thought it was seven percent likely so",
      "offset": 5466.08,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "just by chance this one actually has a",
      "offset": 5468.639,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "pretty good probability and therefore",
      "offset": 5470.719,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "pretty low negative log likelihood",
      "offset": 5472.32,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "and finally here it thought this was one",
      "offset": 5475.28,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "percent likely",
      "offset": 5477.04,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "so overall our average negative log",
      "offset": 5478.32,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "likelihood which is the loss the total",
      "offset": 5480.48,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "loss that summarizes",
      "offset": 5482.88,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "basically the how well this network",
      "offset": 5484.719,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "currently works at least on this one",
      "offset": 5486.48,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "word not on the full data suggested one",
      "offset": 5488.159,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "word is 3.76 which is actually very",
      "offset": 5490.08,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "fairly high loss this is not a very good",
      "offset": 5493.199,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "setting of w's",
      "offset": 5495.04,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "now here's what we can do",
      "offset": 5496.8,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "we're currently getting 3.76",
      "offset": 5498.56,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "we can actually come here and we can",
      "offset": 5501.199,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "change our w we can resample it so let",
      "offset": 5502.96,
      "duration": 5.679
    },
    {
      "lang": "en",
      "text": "me just add one to have a different seed",
      "offset": 5505.92,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "and then we get a different w",
      "offset": 5508.639,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "and then we can rerun this",
      "offset": 5510.48,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "and with this different c with this",
      "offset": 5512.8,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "different setting of w's we now get 3.37",
      "offset": 5514.639,
      "duration": 6.161
    },
    {
      "lang": "en",
      "text": "so this is a much better w right and",
      "offset": 5518.56,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "that and it's better because the",
      "offset": 5520.8,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "probabilities just happen to come out",
      "offset": 5522.159,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "higher for the for the characters that",
      "offset": 5524.719,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "actually are next",
      "offset": 5527.04,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "and so you can imagine actually just",
      "offset": 5528.8,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "resampling this you know we can try two",
      "offset": 5530,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "so",
      "offset": 5534.32,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "okay this was not very good",
      "offset": 5535.44,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "let's try one more",
      "offset": 5537.12,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "we can try three",
      "offset": 5538.48,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "okay this was terrible setting because",
      "offset": 5540.88,
      "duration": 3.96
    },
    {
      "lang": "en",
      "text": "we have a very high loss",
      "offset": 5542.8,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "so anyway i'm going to erase this",
      "offset": 5544.84,
      "duration": 4.44
    },
    {
      "lang": "en",
      "text": "what i'm doing here which is just guess",
      "offset": 5549.92,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "and check of randomly assigning",
      "offset": 5551.44,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "parameters and seeing if the network is",
      "offset": 5552.96,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "good that is uh amateur hour that's not",
      "offset": 5554.639,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "how you optimize a neural net the way",
      "offset": 5557.44,
      "duration": 3.279
    },
    {
      "lang": "en",
      "text": "you optimize your neural net is you",
      "offset": 5559.28,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "start with some random guess and we're",
      "offset": 5560.719,
      "duration": 3.041
    },
    {
      "lang": "en",
      "text": "going to commit to this one even though",
      "offset": 5562.48,
      "duration": 2.64
    },
    {
      "lang": "en",
      "text": "it's not very good",
      "offset": 5563.76,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "but now the big deal is we have a loss",
      "offset": 5565.12,
      "duration": 3.119
    },
    {
      "lang": "en",
      "text": "function",
      "offset": 5566.88,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "so this loss",
      "offset": 5568.239,
      "duration": 4.601
    },
    {
      "lang": "en",
      "text": "is made up only of differentiable",
      "offset": 5569.92,
      "duration": 6.4
    },
    {
      "lang": "en",
      "text": "operations and we can minimize the loss",
      "offset": 5572.84,
      "duration": 4.68
    },
    {
      "lang": "en",
      "text": "by tuning",
      "offset": 5576.32,
      "duration": 2.319
    },
    {
      "lang": "en",
      "text": "ws",
      "offset": 5577.52,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "by computing the gradients of the loss",
      "offset": 5578.639,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "with respect to",
      "offset": 5581.199,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "these w matrices",
      "offset": 5582.56,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "and so then we can tune w to minimize",
      "offset": 5584.96,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "the loss and find a good setting of w",
      "offset": 5587.12,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "using gradient based optimization so",
      "offset": 5589.52,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "let's see how that will work now things",
      "offset": 5591.76,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "are actually going to look almost",
      "offset": 5593.36,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "identical to what we had with micrograd",
      "offset": 5594.56,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "so here",
      "offset": 5597.04,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "i pulled up the lecture from micrograd",
      "offset": 5598.159,
      "duration": 5.601
    },
    {
      "lang": "en",
      "text": "the notebook it's from this repository",
      "offset": 5600.88,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "and when i scroll all the way to the end",
      "offset": 5603.76,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "where we left off with micrograd we had",
      "offset": 5604.96,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "something very very similar",
      "offset": 5606.88,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "we had",
      "offset": 5608.56,
      "duration": 2.639
    },
    {
      "lang": "en",
      "text": "a number of input examples in this case",
      "offset": 5609.6,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "we had four input examples inside axis",
      "offset": 5611.199,
      "duration": 5.361
    },
    {
      "lang": "en",
      "text": "and we had their targets these are",
      "offset": 5614.159,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "targets",
      "offset": 5616.56,
      "duration": 3.119
    },
    {
      "lang": "en",
      "text": "just like here we have our axes now but",
      "offset": 5617.6,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "we have five of them and they're now",
      "offset": 5619.679,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "integers instead of vectors",
      "offset": 5621.199,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "but we're going to convert our integers",
      "offset": 5624.08,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "to vectors except our vectors will be 27",
      "offset": 5625.84,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "large instead of three large",
      "offset": 5628.88,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "and then here what we did is first we",
      "offset": 5631.92,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "did a forward pass where we ran a neural",
      "offset": 5633.6,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "net on all of the inputs",
      "offset": 5635.84,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "to get predictions",
      "offset": 5638.239,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "our neural net at the time this nfx was",
      "offset": 5640.239,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "a multi-layer perceptron",
      "offset": 5642.88,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "our neural net is going to look",
      "offset": 5645.04,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "different because our neural net is just",
      "offset": 5646.56,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "a single layer",
      "offset": 5648.239,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "single linear layer followed by a soft",
      "offset": 5650.4,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "max",
      "offset": 5652.4,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "so that's our neural net",
      "offset": 5653.76,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "and the loss here was the mean squared",
      "offset": 5655.84,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "error so we simply subtracted the",
      "offset": 5657.679,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "prediction from the ground truth and",
      "offset": 5659.52,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "squared it and summed it all up and that",
      "offset": 5661.04,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "was the loss and loss was the single",
      "offset": 5663.12,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "number that summarized the quality of",
      "offset": 5665.28,
      "duration": 5.439
    },
    {
      "lang": "en",
      "text": "the neural net and when loss is low like",
      "offset": 5667.52,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "almost zero that means the neural net is",
      "offset": 5670.719,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "predicting correctly",
      "offset": 5673.76,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "so we had a single number that uh that",
      "offset": 5676.159,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "summarized the uh the performance of the",
      "offset": 5678.48,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "neural net and everything here was",
      "offset": 5680.96,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "differentiable and was stored in massive",
      "offset": 5682.8,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "compute graph",
      "offset": 5684.639,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "and then we iterated over all the",
      "offset": 5686.8,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "parameters we made sure that the",
      "offset": 5688.719,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "gradients are set to zero and we called",
      "offset": 5690.08,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "lost up backward",
      "offset": 5692.32,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "and lasted backward initiated back",
      "offset": 5694.08,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "propagation at the final output node of",
      "offset": 5696.08,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "loss",
      "offset": 5698.4,
      "duration": 2.4
    },
    {
      "lang": "en",
      "text": "right so",
      "offset": 5699.28,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "yeah remember these expressions we had",
      "offset": 5700.8,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "loss all the way at the end we start",
      "offset": 5702.4,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "back propagation and we went all the way",
      "offset": 5703.92,
      "duration": 2.4
    },
    {
      "lang": "en",
      "text": "back",
      "offset": 5705.52,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "and we made sure that we populated all",
      "offset": 5706.32,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "the parameters dot grad",
      "offset": 5708.4,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "so that graph started at zero but back",
      "offset": 5710.719,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "propagation filled it in",
      "offset": 5712.639,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "and then in the update we iterated over",
      "offset": 5714.48,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "all the parameters and we simply did a",
      "offset": 5716.4,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "parameter update where every single",
      "offset": 5718.639,
      "duration": 5.921
    },
    {
      "lang": "en",
      "text": "element of our parameters was nudged in",
      "offset": 5721.44,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "the opposite direction of the gradient",
      "offset": 5724.56,
      "duration": 5.679
    },
    {
      "lang": "en",
      "text": "and so we're going to do the exact same",
      "offset": 5727.52,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "thing here",
      "offset": 5730.239,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "so i'm going to pull this up",
      "offset": 5731.679,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "on the side here",
      "offset": 5734.4,
      "duration": 2.719
    },
    {
      "lang": "en",
      "text": "so that we have it available and we're",
      "offset": 5738.48,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "actually going to do the exact same",
      "offset": 5740.08,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "thing so this was the forward pass so",
      "offset": 5741.28,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "where we did this",
      "offset": 5744.32,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "and probs is our wipe red so now we have",
      "offset": 5746.88,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "to evaluate the loss but we're not using",
      "offset": 5749.44,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "the mean squared error we're using the",
      "offset": 5751.199,
      "duration": 3.281
    },
    {
      "lang": "en",
      "text": "negative log likelihood because we are",
      "offset": 5752.88,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "doing classification we're not doing",
      "offset": 5754.48,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "regression as it's called",
      "offset": 5756.32,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "so here we want to calculate loss",
      "offset": 5758.96,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "now the way we calculate it is it's just",
      "offset": 5762.4,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "this average negative log likelihood",
      "offset": 5764.48,
      "duration": 6.159
    },
    {
      "lang": "en",
      "text": "now this probs here",
      "offset": 5767.119,
      "duration": 6.161
    },
    {
      "lang": "en",
      "text": "has a shape of 5 by 27",
      "offset": 5770.639,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "and so to get all the we basically want",
      "offset": 5773.28,
      "duration": 4.959
    },
    {
      "lang": "en",
      "text": "to pluck out the probabilities at the",
      "offset": 5775.679,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "correct indices here",
      "offset": 5778.239,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "so in particular because the labels are",
      "offset": 5780,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "stored here in array wise",
      "offset": 5782.08,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "basically what we're after is for the",
      "offset": 5784.48,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "first example we're looking at",
      "offset": 5786,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "probability of five right at index five",
      "offset": 5787.52,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "for the second example",
      "offset": 5790.88,
      "duration": 5.279
    },
    {
      "lang": "en",
      "text": "at the the second row or row index one",
      "offset": 5792.719,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "we are interested in the probability",
      "offset": 5796.159,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "assigned to index 13.",
      "offset": 5797.44,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "at the second example we also have 13.",
      "offset": 5800.239,
      "duration": 7.121
    },
    {
      "lang": "en",
      "text": "at the third row we want one",
      "offset": 5803.44,
      "duration": 6.32
    },
    {
      "lang": "en",
      "text": "and then the last row which is four we",
      "offset": 5807.36,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "want zero so these are the probabilities",
      "offset": 5809.76,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "we're interested in right",
      "offset": 5812.32,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "and you can see that they're not amazing",
      "offset": 5814,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "as we saw above",
      "offset": 5815.92,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "so these are the probabilities we want",
      "offset": 5818.56,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "but we want like a more efficient way to",
      "offset": 5820.08,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "access these probabilities",
      "offset": 5822.239,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "not just listing them out in a tuple",
      "offset": 5824.48,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "like this so it turns out that the way",
      "offset": 5826.239,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "to do this in pytorch uh one of the ways",
      "offset": 5827.92,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "at least is we can basically pass in",
      "offset": 5829.84,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "all of these",
      "offset": 5832.88,
      "duration": 2.56
    },
    {
      "lang": "en",
      "text": "sorry about that all of these um",
      "offset": 5836.639,
      "duration": 5.441
    },
    {
      "lang": "en",
      "text": "integers in the vectors",
      "offset": 5839.679,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "so",
      "offset": 5842.08,
      "duration": 1.76
    },
    {
      "lang": "en",
      "text": "the",
      "offset": 5842.88,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "these ones you see how they're just 0 1",
      "offset": 5843.84,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "2 3 4",
      "offset": 5845.92,
      "duration": 3.279
    },
    {
      "lang": "en",
      "text": "we can actually create that using mp",
      "offset": 5847.04,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "not mp sorry torch dot range of 5",
      "offset": 5849.199,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "0 1 2 3 4.",
      "offset": 5852.8,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "so we can index here with torch.range of",
      "offset": 5854.4,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "5",
      "offset": 5857.04,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "and here we index with ys",
      "offset": 5858.239,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "and you see that that gives us",
      "offset": 5861.119,
      "duration": 5.801
    },
    {
      "lang": "en",
      "text": "exactly these numbers",
      "offset": 5863.119,
      "duration": 3.801
    },
    {
      "lang": "en",
      "text": "so that plucks out the probabilities of",
      "offset": 5868.96,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "that the neural network assigns to the",
      "offset": 5871.44,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "correct next character",
      "offset": 5873.92,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "now we take those probabilities and we",
      "offset": 5876.239,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "don't we actually look at the log",
      "offset": 5878.32,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "probability so we want to dot log",
      "offset": 5879.6,
      "duration": 6.079
    },
    {
      "lang": "en",
      "text": "and then we want to just",
      "offset": 5883.52,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "average that up so take the mean of all",
      "offset": 5885.679,
      "duration": 2.881
    },
    {
      "lang": "en",
      "text": "of that",
      "offset": 5887.679,
      "duration": 2.641
    },
    {
      "lang": "en",
      "text": "and then it's the negative",
      "offset": 5888.56,
      "duration": 5.599
    },
    {
      "lang": "en",
      "text": "average log likelihood that is the loss",
      "offset": 5890.32,
      "duration": 7.76
    },
    {
      "lang": "en",
      "text": "so the loss here is 3.7 something and",
      "offset": 5894.159,
      "duration": 7.52
    },
    {
      "lang": "en",
      "text": "you see that this loss 3.76 3.76 is",
      "offset": 5898.08,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "exactly as we've obtained before but",
      "offset": 5901.679,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "this is a vectorized form of that",
      "offset": 5903.44,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "expression",
      "offset": 5905.28,
      "duration": 1.919
    },
    {
      "lang": "en",
      "text": "so",
      "offset": 5906.48,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "we get the same loss",
      "offset": 5907.199,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "and the same loss we can consider",
      "offset": 5909.52,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "service part of this forward pass",
      "offset": 5911.36,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "and we've achieved here now loss",
      "offset": 5914,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "okay so we made our way all the way to",
      "offset": 5916.32,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "loss we've defined the forward pass",
      "offset": 5917.92,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "we forwarded the network and the loss",
      "offset": 5920.08,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "now we're ready to do the backward pass",
      "offset": 5922,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "so backward pass",
      "offset": 5924.239,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "we want to first make sure that all the",
      "offset": 5928,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "gradients are reset so they're at zero",
      "offset": 5929.52,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "now in pytorch you can set the gradients",
      "offset": 5932.4,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "to be zero but you can also just set it",
      "offset": 5935.36,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "to none and setting it to none is more",
      "offset": 5937.199,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "efficient and pi torch will interpret",
      "offset": 5939.36,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "none as like a lack of a gradient and is",
      "offset": 5941.6,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "the same as zeros",
      "offset": 5944.32,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "so this is a way to set to zero the",
      "offset": 5945.76,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "gradient",
      "offset": 5947.92,
      "duration": 2.4
    },
    {
      "lang": "en",
      "text": "and now we do lost it backward",
      "offset": 5950.4,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "before we do lost that backward we need",
      "offset": 5954.639,
      "duration": 2.961
    },
    {
      "lang": "en",
      "text": "one more thing if you remember from",
      "offset": 5956.159,
      "duration": 2.641
    },
    {
      "lang": "en",
      "text": "micrograd",
      "offset": 5957.6,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "pytorch actually requires",
      "offset": 5958.8,
      "duration": 6.319
    },
    {
      "lang": "en",
      "text": "that we pass in requires grad is true",
      "offset": 5961.28,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "so that when we tell",
      "offset": 5965.119,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "pythorge that we are interested in",
      "offset": 5966.96,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "calculating gradients for this leaf",
      "offset": 5968.48,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "tensor by default this is false",
      "offset": 5970.719,
      "duration": 5.121
    },
    {
      "lang": "en",
      "text": "so let me recalculate with that",
      "offset": 5973.44,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "and then set to none and lost that",
      "offset": 5975.84,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "backward",
      "offset": 5977.92,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "now something magical happened when",
      "offset": 5980.639,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "lasted backward was run",
      "offset": 5982.32,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "because pytorch just like micrograd when",
      "offset": 5984.4,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "we did the forward pass here",
      "offset": 5987.199,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "it keeps track of all the operations",
      "offset": 5989.04,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "under the hood it builds a full",
      "offset": 5991.44,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "computational graph just like the graphs",
      "offset": 5993.199,
      "duration": 3.281
    },
    {
      "lang": "en",
      "text": "we've",
      "offset": 5995.6,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "produced in micrograd those graphs exist",
      "offset": 5996.48,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "inside pi torch",
      "offset": 5998.8,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "and so it knows all the dependencies and",
      "offset": 6000.8,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "all the mathematical operations of",
      "offset": 6002.719,
      "duration": 2.241
    },
    {
      "lang": "en",
      "text": "everything",
      "offset": 6004,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "and when you then calculate the loss",
      "offset": 6004.96,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "we can call a dot backward on it",
      "offset": 6007.04,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "and that backward then fills in the",
      "offset": 6009.52,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "gradients of",
      "offset": 6011.679,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "all the intermediates",
      "offset": 6013.6,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "all the way back to w's which are the",
      "offset": 6015.199,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "parameters of our neural net so now we",
      "offset": 6018.32,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "can do w grad and we see that it has",
      "offset": 6020.4,
      "duration": 6.4
    },
    {
      "lang": "en",
      "text": "structure there's stuff inside it",
      "offset": 6023.119,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "and these gradients",
      "offset": 6029.04,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "every single element here",
      "offset": 6030.8,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "so w dot shape is 27 by 27",
      "offset": 6033.36,
      "duration": 7.279
    },
    {
      "lang": "en",
      "text": "w grad shape is the same 27 by 27",
      "offset": 6036.8,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "and every element of w that grad",
      "offset": 6040.639,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "is telling us",
      "offset": 6043.04,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "the influence of that weight on the loss",
      "offset": 6044.4,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "function",
      "offset": 6047.6,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "so for example this number all the way",
      "offset": 6048.719,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "here",
      "offset": 6050.8,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "if this element the zero zero element of",
      "offset": 6051.92,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "w",
      "offset": 6054.32,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "because the gradient is positive is",
      "offset": 6055.52,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "telling us that this has a positive",
      "offset": 6057.6,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "influence in the loss slightly nudging",
      "offset": 6060,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "w",
      "offset": 6063.04,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "slightly taking w 0 0",
      "offset": 6064.4,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "and",
      "offset": 6066.96,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "adding a small h to it",
      "offset": 6067.76,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "would increase the loss",
      "offset": 6070.4,
      "duration": 5.279
    },
    {
      "lang": "en",
      "text": "mildly because this gradient is positive",
      "offset": 6072.4,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "some of these gradients are also",
      "offset": 6075.679,
      "duration": 2.881
    },
    {
      "lang": "en",
      "text": "negative",
      "offset": 6076.719,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "so that's telling us about the gradient",
      "offset": 6078.56,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "information and we can use this gradient",
      "offset": 6080.239,
      "duration": 5.121
    },
    {
      "lang": "en",
      "text": "information to update the weights of",
      "offset": 6082.4,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "this neural network so let's now do the",
      "offset": 6085.36,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "update it's going to be very similar to",
      "offset": 6087.44,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "what we had in micrograd we need no loop",
      "offset": 6089.36,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "over all the parameters because we only",
      "offset": 6092.239,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "have one parameter uh tensor and that is",
      "offset": 6093.76,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "w",
      "offset": 6096.08,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "so we simply do w dot data plus equals",
      "offset": 6097.04,
      "duration": 5.119
    },
    {
      "lang": "en",
      "text": "uh the",
      "offset": 6100.96,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "we can actually copy this almost exactly",
      "offset": 6102.159,
      "duration": 6.641
    },
    {
      "lang": "en",
      "text": "negative 0.1 times w dot grad",
      "offset": 6103.84,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "and that would be the update to the",
      "offset": 6109.36,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "tensor",
      "offset": 6112.239,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "so that updates",
      "offset": 6114.48,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "the tensor",
      "offset": 6115.92,
      "duration": 2.64
    },
    {
      "lang": "en",
      "text": "and",
      "offset": 6118.639,
      "duration": 2.48
    },
    {
      "lang": "en",
      "text": "because the tensor is updated we would",
      "offset": 6119.52,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "expect that now the loss should decrease",
      "offset": 6121.119,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "so",
      "offset": 6124.32,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "here if i print loss",
      "offset": 6125.36,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "that item",
      "offset": 6129.44,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "it was 3.76 right",
      "offset": 6131.119,
      "duration": 5.361
    },
    {
      "lang": "en",
      "text": "so we've updated the w here so if i",
      "offset": 6133.119,
      "duration": 5.841
    },
    {
      "lang": "en",
      "text": "recalculate forward pass",
      "offset": 6136.48,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "loss now should be slightly lower so",
      "offset": 6138.96,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "3.76 goes to",
      "offset": 6141.6,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "3.74",
      "offset": 6143.52,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "and then",
      "offset": 6145.679,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "we can again set to set grad to none and",
      "offset": 6146.719,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "backward",
      "offset": 6149.6,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "update",
      "offset": 6150.8,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "and now the parameters changed again",
      "offset": 6152.48,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "so if we recalculate the forward pass we",
      "offset": 6154.8,
      "duration": 6.76
    },
    {
      "lang": "en",
      "text": "expect a lower loss again 3.72",
      "offset": 6157.199,
      "duration": 4.361
    },
    {
      "lang": "en",
      "text": "okay and this is again doing the we're",
      "offset": 6162.159,
      "duration": 5.441
    },
    {
      "lang": "en",
      "text": "now doing gradient descent",
      "offset": 6164.48,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "and when we achieve a low loss that will",
      "offset": 6168.48,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "mean that the network is assigning high",
      "offset": 6170.639,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "probabilities to the correctness",
      "offset": 6172.639,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "characters okay so i rearranged",
      "offset": 6174.4,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "everything and i put it all together",
      "offset": 6176.08,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "from scratch",
      "offset": 6177.92,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "so here is where we construct our data",
      "offset": 6179.44,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "set of bigrams",
      "offset": 6181.36,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "you see that we are still iterating only",
      "offset": 6183.199,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "on the first word emma",
      "offset": 6184.719,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "i'm going to change that in a second i",
      "offset": 6186.88,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "added a number that counts the number of",
      "offset": 6189.28,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "elements in x's so that we explicitly",
      "offset": 6191.36,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "see that number of examples is five",
      "offset": 6194.56,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "because currently we're just working",
      "offset": 6196.88,
      "duration": 2.56
    },
    {
      "lang": "en",
      "text": "with emma and there's five backgrounds",
      "offset": 6198,
      "duration": 2.56
    },
    {
      "lang": "en",
      "text": "there",
      "offset": 6199.44,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "and here i added a loop of exactly what",
      "offset": 6200.56,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "we had before so we had 10 iterations of",
      "offset": 6202.56,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "grainy descent of forward pass backward",
      "offset": 6205.28,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "pass and an update",
      "offset": 6207.44,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "and so running these two cells",
      "offset": 6208.88,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "initialization and gradient descent",
      "offset": 6210.32,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "gives us some improvement",
      "offset": 6212.8,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "on",
      "offset": 6215.36,
      "duration": 2.799
    },
    {
      "lang": "en",
      "text": "the loss function",
      "offset": 6216.159,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "but now i want to use all the words",
      "offset": 6218.159,
      "duration": 7.201
    },
    {
      "lang": "en",
      "text": "and there's not 5 but 228 000 bigrams",
      "offset": 6221.679,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "now",
      "offset": 6225.36,
      "duration": 2.879
    },
    {
      "lang": "en",
      "text": "however this should require no",
      "offset": 6226.639,
      "duration": 3.121
    },
    {
      "lang": "en",
      "text": "modification whatsoever everything",
      "offset": 6228.239,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "should just run because all the code we",
      "offset": 6229.76,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "wrote doesn't care if there's five",
      "offset": 6231.679,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "migrants or 228 000 bigrams and with",
      "offset": 6233.04,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "everything we should just work so",
      "offset": 6236.159,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "you see that this will just run",
      "offset": 6238.4,
      "duration": 3.279
    },
    {
      "lang": "en",
      "text": "but now we are optimizing over the",
      "offset": 6240.32,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "entire training set of all the bigrams",
      "offset": 6241.679,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "and you see now that we are decreasing",
      "offset": 6244.639,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "very slightly so actually we can",
      "offset": 6246.48,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "probably afford a larger learning rate",
      "offset": 6248.159,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "and probably for even larger learning",
      "offset": 6252.32,
      "duration": 4.68
    },
    {
      "lang": "en",
      "text": "rate",
      "offset": 6254,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "even 50 seems to work on this very very",
      "offset": 6260.639,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "simple example right so let me",
      "offset": 6262.56,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "re-initialize and let's run 100",
      "offset": 6264.32,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "iterations",
      "offset": 6266.88,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "see what happens",
      "offset": 6269.28,
      "duration": 2.64
    },
    {
      "lang": "en",
      "text": "okay",
      "offset": 6272.88,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "we seem to be",
      "offset": 6276.239,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "coming up to some pretty good losses",
      "offset": 6279.04,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "here 2.47",
      "offset": 6280.48,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "let me run 100 more",
      "offset": 6282.719,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "what is the number that we expect by the",
      "offset": 6284.48,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "way in the loss we expect to get",
      "offset": 6286.08,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "something around what we had originally",
      "offset": 6288,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "actually",
      "offset": 6290.239,
      "duration": 3.281
    },
    {
      "lang": "en",
      "text": "so all the way back if you remember in",
      "offset": 6292,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "the beginning of this video when we",
      "offset": 6293.52,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "optimized uh just by counting",
      "offset": 6295.84,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "our loss was roughly 2.47",
      "offset": 6298.88,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "after we had it smoothing",
      "offset": 6301.52,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "but before smoothing we had roughly 2.45",
      "offset": 6303.44,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "likelihood",
      "offset": 6306.56,
      "duration": 3.119
    },
    {
      "lang": "en",
      "text": "sorry loss",
      "offset": 6308.32,
      "duration": 2.64
    },
    {
      "lang": "en",
      "text": "and so that's actually roughly the",
      "offset": 6309.679,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "vicinity of what we expect to achieve",
      "offset": 6310.96,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "but before we achieved it by counting",
      "offset": 6313.76,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "and here we are achieving the roughly",
      "offset": 6315.84,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "the same result but with gradient based",
      "offset": 6317.84,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "optimization",
      "offset": 6319.679,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "so we come to about 2.4",
      "offset": 6320.96,
      "duration": 5.279
    },
    {
      "lang": "en",
      "text": "6 2.45 etc",
      "offset": 6323.92,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "and that makes sense because",
      "offset": 6326.239,
      "duration": 2.48
    },
    {
      "lang": "en",
      "text": "fundamentally we're not taking any",
      "offset": 6327.119,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "additional information we're still just",
      "offset": 6328.719,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "taking in the previous character and",
      "offset": 6330.48,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "trying to predict the next one but",
      "offset": 6331.92,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "instead of doing it explicitly by",
      "offset": 6333.84,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "counting and normalizing",
      "offset": 6335.36,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "we are doing it with gradient-based",
      "offset": 6338.159,
      "duration": 3.281
    },
    {
      "lang": "en",
      "text": "learning and it just so happens that the",
      "offset": 6339.36,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "explicit approach happens to very well",
      "offset": 6341.44,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "optimize the loss function without any",
      "offset": 6344,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "need for a gradient based optimization",
      "offset": 6346.32,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "because the setup for bigram language",
      "offset": 6348.48,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "models are is so straightforward that's",
      "offset": 6350.08,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "so simple we can just afford to estimate",
      "offset": 6352,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "those probabilities directly and",
      "offset": 6354.32,
      "duration": 2.799
    },
    {
      "lang": "en",
      "text": "maintain them",
      "offset": 6356,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "in a table",
      "offset": 6357.119,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "but the gradient-based approach is",
      "offset": 6358.8,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "significantly more flexible",
      "offset": 6360.8,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "so we've actually gained a lot",
      "offset": 6362.88,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "because",
      "offset": 6364.8,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "what we can do now is",
      "offset": 6366.56,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "we can expand this approach and",
      "offset": 6369.119,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "complexify the neural net so currently",
      "offset": 6371.04,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "we're just taking a single character and",
      "offset": 6373.28,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "feeding into a neural net and the neural",
      "offset": 6374.719,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "that's extremely simple but we're about",
      "offset": 6376.32,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "to iterate on this substantially we're",
      "offset": 6378.32,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "going to be taking multiple previous",
      "offset": 6380.56,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "characters and we're going to be feeding",
      "offset": 6382.48,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "feeding them into increasingly more",
      "offset": 6384.56,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "complex neural nets but fundamentally",
      "offset": 6386.08,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "out the output of the neural net will",
      "offset": 6388.56,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "always just be logics",
      "offset": 6390.159,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "and those logits will go through the",
      "offset": 6392.56,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "exact same transformation we are going",
      "offset": 6394,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "to take them through a soft max",
      "offset": 6396,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "calculate the loss function and the",
      "offset": 6397.84,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "negative log likelihood and do gradient",
      "offset": 6399.52,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "based optimization and so actually",
      "offset": 6402.08,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "as we complexify the neural nets and",
      "offset": 6404.88,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "work all the way up to transformers",
      "offset": 6407.44,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "none of this will really fundamentally",
      "offset": 6409.76,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "change none of this will fundamentally",
      "offset": 6411.44,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "change the only thing that will change",
      "offset": 6412.96,
      "duration": 2.64
    },
    {
      "lang": "en",
      "text": "is",
      "offset": 6414.639,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "the way we do the forward pass where we",
      "offset": 6415.6,
      "duration": 3.519
    },
    {
      "lang": "en",
      "text": "take in some previous characters and",
      "offset": 6417.679,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "calculate logits for the next character",
      "offset": 6419.119,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "in the sequence that will become more",
      "offset": 6421.679,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "complex",
      "offset": 6423.92,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "and uh but we'll use the same machinery",
      "offset": 6425.119,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "to optimize it",
      "offset": 6427.36,
      "duration": 3.279
    },
    {
      "lang": "en",
      "text": "and um",
      "offset": 6428.8,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "it's not obvious how we would have",
      "offset": 6430.639,
      "duration": 2.48
    },
    {
      "lang": "en",
      "text": "extended",
      "offset": 6432.08,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "this bigram approach",
      "offset": 6433.119,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "into the case where there are many more",
      "offset": 6434.88,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "characters at the input because",
      "offset": 6437.36,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "eventually these tables would get way",
      "offset": 6439.36,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "too large because there's way too many",
      "offset": 6441.36,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "combinations of what previous characters",
      "offset": 6443.119,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "could be",
      "offset": 6446.159,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "if you only have one previous character",
      "offset": 6447.76,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "we can just keep everything in a table",
      "offset": 6449.6,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "that counts but if you have the last 10",
      "offset": 6451.119,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "characters that are input we can't",
      "offset": 6453.6,
      "duration": 3.119
    },
    {
      "lang": "en",
      "text": "actually keep everything in the table",
      "offset": 6455.44,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "anymore so this is fundamentally an",
      "offset": 6456.719,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "unscalable approach and the neural",
      "offset": 6458.48,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "network approach is significantly more",
      "offset": 6460.239,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "scalable and it's something that",
      "offset": 6462.08,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "actually we can improve on over time so",
      "offset": 6464,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "that's where we will be digging next i",
      "offset": 6466.8,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "wanted to point out two more things",
      "offset": 6468.56,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "number one",
      "offset": 6471.04,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "i want you to notice that",
      "offset": 6472.239,
      "duration": 2.801
    },
    {
      "lang": "en",
      "text": "this",
      "offset": 6474,
      "duration": 2.639
    },
    {
      "lang": "en",
      "text": "x ink here",
      "offset": 6475.04,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "this is made up of one hot vectors and",
      "offset": 6476.639,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "then those one hot vectors are",
      "offset": 6479.04,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "multiplied by this w matrix",
      "offset": 6480.4,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "and we think of this as multiple neurons",
      "offset": 6483.119,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "being forwarded in a fully connected",
      "offset": 6485.76,
      "duration": 2.879
    },
    {
      "lang": "en",
      "text": "manner",
      "offset": 6487.44,
      "duration": 2.56
    },
    {
      "lang": "en",
      "text": "but actually what's happening here is",
      "offset": 6488.639,
      "duration": 3.281
    },
    {
      "lang": "en",
      "text": "that for example",
      "offset": 6490,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "if you have a one hot vector here that",
      "offset": 6491.92,
      "duration": 5.759
    },
    {
      "lang": "en",
      "text": "has a one at say the fifth dimension",
      "offset": 6494.32,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "then because of the way the matrix",
      "offset": 6497.679,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "multiplication works",
      "offset": 6499.119,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "multiplying that one-half vector with w",
      "offset": 6501.119,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "actually ends up plucking out the fifth",
      "offset": 6503.44,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "row of w",
      "offset": 6505.6,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "log logits would become just the fifth",
      "offset": 6507.44,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "row of w",
      "offset": 6509.52,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "and that's because of the way the matrix",
      "offset": 6511.199,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "multiplication works",
      "offset": 6512.96,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "um",
      "offset": 6515.199,
      "duration": 2.561
    },
    {
      "lang": "en",
      "text": "so",
      "offset": 6516.88,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "that's actually what ends up happening",
      "offset": 6517.76,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "so but that's actually exactly what",
      "offset": 6520,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "happened before",
      "offset": 6522,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "because remember all the way up here",
      "offset": 6523.199,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "we have a bigram we took the first",
      "offset": 6526.639,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "character and then that first character",
      "offset": 6528.239,
      "duration": 6.721
    },
    {
      "lang": "en",
      "text": "indexed into a row of this array here",
      "offset": 6530.32,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "and that row gave us the probability",
      "offset": 6534.96,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "distribution for the next character so",
      "offset": 6536.56,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "the first character was used as a lookup",
      "offset": 6538.719,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "into a",
      "offset": 6541.119,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "matrix here to get the probability",
      "offset": 6543.6,
      "duration": 2.639
    },
    {
      "lang": "en",
      "text": "distribution",
      "offset": 6545.119,
      "duration": 2.321
    },
    {
      "lang": "en",
      "text": "well that's actually exactly what's",
      "offset": 6546.239,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "happening here because we're taking the",
      "offset": 6547.44,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "index we're encoding it as one hot and",
      "offset": 6549.6,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "multiplying it by w",
      "offset": 6551.84,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "so logics literally becomes",
      "offset": 6553.44,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "the",
      "offset": 6555.679,
      "duration": 2.161
    },
    {
      "lang": "en",
      "text": "the appropriate row of w",
      "offset": 6558,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "and that gets just as before",
      "offset": 6560.8,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "exponentiated to create the counts",
      "offset": 6562.56,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "and then normalized and becomes",
      "offset": 6564.88,
      "duration": 2.56
    },
    {
      "lang": "en",
      "text": "probability",
      "offset": 6566.239,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "so this w here",
      "offset": 6567.44,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "is literally",
      "offset": 6569.44,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "the same as this array here",
      "offset": 6571.36,
      "duration": 6.879
    },
    {
      "lang": "en",
      "text": "but w remember is the log counts not the",
      "offset": 6575.04,
      "duration": 5.599
    },
    {
      "lang": "en",
      "text": "counts so it's more precise to say that",
      "offset": 6578.239,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "w exponentiated",
      "offset": 6580.639,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "w dot x is this array",
      "offset": 6582.8,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "but this array was filled in by counting",
      "offset": 6586.159,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "and by",
      "offset": 6589.28,
      "duration": 2.64
    },
    {
      "lang": "en",
      "text": "basically",
      "offset": 6590.4,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "populating the counts of bi-grams",
      "offset": 6591.92,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "whereas in the gradient-based framework",
      "offset": 6593.84,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "we initialize it randomly and then we",
      "offset": 6595.84,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "let the loss",
      "offset": 6597.92,
      "duration": 2.56
    },
    {
      "lang": "en",
      "text": "guide us",
      "offset": 6599.36,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "to arrive at the exact same array",
      "offset": 6600.48,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "so this array exactly here",
      "offset": 6603.199,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "is",
      "offset": 6605.76,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "basically the array w at the end of",
      "offset": 6606.719,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "optimization except we arrived at it",
      "offset": 6609.119,
      "duration": 5.841
    },
    {
      "lang": "en",
      "text": "piece by piece by following the loss",
      "offset": 6612.239,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "and that's why we also obtain the same",
      "offset": 6614.96,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "loss function at the end and the second",
      "offset": 6616.639,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "note is if i come here",
      "offset": 6618.56,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "remember the smoothing where we added",
      "offset": 6620.48,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "fake counts to our counts",
      "offset": 6622.56,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "in order to",
      "offset": 6624.88,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "smooth out and make more uniform the",
      "offset": 6626,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "distributions of these probabilities",
      "offset": 6628.48,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "and that prevented us from assigning",
      "offset": 6630.96,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "zero probability to",
      "offset": 6632.48,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "to any one bigram",
      "offset": 6634.96,
      "duration": 5.279
    },
    {
      "lang": "en",
      "text": "now if i increase the count here",
      "offset": 6637.119,
      "duration": 5.681
    },
    {
      "lang": "en",
      "text": "what's happening to the probability",
      "offset": 6640.239,
      "duration": 5.041
    },
    {
      "lang": "en",
      "text": "as i increase the count probability",
      "offset": 6642.8,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "becomes more and more uniform",
      "offset": 6645.28,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "right because these counts go only up to",
      "offset": 6647.92,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "like 900 or whatever so if i'm adding",
      "offset": 6650.32,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "plus a million to every single number",
      "offset": 6652.56,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "here you can see how",
      "offset": 6654.32,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "the row and its probability then when we",
      "offset": 6656.4,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "divide is just going to become more and",
      "offset": 6658.4,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "more close to exactly even probability",
      "offset": 6660.159,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "uniform distribution",
      "offset": 6662.96,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "it turns out that the gradient based",
      "offset": 6665.119,
      "duration": 5.681
    },
    {
      "lang": "en",
      "text": "framework has an equivalent to smoothing",
      "offset": 6666.56,
      "duration": 6.559
    },
    {
      "lang": "en",
      "text": "in particular",
      "offset": 6670.8,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "think through these w's here",
      "offset": 6673.119,
      "duration": 5.361
    },
    {
      "lang": "en",
      "text": "which we initialized randomly",
      "offset": 6675.84,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "we could also think about initializing",
      "offset": 6678.48,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "w's to be zero",
      "offset": 6680.08,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "if all the entries of w are zero",
      "offset": 6682.08,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "then you'll see that logits will become",
      "offset": 6685.92,
      "duration": 2.799
    },
    {
      "lang": "en",
      "text": "all zero",
      "offset": 6687.52,
      "duration": 2.639
    },
    {
      "lang": "en",
      "text": "and then exponentiating those logics",
      "offset": 6688.719,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "becomes all one",
      "offset": 6690.159,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "and then the probabilities turned out to",
      "offset": 6692.08,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "be exactly uniform",
      "offset": 6693.84,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "so basically when w's are all equal to",
      "offset": 6695.679,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "each other or say especially zero",
      "offset": 6698,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "then the probabilities come out",
      "offset": 6701.199,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "completely uniform",
      "offset": 6702.639,
      "duration": 2.801
    },
    {
      "lang": "en",
      "text": "so",
      "offset": 6704.4,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "trying to incentivize w to be near zero",
      "offset": 6705.44,
      "duration": 6.4
    },
    {
      "lang": "en",
      "text": "is basically equivalent to",
      "offset": 6709.199,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "label smoothing and the more you",
      "offset": 6711.84,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "incentivize that in the loss function",
      "offset": 6713.44,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "the more smooth distribution you're",
      "offset": 6715.44,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "going to achieve",
      "offset": 6717.36,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "so this brings us to something that's",
      "offset": 6718.88,
      "duration": 1.96
    },
    {
      "lang": "en",
      "text": "called",
      "offset": 6720.08,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "regularization where we can actually",
      "offset": 6720.84,
      "duration": 4.12
    },
    {
      "lang": "en",
      "text": "augment the loss function to have a",
      "offset": 6722.88,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "small component that we call a",
      "offset": 6724.96,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "regularization loss",
      "offset": 6726.48,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "in particular what we're going to do is",
      "offset": 6728.96,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "we can take w and we can for example",
      "offset": 6730.159,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "square all of its entries",
      "offset": 6732.4,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "and then we can um whoops",
      "offset": 6734.56,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "sorry about that",
      "offset": 6737.76,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "we can take all the entries of w and we",
      "offset": 6738.96,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "can sum them",
      "offset": 6740.8,
      "duration": 2.56
    },
    {
      "lang": "en",
      "text": "and because we're squaring uh there will",
      "offset": 6743.52,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "be no signs anymore um",
      "offset": 6745.76,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "negatives and positives all get squashed",
      "offset": 6748.239,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "to be positive numbers",
      "offset": 6749.84,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "and then the way this works is you",
      "offset": 6751.44,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "achieve zero loss if w is exactly or",
      "offset": 6753.76,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "zero but if w has non-zero numbers you",
      "offset": 6756.239,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "accumulate loss",
      "offset": 6759.44,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "and so we can actually take this and we",
      "offset": 6761.119,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "can add it on here",
      "offset": 6762.8,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "so we can do something like loss plus",
      "offset": 6764.88,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "w square",
      "offset": 6768.8,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "dot sum",
      "offset": 6770.4,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "or let's actually instead of sum let's",
      "offset": 6771.92,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "take a mean because otherwise the sum",
      "offset": 6773.599,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "gets too large",
      "offset": 6775.44,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "so mean is like a little bit more",
      "offset": 6777.44,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "manageable",
      "offset": 6778.88,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "and then we have a regularization loss",
      "offset": 6781.28,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "here say 0.01 times",
      "offset": 6782.96,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "or something like that you can choose",
      "offset": 6785.52,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "the regularization strength",
      "offset": 6786.719,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "and then we can just optimize this and",
      "offset": 6789.28,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "now this optimization actually has two",
      "offset": 6792.239,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "components not only is it trying to make",
      "offset": 6794.4,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "all the probabilities work out but in",
      "offset": 6796.4,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "addition to that there's an additional",
      "offset": 6798.56,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "component that simultaneously tries to",
      "offset": 6799.84,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "make all w's be zero because if w's are",
      "offset": 6802.08,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "non-zero you feel a loss and so",
      "offset": 6804.719,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "minimizing this the only way to achieve",
      "offset": 6806.48,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "that is for w to be zero",
      "offset": 6808.48,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "and so you can think of this as adding",
      "offset": 6810.639,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "like a spring force or like a gravity",
      "offset": 6812.48,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "force that that pushes w to be zero so w",
      "offset": 6814.639,
      "duration": 5.361
    },
    {
      "lang": "en",
      "text": "wants to be zero and the probabilities",
      "offset": 6818.32,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "want to be uniform but they also",
      "offset": 6820,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "simultaneously want to match up your",
      "offset": 6822,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "your probabilities as indicated by the",
      "offset": 6824.239,
      "duration": 3.121
    },
    {
      "lang": "en",
      "text": "data",
      "offset": 6826.32,
      "duration": 2.879
    },
    {
      "lang": "en",
      "text": "and so the strength of this",
      "offset": 6827.36,
      "duration": 5.279
    },
    {
      "lang": "en",
      "text": "regularization is exactly controlling",
      "offset": 6829.199,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "the amount of counts",
      "offset": 6832.639,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "that you add here",
      "offset": 6834.4,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "adding a lot more counts",
      "offset": 6837.199,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "here",
      "offset": 6839.36,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "corresponds to",
      "offset": 6840.639,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "increasing this number",
      "offset": 6842.8,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "because the more you increase it the",
      "offset": 6844.639,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "more this part of the loss function",
      "offset": 6846.48,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "dominates this part and the more these",
      "offset": 6848,
      "duration": 5.599
    },
    {
      "lang": "en",
      "text": "these weights will un be unable to grow",
      "offset": 6850.88,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "because as they grow",
      "offset": 6853.599,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "they uh accumulate way too much loss",
      "offset": 6855.44,
      "duration": 5.759
    },
    {
      "lang": "en",
      "text": "and so if this is strong enough",
      "offset": 6858.32,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "then we are not able to overcome the",
      "offset": 6861.199,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "force of this loss and we will never",
      "offset": 6863.44,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "and basically everything will be uniform",
      "offset": 6866.719,
      "duration": 2.561
    },
    {
      "lang": "en",
      "text": "predictions",
      "offset": 6868.08,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "so i thought that's kind of cool okay",
      "offset": 6869.28,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "and lastly before we wrap up",
      "offset": 6870.96,
      "duration": 3.279
    },
    {
      "lang": "en",
      "text": "i wanted to show you how you would",
      "offset": 6873.04,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "sample from this neural net model",
      "offset": 6874.239,
      "duration": 5.041
    },
    {
      "lang": "en",
      "text": "and i copy-pasted the sampling code from",
      "offset": 6876.8,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "before",
      "offset": 6879.28,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "where remember that we sampled five",
      "offset": 6880.719,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "times",
      "offset": 6883.28,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "and all we did we start at zero we",
      "offset": 6884.239,
      "duration": 6.241
    },
    {
      "lang": "en",
      "text": "grabbed the current ix row of p and that",
      "offset": 6886.4,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "was our probability row",
      "offset": 6890.48,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "from which we sampled the next index and",
      "offset": 6892.32,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "just accumulated that and",
      "offset": 6894.88,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "break when zero",
      "offset": 6896.719,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "and running this",
      "offset": 6898.88,
      "duration": 3.96
    },
    {
      "lang": "en",
      "text": "gave us these",
      "offset": 6900.32,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "results still have the",
      "offset": 6902.84,
      "duration": 4.759
    },
    {
      "lang": "en",
      "text": "p in memory so this is fine",
      "offset": 6905.52,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "now",
      "offset": 6907.599,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "the speed doesn't come from the row of b",
      "offset": 6909.119,
      "duration": 5.761
    },
    {
      "lang": "en",
      "text": "instead it comes from this neural net",
      "offset": 6911.84,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "first we take ix",
      "offset": 6914.88,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "and we encode it into a one hot row of x",
      "offset": 6917.04,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "inc",
      "offset": 6921.04,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "this x inc multiplies rw",
      "offset": 6922.4,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "which really just plucks out the row of",
      "offset": 6925.119,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "w corresponding to ix really that's",
      "offset": 6926.88,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "what's happening",
      "offset": 6929.36,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "and that gets our logits and then we",
      "offset": 6930.4,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "normalize those low jets",
      "offset": 6933.119,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "exponentiate to get counts and then",
      "offset": 6934.719,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "normalize to get uh the distribution and",
      "offset": 6936.56,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "then we can sample from the distribution",
      "offset": 6939.04,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "so if i run this",
      "offset": 6941.28,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "kind of anticlimactic or climatic",
      "offset": 6945.119,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "depending how you look at it but we get",
      "offset": 6947.199,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "the exact same result",
      "offset": 6948.88,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "um",
      "offset": 6950.88,
      "duration": 2.719
    },
    {
      "lang": "en",
      "text": "and that's because this is in the",
      "offset": 6952.239,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "identical model not only does it achieve",
      "offset": 6953.599,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "the same loss",
      "offset": 6955.679,
      "duration": 2.401
    },
    {
      "lang": "en",
      "text": "but",
      "offset": 6956.96,
      "duration": 2.32
    },
    {
      "lang": "en",
      "text": "as i mentioned these are identical",
      "offset": 6958.08,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "models and this w is the log counts of",
      "offset": 6959.28,
      "duration": 5.839
    },
    {
      "lang": "en",
      "text": "what we've estimated before but we came",
      "offset": 6962.48,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "to this answer in a very different way",
      "offset": 6965.119,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "and it's got a very different",
      "offset": 6967.36,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "interpretation but fundamentally this is",
      "offset": 6968.32,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "basically the same model and gives the",
      "offset": 6970.32,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "same samples here and so",
      "offset": 6971.679,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "that's kind of cool okay so we've",
      "offset": 6974.8,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "actually covered a lot of ground we",
      "offset": 6976.159,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "introduced the bigram character level",
      "offset": 6978.159,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "language model",
      "offset": 6980.48,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "we saw how we can train the model how we",
      "offset": 6981.92,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "can sample from the model and how we can",
      "offset": 6984.08,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "evaluate the quality of the model using",
      "offset": 6985.92,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "the negative log likelihood loss",
      "offset": 6988.08,
      "duration": 3.519
    },
    {
      "lang": "en",
      "text": "and then we actually trained the model",
      "offset": 6990.159,
      "duration": 3.281
    },
    {
      "lang": "en",
      "text": "in two completely different ways that",
      "offset": 6991.599,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "actually get the same result and the",
      "offset": 6993.44,
      "duration": 2.799
    },
    {
      "lang": "en",
      "text": "same model",
      "offset": 6994.96,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "in the first way we just counted up the",
      "offset": 6996.239,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "frequency of all the bigrams and",
      "offset": 6998.639,
      "duration": 2.801
    },
    {
      "lang": "en",
      "text": "normalized",
      "offset": 7000.159,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "in a second way we used the",
      "offset": 7001.44,
      "duration": 6.239
    },
    {
      "lang": "en",
      "text": "negative log likelihood loss as a guide",
      "offset": 7004.56,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "to optimizing the counts matrix",
      "offset": 7007.679,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "or the counts array so that the loss is",
      "offset": 7010.8,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "minimized in the in a gradient-based",
      "offset": 7012.719,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "framework and we saw that both of them",
      "offset": 7014.96,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "give the same result",
      "offset": 7016.8,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "and",
      "offset": 7018.4,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "that's it",
      "offset": 7020.32,
      "duration": 2.16
    },
    {
      "lang": "en",
      "text": "now the second one of these the",
      "offset": 7021.36,
      "duration": 2.56
    },
    {
      "lang": "en",
      "text": "gradient-based framework is much more",
      "offset": 7022.48,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "flexible and right now our neural",
      "offset": 7023.92,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "network is super simple we're taking a",
      "offset": 7026.08,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "single previous character and we're",
      "offset": 7028.159,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "taking it through a single linear layer",
      "offset": 7030.56,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "to calculate the logits",
      "offset": 7032.32,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "this is about to complexify so in the",
      "offset": 7034,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "follow-up videos we're going to be",
      "offset": 7036.239,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "taking more and more of these characters",
      "offset": 7037.76,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "and we're going to be feeding them into",
      "offset": 7040.48,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "a neural net but this neural net will",
      "offset": 7041.84,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "still output the exact same thing the",
      "offset": 7043.76,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "neural net will output logits",
      "offset": 7045.36,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "and these logits will still be",
      "offset": 7047.92,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "normalized in the exact same way and all",
      "offset": 7049.199,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "the loss and everything else and the",
      "offset": 7050.88,
      "duration": 2.799
    },
    {
      "lang": "en",
      "text": "gradient gradient-based framework",
      "offset": 7052.239,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "everything stays identical it's just",
      "offset": 7053.679,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "that this neural net will now complexify",
      "offset": 7055.679,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "all the way to transformers",
      "offset": 7058.56,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "so that's gonna be pretty awesome and",
      "offset": 7060.48,
      "duration": 6.239
    },
    {
      "lang": "en",
      "text": "i'm looking forward to it for now bye",
      "offset": 7062.239,
      "duration": 4.48
    }
  ],
  "cleanText": null,
  "dumpedAt": "2025-07-21T18:43:24.220Z"
}