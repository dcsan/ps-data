{
  "episodeId": "t3YJ5hKiMQ0",
  "channelSlug": "@andrejkarpathy",
  "title": "Building makemore Part 5: Building a WaveNet",
  "publishedAt": "2022-11-21T00:32:48.000Z",
  "rawLines": [
    {
      "lang": "en",
      "text": "hi everyone today we are continuing our",
      "offset": 0,
      "duration": 4.5
    },
    {
      "lang": "en",
      "text": "implementation of make more our favorite",
      "offset": 2.58,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "character level language model",
      "offset": 4.5,
      "duration": 3.48
    },
    {
      "lang": "en",
      "text": "now you'll notice that the background",
      "offset": 6.42,
      "duration": 3.3
    },
    {
      "lang": "en",
      "text": "behind me is different that's because I",
      "offset": 7.98,
      "duration": 4.02
    },
    {
      "lang": "en",
      "text": "am in Kyoto and it is awesome so I'm in",
      "offset": 9.72,
      "duration": 3.72
    },
    {
      "lang": "en",
      "text": "a hotel room here",
      "offset": 12,
      "duration": 3.059
    },
    {
      "lang": "en",
      "text": "now over the last few lectures we've",
      "offset": 13.44,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "built up to this architecture that is a",
      "offset": 15.059,
      "duration": 4.681
    },
    {
      "lang": "en",
      "text": "multi-layer perceptron character level",
      "offset": 17.76,
      "duration": 3.72
    },
    {
      "lang": "en",
      "text": "language model so we see that it",
      "offset": 19.74,
      "duration": 3.299
    },
    {
      "lang": "en",
      "text": "receives three previous characters and",
      "offset": 21.48,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "tries to predict the fourth character in",
      "offset": 23.039,
      "duration": 3.721
    },
    {
      "lang": "en",
      "text": "a sequence using a very simple multi",
      "offset": 24.6,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "perceptron using one hidden layer of",
      "offset": 26.76,
      "duration": 4.679
    },
    {
      "lang": "en",
      "text": "neurons with 10ational neuralities",
      "offset": 28.92,
      "duration": 4.38
    },
    {
      "lang": "en",
      "text": "so we'd like to do now in this lecture",
      "offset": 31.439,
      "duration": 3.181
    },
    {
      "lang": "en",
      "text": "is I'd like to complexify this",
      "offset": 33.3,
      "duration": 3.419
    },
    {
      "lang": "en",
      "text": "architecture in particular we would like",
      "offset": 34.62,
      "duration": 4.2
    },
    {
      "lang": "en",
      "text": "to take more characters in a sequence as",
      "offset": 36.719,
      "duration": 4.381
    },
    {
      "lang": "en",
      "text": "an input not just three and in addition",
      "offset": 38.82,
      "duration": 4.14
    },
    {
      "lang": "en",
      "text": "to that we don't just want to feed them",
      "offset": 41.1,
      "duration": 3.9
    },
    {
      "lang": "en",
      "text": "all into a single hidden layer because",
      "offset": 42.96,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "that squashes too much information too",
      "offset": 45,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "quickly instead we would like to make a",
      "offset": 46.8,
      "duration": 4.739
    },
    {
      "lang": "en",
      "text": "deeper model that progressively fuses",
      "offset": 49.079,
      "duration": 4.5
    },
    {
      "lang": "en",
      "text": "this information to make its guess about",
      "offset": 51.539,
      "duration": 4.02
    },
    {
      "lang": "en",
      "text": "the next character in a sequence",
      "offset": 53.579,
      "duration": 3.96
    },
    {
      "lang": "en",
      "text": "and so we'll see that as we make this",
      "offset": 55.559,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "architecture more complex we're actually",
      "offset": 57.539,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "going to arrive at something that looks",
      "offset": 59.879,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "very much like a wavenet",
      "offset": 61.62,
      "duration": 3.899
    },
    {
      "lang": "en",
      "text": "the witness is this paper published by",
      "offset": 63.719,
      "duration": 5.881
    },
    {
      "lang": "en",
      "text": "the point in 2016 and it is also a",
      "offset": 65.519,
      "duration": 6.42
    },
    {
      "lang": "en",
      "text": "language model basically but it tries to",
      "offset": 69.6,
      "duration": 4.019
    },
    {
      "lang": "en",
      "text": "predict audio sequences instead of",
      "offset": 71.939,
      "duration": 3.901
    },
    {
      "lang": "en",
      "text": "character level sequences or Word level",
      "offset": 73.619,
      "duration": 5.341
    },
    {
      "lang": "en",
      "text": "sequences but fundamentally the modeling",
      "offset": 75.84,
      "duration": 4.98
    },
    {
      "lang": "en",
      "text": "setup is identical it is an auto",
      "offset": 78.96,
      "duration": 4.44
    },
    {
      "lang": "en",
      "text": "aggressive model and it tries to predict",
      "offset": 80.82,
      "duration": 4.619
    },
    {
      "lang": "en",
      "text": "next character in a sequence and the",
      "offset": 83.4,
      "duration": 3.42
    },
    {
      "lang": "en",
      "text": "architecture actually takes this",
      "offset": 85.439,
      "duration": 4.141
    },
    {
      "lang": "en",
      "text": "interesting hierarchical sort of",
      "offset": 86.82,
      "duration": 4.38
    },
    {
      "lang": "en",
      "text": "approach to predicting the next",
      "offset": 89.58,
      "duration": 3.66
    },
    {
      "lang": "en",
      "text": "character in a sequence uh with the",
      "offset": 91.2,
      "duration": 4.62
    },
    {
      "lang": "en",
      "text": "street-like structure and this is the",
      "offset": 93.24,
      "duration": 3.66
    },
    {
      "lang": "en",
      "text": "architecture and we're going to",
      "offset": 95.82,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "implement it in the course of this video",
      "offset": 96.9,
      "duration": 4.74
    },
    {
      "lang": "en",
      "text": "so let's get started so the starter code",
      "offset": 98.82,
      "duration": 4.979
    },
    {
      "lang": "en",
      "text": "for part five is very similar to where",
      "offset": 101.64,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "we ended up in in part three recall that",
      "offset": 103.799,
      "duration": 4.14
    },
    {
      "lang": "en",
      "text": "part four was the manual black",
      "offset": 106.439,
      "duration": 3.121
    },
    {
      "lang": "en",
      "text": "replication exercise that is kind of an",
      "offset": 107.939,
      "duration": 3.661
    },
    {
      "lang": "en",
      "text": "aside so we are coming back to part",
      "offset": 109.56,
      "duration": 4.199
    },
    {
      "lang": "en",
      "text": "three copy pasting chunks out of it and",
      "offset": 111.6,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "that is our starter code for part five",
      "offset": 113.759,
      "duration": 3.301
    },
    {
      "lang": "en",
      "text": "I've changed very few things otherwise",
      "offset": 115.2,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "so a lot of this should look familiar to",
      "offset": 117.06,
      "duration": 4.14
    },
    {
      "lang": "en",
      "text": "if you've gone through part three so in",
      "offset": 119.04,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "particular very briefly we are doing",
      "offset": 121.2,
      "duration": 4.199
    },
    {
      "lang": "en",
      "text": "Imports we are reading our our data set",
      "offset": 123.119,
      "duration": 5.941
    },
    {
      "lang": "en",
      "text": "of words and we are processing their set",
      "offset": 125.399,
      "duration": 6.121
    },
    {
      "lang": "en",
      "text": "of words into individual examples and",
      "offset": 129.06,
      "duration": 4.259
    },
    {
      "lang": "en",
      "text": "none of this data generation code has",
      "offset": 131.52,
      "duration": 3.719
    },
    {
      "lang": "en",
      "text": "changed and basically we have lots and",
      "offset": 133.319,
      "duration": 4.381
    },
    {
      "lang": "en",
      "text": "lots of examples in particular we have",
      "offset": 135.239,
      "duration": 6.301
    },
    {
      "lang": "en",
      "text": "182 000 examples of three characters try",
      "offset": 137.7,
      "duration": 6.36
    },
    {
      "lang": "en",
      "text": "to predict the fourth one and we've",
      "offset": 141.54,
      "duration": 4.26
    },
    {
      "lang": "en",
      "text": "broken up every one of these words into",
      "offset": 144.06,
      "duration": 3.72
    },
    {
      "lang": "en",
      "text": "little problems of given three",
      "offset": 145.8,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "characters predict the fourth one so",
      "offset": 147.78,
      "duration": 3.06
    },
    {
      "lang": "en",
      "text": "this is our data set and this is what",
      "offset": 149.64,
      "duration": 3.3
    },
    {
      "lang": "en",
      "text": "we're trying to get the neural lot to do",
      "offset": 150.84,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "now in part three we started to develop",
      "offset": 152.94,
      "duration": 6.6
    },
    {
      "lang": "en",
      "text": "our code around these layer modules",
      "offset": 155.16,
      "duration": 5.82
    },
    {
      "lang": "en",
      "text": "um that are for example like class",
      "offset": 159.54,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "linear and we're doing this because we",
      "offset": 160.98,
      "duration": 3.72
    },
    {
      "lang": "en",
      "text": "want to think of these modules as",
      "offset": 162.9,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "building blocks and like a Lego building",
      "offset": 164.7,
      "duration": 4.44
    },
    {
      "lang": "en",
      "text": "block bricks that we can sort of like",
      "offset": 167.459,
      "duration": 4.441
    },
    {
      "lang": "en",
      "text": "stack up into neural networks and we can",
      "offset": 169.14,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "feed data between these layers and stack",
      "offset": 171.9,
      "duration": 4.74
    },
    {
      "lang": "en",
      "text": "them up into a sort of graphs",
      "offset": 173.459,
      "duration": 5.581
    },
    {
      "lang": "en",
      "text": "now we also developed these layers to",
      "offset": 176.64,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "have apis and signatures very similar to",
      "offset": 179.04,
      "duration": 5.1
    },
    {
      "lang": "en",
      "text": "those that are found in pytorch so we",
      "offset": 181.68,
      "duration": 4.26
    },
    {
      "lang": "en",
      "text": "have torch.nn and it's got all these",
      "offset": 184.14,
      "duration": 3.179
    },
    {
      "lang": "en",
      "text": "layer building blocks that you would use",
      "offset": 185.94,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "in practice and we were developing all",
      "offset": 187.319,
      "duration": 4.441
    },
    {
      "lang": "en",
      "text": "of these to mimic the apis of these so",
      "offset": 189.3,
      "duration": 4.5
    },
    {
      "lang": "en",
      "text": "for example we have linear so there will",
      "offset": 191.76,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "also be a torch.nn.linear and its",
      "offset": 193.8,
      "duration": 4.92
    },
    {
      "lang": "en",
      "text": "signature will be very similar to our",
      "offset": 197.28,
      "duration": 3.3
    },
    {
      "lang": "en",
      "text": "signature and the functionality will be",
      "offset": 198.72,
      "duration": 3.72
    },
    {
      "lang": "en",
      "text": "also quite identical as far as I'm aware",
      "offset": 200.58,
      "duration": 4.019
    },
    {
      "lang": "en",
      "text": "so we have the linear layer with the",
      "offset": 202.44,
      "duration": 4.74
    },
    {
      "lang": "en",
      "text": "Bass from 1D layer and the 10h layer",
      "offset": 204.599,
      "duration": 4.5
    },
    {
      "lang": "en",
      "text": "that we developed previously",
      "offset": 207.18,
      "duration": 4.86
    },
    {
      "lang": "en",
      "text": "and linear just as a matrix multiply in",
      "offset": 209.099,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "the forward pass of this module batch",
      "offset": 212.04,
      "duration": 4.5
    },
    {
      "lang": "en",
      "text": "number of course is this crazy layer",
      "offset": 215.099,
      "duration": 2.341
    },
    {
      "lang": "en",
      "text": "that we developed in the previous",
      "offset": 216.54,
      "duration": 3.779
    },
    {
      "lang": "en",
      "text": "lecture and what's crazy about it is",
      "offset": 217.44,
      "duration": 5.519
    },
    {
      "lang": "en",
      "text": "well there's many things number one it",
      "offset": 220.319,
      "duration": 4.441
    },
    {
      "lang": "en",
      "text": "has these running mean and variances",
      "offset": 222.959,
      "duration": 3.541
    },
    {
      "lang": "en",
      "text": "that are trained outside of back",
      "offset": 224.76,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "propagation they are trained using",
      "offset": 226.5,
      "duration": 5.819
    },
    {
      "lang": "en",
      "text": "exponential moving average inside this",
      "offset": 229.159,
      "duration": 5.261
    },
    {
      "lang": "en",
      "text": "layer when we call the forward pass",
      "offset": 232.319,
      "duration": 4.381
    },
    {
      "lang": "en",
      "text": "in addition to that",
      "offset": 234.42,
      "duration": 4.2
    },
    {
      "lang": "en",
      "text": "there's this training plug because the",
      "offset": 236.7,
      "duration": 3.179
    },
    {
      "lang": "en",
      "text": "behavior of bathroom is different during",
      "offset": 238.62,
      "duration": 3.72
    },
    {
      "lang": "en",
      "text": "train time and evaluation time and so",
      "offset": 239.879,
      "duration": 3.661
    },
    {
      "lang": "en",
      "text": "suddenly we have to be very careful that",
      "offset": 242.34,
      "duration": 3.24
    },
    {
      "lang": "en",
      "text": "bash form is in its correct state that",
      "offset": 243.54,
      "duration": 3.779
    },
    {
      "lang": "en",
      "text": "it's in the evaluation state or training",
      "offset": 245.58,
      "duration": 3.239
    },
    {
      "lang": "en",
      "text": "state so that's something to now keep",
      "offset": 247.319,
      "duration": 2.881
    },
    {
      "lang": "en",
      "text": "track of something that sometimes",
      "offset": 248.819,
      "duration": 2.761
    },
    {
      "lang": "en",
      "text": "introduces bugs",
      "offset": 250.2,
      "duration": 3.48
    },
    {
      "lang": "en",
      "text": "uh because you forget to put it into the",
      "offset": 251.58,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "right mode and finally we saw that",
      "offset": 253.68,
      "duration": 4.38
    },
    {
      "lang": "en",
      "text": "Bachelor couples the statistics or the",
      "offset": 255.659,
      "duration": 4.98
    },
    {
      "lang": "en",
      "text": "the activations across the examples in",
      "offset": 258.06,
      "duration": 4.859
    },
    {
      "lang": "en",
      "text": "the batch so normally we thought of the",
      "offset": 260.639,
      "duration": 4.941
    },
    {
      "lang": "en",
      "text": "bat as just an efficiency thing but now",
      "offset": 262.919,
      "duration": 5.641
    },
    {
      "lang": "en",
      "text": "we are coupling the computation across",
      "offset": 265.58,
      "duration": 5.08
    },
    {
      "lang": "en",
      "text": "batch elements and it's done for the",
      "offset": 268.56,
      "duration": 3.48
    },
    {
      "lang": "en",
      "text": "purposes of controlling the automation",
      "offset": 270.66,
      "duration": 3.24
    },
    {
      "lang": "en",
      "text": "statistics as we saw in the previous",
      "offset": 272.04,
      "duration": 2.939
    },
    {
      "lang": "en",
      "text": "video",
      "offset": 273.9,
      "duration": 3.06
    },
    {
      "lang": "en",
      "text": "so it's a very weird layer at least a",
      "offset": 274.979,
      "duration": 3.381
    },
    {
      "lang": "en",
      "text": "lot of bugs",
      "offset": 276.96,
      "duration": 3.48
    },
    {
      "lang": "en",
      "text": "partly for example because you have to",
      "offset": 278.36,
      "duration": 3.94
    },
    {
      "lang": "en",
      "text": "modulate the training in eval phase and",
      "offset": 280.44,
      "duration": 4.14
    },
    {
      "lang": "en",
      "text": "so on",
      "offset": 282.3,
      "duration": 4.14
    },
    {
      "lang": "en",
      "text": "um in addition for example you have to",
      "offset": 284.58,
      "duration": 4.619
    },
    {
      "lang": "en",
      "text": "wait for uh the mean and the variance to",
      "offset": 286.44,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "settle and to actually reach a steady",
      "offset": 289.199,
      "duration": 4.621
    },
    {
      "lang": "en",
      "text": "state and so um you have to make sure",
      "offset": 291.24,
      "duration": 4.44
    },
    {
      "lang": "en",
      "text": "that you basically there's state in this",
      "offset": 293.82,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "layer and state is harmful uh usually",
      "offset": 295.68,
      "duration": 7.019
    },
    {
      "lang": "en",
      "text": "now I brought out the generator object",
      "offset": 299.82,
      "duration": 4.98
    },
    {
      "lang": "en",
      "text": "previously we had a generator equals g",
      "offset": 302.699,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "and so on inside these layers I've",
      "offset": 304.8,
      "duration": 3.72
    },
    {
      "lang": "en",
      "text": "discarded that in favor of just",
      "offset": 307.02,
      "duration": 5.94
    },
    {
      "lang": "en",
      "text": "initializing the torch RNG outside here",
      "offset": 308.52,
      "duration": 7.02
    },
    {
      "lang": "en",
      "text": "use it just once globally just for",
      "offset": 312.96,
      "duration": 3.78
    },
    {
      "lang": "en",
      "text": "Simplicity",
      "offset": 315.54,
      "duration": 2.7
    },
    {
      "lang": "en",
      "text": "and then here we are starting to build",
      "offset": 316.74,
      "duration": 3.179
    },
    {
      "lang": "en",
      "text": "out some of the neural network elements",
      "offset": 318.24,
      "duration": 4.5
    },
    {
      "lang": "en",
      "text": "this should look very familiar we are we",
      "offset": 319.919,
      "duration": 4.741
    },
    {
      "lang": "en",
      "text": "have our embedding table C and then we",
      "offset": 322.74,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "have a list of players and uh it's a",
      "offset": 324.66,
      "duration": 4.52
    },
    {
      "lang": "en",
      "text": "linear feeds to Bachelor feeds to 10h",
      "offset": 327.06,
      "duration": 5.34
    },
    {
      "lang": "en",
      "text": "and then a linear output layer and its",
      "offset": 329.18,
      "duration": 4.66
    },
    {
      "lang": "en",
      "text": "weights are scaled down so we are not",
      "offset": 332.4,
      "duration": 4.139
    },
    {
      "lang": "en",
      "text": "confidently wrong at the initialization",
      "offset": 333.84,
      "duration": 4.26
    },
    {
      "lang": "en",
      "text": "we see that this is about 12 000",
      "offset": 336.539,
      "duration": 3.961
    },
    {
      "lang": "en",
      "text": "parameters we're telling pytorch that",
      "offset": 338.1,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "the parameters require gradients",
      "offset": 340.5,
      "duration": 4.44
    },
    {
      "lang": "en",
      "text": "the optimization is as far as I'm aware",
      "offset": 342.66,
      "duration": 3.96
    },
    {
      "lang": "en",
      "text": "identical and should look very very",
      "offset": 344.94,
      "duration": 2.819
    },
    {
      "lang": "en",
      "text": "familiar",
      "offset": 346.62,
      "duration": 2.82
    },
    {
      "lang": "en",
      "text": "nothing changed here",
      "offset": 347.759,
      "duration": 5.16
    },
    {
      "lang": "en",
      "text": "uh loss function looks very crazy we",
      "offset": 349.44,
      "duration": 4.979
    },
    {
      "lang": "en",
      "text": "should probably fix this and that's",
      "offset": 352.919,
      "duration": 3.901
    },
    {
      "lang": "en",
      "text": "because 32 batch elements are too few",
      "offset": 354.419,
      "duration": 4.921
    },
    {
      "lang": "en",
      "text": "and so you can get very lucky lucky or",
      "offset": 356.82,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "unlucky in any one of these batches and",
      "offset": 359.34,
      "duration": 4.919
    },
    {
      "lang": "en",
      "text": "it creates a very thick loss function",
      "offset": 361.38,
      "duration": 4.92
    },
    {
      "lang": "en",
      "text": "um so we're going to fix that soon",
      "offset": 364.259,
      "duration": 4.261
    },
    {
      "lang": "en",
      "text": "now once we want to evaluate the trained",
      "offset": 366.3,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "neural network we need to remember",
      "offset": 368.52,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "because of the bathroom layers to set",
      "offset": 369.9,
      "duration": 3.78
    },
    {
      "lang": "en",
      "text": "all the layers to be training equals",
      "offset": 371.88,
      "duration": 3.48
    },
    {
      "lang": "en",
      "text": "false so this only matters for the",
      "offset": 373.68,
      "duration": 3.42
    },
    {
      "lang": "en",
      "text": "bathroom layer so far",
      "offset": 375.36,
      "duration": 4.339
    },
    {
      "lang": "en",
      "text": "and then we evaluate",
      "offset": 377.1,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "we see that currently we have validation",
      "offset": 379.699,
      "duration": 5.681
    },
    {
      "lang": "en",
      "text": "loss of 2.10 which is fairly good but",
      "offset": 382.139,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "there's still ways to go but even at",
      "offset": 385.38,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "2.10 we see that when we sample from the",
      "offset": 388.139,
      "duration": 3.541
    },
    {
      "lang": "en",
      "text": "model we actually get relatively",
      "offset": 390.419,
      "duration": 4.021
    },
    {
      "lang": "en",
      "text": "name-like results that do not exist in a",
      "offset": 391.68,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "training set so for example Yvonne kilo",
      "offset": 394.44,
      "duration": 5.58
    },
    {
      "lang": "en",
      "text": "Pros",
      "offset": 397.68,
      "duration": 5.78
    },
    {
      "lang": "en",
      "text": "Alaia Etc so certainly not",
      "offset": 400.02,
      "duration": 6.299
    },
    {
      "lang": "en",
      "text": "reasonable not unreasonable I would say",
      "offset": 403.46,
      "duration": 5.079
    },
    {
      "lang": "en",
      "text": "but not amazing and we can still push",
      "offset": 406.319,
      "duration": 3.781
    },
    {
      "lang": "en",
      "text": "this validation loss even lower and get",
      "offset": 408.539,
      "duration": 3.66
    },
    {
      "lang": "en",
      "text": "much better samples that are even more",
      "offset": 410.1,
      "duration": 3.24
    },
    {
      "lang": "en",
      "text": "name-like",
      "offset": 412.199,
      "duration": 4.261
    },
    {
      "lang": "en",
      "text": "so let's improve this model",
      "offset": 413.34,
      "duration": 5.16
    },
    {
      "lang": "en",
      "text": "okay first let's fix this graph because",
      "offset": 416.46,
      "duration": 3.72
    },
    {
      "lang": "en",
      "text": "it is daggers in my eyes and I just",
      "offset": 418.5,
      "duration": 3.479
    },
    {
      "lang": "en",
      "text": "can't take it anymore",
      "offset": 420.18,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "um so last I if you recall is a python",
      "offset": 421.979,
      "duration": 5.94
    },
    {
      "lang": "en",
      "text": "list of floats so for example the first",
      "offset": 425.46,
      "duration": 5.239
    },
    {
      "lang": "en",
      "text": "10 elements",
      "offset": 427.919,
      "duration": 2.78
    },
    {
      "lang": "en",
      "text": "now what we'd like to do basically is we",
      "offset": 430.86,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "need to average up",
      "offset": 432.6,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "um some of these values to get a more",
      "offset": 434.22,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "sort of Representative uh value along",
      "offset": 436.199,
      "duration": 4.62
    },
    {
      "lang": "en",
      "text": "the way so one way to do this is the",
      "offset": 439.02,
      "duration": 2.82
    },
    {
      "lang": "en",
      "text": "following",
      "offset": 440.819,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "in part torch if I create for example",
      "offset": 441.84,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "a tensor of the first 10 numbers",
      "offset": 444.9,
      "duration": 4.139
    },
    {
      "lang": "en",
      "text": "then this is currently a one-dimensional",
      "offset": 447.36,
      "duration": 4.02
    },
    {
      "lang": "en",
      "text": "array but recall that I can view this",
      "offset": 449.039,
      "duration": 4.44
    },
    {
      "lang": "en",
      "text": "array as two-dimensional so for example",
      "offset": 451.38,
      "duration": 4.68
    },
    {
      "lang": "en",
      "text": "I can use it as a two by five array and",
      "offset": 453.479,
      "duration": 5.761
    },
    {
      "lang": "en",
      "text": "this is a 2d tensor now two by five and",
      "offset": 456.06,
      "duration": 4.68
    },
    {
      "lang": "en",
      "text": "you see what petroch has done is that",
      "offset": 459.24,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "the first row of this tensor is the",
      "offset": 460.74,
      "duration": 3.959
    },
    {
      "lang": "en",
      "text": "first five elements and the second row",
      "offset": 462.84,
      "duration": 3.9
    },
    {
      "lang": "en",
      "text": "is the second five elements",
      "offset": 464.699,
      "duration": 4.261
    },
    {
      "lang": "en",
      "text": "I can also view it as a five by two as",
      "offset": 466.74,
      "duration": 3.299
    },
    {
      "lang": "en",
      "text": "an example",
      "offset": 468.96,
      "duration": 3.26
    },
    {
      "lang": "en",
      "text": "and then recall that I can also",
      "offset": 470.039,
      "duration": 5.041
    },
    {
      "lang": "en",
      "text": "use negative one in place of one of",
      "offset": 472.22,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "these numbers",
      "offset": 475.08,
      "duration": 3.059
    },
    {
      "lang": "en",
      "text": "and pytorch will calculate what that",
      "offset": 475.979,
      "duration": 3.66
    },
    {
      "lang": "en",
      "text": "number must be in order to make the",
      "offset": 478.139,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "number of elements work out so this can",
      "offset": 479.639,
      "duration": 3.481
    },
    {
      "lang": "en",
      "text": "be",
      "offset": 481.979,
      "duration": 4.021
    },
    {
      "lang": "en",
      "text": "this or like that but it will work of",
      "offset": 483.12,
      "duration": 5.9
    },
    {
      "lang": "en",
      "text": "course this would not work",
      "offset": 486,
      "duration": 3.02
    },
    {
      "lang": "en",
      "text": "okay so this allows it to spread out",
      "offset": 489.24,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "some of the consecutive values into rows",
      "offset": 491.52,
      "duration": 3.959
    },
    {
      "lang": "en",
      "text": "so that's very helpful because what we",
      "offset": 493.56,
      "duration": 4.02
    },
    {
      "lang": "en",
      "text": "can do now is first of all we're going",
      "offset": 495.479,
      "duration": 5.701
    },
    {
      "lang": "en",
      "text": "to create a torshot tensor out of the a",
      "offset": 497.58,
      "duration": 5.1
    },
    {
      "lang": "en",
      "text": "list of floats",
      "offset": 501.18,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "and then we're going to view it as",
      "offset": 502.68,
      "duration": 4.199
    },
    {
      "lang": "en",
      "text": "whatever it is but we're going to",
      "offset": 504.3,
      "duration": 4.86
    },
    {
      "lang": "en",
      "text": "stretch it out into rows of 1000",
      "offset": 506.879,
      "duration": 4.5
    },
    {
      "lang": "en",
      "text": "consecutive elements so the shape of",
      "offset": 509.16,
      "duration": 6.059
    },
    {
      "lang": "en",
      "text": "this now becomes 200 by 1000. and each",
      "offset": 511.379,
      "duration": 6.6
    },
    {
      "lang": "en",
      "text": "row is one thousand um consecutive",
      "offset": 515.219,
      "duration": 4.44
    },
    {
      "lang": "en",
      "text": "elements in this list",
      "offset": 517.979,
      "duration": 3.06
    },
    {
      "lang": "en",
      "text": "so that's very helpful because now we",
      "offset": 519.659,
      "duration": 4.201
    },
    {
      "lang": "en",
      "text": "can do a mean along the rows",
      "offset": 521.039,
      "duration": 6.061
    },
    {
      "lang": "en",
      "text": "and the shape of this will just be 200.",
      "offset": 523.86,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "and so we've taken basically the mean on",
      "offset": 527.1,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "every row so plt.plot of that should be",
      "offset": 528.899,
      "duration": 4.681
    },
    {
      "lang": "en",
      "text": "something nicer",
      "offset": 531.899,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "much better",
      "offset": 533.58,
      "duration": 3.24
    },
    {
      "lang": "en",
      "text": "so we see that we basically made a lot",
      "offset": 535.019,
      "duration": 4.021
    },
    {
      "lang": "en",
      "text": "of progress and then here this is the",
      "offset": 536.82,
      "duration": 4.92
    },
    {
      "lang": "en",
      "text": "learning rate Decay so here we see that",
      "offset": 539.04,
      "duration": 4.5
    },
    {
      "lang": "en",
      "text": "the learning rate Decay subtracted a ton",
      "offset": 541.74,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "of energy out of the system and allowed",
      "offset": 543.54,
      "duration": 3.72
    },
    {
      "lang": "en",
      "text": "us to settle into sort of the local",
      "offset": 545.58,
      "duration": 3.78
    },
    {
      "lang": "en",
      "text": "minimum in this optimization",
      "offset": 547.26,
      "duration": 4.98
    },
    {
      "lang": "en",
      "text": "so this is a much nicer plot let me come",
      "offset": 549.36,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "up and delete the monster and we're",
      "offset": 552.24,
      "duration": 4.74
    },
    {
      "lang": "en",
      "text": "going to be using this going forward now",
      "offset": 555.12,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "next up what I'm bothered by is that you",
      "offset": 556.98,
      "duration": 5.22
    },
    {
      "lang": "en",
      "text": "see our forward pass is a little bit",
      "offset": 559.68,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "gnarly and takes way too many lines of",
      "offset": 562.2,
      "duration": 2.52
    },
    {
      "lang": "en",
      "text": "code",
      "offset": 564,
      "duration": 2.1
    },
    {
      "lang": "en",
      "text": "so in particular we see that we've",
      "offset": 564.72,
      "duration": 3.54
    },
    {
      "lang": "en",
      "text": "organized some of the layers inside the",
      "offset": 566.1,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "layers list but not all of them uh for",
      "offset": 568.26,
      "duration": 4.68
    },
    {
      "lang": "en",
      "text": "no reason so in particular we see that",
      "offset": 570.899,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "we still have the embedding table a",
      "offset": 572.94,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "special case outside of the layers and",
      "offset": 574.5,
      "duration": 4.68
    },
    {
      "lang": "en",
      "text": "in addition to that the viewing",
      "offset": 577.26,
      "duration": 3.54
    },
    {
      "lang": "en",
      "text": "operation here is also outside of our",
      "offset": 579.18,
      "duration": 4.02
    },
    {
      "lang": "en",
      "text": "layers so let's create layers for these",
      "offset": 580.8,
      "duration": 4.26
    },
    {
      "lang": "en",
      "text": "and then we can add those layers to just",
      "offset": 583.2,
      "duration": 3.06
    },
    {
      "lang": "en",
      "text": "our list",
      "offset": 585.06,
      "duration": 3.3
    },
    {
      "lang": "en",
      "text": "so in particular the two things that we",
      "offset": 586.26,
      "duration": 4.44
    },
    {
      "lang": "en",
      "text": "need is here we have this embedding",
      "offset": 588.36,
      "duration": 4.68
    },
    {
      "lang": "en",
      "text": "table and we are indexing at the",
      "offset": 590.7,
      "duration": 6.06
    },
    {
      "lang": "en",
      "text": "integers inside uh the batch XB uh",
      "offset": 593.04,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "inside the tensor xB",
      "offset": 596.76,
      "duration": 4.139
    },
    {
      "lang": "en",
      "text": "so that's an embedding table lookup just",
      "offset": 598.32,
      "duration": 5.1
    },
    {
      "lang": "en",
      "text": "done with indexing and then here we see",
      "offset": 600.899,
      "duration": 4.021
    },
    {
      "lang": "en",
      "text": "that we have this view operation which",
      "offset": 603.42,
      "duration": 2.82
    },
    {
      "lang": "en",
      "text": "if you recall from the previous video",
      "offset": 604.92,
      "duration": 4.859
    },
    {
      "lang": "en",
      "text": "Simply rearranges the character",
      "offset": 606.24,
      "duration": 6.3
    },
    {
      "lang": "en",
      "text": "embeddings and stretches them out into a",
      "offset": 609.779,
      "duration": 5.041
    },
    {
      "lang": "en",
      "text": "row and effectively what print that does",
      "offset": 612.54,
      "duration": 4.44
    },
    {
      "lang": "en",
      "text": "is the concatenation operation basically",
      "offset": 614.82,
      "duration": 4.74
    },
    {
      "lang": "en",
      "text": "except it's free because viewing is very",
      "offset": 616.98,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "cheap in pytorch no no memory is being",
      "offset": 619.56,
      "duration": 5.219
    },
    {
      "lang": "en",
      "text": "copied we're just re-representing how we",
      "offset": 622.74,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "view that tensor so let's create",
      "offset": 624.779,
      "duration": 3.541
    },
    {
      "lang": "en",
      "text": "um",
      "offset": 627.54,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "modules for both of these operations the",
      "offset": 628.32,
      "duration": 4.26
    },
    {
      "lang": "en",
      "text": "embedding operation and flattening",
      "offset": 631.14,
      "duration": 2.759
    },
    {
      "lang": "en",
      "text": "operation",
      "offset": 632.58,
      "duration": 4.5
    },
    {
      "lang": "en",
      "text": "so I actually wrote the code in just to",
      "offset": 633.899,
      "duration": 4.741
    },
    {
      "lang": "en",
      "text": "save some time",
      "offset": 637.08,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "so we have a module embedding and a",
      "offset": 638.64,
      "duration": 4.86
    },
    {
      "lang": "en",
      "text": "module pattern and both of them simply",
      "offset": 640.68,
      "duration": 4.62
    },
    {
      "lang": "en",
      "text": "do the indexing operation in the forward",
      "offset": 643.5,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "pass and the flattening operation here",
      "offset": 645.3,
      "duration": 8.58
    },
    {
      "lang": "en",
      "text": "and this C now will just become a salt",
      "offset": 649.5,
      "duration": 6.959
    },
    {
      "lang": "en",
      "text": "dot weight inside an embedding module",
      "offset": 653.88,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "and I'm calling these layers",
      "offset": 656.459,
      "duration": 3.421
    },
    {
      "lang": "en",
      "text": "specifically embedding a platinum",
      "offset": 658.44,
      "duration": 2.7
    },
    {
      "lang": "en",
      "text": "because it turns out that both of them",
      "offset": 659.88,
      "duration": 3.72
    },
    {
      "lang": "en",
      "text": "actually exist in pi torch so in",
      "offset": 661.14,
      "duration": 4.86
    },
    {
      "lang": "en",
      "text": "phytorch we have n and Dot embedding and",
      "offset": 663.6,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "it also takes the number of embeddings",
      "offset": 666,
      "duration": 3.24
    },
    {
      "lang": "en",
      "text": "and the dimensionality of the bedding",
      "offset": 667.44,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "just like we have here but in addition",
      "offset": 669.24,
      "duration": 3.779
    },
    {
      "lang": "en",
      "text": "python takes in a lot of other keyword",
      "offset": 671.279,
      "duration": 4.441
    },
    {
      "lang": "en",
      "text": "arguments that we are not using for our",
      "offset": 673.019,
      "duration": 4.741
    },
    {
      "lang": "en",
      "text": "purposes yet",
      "offset": 675.72,
      "duration": 3.96
    },
    {
      "lang": "en",
      "text": "and for flatten that also exists in",
      "offset": 677.76,
      "duration": 4.199
    },
    {
      "lang": "en",
      "text": "pytorch and it also takes additional",
      "offset": 679.68,
      "duration": 4.02
    },
    {
      "lang": "en",
      "text": "keyword arguments that we are not using",
      "offset": 681.959,
      "duration": 4.38
    },
    {
      "lang": "en",
      "text": "so we have a very simple platform",
      "offset": 683.7,
      "duration": 4.5
    },
    {
      "lang": "en",
      "text": "but both of them exist in pytorch",
      "offset": 686.339,
      "duration": 4.381
    },
    {
      "lang": "en",
      "text": "they're just a bit more simpler and now",
      "offset": 688.2,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "that we have these we can simply take",
      "offset": 690.72,
      "duration": 6.119
    },
    {
      "lang": "en",
      "text": "out some of these special cased",
      "offset": 693.72,
      "duration": 6.299
    },
    {
      "lang": "en",
      "text": "um things so instead of C we're just",
      "offset": 696.839,
      "duration": 4.74
    },
    {
      "lang": "en",
      "text": "going to have an embedding",
      "offset": 700.019,
      "duration": 5.581
    },
    {
      "lang": "en",
      "text": "and of a cup size and N embed",
      "offset": 701.579,
      "duration": 5.581
    },
    {
      "lang": "en",
      "text": "and then after the embedding we are",
      "offset": 705.6,
      "duration": 3.179
    },
    {
      "lang": "en",
      "text": "going to flatten",
      "offset": 707.16,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "so let's construct those modules and now",
      "offset": 708.779,
      "duration": 4.381
    },
    {
      "lang": "en",
      "text": "I can take out this the",
      "offset": 711,
      "duration": 3.779
    },
    {
      "lang": "en",
      "text": "and here I don't have to special case",
      "offset": 713.16,
      "duration": 4.679
    },
    {
      "lang": "en",
      "text": "anymore because now C is the embeddings",
      "offset": 714.779,
      "duration": 7.141
    },
    {
      "lang": "en",
      "text": "weight and it's inside layers",
      "offset": 717.839,
      "duration": 6.12
    },
    {
      "lang": "en",
      "text": "so this should just work",
      "offset": 721.92,
      "duration": 4.38
    },
    {
      "lang": "en",
      "text": "and then here our forward pass",
      "offset": 723.959,
      "duration": 4.261
    },
    {
      "lang": "en",
      "text": "simplifies substantially because we",
      "offset": 726.3,
      "duration": 3.719
    },
    {
      "lang": "en",
      "text": "don't need to do these now outside of",
      "offset": 728.22,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "these layer outside and explicitly",
      "offset": 730.019,
      "duration": 5.101
    },
    {
      "lang": "en",
      "text": "they're now inside layers",
      "offset": 733.019,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "so we can delete those",
      "offset": 735.12,
      "duration": 4.68
    },
    {
      "lang": "en",
      "text": "but now to to kick things off we want",
      "offset": 737.1,
      "duration": 4.679
    },
    {
      "lang": "en",
      "text": "this little X which in the beginning is",
      "offset": 739.8,
      "duration": 4.38
    },
    {
      "lang": "en",
      "text": "just XB uh the tensor of integers",
      "offset": 741.779,
      "duration": 4.261
    },
    {
      "lang": "en",
      "text": "specifying the identities of these",
      "offset": 744.18,
      "duration": 3.42
    },
    {
      "lang": "en",
      "text": "characters at the input",
      "offset": 746.04,
      "duration": 3.539
    },
    {
      "lang": "en",
      "text": "and so these characters can now directly",
      "offset": 747.6,
      "duration": 3.72
    },
    {
      "lang": "en",
      "text": "feed into the first layer and this",
      "offset": 749.579,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "should just work",
      "offset": 751.32,
      "duration": 3.72
    },
    {
      "lang": "en",
      "text": "so let me come here and insert a break",
      "offset": 752.579,
      "duration": 4.141
    },
    {
      "lang": "en",
      "text": "because I just want to make sure that",
      "offset": 755.04,
      "duration": 3.06
    },
    {
      "lang": "en",
      "text": "the first iteration of this runs and",
      "offset": 756.72,
      "duration": 3.72
    },
    {
      "lang": "en",
      "text": "then there's no mistake so that ran",
      "offset": 758.1,
      "duration": 4.739
    },
    {
      "lang": "en",
      "text": "properly and basically we substantially",
      "offset": 760.44,
      "duration": 4.98
    },
    {
      "lang": "en",
      "text": "simplified the forward pass here okay",
      "offset": 762.839,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "I'm sorry I changed my microphone so",
      "offset": 765.42,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "hopefully the audio is a little bit",
      "offset": 766.92,
      "duration": 2.58
    },
    {
      "lang": "en",
      "text": "better",
      "offset": 768.42,
      "duration": 3.24
    },
    {
      "lang": "en",
      "text": "now one more thing that I would like to",
      "offset": 769.5,
      "duration": 3.959
    },
    {
      "lang": "en",
      "text": "do in order to pytortify our code even",
      "offset": 771.66,
      "duration": 3.299
    },
    {
      "lang": "en",
      "text": "further is that right now we are",
      "offset": 773.459,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "maintaining all of our modules in a",
      "offset": 774.959,
      "duration": 4.201
    },
    {
      "lang": "en",
      "text": "naked list of layers and we can also",
      "offset": 776.82,
      "duration": 4.199
    },
    {
      "lang": "en",
      "text": "simplify this uh because we can",
      "offset": 779.16,
      "duration": 3.9
    },
    {
      "lang": "en",
      "text": "introduce the concept of Pi torch",
      "offset": 781.019,
      "duration": 4.741
    },
    {
      "lang": "en",
      "text": "containers so in tors.nn which we are",
      "offset": 783.06,
      "duration": 3.959
    },
    {
      "lang": "en",
      "text": "basically rebuilding from scratch here",
      "offset": 785.76,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "there's a concept of containers",
      "offset": 787.019,
      "duration": 3.721
    },
    {
      "lang": "en",
      "text": "and these containers are basically a way",
      "offset": 789.12,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "of organizing layers into",
      "offset": 790.74,
      "duration": 5.339
    },
    {
      "lang": "en",
      "text": "lists or dicts and so on so in",
      "offset": 793.2,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "particular there's a sequential which",
      "offset": 796.079,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "maintains a list of layers and is a",
      "offset": 798.24,
      "duration": 5.46
    },
    {
      "lang": "en",
      "text": "module class in pytorch and it basically",
      "offset": 800.639,
      "duration": 4.741
    },
    {
      "lang": "en",
      "text": "just passes a given input through all",
      "offset": 803.7,
      "duration": 3.66
    },
    {
      "lang": "en",
      "text": "the layers sequentially exactly as we",
      "offset": 805.38,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "are doing here",
      "offset": 807.36,
      "duration": 3.719
    },
    {
      "lang": "en",
      "text": "so let's write our own sequential",
      "offset": 808.74,
      "duration": 5.219
    },
    {
      "lang": "en",
      "text": "I've written a code here and basically",
      "offset": 811.079,
      "duration": 4.021
    },
    {
      "lang": "en",
      "text": "the code for sequential is quite",
      "offset": 813.959,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "straightforward we pass in a list of",
      "offset": 815.1,
      "duration": 4.44
    },
    {
      "lang": "en",
      "text": "layers which we keep here and then given",
      "offset": 817.079,
      "duration": 4.621
    },
    {
      "lang": "en",
      "text": "any input in a forward pass we just call",
      "offset": 819.54,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "all the layers sequentially and return",
      "offset": 821.7,
      "duration": 3.72
    },
    {
      "lang": "en",
      "text": "the result in terms of the parameters",
      "offset": 823.38,
      "duration": 3.3
    },
    {
      "lang": "en",
      "text": "it's just all the parameters of the",
      "offset": 825.42,
      "duration": 2.7
    },
    {
      "lang": "en",
      "text": "child modules",
      "offset": 826.68,
      "duration": 4.02
    },
    {
      "lang": "en",
      "text": "so we can run this and we can again",
      "offset": 828.12,
      "duration": 4.74
    },
    {
      "lang": "en",
      "text": "simplify this substantially because we",
      "offset": 830.7,
      "duration": 3.66
    },
    {
      "lang": "en",
      "text": "don't maintain this naked list of layers",
      "offset": 832.86,
      "duration": 4.14
    },
    {
      "lang": "en",
      "text": "we now have a notion of a model which is",
      "offset": 834.36,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "a module and in particular is a",
      "offset": 837,
      "duration": 7.639
    },
    {
      "lang": "en",
      "text": "sequential of all these layers",
      "offset": 840.36,
      "duration": 4.279
    },
    {
      "lang": "en",
      "text": "and now parameters are simply just a",
      "offset": 844.74,
      "duration": 4.74
    },
    {
      "lang": "en",
      "text": "model about parameters",
      "offset": 847.74,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "and so that list comprehension now lives",
      "offset": 849.48,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "here",
      "offset": 851.82,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "and then here we are press here we are",
      "offset": 853.8,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "doing all the things we used to do",
      "offset": 855.899,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "now here the code again simplifies",
      "offset": 857.88,
      "duration": 4.139
    },
    {
      "lang": "en",
      "text": "substantially because we don't have to",
      "offset": 859.98,
      "duration": 4.44
    },
    {
      "lang": "en",
      "text": "do this forwarding here instead of just",
      "offset": 862.019,
      "duration": 4.38
    },
    {
      "lang": "en",
      "text": "call the model on the input data and the",
      "offset": 864.42,
      "duration": 3.659
    },
    {
      "lang": "en",
      "text": "input data here are the integers inside",
      "offset": 866.399,
      "duration": 5.221
    },
    {
      "lang": "en",
      "text": "xB so we can simply do logits which are",
      "offset": 868.079,
      "duration": 5.7
    },
    {
      "lang": "en",
      "text": "the outputs of our model are simply the",
      "offset": 871.62,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "model called on xB",
      "offset": 873.779,
      "duration": 5.101
    },
    {
      "lang": "en",
      "text": "and then the cross entropy here takes",
      "offset": 876.66,
      "duration": 4.619
    },
    {
      "lang": "en",
      "text": "the logits and the targets",
      "offset": 878.88,
      "duration": 4.86
    },
    {
      "lang": "en",
      "text": "so this simplifies substantially",
      "offset": 881.279,
      "duration": 4.981
    },
    {
      "lang": "en",
      "text": "and then this looks good so let's just",
      "offset": 883.74,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "make sure this runs that looks good",
      "offset": 886.26,
      "duration": 5.16
    },
    {
      "lang": "en",
      "text": "now here we actually have some work to",
      "offset": 889.26,
      "duration": 3.54
    },
    {
      "lang": "en",
      "text": "do still here but I'm going to come back",
      "offset": 891.42,
      "duration": 3.539
    },
    {
      "lang": "en",
      "text": "later for now there's no more layers",
      "offset": 892.8,
      "duration": 4.74
    },
    {
      "lang": "en",
      "text": "there's a model that layers but it's not",
      "offset": 894.959,
      "duration": 5.281
    },
    {
      "lang": "en",
      "text": "a to access attributes of these classes",
      "offset": 897.54,
      "duration": 4.38
    },
    {
      "lang": "en",
      "text": "directly so we'll come back and fix this",
      "offset": 900.24,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "later",
      "offset": 901.92,
      "duration": 3.18
    },
    {
      "lang": "en",
      "text": "and then here of course this simplifies",
      "offset": 903.12,
      "duration": 4.5
    },
    {
      "lang": "en",
      "text": "substantially as well because logits are",
      "offset": 905.1,
      "duration": 5.4
    },
    {
      "lang": "en",
      "text": "the model called on x",
      "offset": 907.62,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "and then these low Jets come here",
      "offset": 910.5,
      "duration": 5.399
    },
    {
      "lang": "en",
      "text": "so we can evaluate the train and",
      "offset": 914.1,
      "duration": 3.299
    },
    {
      "lang": "en",
      "text": "validation loss which currently is",
      "offset": 915.899,
      "duration": 3.3
    },
    {
      "lang": "en",
      "text": "terrible because we just initialized the",
      "offset": 917.399,
      "duration": 3.781
    },
    {
      "lang": "en",
      "text": "neural net and then we can also sample",
      "offset": 919.199,
      "duration": 3.481
    },
    {
      "lang": "en",
      "text": "from the model and this simplifies",
      "offset": 921.18,
      "duration": 2.94
    },
    {
      "lang": "en",
      "text": "dramatically as well",
      "offset": 922.68,
      "duration": 2.94
    },
    {
      "lang": "en",
      "text": "because we just want to call the model",
      "offset": 924.12,
      "duration": 6.12
    },
    {
      "lang": "en",
      "text": "onto the context and outcome logits",
      "offset": 925.62,
      "duration": 7.32
    },
    {
      "lang": "en",
      "text": "and these logits go into softmax and get",
      "offset": 930.24,
      "duration": 4.98
    },
    {
      "lang": "en",
      "text": "the probabilities Etc so we can sample",
      "offset": 932.94,
      "duration": 4.68
    },
    {
      "lang": "en",
      "text": "from this model",
      "offset": 935.22,
      "duration": 5.9
    },
    {
      "lang": "en",
      "text": "what did I screw up",
      "offset": 937.62,
      "duration": 3.5
    },
    {
      "lang": "en",
      "text": "okay so I fixed the issue and we now get",
      "offset": 942.3,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "the result that we expect which is",
      "offset": 944.22,
      "duration": 3.9
    },
    {
      "lang": "en",
      "text": "gibberish because the model is not",
      "offset": 946.139,
      "duration": 3.721
    },
    {
      "lang": "en",
      "text": "trained because we re-initialize it from",
      "offset": 948.12,
      "duration": 2.519
    },
    {
      "lang": "en",
      "text": "scratch",
      "offset": 949.86,
      "duration": 2.82
    },
    {
      "lang": "en",
      "text": "the problem was that when I fixed this",
      "offset": 950.639,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "cell to be modeled out layers instead of",
      "offset": 952.68,
      "duration": 3.959
    },
    {
      "lang": "en",
      "text": "just layers I did not actually run the",
      "offset": 954.48,
      "duration": 4.44
    },
    {
      "lang": "en",
      "text": "cell and so our neural net was in a",
      "offset": 956.639,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "training mode and what caused the issue",
      "offset": 958.92,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "here is the bathroom layer as bathroom",
      "offset": 961.44,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "layer of the likes to do because",
      "offset": 963,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "Bachelor was in a training mode and here",
      "offset": 965.279,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "we are passing in an input which is a",
      "offset": 967.56,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "batch of just a single example made up",
      "offset": 969.839,
      "duration": 3.06
    },
    {
      "lang": "en",
      "text": "of the context",
      "offset": 971.639,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "and so if you are trying to pass in a",
      "offset": 972.899,
      "duration": 3.721
    },
    {
      "lang": "en",
      "text": "single example into a bash Norm that is",
      "offset": 975,
      "duration": 3.06
    },
    {
      "lang": "en",
      "text": "in the training mode you're going to end",
      "offset": 976.62,
      "duration": 3.54
    },
    {
      "lang": "en",
      "text": "up estimating the variance using the",
      "offset": 978.06,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "input and the variance of a single",
      "offset": 980.16,
      "duration": 4.38
    },
    {
      "lang": "en",
      "text": "number is is not a number because it is",
      "offset": 981.899,
      "duration": 4.981
    },
    {
      "lang": "en",
      "text": "a measure of a spread so for example the",
      "offset": 984.54,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "variance of just the single number five",
      "offset": 986.88,
      "duration": 4.259
    },
    {
      "lang": "en",
      "text": "you can see is not a number and so",
      "offset": 988.62,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "that's what happened in the master",
      "offset": 991.139,
      "duration": 4.38
    },
    {
      "lang": "en",
      "text": "basically caused an issue and then that",
      "offset": 993.42,
      "duration": 4.38
    },
    {
      "lang": "en",
      "text": "polluted all of the further processing",
      "offset": 995.519,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "so all that we have to do was make sure",
      "offset": 997.8,
      "duration": 5.459
    },
    {
      "lang": "en",
      "text": "that this runs and we basically made the",
      "offset": 999.839,
      "duration": 5.281
    },
    {
      "lang": "en",
      "text": "issue of",
      "offset": 1003.259,
      "duration": 3.421
    },
    {
      "lang": "en",
      "text": "again we didn't actually see the issue",
      "offset": 1005.12,
      "duration": 3.3
    },
    {
      "lang": "en",
      "text": "with the loss we could have evaluated",
      "offset": 1006.68,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "the loss but we got the wrong result",
      "offset": 1008.42,
      "duration": 3.659
    },
    {
      "lang": "en",
      "text": "because basharm was in the training mode",
      "offset": 1009.8,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "and uh and so we still get a result it's",
      "offset": 1012.079,
      "duration": 4.141
    },
    {
      "lang": "en",
      "text": "just the wrong result because it's using",
      "offset": 1014.6,
      "duration": 4.62
    },
    {
      "lang": "en",
      "text": "the uh sample statistics of the batch",
      "offset": 1016.22,
      "duration": 4.44
    },
    {
      "lang": "en",
      "text": "whereas we want to use the running mean",
      "offset": 1019.22,
      "duration": 3.72
    },
    {
      "lang": "en",
      "text": "and running variants inside the bachelor",
      "offset": 1020.66,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "and so",
      "offset": 1022.94,
      "duration": 3.259
    },
    {
      "lang": "en",
      "text": "again an example of introducing a bug",
      "offset": 1024.5,
      "duration": 4.86
    },
    {
      "lang": "en",
      "text": "inline because we did not properly",
      "offset": 1026.199,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "maintain the state of what is training",
      "offset": 1029.36,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "or not okay so I Rewritten everything",
      "offset": 1030.919,
      "duration": 4.5
    },
    {
      "lang": "en",
      "text": "and here's where we are as a reminder we",
      "offset": 1032.959,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "have the training loss of 2.05 and",
      "offset": 1035.419,
      "duration": 3.301
    },
    {
      "lang": "en",
      "text": "validation 2.10",
      "offset": 1037.04,
      "duration": 4.019
    },
    {
      "lang": "en",
      "text": "now because these losses are very",
      "offset": 1038.72,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "similar to each other we have a sense",
      "offset": 1041.059,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "that we are not overfitting too much on",
      "offset": 1042.559,
      "duration": 3.481
    },
    {
      "lang": "en",
      "text": "this task and we can make additional",
      "offset": 1044.419,
      "duration": 3.961
    },
    {
      "lang": "en",
      "text": "progress in our performance by scaling",
      "offset": 1046.04,
      "duration": 3.72
    },
    {
      "lang": "en",
      "text": "up the size of the neural network and",
      "offset": 1048.38,
      "duration": 3.96
    },
    {
      "lang": "en",
      "text": "making everything bigger and deeper",
      "offset": 1049.76,
      "duration": 4.2
    },
    {
      "lang": "en",
      "text": "now currently we are using this",
      "offset": 1052.34,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "architecture here where we are taking in",
      "offset": 1053.96,
      "duration": 3.66
    },
    {
      "lang": "en",
      "text": "some number of characters going into a",
      "offset": 1055.94,
      "duration": 3.42
    },
    {
      "lang": "en",
      "text": "single hidden layer and then going to",
      "offset": 1057.62,
      "duration": 3.66
    },
    {
      "lang": "en",
      "text": "the prediction of the next character",
      "offset": 1059.36,
      "duration": 3.78
    },
    {
      "lang": "en",
      "text": "the problem here is we don't have a",
      "offset": 1061.28,
      "duration": 4.92
    },
    {
      "lang": "en",
      "text": "naive way of making this bigger in a",
      "offset": 1063.14,
      "duration": 5.64
    },
    {
      "lang": "en",
      "text": "productive way we could of course use",
      "offset": 1066.2,
      "duration": 4.859
    },
    {
      "lang": "en",
      "text": "our layers sort of building blocks and",
      "offset": 1068.78,
      "duration": 4.5
    },
    {
      "lang": "en",
      "text": "materials to introduce additional layers",
      "offset": 1071.059,
      "duration": 4.441
    },
    {
      "lang": "en",
      "text": "here and make the network deeper but it",
      "offset": 1073.28,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "is still the case that we are crushing",
      "offset": 1075.5,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "all of the characters into a single",
      "offset": 1076.88,
      "duration": 4.02
    },
    {
      "lang": "en",
      "text": "layer all the way at the beginning",
      "offset": 1078.5,
      "duration": 4.38
    },
    {
      "lang": "en",
      "text": "and even if we make this a bigger layer",
      "offset": 1080.9,
      "duration": 3.96
    },
    {
      "lang": "en",
      "text": "and add neurons it's still kind of like",
      "offset": 1082.88,
      "duration": 4.5
    },
    {
      "lang": "en",
      "text": "silly to squash all that information so",
      "offset": 1084.86,
      "duration": 4.98
    },
    {
      "lang": "en",
      "text": "fast in a single step",
      "offset": 1087.38,
      "duration": 4.26
    },
    {
      "lang": "en",
      "text": "so we'd like to do instead is we'd like",
      "offset": 1089.84,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "our Network to look a lot more like this",
      "offset": 1091.64,
      "duration": 4.02
    },
    {
      "lang": "en",
      "text": "in the wavenet case so you see in the",
      "offset": 1093.44,
      "duration": 3.9
    },
    {
      "lang": "en",
      "text": "wavenet when we are trying to make the",
      "offset": 1095.66,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "prediction for the next character in the",
      "offset": 1097.34,
      "duration": 3.48
    },
    {
      "lang": "en",
      "text": "sequence it is a function of the",
      "offset": 1098.66,
      "duration": 3.66
    },
    {
      "lang": "en",
      "text": "previous characters that are feeding",
      "offset": 1100.82,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "that feed in but not all of these",
      "offset": 1102.32,
      "duration": 4.14
    },
    {
      "lang": "en",
      "text": "different characters are not just",
      "offset": 1105.14,
      "duration": 3.18
    },
    {
      "lang": "en",
      "text": "crushed to a single layer and then you",
      "offset": 1106.46,
      "duration": 5.18
    },
    {
      "lang": "en",
      "text": "have a sandwich they are crushed slowly",
      "offset": 1108.32,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "so in particular we take two characters",
      "offset": 1111.64,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "and we fuse them into sort of like a",
      "offset": 1114.08,
      "duration": 4.26
    },
    {
      "lang": "en",
      "text": "diagram representation and we do that",
      "offset": 1116.12,
      "duration": 3.96
    },
    {
      "lang": "en",
      "text": "for all these characters consecutively",
      "offset": 1118.34,
      "duration": 4.5
    },
    {
      "lang": "en",
      "text": "and then we take the bigrams and we fuse",
      "offset": 1120.08,
      "duration": 6.599
    },
    {
      "lang": "en",
      "text": "those into four character level chunks",
      "offset": 1122.84,
      "duration": 6.3
    },
    {
      "lang": "en",
      "text": "and then we fuse that again and so we do",
      "offset": 1126.679,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "that in this like tree-like hierarchical",
      "offset": 1129.14,
      "duration": 4.38
    },
    {
      "lang": "en",
      "text": "manner so we fuse the information from",
      "offset": 1131,
      "duration": 5.4
    },
    {
      "lang": "en",
      "text": "the previous context slowly into the",
      "offset": 1133.52,
      "duration": 5.22
    },
    {
      "lang": "en",
      "text": "network as it gets deeper and so this is",
      "offset": 1136.4,
      "duration": 3.54
    },
    {
      "lang": "en",
      "text": "the kind of architecture that we want to",
      "offset": 1138.74,
      "duration": 2.04
    },
    {
      "lang": "en",
      "text": "implement",
      "offset": 1139.94,
      "duration": 2.94
    },
    {
      "lang": "en",
      "text": "now in the wave Nets case this is a",
      "offset": 1140.78,
      "duration": 3.66
    },
    {
      "lang": "en",
      "text": "visualization of a stack of dilated",
      "offset": 1142.88,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "causal convolution layers and this makes",
      "offset": 1144.44,
      "duration": 4.38
    },
    {
      "lang": "en",
      "text": "it sound very scary but actually the",
      "offset": 1147.2,
      "duration": 3.719
    },
    {
      "lang": "en",
      "text": "idea is very simple and the fact that",
      "offset": 1148.82,
      "duration": 3.96
    },
    {
      "lang": "en",
      "text": "it's a dilated causal convolution layer",
      "offset": 1150.919,
      "duration": 3.481
    },
    {
      "lang": "en",
      "text": "is really just an implementation detail",
      "offset": 1152.78,
      "duration": 3.24
    },
    {
      "lang": "en",
      "text": "to make everything fast we're going to",
      "offset": 1154.4,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "see that later but for now let's just",
      "offset": 1156.02,
      "duration": 4.38
    },
    {
      "lang": "en",
      "text": "keep the basic idea of it which is this",
      "offset": 1158.24,
      "duration": 4.14
    },
    {
      "lang": "en",
      "text": "Progressive Fusion so we want to make",
      "offset": 1160.4,
      "duration": 4.139
    },
    {
      "lang": "en",
      "text": "the network deeper and at each level we",
      "offset": 1162.38,
      "duration": 3.96
    },
    {
      "lang": "en",
      "text": "want to fuse only two consecutive",
      "offset": 1164.539,
      "duration": 4.681
    },
    {
      "lang": "en",
      "text": "elements two characters then two bigrams",
      "offset": 1166.34,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "then two four grams and so on so let's",
      "offset": 1169.22,
      "duration": 4.86
    },
    {
      "lang": "en",
      "text": "unplant this okay so first up let me",
      "offset": 1172.1,
      "duration": 3.42
    },
    {
      "lang": "en",
      "text": "scroll to where we built the data set",
      "offset": 1174.08,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "and let's change the block size from 3",
      "offset": 1175.52,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "to 8. so we're going to be taking eight",
      "offset": 1177.2,
      "duration": 4.92
    },
    {
      "lang": "en",
      "text": "characters of context to predict the",
      "offset": 1179.84,
      "duration": 4.26
    },
    {
      "lang": "en",
      "text": "ninth character so the data set now",
      "offset": 1182.12,
      "duration": 3.72
    },
    {
      "lang": "en",
      "text": "looks like this we have a lot more",
      "offset": 1184.1,
      "duration": 3.72
    },
    {
      "lang": "en",
      "text": "context feeding in to predict any next",
      "offset": 1185.84,
      "duration": 4.14
    },
    {
      "lang": "en",
      "text": "character in a sequence and these eight",
      "offset": 1187.82,
      "duration": 3.66
    },
    {
      "lang": "en",
      "text": "characters are going to be processed in",
      "offset": 1189.98,
      "duration": 3.48
    },
    {
      "lang": "en",
      "text": "this tree like structure",
      "offset": 1191.48,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "now if we scroll here everything here",
      "offset": 1193.46,
      "duration": 4.68
    },
    {
      "lang": "en",
      "text": "should just be able to work so we should",
      "offset": 1196.039,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "be able to redefine the network",
      "offset": 1198.14,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "you see the number of parameters has",
      "offset": 1199.64,
      "duration": 3.98
    },
    {
      "lang": "en",
      "text": "increased by 10 000 and that's because",
      "offset": 1201.26,
      "duration": 4.86
    },
    {
      "lang": "en",
      "text": "the block size has grown so this first",
      "offset": 1203.62,
      "duration": 4.6
    },
    {
      "lang": "en",
      "text": "linear layer is much much bigger our",
      "offset": 1206.12,
      "duration": 4.38
    },
    {
      "lang": "en",
      "text": "linear layer now takes eight characters",
      "offset": 1208.22,
      "duration": 4.98
    },
    {
      "lang": "en",
      "text": "into this middle layer so there's a lot",
      "offset": 1210.5,
      "duration": 4.98
    },
    {
      "lang": "en",
      "text": "more parameters there but this should",
      "offset": 1213.2,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "just run let me just break right after",
      "offset": 1215.48,
      "duration": 4.98
    },
    {
      "lang": "en",
      "text": "the very first iteration so you see that",
      "offset": 1218.24,
      "duration": 4.2
    },
    {
      "lang": "en",
      "text": "this runs just fine it's just that this",
      "offset": 1220.46,
      "duration": 3.06
    },
    {
      "lang": "en",
      "text": "network doesn't make too much sense",
      "offset": 1222.44,
      "duration": 2.64
    },
    {
      "lang": "en",
      "text": "we're crushing way too much information",
      "offset": 1223.52,
      "duration": 3.42
    },
    {
      "lang": "en",
      "text": "way too fast",
      "offset": 1225.08,
      "duration": 3.959
    },
    {
      "lang": "en",
      "text": "so let's now come in and see how we",
      "offset": 1226.94,
      "duration": 4.02
    },
    {
      "lang": "en",
      "text": "could try to implement the hierarchical",
      "offset": 1229.039,
      "duration": 4.201
    },
    {
      "lang": "en",
      "text": "scheme now before we dive into the",
      "offset": 1230.96,
      "duration": 4.38
    },
    {
      "lang": "en",
      "text": "detail of the re-implementation here I",
      "offset": 1233.24,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "was just curious to actually run it and",
      "offset": 1235.34,
      "duration": 3.18
    },
    {
      "lang": "en",
      "text": "see where we are in terms of the",
      "offset": 1237.559,
      "duration": 2.701
    },
    {
      "lang": "en",
      "text": "Baseline performance of just lazily",
      "offset": 1238.52,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "scaling up the context length so I'll",
      "offset": 1240.26,
      "duration": 4.86
    },
    {
      "lang": "en",
      "text": "let it run we get a nice loss curve and",
      "offset": 1242.84,
      "duration": 3.959
    },
    {
      "lang": "en",
      "text": "then evaluating the loss we actually see",
      "offset": 1245.12,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "quite a bit of improvement just from",
      "offset": 1246.799,
      "duration": 4.441
    },
    {
      "lang": "en",
      "text": "increasing the context line length so I",
      "offset": 1248.48,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "started a little bit of a performance",
      "offset": 1251.24,
      "duration": 3.54
    },
    {
      "lang": "en",
      "text": "log here and previously where we were is",
      "offset": 1252.32,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "we were getting a performance of 2.10 on",
      "offset": 1254.78,
      "duration": 4.98
    },
    {
      "lang": "en",
      "text": "the validation loss and now simply",
      "offset": 1257.84,
      "duration": 3.54
    },
    {
      "lang": "en",
      "text": "scaling up the contact length from 3 to",
      "offset": 1259.76,
      "duration": 5.64
    },
    {
      "lang": "en",
      "text": "8 gives us a performance of 2.02 so",
      "offset": 1261.38,
      "duration": 5.88
    },
    {
      "lang": "en",
      "text": "quite a bit of an improvement here and",
      "offset": 1265.4,
      "duration": 3.42
    },
    {
      "lang": "en",
      "text": "also when you sample from the model you",
      "offset": 1267.26,
      "duration": 2.94
    },
    {
      "lang": "en",
      "text": "see that the names are definitely",
      "offset": 1268.82,
      "duration": 4.2
    },
    {
      "lang": "en",
      "text": "improving qualitatively as well",
      "offset": 1270.2,
      "duration": 4.38
    },
    {
      "lang": "en",
      "text": "so we could of course spend a lot of",
      "offset": 1273.02,
      "duration": 3.06
    },
    {
      "lang": "en",
      "text": "time here tuning",
      "offset": 1274.58,
      "duration": 3.42
    },
    {
      "lang": "en",
      "text": "um uh tuning things and making it even",
      "offset": 1276.08,
      "duration": 3.18
    },
    {
      "lang": "en",
      "text": "bigger and scaling up the network",
      "offset": 1278,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "further even with the simple",
      "offset": 1279.26,
      "duration": 5.1
    },
    {
      "lang": "en",
      "text": "um sort of setup here but let's continue",
      "offset": 1281.6,
      "duration": 5.579
    },
    {
      "lang": "en",
      "text": "and let's Implement here model and treat",
      "offset": 1284.36,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "this as just a rough Baseline",
      "offset": 1287.179,
      "duration": 3.541
    },
    {
      "lang": "en",
      "text": "performance but there's a lot of",
      "offset": 1288.679,
      "duration": 4.261
    },
    {
      "lang": "en",
      "text": "optimization like left on the table in",
      "offset": 1290.72,
      "duration": 3.54
    },
    {
      "lang": "en",
      "text": "terms of some of the hyper parameters",
      "offset": 1292.94,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "that you're hopefully getting a sense of",
      "offset": 1294.26,
      "duration": 4.14
    },
    {
      "lang": "en",
      "text": "now okay so let's scroll up now",
      "offset": 1295.82,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "and come back up and what I've done here",
      "offset": 1298.4,
      "duration": 4.38
    },
    {
      "lang": "en",
      "text": "is I've created a bit of a scratch space",
      "offset": 1301.1,
      "duration": 4.439
    },
    {
      "lang": "en",
      "text": "for us to just like look at the forward",
      "offset": 1302.78,
      "duration": 5.1
    },
    {
      "lang": "en",
      "text": "pass of the neural net and inspect the",
      "offset": 1305.539,
      "duration": 4.26
    },
    {
      "lang": "en",
      "text": "shape of the tensor along the way as the",
      "offset": 1307.88,
      "duration": 5.4
    },
    {
      "lang": "en",
      "text": "neural net uh forwards so here I'm just",
      "offset": 1309.799,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "temporarily for debugging creating a",
      "offset": 1313.28,
      "duration": 4.74
    },
    {
      "lang": "en",
      "text": "batch of just say four examples so four",
      "offset": 1315.799,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "random integers then I'm plucking out",
      "offset": 1318.02,
      "duration": 4.2
    },
    {
      "lang": "en",
      "text": "those rows from our training set",
      "offset": 1320.12,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "and then I'm passing into the model the",
      "offset": 1322.22,
      "duration": 4.26
    },
    {
      "lang": "en",
      "text": "input xB",
      "offset": 1324.679,
      "duration": 3.961
    },
    {
      "lang": "en",
      "text": "now the shape of XB here because we have",
      "offset": 1326.48,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "only four examples is four by eight and",
      "offset": 1328.64,
      "duration": 5.64
    },
    {
      "lang": "en",
      "text": "this eight is now the current block size",
      "offset": 1331.28,
      "duration": 6.72
    },
    {
      "lang": "en",
      "text": "so uh inspecting XP we just see that we",
      "offset": 1334.28,
      "duration": 5.58
    },
    {
      "lang": "en",
      "text": "have four examples each one of them is a",
      "offset": 1338,
      "duration": 3.299
    },
    {
      "lang": "en",
      "text": "row of xB",
      "offset": 1339.86,
      "duration": 4.86
    },
    {
      "lang": "en",
      "text": "and we have eight characters here and",
      "offset": 1341.299,
      "duration": 5.041
    },
    {
      "lang": "en",
      "text": "this integer tensor just contains the",
      "offset": 1344.72,
      "duration": 4.579
    },
    {
      "lang": "en",
      "text": "identities of those characters",
      "offset": 1346.34,
      "duration": 2.959
    },
    {
      "lang": "en",
      "text": "so the first layer of our neural net is",
      "offset": 1349.46,
      "duration": 4.5
    },
    {
      "lang": "en",
      "text": "the embedding layer so passing XB this",
      "offset": 1351.2,
      "duration": 4.44
    },
    {
      "lang": "en",
      "text": "integer tensor through the embedding",
      "offset": 1353.96,
      "duration": 3.719
    },
    {
      "lang": "en",
      "text": "layer creates an output that is four by",
      "offset": 1355.64,
      "duration": 3.419
    },
    {
      "lang": "en",
      "text": "eight by ten",
      "offset": 1357.679,
      "duration": 5.041
    },
    {
      "lang": "en",
      "text": "so our embedding table has for each",
      "offset": 1359.059,
      "duration": 5.701
    },
    {
      "lang": "en",
      "text": "character a 10-dimensional vector that",
      "offset": 1362.72,
      "duration": 3.3
    },
    {
      "lang": "en",
      "text": "we are trying to learn",
      "offset": 1364.76,
      "duration": 3.299
    },
    {
      "lang": "en",
      "text": "and so what the embedding layer does",
      "offset": 1366.02,
      "duration": 4.86
    },
    {
      "lang": "en",
      "text": "here is it plucks out the embedding",
      "offset": 1368.059,
      "duration": 5.461
    },
    {
      "lang": "en",
      "text": "Vector for each one of these integers",
      "offset": 1370.88,
      "duration": 5.159
    },
    {
      "lang": "en",
      "text": "and organizes it all in a four by eight",
      "offset": 1373.52,
      "duration": 4.74
    },
    {
      "lang": "en",
      "text": "by ten tensor now",
      "offset": 1376.039,
      "duration": 4.26
    },
    {
      "lang": "en",
      "text": "so all of these integers are translated",
      "offset": 1378.26,
      "duration": 4.2
    },
    {
      "lang": "en",
      "text": "into 10 dimensional vectors inside this",
      "offset": 1380.299,
      "duration": 4.441
    },
    {
      "lang": "en",
      "text": "three-dimensional tensor now",
      "offset": 1382.46,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "passing that through the flattened layer",
      "offset": 1384.74,
      "duration": 4.679
    },
    {
      "lang": "en",
      "text": "as you recall what this does is it views",
      "offset": 1386.539,
      "duration": 6.181
    },
    {
      "lang": "en",
      "text": "this tensor as just a 4 by 80 tensor and",
      "offset": 1389.419,
      "duration": 5.701
    },
    {
      "lang": "en",
      "text": "what that effectively does is that all",
      "offset": 1392.72,
      "duration": 4.14
    },
    {
      "lang": "en",
      "text": "these 10 dimensional embeddings for all",
      "offset": 1395.12,
      "duration": 3.66
    },
    {
      "lang": "en",
      "text": "these eight characters just end up being",
      "offset": 1396.86,
      "duration": 4.679
    },
    {
      "lang": "en",
      "text": "stretched out into a long row",
      "offset": 1398.78,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "and that looks kind of like a",
      "offset": 1401.539,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "concatenation operation basically so by",
      "offset": 1402.86,
      "duration": 4.74
    },
    {
      "lang": "en",
      "text": "viewing the tensor differently we now",
      "offset": 1405.62,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "have a four by eighty and inside this 80",
      "offset": 1407.6,
      "duration": 4.98
    },
    {
      "lang": "en",
      "text": "it's all the 10 dimensional uh",
      "offset": 1409.94,
      "duration": 5.4
    },
    {
      "lang": "en",
      "text": "vectors just uh concatenate next to each",
      "offset": 1412.58,
      "duration": 3.54
    },
    {
      "lang": "en",
      "text": "other",
      "offset": 1415.34,
      "duration": 2.16
    },
    {
      "lang": "en",
      "text": "and then the linear layer of course",
      "offset": 1416.12,
      "duration": 4.5
    },
    {
      "lang": "en",
      "text": "takes uh 80 and creates 200 channels",
      "offset": 1417.5,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "just via matrix multiplication",
      "offset": 1420.62,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "so so far so good now I'd like to show",
      "offset": 1423.26,
      "duration": 4.02
    },
    {
      "lang": "en",
      "text": "you something surprising",
      "offset": 1425.419,
      "duration": 4.921
    },
    {
      "lang": "en",
      "text": "let's look at the insides of the linear",
      "offset": 1427.28,
      "duration": 5.22
    },
    {
      "lang": "en",
      "text": "layer and remind ourselves how it works",
      "offset": 1430.34,
      "duration": 3.78
    },
    {
      "lang": "en",
      "text": "the linear layer here in the forward",
      "offset": 1432.5,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "pass takes the input X multiplies it",
      "offset": 1434.12,
      "duration": 4.62
    },
    {
      "lang": "en",
      "text": "with a weight and then optionally adds",
      "offset": 1436.82,
      "duration": 3.66
    },
    {
      "lang": "en",
      "text": "bias and the weight here is",
      "offset": 1438.74,
      "duration": 3.419
    },
    {
      "lang": "en",
      "text": "two-dimensional as defined here and the",
      "offset": 1440.48,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "bias is one dimensional here",
      "offset": 1442.159,
      "duration": 4.201
    },
    {
      "lang": "en",
      "text": "so effectively in terms of the shapes",
      "offset": 1444.32,
      "duration": 3.9
    },
    {
      "lang": "en",
      "text": "involved what's happening inside this",
      "offset": 1446.36,
      "duration": 4.62
    },
    {
      "lang": "en",
      "text": "linear layer looks like this right now",
      "offset": 1448.22,
      "duration": 4.68
    },
    {
      "lang": "en",
      "text": "and I'm using random numbers here but",
      "offset": 1450.98,
      "duration": 4.02
    },
    {
      "lang": "en",
      "text": "I'm just illustrating the shapes and",
      "offset": 1452.9,
      "duration": 3.24
    },
    {
      "lang": "en",
      "text": "what happens",
      "offset": 1455,
      "duration": 3.96
    },
    {
      "lang": "en",
      "text": "basically a 4 by 80 input comes into the",
      "offset": 1456.14,
      "duration": 4.68
    },
    {
      "lang": "en",
      "text": "linear layer that's multiplied by this",
      "offset": 1458.96,
      "duration": 4.199
    },
    {
      "lang": "en",
      "text": "80 by 200 weight Matrix inside and",
      "offset": 1460.82,
      "duration": 4.62
    },
    {
      "lang": "en",
      "text": "there's a plus 200 bias and the shape of",
      "offset": 1463.159,
      "duration": 3.481
    },
    {
      "lang": "en",
      "text": "the whole thing that comes out of the",
      "offset": 1465.44,
      "duration": 3.54
    },
    {
      "lang": "en",
      "text": "linear layer is four by two hundred as",
      "offset": 1466.64,
      "duration": 3.899
    },
    {
      "lang": "en",
      "text": "we see here",
      "offset": 1468.98,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "now notice here by the way that this",
      "offset": 1470.539,
      "duration": 5.821
    },
    {
      "lang": "en",
      "text": "here will create a 4x200 tensor and then",
      "offset": 1472.82,
      "duration": 5.339
    },
    {
      "lang": "en",
      "text": "plus 200 there's a broadcasting",
      "offset": 1476.36,
      "duration": 4.679
    },
    {
      "lang": "en",
      "text": "happening here about 4 by 200 broadcasts",
      "offset": 1478.159,
      "duration": 6.421
    },
    {
      "lang": "en",
      "text": "with 200 uh so everything works here",
      "offset": 1481.039,
      "duration": 4.981
    },
    {
      "lang": "en",
      "text": "so now the surprising thing that I'd",
      "offset": 1484.58,
      "duration": 2.94
    },
    {
      "lang": "en",
      "text": "like to show you that you may not expect",
      "offset": 1486.02,
      "duration": 3.96
    },
    {
      "lang": "en",
      "text": "is that this input here that is being",
      "offset": 1487.52,
      "duration": 4.5
    },
    {
      "lang": "en",
      "text": "multiplied uh doesn't actually have to",
      "offset": 1489.98,
      "duration": 5.16
    },
    {
      "lang": "en",
      "text": "be two-dimensional this Matrix multiply",
      "offset": 1492.02,
      "duration": 4.86
    },
    {
      "lang": "en",
      "text": "operator in pytorch is quite powerful",
      "offset": 1495.14,
      "duration": 3.539
    },
    {
      "lang": "en",
      "text": "and in fact you can actually pass in",
      "offset": 1496.88,
      "duration": 3.96
    },
    {
      "lang": "en",
      "text": "higher dimensional arrays or tensors and",
      "offset": 1498.679,
      "duration": 3.781
    },
    {
      "lang": "en",
      "text": "everything works fine so for example",
      "offset": 1500.84,
      "duration": 3.54
    },
    {
      "lang": "en",
      "text": "this could be four by five by eighty and",
      "offset": 1502.46,
      "duration": 3.78
    },
    {
      "lang": "en",
      "text": "the result in that case will become four",
      "offset": 1504.38,
      "duration": 3.779
    },
    {
      "lang": "en",
      "text": "by five by two hundred",
      "offset": 1506.24,
      "duration": 3.66
    },
    {
      "lang": "en",
      "text": "you can add as many dimensions as you",
      "offset": 1508.159,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "like on the left here",
      "offset": 1509.9,
      "duration": 3.779
    },
    {
      "lang": "en",
      "text": "and so effectively what's happening is",
      "offset": 1511.52,
      "duration": 3.72
    },
    {
      "lang": "en",
      "text": "that the matrix multiplication only",
      "offset": 1513.679,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "works on the last Dimension and the",
      "offset": 1515.24,
      "duration": 4.5
    },
    {
      "lang": "en",
      "text": "dimensions before it in the input tensor",
      "offset": 1517.76,
      "duration": 5.299
    },
    {
      "lang": "en",
      "text": "are left unchanged",
      "offset": 1519.74,
      "duration": 3.319
    },
    {
      "lang": "en",
      "text": "so that is basically these um these",
      "offset": 1524.539,
      "duration": 4.681
    },
    {
      "lang": "en",
      "text": "dimensions on the left are all treated",
      "offset": 1527.72,
      "duration": 4.74
    },
    {
      "lang": "en",
      "text": "as just a batch Dimension so we can have",
      "offset": 1529.22,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "multiple batch dimensions and then in",
      "offset": 1532.46,
      "duration": 4.199
    },
    {
      "lang": "en",
      "text": "parallel over all those Dimensions we",
      "offset": 1534.74,
      "duration": 3.299
    },
    {
      "lang": "en",
      "text": "are doing the matrix multiplication on",
      "offset": 1536.659,
      "duration": 2.701
    },
    {
      "lang": "en",
      "text": "the last dimension",
      "offset": 1538.039,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "so this is quite convenient because we",
      "offset": 1539.36,
      "duration": 4.86
    },
    {
      "lang": "en",
      "text": "can use that in our Network now",
      "offset": 1541.88,
      "duration": 4.38
    },
    {
      "lang": "en",
      "text": "because remember that we have these",
      "offset": 1544.22,
      "duration": 4.86
    },
    {
      "lang": "en",
      "text": "eight characters coming in",
      "offset": 1546.26,
      "duration": 5.58
    },
    {
      "lang": "en",
      "text": "and we don't want to now uh flatten all",
      "offset": 1549.08,
      "duration": 4.68
    },
    {
      "lang": "en",
      "text": "of it out into a large eight-dimensional",
      "offset": 1551.84,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "vector",
      "offset": 1553.76,
      "duration": 3.48
    },
    {
      "lang": "en",
      "text": "because we don't want to Matrix multiply",
      "offset": 1554.96,
      "duration": 4.5
    },
    {
      "lang": "en",
      "text": "80.",
      "offset": 1557.24,
      "duration": 4.02
    },
    {
      "lang": "en",
      "text": "into a weight Matrix multiply",
      "offset": 1559.46,
      "duration": 4.26
    },
    {
      "lang": "en",
      "text": "immediately instead we want to group",
      "offset": 1561.26,
      "duration": 3.48
    },
    {
      "lang": "en",
      "text": "these",
      "offset": 1563.72,
      "duration": 3.18
    },
    {
      "lang": "en",
      "text": "like this",
      "offset": 1564.74,
      "duration": 4.919
    },
    {
      "lang": "en",
      "text": "so every consecutive two elements",
      "offset": 1566.9,
      "duration": 4.44
    },
    {
      "lang": "en",
      "text": "one two and three and four and five and",
      "offset": 1569.659,
      "duration": 3.181
    },
    {
      "lang": "en",
      "text": "six and seven and eight all of these",
      "offset": 1571.34,
      "duration": 3.24
    },
    {
      "lang": "en",
      "text": "should be now",
      "offset": 1572.84,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "basically flattened out and multiplied",
      "offset": 1574.58,
      "duration": 4.74
    },
    {
      "lang": "en",
      "text": "by weight Matrix but all of these four",
      "offset": 1577.159,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "groups here we'd like to process in",
      "offset": 1579.32,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "parallel so it's kind of like a batch",
      "offset": 1581.48,
      "duration": 4.38
    },
    {
      "lang": "en",
      "text": "Dimension that we can introduce",
      "offset": 1583.64,
      "duration": 5.159
    },
    {
      "lang": "en",
      "text": "and then we can in parallel basically",
      "offset": 1585.86,
      "duration": 7.14
    },
    {
      "lang": "en",
      "text": "process all of these uh bigram groups in",
      "offset": 1588.799,
      "duration": 6.12
    },
    {
      "lang": "en",
      "text": "the four batch dimensions of an",
      "offset": 1593,
      "duration": 4.2
    },
    {
      "lang": "en",
      "text": "individual example and also over the",
      "offset": 1594.919,
      "duration": 4.62
    },
    {
      "lang": "en",
      "text": "actual batch dimension of the you know",
      "offset": 1597.2,
      "duration": 4.859
    },
    {
      "lang": "en",
      "text": "four examples in our example here so",
      "offset": 1599.539,
      "duration": 4.441
    },
    {
      "lang": "en",
      "text": "let's see how that works effectively",
      "offset": 1602.059,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "what we want is right now we take a 4 by",
      "offset": 1603.98,
      "duration": 3.66
    },
    {
      "lang": "en",
      "text": "80",
      "offset": 1606.86,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "and multiply it by 80 by 200",
      "offset": 1607.64,
      "duration": 4.62
    },
    {
      "lang": "en",
      "text": "to in the linear layer this is what",
      "offset": 1610.7,
      "duration": 3.06
    },
    {
      "lang": "en",
      "text": "happens",
      "offset": 1612.26,
      "duration": 3.899
    },
    {
      "lang": "en",
      "text": "but instead what we want is we don't",
      "offset": 1613.76,
      "duration": 4.86
    },
    {
      "lang": "en",
      "text": "want 80 characters or 80 numbers to come",
      "offset": 1616.159,
      "duration": 4.741
    },
    {
      "lang": "en",
      "text": "in we only want two characters to come",
      "offset": 1618.62,
      "duration": 4.02
    },
    {
      "lang": "en",
      "text": "in on the very first layer and those two",
      "offset": 1620.9,
      "duration": 3.899
    },
    {
      "lang": "en",
      "text": "characters should be fused",
      "offset": 1622.64,
      "duration": 4.86
    },
    {
      "lang": "en",
      "text": "so in other words we just want 20 to",
      "offset": 1624.799,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "come in right 20 numbers would come in",
      "offset": 1627.5,
      "duration": 5.94
    },
    {
      "lang": "en",
      "text": "and here we don't want a 4 by 80 to feed",
      "offset": 1631.039,
      "duration": 4.38
    },
    {
      "lang": "en",
      "text": "into the linear layer we actually want",
      "offset": 1633.44,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "these groups of two to feed in so",
      "offset": 1635.419,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "instead of four by eighty we want this",
      "offset": 1637.76,
      "duration": 5.64
    },
    {
      "lang": "en",
      "text": "to be a 4 by 4 by 20.",
      "offset": 1639.26,
      "duration": 7.74
    },
    {
      "lang": "en",
      "text": "so these are the four groups of two and",
      "offset": 1643.4,
      "duration": 5.1
    },
    {
      "lang": "en",
      "text": "each one of them is ten dimensional",
      "offset": 1647,
      "duration": 2.58
    },
    {
      "lang": "en",
      "text": "vector",
      "offset": 1648.5,
      "duration": 3.059
    },
    {
      "lang": "en",
      "text": "so what we want is now is we need to",
      "offset": 1649.58,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "change the flattened layer so it doesn't",
      "offset": 1651.559,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "output a four by eighty but it outputs a",
      "offset": 1653.179,
      "duration": 4.921
    },
    {
      "lang": "en",
      "text": "four by four by Twenty where basically",
      "offset": 1655.4,
      "duration": 4.44
    },
    {
      "lang": "en",
      "text": "these um",
      "offset": 1658.1,
      "duration": 5.459
    },
    {
      "lang": "en",
      "text": "every two consecutive characters are uh",
      "offset": 1659.84,
      "duration": 6.66
    },
    {
      "lang": "en",
      "text": "packed in on the very last Dimension and",
      "offset": 1663.559,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "then these four is the first batch",
      "offset": 1666.5,
      "duration": 3.9
    },
    {
      "lang": "en",
      "text": "Dimension and this four is the second",
      "offset": 1668.12,
      "duration": 4.5
    },
    {
      "lang": "en",
      "text": "batch Dimension referring to the four",
      "offset": 1670.4,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "groups inside every one of these",
      "offset": 1672.62,
      "duration": 2.7
    },
    {
      "lang": "en",
      "text": "examples",
      "offset": 1674.24,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "and then this will just multiply like",
      "offset": 1675.32,
      "duration": 4.2
    },
    {
      "lang": "en",
      "text": "this so this is what we want to get to",
      "offset": 1677.6,
      "duration": 3.48
    },
    {
      "lang": "en",
      "text": "so we're going to have to change the",
      "offset": 1679.52,
      "duration": 3.42
    },
    {
      "lang": "en",
      "text": "linear layer in terms of how many inputs",
      "offset": 1681.08,
      "duration": 4.68
    },
    {
      "lang": "en",
      "text": "it expects it shouldn't expect 80 it",
      "offset": 1682.94,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "should just expect 20 numbers and we",
      "offset": 1685.76,
      "duration": 3.72
    },
    {
      "lang": "en",
      "text": "have to change our flattened layer so it",
      "offset": 1687.74,
      "duration": 3.66
    },
    {
      "lang": "en",
      "text": "doesn't just fully flatten out this",
      "offset": 1689.48,
      "duration": 4.86
    },
    {
      "lang": "en",
      "text": "entire example it needs to create a 4x4",
      "offset": 1691.4,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "by 20 instead of four by eighty so let's",
      "offset": 1694.34,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "see how this could be implemented",
      "offset": 1697.64,
      "duration": 3.659
    },
    {
      "lang": "en",
      "text": "basically right now we have an input",
      "offset": 1699.38,
      "duration": 4.26
    },
    {
      "lang": "en",
      "text": "that is a four by eight by ten that",
      "offset": 1701.299,
      "duration": 4.38
    },
    {
      "lang": "en",
      "text": "feeds into the flattened layer and",
      "offset": 1703.64,
      "duration": 3.539
    },
    {
      "lang": "en",
      "text": "currently the flattened layer just",
      "offset": 1705.679,
      "duration": 3.961
    },
    {
      "lang": "en",
      "text": "stretches it out so if you remember the",
      "offset": 1707.179,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "implementation of flatten",
      "offset": 1709.64,
      "duration": 4.5
    },
    {
      "lang": "en",
      "text": "it takes RX and it just views it as",
      "offset": 1711.5,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "whatever the batch Dimension is and then",
      "offset": 1714.14,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "negative one",
      "offset": 1715.82,
      "duration": 3.719
    },
    {
      "lang": "en",
      "text": "so effectively what it does right now is",
      "offset": 1717.14,
      "duration": 5.399
    },
    {
      "lang": "en",
      "text": "it does e dot view of 4 negative one and",
      "offset": 1719.539,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "the shape of this of course is 4 by 80.",
      "offset": 1722.539,
      "duration": 5.461
    },
    {
      "lang": "en",
      "text": "so that's what currently happens and we",
      "offset": 1725.779,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "instead want this to be a four by four",
      "offset": 1728,
      "duration": 3.72
    },
    {
      "lang": "en",
      "text": "by Twenty where these consecutive",
      "offset": 1729.86,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "ten-dimensional vectors get concatenated",
      "offset": 1731.72,
      "duration": 5.88
    },
    {
      "lang": "en",
      "text": "so you know how in Python you can take a",
      "offset": 1734.179,
      "duration": 5.88
    },
    {
      "lang": "en",
      "text": "list of range of 10",
      "offset": 1737.6,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "so we have numbers from zero to nine and",
      "offset": 1740.059,
      "duration": 5.1
    },
    {
      "lang": "en",
      "text": "we can index like this to get all the",
      "offset": 1743.36,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "even parts",
      "offset": 1745.159,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "and we can also index like starting at",
      "offset": 1746.48,
      "duration": 4.62
    },
    {
      "lang": "en",
      "text": "one and going in steps up two to get all",
      "offset": 1748.76,
      "duration": 4.5
    },
    {
      "lang": "en",
      "text": "the odd parts",
      "offset": 1751.1,
      "duration": 4.38
    },
    {
      "lang": "en",
      "text": "so one way to implement this it would be",
      "offset": 1753.26,
      "duration": 5.519
    },
    {
      "lang": "en",
      "text": "as follows we can take e and we can",
      "offset": 1755.48,
      "duration": 5.88
    },
    {
      "lang": "en",
      "text": "index into it for all the batch elements",
      "offset": 1758.779,
      "duration": 5.4
    },
    {
      "lang": "en",
      "text": "and then just even elements in this",
      "offset": 1761.36,
      "duration": 7.74
    },
    {
      "lang": "en",
      "text": "Dimension so at indexes 0 2 4 and 8.",
      "offset": 1764.179,
      "duration": 7.561
    },
    {
      "lang": "en",
      "text": "and then all the parts here from this",
      "offset": 1769.1,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "last dimension",
      "offset": 1771.74,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "and this gives us the even characters",
      "offset": 1773.419,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "and then here",
      "offset": 1777.5,
      "duration": 4.62
    },
    {
      "lang": "en",
      "text": "this gives us all the odd characters and",
      "offset": 1779.179,
      "duration": 4.021
    },
    {
      "lang": "en",
      "text": "basically what we want to do is we make",
      "offset": 1782.12,
      "duration": 2.34
    },
    {
      "lang": "en",
      "text": "sure we want to make sure that these get",
      "offset": 1783.2,
      "duration": 4.14
    },
    {
      "lang": "en",
      "text": "concatenated in pi torch and then we",
      "offset": 1784.46,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "want to concatenate these two tensors",
      "offset": 1787.34,
      "duration": 5.819
    },
    {
      "lang": "en",
      "text": "along the second dimension",
      "offset": 1789.5,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "so this and the shape of it would be",
      "offset": 1793.159,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "four by four by Twenty this is",
      "offset": 1795.5,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "definitely the result we want we are",
      "offset": 1797,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "explicitly grabbing the even parts and",
      "offset": 1798.86,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "the odd parts and we're arranging those",
      "offset": 1801.559,
      "duration": 4.921
    },
    {
      "lang": "en",
      "text": "four by four by ten right next to each",
      "offset": 1803.659,
      "duration": 4.74
    },
    {
      "lang": "en",
      "text": "other and concatenate",
      "offset": 1806.48,
      "duration": 4.199
    },
    {
      "lang": "en",
      "text": "so this works but it turns out that what",
      "offset": 1808.399,
      "duration": 4.981
    },
    {
      "lang": "en",
      "text": "also works is you can simply use a view",
      "offset": 1810.679,
      "duration": 5.581
    },
    {
      "lang": "en",
      "text": "again and just request the right shape",
      "offset": 1813.38,
      "duration": 4.679
    },
    {
      "lang": "en",
      "text": "and it just so happens that in this case",
      "offset": 1816.26,
      "duration": 4.98
    },
    {
      "lang": "en",
      "text": "those vectors will again end up being",
      "offset": 1818.059,
      "duration": 5.401
    },
    {
      "lang": "en",
      "text": "arranged in exactly the way we want so",
      "offset": 1821.24,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "in particular if we take e and we just",
      "offset": 1823.46,
      "duration": 3.66
    },
    {
      "lang": "en",
      "text": "view it as a four by four by Twenty",
      "offset": 1825.32,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "which is what we want",
      "offset": 1827.12,
      "duration": 3.539
    },
    {
      "lang": "en",
      "text": "we can check that this is exactly equal",
      "offset": 1828.679,
      "duration": 4.5
    },
    {
      "lang": "en",
      "text": "to but let me call this this is the",
      "offset": 1830.659,
      "duration": 5.64
    },
    {
      "lang": "en",
      "text": "explicit concatenation I suppose",
      "offset": 1833.179,
      "duration": 3.72
    },
    {
      "lang": "en",
      "text": "um",
      "offset": 1836.299,
      "duration": 4.38
    },
    {
      "lang": "en",
      "text": "so explosives dot shape is 4x4 by 20. if",
      "offset": 1836.899,
      "duration": 5.88
    },
    {
      "lang": "en",
      "text": "you just view it as 4x4 by 20 you can",
      "offset": 1840.679,
      "duration": 5.341
    },
    {
      "lang": "en",
      "text": "check that when you compare to explicit",
      "offset": 1842.779,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "uh you got a big this is element wise",
      "offset": 1846.02,
      "duration": 3.899
    },
    {
      "lang": "en",
      "text": "operation so making sure that all of",
      "offset": 1848.299,
      "duration": 4.74
    },
    {
      "lang": "en",
      "text": "them are true that is the truth so",
      "offset": 1849.919,
      "duration": 4.86
    },
    {
      "lang": "en",
      "text": "basically long story short we don't need",
      "offset": 1853.039,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "to make an explicit call to concatenate",
      "offset": 1854.779,
      "duration": 6.181
    },
    {
      "lang": "en",
      "text": "Etc we can simply take this input tensor",
      "offset": 1856.88,
      "duration": 6.539
    },
    {
      "lang": "en",
      "text": "to flatten and we can just view it in",
      "offset": 1860.96,
      "duration": 3.78
    },
    {
      "lang": "en",
      "text": "whatever way we want",
      "offset": 1863.419,
      "duration": 3.661
    },
    {
      "lang": "en",
      "text": "and in particular you don't want to",
      "offset": 1864.74,
      "duration": 4.62
    },
    {
      "lang": "en",
      "text": "stretch things out with negative one we",
      "offset": 1867.08,
      "duration": 3.24
    },
    {
      "lang": "en",
      "text": "want to actually create a",
      "offset": 1869.36,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "three-dimensional array and depending on",
      "offset": 1870.32,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "how many vectors that are consecutive we",
      "offset": 1872.48,
      "duration": 4.199
    },
    {
      "lang": "en",
      "text": "want to",
      "offset": 1875.6,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "um fuse like for example two then we can",
      "offset": 1876.679,
      "duration": 5.1
    },
    {
      "lang": "en",
      "text": "just simply ask for this Dimension to be",
      "offset": 1880.399,
      "duration": 3.78
    },
    {
      "lang": "en",
      "text": "20. and um",
      "offset": 1881.779,
      "duration": 4.681
    },
    {
      "lang": "en",
      "text": "use a negative 1 here and python will",
      "offset": 1884.179,
      "duration": 3.661
    },
    {
      "lang": "en",
      "text": "figure out how many groups it needs to",
      "offset": 1886.46,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "pack into this additional batch",
      "offset": 1887.84,
      "duration": 2.76
    },
    {
      "lang": "en",
      "text": "dimension",
      "offset": 1889.34,
      "duration": 3.3
    },
    {
      "lang": "en",
      "text": "so let's now go into flatten and",
      "offset": 1890.6,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "implement this okay so I scroll up here",
      "offset": 1892.64,
      "duration": 3.779
    },
    {
      "lang": "en",
      "text": "to flatten and what we'd like to do is",
      "offset": 1894.44,
      "duration": 4.14
    },
    {
      "lang": "en",
      "text": "we'd like to change it now so let me",
      "offset": 1896.419,
      "duration": 3.901
    },
    {
      "lang": "en",
      "text": "create a Constructor and take the number",
      "offset": 1898.58,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "of elements that are consecutive that we",
      "offset": 1900.32,
      "duration": 4.26
    },
    {
      "lang": "en",
      "text": "would like to concatenate now in the",
      "offset": 1902.659,
      "duration": 4.14
    },
    {
      "lang": "en",
      "text": "last dimension of the output",
      "offset": 1904.58,
      "duration": 3.74
    },
    {
      "lang": "en",
      "text": "so here we're just going to remember",
      "offset": 1906.799,
      "duration": 3.661
    },
    {
      "lang": "en",
      "text": "solve.n equals n",
      "offset": 1908.32,
      "duration": 3.94
    },
    {
      "lang": "en",
      "text": "and then I want to be careful here",
      "offset": 1910.46,
      "duration": 3.9
    },
    {
      "lang": "en",
      "text": "because pipe pytorch actually has a",
      "offset": 1912.26,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "torch to flatten and its keyword",
      "offset": 1914.36,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "arguments are different and they kind of",
      "offset": 1916.1,
      "duration": 4.14
    },
    {
      "lang": "en",
      "text": "like function differently so R flatten",
      "offset": 1918.2,
      "duration": 3.9
    },
    {
      "lang": "en",
      "text": "is going to start to depart from patreon",
      "offset": 1920.24,
      "duration": 4.14
    },
    {
      "lang": "en",
      "text": "flatten so let me call it flat flatten",
      "offset": 1922.1,
      "duration": 4.38
    },
    {
      "lang": "en",
      "text": "consecutive or something like that just",
      "offset": 1924.38,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "to make sure that our apis are about",
      "offset": 1926.48,
      "duration": 3.299
    },
    {
      "lang": "en",
      "text": "equal",
      "offset": 1928.7,
      "duration": 4.979
    },
    {
      "lang": "en",
      "text": "so this uh basically flattens only some",
      "offset": 1929.779,
      "duration": 5.821
    },
    {
      "lang": "en",
      "text": "n consecutive elements and puts them",
      "offset": 1933.679,
      "duration": 3.901
    },
    {
      "lang": "en",
      "text": "into the last dimension",
      "offset": 1935.6,
      "duration": 5.88
    },
    {
      "lang": "en",
      "text": "now here the shape of X is B by T by C",
      "offset": 1937.58,
      "duration": 5.88
    },
    {
      "lang": "en",
      "text": "so let me",
      "offset": 1941.48,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "pop those out into variables and recall",
      "offset": 1943.46,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "that in our example down below B was 4 T",
      "offset": 1946.279,
      "duration": 6.561
    },
    {
      "lang": "en",
      "text": "was 8 and C was 10.",
      "offset": 1948.98,
      "duration": 3.86
    },
    {
      "lang": "en",
      "text": "now instead of doing x dot view of B by",
      "offset": 1953.539,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "negative one",
      "offset": 1957.08,
      "duration": 5.54
    },
    {
      "lang": "en",
      "text": "right this is what we had before",
      "offset": 1959.539,
      "duration": 3.081
    },
    {
      "lang": "en",
      "text": "we want this to be B by",
      "offset": 1964.22,
      "duration": 5.459
    },
    {
      "lang": "en",
      "text": "um negative 1 by",
      "offset": 1967.1,
      "duration": 5.16
    },
    {
      "lang": "en",
      "text": "and basically here we want c times n",
      "offset": 1969.679,
      "duration": 5.941
    },
    {
      "lang": "en",
      "text": "that's how many consecutive elements we",
      "offset": 1972.26,
      "duration": 4.62
    },
    {
      "lang": "en",
      "text": "want",
      "offset": 1975.62,
      "duration": 3.059
    },
    {
      "lang": "en",
      "text": "and here instead of negative one I don't",
      "offset": 1976.88,
      "duration": 3.24
    },
    {
      "lang": "en",
      "text": "super love the use of negative one",
      "offset": 1978.679,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "because I like to be very explicit so",
      "offset": 1980.12,
      "duration": 3.48
    },
    {
      "lang": "en",
      "text": "that you get error messages when things",
      "offset": 1982.279,
      "duration": 2.701
    },
    {
      "lang": "en",
      "text": "don't go according to your expectation",
      "offset": 1983.6,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "so what do we expect here we expect this",
      "offset": 1984.98,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "to become t",
      "offset": 1987.679,
      "duration": 4.681
    },
    {
      "lang": "en",
      "text": "divide n using integer division here",
      "offset": 1989.059,
      "duration": 5.46
    },
    {
      "lang": "en",
      "text": "so that's what I expect to happen",
      "offset": 1992.36,
      "duration": 3.539
    },
    {
      "lang": "en",
      "text": "and then one more thing I want to do",
      "offset": 1994.519,
      "duration": 3.78
    },
    {
      "lang": "en",
      "text": "here is remember previously all the way",
      "offset": 1995.899,
      "duration": 5.701
    },
    {
      "lang": "en",
      "text": "in the beginning n was three and uh",
      "offset": 1998.299,
      "duration": 4.98
    },
    {
      "lang": "en",
      "text": "basically we're concatenating",
      "offset": 2001.6,
      "duration": 3.72
    },
    {
      "lang": "en",
      "text": "um all the three characters that existed",
      "offset": 2003.279,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "there",
      "offset": 2005.32,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "so we basically are concatenated",
      "offset": 2006.279,
      "duration": 3.541
    },
    {
      "lang": "en",
      "text": "everything",
      "offset": 2008.44,
      "duration": 3.3
    },
    {
      "lang": "en",
      "text": "and so sometimes I can create a spurious",
      "offset": 2009.82,
      "duration": 4.38
    },
    {
      "lang": "en",
      "text": "dimension of one here so if it is the",
      "offset": 2011.74,
      "duration": 5.46
    },
    {
      "lang": "en",
      "text": "case that x dot shape at one is one then",
      "offset": 2014.2,
      "duration": 5.64
    },
    {
      "lang": "en",
      "text": "it's kind of like a spurious dimension",
      "offset": 2017.2,
      "duration": 4.5
    },
    {
      "lang": "en",
      "text": "um so we don't want to return a",
      "offset": 2019.84,
      "duration": 4.38
    },
    {
      "lang": "en",
      "text": "three-dimensional tensor with a one here",
      "offset": 2021.7,
      "duration": 4.38
    },
    {
      "lang": "en",
      "text": "we just want to return a two-dimensional",
      "offset": 2024.22,
      "duration": 4.14
    },
    {
      "lang": "en",
      "text": "tensor exactly as we did before",
      "offset": 2026.08,
      "duration": 4.62
    },
    {
      "lang": "en",
      "text": "so in this case basically we will just",
      "offset": 2028.36,
      "duration": 5.88
    },
    {
      "lang": "en",
      "text": "say x equals x dot squeeze that is a",
      "offset": 2030.7,
      "duration": 6.26
    },
    {
      "lang": "en",
      "text": "pytorch function",
      "offset": 2034.24,
      "duration": 6.779
    },
    {
      "lang": "en",
      "text": "and squeeze takes a dimension that it",
      "offset": 2036.96,
      "duration": 5.86
    },
    {
      "lang": "en",
      "text": "either squeezes out all the dimensions",
      "offset": 2041.019,
      "duration": 4.38
    },
    {
      "lang": "en",
      "text": "of a tensor that are one or you can",
      "offset": 2042.82,
      "duration": 5.4
    },
    {
      "lang": "en",
      "text": "specify the exact Dimension that you",
      "offset": 2045.399,
      "duration": 5.101
    },
    {
      "lang": "en",
      "text": "want to be squeezed and again I like to",
      "offset": 2048.22,
      "duration": 4.5
    },
    {
      "lang": "en",
      "text": "be as explicit as possible always so I",
      "offset": 2050.5,
      "duration": 3.48
    },
    {
      "lang": "en",
      "text": "expect to squeeze out the First",
      "offset": 2052.72,
      "duration": 2.76
    },
    {
      "lang": "en",
      "text": "Dimension only",
      "offset": 2053.98,
      "duration": 3.659
    },
    {
      "lang": "en",
      "text": "of this tensor",
      "offset": 2055.48,
      "duration": 4.02
    },
    {
      "lang": "en",
      "text": "this three-dimensional tensor and if",
      "offset": 2057.639,
      "duration": 3.48
    },
    {
      "lang": "en",
      "text": "this Dimension here is one then I just",
      "offset": 2059.5,
      "duration": 4.74
    },
    {
      "lang": "en",
      "text": "want to return B by c times n",
      "offset": 2061.119,
      "duration": 5.581
    },
    {
      "lang": "en",
      "text": "and so self dot out will be X and then",
      "offset": 2064.24,
      "duration": 4.379
    },
    {
      "lang": "en",
      "text": "we return salt dot out",
      "offset": 2066.7,
      "duration": 4.26
    },
    {
      "lang": "en",
      "text": "so that's the candidate implementation",
      "offset": 2068.619,
      "duration": 4.441
    },
    {
      "lang": "en",
      "text": "and of course this should be self.n",
      "offset": 2070.96,
      "duration": 4.02
    },
    {
      "lang": "en",
      "text": "instead of just n",
      "offset": 2073.06,
      "duration": 3.9
    },
    {
      "lang": "en",
      "text": "so let's run",
      "offset": 2074.98,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "and let's come here now",
      "offset": 2076.96,
      "duration": 4.439
    },
    {
      "lang": "en",
      "text": "and take it for a spin so flatten",
      "offset": 2079.06,
      "duration": 5.059
    },
    {
      "lang": "en",
      "text": "consecutive",
      "offset": 2081.399,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "and in the beginning let's just use",
      "offset": 2084.22,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "eight so this should recover the",
      "offset": 2087.04,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "previous Behavior so flagging",
      "offset": 2089.02,
      "duration": 4.2
    },
    {
      "lang": "en",
      "text": "consecutive of eight uh which is the",
      "offset": 2091.119,
      "duration": 4.5
    },
    {
      "lang": "en",
      "text": "current block size",
      "offset": 2093.22,
      "duration": 4.68
    },
    {
      "lang": "en",
      "text": "we can do this uh that should recover",
      "offset": 2095.619,
      "duration": 3.781
    },
    {
      "lang": "en",
      "text": "the previous Behavior",
      "offset": 2097.9,
      "duration": 4.74
    },
    {
      "lang": "en",
      "text": "so we should be able to run the model",
      "offset": 2099.4,
      "duration": 6.66
    },
    {
      "lang": "en",
      "text": "and here we can inspect I have a little",
      "offset": 2102.64,
      "duration": 5.4
    },
    {
      "lang": "en",
      "text": "code snippet here where I iterate over",
      "offset": 2106.06,
      "duration": 5.46
    },
    {
      "lang": "en",
      "text": "all the layers I print the name of this",
      "offset": 2108.04,
      "duration": 6.72
    },
    {
      "lang": "en",
      "text": "class and the shape",
      "offset": 2111.52,
      "duration": 6.12
    },
    {
      "lang": "en",
      "text": "and so we see the shapes as we expect",
      "offset": 2114.76,
      "duration": 5.22
    },
    {
      "lang": "en",
      "text": "them after every single layer in the top",
      "offset": 2117.64,
      "duration": 5.219
    },
    {
      "lang": "en",
      "text": "bit so now let's try to restructure it",
      "offset": 2119.98,
      "duration": 5.4
    },
    {
      "lang": "en",
      "text": "using our flattened consecutive and do",
      "offset": 2122.859,
      "duration": 5.461
    },
    {
      "lang": "en",
      "text": "it hierarchically so in particular",
      "offset": 2125.38,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "we want to flatten consecutive not just",
      "offset": 2128.32,
      "duration": 4.74
    },
    {
      "lang": "en",
      "text": "not block size but just two",
      "offset": 2130.42,
      "duration": 4.38
    },
    {
      "lang": "en",
      "text": "and then we want to process this with",
      "offset": 2133.06,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "linear now then the number of inputs to",
      "offset": 2134.8,
      "duration": 4.14
    },
    {
      "lang": "en",
      "text": "this linear will not be an embed times",
      "offset": 2137.14,
      "duration": 3.9
    },
    {
      "lang": "en",
      "text": "block size it will now only be n embed",
      "offset": 2138.94,
      "duration": 3.72
    },
    {
      "lang": "en",
      "text": "times two",
      "offset": 2141.04,
      "duration": 3.24
    },
    {
      "lang": "en",
      "text": "20.",
      "offset": 2142.66,
      "duration": 3.959
    },
    {
      "lang": "en",
      "text": "this goes through the first layer and",
      "offset": 2144.28,
      "duration": 4.2
    },
    {
      "lang": "en",
      "text": "now we can in principle just copy paste",
      "offset": 2146.619,
      "duration": 3.181
    },
    {
      "lang": "en",
      "text": "this",
      "offset": 2148.48,
      "duration": 3.06
    },
    {
      "lang": "en",
      "text": "now the next linear layer should expect",
      "offset": 2149.8,
      "duration": 4.14
    },
    {
      "lang": "en",
      "text": "and hidden times two",
      "offset": 2151.54,
      "duration": 7.14
    },
    {
      "lang": "en",
      "text": "and the last piece of it should expect",
      "offset": 2153.94,
      "duration": 7.56
    },
    {
      "lang": "en",
      "text": "and it enters 2 again",
      "offset": 2158.68,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "so this is sort of like the naive",
      "offset": 2161.5,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "version of it",
      "offset": 2163.24,
      "duration": 2.099
    },
    {
      "lang": "en",
      "text": "um",
      "offset": 2164.5,
      "duration": 3.24
    },
    {
      "lang": "en",
      "text": "so running this we now have a much much",
      "offset": 2165.339,
      "duration": 3.721
    },
    {
      "lang": "en",
      "text": "bigger model",
      "offset": 2167.74,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "and we should be able to basically just",
      "offset": 2169.06,
      "duration": 4.38
    },
    {
      "lang": "en",
      "text": "forward the model",
      "offset": 2170.74,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "and now we can inspect uh the numbers in",
      "offset": 2173.44,
      "duration": 4.26
    },
    {
      "lang": "en",
      "text": "between",
      "offset": 2176.5,
      "duration": 3.119
    },
    {
      "lang": "en",
      "text": "so four byte by 20",
      "offset": 2177.7,
      "duration": 3.72
    },
    {
      "lang": "en",
      "text": "was Platinum consecutively into four by",
      "offset": 2179.619,
      "duration": 3.48
    },
    {
      "lang": "en",
      "text": "four by Twenty",
      "offset": 2181.42,
      "duration": 3.54
    },
    {
      "lang": "en",
      "text": "this was projected into four by four by",
      "offset": 2183.099,
      "duration": 3.421
    },
    {
      "lang": "en",
      "text": "two hundred",
      "offset": 2184.96,
      "duration": 4.98
    },
    {
      "lang": "en",
      "text": "and then bash storm just worked out of",
      "offset": 2186.52,
      "duration": 4.98
    },
    {
      "lang": "en",
      "text": "the box we have to verify that bastron",
      "offset": 2189.94,
      "duration": 3.06
    },
    {
      "lang": "en",
      "text": "does the correct thing even though it",
      "offset": 2191.5,
      "duration": 2.82
    },
    {
      "lang": "en",
      "text": "takes a three-dimensional impedance that",
      "offset": 2193,
      "duration": 3.06
    },
    {
      "lang": "en",
      "text": "are two dimensional input",
      "offset": 2194.32,
      "duration": 4.2
    },
    {
      "lang": "en",
      "text": "then we have 10h which is element wise",
      "offset": 2196.06,
      "duration": 5.4
    },
    {
      "lang": "en",
      "text": "then we crushed it again so if we",
      "offset": 2198.52,
      "duration": 4.38
    },
    {
      "lang": "en",
      "text": "flatten consecutively and ended up with",
      "offset": 2201.46,
      "duration": 3.659
    },
    {
      "lang": "en",
      "text": "a four by two by 400 now",
      "offset": 2202.9,
      "duration": 4.26
    },
    {
      "lang": "en",
      "text": "then linear brought it back down to 200",
      "offset": 2205.119,
      "duration": 5.281
    },
    {
      "lang": "en",
      "text": "batch room 10h and lastly we get a 4 by",
      "offset": 2207.16,
      "duration": 5.1
    },
    {
      "lang": "en",
      "text": "400 and we see that the flattened",
      "offset": 2210.4,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "consecutive for the last flatten here uh",
      "offset": 2212.26,
      "duration": 5.099
    },
    {
      "lang": "en",
      "text": "it squeezed out that dimension of one so",
      "offset": 2214.72,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "we only ended up with four by four",
      "offset": 2217.359,
      "duration": 3.621
    },
    {
      "lang": "en",
      "text": "hundred and then linear Bachelor on 10h",
      "offset": 2218.8,
      "duration": 5.279
    },
    {
      "lang": "en",
      "text": "and uh the last linear layer to get our",
      "offset": 2220.98,
      "duration": 5.56
    },
    {
      "lang": "en",
      "text": "logents and so The Lodges end up in the",
      "offset": 2224.079,
      "duration": 4.621
    },
    {
      "lang": "en",
      "text": "same shape as they were before but now",
      "offset": 2226.54,
      "duration": 3.9
    },
    {
      "lang": "en",
      "text": "we actually have a nice three layer",
      "offset": 2228.7,
      "duration": 4.2
    },
    {
      "lang": "en",
      "text": "neural nut and it basically corresponds",
      "offset": 2230.44,
      "duration": 5.22
    },
    {
      "lang": "en",
      "text": "to whoops sorry it basically corresponds",
      "offset": 2232.9,
      "duration": 5.82
    },
    {
      "lang": "en",
      "text": "exactly to this network now except only",
      "offset": 2235.66,
      "duration": 4.439
    },
    {
      "lang": "en",
      "text": "this piece here because we only have",
      "offset": 2238.72,
      "duration": 3.42
    },
    {
      "lang": "en",
      "text": "three layers whereas here in this",
      "offset": 2240.099,
      "duration": 5.161
    },
    {
      "lang": "en",
      "text": "example there's uh four layers with the",
      "offset": 2242.14,
      "duration": 5.939
    },
    {
      "lang": "en",
      "text": "total receptive field size of 16",
      "offset": 2245.26,
      "duration": 4.26
    },
    {
      "lang": "en",
      "text": "characters instead of just eight",
      "offset": 2248.079,
      "duration": 4.5
    },
    {
      "lang": "en",
      "text": "characters so the block size here is 16.",
      "offset": 2249.52,
      "duration": 5.22
    },
    {
      "lang": "en",
      "text": "so this piece of it's basically",
      "offset": 2252.579,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "implemented here",
      "offset": 2254.74,
      "duration": 3.78
    },
    {
      "lang": "en",
      "text": "um now we just have to kind of figure",
      "offset": 2256.66,
      "duration": 4.14
    },
    {
      "lang": "en",
      "text": "out some good Channel numbers to use",
      "offset": 2258.52,
      "duration": 4.26
    },
    {
      "lang": "en",
      "text": "here now in particular I changed the",
      "offset": 2260.8,
      "duration": 4.92
    },
    {
      "lang": "en",
      "text": "number of hidden units to be 68 in this",
      "offset": 2262.78,
      "duration": 4.98
    },
    {
      "lang": "en",
      "text": "architecture because when I use 68 the",
      "offset": 2265.72,
      "duration": 4.02
    },
    {
      "lang": "en",
      "text": "number of parameters comes out to be 22",
      "offset": 2267.76,
      "duration": 4.26
    },
    {
      "lang": "en",
      "text": "000 so that's exactly the same that we",
      "offset": 2269.74,
      "duration": 4.379
    },
    {
      "lang": "en",
      "text": "had before and we have the same amount",
      "offset": 2272.02,
      "duration": 4.14
    },
    {
      "lang": "en",
      "text": "of capacity at this neural net in terms",
      "offset": 2274.119,
      "duration": 3.661
    },
    {
      "lang": "en",
      "text": "of the number of parameters but the",
      "offset": 2276.16,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "question is whether we are utilizing",
      "offset": 2277.78,
      "duration": 2.64
    },
    {
      "lang": "en",
      "text": "those parameters in a more efficient",
      "offset": 2279.16,
      "duration": 4.439
    },
    {
      "lang": "en",
      "text": "architecture so what I did then is I got",
      "offset": 2280.42,
      "duration": 5.159
    },
    {
      "lang": "en",
      "text": "rid of a lot of the debugging cells here",
      "offset": 2283.599,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "and I rerun the optimization and",
      "offset": 2285.579,
      "duration": 4.26
    },
    {
      "lang": "en",
      "text": "scrolling down to the result we see that",
      "offset": 2287.68,
      "duration": 4.74
    },
    {
      "lang": "en",
      "text": "we get the identical performance roughly",
      "offset": 2289.839,
      "duration": 6.061
    },
    {
      "lang": "en",
      "text": "so our validation loss now is 2.029 and",
      "offset": 2292.42,
      "duration": 6.12
    },
    {
      "lang": "en",
      "text": "previously it was 2.027 so controlling",
      "offset": 2295.9,
      "duration": 4.199
    },
    {
      "lang": "en",
      "text": "for the number of parameters changing",
      "offset": 2298.54,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "from the flat to hierarchical is not",
      "offset": 2300.099,
      "duration": 3.301
    },
    {
      "lang": "en",
      "text": "giving us anything yet",
      "offset": 2301.54,
      "duration": 3.96
    },
    {
      "lang": "en",
      "text": "that said there are two things",
      "offset": 2303.4,
      "duration": 4.26
    },
    {
      "lang": "en",
      "text": "um to point out number one we didn't",
      "offset": 2305.5,
      "duration": 4.38
    },
    {
      "lang": "en",
      "text": "really torture the um architecture here",
      "offset": 2307.66,
      "duration": 3.54
    },
    {
      "lang": "en",
      "text": "very much this is just my first guess",
      "offset": 2309.88,
      "duration": 3.239
    },
    {
      "lang": "en",
      "text": "and there's a bunch of hyper parameters",
      "offset": 2311.2,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "search that we could do in order in",
      "offset": 2313.119,
      "duration": 4.021
    },
    {
      "lang": "en",
      "text": "terms of how we allocate uh our budget",
      "offset": 2315.04,
      "duration": 4.74
    },
    {
      "lang": "en",
      "text": "of parameters to what layers number two",
      "offset": 2317.14,
      "duration": 5.64
    },
    {
      "lang": "en",
      "text": "we still may have a bug inside the",
      "offset": 2319.78,
      "duration": 4.92
    },
    {
      "lang": "en",
      "text": "bachelor 1D layer so let's take a look",
      "offset": 2322.78,
      "duration": 2.64
    },
    {
      "lang": "en",
      "text": "at",
      "offset": 2324.7,
      "duration": 4.86
    },
    {
      "lang": "en",
      "text": "um uh that because it runs but does it",
      "offset": 2325.42,
      "duration": 5.4
    },
    {
      "lang": "en",
      "text": "do the right thing",
      "offset": 2329.56,
      "duration": 4.2
    },
    {
      "lang": "en",
      "text": "so I pulled up the layer inspector sort",
      "offset": 2330.82,
      "duration": 4.74
    },
    {
      "lang": "en",
      "text": "of that we have here and printed out the",
      "offset": 2333.76,
      "duration": 3.42
    },
    {
      "lang": "en",
      "text": "shape along the way and currently it",
      "offset": 2335.56,
      "duration": 3.18
    },
    {
      "lang": "en",
      "text": "looks like the batch form is receiving",
      "offset": 2337.18,
      "duration": 5.88
    },
    {
      "lang": "en",
      "text": "an input that is 32 by 4 by 68 right and",
      "offset": 2338.74,
      "duration": 5.64
    },
    {
      "lang": "en",
      "text": "here on the right I have the current",
      "offset": 2343.06,
      "duration": 2.76
    },
    {
      "lang": "en",
      "text": "implementation of Bachelor that we have",
      "offset": 2344.38,
      "duration": 2.28
    },
    {
      "lang": "en",
      "text": "right now",
      "offset": 2345.82,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "now this bachelor assumed in the way we",
      "offset": 2346.66,
      "duration": 4.679
    },
    {
      "lang": "en",
      "text": "wrote it and at the time that X is",
      "offset": 2349.42,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "two-dimensional so it was n by D where n",
      "offset": 2351.339,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "was the batch size so that's why we only",
      "offset": 2355.18,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "reduced uh the mean and the variance",
      "offset": 2357.099,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "over the zeroth dimension but now X will",
      "offset": 2359.02,
      "duration": 4.5
    },
    {
      "lang": "en",
      "text": "basically become three-dimensional so",
      "offset": 2361.66,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "what's happening inside the bachelor",
      "offset": 2363.52,
      "duration": 2.94
    },
    {
      "lang": "en",
      "text": "right now and how come it's working at",
      "offset": 2364.54,
      "duration": 4.2
    },
    {
      "lang": "en",
      "text": "all and not giving any errors the reason",
      "offset": 2366.46,
      "duration": 3.54
    },
    {
      "lang": "en",
      "text": "for that is basically because everything",
      "offset": 2368.74,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "broadcasts properly but the bachelor is",
      "offset": 2370,
      "duration": 4.98
    },
    {
      "lang": "en",
      "text": "not doing what we need what we wanted to",
      "offset": 2372.82,
      "duration": 2.82
    },
    {
      "lang": "en",
      "text": "do",
      "offset": 2374.98,
      "duration": 2.7
    },
    {
      "lang": "en",
      "text": "so in particular let's basically think",
      "offset": 2375.64,
      "duration": 3.24
    },
    {
      "lang": "en",
      "text": "through what's happening inside the",
      "offset": 2377.68,
      "duration": 3.72
    },
    {
      "lang": "en",
      "text": "bathroom uh looking at what's what's do",
      "offset": 2378.88,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "What's Happening Here",
      "offset": 2381.4,
      "duration": 3.959
    },
    {
      "lang": "en",
      "text": "I have the code here",
      "offset": 2383.68,
      "duration": 4.26
    },
    {
      "lang": "en",
      "text": "so we're receiving an input of 32 by 4",
      "offset": 2385.359,
      "duration": 6.901
    },
    {
      "lang": "en",
      "text": "by 68 and then we are doing uh here x",
      "offset": 2387.94,
      "duration": 6.899
    },
    {
      "lang": "en",
      "text": "dot mean here I have e instead of X but",
      "offset": 2392.26,
      "duration": 5.16
    },
    {
      "lang": "en",
      "text": "we're doing the mean over zero and",
      "offset": 2394.839,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "that's actually giving us 1 by 4 by 68.",
      "offset": 2397.42,
      "duration": 3.9
    },
    {
      "lang": "en",
      "text": "so we're doing the mean only over the",
      "offset": 2399.64,
      "duration": 3.66
    },
    {
      "lang": "en",
      "text": "very first Dimension and it's giving us",
      "offset": 2401.32,
      "duration": 3.72
    },
    {
      "lang": "en",
      "text": "a mean and a variance that still",
      "offset": 2403.3,
      "duration": 4.5
    },
    {
      "lang": "en",
      "text": "maintain this Dimension here",
      "offset": 2405.04,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "so these means are only taking over 32",
      "offset": 2407.8,
      "duration": 4.74
    },
    {
      "lang": "en",
      "text": "numbers in the First Dimension and then",
      "offset": 2410.32,
      "duration": 4.259
    },
    {
      "lang": "en",
      "text": "when we perform this everything",
      "offset": 2412.54,
      "duration": 4.44
    },
    {
      "lang": "en",
      "text": "broadcasts correctly still",
      "offset": 2414.579,
      "duration": 6.121
    },
    {
      "lang": "en",
      "text": "but basically what ends up happening is",
      "offset": 2416.98,
      "duration": 7.34
    },
    {
      "lang": "en",
      "text": "when we also look at the running mean",
      "offset": 2420.7,
      "duration": 3.62
    },
    {
      "lang": "en",
      "text": "the shape of it so I'm looking at the",
      "offset": 2426.16,
      "duration": 2.82
    },
    {
      "lang": "en",
      "text": "model that layers at three which is the",
      "offset": 2427.66,
      "duration": 2.76
    },
    {
      "lang": "en",
      "text": "first bathroom layer and they're looking",
      "offset": 2428.98,
      "duration": 3.78
    },
    {
      "lang": "en",
      "text": "at whatever the running mean became and",
      "offset": 2430.42,
      "duration": 3.72
    },
    {
      "lang": "en",
      "text": "its shape",
      "offset": 2432.76,
      "duration": 3.18
    },
    {
      "lang": "en",
      "text": "the shape of this running mean now is 1",
      "offset": 2434.14,
      "duration": 3.9
    },
    {
      "lang": "en",
      "text": "by 4 by 68.",
      "offset": 2435.94,
      "duration": 4.02
    },
    {
      "lang": "en",
      "text": "right instead of it being",
      "offset": 2438.04,
      "duration": 5.16
    },
    {
      "lang": "en",
      "text": "um you know just a size of dimension",
      "offset": 2439.96,
      "duration": 5.34
    },
    {
      "lang": "en",
      "text": "because we have 68 channels we expect to",
      "offset": 2443.2,
      "duration": 4.68
    },
    {
      "lang": "en",
      "text": "have 68 means and variances that we're",
      "offset": 2445.3,
      "duration": 4.14
    },
    {
      "lang": "en",
      "text": "maintaining but actually we have an",
      "offset": 2447.88,
      "duration": 4.02
    },
    {
      "lang": "en",
      "text": "array of 4 by 68 and so basically what",
      "offset": 2449.44,
      "duration": 4.679
    },
    {
      "lang": "en",
      "text": "this is telling us is this bash Norm is",
      "offset": 2451.9,
      "duration": 3.42
    },
    {
      "lang": "en",
      "text": "only",
      "offset": 2454.119,
      "duration": 3.061
    },
    {
      "lang": "en",
      "text": "this bachelor is currently working in",
      "offset": 2455.32,
      "duration": 2.82
    },
    {
      "lang": "en",
      "text": "parallel",
      "offset": 2457.18,
      "duration": 3.62
    },
    {
      "lang": "en",
      "text": "over",
      "offset": 2458.14,
      "duration": 2.66
    },
    {
      "lang": "en",
      "text": "4 times 68 instead of just 68 channels",
      "offset": 2461.26,
      "duration": 7.2
    },
    {
      "lang": "en",
      "text": "so basically we are maintaining",
      "offset": 2466.24,
      "duration": 4.5
    },
    {
      "lang": "en",
      "text": "statistics for every one of these four",
      "offset": 2468.46,
      "duration": 5.159
    },
    {
      "lang": "en",
      "text": "positions individually and independently",
      "offset": 2470.74,
      "duration": 4.5
    },
    {
      "lang": "en",
      "text": "and instead what we want to do is we",
      "offset": 2473.619,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "want to treat this four as a batch",
      "offset": 2475.24,
      "duration": 4.2
    },
    {
      "lang": "en",
      "text": "Dimension just like the zeroth dimension",
      "offset": 2476.98,
      "duration": 5.639
    },
    {
      "lang": "en",
      "text": "so as far as the bachelor is concerned",
      "offset": 2479.44,
      "duration": 5.399
    },
    {
      "lang": "en",
      "text": "it doesn't want to average we don't want",
      "offset": 2482.619,
      "duration": 4.261
    },
    {
      "lang": "en",
      "text": "to average over 32 numbers we want to",
      "offset": 2484.839,
      "duration": 4.381
    },
    {
      "lang": "en",
      "text": "now average over 32 times four numbers",
      "offset": 2486.88,
      "duration": 4.38
    },
    {
      "lang": "en",
      "text": "for every single one of these 68",
      "offset": 2489.22,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "channels",
      "offset": 2491.26,
      "duration": 3.66
    },
    {
      "lang": "en",
      "text": "and uh so let me now",
      "offset": 2492.579,
      "duration": 4.381
    },
    {
      "lang": "en",
      "text": "remove this",
      "offset": 2494.92,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "it turns out that when you look at the",
      "offset": 2496.96,
      "duration": 5.1
    },
    {
      "lang": "en",
      "text": "documentation of torch.mean",
      "offset": 2498.52,
      "duration": 7.22
    },
    {
      "lang": "en",
      "text": "so let's go to torch.me",
      "offset": 2502.06,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "in one of its signatures when we specify",
      "offset": 2509.26,
      "duration": 3.9
    },
    {
      "lang": "en",
      "text": "the dimension",
      "offset": 2511.66,
      "duration": 3.179
    },
    {
      "lang": "en",
      "text": "we see that the dimension here is not",
      "offset": 2513.16,
      "duration": 3.72
    },
    {
      "lang": "en",
      "text": "just it can be in or it can also be a",
      "offset": 2514.839,
      "duration": 5.041
    },
    {
      "lang": "en",
      "text": "tuple of ins so we can reduce over",
      "offset": 2516.88,
      "duration": 5.219
    },
    {
      "lang": "en",
      "text": "multiple integers at the same time over",
      "offset": 2519.88,
      "duration": 4.199
    },
    {
      "lang": "en",
      "text": "multiple Dimensions at the same time so",
      "offset": 2522.099,
      "duration": 3.781
    },
    {
      "lang": "en",
      "text": "instead of just reducing over zero we",
      "offset": 2524.079,
      "duration": 4.441
    },
    {
      "lang": "en",
      "text": "can pass in a tuple 0 1.",
      "offset": 2525.88,
      "duration": 4.92
    },
    {
      "lang": "en",
      "text": "and here zero one as well and then",
      "offset": 2528.52,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "what's going to happen is the output of",
      "offset": 2530.8,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "course is going to be the same",
      "offset": 2532.359,
      "duration": 2.641
    },
    {
      "lang": "en",
      "text": "but now what's going to happen is",
      "offset": 2533.8,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "because we reduce over 0 and 1 if we",
      "offset": 2535,
      "duration": 5.16
    },
    {
      "lang": "en",
      "text": "look at immin.shape",
      "offset": 2537.64,
      "duration": 5.219
    },
    {
      "lang": "en",
      "text": "we see that now we've reduced we took",
      "offset": 2540.16,
      "duration": 5.1
    },
    {
      "lang": "en",
      "text": "the mean over both the zeroth and the",
      "offset": 2542.859,
      "duration": 3.961
    },
    {
      "lang": "en",
      "text": "First Dimension",
      "offset": 2545.26,
      "duration": 3.48
    },
    {
      "lang": "en",
      "text": "so we're just getting 68 numbers and a",
      "offset": 2546.82,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "bunch of spurious Dimensions here",
      "offset": 2548.74,
      "duration": 5.339
    },
    {
      "lang": "en",
      "text": "so now this becomes 1 by 1 by 68 and the",
      "offset": 2550.9,
      "duration": 4.86
    },
    {
      "lang": "en",
      "text": "running mean and the running variance",
      "offset": 2554.079,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "analogously will become one by one by",
      "offset": 2555.76,
      "duration": 3.96
    },
    {
      "lang": "en",
      "text": "68. so even though there are the",
      "offset": 2557.92,
      "duration": 3.9
    },
    {
      "lang": "en",
      "text": "spurious Dimensions uh the current the",
      "offset": 2559.72,
      "duration": 3.899
    },
    {
      "lang": "en",
      "text": "current the correct thing will happen in",
      "offset": 2561.82,
      "duration": 3.779
    },
    {
      "lang": "en",
      "text": "that we are only maintaining means and",
      "offset": 2563.619,
      "duration": 5.641
    },
    {
      "lang": "en",
      "text": "variances for 64 sorry for 68 channels",
      "offset": 2565.599,
      "duration": 5.341
    },
    {
      "lang": "en",
      "text": "and we're not calculating the mean",
      "offset": 2569.26,
      "duration": 5.16
    },
    {
      "lang": "en",
      "text": "variance across 32 times 4 dimensions so",
      "offset": 2570.94,
      "duration": 5.88
    },
    {
      "lang": "en",
      "text": "that's exactly what we want and let's",
      "offset": 2574.42,
      "duration": 4.14
    },
    {
      "lang": "en",
      "text": "change the implementation of bash term",
      "offset": 2576.82,
      "duration": 4.86
    },
    {
      "lang": "en",
      "text": "1D that we have so that it can take in",
      "offset": 2578.56,
      "duration": 4.38
    },
    {
      "lang": "en",
      "text": "two-dimensional or three-dimensional",
      "offset": 2581.68,
      "duration": 4.2
    },
    {
      "lang": "en",
      "text": "inputs and perform accordingly so at the",
      "offset": 2582.94,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "end of the day the fix is relatively",
      "offset": 2585.88,
      "duration": 3.3
    },
    {
      "lang": "en",
      "text": "straightforward basically the dimension",
      "offset": 2587.02,
      "duration": 5.46
    },
    {
      "lang": "en",
      "text": "we want to reduce over is either 0 or",
      "offset": 2589.18,
      "duration": 4.98
    },
    {
      "lang": "en",
      "text": "the Tuple zero and one depending on the",
      "offset": 2592.48,
      "duration": 4.44
    },
    {
      "lang": "en",
      "text": "dimensionality of X so if x dot and dim",
      "offset": 2594.16,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "is two so it's a two dimensional tensor",
      "offset": 2596.92,
      "duration": 3.899
    },
    {
      "lang": "en",
      "text": "then Dimension we want to reduce over is",
      "offset": 2598.96,
      "duration": 3.42
    },
    {
      "lang": "en",
      "text": "just the integer zero",
      "offset": 2600.819,
      "duration": 3.961
    },
    {
      "lang": "en",
      "text": "L if x dot ending is three so it's a",
      "offset": 2602.38,
      "duration": 4.26
    },
    {
      "lang": "en",
      "text": "three-dimensional tensor then the dims",
      "offset": 2604.78,
      "duration": 4.5
    },
    {
      "lang": "en",
      "text": "we're going to assume are zero and one",
      "offset": 2606.64,
      "duration": 5.219
    },
    {
      "lang": "en",
      "text": "that we want to reduce over and then",
      "offset": 2609.28,
      "duration": 4.5
    },
    {
      "lang": "en",
      "text": "here we just pass in dim",
      "offset": 2611.859,
      "duration": 3.24
    },
    {
      "lang": "en",
      "text": "and if the dimensionality of X is",
      "offset": 2613.78,
      "duration": 2.64
    },
    {
      "lang": "en",
      "text": "anything else we'll now get an error",
      "offset": 2615.099,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "which is good",
      "offset": 2616.42,
      "duration": 4.74
    },
    {
      "lang": "en",
      "text": "um so that should be the fix now I want",
      "offset": 2618.7,
      "duration": 4.02
    },
    {
      "lang": "en",
      "text": "to point out one more thing we're",
      "offset": 2621.16,
      "duration": 3.24
    },
    {
      "lang": "en",
      "text": "actually departing from the API of Pi",
      "offset": 2622.72,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "torch here a little bit because when you",
      "offset": 2624.4,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "come to batch room 1D and pytorch you",
      "offset": 2626.319,
      "duration": 4.26
    },
    {
      "lang": "en",
      "text": "can scroll down and you can see that the",
      "offset": 2628.96,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "input to this layer can either be n by C",
      "offset": 2630.579,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "where n is the batch size and C is the",
      "offset": 2633.28,
      "duration": 4.38
    },
    {
      "lang": "en",
      "text": "number of features or channels or it",
      "offset": 2635.38,
      "duration": 3.719
    },
    {
      "lang": "en",
      "text": "actually does accept three-dimensional",
      "offset": 2637.66,
      "duration": 4.02
    },
    {
      "lang": "en",
      "text": "inputs but it expects it to be n by C by",
      "offset": 2639.099,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "L",
      "offset": 2641.68,
      "duration": 3.06
    },
    {
      "lang": "en",
      "text": "where LSA like the sequence length or",
      "offset": 2642.7,
      "duration": 3.24
    },
    {
      "lang": "en",
      "text": "something like that",
      "offset": 2644.74,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "so um",
      "offset": 2645.94,
      "duration": 3.899
    },
    {
      "lang": "en",
      "text": "this is problem because you see how C is",
      "offset": 2647.74,
      "duration": 4.92
    },
    {
      "lang": "en",
      "text": "nested here in the middle and so when it",
      "offset": 2649.839,
      "duration": 4.98
    },
    {
      "lang": "en",
      "text": "gets three-dimensional inputs this bash",
      "offset": 2652.66,
      "duration": 4.679
    },
    {
      "lang": "en",
      "text": "term layer will reduce over zero and two",
      "offset": 2654.819,
      "duration": 5.581
    },
    {
      "lang": "en",
      "text": "instead of zero and one so it basically",
      "offset": 2657.339,
      "duration": 5.161
    },
    {
      "lang": "en",
      "text": "Pi torch batch number one D layer",
      "offset": 2660.4,
      "duration": 5.58
    },
    {
      "lang": "en",
      "text": "assumes that c will always be the First",
      "offset": 2662.5,
      "duration": 5.579
    },
    {
      "lang": "en",
      "text": "Dimension whereas we'll we assume here",
      "offset": 2665.98,
      "duration": 4.92
    },
    {
      "lang": "en",
      "text": "that c is the last Dimension and there",
      "offset": 2668.079,
      "duration": 3.961
    },
    {
      "lang": "en",
      "text": "are some number of batch Dimensions",
      "offset": 2670.9,
      "duration": 3.3
    },
    {
      "lang": "en",
      "text": "beforehand",
      "offset": 2672.04,
      "duration": 2.819
    },
    {
      "lang": "en",
      "text": "um",
      "offset": 2674.2,
      "duration": 2.46
    },
    {
      "lang": "en",
      "text": "and so",
      "offset": 2674.859,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "it expects n by C or M by C by all we",
      "offset": 2676.66,
      "duration": 6.12
    },
    {
      "lang": "en",
      "text": "expect and by C or n by L by C",
      "offset": 2679.42,
      "duration": 5.939
    },
    {
      "lang": "en",
      "text": "and so it's a deviation",
      "offset": 2682.78,
      "duration": 3.9
    },
    {
      "lang": "en",
      "text": "um",
      "offset": 2685.359,
      "duration": 3.661
    },
    {
      "lang": "en",
      "text": "I think it's okay I prefer it this way",
      "offset": 2686.68,
      "duration": 3.96
    },
    {
      "lang": "en",
      "text": "honestly so this is the way that we will",
      "offset": 2689.02,
      "duration": 3.059
    },
    {
      "lang": "en",
      "text": "keep it for our purposes",
      "offset": 2690.64,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "so I redefined the layers re-initialize",
      "offset": 2692.079,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "the neural net and did a single forward",
      "offset": 2694,
      "duration": 3.78
    },
    {
      "lang": "en",
      "text": "pass with a break just for one step",
      "offset": 2695.44,
      "duration": 4.5
    },
    {
      "lang": "en",
      "text": "looking at the shapes along the way",
      "offset": 2697.78,
      "duration": 3.72
    },
    {
      "lang": "en",
      "text": "they're of course identical all the",
      "offset": 2699.94,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "shapes are the same but the way we see",
      "offset": 2701.5,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "that things are actually working as we",
      "offset": 2703.3,
      "duration": 3.72
    },
    {
      "lang": "en",
      "text": "want them to now is that when we look at",
      "offset": 2705.099,
      "duration": 3.48
    },
    {
      "lang": "en",
      "text": "the bathroom layer the running mean",
      "offset": 2707.02,
      "duration": 4.14
    },
    {
      "lang": "en",
      "text": "shape is now one by one by 68. so we're",
      "offset": 2708.579,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "only maintaining 68 means for every one",
      "offset": 2711.16,
      "duration": 4.5
    },
    {
      "lang": "en",
      "text": "of our channels and we're treating both",
      "offset": 2713.619,
      "duration": 4.2
    },
    {
      "lang": "en",
      "text": "the zeroth and the First Dimension as a",
      "offset": 2715.66,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "batch Dimension which is exactly what we",
      "offset": 2717.819,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "want so let me retrain the neural lot",
      "offset": 2719.74,
      "duration": 3.119
    },
    {
      "lang": "en",
      "text": "now okay so I retrained the neural net",
      "offset": 2721.42,
      "duration": 3.72
    },
    {
      "lang": "en",
      "text": "with the bug fix we get a nice curve and",
      "offset": 2722.859,
      "duration": 3.121
    },
    {
      "lang": "en",
      "text": "when we look at the validation",
      "offset": 2725.14,
      "duration": 2.4
    },
    {
      "lang": "en",
      "text": "performance we do actually see a slight",
      "offset": 2725.98,
      "duration": 4.22
    },
    {
      "lang": "en",
      "text": "Improvement so we went from 2.029 to",
      "offset": 2727.54,
      "duration": 5.4
    },
    {
      "lang": "en",
      "text": "2.022 so basically the bug inside the",
      "offset": 2730.2,
      "duration": 5.08
    },
    {
      "lang": "en",
      "text": "bathroom was holding up us back like a",
      "offset": 2732.94,
      "duration": 4.62
    },
    {
      "lang": "en",
      "text": "little bit it looks like and we are",
      "offset": 2735.28,
      "duration": 3.78
    },
    {
      "lang": "en",
      "text": "getting a tiny Improvement now but it's",
      "offset": 2737.56,
      "duration": 2.519
    },
    {
      "lang": "en",
      "text": "not clear if this is statistical",
      "offset": 2739.06,
      "duration": 2.519
    },
    {
      "lang": "en",
      "text": "significant",
      "offset": 2740.079,
      "duration": 2.461
    },
    {
      "lang": "en",
      "text": "um",
      "offset": 2741.579,
      "duration": 2.941
    },
    {
      "lang": "en",
      "text": "and the reason we slightly expect an",
      "offset": 2742.54,
      "duration": 3.539
    },
    {
      "lang": "en",
      "text": "improvement is because we're not",
      "offset": 2744.52,
      "duration": 3.059
    },
    {
      "lang": "en",
      "text": "maintaining so many different means and",
      "offset": 2746.079,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "variances that are only estimated using",
      "offset": 2747.579,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "using 32 numbers effectively now we are",
      "offset": 2749.44,
      "duration": 5.22
    },
    {
      "lang": "en",
      "text": "estimating them using 32 times 4 numbers",
      "offset": 2752.38,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "so you just have a lot more numbers that",
      "offset": 2754.66,
      "duration": 3.9
    },
    {
      "lang": "en",
      "text": "go into any one estimate of the mean and",
      "offset": 2756.7,
      "duration": 4.74
    },
    {
      "lang": "en",
      "text": "variance and it allows things to be a",
      "offset": 2758.56,
      "duration": 4.62
    },
    {
      "lang": "en",
      "text": "bit more stable and less Wiggly inside",
      "offset": 2761.44,
      "duration": 5.58
    },
    {
      "lang": "en",
      "text": "those estimates of those statistics so",
      "offset": 2763.18,
      "duration": 5.34
    },
    {
      "lang": "en",
      "text": "pretty nice with this more General",
      "offset": 2767.02,
      "duration": 3.299
    },
    {
      "lang": "en",
      "text": "architecture in place we are now set up",
      "offset": 2768.52,
      "duration": 3.9
    },
    {
      "lang": "en",
      "text": "to push the performance further by",
      "offset": 2770.319,
      "duration": 4.141
    },
    {
      "lang": "en",
      "text": "increasing the size of the network so",
      "offset": 2772.42,
      "duration": 3.78
    },
    {
      "lang": "en",
      "text": "for example I bumped up the number of",
      "offset": 2774.46,
      "duration": 4.859
    },
    {
      "lang": "en",
      "text": "embeddings to 24 instead of 10 and also",
      "offset": 2776.2,
      "duration": 4.86
    },
    {
      "lang": "en",
      "text": "increased number of hidden units but",
      "offset": 2779.319,
      "duration": 3.721
    },
    {
      "lang": "en",
      "text": "using the exact same architecture we now",
      "offset": 2781.06,
      "duration": 4.92
    },
    {
      "lang": "en",
      "text": "have 76 000 parameters and the training",
      "offset": 2783.04,
      "duration": 5.279
    },
    {
      "lang": "en",
      "text": "takes a lot longer but we do get a nice",
      "offset": 2785.98,
      "duration": 3.72
    },
    {
      "lang": "en",
      "text": "curve and then when you actually",
      "offset": 2788.319,
      "duration": 3.121
    },
    {
      "lang": "en",
      "text": "evaluate the performance we are now",
      "offset": 2789.7,
      "duration": 3.78
    },
    {
      "lang": "en",
      "text": "getting validation performance of 1.993",
      "offset": 2791.44,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "so we've crossed over the 2.0 sort of",
      "offset": 2793.48,
      "duration": 6.3
    },
    {
      "lang": "en",
      "text": "territory and right about 1.99 but we",
      "offset": 2796.96,
      "duration": 5.22
    },
    {
      "lang": "en",
      "text": "are starting to have to wait quite a bit",
      "offset": 2799.78,
      "duration": 4.38
    },
    {
      "lang": "en",
      "text": "longer and we're a little bit in the",
      "offset": 2802.18,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "dark with respect to the correct setting",
      "offset": 2804.16,
      "duration": 3.179
    },
    {
      "lang": "en",
      "text": "of the hyper parameters here and the",
      "offset": 2806.02,
      "duration": 2.7
    },
    {
      "lang": "en",
      "text": "learning rates and so on because the",
      "offset": 2807.339,
      "duration": 2.76
    },
    {
      "lang": "en",
      "text": "experiments are starting to take longer",
      "offset": 2808.72,
      "duration": 3.48
    },
    {
      "lang": "en",
      "text": "to train and so we are missing sort of",
      "offset": 2810.099,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "like an experimental harness on which we",
      "offset": 2812.2,
      "duration": 4.44
    },
    {
      "lang": "en",
      "text": "could run a number of experiments and",
      "offset": 2814.66,
      "duration": 3.419
    },
    {
      "lang": "en",
      "text": "really tune this architecture very well",
      "offset": 2816.64,
      "duration": 3.24
    },
    {
      "lang": "en",
      "text": "so I'd like to conclude now with a few",
      "offset": 2818.079,
      "duration": 4.02
    },
    {
      "lang": "en",
      "text": "notes we basically improved our",
      "offset": 2819.88,
      "duration": 4.68
    },
    {
      "lang": "en",
      "text": "performance from a starting of 2.1 down",
      "offset": 2822.099,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "to 1.9 but I don't want that to be the",
      "offset": 2824.56,
      "duration": 4.019
    },
    {
      "lang": "en",
      "text": "focus because honestly we're kind of in",
      "offset": 2826.9,
      "duration": 3.3
    },
    {
      "lang": "en",
      "text": "the dark we have no experimental harness",
      "offset": 2828.579,
      "duration": 3.721
    },
    {
      "lang": "en",
      "text": "we're just guessing and checking and",
      "offset": 2830.2,
      "duration": 3.659
    },
    {
      "lang": "en",
      "text": "this whole thing is terrible we're just",
      "offset": 2832.3,
      "duration": 3.18
    },
    {
      "lang": "en",
      "text": "looking at the training loss normally",
      "offset": 2833.859,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "you want to look at both the training",
      "offset": 2835.48,
      "duration": 4.379
    },
    {
      "lang": "en",
      "text": "and the validation loss together and the",
      "offset": 2837.22,
      "duration": 3.72
    },
    {
      "lang": "en",
      "text": "whole thing looks different if you're",
      "offset": 2839.859,
      "duration": 3.48
    },
    {
      "lang": "en",
      "text": "actually trying to squeeze out numbers",
      "offset": 2840.94,
      "duration": 4.98
    },
    {
      "lang": "en",
      "text": "that said we did implement this",
      "offset": 2843.339,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "architecture from the wavenet paper but",
      "offset": 2845.92,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "we did not implement this specific uh",
      "offset": 2848.14,
      "duration": 5.16
    },
    {
      "lang": "en",
      "text": "forward pass of it where you have a more",
      "offset": 2851.44,
      "duration": 4.379
    },
    {
      "lang": "en",
      "text": "complicated a linear layer sort of that",
      "offset": 2853.3,
      "duration": 5.279
    },
    {
      "lang": "en",
      "text": "is this gated linear layer kind of and",
      "offset": 2855.819,
      "duration": 4.741
    },
    {
      "lang": "en",
      "text": "there's residual connections and Skip",
      "offset": 2858.579,
      "duration": 3.54
    },
    {
      "lang": "en",
      "text": "connections and so on so we did not",
      "offset": 2860.56,
      "duration": 3.48
    },
    {
      "lang": "en",
      "text": "Implement that we just implemented this",
      "offset": 2862.119,
      "duration": 4.381
    },
    {
      "lang": "en",
      "text": "structure I would like to briefly hint",
      "offset": 2864.04,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "or preview how what we've done here",
      "offset": 2866.5,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "relates to convolutional neural networks",
      "offset": 2868.359,
      "duration": 4.441
    },
    {
      "lang": "en",
      "text": "as used in the wavenet paper and",
      "offset": 2870.339,
      "duration": 3.961
    },
    {
      "lang": "en",
      "text": "basically the use of convolutions is",
      "offset": 2872.8,
      "duration": 3.539
    },
    {
      "lang": "en",
      "text": "strictly for efficiency it doesn't",
      "offset": 2874.3,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "actually change the model we've",
      "offset": 2876.339,
      "duration": 2.101
    },
    {
      "lang": "en",
      "text": "implemented",
      "offset": 2877.42,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "so here for example",
      "offset": 2878.44,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "let me look at a specific name to work",
      "offset": 2880.78,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "with an example so there's a name in our",
      "offset": 2882.76,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "training set and it's DeAndre and it has",
      "offset": 2885.339,
      "duration": 4.861
    },
    {
      "lang": "en",
      "text": "seven letters so that is eight",
      "offset": 2888.04,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "independent examples in our model so all",
      "offset": 2890.2,
      "duration": 4.5
    },
    {
      "lang": "en",
      "text": "these rows here are independent examples",
      "offset": 2892.839,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "of the Android",
      "offset": 2894.7,
      "duration": 4.02
    },
    {
      "lang": "en",
      "text": "now you can forward of course any one of",
      "offset": 2896.44,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "these rows independently so I can take",
      "offset": 2898.72,
      "duration": 5.82
    },
    {
      "lang": "en",
      "text": "my model and call call it on any",
      "offset": 2900.76,
      "duration": 5.94
    },
    {
      "lang": "en",
      "text": "individual index notice by the way here",
      "offset": 2904.54,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "I'm being a little bit tricky",
      "offset": 2906.7,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "the reason for this is that extra at",
      "offset": 2908.38,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "seven that shape is just",
      "offset": 2910.3,
      "duration": 6.12
    },
    {
      "lang": "en",
      "text": "um one dimensional array of eight so you",
      "offset": 2913.18,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "can't actually call the model on it",
      "offset": 2916.42,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "you're going to get an error because",
      "offset": 2917.98,
      "duration": 3.72
    },
    {
      "lang": "en",
      "text": "there's no batch dimension",
      "offset": 2919.78,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "so when you do extra at",
      "offset": 2921.7,
      "duration": 6.18
    },
    {
      "lang": "en",
      "text": "a list of seven then the shape of this",
      "offset": 2925.06,
      "duration": 4.68
    },
    {
      "lang": "en",
      "text": "becomes one by eight so I get an extra",
      "offset": 2927.88,
      "duration": 4.14
    },
    {
      "lang": "en",
      "text": "batch dimension of one and then we can",
      "offset": 2929.74,
      "duration": 3.72
    },
    {
      "lang": "en",
      "text": "forward the model",
      "offset": 2932.02,
      "duration": 3.18
    },
    {
      "lang": "en",
      "text": "so",
      "offset": 2933.46,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "that forwards a single example and you",
      "offset": 2935.2,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "might imagine that you actually may want",
      "offset": 2937.78,
      "duration": 3.9
    },
    {
      "lang": "en",
      "text": "to forward all of these eight",
      "offset": 2939.04,
      "duration": 4.2
    },
    {
      "lang": "en",
      "text": "um at the same time",
      "offset": 2941.68,
      "duration": 4.02
    },
    {
      "lang": "en",
      "text": "so pre-allocating some memory and then",
      "offset": 2943.24,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "doing a for Loop eight times and",
      "offset": 2945.7,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "forwarding all of those eight here will",
      "offset": 2947.56,
      "duration": 4.259
    },
    {
      "lang": "en",
      "text": "give us all the logits in all these",
      "offset": 2950.26,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "different cases",
      "offset": 2951.819,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "now for us with the model as we've",
      "offset": 2953.14,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "implemented it right now this is eight",
      "offset": 2954.819,
      "duration": 3.54
    },
    {
      "lang": "en",
      "text": "independent calls to our model",
      "offset": 2956.5,
      "duration": 4.26
    },
    {
      "lang": "en",
      "text": "but what convolutions allow you to do is",
      "offset": 2958.359,
      "duration": 4.381
    },
    {
      "lang": "en",
      "text": "it allow you to basically slide this",
      "offset": 2960.76,
      "duration": 4.02
    },
    {
      "lang": "en",
      "text": "model efficiently over the input",
      "offset": 2962.74,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "sequence and so this for Loop can be",
      "offset": 2964.78,
      "duration": 6.299
    },
    {
      "lang": "en",
      "text": "done not outside in Python but inside of",
      "offset": 2967.78,
      "duration": 5.94
    },
    {
      "lang": "en",
      "text": "kernels in Cuda and so this for Loop",
      "offset": 2971.079,
      "duration": 4.621
    },
    {
      "lang": "en",
      "text": "gets hidden into the convolution",
      "offset": 2973.72,
      "duration": 3.78
    },
    {
      "lang": "en",
      "text": "so the convolution basically you can",
      "offset": 2975.7,
      "duration": 4.44
    },
    {
      "lang": "en",
      "text": "cover this it's a for Loop applying a",
      "offset": 2977.5,
      "duration": 5.579
    },
    {
      "lang": "en",
      "text": "little linear filter over space of some",
      "offset": 2980.14,
      "duration": 5.16
    },
    {
      "lang": "en",
      "text": "input sequence and in our case the space",
      "offset": 2983.079,
      "duration": 3.481
    },
    {
      "lang": "en",
      "text": "we're interested in is one dimensional",
      "offset": 2985.3,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "and we're interested in sliding these",
      "offset": 2986.56,
      "duration": 4.519
    },
    {
      "lang": "en",
      "text": "filters over the input data",
      "offset": 2988.18,
      "duration": 6.179
    },
    {
      "lang": "en",
      "text": "so this diagram actually is fairly good",
      "offset": 2991.079,
      "duration": 4.361
    },
    {
      "lang": "en",
      "text": "as well",
      "offset": 2994.359,
      "duration": 2.821
    },
    {
      "lang": "en",
      "text": "basically what we've done is here they",
      "offset": 2995.44,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "are highlighting in Black one individ",
      "offset": 2997.18,
      "duration": 4.26
    },
    {
      "lang": "en",
      "text": "one single sort of like tree of this",
      "offset": 2999.28,
      "duration": 4.38
    },
    {
      "lang": "en",
      "text": "calculation so just calculating the",
      "offset": 3001.44,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "single output example here",
      "offset": 3003.66,
      "duration": 3.48
    },
    {
      "lang": "en",
      "text": "um",
      "offset": 3006,
      "duration": 2.819
    },
    {
      "lang": "en",
      "text": "and so this is basically what we've",
      "offset": 3007.14,
      "duration": 3.78
    },
    {
      "lang": "en",
      "text": "implemented here we've implemented a",
      "offset": 3008.819,
      "duration": 4.681
    },
    {
      "lang": "en",
      "text": "single this black structure we've",
      "offset": 3010.92,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "implemented that and calculated a single",
      "offset": 3013.5,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "output like a single example",
      "offset": 3015.48,
      "duration": 3.66
    },
    {
      "lang": "en",
      "text": "but what collusions allow you to do is",
      "offset": 3017.579,
      "duration": 3.24
    },
    {
      "lang": "en",
      "text": "it allows you to take this black",
      "offset": 3019.14,
      "duration": 4.439
    },
    {
      "lang": "en",
      "text": "structure and kind of like slide it over",
      "offset": 3020.819,
      "duration": 5.461
    },
    {
      "lang": "en",
      "text": "the input sequence here and calculate",
      "offset": 3023.579,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "all of these orange outputs at the same",
      "offset": 3026.28,
      "duration": 5.46
    },
    {
      "lang": "en",
      "text": "time or here that corresponds to",
      "offset": 3029.579,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "calculating all of these outputs of",
      "offset": 3031.74,
      "duration": 5.46
    },
    {
      "lang": "en",
      "text": "um at all the positions of DeAndre at",
      "offset": 3034.859,
      "duration": 3.72
    },
    {
      "lang": "en",
      "text": "the same time",
      "offset": 3037.2,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "and the reason that this is much more",
      "offset": 3038.579,
      "duration": 4.621
    },
    {
      "lang": "en",
      "text": "efficient is because number one as I",
      "offset": 3041.04,
      "duration": 3.96
    },
    {
      "lang": "en",
      "text": "mentioned the for Loop is inside the",
      "offset": 3043.2,
      "duration": 5.1
    },
    {
      "lang": "en",
      "text": "Cuda kernels in the sliding so that",
      "offset": 3045,
      "duration": 5.4
    },
    {
      "lang": "en",
      "text": "makes it efficient but number two notice",
      "offset": 3048.3,
      "duration": 4.14
    },
    {
      "lang": "en",
      "text": "the variable reuse here for example if",
      "offset": 3050.4,
      "duration": 3.659
    },
    {
      "lang": "en",
      "text": "we look at this circle this node here",
      "offset": 3052.44,
      "duration": 4.379
    },
    {
      "lang": "en",
      "text": "this node here is the right child of",
      "offset": 3054.059,
      "duration": 5.641
    },
    {
      "lang": "en",
      "text": "this node but is also the left child of",
      "offset": 3056.819,
      "duration": 4.26
    },
    {
      "lang": "en",
      "text": "the node here",
      "offset": 3059.7,
      "duration": 3.659
    },
    {
      "lang": "en",
      "text": "and so basically this node and its value",
      "offset": 3061.079,
      "duration": 4.441
    },
    {
      "lang": "en",
      "text": "is used twice",
      "offset": 3063.359,
      "duration": 5.22
    },
    {
      "lang": "en",
      "text": "and so right now in this naive way we'd",
      "offset": 3065.52,
      "duration": 5.64
    },
    {
      "lang": "en",
      "text": "have to recalculate it but here we are",
      "offset": 3068.579,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "allowed to reuse it",
      "offset": 3071.16,
      "duration": 2.939
    },
    {
      "lang": "en",
      "text": "so in the convolutional neural network",
      "offset": 3072.66,
      "duration": 3.54
    },
    {
      "lang": "en",
      "text": "you think of these linear layers that we",
      "offset": 3074.099,
      "duration": 5.22
    },
    {
      "lang": "en",
      "text": "have up above as filters and we take",
      "offset": 3076.2,
      "duration": 5.1
    },
    {
      "lang": "en",
      "text": "these filters and they're linear filters",
      "offset": 3079.319,
      "duration": 3.78
    },
    {
      "lang": "en",
      "text": "and you slide them over input sequence",
      "offset": 3081.3,
      "duration": 4.019
    },
    {
      "lang": "en",
      "text": "and we calculate the first layer and",
      "offset": 3083.099,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "then the second layer and then the third",
      "offset": 3085.319,
      "duration": 3.3
    },
    {
      "lang": "en",
      "text": "layer and then the output layer of the",
      "offset": 3086.94,
      "duration": 3.3
    },
    {
      "lang": "en",
      "text": "sandwich and it's all done very",
      "offset": 3088.619,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "efficiently using these convolutions",
      "offset": 3090.24,
      "duration": 3.9
    },
    {
      "lang": "en",
      "text": "so we're going to cover that in a future",
      "offset": 3092.7,
      "duration": 3.119
    },
    {
      "lang": "en",
      "text": "video the second thing I hope you took",
      "offset": 3094.14,
      "duration": 3.66
    },
    {
      "lang": "en",
      "text": "away from this video is you've seen me",
      "offset": 3095.819,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "basically Implement all of these layer",
      "offset": 3097.8,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "Lego building blocks or module building",
      "offset": 3100.38,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "blocks and I'm implementing them over",
      "offset": 3102.839,
      "duration": 3.961
    },
    {
      "lang": "en",
      "text": "here and we've implemented a number of",
      "offset": 3105.18,
      "duration": 3.24
    },
    {
      "lang": "en",
      "text": "layers together and we've also",
      "offset": 3106.8,
      "duration": 4.259
    },
    {
      "lang": "en",
      "text": "implemented these these containers and",
      "offset": 3108.42,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "we've overall pytorchified our code",
      "offset": 3111.059,
      "duration": 3.78
    },
    {
      "lang": "en",
      "text": "quite a bit more",
      "offset": 3113.46,
      "duration": 3.42
    },
    {
      "lang": "en",
      "text": "now basically what we're doing here is",
      "offset": 3114.839,
      "duration": 4.381
    },
    {
      "lang": "en",
      "text": "we're re-implementing torch.nn which is",
      "offset": 3116.88,
      "duration": 5.479
    },
    {
      "lang": "en",
      "text": "the neural networks library on top of",
      "offset": 3119.22,
      "duration": 5.399
    },
    {
      "lang": "en",
      "text": "torch.tensor and it looks very much like",
      "offset": 3122.359,
      "duration": 4.661
    },
    {
      "lang": "en",
      "text": "this except it is much better because",
      "offset": 3124.619,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "because it's in pi torch instead of",
      "offset": 3127.02,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "jingling my Jupiter notebook so I think",
      "offset": 3128.94,
      "duration": 4.379
    },
    {
      "lang": "en",
      "text": "going forward I will probably have",
      "offset": 3131.579,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "considered us having unlocked",
      "offset": 3133.319,
      "duration": 4.74
    },
    {
      "lang": "en",
      "text": "um torch.nn we understand roughly what's",
      "offset": 3135.66,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "in there how these modules work how",
      "offset": 3138.059,
      "duration": 3.901
    },
    {
      "lang": "en",
      "text": "they're nested and what they're doing on",
      "offset": 3139.74,
      "duration": 4.26
    },
    {
      "lang": "en",
      "text": "top of torture tensor so hopefully we'll",
      "offset": 3141.96,
      "duration": 4.02
    },
    {
      "lang": "en",
      "text": "just uh we'll just switch over and",
      "offset": 3144,
      "duration": 3.66
    },
    {
      "lang": "en",
      "text": "continue and start using torch.net",
      "offset": 3145.98,
      "duration": 3.42
    },
    {
      "lang": "en",
      "text": "directly the next thing I hope you got a",
      "offset": 3147.66,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "bit of a sense of is what the",
      "offset": 3149.4,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "development process of building deep",
      "offset": 3151.26,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "neural networks looks like which I think",
      "offset": 3153.24,
      "duration": 3.54
    },
    {
      "lang": "en",
      "text": "was relatively representative to some",
      "offset": 3155.099,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "extent so number one we are spending a",
      "offset": 3156.78,
      "duration": 4.74
    },
    {
      "lang": "en",
      "text": "lot of time in the documentation page of",
      "offset": 3159.66,
      "duration": 4.38
    },
    {
      "lang": "en",
      "text": "pytorch and we're reading through all",
      "offset": 3161.52,
      "duration": 3.98
    },
    {
      "lang": "en",
      "text": "the layers looking at documentations",
      "offset": 3164.04,
      "duration": 4.2
    },
    {
      "lang": "en",
      "text": "where the shapes of the inputs what can",
      "offset": 3165.5,
      "duration": 5.819
    },
    {
      "lang": "en",
      "text": "they be what does the layer do and so on",
      "offset": 3168.24,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "unfortunately I have to say the",
      "offset": 3171.319,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "patreon's documentation is not are very",
      "offset": 3173.04,
      "duration": 4.92
    },
    {
      "lang": "en",
      "text": "good they spend a ton of time on",
      "offset": 3175.559,
      "duration": 4.02
    },
    {
      "lang": "en",
      "text": "Hardcore engineering of all kinds of",
      "offset": 3177.96,
      "duration": 3.72
    },
    {
      "lang": "en",
      "text": "distributed Primitives Etc but as far as",
      "offset": 3179.579,
      "duration": 3.78
    },
    {
      "lang": "en",
      "text": "I can tell no one is maintaining any",
      "offset": 3181.68,
      "duration": 4.74
    },
    {
      "lang": "en",
      "text": "documentation it will lie to you it will",
      "offset": 3183.359,
      "duration": 5.581
    },
    {
      "lang": "en",
      "text": "be wrong it will be incomplete it will",
      "offset": 3186.42,
      "duration": 5.58
    },
    {
      "lang": "en",
      "text": "be unclear so unfortunately it is what",
      "offset": 3188.94,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "it is and you just kind of do your best",
      "offset": 3192,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "um with what they've given us",
      "offset": 3194.7,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "um number two",
      "offset": 3198,
      "duration": 4.38
    },
    {
      "lang": "en",
      "text": "uh the other thing that I hope you got a",
      "offset": 3200.7,
      "duration": 3.899
    },
    {
      "lang": "en",
      "text": "sense of is there's a ton of trying to",
      "offset": 3202.38,
      "duration": 4.14
    },
    {
      "lang": "en",
      "text": "make the shapes work and there's a lot",
      "offset": 3204.599,
      "duration": 3.061
    },
    {
      "lang": "en",
      "text": "of gymnastics around these",
      "offset": 3206.52,
      "duration": 2.7
    },
    {
      "lang": "en",
      "text": "multi-dimensional arrays and are they",
      "offset": 3207.66,
      "duration": 2.76
    },
    {
      "lang": "en",
      "text": "two-dimensional three-dimensional",
      "offset": 3209.22,
      "duration": 3.3
    },
    {
      "lang": "en",
      "text": "four-dimensional uh what layers take",
      "offset": 3210.42,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "what shapes is it NCL or NLC and you're",
      "offset": 3212.52,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "promoting and viewing and it just can",
      "offset": 3216.18,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "get pretty messy and so that brings me",
      "offset": 3219,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "to number three I very often prototype",
      "offset": 3220.98,
      "duration": 3.96
    },
    {
      "lang": "en",
      "text": "these layers and implementations in",
      "offset": 3223.559,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "jupyter notebooks and make sure that all",
      "offset": 3224.94,
      "duration": 3.78
    },
    {
      "lang": "en",
      "text": "the shapes work out and I'm spending a",
      "offset": 3226.559,
      "duration": 4.26
    },
    {
      "lang": "en",
      "text": "lot of time basically babysitting the",
      "offset": 3228.72,
      "duration": 3.42
    },
    {
      "lang": "en",
      "text": "shapes and making sure everything is",
      "offset": 3230.819,
      "duration": 3.481
    },
    {
      "lang": "en",
      "text": "correct and then once I'm satisfied with",
      "offset": 3232.14,
      "duration": 3.24
    },
    {
      "lang": "en",
      "text": "the functionality in the Jupiter",
      "offset": 3234.3,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "notebook I will take that code and copy",
      "offset": 3235.38,
      "duration": 4.199
    },
    {
      "lang": "en",
      "text": "paste it into my repository of actual",
      "offset": 3237.3,
      "duration": 5.1
    },
    {
      "lang": "en",
      "text": "code that I'm training with and so then",
      "offset": 3239.579,
      "duration": 4.98
    },
    {
      "lang": "en",
      "text": "I'm working with vs code on the side so",
      "offset": 3242.4,
      "duration": 3.659
    },
    {
      "lang": "en",
      "text": "I usually have jupyter notebook and vs",
      "offset": 3244.559,
      "duration": 3.3
    },
    {
      "lang": "en",
      "text": "code I develop in Jupiter notebook I",
      "offset": 3246.059,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "paste into vs code and then I kick off",
      "offset": 3247.859,
      "duration": 3.781
    },
    {
      "lang": "en",
      "text": "experiments from from the reaper of",
      "offset": 3249.66,
      "duration": 4.919
    },
    {
      "lang": "en",
      "text": "course from the code repository so",
      "offset": 3251.64,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "that's roughly some notes on the",
      "offset": 3254.579,
      "duration": 2.881
    },
    {
      "lang": "en",
      "text": "development process of working with",
      "offset": 3256.2,
      "duration": 3.18
    },
    {
      "lang": "en",
      "text": "neurons lastly I think this lecture",
      "offset": 3257.46,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "unlocks a lot of potential further",
      "offset": 3259.38,
      "duration": 4.26
    },
    {
      "lang": "en",
      "text": "lectures because number one we have to",
      "offset": 3261.54,
      "duration": 3.539
    },
    {
      "lang": "en",
      "text": "convert our neural network to actually",
      "offset": 3263.64,
      "duration": 3.479
    },
    {
      "lang": "en",
      "text": "use these dilated causal convolutional",
      "offset": 3265.079,
      "duration": 5.701
    },
    {
      "lang": "en",
      "text": "layers so implementing the comnet number",
      "offset": 3267.119,
      "duration": 5.661
    },
    {
      "lang": "en",
      "text": "two potentially starting to get into",
      "offset": 3270.78,
      "duration": 4.14
    },
    {
      "lang": "en",
      "text": "what this means whatever residual",
      "offset": 3272.78,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "connections and Skip connections and why",
      "offset": 3274.92,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "are they useful",
      "offset": 3276.3,
      "duration": 4.5
    },
    {
      "lang": "en",
      "text": "number three we as I mentioned we don't",
      "offset": 3277.92,
      "duration": 4.98
    },
    {
      "lang": "en",
      "text": "have any experimental harness so right",
      "offset": 3280.8,
      "duration": 3.24
    },
    {
      "lang": "en",
      "text": "now I'm just guessing checking",
      "offset": 3282.9,
      "duration": 2.82
    },
    {
      "lang": "en",
      "text": "everything this is not representative of",
      "offset": 3284.04,
      "duration": 3.66
    },
    {
      "lang": "en",
      "text": "typical deep learning workflows you have",
      "offset": 3285.72,
      "duration": 4.2
    },
    {
      "lang": "en",
      "text": "to set up your evaluation harness you",
      "offset": 3287.7,
      "duration": 3.96
    },
    {
      "lang": "en",
      "text": "can kick off experiments you have lots",
      "offset": 3289.92,
      "duration": 3.179
    },
    {
      "lang": "en",
      "text": "of arguments that your script can take",
      "offset": 3291.66,
      "duration": 3.179
    },
    {
      "lang": "en",
      "text": "you're you're kicking off a lot of",
      "offset": 3293.099,
      "duration": 3.24
    },
    {
      "lang": "en",
      "text": "experimentation you're looking at a lot",
      "offset": 3294.839,
      "duration": 2.881
    },
    {
      "lang": "en",
      "text": "of plots of training and validation",
      "offset": 3296.339,
      "duration": 3.301
    },
    {
      "lang": "en",
      "text": "losses and you're looking at what is",
      "offset": 3297.72,
      "duration": 3.48
    },
    {
      "lang": "en",
      "text": "working and what is not working and",
      "offset": 3299.64,
      "duration": 2.699
    },
    {
      "lang": "en",
      "text": "you're working on this like population",
      "offset": 3301.2,
      "duration": 3.119
    },
    {
      "lang": "en",
      "text": "level and you're doing all these hyper",
      "offset": 3302.339,
      "duration": 4.02
    },
    {
      "lang": "en",
      "text": "parameter searches and so we've done",
      "offset": 3304.319,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "none of that so far so how to set that",
      "offset": 3306.359,
      "duration": 5.581
    },
    {
      "lang": "en",
      "text": "up and how to make it good I think as a",
      "offset": 3309.599,
      "duration": 5.281
    },
    {
      "lang": "en",
      "text": "whole another topic number three we",
      "offset": 3311.94,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "should probably cover recurring neural",
      "offset": 3314.88,
      "duration": 4.199
    },
    {
      "lang": "en",
      "text": "networks RNs lstm's grooves and of",
      "offset": 3316.26,
      "duration": 6.42
    },
    {
      "lang": "en",
      "text": "course Transformers so many uh places to",
      "offset": 3319.079,
      "duration": 5.821
    },
    {
      "lang": "en",
      "text": "go and we'll cover that in the future",
      "offset": 3322.68,
      "duration": 5.159
    },
    {
      "lang": "en",
      "text": "for now bye sorry I forgot to say that",
      "offset": 3324.9,
      "duration": 5.219
    },
    {
      "lang": "en",
      "text": "if you are interested I think it is kind",
      "offset": 3327.839,
      "duration": 3.78
    },
    {
      "lang": "en",
      "text": "of interesting to try to beat this",
      "offset": 3330.119,
      "duration": 4.2
    },
    {
      "lang": "en",
      "text": "number 1.993 because I really haven't",
      "offset": 3331.619,
      "duration": 4.621
    },
    {
      "lang": "en",
      "text": "tried a lot of experimentation here and",
      "offset": 3334.319,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "there's quite a bit of fruit potentially",
      "offset": 3336.24,
      "duration": 4.379
    },
    {
      "lang": "en",
      "text": "to still purchase further so I haven't",
      "offset": 3337.92,
      "duration": 4.98
    },
    {
      "lang": "en",
      "text": "tried any other ways of allocating these",
      "offset": 3340.619,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "channels in this neural net maybe the",
      "offset": 3342.9,
      "duration": 4.439
    },
    {
      "lang": "en",
      "text": "number of dimensions for the embedding",
      "offset": 3344.94,
      "duration": 4.619
    },
    {
      "lang": "en",
      "text": "is all wrong maybe it's possible to",
      "offset": 3347.339,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "actually take the original network with",
      "offset": 3349.559,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "just one hidden layer and make it big",
      "offset": 3350.94,
      "duration": 3.72
    },
    {
      "lang": "en",
      "text": "enough and actually beat my fancy",
      "offset": 3353.16,
      "duration": 3.74
    },
    {
      "lang": "en",
      "text": "hierarchical Network it's not obvious",
      "offset": 3354.66,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "that would be kind of embarrassing if",
      "offset": 3356.9,
      "duration": 4.419
    },
    {
      "lang": "en",
      "text": "this did not do better even once you",
      "offset": 3359.46,
      "duration": 4.02
    },
    {
      "lang": "en",
      "text": "torture it a little bit maybe you can",
      "offset": 3361.319,
      "duration": 3.421
    },
    {
      "lang": "en",
      "text": "read the weight net paper and try to",
      "offset": 3363.48,
      "duration": 2.579
    },
    {
      "lang": "en",
      "text": "figure out how some of these layers work",
      "offset": 3364.74,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "and Implement them yourselves using what",
      "offset": 3366.059,
      "duration": 2.821
    },
    {
      "lang": "en",
      "text": "we have",
      "offset": 3367.74,
      "duration": 2.879
    },
    {
      "lang": "en",
      "text": "and of course you can always tune some",
      "offset": 3368.88,
      "duration": 3.719
    },
    {
      "lang": "en",
      "text": "of the initialization or some of the",
      "offset": 3370.619,
      "duration": 4.44
    },
    {
      "lang": "en",
      "text": "optimization and see if you can improve",
      "offset": 3372.599,
      "duration": 4.26
    },
    {
      "lang": "en",
      "text": "it that way so I'd be curious if people",
      "offset": 3375.059,
      "duration": 3.8
    },
    {
      "lang": "en",
      "text": "can come up with some ways to beat this",
      "offset": 3376.859,
      "duration": 6.021
    },
    {
      "lang": "en",
      "text": "and yeah that's it for now bye",
      "offset": 3378.859,
      "duration": 4.021
    }
  ],
  "cleanText": null,
  "dumpedAt": "2025-07-21T18:43:26.424Z"
}