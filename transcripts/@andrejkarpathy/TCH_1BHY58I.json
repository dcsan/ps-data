{
  "episodeId": "TCH_1BHY58I",
  "channelSlug": "@andrejkarpathy",
  "title": "Building makemore Part 2: MLP",
  "publishedAt": "2022-09-12T14:43:06.000Z",
  "rawLines": [
    {
      "lang": "en",
      "text": "hi everyone",
      "offset": 0.64,
      "duration": 2.64
    },
    {
      "lang": "en",
      "text": "today we are continuing our",
      "offset": 1.92,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "implementation of makemore",
      "offset": 3.28,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "now in the last lecture we implemented",
      "offset": 5.2,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "the bigram language model and we",
      "offset": 6.879,
      "duration": 4.001
    },
    {
      "lang": "en",
      "text": "implemented it both using counts and",
      "offset": 8.559,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "also using a super simple neural network",
      "offset": 10.88,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "that had a single linear layer",
      "offset": 12.88,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "now this is the",
      "offset": 15.599,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "jupyter notebook that we built out last",
      "offset": 17.52,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "lecture",
      "offset": 19.279,
      "duration": 2.401
    },
    {
      "lang": "en",
      "text": "and we saw that the way we approached",
      "offset": 20.4,
      "duration": 2.879
    },
    {
      "lang": "en",
      "text": "this is that we looked at only the",
      "offset": 21.68,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "single previous character and we",
      "offset": 23.279,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "predicted the distribution for the",
      "offset": 24.88,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "character that would go next in the",
      "offset": 26.48,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "sequence and we did that by taking",
      "offset": 28.24,
      "duration": 3.999
    },
    {
      "lang": "en",
      "text": "counts and normalizing them into",
      "offset": 30.32,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "probabilities",
      "offset": 32.239,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "so that each row here sums to one",
      "offset": 33.68,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "now this is all well and good if you",
      "offset": 36.719,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "only have one character of previous",
      "offset": 38.559,
      "duration": 2.961
    },
    {
      "lang": "en",
      "text": "context",
      "offset": 40.399,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "and this works and it's approachable the",
      "offset": 41.52,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "problem with this model of course is",
      "offset": 43.84,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "that",
      "offset": 45.28,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "the predictions from this model are not",
      "offset": 46.8,
      "duration": 3.279
    },
    {
      "lang": "en",
      "text": "very good because you only take one",
      "offset": 48.32,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "character of context so the model didn't",
      "offset": 50.079,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "produce very name like sounding things",
      "offset": 52.8,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "now the problem with this approach",
      "offset": 56.079,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "though is that if we are to take more",
      "offset": 57.92,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "context into account when predicting the",
      "offset": 59.92,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "next character in a sequence things",
      "offset": 61.84,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "quickly blow up and this table the size",
      "offset": 63.68,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "of this table grows and in fact it grows",
      "offset": 66.08,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "exponentially with the length of the",
      "offset": 68.32,
      "duration": 2.799
    },
    {
      "lang": "en",
      "text": "context",
      "offset": 70,
      "duration": 2.24
    },
    {
      "lang": "en",
      "text": "because if we only take a single",
      "offset": 71.119,
      "duration": 2.801
    },
    {
      "lang": "en",
      "text": "character at a time that's 27",
      "offset": 72.24,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "possibilities of context",
      "offset": 73.92,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "but if we take two characters in the",
      "offset": 75.84,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "past and try to predict the third one",
      "offset": 77.759,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "suddenly the number of rows in this",
      "offset": 79.759,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "matrix you can look at it that way",
      "offset": 81.6,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "is 27 times 27 so there's 729",
      "offset": 83.759,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "possibilities for what could have come",
      "offset": 86.64,
      "duration": 3.519
    },
    {
      "lang": "en",
      "text": "in the context",
      "offset": 88.72,
      "duration": 3.039
    },
    {
      "lang": "en",
      "text": "if we take three characters as the",
      "offset": 90.159,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "context suddenly we have",
      "offset": 91.759,
      "duration": 5.761
    },
    {
      "lang": "en",
      "text": "20 000 possibilities of context",
      "offset": 94.4,
      "duration": 5.679
    },
    {
      "lang": "en",
      "text": "and so there's just way too many rows of",
      "offset": 97.52,
      "duration": 6.239
    },
    {
      "lang": "en",
      "text": "this matrix it's way too few counts",
      "offset": 100.079,
      "duration": 5.441
    },
    {
      "lang": "en",
      "text": "for each possibility and the whole thing",
      "offset": 103.759,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "just kind of explodes and doesn't work",
      "offset": 105.52,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "very well",
      "offset": 107.36,
      "duration": 3.119
    },
    {
      "lang": "en",
      "text": "so that's why today we're going to move",
      "offset": 108.96,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "on to this bullet point here and we're",
      "offset": 110.479,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "going to implement a multi-layer",
      "offset": 112.399,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "perceptron model to predict the next uh",
      "offset": 113.759,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "character in a sequence",
      "offset": 116.72,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "and this modeling approach that we're",
      "offset": 118.24,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "going to adopt follows this paper",
      "offset": 120.079,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "benguetal 2003",
      "offset": 121.84,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "so i have the paper pulled up here",
      "offset": 124.32,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "now this isn't the very first paper that",
      "offset": 126.399,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "proposed the use of multiglio",
      "offset": 128.08,
      "duration": 3.519
    },
    {
      "lang": "en",
      "text": "perceptrons or neural networks to",
      "offset": 130,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "predict the next character or token in a",
      "offset": 131.599,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "sequence but it's definitely one that is",
      "offset": 133.68,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "uh was very influential around that time",
      "offset": 135.84,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "it is very often cited to stand in for",
      "offset": 138,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "this idea and i think it's a very nice",
      "offset": 140,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "write-up and so this is the paper that",
      "offset": 142,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "we're going to first look at and then",
      "offset": 143.76,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "implement now this paper has 19 pages so",
      "offset": 145.84,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "we don't have time to go into",
      "offset": 148.4,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "the full detail of this paper but i",
      "offset": 150.16,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "invite you to read it",
      "offset": 151.68,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "it's very readable interesting and has a",
      "offset": 153.2,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "lot of interesting ideas in it as well",
      "offset": 154.64,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "in the introduction they describe the",
      "offset": 157.04,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "exact same problem i just described and",
      "offset": 158.4,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "then to address it they propose the",
      "offset": 160.56,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "following model",
      "offset": 162.319,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "now keep in mind that we are building a",
      "offset": 163.92,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "character level language model so we're",
      "offset": 166.56,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "working on the level of characters in",
      "offset": 168.16,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "this paper they have a vocabulary of 17",
      "offset": 170.08,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "000 possible words and they instead",
      "offset": 172.64,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "build a word level language model but",
      "offset": 174.8,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "we're going to still stick with the",
      "offset": 176.959,
      "duration": 2.481
    },
    {
      "lang": "en",
      "text": "characters but we'll take the same",
      "offset": 178.08,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "modeling approach",
      "offset": 179.44,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "now what they do is basically they",
      "offset": 181.28,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "propose to take every one of these words",
      "offset": 183.519,
      "duration": 3.521
    },
    {
      "lang": "en",
      "text": "seventeen thousand words and they're",
      "offset": 185.28,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "going to associate to each word a say",
      "offset": 187.04,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "thirty dimensional feature vector",
      "offset": 190.08,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "so every word is now",
      "offset": 192.72,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "embedded into a thirty dimensional space",
      "offset": 195.12,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "you can think of it that way so we have",
      "offset": 197.68,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "17 000 points or vectors in a 30",
      "offset": 199.84,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "dimensional space",
      "offset": 202.48,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "and that's um you might imagine that's",
      "offset": 203.84,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "very crowded that's a lot of points for",
      "offset": 205.76,
      "duration": 3.119
    },
    {
      "lang": "en",
      "text": "a very small space",
      "offset": 207.2,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "now",
      "offset": 208.879,
      "duration": 2.321
    },
    {
      "lang": "en",
      "text": "in the beginning these words are",
      "offset": 210.08,
      "duration": 2.56
    },
    {
      "lang": "en",
      "text": "initialized completely randomly so",
      "offset": 211.2,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "they're spread out at random",
      "offset": 212.64,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "but then we're going to tune these",
      "offset": 214.72,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "embeddings of these words using back",
      "offset": 216.56,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "propagation",
      "offset": 218.56,
      "duration": 2.64
    },
    {
      "lang": "en",
      "text": "so during the course of training of this",
      "offset": 219.68,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "neural network these points or vectors",
      "offset": 221.2,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "are going to basically move around in",
      "offset": 223.2,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "this space and you might imagine that",
      "offset": 224.72,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "for example words that have very similar",
      "offset": 226.64,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "meanings or that are indeed synonyms of",
      "offset": 228.879,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "each other might end up in a very",
      "offset": 230.959,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "similar part of the space and conversely",
      "offset": 232.48,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "words that mean very different things",
      "offset": 234.799,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "would go somewhere else in a space",
      "offset": 236.4,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "now their modeling approach otherwise is",
      "offset": 239.28,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "identical to ours they are using a",
      "offset": 241.2,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "multi-layer neural network to predict",
      "offset": 243.36,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "the next word given the previous words",
      "offset": 244.799,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "and to train the neural network they are",
      "offset": 247.28,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "maximizing the log likelihood of the",
      "offset": 248.959,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "training data just like we did",
      "offset": 250.64,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "so the modeling approach itself is",
      "offset": 252.799,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "identical now here they have a concrete",
      "offset": 254.72,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "example of this intuition",
      "offset": 256.72,
      "duration": 3.519
    },
    {
      "lang": "en",
      "text": "why does it work",
      "offset": 258.799,
      "duration": 3.041
    },
    {
      "lang": "en",
      "text": "basically suppose that for example you",
      "offset": 260.239,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "are trying to predict a dog was running",
      "offset": 261.84,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "in a blank",
      "offset": 263.759,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "now suppose that the exact phrase a dog",
      "offset": 265.6,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "was running in a",
      "offset": 268.08,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "has never occurred in a training data",
      "offset": 269.6,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "and here you are at sort of test time",
      "offset": 271.759,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "later when the model is deployed",
      "offset": 273.919,
      "duration": 2.401
    },
    {
      "lang": "en",
      "text": "somewhere",
      "offset": 275.36,
      "duration": 3.119
    },
    {
      "lang": "en",
      "text": "and it's trying to make a sentence and",
      "offset": 276.32,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "it's saying a dog was running in a blank",
      "offset": 278.479,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "and because it's never encountered this",
      "offset": 281.36,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "exact phrase in the training set you're",
      "offset": 283.04,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "out of distribution as we say like you",
      "offset": 285.36,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "don't have fundamentally any",
      "offset": 287.68,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "reason to suspect",
      "offset": 289.6,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "what might come next",
      "offset": 292.24,
      "duration": 3.519
    },
    {
      "lang": "en",
      "text": "but this approach actually allows you to",
      "offset": 294,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "get around that because maybe you didn't",
      "offset": 295.759,
      "duration": 3.521
    },
    {
      "lang": "en",
      "text": "see the exact phrase a dog was running",
      "offset": 297.52,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "in a something but maybe you've seen",
      "offset": 299.28,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "similar phrases maybe you've seen the",
      "offset": 301.36,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "phrase the dog was running in a blank",
      "offset": 303.04,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "and maybe your network has learned that",
      "offset": 306.16,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "a and the",
      "offset": 307.68,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "are like frequently are interchangeable",
      "offset": 308.96,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "with each other and so maybe it took the",
      "offset": 310.96,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "embedding for a and the embedding for",
      "offset": 313.199,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "the and it actually put them like nearby",
      "offset": 315.039,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "each other in the space and so you can",
      "offset": 317.039,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "transfer knowledge through that",
      "offset": 318.88,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "embedding and you can generalize in that",
      "offset": 320.639,
      "duration": 2.641
    },
    {
      "lang": "en",
      "text": "way",
      "offset": 322.479,
      "duration": 2.56
    },
    {
      "lang": "en",
      "text": "similarly the network could know that",
      "offset": 323.28,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "cats and dogs are animals and they",
      "offset": 325.039,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "co-occur in lots of very similar",
      "offset": 326.96,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "contexts and so even though you haven't",
      "offset": 328.639,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "seen this exact phrase",
      "offset": 330.72,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "or if you haven't seen exactly walking",
      "offset": 332.479,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "or running",
      "offset": 334.16,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "you can through the embedding space",
      "offset": 335.199,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "transfer knowledge and you can",
      "offset": 337.52,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "generalize to novel scenarios",
      "offset": 339.12,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "so let's now scroll down to the diagram",
      "offset": 342.08,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "of the neural network",
      "offset": 343.6,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "they have a nice diagram here",
      "offset": 344.96,
      "duration": 4.959
    },
    {
      "lang": "en",
      "text": "and in this example we are taking three",
      "offset": 347.36,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "previous words",
      "offset": 349.919,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "and we are trying to predict the fourth",
      "offset": 351.759,
      "duration": 2.241
    },
    {
      "lang": "en",
      "text": "word",
      "offset": 353.199,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "in a sequence",
      "offset": 354,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "now these three previous words as i",
      "offset": 355.919,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "mentioned uh we have a vocabulary of 17",
      "offset": 357.68,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "000 um possible words",
      "offset": 360.4,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "so every one of these",
      "offset": 362.88,
      "duration": 3.039
    },
    {
      "lang": "en",
      "text": "basically basically",
      "offset": 364.8,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "are the index of the incoming word",
      "offset": 365.919,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "and because there are 17 000 words this",
      "offset": 369.28,
      "duration": 7.56
    },
    {
      "lang": "en",
      "text": "is an integer between 0 and 16999",
      "offset": 371.759,
      "duration": 5.081
    },
    {
      "lang": "en",
      "text": "now there's also a lookup table that",
      "offset": 377.199,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "they call c",
      "offset": 379.36,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "this lookup table is a matrix that is 17",
      "offset": 380.88,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "000 by say 30.",
      "offset": 383.36,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "and basically what we're doing here is",
      "offset": 385.919,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "we're treating this as a lookup table",
      "offset": 387.28,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "and so every index is",
      "offset": 389.6,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "plucking out a row of this embedding",
      "offset": 391.68,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "matrix",
      "offset": 394.24,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "so that each index is converted to the",
      "offset": 395.28,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "30 dimensional vector that corresponds",
      "offset": 397.44,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "to the embedding vector for that word",
      "offset": 399.759,
      "duration": 5.841
    },
    {
      "lang": "en",
      "text": "so here we have the input layer of 30",
      "offset": 402.88,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "neurons for three words making up 90",
      "offset": 405.6,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "neurons in total",
      "offset": 408.8,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "and here they're saying that this matrix",
      "offset": 410.639,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "c is shared across all the words so",
      "offset": 412.16,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "we're always indexing into the same",
      "offset": 414.88,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "matrix c over and over um",
      "offset": 416.24,
      "duration": 4.6
    },
    {
      "lang": "en",
      "text": "for each one of these",
      "offset": 419.36,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "words next up is the hidden layer of",
      "offset": 420.84,
      "duration": 4.359
    },
    {
      "lang": "en",
      "text": "this neural network the size of this",
      "offset": 423.28,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "hidden neural layer of this neural net",
      "offset": 425.199,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "is a hoppy parameter so we use the word",
      "offset": 427.68,
      "duration": 3.519
    },
    {
      "lang": "en",
      "text": "hyperparameter when it's kind of like a",
      "offset": 429.84,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "design choice up to the designer of the",
      "offset": 431.199,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "neural net and this can be as large as",
      "offset": 432.8,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "you'd like or as small as you'd like so",
      "offset": 435.28,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "for example the size could be a hundred",
      "offset": 436.8,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "and we are going to go over multiple",
      "offset": 439.199,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "choices of the size of this hidden layer",
      "offset": 440.639,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "and we're going to evaluate how well",
      "offset": 443.199,
      "duration": 2.801
    },
    {
      "lang": "en",
      "text": "they work",
      "offset": 444.56,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "so say there were 100 neurons here all",
      "offset": 446,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "of them would be fully connected to the",
      "offset": 448.319,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "90 words or 90 um",
      "offset": 450,
      "duration": 5.759
    },
    {
      "lang": "en",
      "text": "numbers that make up these three words",
      "offset": 452.96,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "so this is a fully connected layer",
      "offset": 455.759,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "then there's a 10 inch long linearity",
      "offset": 457.84,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "and then there's this output layer and",
      "offset": 460.08,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "because there are 17 000 possible words",
      "offset": 462.08,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "that could come next",
      "offset": 464.4,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "this layer has 17 000 neurons",
      "offset": 465.84,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "and all of them are fully connected to",
      "offset": 468.639,
      "duration": 6.081
    },
    {
      "lang": "en",
      "text": "all of these neurons in the hidden layer",
      "offset": 471.28,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "so there's a lot of parameters here",
      "offset": 474.72,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "because there's a lot of words so most",
      "offset": 476.08,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "computation is here this is the",
      "offset": 478.56,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "expensive layer",
      "offset": 479.84,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "now there are 17 000 logits here so on",
      "offset": 481.68,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "top of there we have the softmax layer",
      "offset": 484.72,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "which we've seen in our previous video",
      "offset": 486.879,
      "duration": 3.961
    },
    {
      "lang": "en",
      "text": "as well so every one of these logits is",
      "offset": 488.479,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "exponentiated and then everything is",
      "offset": 490.84,
      "duration": 4.28
    },
    {
      "lang": "en",
      "text": "normalized to sum to 1 so that we have a",
      "offset": 492.72,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "nice probability distribution for the",
      "offset": 495.12,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "next word in the sequence",
      "offset": 497.28,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "now of course during training we",
      "offset": 499.84,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "actually have the label we have the",
      "offset": 501.44,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "identity of the next word in a sequence",
      "offset": 502.96,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "that word",
      "offset": 505.68,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "or its index is used to pluck out the",
      "offset": 506.96,
      "duration": 5.759
    },
    {
      "lang": "en",
      "text": "probability of that word",
      "offset": 510.72,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "and then we are maximizing the",
      "offset": 512.719,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "probability of that word",
      "offset": 514.8,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "with respect to the parameters of this",
      "offset": 517.039,
      "duration": 2.721
    },
    {
      "lang": "en",
      "text": "neural net",
      "offset": 518.64,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "so the parameters are the weights and",
      "offset": 519.76,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "biases of this output layer",
      "offset": 521.36,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "the weights and biases of the hidden",
      "offset": 524.08,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "layer and the embedding lookup table c",
      "offset": 525.839,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "and all of that is optimized using back",
      "offset": 529.12,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "propagation",
      "offset": 530.8,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "and these uh dashed arrows ignore those",
      "offset": 532.32,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "uh that represents a variation of a",
      "offset": 535.279,
      "duration": 3.041
    },
    {
      "lang": "en",
      "text": "neural nut that we are not going to",
      "offset": 537.04,
      "duration": 2.799
    },
    {
      "lang": "en",
      "text": "explore in this video",
      "offset": 538.32,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "so that's the setup and now let's",
      "offset": 539.839,
      "duration": 2.641
    },
    {
      "lang": "en",
      "text": "implement it",
      "offset": 541.2,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "okay so i started a brand new notebook",
      "offset": 542.48,
      "duration": 3.039
    },
    {
      "lang": "en",
      "text": "for this lecture",
      "offset": 544.16,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "we are importing pytorch and we are",
      "offset": 545.519,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "importing matplotlib so we can create",
      "offset": 547.76,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "figures",
      "offset": 549.44,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "then i am reading all the names into a",
      "offset": 550.72,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "list of words like i did before and i'm",
      "offset": 553.12,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "showing the first eight right here",
      "offset": 555.2,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "keep in mind that we have a 32 000 in",
      "offset": 558.08,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "total these are just the first eight",
      "offset": 560.32,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "and then here i'm building out the",
      "offset": 562.8,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "vocabulary of characters and all the",
      "offset": 564,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "mappings from the characters as strings",
      "offset": 565.68,
      "duration": 5.599
    },
    {
      "lang": "en",
      "text": "to integers and vice versa",
      "offset": 568.24,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "now the first thing we want to do is we",
      "offset": 571.279,
      "duration": 3.041
    },
    {
      "lang": "en",
      "text": "want to compile the data set for the",
      "offset": 572.72,
      "duration": 2.799
    },
    {
      "lang": "en",
      "text": "neural network",
      "offset": 574.32,
      "duration": 3.519
    },
    {
      "lang": "en",
      "text": "and i had to rewrite this code um i'll",
      "offset": 575.519,
      "duration": 5.601
    },
    {
      "lang": "en",
      "text": "show you in a second what it looks like",
      "offset": 577.839,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "so this is the code that i created for",
      "offset": 581.12,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "the dataset creation so let me first run",
      "offset": 583.04,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "it and then i'll briefly explain how",
      "offset": 584.959,
      "duration": 3.521
    },
    {
      "lang": "en",
      "text": "this works",
      "offset": 586.64,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "so first we're going to define something",
      "offset": 588.48,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "called block size and this is basically",
      "offset": 590.08,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "the context length of how many",
      "offset": 592.399,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "characters do we take to predict the",
      "offset": 594.16,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "next one so here in this example we're",
      "offset": 596,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "taking three characters to predict the",
      "offset": 598,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "fourth one so we have a block size of",
      "offset": 599.92,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "three that's the size of the block that",
      "offset": 602,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "supports the prediction",
      "offset": 604.32,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "then here i'm building out the x and y",
      "offset": 606.56,
      "duration": 5.839
    },
    {
      "lang": "en",
      "text": "the x are the input to the neural net",
      "offset": 609.92,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "and the y are the labels for each",
      "offset": 612.399,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "example inside x",
      "offset": 615.12,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "then i'm airing over the first five",
      "offset": 617.519,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "words i'm doing first five just for",
      "offset": 619.36,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "efficiency while we are developing all",
      "offset": 621.279,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "the code but then later we're going to",
      "offset": 623.12,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "come here and erase this so that we use",
      "offset": 624.88,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "the entire training set",
      "offset": 626.8,
      "duration": 4.039
    },
    {
      "lang": "en",
      "text": "so here i'm printing the word",
      "offset": 629.04,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "emma and here i'm basically showing the",
      "offset": 630.839,
      "duration": 5.161
    },
    {
      "lang": "en",
      "text": "examples that we can generate the five",
      "offset": 633.92,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "examples that we can generate out of the",
      "offset": 636,
      "duration": 2.399
    },
    {
      "lang": "en",
      "text": "single",
      "offset": 637.519,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "um sort of word emma",
      "offset": 638.399,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "so",
      "offset": 641.12,
      "duration": 2.959
    },
    {
      "lang": "en",
      "text": "when we are given the context of just uh",
      "offset": 642.079,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "dot dot the first character in a",
      "offset": 644.079,
      "duration": 3.521
    },
    {
      "lang": "en",
      "text": "sequence is e",
      "offset": 645.92,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "in this context the label is m",
      "offset": 647.6,
      "duration": 5.679
    },
    {
      "lang": "en",
      "text": "when the context is this the label is m",
      "offset": 650.32,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "and so forth and so the way i build this",
      "offset": 653.279,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "out is first i start with a padded",
      "offset": 655.36,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "context of just zero tokens",
      "offset": 656.959,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "then i iterate over all the characters i",
      "offset": 659.76,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "get the character in the sequence and i",
      "offset": 661.92,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "basically build out the array y of this",
      "offset": 664.8,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "current character and the array x which",
      "offset": 667.2,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "stores the current running context",
      "offset": 669.519,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "and then here see i print everything and",
      "offset": 671.839,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "here i um crop the context and enter the",
      "offset": 674.079,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "new character in a sequence so this is",
      "offset": 677.279,
      "duration": 5.361
    },
    {
      "lang": "en",
      "text": "kind of like a rolling window of context",
      "offset": 678.959,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "now we can change the block size here to",
      "offset": 682.64,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "for example four",
      "offset": 684.16,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "and in that case we'll be predicting the",
      "offset": 685.76,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "fifth character given the previous four",
      "offset": 687.839,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "or it can be five",
      "offset": 690.24,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "and then it would look like this",
      "offset": 691.76,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "or it can be say ten",
      "offset": 693.92,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "and then it would look something like",
      "offset": 696.56,
      "duration": 2.48
    },
    {
      "lang": "en",
      "text": "this we're taking ten characters to",
      "offset": 697.6,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "predict the eleventh one",
      "offset": 699.04,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "and we're always padding with dots",
      "offset": 700.959,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "so let me bring this back to three",
      "offset": 703.2,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "just so that we have what we have here",
      "offset": 705.519,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "in the paper",
      "offset": 707.92,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "and finally the data set right now looks",
      "offset": 710,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "as follows",
      "offset": 711.6,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "from these five words we have created a",
      "offset": 713.44,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "data set of 32 examples",
      "offset": 715.44,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "and each input of the neural net is",
      "offset": 717.92,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "three integers and we have a label that",
      "offset": 719.76,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "is also an integer",
      "offset": 722,
      "duration": 2.56
    },
    {
      "lang": "en",
      "text": "y",
      "offset": 723.68,
      "duration": 2.959
    },
    {
      "lang": "en",
      "text": "so x looks like this",
      "offset": 724.56,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "these are the individual examples",
      "offset": 726.639,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "and then y are the labels",
      "offset": 728.8,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "so",
      "offset": 732.399,
      "duration": 2.961
    },
    {
      "lang": "en",
      "text": "given this",
      "offset": 733.68,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "let's now write a neural network that",
      "offset": 735.36,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "takes these axes and predicts the y's",
      "offset": 737.04,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "first let's build the embedding lookup",
      "offset": 739.2,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "table c",
      "offset": 741.2,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "so we have 27 possible characters and",
      "offset": 743.12,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "we're going to embed them in a lower",
      "offset": 745.279,
      "duration": 3.041
    },
    {
      "lang": "en",
      "text": "dimensional space",
      "offset": 746.88,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "in the paper they have 17 000 words and",
      "offset": 748.32,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "they bet them in uh spaces as small",
      "offset": 751.36,
      "duration": 7.12
    },
    {
      "lang": "en",
      "text": "dimensional as 30. so they cram 17 000",
      "offset": 753.92,
      "duration": 6.8
    },
    {
      "lang": "en",
      "text": "words into 30 dimensional space in our",
      "offset": 758.48,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "case we have only 27 possible characters",
      "offset": 760.72,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "so let's grab them in something as small",
      "offset": 763.36,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "as to start with for example a",
      "offset": 765.2,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "two-dimensional space",
      "offset": 766.8,
      "duration": 3.279
    },
    {
      "lang": "en",
      "text": "so this lookup table will be random",
      "offset": 768.48,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "numbers",
      "offset": 770.079,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "and we'll have 27 rows and we'll have",
      "offset": 771.2,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "two columns",
      "offset": 774.16,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "right so each 20 each one of 27",
      "offset": 776.079,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "characters will have a two-dimensional",
      "offset": 778.48,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "embedding",
      "offset": 780.24,
      "duration": 5.279
    },
    {
      "lang": "en",
      "text": "so that's our matrix c of embeddings in",
      "offset": 782,
      "duration": 5.839
    },
    {
      "lang": "en",
      "text": "the beginning initialized randomly",
      "offset": 785.519,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "now before we embed all of the integers",
      "offset": 787.839,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "inside the input x using this lookup",
      "offset": 790.16,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "table c",
      "offset": 792.48,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "let me actually just try to embed a",
      "offset": 794.16,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "single individual integer like say five",
      "offset": 795.92,
      "duration": 5.919
    },
    {
      "lang": "en",
      "text": "so we get a sense of how this works",
      "offset": 799.6,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "now",
      "offset": 801.839,
      "duration": 2.401
    },
    {
      "lang": "en",
      "text": "one way this works of course is we can",
      "offset": 802.72,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "just take the c and we can index into",
      "offset": 804.24,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "row five",
      "offset": 806.399,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "and that gives us a vector the fifth row",
      "offset": 807.839,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "of c",
      "offset": 810.399,
      "duration": 2.641
    },
    {
      "lang": "en",
      "text": "and um",
      "offset": 811.6,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "this is one way to do it",
      "offset": 813.04,
      "duration": 3.039
    },
    {
      "lang": "en",
      "text": "the other way that i presented in the",
      "offset": 814.72,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "previous lecture is actually seemingly",
      "offset": 816.079,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "different but actually identical so in",
      "offset": 818.639,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "the previous lecture what we did is we",
      "offset": 820.959,
      "duration": 3.281
    },
    {
      "lang": "en",
      "text": "took these integers and we used the one",
      "offset": 822.32,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "hot encoding to first encode them so f.1",
      "offset": 824.24,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "hot",
      "offset": 827.6,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "we want to encode integer 5 and we want",
      "offset": 828.72,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "to tell it that the number of classes is",
      "offset": 831.2,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "27 so that's the 26 dimensional vector",
      "offset": 832.88,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "of all zeros except the fifth bit is",
      "offset": 835.839,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "turned on",
      "offset": 837.92,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "now this actually doesn't work",
      "offset": 839.92,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "the reason is that",
      "offset": 843.12,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "this input actually must be a doorstop",
      "offset": 844.639,
      "duration": 3.281
    },
    {
      "lang": "en",
      "text": "tensor",
      "offset": 846,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "and i'm making some of these errors",
      "offset": 847.92,
      "duration": 2.64
    },
    {
      "lang": "en",
      "text": "intentionally just so you get to see",
      "offset": 849.199,
      "duration": 3.521
    },
    {
      "lang": "en",
      "text": "some errors and how to fix them",
      "offset": 850.56,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "so this must be a tester not an int",
      "offset": 852.72,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "fairly straightforward to fix we get a",
      "offset": 854.32,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "one hot vector the fifth dimension is",
      "offset": 856.56,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "one and the shape of this is 27.",
      "offset": 858.959,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "and now notice that just as i briefly",
      "offset": 862.24,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "alluded to in the previous video if we",
      "offset": 864.639,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "take this one hot vector and we multiply",
      "offset": 866.48,
      "duration": 5.359
    },
    {
      "lang": "en",
      "text": "it by c",
      "offset": 869.12,
      "duration": 2.719
    },
    {
      "lang": "en",
      "text": "then",
      "offset": 873.76,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "what would you expect",
      "offset": 875.68,
      "duration": 3.519
    },
    {
      "lang": "en",
      "text": "well number one",
      "offset": 877.36,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "first you'd expect an error",
      "offset": 879.199,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "because",
      "offset": 881.76,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "expected scalar type long but found",
      "offset": 883.6,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "float",
      "offset": 885.36,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "so a little bit confusing but",
      "offset": 886.32,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "the problem here is that one hot the",
      "offset": 888.56,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "data type of it",
      "offset": 890.16,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "is",
      "offset": 892.16,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "long it's a 64-bit integer but this is a",
      "offset": 893.12,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "float tensor and so pytorch doesn't know",
      "offset": 896.56,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "how to multiply an int with a float and",
      "offset": 899.04,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "that's why we had to explicitly cast",
      "offset": 901.6,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "this to a float so that we can multiply",
      "offset": 903.519,
      "duration": 5.841
    },
    {
      "lang": "en",
      "text": "now the output actually here",
      "offset": 906.32,
      "duration": 4.959
    },
    {
      "lang": "en",
      "text": "is identical",
      "offset": 909.36,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "and that it's identical because of the",
      "offset": 911.279,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "way the matrix multiplication here works",
      "offset": 912.72,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "we have the one hot um vector",
      "offset": 915.04,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "multiplying columns of c",
      "offset": 917.279,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "and because of all the zeros they",
      "offset": 919.839,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "actually end up masking out everything",
      "offset": 921.68,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "in c except for the fifth row which is",
      "offset": 923.519,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "plucked out",
      "offset": 925.6,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "and so we actually arrive at the same",
      "offset": 927.199,
      "duration": 2.64
    },
    {
      "lang": "en",
      "text": "result",
      "offset": 928.8,
      "duration": 2.56
    },
    {
      "lang": "en",
      "text": "and that tells you that",
      "offset": 929.839,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "here we can interpret this first",
      "offset": 931.36,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "piece here this embedding of the integer",
      "offset": 933.519,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "we can either think of it as the integer",
      "offset": 935.6,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "indexing into a lookup table c but",
      "offset": 937.6,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "equivalently we can also think of this",
      "offset": 940.079,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "little piece here as a first layer of",
      "offset": 942.079,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "this bigger neural net",
      "offset": 944.399,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "this layer here has neurons that have no",
      "offset": 946.399,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "non-linearity there's no 10h they're",
      "offset": 948.959,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "just linear neurons and their weight",
      "offset": 950.72,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "matrix is c",
      "offset": 952.8,
      "duration": 4.959
    },
    {
      "lang": "en",
      "text": "and then we are encoding integers into",
      "offset": 955.44,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "one hot and feeding those into a neural",
      "offset": 957.759,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "net and this first layer basically",
      "offset": 959.519,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "embeds them",
      "offset": 961.44,
      "duration": 2.959
    },
    {
      "lang": "en",
      "text": "so those are two equivalent ways of",
      "offset": 962.639,
      "duration": 3.521
    },
    {
      "lang": "en",
      "text": "doing the same thing we're just going to",
      "offset": 964.399,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "index because it's much much faster and",
      "offset": 966.16,
      "duration": 3.119
    },
    {
      "lang": "en",
      "text": "we're going to discard this",
      "offset": 968.24,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "interpretation of one hot inputs into",
      "offset": 969.279,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "neural nets and we're just going to",
      "offset": 972.399,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "index integers and create and use",
      "offset": 974.16,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "embedding tables now embedding a single",
      "offset": 976.24,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "integer like 5 is easy enough we can",
      "offset": 978.32,
      "duration": 4.959
    },
    {
      "lang": "en",
      "text": "simply ask pytorch to retrieve the fifth",
      "offset": 980.88,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "row of c",
      "offset": 983.279,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "or the row index five of c",
      "offset": 984.48,
      "duration": 5.599
    },
    {
      "lang": "en",
      "text": "but how do we simultaneously embed all",
      "offset": 987.68,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "of these 32 by three integers stored in",
      "offset": 990.079,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "array x",
      "offset": 992.88,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "luckily pytorch indexing is fairly",
      "offset": 994.639,
      "duration": 3.521
    },
    {
      "lang": "en",
      "text": "flexible and quite powerful",
      "offset": 996.32,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "so it doesn't just work to",
      "offset": 998.16,
      "duration": 6.239
    },
    {
      "lang": "en",
      "text": "ask for a single element five like this",
      "offset": 1001.12,
      "duration": 5.279
    },
    {
      "lang": "en",
      "text": "you can actually index using lists so",
      "offset": 1004.399,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "for example we can get the rows five six",
      "offset": 1006.399,
      "duration": 3.041
    },
    {
      "lang": "en",
      "text": "and seven",
      "offset": 1008.48,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "and this will just work like this we can",
      "offset": 1009.44,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "index with a list",
      "offset": 1011.839,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "it doesn't just have to be a list it can",
      "offset": 1013.759,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "also be a actually a tensor of integers",
      "offset": 1015.199,
      "duration": 5.361
    },
    {
      "lang": "en",
      "text": "and we can index with that",
      "offset": 1018.48,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "so this is a integer tensor 567 and this",
      "offset": 1020.56,
      "duration": 5.359
    },
    {
      "lang": "en",
      "text": "will just work as well",
      "offset": 1023.6,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "in fact we can also for example repeat",
      "offset": 1025.919,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "row 7 and retrieve it multiple times",
      "offset": 1028.4,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "and",
      "offset": 1030.959,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "that same index will just get embedded",
      "offset": 1032.16,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "multiple times here",
      "offset": 1034.16,
      "duration": 3.279
    },
    {
      "lang": "en",
      "text": "so here we are indexing with a",
      "offset": 1036.079,
      "duration": 2.801
    },
    {
      "lang": "en",
      "text": "one-dimensional",
      "offset": 1037.439,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "tensor of integers but it turns out that",
      "offset": 1038.88,
      "duration": 3.959
    },
    {
      "lang": "en",
      "text": "you can also index with",
      "offset": 1041.439,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "multi-dimensional tensors of integers",
      "offset": 1042.839,
      "duration": 4.681
    },
    {
      "lang": "en",
      "text": "here we have a two-dimensional in tensor",
      "offset": 1045.12,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "of integers so we can simply just do c",
      "offset": 1047.52,
      "duration": 7.12
    },
    {
      "lang": "en",
      "text": "at x and this just works",
      "offset": 1050.48,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "and the shape of this",
      "offset": 1054.64,
      "duration": 2.56
    },
    {
      "lang": "en",
      "text": "is",
      "offset": 1056.16,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "32 by 3 which is the original shape and",
      "offset": 1057.2,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "now for every one of those 32 by 3",
      "offset": 1060.16,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "integers we've retrieved the embedding",
      "offset": 1061.679,
      "duration": 3.161
    },
    {
      "lang": "en",
      "text": "vector",
      "offset": 1063.76,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "here so basically we have that as an",
      "offset": 1064.84,
      "duration": 4.92
    },
    {
      "lang": "en",
      "text": "example",
      "offset": 1068.559,
      "duration": 5.761
    },
    {
      "lang": "en",
      "text": "the 13th or example index 13",
      "offset": 1069.76,
      "duration": 7.44
    },
    {
      "lang": "en",
      "text": "the second dimension is the integer 1 as",
      "offset": 1074.32,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "an example",
      "offset": 1077.2,
      "duration": 2.479
    },
    {
      "lang": "en",
      "text": "and so",
      "offset": 1078.72,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "here if we do c of x which gives us that",
      "offset": 1079.679,
      "duration": 6.161
    },
    {
      "lang": "en",
      "text": "array and then we index into 13 by two",
      "offset": 1082.799,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "of that array",
      "offset": 1085.84,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "then we we get the embedding",
      "offset": 1087.12,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "here",
      "offset": 1089.6,
      "duration": 2.48
    },
    {
      "lang": "en",
      "text": "and you can verify that",
      "offset": 1090.48,
      "duration": 2.319
    },
    {
      "lang": "en",
      "text": "c",
      "offset": 1092.08,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "at one which is the integer at that",
      "offset": 1092.799,
      "duration": 7.041
    },
    {
      "lang": "en",
      "text": "location is indeed equal to this",
      "offset": 1095.6,
      "duration": 6.079
    },
    {
      "lang": "en",
      "text": "you see they're equal",
      "offset": 1099.84,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "so basically long story short pytorch",
      "offset": 1101.679,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "indexing is awesome and to embed",
      "offset": 1103.52,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "simultaneously all of the integers in x",
      "offset": 1106.559,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "we can simply do c of x",
      "offset": 1109.44,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "and that is our embedding",
      "offset": 1111.28,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "and that just works",
      "offset": 1113.12,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "now let's construct this layer here the",
      "offset": 1115.039,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "hidden layer",
      "offset": 1117.12,
      "duration": 5.439
    },
    {
      "lang": "en",
      "text": "so we have that w1 as i'll call it are",
      "offset": 1118.799,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "these weights which we will initialize",
      "offset": 1122.559,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "randomly",
      "offset": 1124.799,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "now the number of inputs to this layer",
      "offset": 1126,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "is going to be",
      "offset": 1128.4,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "three times two right because we have",
      "offset": 1129.6,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "two dimensional embeddings and we have",
      "offset": 1131.28,
      "duration": 2.399
    },
    {
      "lang": "en",
      "text": "three of them",
      "offset": 1132.64,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "so the number of inputs is 6",
      "offset": 1133.679,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "and the number of neurons in this layer",
      "offset": 1136.16,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "is a variable up to us",
      "offset": 1138.4,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "let's use 100 neurons as an example",
      "offset": 1140.24,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "and then biases",
      "offset": 1142.88,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "will be also initialized randomly as an",
      "offset": 1144.64,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "example",
      "offset": 1146.559,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "and let's and we just need 100 of them",
      "offset": 1147.52,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "now the problem with this is we can't",
      "offset": 1151.2,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "simply normally we would take the input",
      "offset": 1153.36,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "in this case that's embedding and we'd",
      "offset": 1155.6,
      "duration": 4.959
    },
    {
      "lang": "en",
      "text": "like to multiply it with these weights",
      "offset": 1157.52,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "and then we would like to add the bias",
      "offset": 1160.559,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "this is roughly what we want to do",
      "offset": 1162.24,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "but the problem here is that these",
      "offset": 1164.4,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "embeddings are stacked up in the",
      "offset": 1165.919,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "dimensions of this input tensor",
      "offset": 1167.76,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "so this will not work this matrix",
      "offset": 1169.919,
      "duration": 2.961
    },
    {
      "lang": "en",
      "text": "multiplication because this is a shape",
      "offset": 1171.36,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "32 by 3 by 2 and i can't multiply that",
      "offset": 1172.88,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "by 6 by 100",
      "offset": 1175.36,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "so somehow we need to concatenate these",
      "offset": 1177.12,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "inputs here together so that we can do",
      "offset": 1180,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "something along these lines which",
      "offset": 1181.919,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "currently does not work",
      "offset": 1183.12,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "so how do we transform this 32 by 3 by 2",
      "offset": 1185.039,
      "duration": 5.681
    },
    {
      "lang": "en",
      "text": "into a 32 by 6 so that we can actually",
      "offset": 1187.919,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "perform",
      "offset": 1190.72,
      "duration": 2.079
    },
    {
      "lang": "en",
      "text": "this",
      "offset": 1191.84,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "multiplication over here i'd like to",
      "offset": 1192.799,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "show you that there are usually many",
      "offset": 1194.64,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "ways of",
      "offset": 1196.16,
      "duration": 3.519
    },
    {
      "lang": "en",
      "text": "implementing what you'd like to do in",
      "offset": 1197.44,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "torch and some of them will be faster",
      "offset": 1199.679,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "better shorter etc",
      "offset": 1201.84,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "and that's because torch is a very large",
      "offset": 1203.76,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "library and it's got lots and lots of",
      "offset": 1206.24,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "functions so if you just go to the",
      "offset": 1207.919,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "documentation and click on torch you'll",
      "offset": 1209.44,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "see that my slider here is very tiny and",
      "offset": 1211.36,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "that's because there are so many",
      "offset": 1214.16,
      "duration": 2.48
    },
    {
      "lang": "en",
      "text": "functions that you can call on these",
      "offset": 1215.28,
      "duration": 2.48
    },
    {
      "lang": "en",
      "text": "tensors",
      "offset": 1216.64,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "to transform them create them multiply",
      "offset": 1217.76,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "them add them perform all kinds of",
      "offset": 1220.08,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "different operations on them",
      "offset": 1222,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "and so this is kind of like",
      "offset": 1224.88,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "the space of possibility if you will",
      "offset": 1228.48,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "now one of the things that you can do is",
      "offset": 1231.12,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "if we can control here ctrl f for",
      "offset": 1232.48,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "concatenate and we see that there's a",
      "offset": 1234.64,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "function",
      "offset": 1236.559,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "torque.cat short for concatenate",
      "offset": 1237.44,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "and this concatenates the given sequence",
      "offset": 1240.48,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "of tensors in a given dimension",
      "offset": 1242,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "and these sensors must have the same",
      "offset": 1245.12,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "shape etc so we can use the concatenate",
      "offset": 1246.72,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "operation to in a naive way concatenate",
      "offset": 1249.2,
      "duration": 6.8
    },
    {
      "lang": "en",
      "text": "these three embeddings for each input",
      "offset": 1252.24,
      "duration": 6.559
    },
    {
      "lang": "en",
      "text": "so in this case we have m of",
      "offset": 1256,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "amp of the shape and really what we want",
      "offset": 1258.799,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "to do is we want to retrieve these three",
      "offset": 1261.039,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "parts and concatenate them",
      "offset": 1263.12,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "so we want to grab all the examples",
      "offset": 1264.96,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "we want to grab",
      "offset": 1268.48,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "first the zeroth",
      "offset": 1270.24,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "index",
      "offset": 1273.44,
      "duration": 3.119
    },
    {
      "lang": "en",
      "text": "and then all of",
      "offset": 1274.559,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "this",
      "offset": 1276.559,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "so this plucks out",
      "offset": 1277.76,
      "duration": 6.56
    },
    {
      "lang": "en",
      "text": "the 32 by 2 embeddings of just the first",
      "offset": 1280.4,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "word here",
      "offset": 1284.32,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "and so basically we want this guy",
      "offset": 1286.4,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "we want the first dimension and we want",
      "offset": 1288.559,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "the second dimension",
      "offset": 1290.88,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "and these are the three pieces",
      "offset": 1292.559,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "individually",
      "offset": 1294.64,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "and then we want to treat this as a",
      "offset": 1296.4,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "sequence and we want to torch that cat",
      "offset": 1298,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "on that sequence so this is the list",
      "offset": 1300.559,
      "duration": 5.041
    },
    {
      "lang": "en",
      "text": "tor.cat takes a",
      "offset": 1303.6,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "sequence",
      "offset": 1305.6,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "of tensors and then we have to tell it",
      "offset": 1306.72,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "along which dimension to concatenate",
      "offset": 1308.48,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "so in this case all these are 32 by 2",
      "offset": 1311.2,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "and we want to concatenate not across",
      "offset": 1313.6,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "dimension 0 by the cross dimension one",
      "offset": 1315.28,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "so passing in one",
      "offset": 1318.159,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "gives us a result",
      "offset": 1320,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "the shape of this is 32 by 6 exactly as",
      "offset": 1321.919,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "we'd like",
      "offset": 1324.24,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "so that basically took 32 and squashed",
      "offset": 1325.36,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "these by concatenating them into 32 by",
      "offset": 1328.32,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "6.",
      "offset": 1330.559,
      "duration": 2.881
    },
    {
      "lang": "en",
      "text": "now this is kind of ugly because this",
      "offset": 1331.52,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "code would not generalize if we want to",
      "offset": 1333.44,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "later change the block size right now we",
      "offset": 1335.6,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "have three inputs",
      "offset": 1337.679,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "three words but what if we had five",
      "offset": 1339.679,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "then here we would have to change the",
      "offset": 1342.24,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "code because i'm indexing directly well",
      "offset": 1343.52,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "torch comes to rescue again because that",
      "offset": 1346.08,
      "duration": 5.599
    },
    {
      "lang": "en",
      "text": "turns out to be a function called unbind",
      "offset": 1348.159,
      "duration": 6.961
    },
    {
      "lang": "en",
      "text": "and it removes a tensor dimension",
      "offset": 1351.679,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "so it removes the tensor dimension",
      "offset": 1355.28,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "returns a tuple of all slices along a",
      "offset": 1356.72,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "given dimension",
      "offset": 1359.039,
      "duration": 2.64
    },
    {
      "lang": "en",
      "text": "without it",
      "offset": 1360.4,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "so this is exactly what we need",
      "offset": 1361.679,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "and basically when we call torch dot",
      "offset": 1363.84,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "unbind",
      "offset": 1365.679,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "torch dot unbind",
      "offset": 1368,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "of m",
      "offset": 1370.32,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "and pass in dimension",
      "offset": 1371.679,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "1 index 1",
      "offset": 1374.72,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "this gives us a list of",
      "offset": 1376.48,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "a list of tensors exactly equivalent to",
      "offset": 1379.28,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "this",
      "offset": 1381.6,
      "duration": 2.559
    },
    {
      "lang": "en",
      "text": "so running this",
      "offset": 1382.48,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "gives us a line",
      "offset": 1384.159,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "3",
      "offset": 1386.4,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "and it's exactly this list and so we can",
      "offset": 1387.52,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "call torch.cat on it",
      "offset": 1389.44,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "and along the first dimension",
      "offset": 1392.559,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "and this works",
      "offset": 1394.96,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "and this shape is the same",
      "offset": 1396.88,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "but now this is uh it doesn't matter if",
      "offset": 1399.12,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "we have block size 3 or 5 or 10 this",
      "offset": 1401.12,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "will just work",
      "offset": 1403.36,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "so this is one way to do it but it turns",
      "offset": 1404.799,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "out that in this case there's actually a",
      "offset": 1406.96,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "significantly better and more efficient",
      "offset": 1408.96,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "way and this gives me an opportunity to",
      "offset": 1410.72,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "hint at some of the internals of",
      "offset": 1412.88,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "torch.tensor",
      "offset": 1414.72,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "so let's create",
      "offset": 1416.4,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "an array here",
      "offset": 1417.919,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "of elements from 0 to 17",
      "offset": 1420.159,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "and the shape of this",
      "offset": 1422.559,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "is just 18. it's a single picture of 18",
      "offset": 1424.24,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "numbers",
      "offset": 1426.48,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "it turns out that we can very quickly",
      "offset": 1427.919,
      "duration": 5.041
    },
    {
      "lang": "en",
      "text": "re-represent this as different sized and",
      "offset": 1429.919,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "dimensional tensors",
      "offset": 1432.96,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "we do this by calling a view",
      "offset": 1434.48,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "and we can say that actually this is not",
      "offset": 1437.44,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "a single vector of 18 this is a two by",
      "offset": 1439.28,
      "duration": 6.32
    },
    {
      "lang": "en",
      "text": "nine tensor or alternatively this is a",
      "offset": 1442.159,
      "duration": 5.841
    },
    {
      "lang": "en",
      "text": "nine by two tensor",
      "offset": 1445.6,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "or this is actually a three by three by",
      "offset": 1448,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "two tensor",
      "offset": 1450.159,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "as long as the total number of elements",
      "offset": 1451.84,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "here multiply to be the same",
      "offset": 1453.76,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "this will just work and",
      "offset": 1456.48,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "in pytorch this operation calling that",
      "offset": 1458.96,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "view is extremely efficient",
      "offset": 1461.52,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "and the reason for that is that",
      "offset": 1464,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "in each tensor there's something called",
      "offset": 1466,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "the underlying storage",
      "offset": 1467.84,
      "duration": 4.959
    },
    {
      "lang": "en",
      "text": "and the storage is just the numbers",
      "offset": 1470.48,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "always as a one-dimensional vector and",
      "offset": 1472.799,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "this is how this tensor is represented",
      "offset": 1474.88,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "in the computer memory it's always a",
      "offset": 1477.12,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "one-dimensional vector",
      "offset": 1478.64,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "but when we call that view we are",
      "offset": 1481.679,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "manipulating some of attributes of that",
      "offset": 1484,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "tensor that dictate how this",
      "offset": 1486.24,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "one-dimensional sequence is interpreted",
      "offset": 1488.64,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "to be an n-dimensional tensor",
      "offset": 1490.96,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "and so what's happening here is that no",
      "offset": 1493.6,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "memory is being changed copied moved or",
      "offset": 1495.36,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "created when we call that view the",
      "offset": 1497.36,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "storage",
      "offset": 1499.44,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "is identical but when you call that view",
      "offset": 1500.559,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "some of the internal",
      "offset": 1503.12,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "attributes of the view of the sensor are",
      "offset": 1505.12,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "being manipulated and changed in",
      "offset": 1507.84,
      "duration": 2.719
    },
    {
      "lang": "en",
      "text": "particular that's something there's",
      "offset": 1509.52,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "something called a storage offset",
      "offset": 1510.559,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "strides and shapes and those are",
      "offset": 1512.32,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "manipulated so that this one-dimensional",
      "offset": 1514.159,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "sequence of bytes is seen as different",
      "offset": 1516,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "and dimensional arrays",
      "offset": 1518.559,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "there's a blog post here from eric",
      "offset": 1520.559,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "called pi torch internals where he goes",
      "offset": 1522.72,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "into some of this with respect to tensor",
      "offset": 1525.279,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "and how the view of the tensor is",
      "offset": 1527.52,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "represented",
      "offset": 1529.12,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "and this is really just like a logical",
      "offset": 1530.32,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "construct of representing the physical",
      "offset": 1532.4,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "memory",
      "offset": 1534.32,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "and so this is a pretty good um blog",
      "offset": 1535.76,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "post that you can go into i might also",
      "offset": 1538.24,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "create an entire video on the internals",
      "offset": 1540,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "of torch tensor and how this works",
      "offset": 1541.84,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "for here we just note that this is an",
      "offset": 1544.4,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "extremely efficient operation",
      "offset": 1546.159,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "and if i delete this and come back to",
      "offset": 1548.08,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "our end",
      "offset": 1550.559,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "we see that the shape of our end is 32",
      "offset": 1553.12,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "by three by two but we can simply",
      "offset": 1555.36,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "ask for pytorch to view this instead as",
      "offset": 1558.08,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "a 32 by six",
      "offset": 1560.559,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "and the way this gets flattened into a",
      "offset": 1562.96,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "32 by six array",
      "offset": 1565.36,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "just happens that",
      "offset": 1567.6,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "these two",
      "offset": 1569.36,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "get stacked up",
      "offset": 1570.88,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "in a single row and so that's basically",
      "offset": 1572.48,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "the concatenation operation that we're",
      "offset": 1574.4,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "after",
      "offset": 1575.84,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "and you can verify that this actually",
      "offset": 1577.2,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "gives the exact same result as what we",
      "offset": 1578.64,
      "duration": 3.519
    },
    {
      "lang": "en",
      "text": "had before",
      "offset": 1580.4,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "so this is an element y equals and you",
      "offset": 1582.159,
      "duration": 3.041
    },
    {
      "lang": "en",
      "text": "can see that all the elements of these",
      "offset": 1583.76,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "two tensors are the same",
      "offset": 1585.2,
      "duration": 5.599
    },
    {
      "lang": "en",
      "text": "and so we get the exact same result",
      "offset": 1587.679,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "so long story short we can actually just",
      "offset": 1590.799,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "come here",
      "offset": 1593.039,
      "duration": 5.721
    },
    {
      "lang": "en",
      "text": "and if we just view this as a 32x6",
      "offset": 1594.159,
      "duration": 4.601
    },
    {
      "lang": "en",
      "text": "instead then this multiplication will",
      "offset": 1598.88,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "work and give us the hidden states that",
      "offset": 1600.799,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "we're after",
      "offset": 1603.12,
      "duration": 2.799
    },
    {
      "lang": "en",
      "text": "so if this is h",
      "offset": 1604.24,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "then h shape is now",
      "offset": 1605.919,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "the 100 dimensional activations for",
      "offset": 1608.4,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "every one of our 32 examples",
      "offset": 1611.039,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "and this gives the desired result let me",
      "offset": 1613.44,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "do two things here number one let's not",
      "offset": 1615.6,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "use 32 we can for example do something",
      "offset": 1617.919,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "like",
      "offset": 1620.48,
      "duration": 2.24
    },
    {
      "lang": "en",
      "text": "m.shape at 0",
      "offset": 1623,
      "duration": 4.44
    },
    {
      "lang": "en",
      "text": "so that we don't hard code these numbers",
      "offset": 1625.44,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "and this would work for any size of this",
      "offset": 1627.44,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "amp",
      "offset": 1629.36,
      "duration": 2.559
    },
    {
      "lang": "en",
      "text": "or alternatively we can also do negative",
      "offset": 1630.32,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "one when we do negative one pi torch",
      "offset": 1631.919,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "will infer what this should be",
      "offset": 1634.159,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "because the number of elements must be",
      "offset": 1636.72,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "the same and we're saying that this is 6",
      "offset": 1637.919,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "by church will derive that this must be",
      "offset": 1640.32,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "32 or whatever else it is if m is of",
      "offset": 1641.919,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "different size",
      "offset": 1644.799,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "the other thing is here um",
      "offset": 1646.799,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "one more thing i'd like to point out is",
      "offset": 1649.52,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "here when we do the concatenation",
      "offset": 1653.039,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "this actually is much less efficient",
      "offset": 1655.2,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "because um this concatenation would",
      "offset": 1657.6,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "create a whole new tensor with a whole",
      "offset": 1659.919,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "new storage so new memory is being",
      "offset": 1661.36,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "created because there's no way to",
      "offset": 1663.279,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "concatenate tensors just by manipulating",
      "offset": 1664.88,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "the view attributes",
      "offset": 1666.96,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "so this is inefficient and creates all",
      "offset": 1668.48,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "kinds of new memory",
      "offset": 1670.08,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "uh so let me delete this now",
      "offset": 1672.32,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "we don't need this",
      "offset": 1675.52,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "and here to calculate h we want to also",
      "offset": 1677.2,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "dot 10h",
      "offset": 1679.6,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "of this",
      "offset": 1681.44,
      "duration": 2.959
    },
    {
      "lang": "en",
      "text": "to get our",
      "offset": 1682.48,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "oops to get our h",
      "offset": 1684.399,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "so these are now numbers between",
      "offset": 1687.039,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "negative one and one because of the 10h",
      "offset": 1688.24,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "and we have",
      "offset": 1690.48,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "that the shape is 32 by 100",
      "offset": 1691.679,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "and that is basically this hidden layer",
      "offset": 1694,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "of activations here",
      "offset": 1695.919,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "for every one of our 32 examples",
      "offset": 1697.6,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "now there's one more thing i've lost",
      "offset": 1700.24,
      "duration": 2.799
    },
    {
      "lang": "en",
      "text": "over that we have to be very careful",
      "offset": 1701.52,
      "duration": 2.879
    },
    {
      "lang": "en",
      "text": "with and that this",
      "offset": 1703.039,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "and that's this plus here",
      "offset": 1704.399,
      "duration": 3.121
    },
    {
      "lang": "en",
      "text": "in particular we want to make sure that",
      "offset": 1706.159,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "the broadcasting will do what we like",
      "offset": 1707.52,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "the shape of this is 32 by 100 and the",
      "offset": 1710.72,
      "duration": 4.959
    },
    {
      "lang": "en",
      "text": "ones shape is 100.",
      "offset": 1713.36,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "so we see that the addition here will",
      "offset": 1715.679,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "broadcast these two and in particular we",
      "offset": 1717.6,
      "duration": 6.64
    },
    {
      "lang": "en",
      "text": "have 32 by 100 broadcasting to 100.",
      "offset": 1720,
      "duration": 7.2
    },
    {
      "lang": "en",
      "text": "so broadcasting will align on the right",
      "offset": 1724.24,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "create a fake dimension here so this",
      "offset": 1727.2,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "will become a 1 by 100 row vector and",
      "offset": 1729.279,
      "duration": 5.441
    },
    {
      "lang": "en",
      "text": "then it will copy vertically",
      "offset": 1731.919,
      "duration": 5.281
    },
    {
      "lang": "en",
      "text": "for every one of these rows of 32 and do",
      "offset": 1734.72,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "an element wise addition",
      "offset": 1737.2,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "so in this case the correct thing will",
      "offset": 1738.88,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "be happening because the same bias",
      "offset": 1740.48,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "vector will be added to all the rows",
      "offset": 1742.48,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "of",
      "offset": 1745.84,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "this matrix so that is correct that's",
      "offset": 1746.64,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "what we'd like and it's always good",
      "offset": 1748.96,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "practice you just make sure",
      "offset": 1751.279,
      "duration": 2.801
    },
    {
      "lang": "en",
      "text": "so that you don't shoot yourself in the",
      "offset": 1752.88,
      "duration": 3.279
    },
    {
      "lang": "en",
      "text": "foot and finally let's create the final",
      "offset": 1754.08,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "layer here",
      "offset": 1756.159,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "so let's create",
      "offset": 1757.6,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "w2 and v2",
      "offset": 1759.84,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "the input now is 100",
      "offset": 1762.64,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "and the output number of neurons will be",
      "offset": 1764.88,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "for us 27 because we have 27 possible",
      "offset": 1766.96,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "characters that come next",
      "offset": 1769.76,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "so the biases will be 27 as well",
      "offset": 1771.76,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "so therefore the logits which are the",
      "offset": 1774.88,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "outputs of this neural net",
      "offset": 1776.64,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "are going to be um",
      "offset": 1778.559,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "h",
      "offset": 1781.44,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "multiplied by w2 plus b2",
      "offset": 1782.399,
      "duration": 8.081
    },
    {
      "lang": "en",
      "text": "logistic shape is 32 by 27",
      "offset": 1787.12,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "and the logits look",
      "offset": 1790.48,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "good now exactly as we saw in the",
      "offset": 1792.24,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "previous video we want to take these",
      "offset": 1794.159,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "logits and we want to first exponentiate",
      "offset": 1795.679,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "them to get our fake counts",
      "offset": 1798,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "and then we want to normalize them into",
      "offset": 1800,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "a probability",
      "offset": 1801.36,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "so prob is counts divide",
      "offset": 1802.96,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "and now",
      "offset": 1805.6,
      "duration": 4.959
    },
    {
      "lang": "en",
      "text": "counts dot sum along the first dimension",
      "offset": 1806.88,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "and keep them as true exactly as in the",
      "offset": 1810.559,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "previous video",
      "offset": 1812.48,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "and so",
      "offset": 1814.399,
      "duration": 5.681
    },
    {
      "lang": "en",
      "text": "prob that shape now is 32 by 27",
      "offset": 1816,
      "duration": 7.52
    },
    {
      "lang": "en",
      "text": "and you'll see that every row of prob",
      "offset": 1820.08,
      "duration": 6.4
    },
    {
      "lang": "en",
      "text": "sums to one so it's normalized",
      "offset": 1823.52,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "so that gives us the probabilities now",
      "offset": 1826.48,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "of course we have the actual letter that",
      "offset": 1828.559,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "comes next and that comes from this",
      "offset": 1830.24,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "array y",
      "offset": 1832.24,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "which we which we created during the",
      "offset": 1834.24,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "dataset creation so why is this last",
      "offset": 1836.32,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "piece here which is the identity of the",
      "offset": 1839.039,
      "duration": 3.281
    },
    {
      "lang": "en",
      "text": "next character in the sequence that we'd",
      "offset": 1840.88,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "like to now predict",
      "offset": 1842.32,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "so what we'd like to do now is just as",
      "offset": 1844.799,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "in the previous video we'd like to index",
      "offset": 1846.64,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "into the rows of prob and in each row",
      "offset": 1848.48,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "we'd like to pluck out the probability",
      "offset": 1851.279,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "assigned to the correct character",
      "offset": 1852.96,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "as given here",
      "offset": 1855.12,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "so first we have torch.range of 32 which",
      "offset": 1856.96,
      "duration": 6.64
    },
    {
      "lang": "en",
      "text": "is kind of like a iterator over",
      "offset": 1860.48,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "numbers from 0 to 31",
      "offset": 1863.6,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "and then we can index into prob in the",
      "offset": 1865.76,
      "duration": 3.279
    },
    {
      "lang": "en",
      "text": "following way",
      "offset": 1867.52,
      "duration": 2.68
    },
    {
      "lang": "en",
      "text": "prop in",
      "offset": 1869.039,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "torch.range of 32 which iterates the",
      "offset": 1870.2,
      "duration": 5.479
    },
    {
      "lang": "en",
      "text": "roads and in each row we'd like to grab",
      "offset": 1872.799,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "this column as given by y",
      "offset": 1875.679,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "so this gives the current probabilities",
      "offset": 1879.279,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "as assigned by this neural network with",
      "offset": 1881.279,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "this setting of its weights",
      "offset": 1883.12,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "to the correct character in the sequence",
      "offset": 1884.88,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "and you can see here that this looks",
      "offset": 1887.76,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "okay for some of these characters like",
      "offset": 1889.52,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "this is basically 0.2",
      "offset": 1890.96,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "but it doesn't look very good at all for",
      "offset": 1892.96,
      "duration": 4.68
    },
    {
      "lang": "en",
      "text": "many other characters like this is",
      "offset": 1894.48,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "0.0701 probability and so the network",
      "offset": 1897.64,
      "duration": 4.44
    },
    {
      "lang": "en",
      "text": "thinks that some of these are extremely",
      "offset": 1900.48,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "unlikely but of course we haven't",
      "offset": 1902.08,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "trained the neural network yet so",
      "offset": 1903.679,
      "duration": 5.281
    },
    {
      "lang": "en",
      "text": "this will improve and ideally all of",
      "offset": 1907.12,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "these numbers here of course are one",
      "offset": 1908.96,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "because then we are correctly predicting",
      "offset": 1910.88,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "the next character",
      "offset": 1912.32,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "now just as in the previous video we",
      "offset": 1913.84,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "want to take these probabilities we want",
      "offset": 1915.519,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "to look at the lock probability",
      "offset": 1917.44,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "and then we want to look at the average",
      "offset": 1919.36,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "probability",
      "offset": 1920.96,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "and the negative of it to create the",
      "offset": 1922.08,
      "duration": 5.439
    },
    {
      "lang": "en",
      "text": "negative log likelihood loss",
      "offset": 1924.48,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "so the loss here is 17",
      "offset": 1927.519,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "and this is the loss that we'd like to",
      "offset": 1930.24,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "minimize to get the network to predict",
      "offset": 1931.76,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "the correct character in the sequence",
      "offset": 1934.159,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "okay so i rewrote everything here and",
      "offset": 1936.64,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "made it a bit more respectable so here's",
      "offset": 1938.72,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "our data set here's all the parameters",
      "offset": 1940.799,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "that we defined",
      "offset": 1942.96,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "i'm now using a generator to make it",
      "offset": 1944.559,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "reproducible",
      "offset": 1946.32,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "i clustered all the parameters into a",
      "offset": 1947.679,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "single list of parameters so that for",
      "offset": 1949.36,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "example it's easy to count them and see",
      "offset": 1951.44,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "that in total we currently have about",
      "offset": 1953.519,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "3400 parameters",
      "offset": 1954.799,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "and this is the forward pass as we",
      "offset": 1957.12,
      "duration": 2.48
    },
    {
      "lang": "en",
      "text": "developed it",
      "offset": 1958.399,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "and we arrive at a single number here",
      "offset": 1959.6,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "the loss that is currently expressing",
      "offset": 1961.919,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "how well",
      "offset": 1964,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "this neural network works with the",
      "offset": 1965.039,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "current setting of parameters",
      "offset": 1966.96,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "now i would like to make it even more",
      "offset": 1968.72,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "respectable so in particular see these",
      "offset": 1970.08,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "lines here where we take the logits and",
      "offset": 1972.32,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "we calculate the loss",
      "offset": 1974.64,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "we're not actually reinventing the wheel",
      "offset": 1977.44,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "here this is just um",
      "offset": 1979.44,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "classification and many people use",
      "offset": 1981.44,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "classification and that's why there is a",
      "offset": 1983.36,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "functional.cross entropy function in",
      "offset": 1985.679,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "pytorch to calculate this much more",
      "offset": 1987.44,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "efficiently",
      "offset": 1989.6,
      "duration": 2.799
    },
    {
      "lang": "en",
      "text": "so we can just simply call f.cross",
      "offset": 1990.72,
      "duration": 2.64
    },
    {
      "lang": "en",
      "text": "entropy",
      "offset": 1992.399,
      "duration": 2.481
    },
    {
      "lang": "en",
      "text": "and we can pass in the logits and we can",
      "offset": 1993.36,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "pass in the",
      "offset": 1994.88,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "array of targets y",
      "offset": 1996.32,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "and this calculates the exact same loss",
      "offset": 1998.08,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "so in fact we can simply put this here",
      "offset": 2002.24,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "and erase these three lines and we're",
      "offset": 2005.279,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "going to get the exact same result now",
      "offset": 2007.36,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "there are actually many good reasons to",
      "offset": 2009.919,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "prefer f.cross entropy over rolling your",
      "offset": 2011.44,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "own implementation like this i did this",
      "offset": 2014.24,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "for educational reasons but you'd never",
      "offset": 2016.24,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "use this in practice why is that",
      "offset": 2017.919,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "number one when you use f.cross entropy",
      "offset": 2020.559,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "by torch will not actually create all",
      "offset": 2022.88,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "these intermediate tensors because these",
      "offset": 2024.96,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "are all new tensors in memory and all",
      "offset": 2026.88,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "this is fairly inefficient to run like",
      "offset": 2029.12,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "this instead pytorch will cluster up all",
      "offset": 2031.36,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "these operations and very often create",
      "offset": 2034,
      "duration": 5.519
    },
    {
      "lang": "en",
      "text": "have fused kernels that very efficiently",
      "offset": 2036.96,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "evaluate these expressions that are sort",
      "offset": 2039.519,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "of like clustered mathematical",
      "offset": 2041.279,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "operations",
      "offset": 2043.12,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "number two the backward pass can be made",
      "offset": 2044.64,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "much more efficient and not just because",
      "offset": 2046.32,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "it's a fused kernel but also",
      "offset": 2048.32,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "analytically and mathematically it's",
      "offset": 2050.399,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "much it's often a very much simpler",
      "offset": 2052.48,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "backward pass to implement",
      "offset": 2055.359,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "we actually sell this with micrograd",
      "offset": 2057.44,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "you see here when we implemented 10h the",
      "offset": 2059.599,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "forward pass of this operation to",
      "offset": 2062,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "calculate the 10h was actually a fairly",
      "offset": 2063.919,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "complicated mathematical expression",
      "offset": 2065.76,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "but because it's a clustered",
      "offset": 2068.24,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "mathematical expression when we did the",
      "offset": 2069.359,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "backward pass we didn't individually",
      "offset": 2071.52,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "backward through the x and the two times",
      "offset": 2073.44,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "and the minus one in division etc we",
      "offset": 2075.919,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "just said it's one minus t squared and",
      "offset": 2078.159,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "that's a much simpler mathematical",
      "offset": 2080.24,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "expression",
      "offset": 2082.079,
      "duration": 2.641
    },
    {
      "lang": "en",
      "text": "and we were able to do this because",
      "offset": 2083.2,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "we're able to reuse calculations and",
      "offset": 2084.72,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "because we are able to mathematically",
      "offset": 2086.48,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "and analytically derive the derivative",
      "offset": 2088.079,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "and often that expression simplifies",
      "offset": 2090.24,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "mathematically and so there's much less",
      "offset": 2092.32,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "to implement",
      "offset": 2094.8,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "so not only can can it be made more",
      "offset": 2096.079,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "efficient because it runs in a fused",
      "offset": 2097.76,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "kernel but also because the expressions",
      "offset": 2099.52,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "can take a much simpler form",
      "offset": 2101.68,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "mathematically",
      "offset": 2103.28,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "so that's number one number two",
      "offset": 2105.92,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "under the hood f that cross entropy can",
      "offset": 2108.56,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "also be significantly more um",
      "offset": 2110.48,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "numerically well behaved let me show you",
      "offset": 2113.599,
      "duration": 5.041
    },
    {
      "lang": "en",
      "text": "an example of how this works",
      "offset": 2115.44,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "suppose we have a logits of negative 2 3",
      "offset": 2119.2,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "negative 3 0 and 5",
      "offset": 2121.52,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "and then we are taking the exponent of",
      "offset": 2124,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "it and normalizing it to sum to 1.",
      "offset": 2125.44,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "so",
      "offset": 2127.599,
      "duration": 2.48
    },
    {
      "lang": "en",
      "text": "when logits take on this values",
      "offset": 2128.32,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "everything is well and good and we get a",
      "offset": 2130.079,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "nice probability distribution",
      "offset": 2131.44,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "now consider what happens when some of",
      "offset": 2133.76,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "these logits take on more extreme values",
      "offset": 2135.2,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "and that can happen during optimization",
      "offset": 2137.44,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "of the neural network",
      "offset": 2138.96,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "suppose that some of these numbers grow",
      "offset": 2140.72,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "very negative like say negative 100",
      "offset": 2142.48,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "then actually everything will come out",
      "offset": 2145.2,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "fine we still get the probabilities that",
      "offset": 2147.119,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "um",
      "offset": 2149.28,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "you know are well behaved and they sum",
      "offset": 2150.079,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "to one and everything is great",
      "offset": 2152.32,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "but because of the way the x works if",
      "offset": 2154.4,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "you have very positive logits let's say",
      "offset": 2156.64,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "positive 100 in here",
      "offset": 2158.64,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "you actually start to run into trouble",
      "offset": 2160.72,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "and we get not a number here",
      "offset": 2162.24,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "and the reason for that is that these",
      "offset": 2164.48,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "counts",
      "offset": 2166.32,
      "duration": 2.4
    },
    {
      "lang": "en",
      "text": "have an if here",
      "offset": 2168.8,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "so if you pass in a very negative number",
      "offset": 2170.4,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "to x you just get a very negative sorry",
      "offset": 2172.48,
      "duration": 5.119
    },
    {
      "lang": "en",
      "text": "not negative but very small number very",
      "offset": 2175.44,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "very near zero and that's fine",
      "offset": 2177.599,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "but if you pass in a very positive",
      "offset": 2179.839,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "number suddenly we run out of range in",
      "offset": 2181.119,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "our floating point number that",
      "offset": 2183.68,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "represents these counts",
      "offset": 2185.52,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "so basically we're taking e and we're",
      "offset": 2188.16,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "raising it to the power of 100 and that",
      "offset": 2189.76,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "gives us if because we run out of",
      "offset": 2191.92,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "dynamic range on this floating point",
      "offset": 2194,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "number that is count",
      "offset": 2195.839,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "and so we cannot pass very large logits",
      "offset": 2197.839,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "through this expression",
      "offset": 2201.119,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "now let me reset these numbers to",
      "offset": 2203.839,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "something reasonable",
      "offset": 2205.28,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "the way pi torch solved this",
      "offset": 2207.28,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "is that",
      "offset": 2209.2,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "you see how we have a well-behaved",
      "offset": 2210.24,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "result here",
      "offset": 2212.24,
      "duration": 2.48
    },
    {
      "lang": "en",
      "text": "it turns out that because of the",
      "offset": 2213.52,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "normalization here you can actually",
      "offset": 2214.72,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "offset logits by any arbitrary constant",
      "offset": 2216.64,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "value that you want so if i add 1 here",
      "offset": 2219.599,
      "duration": 5.121
    },
    {
      "lang": "en",
      "text": "you actually get the exact same result",
      "offset": 2222.72,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "or if i add 2",
      "offset": 2224.72,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "or if i subtract three",
      "offset": 2226.32,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "any offset will produce the exact same",
      "offset": 2228.8,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "probabilities",
      "offset": 2230.8,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "so because negative numbers are okay but",
      "offset": 2232.72,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "positive numbers can actually overflow",
      "offset": 2235.119,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "this x what patrick does is it",
      "offset": 2236.96,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "internally calculates the maximum value",
      "offset": 2239.52,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "that occurs in the logits and it",
      "offset": 2241.599,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "subtracts it so in this case it would",
      "offset": 2243.359,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "subtract five",
      "offset": 2245.28,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "and so therefore the greatest number in",
      "offset": 2246.96,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "logits will become zero and all the",
      "offset": 2248.64,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "other numbers will become some negative",
      "offset": 2250.64,
      "duration": 2.479
    },
    {
      "lang": "en",
      "text": "numbers",
      "offset": 2252.079,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "and then the result of this is always",
      "offset": 2253.119,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "well behaved so even if we have 100 here",
      "offset": 2255.119,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "previously",
      "offset": 2257.92,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "not good but because pytorch will",
      "offset": 2259.119,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "subtract 100 this will work",
      "offset": 2261.119,
      "duration": 5.841
    },
    {
      "lang": "en",
      "text": "and so there's many good reasons to call",
      "offset": 2264.32,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "cross-entropy number one the forward",
      "offset": 2266.96,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "pass can be much more efficient the",
      "offset": 2269.2,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "backward pass can be much more efficient",
      "offset": 2270.8,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "and also things can be much more",
      "offset": 2273.04,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "numerically well behaved okay so let's",
      "offset": 2274.96,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "now set up the training of this neural",
      "offset": 2277.04,
      "duration": 2.24
    },
    {
      "lang": "en",
      "text": "net",
      "offset": 2278.24,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "we have the forward pass",
      "offset": 2279.28,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "uh we don't need these",
      "offset": 2282.48,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "is that we have the losses equal to the",
      "offset": 2284.64,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "f.cross entropy that's the forward pass",
      "offset": 2286.8,
      "duration": 5.279
    },
    {
      "lang": "en",
      "text": "then we need the backward pass first we",
      "offset": 2289.76,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "want to set the gradients to be zero so",
      "offset": 2292.079,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "for p in parameters",
      "offset": 2294.64,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "we want to make sure that p dot grad is",
      "offset": 2296.24,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "none which is the same as setting it to",
      "offset": 2298.079,
      "duration": 2.801
    },
    {
      "lang": "en",
      "text": "zero in pi torch",
      "offset": 2299.44,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "and then lost that backward to populate",
      "offset": 2300.88,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "those gradients",
      "offset": 2303.04,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "once we have the gradients we can do the",
      "offset": 2304.4,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "parameter update so for p in parameters",
      "offset": 2305.76,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "we want to take all the",
      "offset": 2308.24,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "data and we want to nudge it",
      "offset": 2310,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "learning rate times p dot grad",
      "offset": 2312.4,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "and then we want to repeat this",
      "offset": 2316.88,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "a few times",
      "offset": 2319.76,
      "duration": 2.64
    },
    {
      "lang": "en",
      "text": "and let's print the loss here as well",
      "offset": 2323.92,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "now this won't suffice and it will",
      "offset": 2328.8,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "create an error because we also have to",
      "offset": 2330.079,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "go for pn parameters",
      "offset": 2331.92,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "and we have to make sure that p dot",
      "offset": 2333.92,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "requires grad is set to true in pi torch",
      "offset": 2335.599,
      "duration": 8.161
    },
    {
      "lang": "en",
      "text": "and this should just work",
      "offset": 2339.76,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "okay so we started off with loss of 17",
      "offset": 2343.76,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "and we're decreasing it",
      "offset": 2345.76,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "let's run longer",
      "offset": 2348.079,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "and you see how the loss decreases",
      "offset": 2350.079,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "a lot here so",
      "offset": 2352.88,
      "duration": 2.959
    },
    {
      "lang": "en",
      "text": "if we just run for a thousand times",
      "offset": 2357.2,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "we get a very very low loss and that",
      "offset": 2359.599,
      "duration": 2.881
    },
    {
      "lang": "en",
      "text": "means that we're making very good",
      "offset": 2361.52,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "predictions now the reason that this is",
      "offset": 2362.48,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "so straightforward right now",
      "offset": 2365.28,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "is because we're only um",
      "offset": 2367.04,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "overfitting 32 examples",
      "offset": 2369.359,
      "duration": 5.281
    },
    {
      "lang": "en",
      "text": "so we only have 32 examples uh of the",
      "offset": 2372.32,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "first five words",
      "offset": 2374.64,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "and therefore it's very easy to make",
      "offset": 2376.4,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "this neural net fit only these two 32",
      "offset": 2377.92,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "examples because we have 3 400",
      "offset": 2380.8,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "parameters and only 32 examples so we're",
      "offset": 2383.119,
      "duration": 5.041
    },
    {
      "lang": "en",
      "text": "doing what's called overfitting a single",
      "offset": 2386.32,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "batch of the data",
      "offset": 2388.16,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "and getting a very low loss and good",
      "offset": 2390.16,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "predictions",
      "offset": 2392.32,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "um but that's just because we have so",
      "offset": 2393.44,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "many parameters for so few examples so",
      "offset": 2395.04,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "it's easy to",
      "offset": 2396.88,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "uh make this be very low",
      "offset": 2398,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "now we're not able to achieve exactly",
      "offset": 2400.079,
      "duration": 2.561
    },
    {
      "lang": "en",
      "text": "zero",
      "offset": 2401.76,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "and the reason for that is we can for",
      "offset": 2402.64,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "example look at logits which are being",
      "offset": 2404.48,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "predicted",
      "offset": 2406.32,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "and we can look at the max along the",
      "offset": 2408.4,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "first dimension",
      "offset": 2411.44,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "and in pi torch",
      "offset": 2413.52,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "max reports both the actual values that",
      "offset": 2415.52,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "take on the maximum number but also the",
      "offset": 2417.52,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "indices of piece",
      "offset": 2420.24,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "and you'll see that the indices are very",
      "offset": 2422.079,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "close to the labels",
      "offset": 2423.52,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "but in some cases they differ",
      "offset": 2426.32,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "for example in this very first example",
      "offset": 2428.64,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "the predicted index is 19 but the label",
      "offset": 2431.2,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "is five",
      "offset": 2433.52,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "and we're not able to make loss be zero",
      "offset": 2435.04,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "and fundamentally that's because here",
      "offset": 2436.96,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "the very first or the zeroth index is",
      "offset": 2440.56,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "the example where dot dot dot is",
      "offset": 2442.96,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "supposed to predict e but you see how",
      "offset": 2444.4,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "dot dot dot is also supposed to predict",
      "offset": 2446.16,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "an o and dot dot is also supposed to",
      "offset": 2448.079,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "predict an i and then s as well and so",
      "offset": 2450.48,
      "duration": 6.72
    },
    {
      "lang": "en",
      "text": "basically e o a or s are all possible",
      "offset": 2453.28,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "outcomes in a training set for the exact",
      "offset": 2457.2,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "same input so we're not able to",
      "offset": 2459.2,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "completely over fit and um",
      "offset": 2461.04,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "and make the loss be exactly zero so but",
      "offset": 2463.52,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "we're getting very close in the cases",
      "offset": 2466.48,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "where",
      "offset": 2468.88,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "there's a unique input for a unique",
      "offset": 2469.839,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "output in those cases we do what's",
      "offset": 2471.76,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "called overfit and we basically get the",
      "offset": 2473.52,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "exact same and the exact correct result",
      "offset": 2475.68,
      "duration": 5.439
    },
    {
      "lang": "en",
      "text": "so now all we have to do",
      "offset": 2479.04,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "is we just need to make sure that we",
      "offset": 2481.119,
      "duration": 3.041
    },
    {
      "lang": "en",
      "text": "read in the full data set and optimize",
      "offset": 2482.72,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "the neural net",
      "offset": 2484.16,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "okay so let's swing back up",
      "offset": 2485.44,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "where we created the dataset",
      "offset": 2487.2,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "and we see that here we only use the",
      "offset": 2489.119,
      "duration": 3.521
    },
    {
      "lang": "en",
      "text": "first five words so let me now erase",
      "offset": 2490.64,
      "duration": 2.959
    },
    {
      "lang": "en",
      "text": "this",
      "offset": 2492.64,
      "duration": 2.479
    },
    {
      "lang": "en",
      "text": "and let me erase the print statements",
      "offset": 2493.599,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "otherwise we'd be printing way too much",
      "offset": 2495.119,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "and so when we processed the full data",
      "offset": 2498,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "set of all the words we now had 228 000",
      "offset": 2499.76,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "examples instead of just 32.",
      "offset": 2502.96,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "so let's now scroll back down",
      "offset": 2505.2,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "to this is much larger reinitialize the",
      "offset": 2507.52,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "weights the same number of parameters",
      "offset": 2510,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "they all require gradients",
      "offset": 2512.319,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "and then let's push this print out",
      "offset": 2514.319,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "lost.item to be here",
      "offset": 2516.4,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "and let's just see how the optimization",
      "offset": 2518.64,
      "duration": 5.08
    },
    {
      "lang": "en",
      "text": "goes if we run this",
      "offset": 2519.92,
      "duration": 3.8
    },
    {
      "lang": "en",
      "text": "okay so we started with a fairly high",
      "offset": 2524.4,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "loss and then as we're optimizing the",
      "offset": 2525.839,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "loss is coming down",
      "offset": 2527.76,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "but you'll notice that it takes quite a",
      "offset": 2532,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "bit of time for every single iteration",
      "offset": 2533.599,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "so let's actually address that because",
      "offset": 2535.839,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "we're doing way too much work forwarding",
      "offset": 2537.839,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "and backwarding 220 000 examples",
      "offset": 2539.52,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "in practice what people usually do is",
      "offset": 2542.56,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "they perform forward and backward pass",
      "offset": 2544.319,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "and update on many batches of the data",
      "offset": 2546.8,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "so what we will want to do is we want to",
      "offset": 2549.599,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "randomly select some portion of the data",
      "offset": 2551.68,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "set and that's a mini batch and then",
      "offset": 2553.68,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "only forward backward and update on that",
      "offset": 2555.68,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "little mini batch and then",
      "offset": 2557.52,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "we iterate on those many batches",
      "offset": 2560.079,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "so in pytorch we can for example use",
      "offset": 2562,
      "duration": 3.119
    },
    {
      "lang": "en",
      "text": "storage.randint",
      "offset": 2563.599,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "we can generate numbers between 0 and 5",
      "offset": 2565.119,
      "duration": 5.521
    },
    {
      "lang": "en",
      "text": "and make 32 of them",
      "offset": 2567.599,
      "duration": 3.041
    },
    {
      "lang": "en",
      "text": "i believe the size has to be a",
      "offset": 2572.16,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "tuple",
      "offset": 2574.319,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "in my torch",
      "offset": 2576.4,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "so we can have a tuple 32 of numbers",
      "offset": 2577.76,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "between zero and five but actually we",
      "offset": 2580.56,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "want x dot shape of zero here",
      "offset": 2582.56,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "and so this creates uh integers that",
      "offset": 2585.44,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "index into our data set and there's 32",
      "offset": 2588.16,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "of them so if our mini batch size is 32",
      "offset": 2590.16,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "then we can come here and we can first",
      "offset": 2593.2,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "do a mini batch",
      "offset": 2594.96,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "construct",
      "offset": 2598,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "so in the integers that we want to",
      "offset": 2600.24,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "optimize in this",
      "offset": 2602,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "single iteration",
      "offset": 2604.24,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "are in the ix",
      "offset": 2605.68,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "and then we want to index into",
      "offset": 2607.68,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "x",
      "offset": 2609.92,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "with ix to only grab those rows",
      "offset": 2610.8,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "so we're only getting 32 rows of x",
      "offset": 2614.16,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "and therefore embeddings will again be",
      "offset": 2616.8,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "32 by three by two not two hundred",
      "offset": 2618.4,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "thousand by three by two",
      "offset": 2620.88,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "and then this ix has to be used not just",
      "offset": 2623.28,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "to index into x",
      "offset": 2625.28,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "but also to index into y",
      "offset": 2626.96,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "and now this should be many batches and",
      "offset": 2630.56,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "this should be much much faster so",
      "offset": 2632.64,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "okay so it's instant almost",
      "offset": 2635.44,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "so this way we can run many many",
      "offset": 2637.92,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "examples",
      "offset": 2640.079,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "nearly instantly and decrease the loss",
      "offset": 2641.52,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "much much faster",
      "offset": 2643.839,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "now because we're only dealing with mini",
      "offset": 2645.599,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "batches the quality of our gradient is",
      "offset": 2647.04,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "lower so the direction is not as",
      "offset": 2649.52,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "reliable it's not the actual gradient",
      "offset": 2651.52,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "direction",
      "offset": 2653.44,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "but the gradient direction is good",
      "offset": 2654.64,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "enough even when it's estimating on only",
      "offset": 2656.48,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "32 examples that it is useful and so",
      "offset": 2658.64,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "it's much better to have an approximate",
      "offset": 2662.24,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "gradient and just make more steps than",
      "offset": 2664.56,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "it is to evaluate the exact gradient and",
      "offset": 2666.72,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "take fewer steps so that's why in",
      "offset": 2669.04,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "practice uh this works quite well",
      "offset": 2671.359,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "so let's now continue the optimization",
      "offset": 2674.4,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "let me take out this lost item from here",
      "offset": 2678.319,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "and uh",
      "offset": 2681.68,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "place it over here at the end",
      "offset": 2682.88,
      "duration": 7.199
    },
    {
      "lang": "en",
      "text": "okay so we're hovering around 2.5 or so",
      "offset": 2685.839,
      "duration": 5.681
    },
    {
      "lang": "en",
      "text": "however this is only the loss for that",
      "offset": 2690.079,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "mini batch so let's actually evaluate",
      "offset": 2691.52,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "the loss",
      "offset": 2693.359,
      "duration": 3.121
    },
    {
      "lang": "en",
      "text": "here",
      "offset": 2694.96,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "for all of x",
      "offset": 2696.48,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "and for all of y just so we have a",
      "offset": 2698.24,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "full sense of exactly how all the model",
      "offset": 2700.96,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "is doing right now",
      "offset": 2703.04,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "so right now we're at about 2.7 on the",
      "offset": 2705.52,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "entire training set",
      "offset": 2707.2,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "so let's",
      "offset": 2709.119,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "run the optimization for a while",
      "offset": 2710.16,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "okay right 2.6",
      "offset": 2712.56,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "2.57",
      "offset": 2715.28,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "2.53",
      "offset": 2717.52,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "okay",
      "offset": 2721.92,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "so one issue of course is we don't know",
      "offset": 2722.8,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "if we're stepping too slow or too fast",
      "offset": 2725.04,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "so this point one i just guessed it",
      "offset": 2728.64,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "so one question is how do you determine",
      "offset": 2730.8,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "this learning rate",
      "offset": 2733.2,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "and how do we gain confidence that we're",
      "offset": 2734.88,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "stepping in the right",
      "offset": 2737.359,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "sort of speed so i'll show you one way",
      "offset": 2739.44,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "to determine a reasonable learning rate",
      "offset": 2741.2,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "it works as follows let's reset our",
      "offset": 2743.52,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "parameters",
      "offset": 2746.079,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "to the initial",
      "offset": 2747.52,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "settings",
      "offset": 2749.76,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "and now let's",
      "offset": 2751.04,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "print in every step",
      "offset": 2752.72,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "but let's only do 10 steps or so",
      "offset": 2754.96,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "or maybe maybe 100 steps",
      "offset": 2758.72,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "we want to find like a very reasonable",
      "offset": 2760.96,
      "duration": 2.879
    },
    {
      "lang": "en",
      "text": "set",
      "offset": 2762.72,
      "duration": 3.119
    },
    {
      "lang": "en",
      "text": "search range if you will so for example",
      "offset": 2763.839,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "if this is like very low",
      "offset": 2765.839,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "then",
      "offset": 2767.92,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "we see that the loss is barely",
      "offset": 2769.839,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "decreasing so that's not",
      "offset": 2771.359,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "that's like too low basically so let's",
      "offset": 2773.28,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "try",
      "offset": 2775.599,
      "duration": 3.121
    },
    {
      "lang": "en",
      "text": "this one",
      "offset": 2776.56,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "okay so we're decreasing the loss but",
      "offset": 2778.72,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "like not very quickly so that's a pretty",
      "offset": 2780.16,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "good low range",
      "offset": 2781.76,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "now let's reset it again",
      "offset": 2783.52,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "and now let's try to find the place at",
      "offset": 2785.839,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "which the loss kind of explodes",
      "offset": 2787.28,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "uh so maybe at negative one",
      "offset": 2789.44,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "okay we see that we're minimizing the",
      "offset": 2793.119,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "loss but you see how uh it's kind of",
      "offset": 2794.96,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "unstable it goes up and down quite a bit",
      "offset": 2796.88,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "um so negative one is probably like a",
      "offset": 2799.359,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "fast learning rate let's try negative",
      "offset": 2801.2,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "10.",
      "offset": 2804.319,
      "duration": 2.641
    },
    {
      "lang": "en",
      "text": "okay so this",
      "offset": 2805.76,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "isn't optimizing this is not working",
      "offset": 2806.96,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "very well so negative 10 is way too big",
      "offset": 2808.88,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "negative one was already kind of big",
      "offset": 2810.96,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "um",
      "offset": 2813.119,
      "duration": 3.281
    },
    {
      "lang": "en",
      "text": "so therefore",
      "offset": 2814.8,
      "duration": 2.799
    },
    {
      "lang": "en",
      "text": "negative one was like somewhat",
      "offset": 2816.4,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "reasonable if i reset",
      "offset": 2817.599,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "so i'm thinking that the right learning",
      "offset": 2820.4,
      "duration": 2.959
    },
    {
      "lang": "en",
      "text": "rate is somewhere between",
      "offset": 2821.599,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "uh negative zero point zero zero one and",
      "offset": 2823.359,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "um",
      "offset": 2825.92,
      "duration": 2.32
    },
    {
      "lang": "en",
      "text": "negative one",
      "offset": 2826.96,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "so the way we can do this here is we can",
      "offset": 2828.24,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "use uh torch shot lens space",
      "offset": 2830,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "and we want to basically do something",
      "offset": 2833.119,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "like this between zero and one but",
      "offset": 2834.319,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "um",
      "offset": 2837.76,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "those number of steps is one more",
      "offset": 2839.119,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "parameter that's required let's do a",
      "offset": 2840.96,
      "duration": 5.359
    },
    {
      "lang": "en",
      "text": "thousand steps this creates 1000",
      "offset": 2842.48,
      "duration": 7.28
    },
    {
      "lang": "en",
      "text": "numbers between 0.01 and 1",
      "offset": 2846.319,
      "duration": 5.121
    },
    {
      "lang": "en",
      "text": "but it doesn't really make sense to step",
      "offset": 2849.76,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "between these linearly so instead let me",
      "offset": 2851.44,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "create learning rate exponent",
      "offset": 2853.599,
      "duration": 5.601
    },
    {
      "lang": "en",
      "text": "and instead of 0.001 this will be a",
      "offset": 2856.48,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "negative 3 and this will be a zero",
      "offset": 2859.2,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "and then the actual lrs that we want to",
      "offset": 2861.68,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "search over are going to be 10 to the",
      "offset": 2863.68,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "power of lre",
      "offset": 2865.44,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "so now what we're doing is we're",
      "offset": 2868.319,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "stepping linearly between the exponents",
      "offset": 2869.359,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "of these learning rates this is 0.001",
      "offset": 2871.359,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "and this is 1 because 10 to the power of",
      "offset": 2874.16,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "0 is 1.",
      "offset": 2877.119,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "and therefore we are spaced",
      "offset": 2878.56,
      "duration": 3.519
    },
    {
      "lang": "en",
      "text": "exponentially in this interval",
      "offset": 2880.079,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "so these are the candidate learning",
      "offset": 2882.079,
      "duration": 2.641
    },
    {
      "lang": "en",
      "text": "rates",
      "offset": 2883.44,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "that we want to sort of like search over",
      "offset": 2884.72,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "roughly",
      "offset": 2886.48,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "so now what we're going to do is",
      "offset": 2887.76,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "here we are going to run the",
      "offset": 2890.88,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "optimization for 1000 steps",
      "offset": 2892.48,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "and instead of using a fixed number",
      "offset": 2894.64,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "we are going to use learning rate",
      "offset": 2896.72,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "indexing into here lrs of i",
      "offset": 2899.52,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "and make this i",
      "offset": 2902.48,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "so basically let me reset this to be",
      "offset": 2905.68,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "again starting from random",
      "offset": 2908.24,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "creating these learning rates between",
      "offset": 2910.48,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "negative",
      "offset": 2912.079,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "zero points between 0.001 and um",
      "offset": 2913.28,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "one but exponentially stopped",
      "offset": 2916.88,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "and here what we're doing is we're",
      "offset": 2919.52,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "iterating a thousand times",
      "offset": 2921.44,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "we're going to use the learning rate",
      "offset": 2923.28,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "um that's in the beginning very very low",
      "offset": 2925.599,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "in the beginning is going to be 0.001",
      "offset": 2928.079,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "but by the end it's going to be",
      "offset": 2930.48,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "1.",
      "offset": 2932.24,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "and then we're going to step with that",
      "offset": 2933.44,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "learning rate",
      "offset": 2935.04,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "and now what we want to do is we want to",
      "offset": 2937.119,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "keep track of the uh",
      "offset": 2938.72,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "learning rates that we used and we want",
      "offset": 2944.24,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "to look at the losses",
      "offset": 2945.92,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "that resulted",
      "offset": 2947.92,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "and so here let me",
      "offset": 2949.839,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "track stats",
      "offset": 2952.319,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "so lri.append lr",
      "offset": 2954.16,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "and um lost side that append",
      "offset": 2956.96,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "loss that item",
      "offset": 2960.24,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "okay",
      "offset": 2962.4,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "so again reset everything",
      "offset": 2963.68,
      "duration": 6.72
    },
    {
      "lang": "en",
      "text": "and then run",
      "offset": 2967.2,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "and so basically we started with a very",
      "offset": 2970.4,
      "duration": 2.64
    },
    {
      "lang": "en",
      "text": "low learning rate and we went all the",
      "offset": 2971.76,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "way up to a learning rate of negative",
      "offset": 2973.04,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "one",
      "offset": 2975.119,
      "duration": 2.561
    },
    {
      "lang": "en",
      "text": "and now what we can do is we can plt",
      "offset": 2975.92,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "that plot",
      "offset": 2977.68,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "and we can plot the two so we can plot",
      "offset": 2978.96,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "the learning rates on the x-axis and the",
      "offset": 2981.28,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "losses we saw on the y-axis",
      "offset": 2983.52,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "and often you're going to find that your",
      "offset": 2986.16,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "plot looks something like this",
      "offset": 2988,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "where in the beginning",
      "offset": 2990.16,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "you had very low learning rates so",
      "offset": 2992,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "basically anything",
      "offset": 2993.44,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "barely anything happened",
      "offset": 2994.96,
      "duration": 5.119
    },
    {
      "lang": "en",
      "text": "then we got to like a nice spot here",
      "offset": 2997.04,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "and then as we increase the learning",
      "offset": 3000.079,
      "duration": 2.881
    },
    {
      "lang": "en",
      "text": "rate enough",
      "offset": 3001.44,
      "duration": 2.879
    },
    {
      "lang": "en",
      "text": "we basically started to be kind of",
      "offset": 3002.96,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "unstable here",
      "offset": 3004.319,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "so a good learning rate turns out to be",
      "offset": 3005.92,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "somewhere around here",
      "offset": 3007.68,
      "duration": 5.679
    },
    {
      "lang": "en",
      "text": "um and because we have lri here",
      "offset": 3010.079,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "um",
      "offset": 3013.359,
      "duration": 3.281
    },
    {
      "lang": "en",
      "text": "we actually may want to",
      "offset": 3014.72,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "um",
      "offset": 3016.64,
      "duration": 2.32
    },
    {
      "lang": "en",
      "text": "do not lr",
      "offset": 3019.119,
      "duration": 3.521
    },
    {
      "lang": "en",
      "text": "not the learning rate but the exponent",
      "offset": 3020.96,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "so that would be the lre at i is maybe",
      "offset": 3022.64,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "what we want to log so let me reset this",
      "offset": 3025.359,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "and redo that calculation",
      "offset": 3027.839,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "but now on the x axis we have the",
      "offset": 3030.079,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "[Music]",
      "offset": 3032.16,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "exponent of the learning rate and so we",
      "offset": 3034.079,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "can see the exponent of the learning",
      "offset": 3036.079,
      "duration": 2.721
    },
    {
      "lang": "en",
      "text": "rate that is good to use it would be",
      "offset": 3037.44,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "sort of like roughly in the valley here",
      "offset": 3038.8,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "because here the learning rates are just",
      "offset": 3041.359,
      "duration": 3.521
    },
    {
      "lang": "en",
      "text": "way too low and then here where we",
      "offset": 3042.64,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "expect relatively good learning rates",
      "offset": 3044.88,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "somewhere here and then here things are",
      "offset": 3046.24,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "starting to explode so somewhere around",
      "offset": 3047.92,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "negative one x the exponent of the",
      "offset": 3050.48,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "learning rate is a pretty good setting",
      "offset": 3051.92,
      "duration": 6.159
    },
    {
      "lang": "en",
      "text": "and 10 to the negative one is 0.1 so 0.1",
      "offset": 3054.16,
      "duration": 5.679
    },
    {
      "lang": "en",
      "text": "is actually 0.1 was actually a fairly",
      "offset": 3058.079,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "good learning rate around here",
      "offset": 3059.839,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "and that's what we had in the initial",
      "offset": 3062.16,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "setting",
      "offset": 3063.839,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "but that's roughly how you would",
      "offset": 3065.359,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "determine it and so here now we can take",
      "offset": 3066.559,
      "duration": 6.241
    },
    {
      "lang": "en",
      "text": "out the tracking of these",
      "offset": 3069.52,
      "duration": 6.079
    },
    {
      "lang": "en",
      "text": "and we can just simply set lr to be 10",
      "offset": 3072.8,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "to the negative one or",
      "offset": 3075.599,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "basically otherwise 0.1 as it was before",
      "offset": 3078,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "and now we have some confidence that",
      "offset": 3080.8,
      "duration": 2.64
    },
    {
      "lang": "en",
      "text": "this is actually a fairly good learning",
      "offset": 3081.92,
      "duration": 2.48
    },
    {
      "lang": "en",
      "text": "rate",
      "offset": 3083.44,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "and so now we can do is we can crank up",
      "offset": 3084.4,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "the iterations",
      "offset": 3086.16,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "we can reset our optimization",
      "offset": 3087.76,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "and",
      "offset": 3090.72,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "we can run for a pretty long time using",
      "offset": 3092.079,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "this learning rate",
      "offset": 3094.64,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "oops and we don't want to print that's",
      "offset": 3096.48,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "way too much printing",
      "offset": 3098.559,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "so let me again reset",
      "offset": 3100.079,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "and run ten thousand stops",
      "offset": 3102.079,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "okay so we're 0.2 2.48 roughly let's run",
      "offset": 3108.48,
      "duration": 6.8
    },
    {
      "lang": "en",
      "text": "another 10 000 steps",
      "offset": 3112.079,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "2.46",
      "offset": 3118.64,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "and now let's do one learning rate decay",
      "offset": 3120.24,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "what this means is we're going to take",
      "offset": 3122.4,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "our learning rate and we're going to 10x",
      "offset": 3123.68,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "lower it and so we're at the late stages",
      "offset": 3125.92,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "of training potentially and we may want",
      "offset": 3128.48,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "to go a bit slower let's do one more",
      "offset": 3130.4,
      "duration": 6.4
    },
    {
      "lang": "en",
      "text": "actually at 0.1 just to see if",
      "offset": 3132.88,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "we're making a dent here",
      "offset": 3136.96,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "okay we're still making dent and by the",
      "offset": 3138.88,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "way the",
      "offset": 3140.64,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "bi-gram loss that we achieved last video",
      "offset": 3141.68,
      "duration": 5.439
    },
    {
      "lang": "en",
      "text": "was 2.45 so we've already surpassed the",
      "offset": 3144.24,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "bi-gram model",
      "offset": 3147.119,
      "duration": 3.281
    },
    {
      "lang": "en",
      "text": "and once i get a sense that this is",
      "offset": 3149.2,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "actually kind of starting to plateau off",
      "offset": 3150.4,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "people like to do as i mentioned this",
      "offset": 3152.48,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "learning rate decay so let's try to",
      "offset": 3154.24,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "decay the loss",
      "offset": 3156.24,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "the learning rate i mean",
      "offset": 3157.68,
      "duration": 2.879
    },
    {
      "lang": "en",
      "text": "and we achieve it about 2.3 now",
      "offset": 3162.16,
      "duration": 5.919
    },
    {
      "lang": "en",
      "text": "obviously this is janky and not exactly",
      "offset": 3166.4,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "how you would train it in production but",
      "offset": 3168.079,
      "duration": 3.121
    },
    {
      "lang": "en",
      "text": "this is roughly what you're going",
      "offset": 3170,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "through you first find a decent learning",
      "offset": 3171.2,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "rate using the approach that i showed",
      "offset": 3173.359,
      "duration": 2.321
    },
    {
      "lang": "en",
      "text": "you",
      "offset": 3174.8,
      "duration": 2.4
    },
    {
      "lang": "en",
      "text": "then you start with that learning rate",
      "offset": 3175.68,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "and you train for a while",
      "offset": 3177.2,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "and then at the end people like to do a",
      "offset": 3178.72,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "learning rate decay where you decay the",
      "offset": 3180.24,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "learning rate by say a factor of 10 and",
      "offset": 3182.24,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "you do a few more steps and then you get",
      "offset": 3183.92,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "a trained network roughly speaking",
      "offset": 3186.24,
      "duration": 5.359
    },
    {
      "lang": "en",
      "text": "so we've achieved 2.3 and dramatically",
      "offset": 3188.64,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "improved on the bi-gram language model",
      "offset": 3191.599,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "using this simple neural net as",
      "offset": 3193.52,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "described here",
      "offset": 3195.359,
      "duration": 5.041
    },
    {
      "lang": "en",
      "text": "using these 3 400 parameters now there's",
      "offset": 3197.119,
      "duration": 5.361
    },
    {
      "lang": "en",
      "text": "something we have to be careful with",
      "offset": 3200.4,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "i said that we have a better model",
      "offset": 3202.48,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "because we are achieving a lower loss",
      "offset": 3204.079,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "2.3 much lower than 2.45 with the",
      "offset": 3206.319,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "bi-gram model previously",
      "offset": 3208.88,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "now that's not exactly true and the",
      "offset": 3210.64,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "reason that's not true is that",
      "offset": 3212.559,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "this is actually fairly small model but",
      "offset": 3217.2,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "these models can get larger and larger",
      "offset": 3219.359,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "if you keep adding neurons and",
      "offset": 3221.04,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "parameters so you can imagine that we",
      "offset": 3222.319,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "don't potentially have a thousand",
      "offset": 3224.48,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "parameters we could have 10 000 or 100",
      "offset": 3225.839,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "000 or millions of parameters",
      "offset": 3227.76,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "and as the capacity of the neural",
      "offset": 3229.839,
      "duration": 2.961
    },
    {
      "lang": "en",
      "text": "network grows",
      "offset": 3231.2,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "it becomes more and more capable of",
      "offset": 3232.8,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "overfitting your training set",
      "offset": 3234.96,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "what that means is that the loss on the",
      "offset": 3236.88,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "training set on the data that you're",
      "offset": 3239.04,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "training on will become very very low as",
      "offset": 3240.8,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "low as zero",
      "offset": 3243.44,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "but all that the model is doing is",
      "offset": 3244.8,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "memorizing your training set verbatim so",
      "offset": 3246.24,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "if you take that model and it looks like",
      "offset": 3249.04,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "it's working really well but you try to",
      "offset": 3250.64,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "sample from it you will basically only",
      "offset": 3252.4,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "get examples exactly as they are in the",
      "offset": 3254.319,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "training set you won't get any new data",
      "offset": 3256.4,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "in addition to that if you try to",
      "offset": 3259.04,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "evaluate the loss on some withheld names",
      "offset": 3260.48,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "or other words",
      "offset": 3263.52,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "you will actually see that the loss on",
      "offset": 3264.88,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "those can be very high and so basically",
      "offset": 3266.4,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "it's not a good model",
      "offset": 3269.04,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "so the standard in the field is to split",
      "offset": 3270.8,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "up your data set into three splits as we",
      "offset": 3272.88,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "call them we have the training split the",
      "offset": 3275.28,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "dev split or the validation split",
      "offset": 3277.68,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "and the test split",
      "offset": 3280.079,
      "duration": 3.121
    },
    {
      "lang": "en",
      "text": "so",
      "offset": 3282.319,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "training split",
      "offset": 3283.2,
      "duration": 6.72
    },
    {
      "lang": "en",
      "text": "test or um sorry dev or validation split",
      "offset": 3285.52,
      "duration": 7.599
    },
    {
      "lang": "en",
      "text": "and test split and typically this would",
      "offset": 3289.92,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "be say eighty percent of your data set",
      "offset": 3293.119,
      "duration": 3.281
    },
    {
      "lang": "en",
      "text": "this could be ten percent and this ten",
      "offset": 3294.88,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "percent roughly",
      "offset": 3296.4,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "so you have these three splits of the",
      "offset": 3298.319,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "data",
      "offset": 3300,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "now these eighty percent of your",
      "offset": 3301.359,
      "duration": 3.281
    },
    {
      "lang": "en",
      "text": "trainings of the data set the training",
      "offset": 3302.8,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "set is used to optimize the parameters",
      "offset": 3304.64,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "of the model just like we're doing here",
      "offset": 3306.88,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "using gradient descent",
      "offset": 3308.64,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "these 10 percent of the",
      "offset": 3310.72,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "examples the dev or validation split",
      "offset": 3312.48,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "they're used for development over all",
      "offset": 3314.72,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "the hyper parameters of your model so",
      "offset": 3316.8,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "hyper parameters are for example the",
      "offset": 3319.119,
      "duration": 3.521
    },
    {
      "lang": "en",
      "text": "size of this hidden layer",
      "offset": 3321.04,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "the size of the embedding so this is a",
      "offset": 3322.64,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "hundred or a two for us but we could try",
      "offset": 3324.72,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "different things",
      "offset": 3326.48,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "the strength of the regularization which",
      "offset": 3328,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "we aren't using yet so far",
      "offset": 3329.92,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "so there's lots of different hybrid",
      "offset": 3331.92,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "parameters and settings that go into",
      "offset": 3333.119,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "defining your neural net and you can try",
      "offset": 3334.72,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "many different variations of them and",
      "offset": 3336.8,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "see whichever one works best on your",
      "offset": 3338.8,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "validation split",
      "offset": 3341.119,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "so this is used to train the parameters",
      "offset": 3343.119,
      "duration": 5.521
    },
    {
      "lang": "en",
      "text": "this is used to train the hyperprimers",
      "offset": 3345.76,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "and test split is used to evaluate",
      "offset": 3348.64,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "basically the performance of the model",
      "offset": 3351.68,
      "duration": 2.399
    },
    {
      "lang": "en",
      "text": "at the end",
      "offset": 3353.04,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "so we're only evaluating the loss on the",
      "offset": 3354.079,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "test plate very very sparingly and very",
      "offset": 3355.92,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "few times because every single time you",
      "offset": 3357.839,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "evaluate your test loss and you learn",
      "offset": 3360.4,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "something from it",
      "offset": 3362.559,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "you are basically starting to also train",
      "offset": 3363.839,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "on the test split",
      "offset": 3366.48,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "so you are only allowed to test the loss",
      "offset": 3368,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "on a test",
      "offset": 3370.48,
      "duration": 2.639
    },
    {
      "lang": "en",
      "text": "set",
      "offset": 3371.599,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "very very few times otherwise you risk",
      "offset": 3373.119,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "overfitting to it as well as you",
      "offset": 3375.68,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "experiment on your model",
      "offset": 3377.599,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "so let's also split up our training data",
      "offset": 3379.52,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "into train dev and test and then we are",
      "offset": 3382,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "going to train on train",
      "offset": 3385.04,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "and only evaluate on tests very very",
      "offset": 3386.559,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "sparingly okay so here we go",
      "offset": 3388.24,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "here is where we took all the words and",
      "offset": 3391.119,
      "duration": 5.041
    },
    {
      "lang": "en",
      "text": "put them into x and y tensors",
      "offset": 3393.44,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "so instead let me create a new cell here",
      "offset": 3396.16,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "and let me just copy paste some code",
      "offset": 3398.24,
      "duration": 2.559
    },
    {
      "lang": "en",
      "text": "here",
      "offset": 3399.92,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "because i don't think it's that",
      "offset": 3400.799,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "complex but",
      "offset": 3403.04,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "we're going to try to save a little bit",
      "offset": 3405.359,
      "duration": 2.24
    },
    {
      "lang": "en",
      "text": "of time",
      "offset": 3406.48,
      "duration": 3.119
    },
    {
      "lang": "en",
      "text": "i'm converting this to be a function now",
      "offset": 3407.599,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "and this function takes some list of",
      "offset": 3409.599,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "words and builds the arrays x and y for",
      "offset": 3411.52,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "those words only",
      "offset": 3414.319,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "and then here i am shuffling up all the",
      "offset": 3416.559,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "words so these are the input words that",
      "offset": 3419.28,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "we get",
      "offset": 3421.359,
      "duration": 3.521
    },
    {
      "lang": "en",
      "text": "we are randomly shuffling them all up",
      "offset": 3422.24,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "and then um",
      "offset": 3424.88,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "we're going to",
      "offset": 3426.64,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "set n1 to be",
      "offset": 3428.16,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "the number of examples that there's 80",
      "offset": 3429.92,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "of the words and n2 to be",
      "offset": 3431.68,
      "duration": 2.879
    },
    {
      "lang": "en",
      "text": "90",
      "offset": 3433.76,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "of the way of the words so basically if",
      "offset": 3434.559,
      "duration": 7.04
    },
    {
      "lang": "en",
      "text": "len of words is 32 000 n1 is",
      "offset": 3436.96,
      "duration": 7.359
    },
    {
      "lang": "en",
      "text": "well sorry i should probably run this",
      "offset": 3441.599,
      "duration": 6.561
    },
    {
      "lang": "en",
      "text": "n1 is 25 000 and n2 is 28 000.",
      "offset": 3444.319,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "and so here we see that",
      "offset": 3448.16,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "i'm calling build data set to build the",
      "offset": 3449.839,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "training set x and y",
      "offset": 3452,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "by indexing into up to and one so we're",
      "offset": 3453.92,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "going to have only 25 000 training words",
      "offset": 3456.72,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "and then we're going to have",
      "offset": 3459.68,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "roughly",
      "offset": 3462.48,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "n2 minus n1",
      "offset": 3464.24,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "3 3 000 validation examples or dev",
      "offset": 3466.079,
      "duration": 7.28
    },
    {
      "lang": "en",
      "text": "examples and we're going to have",
      "offset": 3469.52,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "when of words basically minus and two",
      "offset": 3473.52,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "or",
      "offset": 3477.28,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "3 204 examples",
      "offset": 3478.16,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "here for the test set",
      "offset": 3480.96,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "so",
      "offset": 3483.68,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "now we have x's and y's",
      "offset": 3484.799,
      "duration": 5.681
    },
    {
      "lang": "en",
      "text": "for all those three splits",
      "offset": 3487.119,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "oh yeah i'm printing their size here",
      "offset": 3493.359,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "inside the function as well",
      "offset": 3494.559,
      "duration": 2.961
    },
    {
      "lang": "en",
      "text": "but here we don't have words but these",
      "offset": 3498.88,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "are already the individual examples made",
      "offset": 3500.559,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "from those words",
      "offset": 3502.559,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "so let's now scroll down here",
      "offset": 3505.2,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "and the data set now for training is",
      "offset": 3507.599,
      "duration": 5.601
    },
    {
      "lang": "en",
      "text": "more like this",
      "offset": 3511.2,
      "duration": 6.639
    },
    {
      "lang": "en",
      "text": "and then when we reset the network",
      "offset": 3513.2,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "when we're training we're only going to",
      "offset": 3518.319,
      "duration": 5.601
    },
    {
      "lang": "en",
      "text": "be training using x train",
      "offset": 3520.079,
      "duration": 7.681
    },
    {
      "lang": "en",
      "text": "x train and y train",
      "offset": 3523.92,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "so that's the only thing we're training",
      "offset": 3527.76,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "on",
      "offset": 3529.28,
      "duration": 2.559
    },
    {
      "lang": "en",
      "text": "let's see where we are on the",
      "offset": 3537.68,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "single batch",
      "offset": 3540.16,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "let's now train maybe a few more steps",
      "offset": 3542.16,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "training neural networks can take a",
      "offset": 3548,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "while usually you don't do it inline you",
      "offset": 3549.359,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "launch a bunch of jobs and you wait for",
      "offset": 3551.2,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "them to finish um can take in multiple",
      "offset": 3552.72,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "days and so on",
      "offset": 3555.28,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "luckily this is a very small network",
      "offset": 3556.64,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "okay so the loss is pretty good",
      "offset": 3560.96,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "oh we accidentally used a learning rate",
      "offset": 3563.28,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "that is way too low",
      "offset": 3565.68,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "so let me actually come back",
      "offset": 3567.52,
      "duration": 6.839
    },
    {
      "lang": "en",
      "text": "we use the decay learning rate of 0.01",
      "offset": 3569.76,
      "duration": 4.599
    },
    {
      "lang": "en",
      "text": "so this will train much faster",
      "offset": 3575.119,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "and then here when we evaluate",
      "offset": 3577.119,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "let's use the dep set here",
      "offset": 3579.28,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "xdev",
      "offset": 3582.319,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "and ydev to evaluate the loss",
      "offset": 3583.599,
      "duration": 5.121
    },
    {
      "lang": "en",
      "text": "okay",
      "offset": 3587.119,
      "duration": 3.521
    },
    {
      "lang": "en",
      "text": "and let's now decay the learning rate",
      "offset": 3588.72,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "and only do say 10 000 examples",
      "offset": 3590.64,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "and let's evaluate the dev loss",
      "offset": 3595.44,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "ones here",
      "offset": 3597.68,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "okay so we're getting about 2.3 on dev",
      "offset": 3599.2,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "and so the neural network when it was",
      "offset": 3601.44,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "training did not see these dev examples",
      "offset": 3602.64,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "it hasn't optimized on them and yet",
      "offset": 3605.44,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "when we evaluate the loss on these dev",
      "offset": 3608,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "we actually get a pretty decent loss",
      "offset": 3610.16,
      "duration": 5.919
    },
    {
      "lang": "en",
      "text": "and so we can also look at what the",
      "offset": 3612.319,
      "duration": 6.641
    },
    {
      "lang": "en",
      "text": "loss is on all of training set",
      "offset": 3616.079,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "oops",
      "offset": 3618.96,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "and so we see that the training and the",
      "offset": 3620.559,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "dev loss are about equal so we're not",
      "offset": 3622.319,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "over fitting",
      "offset": 3624.64,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "um this model is not powerful enough to",
      "offset": 3625.76,
      "duration": 5.359
    },
    {
      "lang": "en",
      "text": "just be purely memorizing the data and",
      "offset": 3628.319,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "so far we are what's called underfitting",
      "offset": 3631.119,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "because the training loss and the dev or",
      "offset": 3633.839,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "test losses are roughly equal so what",
      "offset": 3635.92,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "that typically means is that our network",
      "offset": 3638.16,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "is very tiny very small and we expect to",
      "offset": 3640.319,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "make performance improvements by scaling",
      "offset": 3643.44,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "up the size of this neural net so let's",
      "offset": 3646.079,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "do that now so let's come over here",
      "offset": 3647.92,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "and let's increase the size of the",
      "offset": 3650.16,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "neural net the easiest way to do this is",
      "offset": 3651.599,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "we can come here to the hidden layer",
      "offset": 3653.839,
      "duration": 3.121
    },
    {
      "lang": "en",
      "text": "which currently has 100 neurons and",
      "offset": 3655.2,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "let's just bump this up so let's do 300",
      "offset": 3656.96,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "neurons",
      "offset": 3658.96,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "and then this is also 300 biases and",
      "offset": 3660.64,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "here we have 300 inputs into the final",
      "offset": 3663.44,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "layer",
      "offset": 3665.92,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "so",
      "offset": 3667.28,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "let's initialize our neural net we now",
      "offset": 3668.64,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "have ten thousand ex ten thousand",
      "offset": 3670.88,
      "duration": 2.64
    },
    {
      "lang": "en",
      "text": "parameters instead of three thousand",
      "offset": 3672.24,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "parameters",
      "offset": 3673.52,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "and then we're not using this",
      "offset": 3675.839,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "and then here what i'd like to do is i'd",
      "offset": 3678.24,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "like to actually uh keep track of uh",
      "offset": 3679.599,
      "duration": 5.041
    },
    {
      "lang": "en",
      "text": "tap",
      "offset": 3683.28,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "um",
      "offset": 3684.64,
      "duration": 2.24
    },
    {
      "lang": "en",
      "text": "okay let's just do this let's keep stats",
      "offset": 3687.599,
      "duration": 3.281
    },
    {
      "lang": "en",
      "text": "again",
      "offset": 3689.839,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "and here when we're keeping track of the",
      "offset": 3690.88,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "loss let's just also keep track of the",
      "offset": 3694.88,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "steps and let's just have i here",
      "offset": 3697.839,
      "duration": 6.321
    },
    {
      "lang": "en",
      "text": "and let's train on thirty thousand",
      "offset": 3700.88,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "or rather say",
      "offset": 3704.16,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "okay let's try thirty thousand",
      "offset": 3706.079,
      "duration": 5.121
    },
    {
      "lang": "en",
      "text": "and we are at point one",
      "offset": 3708.16,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "and",
      "offset": 3711.2,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "we should be able to run this",
      "offset": 3712.48,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "and optimize the neural net",
      "offset": 3714.88,
      "duration": 4.52
    },
    {
      "lang": "en",
      "text": "and then here basically i want to",
      "offset": 3717.28,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "plt.plot",
      "offset": 3719.4,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "the steps",
      "offset": 3720.96,
      "duration": 5.24
    },
    {
      "lang": "en",
      "text": "against the loss",
      "offset": 3722.4,
      "duration": 3.8
    },
    {
      "lang": "en",
      "text": "so these are the x's and y's",
      "offset": 3729.04,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "and this is",
      "offset": 3731.52,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "the loss function and how it's being",
      "offset": 3733.52,
      "duration": 3.039
    },
    {
      "lang": "en",
      "text": "optimized",
      "offset": 3735.359,
      "duration": 2.801
    },
    {
      "lang": "en",
      "text": "now you see that there's quite a bit of",
      "offset": 3736.559,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "thickness to this and that's because we",
      "offset": 3738.16,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "are optimizing over these mini batches",
      "offset": 3739.839,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "and the mini batches create a little bit",
      "offset": 3741.76,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "of noise",
      "offset": 3743.44,
      "duration": 2.399
    },
    {
      "lang": "en",
      "text": "in this",
      "offset": 3744.48,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "uh where are we in the def set we are at",
      "offset": 3745.839,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "2.5 so we still haven't optimized this",
      "offset": 3748.079,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "neural net very well",
      "offset": 3750.559,
      "duration": 2.721
    },
    {
      "lang": "en",
      "text": "and that's probably because we made it",
      "offset": 3752.079,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "bigger it might take longer for this",
      "offset": 3753.28,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "neural net to converge",
      "offset": 3754.799,
      "duration": 2.721
    },
    {
      "lang": "en",
      "text": "um",
      "offset": 3756.4,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "and so let's continue training",
      "offset": 3757.52,
      "duration": 5.279
    },
    {
      "lang": "en",
      "text": "um",
      "offset": 3760.799,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "yeah let's just continue training",
      "offset": 3762.799,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "one possibility is that the batch size",
      "offset": 3766.559,
      "duration": 2.721
    },
    {
      "lang": "en",
      "text": "is so low",
      "offset": 3768,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "that uh we just have way too much noise",
      "offset": 3769.28,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "in the training and we may want to",
      "offset": 3771.68,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "increase the batch size so that we have",
      "offset": 3773.359,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "a bit more um correct gradient and we're",
      "offset": 3774.72,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "not thrashing too much and we can",
      "offset": 3777.359,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "actually like optimize more properly",
      "offset": 3779.68,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "okay",
      "offset": 3787.92,
      "duration": 2.56
    },
    {
      "lang": "en",
      "text": "this will now become meaningless because",
      "offset": 3788.799,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "we've reinitialized these so",
      "offset": 3790.48,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "yeah this looks not",
      "offset": 3793.2,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "pleasing right now but there probably is",
      "offset": 3794.88,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "like a tiny improvement but it's so hard",
      "offset": 3796.96,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "to tell",
      "offset": 3798.559,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "let's go again",
      "offset": 3800.319,
      "duration": 5.121
    },
    {
      "lang": "en",
      "text": "2.52",
      "offset": 3802.559,
      "duration": 2.881
    },
    {
      "lang": "en",
      "text": "let's try to decrease the learning rate",
      "offset": 3805.52,
      "duration": 5.079
    },
    {
      "lang": "en",
      "text": "by factor two",
      "offset": 3807.039,
      "duration": 3.56
    },
    {
      "lang": "en",
      "text": "okay we're at 2.32",
      "offset": 3830,
      "duration": 5.88
    },
    {
      "lang": "en",
      "text": "let's continue training",
      "offset": 3832.24,
      "duration": 3.64
    },
    {
      "lang": "en",
      "text": "we basically expect to see a lower loss",
      "offset": 3845.68,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "than what we had before because now we",
      "offset": 3847.52,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "have a much much bigger model and we",
      "offset": 3848.96,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "were under fitting so we'd expect that",
      "offset": 3850.88,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "increasing the size of the model should",
      "offset": 3852.799,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "help the neural net",
      "offset": 3854.079,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "2.32 okay so that's not happening too",
      "offset": 3856.079,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "well",
      "offset": 3858.4,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "now one other concern is that even",
      "offset": 3859.44,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "though we've made the 10h layer here or",
      "offset": 3861.2,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "the hidden layer much much bigger it",
      "offset": 3863.599,
      "duration": 3.121
    },
    {
      "lang": "en",
      "text": "could be that the bottleneck of the",
      "offset": 3865.52,
      "duration": 3.279
    },
    {
      "lang": "en",
      "text": "network right now are these embeddings",
      "offset": 3866.72,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "that are two dimensional it can be that",
      "offset": 3868.799,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "we're just cramming way too many",
      "offset": 3870.799,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "characters into just two dimensions and",
      "offset": 3872.079,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "the neural net is not able to really use",
      "offset": 3874.079,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "that space effectively and that that is",
      "offset": 3876.48,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "sort of like the bottleneck to our",
      "offset": 3878.72,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "network's performance",
      "offset": 3879.92,
      "duration": 5.119
    },
    {
      "lang": "en",
      "text": "okay 2.23 so just by decreasing the",
      "offset": 3882.319,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "learning rate i was able to make quite a",
      "offset": 3885.039,
      "duration": 3.121
    },
    {
      "lang": "en",
      "text": "bit of progress let's run this one more",
      "offset": 3886.4,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "time",
      "offset": 3888.16,
      "duration": 2.24
    },
    {
      "lang": "en",
      "text": "and then evaluate the training and the",
      "offset": 3891.599,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "dev loss",
      "offset": 3893.28,
      "duration": 2.559
    },
    {
      "lang": "en",
      "text": "now one more thing after training that",
      "offset": 3896.64,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "i'd like to do is i'd like to visualize",
      "offset": 3898.16,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "the um",
      "offset": 3900.88,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "embedding vectors for these",
      "offset": 3902.72,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "characters before we scale up the",
      "offset": 3905.599,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "embedding size from two",
      "offset": 3907.92,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "because we'd like to make uh this",
      "offset": 3909.92,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "bottleneck potentially go away",
      "offset": 3911.28,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "but once i make this greater than two we",
      "offset": 3913.52,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "won't be able to visualize them",
      "offset": 3915.28,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "so here",
      "offset": 3917.28,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "okay we're at 2.23 and 2.24",
      "offset": 3918.24,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "so um we're not improving much more and",
      "offset": 3921.52,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "maybe the bottleneck now is the",
      "offset": 3924.16,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "character embedding size which is two",
      "offset": 3925.52,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "so here i have a bunch of code that will",
      "offset": 3928.48,
      "duration": 2.559
    },
    {
      "lang": "en",
      "text": "create a figure",
      "offset": 3929.92,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "and then we're going to visualize",
      "offset": 3931.039,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "the embeddings that were trained by the",
      "offset": 3934,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "neural net",
      "offset": 3935.52,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "on these characters because right now",
      "offset": 3936.72,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "the embedding has just two so we can",
      "offset": 3938.48,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "visualize all the characters with the x",
      "offset": 3940.4,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "and the y coordinates as the two",
      "offset": 3942.24,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "embedding locations for each of these",
      "offset": 3943.92,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "characters",
      "offset": 3946.079,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "and so here are the x coordinates and",
      "offset": 3947.76,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "the y coordinates which are the columns",
      "offset": 3950,
      "duration": 2.799
    },
    {
      "lang": "en",
      "text": "of c",
      "offset": 3951.76,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "and then for each one i also include the",
      "offset": 3952.799,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "text of the little character",
      "offset": 3955.44,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "so here what we see is actually kind of",
      "offset": 3958.319,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "interesting",
      "offset": 3959.839,
      "duration": 2.401
    },
    {
      "lang": "en",
      "text": "the network has basically learned to",
      "offset": 3962.4,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "separate out the characters and cluster",
      "offset": 3964.48,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "them a little bit uh so for example you",
      "offset": 3966.16,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "see how the vowels",
      "offset": 3968.319,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "a e i o u are clustered up here",
      "offset": 3969.839,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "so that's telling us that is that the",
      "offset": 3972.799,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "neural net treats these is very similar",
      "offset": 3974.24,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "right because when they feed into the",
      "offset": 3976.16,
      "duration": 2.399
    },
    {
      "lang": "en",
      "text": "neural net",
      "offset": 3977.44,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "the embedding uh for all these",
      "offset": 3978.559,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "characters is very similar and so the",
      "offset": 3980.72,
      "duration": 3.119
    },
    {
      "lang": "en",
      "text": "neural net thinks that they're very",
      "offset": 3982.64,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "similar and kind of like interchangeable",
      "offset": 3983.839,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "if that makes sense",
      "offset": 3985.76,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "um",
      "offset": 3987.359,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "then the the points that are like really",
      "offset": 3989.2,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "far away are for example q q is kind of",
      "offset": 3991.119,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "treated as an exception and q has a very",
      "offset": 3993.44,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "special",
      "offset": 3995.68,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "embedding vector so to speak",
      "offset": 3996.64,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "similarly dot which is a special",
      "offset": 3998.72,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "character is all the way out here",
      "offset": 4000.4,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "and a lot of the other letters are sort",
      "offset": 4002.48,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "of like clustered up here and so it's",
      "offset": 4004.24,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "kind of interesting that there's a",
      "offset": 4006.64,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "little bit of structure here",
      "offset": 4008,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "after the training",
      "offset": 4010.079,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "and it's not definitely not random and",
      "offset": 4011.599,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "these embeddings make sense",
      "offset": 4013.68,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "so we're now going to scale up the",
      "offset": 4015.92,
      "duration": 3.119
    },
    {
      "lang": "en",
      "text": "embedding size and won't be able to",
      "offset": 4017.76,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "visualize it directly but we expect that",
      "offset": 4019.039,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "because we're under fitting",
      "offset": 4021.68,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "and we made this layer much bigger and",
      "offset": 4023.68,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "did not sufficiently improve the loss",
      "offset": 4026.079,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "we're thinking that the um",
      "offset": 4028.16,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "constraint to better performance right",
      "offset": 4030.799,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "now could be these embedding pictures so",
      "offset": 4032.559,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "let's make them bigger okay so let's",
      "offset": 4035.119,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "scroll up here",
      "offset": 4036.64,
      "duration": 2.399
    },
    {
      "lang": "en",
      "text": "and now we don't have two dimensional",
      "offset": 4037.839,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "embeddings we are going to have",
      "offset": 4039.039,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "say 10 dimensional embeddings for each",
      "offset": 4041.599,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "word",
      "offset": 4043.44,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "then",
      "offset": 4045.119,
      "duration": 5.361
    },
    {
      "lang": "en",
      "text": "this layer will receive 3 times 10 so 30",
      "offset": 4046.16,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "inputs",
      "offset": 4050.48,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "will go into",
      "offset": 4051.52,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "the hidden layer",
      "offset": 4053.76,
      "duration": 3.279
    },
    {
      "lang": "en",
      "text": "let's also make the hidden layer a bit",
      "offset": 4055.76,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "smaller so instead of 300 let's just do",
      "offset": 4057.039,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "200 neurons in that hidden layer",
      "offset": 4058.88,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "so now the total number of elements will",
      "offset": 4061.599,
      "duration": 5.601
    },
    {
      "lang": "en",
      "text": "be slightly bigger at 11 000",
      "offset": 4063.52,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "and then here we have to be a bit",
      "offset": 4067.2,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "careful because um",
      "offset": 4068.4,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "okay the learning rate we set to 0.1",
      "offset": 4070.559,
      "duration": 5.441
    },
    {
      "lang": "en",
      "text": "here we are hardcoded in six and",
      "offset": 4073.2,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "obviously if you're working in",
      "offset": 4076,
      "duration": 1.76
    },
    {
      "lang": "en",
      "text": "production you don't wanna be",
      "offset": 4076.88,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "hard-coding magic numbers but instead of",
      "offset": 4077.76,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "six this should now be thirty",
      "offset": 4080,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "um",
      "offset": 4082.319,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "and let's run for fifty thousand",
      "offset": 4084.079,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "iterations and let me split out the",
      "offset": 4085.68,
      "duration": 5.119
    },
    {
      "lang": "en",
      "text": "initialization here outside",
      "offset": 4087.92,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "so that when we run this cell multiple",
      "offset": 4090.799,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "times it's not going to wipe out",
      "offset": 4092.64,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "our loss",
      "offset": 4094.4,
      "duration": 2.48
    },
    {
      "lang": "en",
      "text": "in addition to that",
      "offset": 4097.52,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "here",
      "offset": 4099.44,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "let's instead of logging lost.item let's",
      "offset": 4100.64,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "actually",
      "offset": 4102.64,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "log the",
      "offset": 4103.44,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "let's",
      "offset": 4105.52,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "do log 10",
      "offset": 4106.56,
      "duration": 6.159
    },
    {
      "lang": "en",
      "text": "i believe that's a function of the loss",
      "offset": 4108.56,
      "duration": 5.759
    },
    {
      "lang": "en",
      "text": "and i'll show you why in a second let's",
      "offset": 4112.719,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "optimize this",
      "offset": 4114.319,
      "duration": 2.721
    },
    {
      "lang": "en",
      "text": "basically i'd like to plot the log loss",
      "offset": 4117.12,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "instead of the loss because when you",
      "offset": 4119.279,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "plot the loss many times it can have",
      "offset": 4120.88,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "this hockey stick appearance and log",
      "offset": 4122.239,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "squashes it in",
      "offset": 4124.96,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "uh so it just kind of like looks nicer",
      "offset": 4126.799,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "so the x-axis is step i",
      "offset": 4128.96,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "and the y-axis will be the loss i",
      "offset": 4131.679,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "and then here this is 30.",
      "offset": 4140.799,
      "duration": 6.92
    },
    {
      "lang": "en",
      "text": "ideally we wouldn't be hard-coding these",
      "offset": 4143.12,
      "duration": 4.599
    },
    {
      "lang": "en",
      "text": "okay so let's look at the loss",
      "offset": 4148.719,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "okay it's again very thick because the",
      "offset": 4151.44,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "mini batch size is very small but the",
      "offset": 4153.44,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "total loss over the training set is 2.3",
      "offset": 4155.52,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "and the the tests or the def set is 2.38",
      "offset": 4158,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "as well",
      "offset": 4160.88,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "so so far so good uh let's try to now",
      "offset": 4161.92,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "decrease the learning rate",
      "offset": 4164.08,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "by a factor of 10",
      "offset": 4165.679,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "and train for another 50 000 iterations",
      "offset": 4169.12,
      "duration": 4.44
    },
    {
      "lang": "en",
      "text": "we'd hope that we would be able to beat",
      "offset": 4175.279,
      "duration": 5.321
    },
    {
      "lang": "en",
      "text": "uh 2.32",
      "offset": 4177.279,
      "duration": 3.321
    },
    {
      "lang": "en",
      "text": "but again we're just kind of like doing",
      "offset": 4183.279,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "this very haphazardly so i don't",
      "offset": 4184.719,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "actually have confidence that our",
      "offset": 4186.799,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "learning rate is set very well that our",
      "offset": 4188.48,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "learning rate decay which we just do",
      "offset": 4190.48,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "at random is set very well",
      "offset": 4192.799,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "and um so the optimization here is kind",
      "offset": 4195.199,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "of suspect to be honest and this is not",
      "offset": 4197.52,
      "duration": 2.719
    },
    {
      "lang": "en",
      "text": "how you would do it typically in",
      "offset": 4199.28,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "production in production you would",
      "offset": 4200.239,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "create parameters or hyper parameters",
      "offset": 4202,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "out of all these settings and then you",
      "offset": 4204.08,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "would run lots of experiments and see",
      "offset": 4205.679,
      "duration": 5.601
    },
    {
      "lang": "en",
      "text": "whichever ones are working well for you",
      "offset": 4207.36,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "okay",
      "offset": 4211.28,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "so we have 2.17 now and 2.2 okay so you",
      "offset": 4212.64,
      "duration": 6.8
    },
    {
      "lang": "en",
      "text": "see how the training and the validation",
      "offset": 4216.8,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "performance are starting to slightly",
      "offset": 4219.44,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "slowly depart",
      "offset": 4221.6,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "so maybe we're getting the sense that",
      "offset": 4223.12,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "the neural net",
      "offset": 4224.8,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "is getting good enough or",
      "offset": 4226.08,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "that number of parameters is large",
      "offset": 4228.4,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "enough that we are slowly starting to",
      "offset": 4230.32,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "overfit",
      "offset": 4232.32,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "let's maybe run one more iteration of",
      "offset": 4234.159,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "this",
      "offset": 4236.4,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "and see where we get",
      "offset": 4237.92,
      "duration": 2.799
    },
    {
      "lang": "en",
      "text": "but yeah basically you would be running",
      "offset": 4241.52,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "lots of experiments and then you are",
      "offset": 4243.28,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "slowly scrutinizing whichever ones give",
      "offset": 4244.8,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "you the best depth performance and then",
      "offset": 4246.4,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "once you find all the",
      "offset": 4248.56,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "hyper parameters that make your dev",
      "offset": 4250,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "performance good you take that model and",
      "offset": 4251.76,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "you evaluate the test set performance a",
      "offset": 4254,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "single time and that's the number that",
      "offset": 4255.84,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "you report in your paper or wherever",
      "offset": 4257.679,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "else you want to talk about and brag",
      "offset": 4259.679,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "about your model",
      "offset": 4261.199,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "so let's then rerun the plot and rerun",
      "offset": 4265.52,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "the train and death",
      "offset": 4268.159,
      "duration": 2.801
    },
    {
      "lang": "en",
      "text": "and because we're getting lower loss now",
      "offset": 4271.12,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "it is the case that the embedding size",
      "offset": 4272.8,
      "duration": 6.64
    },
    {
      "lang": "en",
      "text": "of these was holding us back very likely",
      "offset": 4275.199,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "okay so 2.162.19 is what we're roughly",
      "offset": 4280.08,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "getting",
      "offset": 4282.88,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "so there's many ways to go from many",
      "offset": 4284.48,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "ways to go from here we can continue",
      "offset": 4286.48,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "tuning the optimization",
      "offset": 4288.239,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "we can continue for example playing with",
      "offset": 4290.239,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "the sizes of the neural net or we can",
      "offset": 4292.08,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "increase the number of uh",
      "offset": 4294.08,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "words or characters in our case that we",
      "offset": 4296.159,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "are taking as an input so instead of",
      "offset": 4298.32,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "just three characters we could be taking",
      "offset": 4299.84,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "more characters as an input and that",
      "offset": 4301.44,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "could further improve the loss",
      "offset": 4303.6,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "okay so i changed the code slightly so",
      "offset": 4306.48,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "we have here 200 000 steps of the",
      "offset": 4308.159,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "optimization and in the first 100 000",
      "offset": 4310.48,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "we're using a learning rate of 0.1 and",
      "offset": 4312.8,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "then in the next 100 000 we're using a",
      "offset": 4314.56,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "learning rate of 0.01",
      "offset": 4316.32,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "this is the loss that i achieve",
      "offset": 4318.48,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "and these are the performance on the",
      "offset": 4320.4,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "training and validation loss",
      "offset": 4321.92,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "and in particular the best validation",
      "offset": 4323.92,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "loss i've been able to obtain in the",
      "offset": 4325.76,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "last 30 minutes or so is 2.17",
      "offset": 4327.04,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "so now i invite you to beat this number",
      "offset": 4330.56,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "and you have quite a few knobs available",
      "offset": 4332.8,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "to you to i think surpass this number",
      "offset": 4334.4,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "so number one you can of course change",
      "offset": 4337.04,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "the number of neurons in the hidden",
      "offset": 4338.719,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "layer of this model you can change the",
      "offset": 4340.32,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "dimensionality of the embedding",
      "offset": 4342.239,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "lookup table",
      "offset": 4344.239,
      "duration": 2.561
    },
    {
      "lang": "en",
      "text": "you can change the number of characters",
      "offset": 4345.44,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "that are feeding in as an input",
      "offset": 4346.8,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "as the context into this model",
      "offset": 4349.76,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "and then of course you can change the",
      "offset": 4352.4,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "details of the optimization how long are",
      "offset": 4353.52,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "we running what is the learning rate how",
      "offset": 4355.52,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "does it change over time",
      "offset": 4357.52,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "how does it decay",
      "offset": 4359.36,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "you can change the batch size and you",
      "offset": 4361.12,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "may be able to actually achieve a much",
      "offset": 4362.64,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "better convergence speed",
      "offset": 4364.32,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "in terms of",
      "offset": 4366.4,
      "duration": 2.64
    },
    {
      "lang": "en",
      "text": "how many seconds or minutes it takes to",
      "offset": 4367.52,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "train the model and get",
      "offset": 4369.04,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "your result in terms of really good",
      "offset": 4371.52,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "loss",
      "offset": 4374,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "and then of course i actually invite you",
      "offset": 4375.679,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "to read this paper it is 19 pages but at",
      "offset": 4377.12,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "this point you should actually be able",
      "offset": 4379.6,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "to read a good chunk of this paper and",
      "offset": 4380.96,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "understand",
      "offset": 4383.12,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "pretty good chunks of it",
      "offset": 4384.88,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "and this paper also has quite a few",
      "offset": 4386.48,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "ideas for improvements that you can play",
      "offset": 4388.08,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "with",
      "offset": 4389.84,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "so all of those are not available to you",
      "offset": 4391.04,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "and you should be able to beat this",
      "offset": 4393.12,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "number i'm leaving that as an exercise",
      "offset": 4394.56,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "to the reader and that's it for now and",
      "offset": 4396.4,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "i'll see you next time",
      "offset": 4398.88,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "before we wrap up i also wanted to show",
      "offset": 4404.239,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "how you would sample from the model",
      "offset": 4405.76,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "so we're going to generate 20 samples",
      "offset": 4408.159,
      "duration": 5.601
    },
    {
      "lang": "en",
      "text": "at first we begin with all dots so",
      "offset": 4411.04,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "that's the context",
      "offset": 4413.76,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "and then until we generate",
      "offset": 4415.36,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "the zeroth character again",
      "offset": 4417.44,
      "duration": 6.239
    },
    {
      "lang": "en",
      "text": "we're going to embed the current context",
      "offset": 4420,
      "duration": 7.199
    },
    {
      "lang": "en",
      "text": "using the embedding table c now usually",
      "offset": 4423.679,
      "duration": 5.761
    },
    {
      "lang": "en",
      "text": "uh here the first dimension was the size",
      "offset": 4427.199,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "of the training set but here we're only",
      "offset": 4429.44,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "working with a single example that we're",
      "offset": 4431.12,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "generating so this is just the mission",
      "offset": 4432.4,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "one just for simplicity",
      "offset": 4435.04,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "and so this embedding then gets",
      "offset": 4438.56,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "projected into the end state you get the",
      "offset": 4440.64,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "logits",
      "offset": 4442.48,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "now we calculate the probabilities for",
      "offset": 4443.6,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "that you can use f.softmax",
      "offset": 4445.679,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "of logits and that just basically",
      "offset": 4449.12,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "exponentiates the logits and makes them",
      "offset": 4450.88,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "sum to one and similar to cross entropy",
      "offset": 4452.32,
      "duration": 6.32
    },
    {
      "lang": "en",
      "text": "it is careful that there's no overflows",
      "offset": 4455.12,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "once we have the probabilities we sample",
      "offset": 4458.64,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "from them using torture multinomial to",
      "offset": 4460.159,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "get our next index and then we shift the",
      "offset": 4462,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "context window to append the index and",
      "offset": 4464.48,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "record it and then we can just",
      "offset": 4466.8,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "decode all the integers to strings",
      "offset": 4469.679,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "and print them out",
      "offset": 4471.92,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "and so these are some example samples",
      "offset": 4473.36,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "and you can see that the model now works",
      "offset": 4474.96,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "much better so the words here are much",
      "offset": 4476.88,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "more word like or name like so we have",
      "offset": 4478.96,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "things like ham",
      "offset": 4481.52,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "joes",
      "offset": 4484.4,
      "duration": 2.4
    },
    {
      "lang": "en",
      "text": "you know it's starting to sound a little",
      "offset": 4488.08,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "bit more name-like so we're definitely",
      "offset": 4489.52,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "making progress but we can still improve",
      "offset": 4491.28,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "on this model quite a lot",
      "offset": 4493.44,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "okay sorry there's some bonus content i",
      "offset": 4495.44,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "wanted to mention that i want to make",
      "offset": 4497.52,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "these notebooks more accessible and so i",
      "offset": 4499.44,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "don't want you to have to like install",
      "offset": 4501.84,
      "duration": 2.879
    },
    {
      "lang": "en",
      "text": "jupyter notebooks and torch and",
      "offset": 4503.36,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "everything else so i will be sharing a",
      "offset": 4504.719,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "link to a google colab",
      "offset": 4506.719,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "and google collab will look like a",
      "offset": 4508.96,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "notebook in your browser and you can",
      "offset": 4510.719,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "just go to the url and you'll be able to",
      "offset": 4513.12,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "execute all of the code that you saw in",
      "offset": 4515.36,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "the google collab and so this is me",
      "offset": 4517.76,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "executing the code in this lecture and i",
      "offset": 4520,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "shortened it a little bit but basically",
      "offset": 4522.32,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "you're able to train the exact same",
      "offset": 4524.719,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "network and then plot and sample from",
      "offset": 4525.92,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "the model and everything is ready for",
      "offset": 4528.48,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "you to like tinker with the numbers",
      "offset": 4530,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "right there in your browser no",
      "offset": 4531.679,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "installation necessary",
      "offset": 4533.12,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "so i just wanted to point that out and",
      "offset": 4535.52,
      "duration": 2.639
    },
    {
      "lang": "en",
      "text": "the link to this will be in the video",
      "offset": 4536.719,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "description",
      "offset": 4538.159,
      "duration": 2.641
    }
  ],
  "cleanText": null,
  "dumpedAt": "2025-07-21T18:43:24.247Z"
}