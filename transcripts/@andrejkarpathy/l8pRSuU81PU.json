{
  "episodeId": "l8pRSuU81PU",
  "channelSlug": "@andrejkarpathy",
  "title": "Let's reproduce GPT-2 (124M)",
  "publishedAt": "2024-06-09T23:31:35.000Z",
  "rawLines": [
    {
      "lang": "en",
      "text": "hi everyone so today we are going to be",
      "offset": 0.16,
      "duration": 4.52
    },
    {
      "lang": "en",
      "text": "continuing our Zero to Hero series and",
      "offset": 2.36,
      "duration": 3.64
    },
    {
      "lang": "en",
      "text": "in particular today we are going to",
      "offset": 4.68,
      "duration": 4.959
    },
    {
      "lang": "en",
      "text": "reproduce the gpt2 model the 124 million",
      "offset": 6,
      "duration": 7.36
    },
    {
      "lang": "en",
      "text": "version of it so when openi released",
      "offset": 9.639,
      "duration": 7.121
    },
    {
      "lang": "en",
      "text": "gpt2 this was 2019 and they released it",
      "offset": 13.36,
      "duration": 5.679
    },
    {
      "lang": "en",
      "text": "with this blog post on top of that they",
      "offset": 16.76,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "released this paper and on top of that",
      "offset": 19.039,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "they released this code on GitHub so",
      "offset": 21.32,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "open a/",
      "offset": 23.359,
      "duration": 3.881
    },
    {
      "lang": "en",
      "text": "gpt2 now when we talk about reproducing",
      "offset": 24.68,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "gpt2 we have to be careful because in",
      "offset": 27.24,
      "duration": 3.519
    },
    {
      "lang": "en",
      "text": "particular in this video we're going to",
      "offset": 29.32,
      "duration": 4.04
    },
    {
      "lang": "en",
      "text": "be reproducing the 124 million parameter",
      "offset": 30.759,
      "duration": 4.681
    },
    {
      "lang": "en",
      "text": "model so the thing to realize is that",
      "offset": 33.36,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "there's always a miniseries when these",
      "offset": 35.44,
      "duration": 5.08
    },
    {
      "lang": "en",
      "text": "are releases are made so there are the",
      "offset": 37.68,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "gpt2 miniseries made up of models at",
      "offset": 40.52,
      "duration": 4.519
    },
    {
      "lang": "en",
      "text": "different sizes and usually the biggest",
      "offset": 42.96,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "model is called the",
      "offset": 45.039,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "gpt2 but basically the reason we do that",
      "offset": 46.879,
      "duration": 4.2
    },
    {
      "lang": "en",
      "text": "is because you can put the model sizes",
      "offset": 49.199,
      "duration": 4.52
    },
    {
      "lang": "en",
      "text": "on the x-axis of plots like this and on",
      "offset": 51.079,
      "duration": 4.521
    },
    {
      "lang": "en",
      "text": "the Y AIS you put a lot of uh Downstream",
      "offset": 53.719,
      "duration": 3.32
    },
    {
      "lang": "en",
      "text": "metrics that you're interested in like",
      "offset": 55.6,
      "duration": 2.959
    },
    {
      "lang": "en",
      "text": "translation summarization question",
      "offset": 57.039,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "answering and so on and you can chart",
      "offset": 58.559,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "out these scaling laws so basically as",
      "offset": 60.48,
      "duration": 4.6
    },
    {
      "lang": "en",
      "text": "the model size increases you're getting",
      "offset": 63.559,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "better and better at Downstream metrics",
      "offset": 65.08,
      "duration": 4.84
    },
    {
      "lang": "en",
      "text": "and so in particular for",
      "offset": 67.72,
      "duration": 5.16
    },
    {
      "lang": "en",
      "text": "gpt2 if we scroll down in paper there",
      "offset": 69.92,
      "duration": 5.559
    },
    {
      "lang": "en",
      "text": "are four models in the gpt2 miniseries",
      "offset": 72.88,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "starting at 124 million all the way up",
      "offset": 75.479,
      "duration": 6.801
    },
    {
      "lang": "en",
      "text": "to 1558 million now the reason my",
      "offset": 78.24,
      "duration": 5.72
    },
    {
      "lang": "en",
      "text": "numbers the way I say them disagree with",
      "offset": 82.28,
      "duration": 3.64
    },
    {
      "lang": "en",
      "text": "this table is that this table is wrong",
      "offset": 83.96,
      "duration": 5.24
    },
    {
      "lang": "en",
      "text": "if you actually go to the uh gpt2 uh",
      "offset": 85.92,
      "duration": 6.239
    },
    {
      "lang": "en",
      "text": "GitHub repo they sort of say that um",
      "offset": 89.2,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "there was an error in how they added up",
      "offset": 92.159,
      "duration": 3.161
    },
    {
      "lang": "en",
      "text": "the parameters but basically this is the",
      "offset": 93.759,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "124 million parameter model Etc so the",
      "offset": 95.32,
      "duration": 5.56
    },
    {
      "lang": "en",
      "text": "124 million parameter had 12 layers in",
      "offset": 98.64,
      "duration": 5.96
    },
    {
      "lang": "en",
      "text": "the Transformer and it had 768 channels",
      "offset": 100.88,
      "duration": 6.76
    },
    {
      "lang": "en",
      "text": "in the Transformer 768 dimensions and",
      "offset": 104.6,
      "duration": 4.04
    },
    {
      "lang": "en",
      "text": "I'm going to be assuming some",
      "offset": 107.64,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "familiarity with what these terms mean",
      "offset": 108.64,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "because I covered all of this in my",
      "offset": 110.64,
      "duration": 4.04
    },
    {
      "lang": "en",
      "text": "previous video let's build gpt2 uh let's",
      "offset": 111.84,
      "duration": 4.919
    },
    {
      "lang": "en",
      "text": "build GPT from scratch so I covered that",
      "offset": 114.68,
      "duration": 4.759
    },
    {
      "lang": "en",
      "text": "in the previous video in this playlist",
      "offset": 116.759,
      "duration": 4.521
    },
    {
      "lang": "en",
      "text": "now if we do everything correctly and",
      "offset": 119.439,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "everything works out well by the end of",
      "offset": 121.28,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "this video we're going to see something",
      "offset": 123.039,
      "duration": 3.281
    },
    {
      "lang": "en",
      "text": "like this where we're looking at the",
      "offset": 124.56,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "validation loss which basically um",
      "offset": 126.32,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "measures how good we are at predicting",
      "offset": 130,
      "duration": 3.879
    },
    {
      "lang": "en",
      "text": "the next token in a sequence on some",
      "offset": 131.84,
      "duration": 3.56
    },
    {
      "lang": "en",
      "text": "validation data that the model has not",
      "offset": 133.879,
      "duration": 3.881
    },
    {
      "lang": "en",
      "text": "seen during training and we see that we",
      "offset": 135.4,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "go from doing that task not very well",
      "offset": 137.76,
      "duration": 4.28
    },
    {
      "lang": "en",
      "text": "because we're initializing from scratch",
      "offset": 140.4,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "all the way to doing that task quite",
      "offset": 142.04,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "well um by the end of the training and",
      "offset": 143.599,
      "duration": 4.841
    },
    {
      "lang": "en",
      "text": "hopefully we're going to beat the gpt2",
      "offset": 146.28,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "uh 124 M model",
      "offset": 148.44,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "now previously when they were working on",
      "offset": 150.76,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "this this is already 5 years ago so this",
      "offset": 152.92,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "was probably a fairly complicated",
      "offset": 155.08,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "optimization at the time and the gpus",
      "offset": 156.84,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "and the compute was a lot smaller today",
      "offset": 158.519,
      "duration": 4.44
    },
    {
      "lang": "en",
      "text": "you can reproduce this model in roughly",
      "offset": 161.159,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "an hour or probably less even and it",
      "offset": 162.959,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "will cost you about 10 bucks if you want",
      "offset": 165.72,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "to do this on the cloud uh Cloud Compu a",
      "offset": 167.44,
      "duration": 4.92
    },
    {
      "lang": "en",
      "text": "sort of computer that you can all rent",
      "offset": 169.959,
      "duration": 4.761
    },
    {
      "lang": "en",
      "text": "and if you pay $10 for that computer you",
      "offset": 172.36,
      "duration": 4.44
    },
    {
      "lang": "en",
      "text": "wait about an hour or less you can",
      "offset": 174.72,
      "duration": 3.96
    },
    {
      "lang": "en",
      "text": "actually achieve a model that is as good",
      "offset": 176.8,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "as this model that open ey released and",
      "offset": 178.68,
      "duration": 5.559
    },
    {
      "lang": "en",
      "text": "uh one more thing to mention is unlike",
      "offset": 182.08,
      "duration": 4.28
    },
    {
      "lang": "en",
      "text": "many other models open ey did release",
      "offset": 184.239,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "the weights for gpt2 so those weights",
      "offset": 186.36,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "are all available in this repository but",
      "offset": 188.879,
      "duration": 5.321
    },
    {
      "lang": "en",
      "text": "the gpt2 paper is not always as good",
      "offset": 191.4,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "with all of the details of training so",
      "offset": 194.2,
      "duration": 3.8
    },
    {
      "lang": "en",
      "text": "in addition to the gpt2 paper we're",
      "offset": 196.44,
      "duration": 4.04
    },
    {
      "lang": "en",
      "text": "going to be referencing the gpt3 paper",
      "offset": 198,
      "duration": 4.599
    },
    {
      "lang": "en",
      "text": "which is a lot more Concrete in a lot of",
      "offset": 200.48,
      "duration": 4.36
    },
    {
      "lang": "en",
      "text": "the hyp parameters and optimization",
      "offset": 202.599,
      "duration": 5.321
    },
    {
      "lang": "en",
      "text": "settings and so on um and it's not a",
      "offset": 204.84,
      "duration": 4.44
    },
    {
      "lang": "en",
      "text": "huge departure in the architecture from",
      "offset": 207.92,
      "duration": 4.039
    },
    {
      "lang": "en",
      "text": "the GPT 2 uh version of the model so",
      "offset": 209.28,
      "duration": 4.039
    },
    {
      "lang": "en",
      "text": "we're going to be referencing both gpt2",
      "offset": 211.959,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "and gpt3 as we try to reproduce gpt2 124",
      "offset": 213.319,
      "duration": 6.681
    },
    {
      "lang": "en",
      "text": "M uh so let's go so the first thing I",
      "offset": 216.92,
      "duration": 4.519
    },
    {
      "lang": "en",
      "text": "would like to do is actually start at",
      "offset": 220,
      "duration": 3.48
    },
    {
      "lang": "en",
      "text": "the end or at the Target so in other",
      "offset": 221.439,
      "duration": 5.72
    },
    {
      "lang": "en",
      "text": "words let's load the GPT to 124 M model",
      "offset": 223.48,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "as it was released by openi and maybe",
      "offset": 227.159,
      "duration": 3.16
    },
    {
      "lang": "en",
      "text": "take it for a spin let's sample some",
      "offset": 228.92,
      "duration": 3.64
    },
    {
      "lang": "en",
      "text": "tokens from it now the issue with that",
      "offset": 230.319,
      "duration": 4.041
    },
    {
      "lang": "en",
      "text": "is when you go into the code base of",
      "offset": 232.56,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "gpt2 and you go into the source and you",
      "offset": 234.36,
      "duration": 4.599
    },
    {
      "lang": "en",
      "text": "click in on the model. pi you'll realize",
      "offset": 236.64,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "that actually this is using tensorflow",
      "offset": 238.959,
      "duration": 4.28
    },
    {
      "lang": "en",
      "text": "so the original gpt2 code here was",
      "offset": 241.519,
      "duration": 5.241
    },
    {
      "lang": "en",
      "text": "written in tensor flow which is",
      "offset": 243.239,
      "duration": 6.72
    },
    {
      "lang": "en",
      "text": "um you know not let's just say not used",
      "offset": 246.76,
      "duration": 5.399
    },
    {
      "lang": "en",
      "text": "as much anymore um so we'd like to use",
      "offset": 249.959,
      "duration": 4.28
    },
    {
      "lang": "en",
      "text": "pytorch uh because it's a lot friendlier",
      "offset": 252.159,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "easier and I just personally like a lot",
      "offset": 254.239,
      "duration": 3.24
    },
    {
      "lang": "en",
      "text": "more the problem with that is the",
      "offset": 256,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "initial code is intenser flow we'd like",
      "offset": 257.479,
      "duration": 4.361
    },
    {
      "lang": "en",
      "text": "to use pytorch so instead uh to get the",
      "offset": 259.359,
      "duration": 3.881
    },
    {
      "lang": "en",
      "text": "target we're going to use the hugging",
      "offset": 261.84,
      "duration": 5.16
    },
    {
      "lang": "en",
      "text": "face Transformers um code which I like a",
      "offset": 263.24,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "lot more so when you go into the",
      "offset": 267,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "Transformers source Transformers models",
      "offset": 268.84,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "gpt2 modeling gpt2 Pi you will see that",
      "offset": 270.52,
      "duration": 5.239
    },
    {
      "lang": "en",
      "text": "they have the gpt2 implementation of",
      "offset": 273.88,
      "duration": 3.96
    },
    {
      "lang": "en",
      "text": "that Transformer here in this",
      "offset": 275.759,
      "duration": 7.16
    },
    {
      "lang": "en",
      "text": "file um and it's like medium readable",
      "offset": 277.84,
      "duration": 7.68
    },
    {
      "lang": "en",
      "text": "but not fully readable um but what it",
      "offset": 282.919,
      "duration": 4.601
    },
    {
      "lang": "en",
      "text": "does is it did all the work of",
      "offset": 285.52,
      "duration": 4.519
    },
    {
      "lang": "en",
      "text": "converting all those weights uh from",
      "offset": 287.52,
      "duration": 4.76
    },
    {
      "lang": "en",
      "text": "tensor flow to pytorch Friendly and so",
      "offset": 290.039,
      "duration": 4.521
    },
    {
      "lang": "en",
      "text": "it's much easier to load and work with",
      "offset": 292.28,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "so in particular we can look at the",
      "offset": 294.56,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "gpt2 um model here and we can load it",
      "offset": 296.759,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "using hugging face Transformers so",
      "offset": 299.68,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "swinging over this is what that looks",
      "offset": 301.759,
      "duration": 5.961
    },
    {
      "lang": "en",
      "text": "like from Transformers import the DP GT2",
      "offset": 303.84,
      "duration": 8.4
    },
    {
      "lang": "en",
      "text": "LM head model and then from pre-train",
      "offset": 307.72,
      "duration": 7.36
    },
    {
      "lang": "en",
      "text": "gpt2 uh now one awkward thing about this",
      "offset": 312.24,
      "duration": 4.92
    },
    {
      "lang": "en",
      "text": "is that when you do gpt2 as the model",
      "offset": 315.08,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "that we're loading this actually is the",
      "offset": 317.16,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "124 million parameter model if you want",
      "offset": 319,
      "duration": 6.72
    },
    {
      "lang": "en",
      "text": "the actual the gpt2 the 1.5 billion then",
      "offset": 322.28,
      "duration": 6.6
    },
    {
      "lang": "en",
      "text": "you actually want to do- XL so this is",
      "offset": 325.72,
      "duration": 6.52
    },
    {
      "lang": "en",
      "text": "the 12 4 M our Target now what we're",
      "offset": 328.88,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "doing is when we actually get this we're",
      "offset": 332.24,
      "duration": 4.92
    },
    {
      "lang": "en",
      "text": "initializing the uh pytorch NN module as",
      "offset": 333.84,
      "duration": 5.079
    },
    {
      "lang": "en",
      "text": "defined here in this",
      "offset": 337.16,
      "duration": 4.039
    },
    {
      "lang": "en",
      "text": "class from it I want to get just the",
      "offset": 338.919,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "state dict which is just a raw tensors",
      "offset": 341.199,
      "duration": 5.681
    },
    {
      "lang": "en",
      "text": "so we just have um the tensors of that",
      "offset": 344.199,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "file and by the way here this is a",
      "offset": 346.88,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "jupyter notebook uh but this is jupyter",
      "offset": 349.16,
      "duration": 4.92
    },
    {
      "lang": "en",
      "text": "notebook running inside vs code uh so I",
      "offset": 351.199,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "like to work with it all in a single",
      "offset": 354.08,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "sort of interface so I like to use vs",
      "offset": 356,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "code so this is the jupyter notebook",
      "offset": 357.84,
      "duration": 6.12
    },
    {
      "lang": "en",
      "text": "extension inside the es",
      "offset": 360.56,
      "duration": 5.56
    },
    {
      "lang": "en",
      "text": "code so when we get the state dick this",
      "offset": 363.96,
      "duration": 5.4
    },
    {
      "lang": "en",
      "text": "is just a dict so we can print the key",
      "offset": 366.12,
      "duration": 5.519
    },
    {
      "lang": "en",
      "text": "and the value which is the tensor and",
      "offset": 369.36,
      "duration": 4.279
    },
    {
      "lang": "en",
      "text": "let's just look at the shapes so these",
      "offset": 371.639,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "are sort of",
      "offset": 373.639,
      "duration": 4.041
    },
    {
      "lang": "en",
      "text": "the uh different parameters inside the",
      "offset": 374.919,
      "duration": 7.28
    },
    {
      "lang": "en",
      "text": "gbt2 model and their shape so the W",
      "offset": 377.68,
      "duration": 7.639
    },
    {
      "lang": "en",
      "text": "weight for token",
      "offset": 382.199,
      "duration": 5.321
    },
    {
      "lang": "en",
      "text": "embedding is of size",
      "offset": 385.319,
      "duration": 5.681
    },
    {
      "lang": "en",
      "text": "50257 by 768 where this is coming from",
      "offset": 387.52,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "is that we have",
      "offset": 391,
      "duration": 6.4
    },
    {
      "lang": "en",
      "text": "50257 tokens in the gpt2 vocabulary um",
      "offset": 392.52,
      "duration": 6.959
    },
    {
      "lang": "en",
      "text": "and the tokens by the way these are",
      "offset": 397.4,
      "duration": 3.56
    },
    {
      "lang": "en",
      "text": "exactly the tokens that we spoken about",
      "offset": 399.479,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "in the previous video on my tokenization",
      "offset": 400.96,
      "duration": 4.92
    },
    {
      "lang": "en",
      "text": "Series so the previous videos just",
      "offset": 403.56,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "before this I go into a ton of detail on",
      "offset": 405.88,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "tokenization gpt2 tokenizer happens to",
      "offset": 407.479,
      "duration": 5.641
    },
    {
      "lang": "en",
      "text": "have this many tokens for each",
      "offset": 409.88,
      "duration": 6.56
    },
    {
      "lang": "en",
      "text": "token we have a 768 dimensional",
      "offset": 413.12,
      "duration": 5.359
    },
    {
      "lang": "en",
      "text": "embedding that is the distributed",
      "offset": 416.44,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "representation that stands in for that",
      "offset": 418.479,
      "duration": 5.081
    },
    {
      "lang": "en",
      "text": "token so each token is a little string",
      "offset": 421.4,
      "duration": 5.56
    },
    {
      "lang": "en",
      "text": "piece and then the 768 numbers are the",
      "offset": 423.56,
      "duration": 5.359
    },
    {
      "lang": "en",
      "text": "vector that represents that",
      "offset": 426.96,
      "duration": 3.959
    },
    {
      "lang": "en",
      "text": "token and so this is just our lookup",
      "offset": 428.919,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "table for tokens and then here we have",
      "offset": 430.919,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "the lookup table for the positions so",
      "offset": 433.319,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "because gbt2 has a maximum sequence",
      "offset": 436.12,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "length of",
      "offset": 438.319,
      "duration": 5.241
    },
    {
      "lang": "en",
      "text": "1024 we have up to 1,24 positions that",
      "offset": 439.56,
      "duration": 6.039
    },
    {
      "lang": "en",
      "text": "each token can be attending to in the",
      "offset": 443.56,
      "duration": 4.6
    },
    {
      "lang": "en",
      "text": "past and every one of those positions in",
      "offset": 445.599,
      "duration": 5.681
    },
    {
      "lang": "en",
      "text": "gpd2 has a fixed Vector of",
      "offset": 448.16,
      "duration": 5.8
    },
    {
      "lang": "en",
      "text": "768 that is learned by",
      "offset": 451.28,
      "duration": 5.359
    },
    {
      "lang": "en",
      "text": "optimization um and so this is the",
      "offset": 453.96,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "position embedding and the token",
      "offset": 456.639,
      "duration": 4.601
    },
    {
      "lang": "en",
      "text": "embedding um and then everything here is",
      "offset": 458.68,
      "duration": 4.6
    },
    {
      "lang": "en",
      "text": "just the other weights and biases and",
      "offset": 461.24,
      "duration": 4.04
    },
    {
      "lang": "en",
      "text": "everything else of this",
      "offset": 463.28,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "Transformer so when you just take for",
      "offset": 465.28,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "example the positional embeddings and",
      "offset": 467.36,
      "duration": 3.64
    },
    {
      "lang": "en",
      "text": "flatten it out and take just the 20",
      "offset": 469.28,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "elements you can see that these are just",
      "offset": 471,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "the parameters these are weights floats",
      "offset": 472.96,
      "duration": 6.079
    },
    {
      "lang": "en",
      "text": "just we can take and we can plot them so",
      "offset": 476.28,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "these are the position embeddings and we",
      "offset": 479.039,
      "duration": 4.12
    },
    {
      "lang": "en",
      "text": "get something like this and you can see",
      "offset": 481.96,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "that this has structure and it has",
      "offset": 483.159,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "structure because what we what we have",
      "offset": 484.96,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "here really is every Row in this",
      "offset": 487.479,
      "duration": 5.4
    },
    {
      "lang": "en",
      "text": "visualization is a different position a",
      "offset": 490.08,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "fixed absolute position in um the range",
      "offset": 492.879,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "from 0 to",
      "offset": 496.24,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "1024 and each row here is the",
      "offset": 497.44,
      "duration": 5.56
    },
    {
      "lang": "en",
      "text": "representation of that position and so",
      "offset": 499.919,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "it has structure because these",
      "offset": 503,
      "duration": 3.639
    },
    {
      "lang": "en",
      "text": "positional embeddings end up learning",
      "offset": 504.479,
      "duration": 5.16
    },
    {
      "lang": "en",
      "text": "these sinusoids and cosiness um that",
      "offset": 506.639,
      "duration": 5.241
    },
    {
      "lang": "en",
      "text": "sort of like represent each of these",
      "offset": 509.639,
      "duration": 5.481
    },
    {
      "lang": "en",
      "text": "positions and uh each row here stands in",
      "offset": 511.88,
      "duration": 4.92
    },
    {
      "lang": "en",
      "text": "for that position and is processed by",
      "offset": 515.12,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "the Transformer to recover all the",
      "offset": 516.8,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "relative positions and uh sort of",
      "offset": 518.8,
      "duration": 5.32
    },
    {
      "lang": "en",
      "text": "realize which token is where and um",
      "offset": 521.76,
      "duration": 3.519
    },
    {
      "lang": "en",
      "text": "attend to them depending on their",
      "offset": 524.12,
      "duration": 2.92
    },
    {
      "lang": "en",
      "text": "position not just their",
      "offset": 525.279,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "content so when we actually just look",
      "offset": 527.04,
      "duration": 6.28
    },
    {
      "lang": "en",
      "text": "into an individual column inside these",
      "offset": 529.68,
      "duration": 6.04
    },
    {
      "lang": "en",
      "text": "and I just grabbed three random columns",
      "offset": 533.32,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "you'll see that for example here we are",
      "offset": 535.72,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "focusing on every every single um",
      "offset": 537.959,
      "duration": 5.961
    },
    {
      "lang": "en",
      "text": "Channel and we're looking",
      "offset": 541.56,
      "duration": 6.12
    },
    {
      "lang": "en",
      "text": "at what that channel is doing as a",
      "offset": 543.92,
      "duration": 7.479
    },
    {
      "lang": "en",
      "text": "function of uh position from one from Z",
      "offset": 547.68,
      "duration": 4.44
    },
    {
      "lang": "en",
      "text": "to",
      "offset": 551.399,
      "duration": 2.641
    },
    {
      "lang": "en",
      "text": "1223",
      "offset": 552.12,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "really and we can see that some of these",
      "offset": 554.04,
      "duration": 3.72
    },
    {
      "lang": "en",
      "text": "channels basically like respond more or",
      "offset": 555.959,
      "duration": 3.961
    },
    {
      "lang": "en",
      "text": "less to different parts of the position",
      "offset": 557.76,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "Spectrum so this green channel uh really",
      "offset": 559.92,
      "duration": 7.039
    },
    {
      "lang": "en",
      "text": "likes to fire for everything after 200",
      "offset": 562.88,
      "duration": 7.6
    },
    {
      "lang": "en",
      "text": "uh up to 800 but not less a lot less and",
      "offset": 566.959,
      "duration": 6.201
    },
    {
      "lang": "en",
      "text": "has a sharp drop off here near zero so",
      "offset": 570.48,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "who knows what these embeddings are",
      "offset": 573.16,
      "duration": 3.08
    },
    {
      "lang": "en",
      "text": "doing and why they are the way they are",
      "offset": 574.399,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "you can tell for example that because",
      "offset": 576.24,
      "duration": 2.599
    },
    {
      "lang": "en",
      "text": "they're a bit more Jagged and they're",
      "offset": 577.519,
      "duration": 2.961
    },
    {
      "lang": "en",
      "text": "kind of noisy you can tell that this",
      "offset": 578.839,
      "duration": 4.201
    },
    {
      "lang": "en",
      "text": "model was not fully trained and the more",
      "offset": 580.48,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "trained this model was the more you",
      "offset": 583.04,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "would expect to smooth this out and so",
      "offset": 585.2,
      "duration": 3.16
    },
    {
      "lang": "en",
      "text": "this is telling you that this is a",
      "offset": 587.44,
      "duration": 3.959
    },
    {
      "lang": "en",
      "text": "little bit of an undertrained model um",
      "offset": 588.36,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "but in principle actually these curves",
      "offset": 591.399,
      "duration": 4.041
    },
    {
      "lang": "en",
      "text": "don't even have to be smooth this should",
      "offset": 593.64,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "just be totally random noise and in fact",
      "offset": 595.44,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "in the beginning of the optimization it",
      "offset": 597.64,
      "duration": 3.72
    },
    {
      "lang": "en",
      "text": "is complete random noise because this",
      "offset": 598.88,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "position embedding table is initialized",
      "offset": 601.36,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "completely at random so in the beginning",
      "offset": 603.2,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "you have jaggedness and the fact that",
      "offset": 605.04,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "you end up with something smooth is",
      "offset": 607.519,
      "duration": 4.281
    },
    {
      "lang": "en",
      "text": "already kind of impressive um that that",
      "offset": 609.04,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "just falls out of the optimization",
      "offset": 611.8,
      "duration": 2.68
    },
    {
      "lang": "en",
      "text": "because in principle you shouldn't even",
      "offset": 613.44,
      "duration": 3.48
    },
    {
      "lang": "en",
      "text": "be able to get any single graph out of",
      "offset": 614.48,
      "duration": 3.88
    },
    {
      "lang": "en",
      "text": "this that makes sense but we actually",
      "offset": 616.92,
      "duration": 2.919
    },
    {
      "lang": "en",
      "text": "get something that looks a little bit",
      "offset": 618.36,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "noisy but for the most part looks",
      "offset": 619.839,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "sinusoidal like um in the original",
      "offset": 621.32,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "Transformer um in the original",
      "offset": 624.8,
      "duration": 3.32
    },
    {
      "lang": "en",
      "text": "Transformer paper the attention is all",
      "offset": 626.6,
      "duration": 3.64
    },
    {
      "lang": "en",
      "text": "you need paper the positional embeddings",
      "offset": 628.12,
      "duration": 4.36
    },
    {
      "lang": "en",
      "text": "are actually initialized and fixed if I",
      "offset": 630.24,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "remember correctly to sinusoids and",
      "offset": 632.48,
      "duration": 4.919
    },
    {
      "lang": "en",
      "text": "cosiness of uh different frequencies and",
      "offset": 634.48,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "that's the positional coding and it's",
      "offset": 637.399,
      "duration": 3.321
    },
    {
      "lang": "en",
      "text": "fixed but in gpt2 these are just",
      "offset": 638.72,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "parameters and they're trained from",
      "offset": 640.72,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "scratch just like any other parameter uh",
      "offset": 641.92,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "and that seems to work about as well and",
      "offset": 644.72,
      "duration": 3.239
    },
    {
      "lang": "en",
      "text": "so what they do is they kind of like",
      "offset": 646.8,
      "duration": 3.64
    },
    {
      "lang": "en",
      "text": "recover these sinusoidal like features",
      "offset": 647.959,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "during the",
      "offset": 650.44,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "optimization we can also look at any of",
      "offset": 652.2,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "the other matrices here so here I took",
      "offset": 654.279,
      "duration": 6.081
    },
    {
      "lang": "en",
      "text": "the first layer of the",
      "offset": 657.48,
      "duration": 5.32
    },
    {
      "lang": "en",
      "text": "Transformer and looking at like one of",
      "offset": 660.36,
      "duration": 5.08
    },
    {
      "lang": "en",
      "text": "its weights and just the first block of",
      "offset": 662.8,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "300 by 300 and you see some structure",
      "offset": 665.44,
      "duration": 5.399
    },
    {
      "lang": "en",
      "text": "but like again like who knows what any",
      "offset": 668.56,
      "duration": 4.36
    },
    {
      "lang": "en",
      "text": "of this is if you're into mechanistic",
      "offset": 670.839,
      "duration": 3.961
    },
    {
      "lang": "en",
      "text": "interpretability you might get a real",
      "offset": 672.92,
      "duration": 3.479
    },
    {
      "lang": "en",
      "text": "kick out of trying to figure out like",
      "offset": 674.8,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "what is going on what is this structure",
      "offset": 676.399,
      "duration": 3.56
    },
    {
      "lang": "en",
      "text": "and what does this all mean but we're",
      "offset": 678.399,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "not going to be doing that in this video",
      "offset": 679.959,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "but we definitely see that there's some",
      "offset": 681.6,
      "duration": 3.32
    },
    {
      "lang": "en",
      "text": "interesting structure and that's kind of",
      "offset": 682.959,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "cool what we're mostly interested in is",
      "offset": 684.92,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "we've loaded the weights of this model",
      "offset": 686.959,
      "duration": 3.88
    },
    {
      "lang": "en",
      "text": "that was released by open Ai and now",
      "offset": 688.6,
      "duration": 4.44
    },
    {
      "lang": "en",
      "text": "using the hogging face Transformers we",
      "offset": 690.839,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "can not just get all the raw weights but",
      "offset": 693.04,
      "duration": 6.039
    },
    {
      "lang": "en",
      "text": "we can also get the um what they call",
      "offset": 695.959,
      "duration": 6.241
    },
    {
      "lang": "en",
      "text": "Pipeline and sample from it so this is",
      "offset": 699.079,
      "duration": 5.281
    },
    {
      "lang": "en",
      "text": "the prefix hello I'm a language model",
      "offset": 702.2,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "comma and then we're sampling uh 30",
      "offset": 704.36,
      "duration": 6.52
    },
    {
      "lang": "en",
      "text": "tokens and we getting five sequences and",
      "offset": 707.72,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "I ran this and this is what it produced",
      "offset": 710.88,
      "duration": 4.84
    },
    {
      "lang": "en",
      "text": "um hell language",
      "offset": 713.88,
      "duration": 3.72
    },
    {
      "lang": "en",
      "text": "model but what I'm really doing is",
      "offset": 715.72,
      "duration": 3.96
    },
    {
      "lang": "en",
      "text": "making a human readable document there",
      "offset": 717.6,
      "duration": 4.039
    },
    {
      "lang": "en",
      "text": "are other languages but those are dot",
      "offset": 719.68,
      "duration": 3.8
    },
    {
      "lang": "en",
      "text": "dot dot so you can read through these if",
      "offset": 721.639,
      "duration": 3.561
    },
    {
      "lang": "en",
      "text": "you like but basically these are five",
      "offset": 723.48,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "different completions of the same prefix",
      "offset": 725.2,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "from this uh gbt",
      "offset": 727.56,
      "duration": 5.48
    },
    {
      "lang": "en",
      "text": "2124m now uh if I go here I took this",
      "offset": 729.44,
      "duration": 7.399
    },
    {
      "lang": "en",
      "text": "example from here and sadly even though",
      "offset": 733.04,
      "duration": 5.72
    },
    {
      "lang": "en",
      "text": "we are fixing the seed we are getting",
      "offset": 736.839,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "different Generations from the snippet",
      "offset": 738.76,
      "duration": 5.439
    },
    {
      "lang": "en",
      "text": "than what they got so presumably the",
      "offset": 741.639,
      "duration": 6.681
    },
    {
      "lang": "en",
      "text": "code changed um but what we see though",
      "offset": 744.199,
      "duration": 5.681
    },
    {
      "lang": "en",
      "text": "at this stage that's important is that",
      "offset": 748.32,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "we are getting coherent text so we've",
      "offset": 749.88,
      "duration": 4.28
    },
    {
      "lang": "en",
      "text": "loaded the model successfully we can",
      "offset": 752.399,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "look at all its parameters and the keys",
      "offset": 754.16,
      "duration": 5.16
    },
    {
      "lang": "en",
      "text": "tell us where in the model these come",
      "offset": 756.48,
      "duration": 5.159
    },
    {
      "lang": "en",
      "text": "from and we want to actually write our",
      "offset": 759.32,
      "duration": 4.28
    },
    {
      "lang": "en",
      "text": "own gpt2 class so that we have full",
      "offset": 761.639,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "understanding of what's happening there",
      "offset": 763.6,
      "duration": 2.44
    },
    {
      "lang": "en",
      "text": "we don't want to be working with",
      "offset": 764.839,
      "duration": 4.44
    },
    {
      "lang": "en",
      "text": "something like uh the modeling gpt2 Pi",
      "offset": 766.04,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "because it's just too complicated we",
      "offset": 769.279,
      "duration": 2.281
    },
    {
      "lang": "en",
      "text": "want to write this from scratch",
      "offset": 770.44,
      "duration": 2.6
    },
    {
      "lang": "en",
      "text": "ourselves so we're going to be",
      "offset": 771.56,
      "duration": 3.279
    },
    {
      "lang": "en",
      "text": "implementing the GPT model here in",
      "offset": 773.04,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "parallel and as our first task let's",
      "offset": 774.839,
      "duration": 7.081
    },
    {
      "lang": "en",
      "text": "load the gpt2 124 M into the class that",
      "offset": 777.519,
      "duration": 6.921
    },
    {
      "lang": "en",
      "text": "we're going to develop here from scratch",
      "offset": 781.92,
      "duration": 4.68
    },
    {
      "lang": "en",
      "text": "that's going to give us confidence that",
      "offset": 784.44,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "we can load the open ey model and",
      "offset": 786.6,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "therefore there's a setting of Weights",
      "offset": 788.76,
      "duration": 4.759
    },
    {
      "lang": "en",
      "text": "that exactly is the 124 model but then",
      "offset": 790.92,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "of course what we're going to do is",
      "offset": 793.519,
      "duration": 2.201
    },
    {
      "lang": "en",
      "text": "we're going to initialize the model from",
      "offset": 794.44,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "scratch instead and try try to train it",
      "offset": 795.72,
      "duration": 5.239
    },
    {
      "lang": "en",
      "text": "ourselves um on a bunch of documents",
      "offset": 798.519,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "that we're going to get and we're going",
      "offset": 800.959,
      "duration": 3.56
    },
    {
      "lang": "en",
      "text": "to try to surpass that model so we're",
      "offset": 802.44,
      "duration": 3.32
    },
    {
      "lang": "en",
      "text": "going to get different weights and",
      "offset": 804.519,
      "duration": 2.721
    },
    {
      "lang": "en",
      "text": "everything's going to look different",
      "offset": 805.76,
      "duration": 3.879
    },
    {
      "lang": "en",
      "text": "hopefully better even um",
      "offset": 807.24,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "but uh we're going to have a lot of",
      "offset": 809.639,
      "duration": 2.921
    },
    {
      "lang": "en",
      "text": "confidence that because we can load the",
      "offset": 811.16,
      "duration": 3.4
    },
    {
      "lang": "en",
      "text": "openi model we are in the same model",
      "offset": 812.56,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "family and model class and we just have",
      "offset": 814.56,
      "duration": 3.399
    },
    {
      "lang": "en",
      "text": "to ReDiscover a good setting of the",
      "offset": 816.56,
      "duration": 4.76
    },
    {
      "lang": "en",
      "text": "weights uh but from scratch so let's now",
      "offset": 817.959,
      "duration": 5.961
    },
    {
      "lang": "en",
      "text": "write the gbt2 model and let's load the",
      "offset": 821.32,
      "duration": 4.44
    },
    {
      "lang": "en",
      "text": "weights and make sure that we can also",
      "offset": 823.92,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "generate text that looks coherent okay",
      "offset": 825.76,
      "duration": 4.12
    },
    {
      "lang": "en",
      "text": "so let's now swing over to the attention",
      "offset": 828.32,
      "duration": 2.879
    },
    {
      "lang": "en",
      "text": "is all un need paper that started",
      "offset": 829.88,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "everything and let's scroll over to the",
      "offset": 831.199,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "model architecture the original",
      "offset": 833.079,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "Transformer now remember that gpt2 is",
      "offset": 835.199,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "slightly modified from the or or",
      "offset": 837.72,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "Transformer in particular we do not have",
      "offset": 839.36,
      "duration": 5.839
    },
    {
      "lang": "en",
      "text": "uh the encoder gpt2 is a decoder only",
      "offset": 842.519,
      "duration": 4.76
    },
    {
      "lang": "en",
      "text": "Transformer as we call it so this entire",
      "offset": 845.199,
      "duration": 4.601
    },
    {
      "lang": "en",
      "text": "encoder here is missing in addition to",
      "offset": 847.279,
      "duration": 5.041
    },
    {
      "lang": "en",
      "text": "that this cross attention here that was",
      "offset": 849.8,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "using that encoder is also missing so we",
      "offset": 852.32,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "delete this entire part everything else",
      "offset": 854.8,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "stays almost the same but there are some",
      "offset": 858,
      "duration": 3.959
    },
    {
      "lang": "en",
      "text": "differences that we're going to uh sort",
      "offset": 860.24,
      "duration": 6.519
    },
    {
      "lang": "en",
      "text": "of look at here so there are two main",
      "offset": 861.959,
      "duration": 7.44
    },
    {
      "lang": "en",
      "text": "differences when we go to the gb2 page",
      "offset": 866.759,
      "duration": 6.121
    },
    {
      "lang": "en",
      "text": "under 2.3 model we notice that first",
      "offset": 869.399,
      "duration": 5.401
    },
    {
      "lang": "en",
      "text": "there's a reshuffling of the layer Norms",
      "offset": 872.88,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "so they change place and second an",
      "offset": 874.8,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "additional layer normalization was added",
      "offset": 878.24,
      "duration": 5.519
    },
    {
      "lang": "en",
      "text": "here to the final self detention block",
      "offset": 880.56,
      "duration": 5.839
    },
    {
      "lang": "en",
      "text": "so basically all the layer Norms here",
      "offset": 883.759,
      "duration": 4.601
    },
    {
      "lang": "en",
      "text": "instead of being after the MLP or after",
      "offset": 886.399,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "the attention they SN before it and an",
      "offset": 888.36,
      "duration": 4.279
    },
    {
      "lang": "en",
      "text": "additional layer Norm gets added here",
      "offset": 890.88,
      "duration": 3.48
    },
    {
      "lang": "en",
      "text": "right before the final",
      "offset": 892.639,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "classifier so now let's Implement some",
      "offset": 894.36,
      "duration": 4.68
    },
    {
      "lang": "en",
      "text": "of the first sort of skeleton NN module",
      "offset": 896.72,
      "duration": 6.2
    },
    {
      "lang": "en",
      "text": "modules here in our GPT NN module and in",
      "offset": 899.04,
      "duration": 5.359
    },
    {
      "lang": "en",
      "text": "particular we're going to try to match",
      "offset": 902.92,
      "duration": 3.8
    },
    {
      "lang": "en",
      "text": "up this schema here that is used by",
      "offset": 904.399,
      "duration": 4.12
    },
    {
      "lang": "en",
      "text": "hugging face Transformers because that",
      "offset": 906.72,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "will make it much easier to load these",
      "offset": 908.519,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "weights from this state dict so we want",
      "offset": 910.079,
      "duration": 5.56
    },
    {
      "lang": "en",
      "text": "something that reflects uh this schema",
      "offset": 912.92,
      "duration": 7
    },
    {
      "lang": "en",
      "text": "here so here's what I came up with",
      "offset": 915.639,
      "duration": 6.56
    },
    {
      "lang": "en",
      "text": "um basically we see that the main",
      "offset": 919.92,
      "duration": 4.919
    },
    {
      "lang": "en",
      "text": "container here that has all the modules",
      "offset": 922.199,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "is called Transformer so I'm reflecting",
      "offset": 924.839,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "that with an NN module dict and this is",
      "offset": 926.759,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "basically a module that allows you to",
      "offset": 929.24,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "index into the subm modules using keys",
      "offset": 930.759,
      "duration": 5.801
    },
    {
      "lang": "en",
      "text": "just like a dictionary uh",
      "offset": 934.24,
      "duration": 4.92
    },
    {
      "lang": "en",
      "text": "strings within it we have the weights of",
      "offset": 936.56,
      "duration": 5.399
    },
    {
      "lang": "en",
      "text": "the token embeddings WT and that's an N",
      "offset": 939.16,
      "duration": 4.919
    },
    {
      "lang": "en",
      "text": "embedding and the weights of the",
      "offset": 941.959,
      "duration": 3.521
    },
    {
      "lang": "en",
      "text": "position embeddings which is also just",
      "offset": 944.079,
      "duration": 3.521
    },
    {
      "lang": "en",
      "text": "an N embedding and if you remember n",
      "offset": 945.48,
      "duration": 3.88
    },
    {
      "lang": "en",
      "text": "embedding is really just a fancy little",
      "offset": 947.6,
      "duration": 5.56
    },
    {
      "lang": "en",
      "text": "wrapper module around just a single um",
      "offset": 949.36,
      "duration": 7.2
    },
    {
      "lang": "en",
      "text": "single array of numbers a single uh",
      "offset": 953.16,
      "duration": 5.4
    },
    {
      "lang": "en",
      "text": "block of numbers just like this it's a",
      "offset": 956.56,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "single tensor and an embedding is a",
      "offset": 958.56,
      "duration": 6.199
    },
    {
      "lang": "en",
      "text": "glorified um wrapper around a tensor",
      "offset": 962,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "that allows you to access its elements",
      "offset": 964.759,
      "duration": 4.121
    },
    {
      "lang": "en",
      "text": "uh by indexing into the",
      "offset": 967.12,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "rows now in addition to that we see here",
      "offset": 968.88,
      "duration": 5.72
    },
    {
      "lang": "en",
      "text": "that we have a h and then there's a this",
      "offset": 971.279,
      "duration": 5.281
    },
    {
      "lang": "en",
      "text": "is index using numbers instead of",
      "offset": 974.6,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "indexed using strings so there's a h. 0",
      "offset": 976.56,
      "duration": 7.32
    },
    {
      "lang": "en",
      "text": "1 2 Etc all the way up till h. 11 and",
      "offset": 979.639,
      "duration": 6.361
    },
    {
      "lang": "en",
      "text": "that's because there are 12 layers here",
      "offset": 983.88,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "in this Transformer so to reflect that",
      "offset": 986,
      "duration": 5.24
    },
    {
      "lang": "en",
      "text": "I'm creating also an H I think that",
      "offset": 988.6,
      "duration": 4.84
    },
    {
      "lang": "en",
      "text": "probably stands for hidden and instead",
      "offset": 991.24,
      "duration": 4.279
    },
    {
      "lang": "en",
      "text": "of a module dict this is a model list so",
      "offset": 993.44,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "we can index it using integers exactly",
      "offset": 995.519,
      "duration": 6.481
    },
    {
      "lang": "en",
      "text": "as we see here 01 2 Etc and the modular",
      "offset": 997.759,
      "duration": 8.32
    },
    {
      "lang": "en",
      "text": "list has a n layer blocks and the blocks",
      "offset": 1002,
      "duration": 6.519
    },
    {
      "lang": "en",
      "text": "are yet to be defined in a module in a",
      "offset": 1006.079,
      "duration": 4.521
    },
    {
      "lang": "en",
      "text": "bit in addition to that following the",
      "offset": 1008.519,
      "duration": 5.081
    },
    {
      "lang": "en",
      "text": "gpt2 paper we have we need an additional",
      "offset": 1010.6,
      "duration": 5.679
    },
    {
      "lang": "en",
      "text": "final layer Norm that we're going to put",
      "offset": 1013.6,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "in there and then we have the final",
      "offset": 1016.279,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "classifier uh the language model head",
      "offset": 1018.16,
      "duration": 7.56
    },
    {
      "lang": "en",
      "text": "which um projects from 768 the number of",
      "offset": 1021.399,
      "duration": 6.64
    },
    {
      "lang": "en",
      "text": "embedding dimensions in this GPT all the",
      "offset": 1025.72,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "way to the vocab size which is",
      "offset": 1028.039,
      "duration": 5.561
    },
    {
      "lang": "en",
      "text": "50257 and gpt2 uses no bias for this",
      "offset": 1030.36,
      "duration": 6.599
    },
    {
      "lang": "en",
      "text": "final uh sort of projection so this is",
      "offset": 1033.6,
      "duration": 5.64
    },
    {
      "lang": "en",
      "text": "the skeleton and you can see that it",
      "offset": 1036.959,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "reflects this so the wte is the token",
      "offset": 1039.24,
      "duration": 5.64
    },
    {
      "lang": "en",
      "text": "embeddings here it's called output",
      "offset": 1042.799,
      "duration": 3.801
    },
    {
      "lang": "en",
      "text": "embedding but it's really the token",
      "offset": 1044.88,
      "duration": 4.36
    },
    {
      "lang": "en",
      "text": "embeddings the PE is the positional",
      "offset": 1046.6,
      "duration": 4.6
    },
    {
      "lang": "en",
      "text": "codings uh those two pieces of",
      "offset": 1049.24,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "information as we saw previously are",
      "offset": 1051.2,
      "duration": 2.839
    },
    {
      "lang": "en",
      "text": "going to add and then go into the",
      "offset": 1052.44,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "Transformer the H is the all the blocks",
      "offset": 1054.039,
      "duration": 6.081
    },
    {
      "lang": "en",
      "text": "in Gray and the LNF is this new layer",
      "offset": 1057.16,
      "duration": 6.639
    },
    {
      "lang": "en",
      "text": "that gets added here by the gpt2 model",
      "offset": 1060.12,
      "duration": 7.2
    },
    {
      "lang": "en",
      "text": "and LM head is this linear part here so",
      "offset": 1063.799,
      "duration": 6.361
    },
    {
      "lang": "en",
      "text": "that's the skeleton of the gpt2 we now",
      "offset": 1067.32,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "have to implement the block okay so",
      "offset": 1070.16,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "let's now recurse to the block itself so",
      "offset": 1073,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "we want to define the block um so I'll",
      "offset": 1075.52,
      "duration": 6.72
    },
    {
      "lang": "en",
      "text": "start putting them here so the block I",
      "offset": 1079,
      "duration": 5.48
    },
    {
      "lang": "en",
      "text": "like to write out like",
      "offset": 1082.24,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "this uh these are some of the",
      "offset": 1084.48,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "initializations and then this is the",
      "offset": 1086.159,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "actual forward pass of what this block",
      "offset": 1087.44,
      "duration": 4.599
    },
    {
      "lang": "en",
      "text": "computes and notice here that there's a",
      "offset": 1089.6,
      "duration": 4.84
    },
    {
      "lang": "en",
      "text": "change from the Transformer again that",
      "offset": 1092.039,
      "duration": 5.64
    },
    {
      "lang": "en",
      "text": "is mentioned in the gpt2 paper so here",
      "offset": 1094.44,
      "duration": 5.64
    },
    {
      "lang": "en",
      "text": "the layer normalizations are after the",
      "offset": 1097.679,
      "duration": 4.36
    },
    {
      "lang": "en",
      "text": "application of attention or feed forward",
      "offset": 1100.08,
      "duration": 4.12
    },
    {
      "lang": "en",
      "text": "in addition to that note that the",
      "offset": 1102.039,
      "duration": 4.361
    },
    {
      "lang": "en",
      "text": "normalizations are inside the residual",
      "offset": 1104.2,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "stream you see how feed forward is",
      "offset": 1106.4,
      "duration": 4.519
    },
    {
      "lang": "en",
      "text": "applied and this arrow goes through and",
      "offset": 1108.28,
      "duration": 4.84
    },
    {
      "lang": "en",
      "text": "through the normalization so that means",
      "offset": 1110.919,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "that your residual pathway has",
      "offset": 1113.12,
      "duration": 4.2
    },
    {
      "lang": "en",
      "text": "normalizations inside them and this is",
      "offset": 1115.08,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "not very good or desirable uh you",
      "offset": 1117.32,
      "duration": 4.68
    },
    {
      "lang": "en",
      "text": "actually prefer to have a single uh",
      "offset": 1119.88,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "clean residual stream all the way from",
      "offset": 1122,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "supervision all the way down to the",
      "offset": 1124.12,
      "duration": 4.36
    },
    {
      "lang": "en",
      "text": "inputs the tokens and this is very",
      "offset": 1125.84,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "desirable and nice because the gradients",
      "offset": 1128.48,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "that flow from the top if you remember",
      "offset": 1131.76,
      "duration": 4.6
    },
    {
      "lang": "en",
      "text": "from your microad addition just",
      "offset": 1134,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "distributes gradients during the",
      "offset": 1136.36,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "backwards state to both of its branches",
      "offset": 1138.24,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "equally so addition is a branch in the",
      "offset": 1140.6,
      "duration": 5.64
    },
    {
      "lang": "en",
      "text": "gradients and so that means that the",
      "offset": 1144.08,
      "duration": 4.599
    },
    {
      "lang": "en",
      "text": "gradients from the top flows straight to",
      "offset": 1146.24,
      "duration": 4.6
    },
    {
      "lang": "en",
      "text": "the inputs the tokens through the",
      "offset": 1148.679,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "residual Pathways unchanged but then in",
      "offset": 1150.84,
      "duration": 4.12
    },
    {
      "lang": "en",
      "text": "addition to that the gradient also flows",
      "offset": 1153.4,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "through the blocks and the blocks you",
      "offset": 1154.96,
      "duration": 3.719
    },
    {
      "lang": "en",
      "text": "know contribute their own contribution",
      "offset": 1157,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "over time and kick in and change the",
      "offset": 1158.679,
      "duration": 3.641
    },
    {
      "lang": "en",
      "text": "optimization over time but basically",
      "offset": 1160.12,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "clean residual pathway is desirable from",
      "offset": 1162.32,
      "duration": 6.4
    },
    {
      "lang": "en",
      "text": "an optimization perspective and then the",
      "offset": 1165.32,
      "duration": 5.599
    },
    {
      "lang": "en",
      "text": "this is the pre-normalization version",
      "offset": 1168.72,
      "duration": 4.04
    },
    {
      "lang": "en",
      "text": "where you see that RX first goes through",
      "offset": 1170.919,
      "duration": 3.64
    },
    {
      "lang": "en",
      "text": "the layer normalization and then the",
      "offset": 1172.76,
      "duration": 5.32
    },
    {
      "lang": "en",
      "text": "attention and then goes uh back out to",
      "offset": 1174.559,
      "duration": 6.321
    },
    {
      "lang": "en",
      "text": "go to the L ration number two and the",
      "offset": 1178.08,
      "duration": 5.079
    },
    {
      "lang": "en",
      "text": "multia perceptron sometimes also",
      "offset": 1180.88,
      "duration": 3.96
    },
    {
      "lang": "en",
      "text": "referred to as a feed forward Network or",
      "offset": 1183.159,
      "duration": 4.601
    },
    {
      "lang": "en",
      "text": "an FFN and then that goes into the",
      "offset": 1184.84,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "residual stream again and the one more",
      "offset": 1187.76,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "thing that is kind of interesting to",
      "offset": 1190.36,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "note is that recall that attention is a",
      "offset": 1191.36,
      "duration": 4.52
    },
    {
      "lang": "en",
      "text": "communication operation it is where all",
      "offset": 1193.72,
      "duration": 4.76
    },
    {
      "lang": "en",
      "text": "the tokens and there's 1,24 tokens lined",
      "offset": 1195.88,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "up in a sequence and this is where the",
      "offset": 1198.48,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "tokens communicate this is where they",
      "offset": 1200.84,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "exchange information so attention is a",
      "offset": 1202.4,
      "duration": 6.2
    },
    {
      "lang": "en",
      "text": "um aggregation function it's a pooling",
      "offset": 1206.039,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "function it's a weighted sum function it",
      "offset": 1208.6,
      "duration": 8.199
    },
    {
      "lang": "en",
      "text": "is a reduce operation whereas MLP this",
      "offset": 1212.039,
      "duration": 6.681
    },
    {
      "lang": "en",
      "text": "uh MLP here happens at every single",
      "offset": 1216.799,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "token individually there's no",
      "offset": 1218.72,
      "duration": 3.24
    },
    {
      "lang": "en",
      "text": "information being collected or exchanged",
      "offset": 1220,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "between the tokens so the attention is",
      "offset": 1221.96,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "the reduce and the MLP is the map and",
      "offset": 1224.559,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "what you end up with is that the",
      "offset": 1227.48,
      "duration": 3.319
    },
    {
      "lang": "en",
      "text": "Transformer just ends up just being a",
      "offset": 1228.64,
      "duration": 4.76
    },
    {
      "lang": "en",
      "text": "repeated application of map produce if",
      "offset": 1230.799,
      "duration": 5.24
    },
    {
      "lang": "en",
      "text": "you want to think about it that way so",
      "offset": 1233.4,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "um this is where they communicate and",
      "offset": 1236.039,
      "duration": 3.401
    },
    {
      "lang": "en",
      "text": "this is where they think individually",
      "offset": 1237.559,
      "duration": 3.641
    },
    {
      "lang": "en",
      "text": "about the information that they gathered",
      "offset": 1239.44,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "and every one of these blocks uh",
      "offset": 1241.2,
      "duration": 4.959
    },
    {
      "lang": "en",
      "text": "iteratively refines the um",
      "offset": 1243.36,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "representation is at the residual stream",
      "offset": 1246.159,
      "duration": 4.921
    },
    {
      "lang": "en",
      "text": "so this is our block um slightly",
      "offset": 1248.72,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "modified from this picture Okay so let's",
      "offset": 1251.08,
      "duration": 6.44
    },
    {
      "lang": "en",
      "text": "now move on to the MLP so the MLP block",
      "offset": 1253.6,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "uh I implemented as follows",
      "offset": 1257.52,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "it is relatively straightforward we",
      "offset": 1259.12,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "basically have two linear projections",
      "offset": 1260.72,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "here that are sandwiched in between the",
      "offset": 1262.799,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "G",
      "offset": 1265.12,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "nonlinearity so nn. G approximate is 10h",
      "offset": 1266.4,
      "duration": 6.8
    },
    {
      "lang": "en",
      "text": "now when we swing on uh swing over to",
      "offset": 1271.12,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "the Pyro documentation this is n.g and",
      "offset": 1273.2,
      "duration": 5.16
    },
    {
      "lang": "en",
      "text": "it has this format and it has two",
      "offset": 1276.48,
      "duration": 4.04
    },
    {
      "lang": "en",
      "text": "versions the original version of G which",
      "offset": 1278.36,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "we'll step into into in a bit and the",
      "offset": 1280.52,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "approximate version of Galo which we can",
      "offset": 1282.84,
      "duration": 2.92
    },
    {
      "lang": "en",
      "text": "request using",
      "offset": 1284.52,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "10 so as you can see just as a preview",
      "offset": 1285.76,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "here G is a basically like a reu except",
      "offset": 1288.6,
      "duration": 7.16
    },
    {
      "lang": "en",
      "text": "there's no flat exactly Flat Tail here",
      "offset": 1292.24,
      "duration": 6.2
    },
    {
      "lang": "en",
      "text": "at exactly zero but otherwise it looks",
      "offset": 1295.76,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "very much like a slightly smoother reu",
      "offset": 1298.44,
      "duration": 5.4
    },
    {
      "lang": "en",
      "text": "it comes from this paper here Gan error",
      "offset": 1301.12,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "linear units and uh you can step through",
      "offset": 1303.84,
      "duration": 4.36
    },
    {
      "lang": "en",
      "text": "this paper and there's some mathematical",
      "offset": 1306.64,
      "duration": 3.8
    },
    {
      "lang": "en",
      "text": "calac reasoning that leads to an",
      "offset": 1308.2,
      "duration": 3.4
    },
    {
      "lang": "en",
      "text": "interpretation that leads to the",
      "offset": 1310.44,
      "duration": 3.479
    },
    {
      "lang": "en",
      "text": "specific formulation it has to do with",
      "offset": 1311.6,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "stochastic radial risers and the",
      "offset": 1313.919,
      "duration": 3.561
    },
    {
      "lang": "en",
      "text": "expectation of a modification to",
      "offset": 1316.08,
      "duration": 3.16
    },
    {
      "lang": "en",
      "text": "Adaptive dropout so you can read through",
      "offset": 1317.48,
      "duration": 4.36
    },
    {
      "lang": "en",
      "text": "all of that if you'd like here and",
      "offset": 1319.24,
      "duration": 4.12
    },
    {
      "lang": "en",
      "text": "there's a little bit of history as to",
      "offset": 1321.84,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "why there is an an approximate version",
      "offset": 1323.36,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "of G and that comes from this issue here",
      "offset": 1325.2,
      "duration": 6.32
    },
    {
      "lang": "en",
      "text": "as far as I can tell and in this issue",
      "offset": 1328.32,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "Daniel Hendrix mentions that at the time",
      "offset": 1331.52,
      "duration": 5.48
    },
    {
      "lang": "en",
      "text": "when they developed this nonlinearity",
      "offset": 1334.24,
      "duration": 4.76
    },
    {
      "lang": "en",
      "text": "the Earth function which you need to",
      "offset": 1337,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "evaluate the exact G was very slow in",
      "offset": 1339,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "tensor flow so they ended up basically",
      "offset": 1341.559,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "developing this approximation and this",
      "offset": 1343.08,
      "duration": 3.959
    },
    {
      "lang": "en",
      "text": "approximation that then ended up being",
      "offset": 1345.159,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "picked up by Bert and by GP P2 Etc but",
      "offset": 1347.039,
      "duration": 4.601
    },
    {
      "lang": "en",
      "text": "today there's no real good reason to use",
      "offset": 1350.159,
      "duration": 3.161
    },
    {
      "lang": "en",
      "text": "the approximate version you'd prefer to",
      "offset": 1351.64,
      "duration": 5.08
    },
    {
      "lang": "en",
      "text": "just use the exact version um because I",
      "offset": 1353.32,
      "duration": 4.959
    },
    {
      "lang": "en",
      "text": "my expectation is that there's no big",
      "offset": 1356.72,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "difference anymore and this is kind of",
      "offset": 1358.279,
      "duration": 5.561
    },
    {
      "lang": "en",
      "text": "like a historical um kind of Quirk um",
      "offset": 1360.08,
      "duration": 5.56
    },
    {
      "lang": "en",
      "text": "but we are trying to reproduce gpt2",
      "offset": 1363.84,
      "duration": 5.16
    },
    {
      "lang": "en",
      "text": "exactly and gpt2 used the 10h",
      "offset": 1365.64,
      "duration": 5.72
    },
    {
      "lang": "en",
      "text": "approximate version so we prefer to",
      "offset": 1369,
      "duration": 3.4
    },
    {
      "lang": "en",
      "text": "stick with",
      "offset": 1371.36,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "that um now one other reason to actually",
      "offset": 1372.4,
      "duration": 5.159
    },
    {
      "lang": "en",
      "text": "just intuitively use G instead of veru",
      "offset": 1375.36,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "is previously in the in videos in the",
      "offset": 1377.559,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "past we've spoken about the dead reu",
      "offset": 1379.679,
      "duration": 5.161
    },
    {
      "lang": "en",
      "text": "neuron problem where in this tale of a",
      "offset": 1382.039,
      "duration": 5.721
    },
    {
      "lang": "en",
      "text": "reu if it's exactly flat at zero any",
      "offset": 1384.84,
      "duration": 4.959
    },
    {
      "lang": "en",
      "text": "activations that fall there will get",
      "offset": 1387.76,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "exactly zero gradient there's no change",
      "offset": 1389.799,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "there's no adaptation there's no",
      "offset": 1391.6,
      "duration": 3.88
    },
    {
      "lang": "en",
      "text": "development of the network if any of",
      "offset": 1393.48,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "these activations end in this flat",
      "offset": 1395.48,
      "duration": 4.679
    },
    {
      "lang": "en",
      "text": "region but the G always contributes a",
      "offset": 1397.32,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "local gradient and so there's always",
      "offset": 1400.159,
      "duration": 3.281
    },
    {
      "lang": "en",
      "text": "going to be a change always going to be",
      "offset": 1402.039,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "an adaptation and sort of smoothing it",
      "offset": 1403.44,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "out ends up empirically working better",
      "offset": 1405.48,
      "duration": 3.64
    },
    {
      "lang": "en",
      "text": "in practice as demonstrated in this",
      "offset": 1407.36,
      "duration": 3.88
    },
    {
      "lang": "en",
      "text": "paper and also as demonstrated by it",
      "offset": 1409.12,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "being picked up by the bird paper gbt2",
      "offset": 1411.24,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "paper and so on so for that reason we",
      "offset": 1413.52,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "adopt this nonlinearity uh here in the",
      "offset": 1415.799,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "10 in the gbt2 reproduction now in more",
      "offset": 1418.32,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "modern networks also like llama 3 and so",
      "offset": 1421,
      "duration": 4.44
    },
    {
      "lang": "en",
      "text": "on this nonlinearity also further",
      "offset": 1423.039,
      "duration": 5.081
    },
    {
      "lang": "en",
      "text": "changes uh to swiglo and other variants",
      "offset": 1425.44,
      "duration": 5.16
    },
    {
      "lang": "en",
      "text": "like that uh but for gpt2 they Ed this",
      "offset": 1428.12,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "approximate",
      "offset": 1430.6,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "G okay and finally we have the attention",
      "offset": 1431.799,
      "duration": 5.281
    },
    {
      "lang": "en",
      "text": "operation so let me paste in my",
      "offset": 1434.6,
      "duration": 5.48
    },
    {
      "lang": "en",
      "text": "attention",
      "offset": 1437.08,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "so I know this is a lot so I'm going to",
      "offset": 1440.44,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "go through this a bit quickly a bit",
      "offset": 1442.08,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "slowly but not too slowly because we",
      "offset": 1443.88,
      "duration": 3.399
    },
    {
      "lang": "en",
      "text": "have covered this in the previous video",
      "offset": 1445.919,
      "duration": 4.201
    },
    {
      "lang": "en",
      "text": "and I would just point you there um so",
      "offset": 1447.279,
      "duration": 4.76
    },
    {
      "lang": "en",
      "text": "this is the attention operation now in",
      "offset": 1450.12,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "the previous video you will remember",
      "offset": 1452.039,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "this is not just attention this is um",
      "offset": 1453.799,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "multi-headed attention right and so in",
      "offset": 1456.52,
      "duration": 4.12
    },
    {
      "lang": "en",
      "text": "the previous video we had this",
      "offset": 1459.559,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "multi-headed attention module and this",
      "offset": 1460.64,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "implementation made it obvious that",
      "offset": 1463.72,
      "duration": 3.079
    },
    {
      "lang": "en",
      "text": "these heads are not actually that",
      "offset": 1465.279,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "complicated uh there's basically",
      "offset": 1466.799,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "in parallel inside every attention block",
      "offset": 1468.88,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "there's multiple heads and they're all",
      "offset": 1472.159,
      "duration": 4.52
    },
    {
      "lang": "en",
      "text": "functioning in parallel and uh their",
      "offset": 1473.6,
      "duration": 5.16
    },
    {
      "lang": "en",
      "text": "outputs are just being concatenated and",
      "offset": 1476.679,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "that becomes the output of the",
      "offset": 1478.76,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "multi-headed attention so the heads are",
      "offset": 1480.52,
      "duration": 4.68
    },
    {
      "lang": "en",
      "text": "just kind of like parallel streams and",
      "offset": 1482.919,
      "duration": 3.64
    },
    {
      "lang": "en",
      "text": "their outputs get",
      "offset": 1485.2,
      "duration": 3.4
    },
    {
      "lang": "en",
      "text": "concatenated and so it was very simple",
      "offset": 1486.559,
      "duration": 4.84
    },
    {
      "lang": "en",
      "text": "and made the head be kind of like U",
      "offset": 1488.6,
      "duration": 5.439
    },
    {
      "lang": "en",
      "text": "fairly straightforward in terms of its",
      "offset": 1491.399,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "implementation what happens here is that",
      "offset": 1494.039,
      "duration": 4.041
    },
    {
      "lang": "en",
      "text": "instead of having two separate modules",
      "offset": 1496.36,
      "duration": 3.48
    },
    {
      "lang": "en",
      "text": "and indeed many more modules that get",
      "offset": 1498.08,
      "duration": 3.8
    },
    {
      "lang": "en",
      "text": "concatenated all of that is just put",
      "offset": 1499.84,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "into a single uh self attention uh",
      "offset": 1501.88,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "module and instead I'm being very",
      "offset": 1504.88,
      "duration": 5.399
    },
    {
      "lang": "en",
      "text": "careful and doing a bunch of transpose",
      "offset": 1507.32,
      "duration": 6.079
    },
    {
      "lang": "en",
      "text": "split um tensor gymnastics to make this",
      "offset": 1510.279,
      "duration": 5.321
    },
    {
      "lang": "en",
      "text": "very efficient in pych but fundamentally",
      "offset": 1513.399,
      "duration": 3.721
    },
    {
      "lang": "en",
      "text": "and algorithmically nothing is different",
      "offset": 1515.6,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "from the implementation we saw",
      "offset": 1517.12,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "before um in this uh give",
      "offset": 1519.76,
      "duration": 6.159
    },
    {
      "lang": "en",
      "text": "repository so to remind you very briefly",
      "offset": 1522.72,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "and I don't want to go in this uh into",
      "offset": 1525.919,
      "duration": 4.681
    },
    {
      "lang": "en",
      "text": "this in too many in too much time but we",
      "offset": 1527.919,
      "duration": 4.76
    },
    {
      "lang": "en",
      "text": "have these tokens lined up in a sequence",
      "offset": 1530.6,
      "duration": 5.319
    },
    {
      "lang": "en",
      "text": "and there's 1,20 of them and then each",
      "offset": 1532.679,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "token at this stage of the attention",
      "offset": 1535.919,
      "duration": 4.841
    },
    {
      "lang": "en",
      "text": "emits three vectors the query key and",
      "offset": 1537.88,
      "duration": 7.08
    },
    {
      "lang": "en",
      "text": "the value and first what happens here um",
      "offset": 1540.76,
      "duration": 6.159
    },
    {
      "lang": "en",
      "text": "is that the queries and the keys have to",
      "offset": 1544.96,
      "duration": 4.439
    },
    {
      "lang": "en",
      "text": "multiply each other to get sort of the",
      "offset": 1546.919,
      "duration": 5.721
    },
    {
      "lang": "en",
      "text": "attention um amount like how interesting",
      "offset": 1549.399,
      "duration": 4.841
    },
    {
      "lang": "en",
      "text": "they find each other so they have to",
      "offset": 1552.64,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "interact multiplicatively so what we're",
      "offset": 1554.24,
      "duration": 4.2
    },
    {
      "lang": "en",
      "text": "doing here is we're calculating the qkv",
      "offset": 1556.32,
      "duration": 4.12
    },
    {
      "lang": "en",
      "text": "we splitting it and then there's a bunch",
      "offset": 1558.44,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "of gymnastics as I mentioned here and",
      "offset": 1560.44,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "the way this works is that we're",
      "offset": 1563.08,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "basically making the number of heads and",
      "offset": 1564.279,
      "duration": 6.12
    },
    {
      "lang": "en",
      "text": "H into a batch Dimension and so it's a",
      "offset": 1566.919,
      "duration": 5.841
    },
    {
      "lang": "en",
      "text": "batch Dimension just like B so that in",
      "offset": 1570.399,
      "duration": 4.52
    },
    {
      "lang": "en",
      "text": "these operations that follow pytorch",
      "offset": 1572.76,
      "duration": 5.399
    },
    {
      "lang": "en",
      "text": "treats B and NH as batches and it",
      "offset": 1574.919,
      "duration": 5.48
    },
    {
      "lang": "en",
      "text": "applies all the operations on all of",
      "offset": 1578.159,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "them in parallel in both the batch and",
      "offset": 1580.399,
      "duration": 2.841
    },
    {
      "lang": "en",
      "text": "the",
      "offset": 1582.559,
      "duration": 2.881
    },
    {
      "lang": "en",
      "text": "heads and the operations that get",
      "offset": 1583.24,
      "duration": 3.96
    },
    {
      "lang": "en",
      "text": "applied are number one the queries and",
      "offset": 1585.44,
      "duration": 4.959
    },
    {
      "lang": "en",
      "text": "the keys intera to give us her attention",
      "offset": 1587.2,
      "duration": 4.92
    },
    {
      "lang": "en",
      "text": "this is the autoaggressive mask that",
      "offset": 1590.399,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "makes sure that the tokens only attend",
      "offset": 1592.12,
      "duration": 5.32
    },
    {
      "lang": "en",
      "text": "to tokens before them and never to",
      "offset": 1595.2,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "tokens in the",
      "offset": 1597.44,
      "duration": 4.359
    },
    {
      "lang": "en",
      "text": "future the softmax here normalizes the",
      "offset": 1599.039,
      "duration": 6.281
    },
    {
      "lang": "en",
      "text": "attention so it sums to one always and",
      "offset": 1601.799,
      "duration": 5.321
    },
    {
      "lang": "en",
      "text": "then recall from the previous video that",
      "offset": 1605.32,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "doing the attention Matrix multiply with",
      "offset": 1607.12,
      "duration": 3.559
    },
    {
      "lang": "en",
      "text": "the values is basically a way to do a",
      "offset": 1608.76,
      "duration": 4.519
    },
    {
      "lang": "en",
      "text": "weighted sum of the values of the tokens",
      "offset": 1610.679,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "that we found interesting at every",
      "offset": 1613.279,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "single token and then the final",
      "offset": 1615.08,
      "duration": 4.76
    },
    {
      "lang": "en",
      "text": "transpose conf VI and view is just",
      "offset": 1617.44,
      "duration": 4.839
    },
    {
      "lang": "en",
      "text": "reassembling all of that again and this",
      "offset": 1619.84,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "actually performs the concatenation",
      "offset": 1622.279,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "operation so you can step through this",
      "offset": 1624.159,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "uh slowly if you'd like um but it is",
      "offset": 1626.279,
      "duration": 4.601
    },
    {
      "lang": "en",
      "text": "equivalent mathematically to our",
      "offset": 1628.64,
      "duration": 4.279
    },
    {
      "lang": "en",
      "text": "previous implementation is just more",
      "offset": 1630.88,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "efficient in P torch so that's why I",
      "offset": 1632.919,
      "duration": 3.24
    },
    {
      "lang": "en",
      "text": "chose this implementation",
      "offset": 1634.559,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "instead now in addition to that I'm",
      "offset": 1636.159,
      "duration": 3.4
    },
    {
      "lang": "en",
      "text": "being careful with how I name my",
      "offset": 1638.159,
      "duration": 4.281
    },
    {
      "lang": "en",
      "text": "variables so for example cattin is the",
      "offset": 1639.559,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "same as seaten and so actually our keys",
      "offset": 1642.44,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "should basically exactly follow the",
      "offset": 1645.399,
      "duration": 2.76
    },
    {
      "lang": "en",
      "text": "schema of the hugging face train",
      "offset": 1647,
      "duration": 2.799
    },
    {
      "lang": "en",
      "text": "Transformers code and that will make it",
      "offset": 1648.159,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "very easy for us to now Port over all",
      "offset": 1649.799,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "the weights from exactly this sort of",
      "offset": 1652.32,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "naming conventions because all of our",
      "offset": 1654.919,
      "duration": 4.601
    },
    {
      "lang": "en",
      "text": "variables are named the same thing but",
      "offset": 1656.88,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "um at this point we have finished the",
      "offset": 1659.52,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "gpt2 implementation and what that allows",
      "offset": 1661.679,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "us to do is we don't have to basically",
      "offset": 1664.399,
      "duration": 3.801
    },
    {
      "lang": "en",
      "text": "use uh this file from hugging face which",
      "offset": 1666.08,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "is fairly long",
      "offset": 1668.2,
      "duration": 4.76
    },
    {
      "lang": "en",
      "text": "um this",
      "offset": 1670,
      "duration": 7.32
    },
    {
      "lang": "en",
      "text": "is uh 2,000 lines of code um instead we",
      "offset": 1672.96,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "just have a less than 100 lines of code",
      "offset": 1677.32,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "and this is the complete uh gpd2",
      "offset": 1679.44,
      "duration": 3.4
    },
    {
      "lang": "en",
      "text": "implementation so at this stage we",
      "offset": 1681.159,
      "duration": 3.721
    },
    {
      "lang": "en",
      "text": "should just be able to take over all the",
      "offset": 1682.84,
      "duration": 4.839
    },
    {
      "lang": "en",
      "text": "weights set them and then do generation",
      "offset": 1684.88,
      "duration": 4.519
    },
    {
      "lang": "en",
      "text": "so let's see what that looks like okay",
      "offset": 1687.679,
      "duration": 3.961
    },
    {
      "lang": "en",
      "text": "so here I've also changed the GPT config",
      "offset": 1689.399,
      "duration": 3.721
    },
    {
      "lang": "en",
      "text": "so that the numbers here the H",
      "offset": 1691.64,
      "duration": 4.039
    },
    {
      "lang": "en",
      "text": "parameters agree with the gpt2 124 M",
      "offset": 1693.12,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "model so the maximum sequence length",
      "offset": 1695.679,
      "duration": 5.321
    },
    {
      "lang": "en",
      "text": "which I call block size here is 124 the",
      "offset": 1697.6,
      "duration": 7.4
    },
    {
      "lang": "en",
      "text": "number of tokens is 50250 257 which if",
      "offset": 1701,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "you watch my tokenizer video know that",
      "offset": 1705,
      "duration": 6.559
    },
    {
      "lang": "en",
      "text": "this is 50,000 m merges BP merges 256",
      "offset": 1707,
      "duration": 8
    },
    {
      "lang": "en",
      "text": "bite tokens the leaves of the BP tree",
      "offset": 1711.559,
      "duration": 5.24
    },
    {
      "lang": "en",
      "text": "and one special end of text token that",
      "offset": 1715,
      "duration": 3.799
    },
    {
      "lang": "en",
      "text": "delimits different documents and can",
      "offset": 1716.799,
      "duration": 4.76
    },
    {
      "lang": "en",
      "text": "start generation as well and there are",
      "offset": 1718.799,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "12 layers there are 12 heads in the",
      "offset": 1721.559,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "attention and the dimension of the",
      "offset": 1723.2,
      "duration": 3.719
    },
    {
      "lang": "en",
      "text": "Transformers was",
      "offset": 1725.399,
      "duration": 4.201
    },
    {
      "lang": "en",
      "text": "768 so here's how we can now load the",
      "offset": 1726.919,
      "duration": 5.561
    },
    {
      "lang": "en",
      "text": "parameters from hugging face to uh our",
      "offset": 1729.6,
      "duration": 5.24
    },
    {
      "lang": "en",
      "text": "code here and initialize the GPT class",
      "offset": 1732.48,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "with those parameters so let me just",
      "offset": 1734.84,
      "duration": 4.199
    },
    {
      "lang": "en",
      "text": "copy paste a bunch of code",
      "offset": 1736.48,
      "duration": 4.439
    },
    {
      "lang": "en",
      "text": "here and I'm not going to go through",
      "offset": 1739.039,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "this code too slow too quickly too",
      "offset": 1740.919,
      "duration": 6.721
    },
    {
      "lang": "en",
      "text": "slowly because um honestly it's not that",
      "offset": 1743.919,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "interesting it's not that exciting we're",
      "offset": 1747.64,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "just loading the weights so it's kind of",
      "offset": 1748.88,
      "duration": 3.96
    },
    {
      "lang": "en",
      "text": "dry but as I mentioned there are four",
      "offset": 1750.36,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "models in this miniseries of gpt2 this",
      "offset": 1752.84,
      "duration": 5.24
    },
    {
      "lang": "en",
      "text": "is some of the Jupiter code um code that",
      "offset": 1755.36,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "we had here on the right I'm just pting",
      "offset": 1758.08,
      "duration": 4.12
    },
    {
      "lang": "en",
      "text": "it over these are the hyper parameters",
      "offset": 1760.32,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "of the gpt2 models uh we're creating the",
      "offset": 1762.2,
      "duration": 5.079
    },
    {
      "lang": "en",
      "text": "config object and creating our own model",
      "offset": 1764.96,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "and then what's Happening Here is we're",
      "offset": 1767.279,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "creating the state dict both for our",
      "offset": 1768.399,
      "duration": 5.4
    },
    {
      "lang": "en",
      "text": "model and for the hugging face",
      "offset": 1770.64,
      "duration": 5.919
    },
    {
      "lang": "en",
      "text": "model um and then what we're doing here",
      "offset": 1773.799,
      "duration": 4.201
    },
    {
      "lang": "en",
      "text": "is we're going over the hugging face",
      "offset": 1776.559,
      "duration": 5.801
    },
    {
      "lang": "en",
      "text": "model keys and we're copying over those",
      "offset": 1778,
      "duration": 7.08
    },
    {
      "lang": "en",
      "text": "tensors and in the process we are kind",
      "offset": 1782.36,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "of ignoring a few of the buffers they're",
      "offset": 1785.08,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "not parameters they're buffers so for",
      "offset": 1787.36,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "example attention dobias uh that's just",
      "offset": 1789.08,
      "duration": 4.68
    },
    {
      "lang": "en",
      "text": "used for the autoaggressive mask and so",
      "offset": 1791.2,
      "duration": 5.599
    },
    {
      "lang": "en",
      "text": "we are ignoring some of those masks and",
      "offset": 1793.76,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "uh that's it and then then one",
      "offset": 1796.799,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "additional kind of annoyance is that",
      "offset": 1798.399,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "this comes from the tensorflow repo and",
      "offset": 1800.559,
      "duration": 3.48
    },
    {
      "lang": "en",
      "text": "I'm not sure how this is a little bit",
      "offset": 1802.24,
      "duration": 3.279
    },
    {
      "lang": "en",
      "text": "annoying but some of the weights are",
      "offset": 1804.039,
      "duration": 4.12
    },
    {
      "lang": "en",
      "text": "transposed from what pytorch would want",
      "offset": 1805.519,
      "duration": 5.081
    },
    {
      "lang": "en",
      "text": "and so manually I hardcoded the weights",
      "offset": 1808.159,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "that should be transposed and then we",
      "offset": 1810.6,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "transpose them if that is so and then we",
      "offset": 1812.64,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "return this model so the from",
      "offset": 1815.48,
      "duration": 4.919
    },
    {
      "lang": "en",
      "text": "pre-trained is a",
      "offset": 1818.32,
      "duration": 5.239
    },
    {
      "lang": "en",
      "text": "Constructor or class method in Python",
      "offset": 1820.399,
      "duration": 6.201
    },
    {
      "lang": "en",
      "text": "that Returns the GPT object if we just",
      "offset": 1823.559,
      "duration": 5.161
    },
    {
      "lang": "en",
      "text": "give it the model type which in our case",
      "offset": 1826.6,
      "duration": 3.959
    },
    {
      "lang": "en",
      "text": "is gpt2 the smallest model that we're",
      "offset": 1828.72,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "interested in so this is the code and",
      "offset": 1830.559,
      "duration": 5.401
    },
    {
      "lang": "en",
      "text": "this is how you would use it and um we",
      "offset": 1833.12,
      "duration": 5.64
    },
    {
      "lang": "en",
      "text": "can pop open the terminal here in vs",
      "offset": 1835.96,
      "duration": 8.36
    },
    {
      "lang": "en",
      "text": "code and we can python train gbt2 pi and",
      "offset": 1838.76,
      "duration": 7.96
    },
    {
      "lang": "en",
      "text": "fingers",
      "offset": 1844.32,
      "duration": 2.4
    },
    {
      "lang": "en",
      "text": "crossed okay so we didn't crash and so",
      "offset": 1846.88,
      "duration": 5.799
    },
    {
      "lang": "en",
      "text": "we can load the weights and the biases",
      "offset": 1850.559,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "and everything else into our Ann module",
      "offset": 1852.679,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "but now let's also get additional",
      "offset": 1855.76,
      "duration": 2.56
    },
    {
      "lang": "en",
      "text": "confidence that this is working and",
      "offset": 1857.08,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "let's try to actually generate from this",
      "offset": 1858.32,
      "duration": 3.56
    },
    {
      "lang": "en",
      "text": "model okay now before we can actually",
      "offset": 1860.08,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "generate from this model we have to be",
      "offset": 1861.88,
      "duration": 3.039
    },
    {
      "lang": "en",
      "text": "able to forward it we didn't actually",
      "offset": 1863.12,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "write that code yet so here's the",
      "offset": 1864.919,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "forward",
      "offset": 1866.96,
      "duration": 4.12
    },
    {
      "lang": "en",
      "text": "function so the input to the forward is",
      "offset": 1868.12,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "going to be our indices our tokens uh",
      "offset": 1871.08,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "token indices and they are always of",
      "offset": 1873.96,
      "duration": 5.64
    },
    {
      "lang": "en",
      "text": "shape B BYT and so we have batch",
      "offset": 1876.08,
      "duration": 6.199
    },
    {
      "lang": "en",
      "text": "dimension of B and then we have the time",
      "offset": 1879.6,
      "duration": 6.439
    },
    {
      "lang": "en",
      "text": "dimension of up to T and the T can't be",
      "offset": 1882.279,
      "duration": 5.481
    },
    {
      "lang": "en",
      "text": "more than the block size the block size",
      "offset": 1886.039,
      "duration": 4.6
    },
    {
      "lang": "en",
      "text": "is is the maximum sequence length so B",
      "offset": 1887.76,
      "duration": 5.08
    },
    {
      "lang": "en",
      "text": "BYT indices arranged is sort of like a",
      "offset": 1890.639,
      "duration": 4.441
    },
    {
      "lang": "en",
      "text": "two-dimensional layout and remember that",
      "offset": 1892.84,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "basically every single row of this is of",
      "offset": 1895.08,
      "duration": 6.36
    },
    {
      "lang": "en",
      "text": "size up to uh block size and this is T",
      "offset": 1897.559,
      "duration": 5.881
    },
    {
      "lang": "en",
      "text": "tokens that are in a sequence and then",
      "offset": 1901.44,
      "duration": 4.959
    },
    {
      "lang": "en",
      "text": "we have B independent sequences stacked",
      "offset": 1903.44,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "up in a batch so that this is",
      "offset": 1906.399,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "efficient now here we are forwarding the",
      "offset": 1908.639,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "position embeddings and the token",
      "offset": 1911.2,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "embeddings and this code should be very",
      "offset": 1912.24,
      "duration": 4.279
    },
    {
      "lang": "en",
      "text": "recognizable from the previous lecture",
      "offset": 1914.399,
      "duration": 5.361
    },
    {
      "lang": "en",
      "text": "so um we basically use uh a range which",
      "offset": 1916.519,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "is kind of like a version of range but",
      "offset": 1919.76,
      "duration": 4.36
    },
    {
      "lang": "en",
      "text": "for pytorch uh and we're iterating from",
      "offset": 1921.32,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "Z to T and creating this uh positions uh",
      "offset": 1924.12,
      "duration": 6.12
    },
    {
      "lang": "en",
      "text": "sort of uh indices",
      "offset": 1927.32,
      "duration": 5.079
    },
    {
      "lang": "en",
      "text": "um and then we are making sure that",
      "offset": 1930.24,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "they're in the same device as idx",
      "offset": 1932.399,
      "duration": 2.841
    },
    {
      "lang": "en",
      "text": "because we're not going to be training",
      "offset": 1934.24,
      "duration": 2.52
    },
    {
      "lang": "en",
      "text": "on only CPU that's going to be too",
      "offset": 1935.24,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "inefficient we want to be training on",
      "offset": 1936.76,
      "duration": 3.72
    },
    {
      "lang": "en",
      "text": "GPU and that's going to come in in a",
      "offset": 1938.24,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "bit uh then we have the position",
      "offset": 1940.48,
      "duration": 3.88
    },
    {
      "lang": "en",
      "text": "embeddings and the token embeddings and",
      "offset": 1942.32,
      "duration": 4.599
    },
    {
      "lang": "en",
      "text": "the addition operation of those two now",
      "offset": 1944.36,
      "duration": 3.96
    },
    {
      "lang": "en",
      "text": "notice that the position embed are going",
      "offset": 1946.919,
      "duration": 4.201
    },
    {
      "lang": "en",
      "text": "to be identical for every single row of",
      "offset": 1948.32,
      "duration": 5.56
    },
    {
      "lang": "en",
      "text": "uh of input and so there's broadcasting",
      "offset": 1951.12,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "hidden inside this plus where we have to",
      "offset": 1953.88,
      "duration": 4.679
    },
    {
      "lang": "en",
      "text": "create an additional Dimension here and",
      "offset": 1956.72,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "then these two add up because the same",
      "offset": 1958.559,
      "duration": 3.401
    },
    {
      "lang": "en",
      "text": "position embeddings apply at every",
      "offset": 1960.639,
      "duration": 3.64
    },
    {
      "lang": "en",
      "text": "single row of our example stacked up in",
      "offset": 1961.96,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "a batch then we forward the Transformer",
      "offset": 1964.279,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "blocks and finally the last layer norm",
      "offset": 1966.84,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "and the LM head so what comes out after",
      "offset": 1969.08,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "forward is the logits and if the input",
      "offset": 1972.36,
      "duration": 6.12
    },
    {
      "lang": "en",
      "text": "was B BYT indices then at every single B",
      "offset": 1975.24,
      "duration": 7.72
    },
    {
      "lang": "en",
      "text": "by T we will calculate the uh logits for",
      "offset": 1978.48,
      "duration": 7
    },
    {
      "lang": "en",
      "text": "what token comes next in the sequence so",
      "offset": 1982.96,
      "duration": 6.599
    },
    {
      "lang": "en",
      "text": "what is the token B t+1 the one on the",
      "offset": 1985.48,
      "duration": 7.48
    },
    {
      "lang": "en",
      "text": "right of this token and B app size here",
      "offset": 1989.559,
      "duration": 6.561
    },
    {
      "lang": "en",
      "text": "is the number of possible tokens and so",
      "offset": 1992.96,
      "duration": 4.76
    },
    {
      "lang": "en",
      "text": "therefore this is the tensor that we're",
      "offset": 1996.12,
      "duration": 3.72
    },
    {
      "lang": "en",
      "text": "going to obtain and these low jits are",
      "offset": 1997.72,
      "duration": 4.959
    },
    {
      "lang": "en",
      "text": "just a softmax away from becoming",
      "offset": 1999.84,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "probabilities so this is the forward",
      "offset": 2002.679,
      "duration": 4.84
    },
    {
      "lang": "en",
      "text": "pass of the network and now we can get",
      "offset": 2005.2,
      "duration": 3.8
    },
    {
      "lang": "en",
      "text": "load and so we're going to be able to",
      "offset": 2007.519,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "generate from the model",
      "offset": 2009,
      "duration": 3.519
    },
    {
      "lang": "en",
      "text": "imminently okay so now we're going to",
      "offset": 2010.639,
      "duration": 4.361
    },
    {
      "lang": "en",
      "text": "try to set up the identical thing on the",
      "offset": 2012.519,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "left here that matches hug and face on",
      "offset": 2015,
      "duration": 4.2
    },
    {
      "lang": "en",
      "text": "the right so here we've sampled from the",
      "offset": 2016.679,
      "duration": 5.96
    },
    {
      "lang": "en",
      "text": "pipeline and we sampled five times up to",
      "offset": 2019.2,
      "duration": 6.12
    },
    {
      "lang": "en",
      "text": "30 tokens with the prefix of hello I'm a",
      "offset": 2022.639,
      "duration": 4.04
    },
    {
      "lang": "en",
      "text": "language model and these are the",
      "offset": 2025.32,
      "duration": 3.16
    },
    {
      "lang": "en",
      "text": "completions that we achieved so we're",
      "offset": 2026.679,
      "duration": 2.921
    },
    {
      "lang": "en",
      "text": "going to try to replicate that on the",
      "offset": 2028.48,
      "duration": 3.4
    },
    {
      "lang": "en",
      "text": "left here so number turn sequences is",
      "offset": 2029.6,
      "duration": 4.28
    },
    {
      "lang": "en",
      "text": "five max length is 30 so the first thing",
      "offset": 2031.88,
      "duration": 3.279
    },
    {
      "lang": "en",
      "text": "we do of course is we initialize our",
      "offset": 2033.88,
      "duration": 3.48
    },
    {
      "lang": "en",
      "text": "model then we put it into evaluation",
      "offset": 2035.159,
      "duration": 4.76
    },
    {
      "lang": "en",
      "text": "mode now this is a good practice to put",
      "offset": 2037.36,
      "duration": 4.039
    },
    {
      "lang": "en",
      "text": "the model into eval when you're not",
      "offset": 2039.919,
      "duration": 2.521
    },
    {
      "lang": "en",
      "text": "going to be training it you're just",
      "offset": 2041.399,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "going to be using it and I don't",
      "offset": 2042.44,
      "duration": 4.68
    },
    {
      "lang": "en",
      "text": "actually know if this is doing anything",
      "offset": 2045.799,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "right now for the following reason our",
      "offset": 2047.12,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "model up above here contains no modules",
      "offset": 2049.079,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "or layers that actually have a different",
      "offset": 2051.52,
      "duration": 4.68
    },
    {
      "lang": "en",
      "text": "uh Behavior at training or evaluation",
      "offset": 2054.04,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "time so for example Dropout batch norm",
      "offset": 2056.2,
      "duration": 3.879
    },
    {
      "lang": "en",
      "text": "and a bunch of other layers have this",
      "offset": 2058.52,
      "duration": 3.559
    },
    {
      "lang": "en",
      "text": "kind of behavior but all of these layers",
      "offset": 2060.079,
      "duration": 3.481
    },
    {
      "lang": "en",
      "text": "that we've used here should be identical",
      "offset": 2062.079,
      "duration": 5.401
    },
    {
      "lang": "en",
      "text": "in both training and evaluation time um",
      "offset": 2063.56,
      "duration": 6.039
    },
    {
      "lang": "en",
      "text": "so so potentially model that eval does",
      "offset": 2067.48,
      "duration": 3.72
    },
    {
      "lang": "en",
      "text": "nothing but then I'm not actually sure",
      "offset": 2069.599,
      "duration": 3.401
    },
    {
      "lang": "en",
      "text": "if this is the case and maybe pytorch",
      "offset": 2071.2,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "internals uh do some clever things",
      "offset": 2073,
      "duration": 3.72
    },
    {
      "lang": "en",
      "text": "depending on the evaluation mode uh",
      "offset": 2075.04,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "inside here the next thing we're doing",
      "offset": 2076.72,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "here is we are moving the entire model",
      "offset": 2079.44,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "to Cuda so we're moving this all of the",
      "offset": 2081.72,
      "duration": 5.879
    },
    {
      "lang": "en",
      "text": "tensors to GPU so I'm sshed here to a",
      "offset": 2084.24,
      "duration": 5.32
    },
    {
      "lang": "en",
      "text": "cloud box and I have a bunch of gpus on",
      "offset": 2087.599,
      "duration": 5.48
    },
    {
      "lang": "en",
      "text": "this box and here I'm moving the entire",
      "offset": 2089.56,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "model and all of its members and all of",
      "offset": 2093.079,
      "duration": 3.721
    },
    {
      "lang": "en",
      "text": "its tensors and everything like that",
      "offset": 2094.76,
      "duration": 4.52
    },
    {
      "lang": "en",
      "text": "everything gets shipped off to basically",
      "offset": 2096.8,
      "duration": 4.44
    },
    {
      "lang": "en",
      "text": "a whole separate computer that is",
      "offset": 2099.28,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "sitting on the GPU and the GPU is",
      "offset": 2101.24,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "connected to the uh CPU and they can",
      "offset": 2103.28,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "communicate but it's basically a whole",
      "offset": 2105.48,
      "duration": 2.84
    },
    {
      "lang": "en",
      "text": "separate computer with its own computer",
      "offset": 2106.72,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "architecture and it's really well",
      "offset": 2108.32,
      "duration": 3.64
    },
    {
      "lang": "en",
      "text": "catered to parallel processing tasks",
      "offset": 2109.72,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "like those of running neural networks so",
      "offset": 2111.96,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "I'm doing this so that the model lives",
      "offset": 2114.359,
      "duration": 5.161
    },
    {
      "lang": "en",
      "text": "on the GPU a whole separate computer and",
      "offset": 2116.359,
      "duration": 4.521
    },
    {
      "lang": "en",
      "text": "it's just going to make our code a lot",
      "offset": 2119.52,
      "duration": 3.319
    },
    {
      "lang": "en",
      "text": "more efficient because all of this stuff",
      "offset": 2120.88,
      "duration": 4.199
    },
    {
      "lang": "en",
      "text": "runs a lot more efficiently on the",
      "offset": 2122.839,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "gpus so that's the model",
      "offset": 2125.079,
      "duration": 6.801
    },
    {
      "lang": "en",
      "text": "itself now uh the next thing we want to",
      "offset": 2129.079,
      "duration": 5.321
    },
    {
      "lang": "en",
      "text": "do is we want to start with this as the",
      "offset": 2131.88,
      "duration": 5.4
    },
    {
      "lang": "en",
      "text": "prefix when we do the generation so",
      "offset": 2134.4,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "let's actually create those prefix",
      "offset": 2137.28,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "tokens so here's the code that I've",
      "offset": 2139.599,
      "duration": 3.641
    },
    {
      "lang": "en",
      "text": "written we're going to import the tich",
      "offset": 2141.52,
      "duration": 4.2
    },
    {
      "lang": "en",
      "text": "token library from open Ai and we're",
      "offset": 2143.24,
      "duration": 4.839
    },
    {
      "lang": "en",
      "text": "going to get the gpt2 encoding so that's",
      "offset": 2145.72,
      "duration": 5.359
    },
    {
      "lang": "en",
      "text": "the tokenizer for gpt2 and then we're",
      "offset": 2148.079,
      "duration": 6.361
    },
    {
      "lang": "en",
      "text": "going to encode this string and get a",
      "offset": 2151.079,
      "duration": 6.361
    },
    {
      "lang": "en",
      "text": "list of integers which are the tokens uh",
      "offset": 2154.44,
      "duration": 4.84
    },
    {
      "lang": "en",
      "text": "now these integers here should actually",
      "offset": 2157.44,
      "duration": 3.639
    },
    {
      "lang": "en",
      "text": "be fairly straightforward because we can",
      "offset": 2159.28,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "just copy paste this string and we can",
      "offset": 2161.079,
      "duration": 4.681
    },
    {
      "lang": "en",
      "text": "sort of inspect what it is in tick",
      "offset": 2164.079,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "tokenizer so just pasting that in these",
      "offset": 2165.76,
      "duration": 3.8
    },
    {
      "lang": "en",
      "text": "are the tokens that are going to come",
      "offset": 2168.319,
      "duration": 4.04
    },
    {
      "lang": "en",
      "text": "out so this list of integers is what we",
      "offset": 2169.56,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "expect tokens to become and as you",
      "offset": 2172.359,
      "duration": 5.321
    },
    {
      "lang": "en",
      "text": "recall if you saw my video of course all",
      "offset": 2175.8,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "the tokens they're just little string",
      "offset": 2177.68,
      "duration": 3.96
    },
    {
      "lang": "en",
      "text": "chunks right so these are this is the",
      "offset": 2179,
      "duration": 6.319
    },
    {
      "lang": "en",
      "text": "chunc of this string into gpt2",
      "offset": 2181.64,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "tokens so once we have those tokens it's",
      "offset": 2185.319,
      "duration": 4.841
    },
    {
      "lang": "en",
      "text": "a list of integers we can create a torch",
      "offset": 2187.72,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "tensor out of it in this case it's eight",
      "offset": 2190.16,
      "duration": 4.199
    },
    {
      "lang": "en",
      "text": "tokens and then we're going to replicate",
      "offset": 2192.28,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "these eight tokens for five times to get",
      "offset": 2194.359,
      "duration": 5.921
    },
    {
      "lang": "en",
      "text": "five rows of eight tokens and that is",
      "offset": 2196.839,
      "duration": 8.201
    },
    {
      "lang": "en",
      "text": "our initial um input X as I call it here",
      "offset": 2200.28,
      "duration": 7.839
    },
    {
      "lang": "en",
      "text": "and it lives on the GPU as well so X now",
      "offset": 2205.04,
      "duration": 7.2
    },
    {
      "lang": "en",
      "text": "is this idx that we can put into forward",
      "offset": 2208.119,
      "duration": 7.161
    },
    {
      "lang": "en",
      "text": "to get our logits so that we know what",
      "offset": 2212.24,
      "duration": 6.04
    },
    {
      "lang": "en",
      "text": "comes as the sixth token",
      "offset": 2215.28,
      "duration": 6.039
    },
    {
      "lang": "en",
      "text": "uh sorry as the ninth token in every one",
      "offset": 2218.28,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "of these five rows okay and we are now",
      "offset": 2221.319,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "ready to generate so let me paste in one",
      "offset": 2224.04,
      "duration": 3.4
    },
    {
      "lang": "en",
      "text": "more code block",
      "offset": 2225.8,
      "duration": 4.039
    },
    {
      "lang": "en",
      "text": "here um so what's happening here in this",
      "offset": 2227.44,
      "duration": 5.48
    },
    {
      "lang": "en",
      "text": "code block is we have this x which is of",
      "offset": 2229.839,
      "duration": 6.841
    },
    {
      "lang": "en",
      "text": "size B BYT right so batch by time and",
      "offset": 2232.92,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "we're going to be in every iteration of",
      "offset": 2236.68,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "this loop we're going to be adding a",
      "offset": 2238.28,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "column of new indices into each one of",
      "offset": 2239.88,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "these rows right and so these are the",
      "offset": 2242.68,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "new indices and we're appending them to",
      "offset": 2244.88,
      "duration": 4.36
    },
    {
      "lang": "en",
      "text": "the the sequence as we're sampling so",
      "offset": 2247.4,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "with each Loop iteration we get one more",
      "offset": 2249.24,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "column into X and all of the operations",
      "offset": 2251.48,
      "duration": 4.92
    },
    {
      "lang": "en",
      "text": "happen in the context manager of torch.",
      "offset": 2254.76,
      "duration": 3.48
    },
    {
      "lang": "en",
      "text": "nograd this is just telling pytorch that",
      "offset": 2256.4,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "we're not going to be calling that",
      "offset": 2258.24,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "backward on any of this so it doesn't",
      "offset": 2259.4,
      "duration": 3.719
    },
    {
      "lang": "en",
      "text": "have to cach all the intermediate",
      "offset": 2261.599,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "tensors it's not going to have to",
      "offset": 2263.119,
      "duration": 3.081
    },
    {
      "lang": "en",
      "text": "prepare in any way for a potential",
      "offset": 2264.599,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "backward later and this saves a lot of",
      "offset": 2266.2,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "space and also possibly uh some time so",
      "offset": 2268.76,
      "duration": 5.8
    },
    {
      "lang": "en",
      "text": "we get our low jits we get the loow jits",
      "offset": 2272.28,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "at only the last location we throw away",
      "offset": 2274.56,
      "duration": 4.519
    },
    {
      "lang": "en",
      "text": "all the other low jits uh we don't need",
      "offset": 2277.16,
      "duration": 4.36
    },
    {
      "lang": "en",
      "text": "them we only care about the last columns",
      "offset": 2279.079,
      "duration": 5.721
    },
    {
      "lang": "en",
      "text": "low jits so this is being wasteful uh",
      "offset": 2281.52,
      "duration": 4.92
    },
    {
      "lang": "en",
      "text": "but uh this is just kind of like an",
      "offset": 2284.8,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "inefficient implementation of",
      "offset": 2286.44,
      "duration": 4.12
    },
    {
      "lang": "en",
      "text": "sampling um so it's correct but",
      "offset": 2288.24,
      "duration": 4.92
    },
    {
      "lang": "en",
      "text": "inefficient so we get the last column of",
      "offset": 2290.56,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "loow jits pass it through soft Max to",
      "offset": 2293.16,
      "duration": 3.64
    },
    {
      "lang": "en",
      "text": "get our probabilities then here I'm",
      "offset": 2294.96,
      "duration": 3.96
    },
    {
      "lang": "en",
      "text": "doing top case sampling of 50 and I'm",
      "offset": 2296.8,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "doing that because this is the hugging",
      "offset": 2298.92,
      "duration": 4.12
    },
    {
      "lang": "en",
      "text": "face default so just looking at the",
      "offset": 2300.24,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "hugging face docks here of a pipeline um",
      "offset": 2303.04,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "there's a bunch of",
      "offset": 2306.72,
      "duration": 5.879
    },
    {
      "lang": "en",
      "text": "quarks that go into hugging face and I",
      "offset": 2308.56,
      "duration": 6.039
    },
    {
      "lang": "en",
      "text": "mean it's it's kind of a lot honestly",
      "offset": 2312.599,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "but I guess the important one that I",
      "offset": 2314.599,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "noticed is that they're using top K by",
      "offset": 2316.44,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "default which is 50 and what that does",
      "offset": 2318.2,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "is that uh so that's being used here as",
      "offset": 2321.079,
      "duration": 4.76
    },
    {
      "lang": "en",
      "text": "well and what that does is basically we",
      "offset": 2323.88,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "want to take our probabilities and we",
      "offset": 2325.839,
      "duration": 3.561
    },
    {
      "lang": "en",
      "text": "only want to keep the top 50",
      "offset": 2327.48,
      "duration": 4.28
    },
    {
      "lang": "en",
      "text": "probabilities and anything that is lower",
      "offset": 2329.4,
      "duration": 4.76
    },
    {
      "lang": "en",
      "text": "than the 50th probability uh we just",
      "offset": 2331.76,
      "duration": 5.16
    },
    {
      "lang": "en",
      "text": "clamp to zero and renormalize and so",
      "offset": 2334.16,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "that way we are never sampling very rare",
      "offset": 2336.92,
      "duration": 4.439
    },
    {
      "lang": "en",
      "text": "tokens uh the tokens we're going to be",
      "offset": 2339.52,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "sampling are always in the top 50 of",
      "offset": 2341.359,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "most likely tokens and this helps keep",
      "offset": 2343.44,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "the model kind of on track and it",
      "offset": 2345.52,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "doesn't blabber on and it doesn't get",
      "offset": 2347.28,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "lost and doesn't go off the rails as",
      "offset": 2348.64,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "easily uh and it kind of like um sticks",
      "offset": 2350.4,
      "duration": 5.32
    },
    {
      "lang": "en",
      "text": "in the vicinity of likely tokens a lot",
      "offset": 2353.119,
      "duration": 4.441
    },
    {
      "lang": "en",
      "text": "better so this is the way to do it in",
      "offset": 2355.72,
      "duration": 3.24
    },
    {
      "lang": "en",
      "text": "pytorch and you can step through it if",
      "offset": 2357.56,
      "duration": 2.559
    },
    {
      "lang": "en",
      "text": "you like I don't think it's super",
      "offset": 2358.96,
      "duration": 3.32
    },
    {
      "lang": "en",
      "text": "insightful so I'll speed through it but",
      "offset": 2360.119,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "roughly speaking we get this new column",
      "offset": 2362.28,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "of of tokens we append them on x and",
      "offset": 2364.28,
      "duration": 6.44
    },
    {
      "lang": "en",
      "text": "basically The Columns of X grow until",
      "offset": 2367.8,
      "duration": 5.24
    },
    {
      "lang": "en",
      "text": "this y Loop gets tripped up and then",
      "offset": 2370.72,
      "duration": 8.08
    },
    {
      "lang": "en",
      "text": "finally we have an entire X of size um 5",
      "offset": 2373.04,
      "duration": 8.64
    },
    {
      "lang": "en",
      "text": "by 30 in this case in this example and",
      "offset": 2378.8,
      "duration": 5.08
    },
    {
      "lang": "en",
      "text": "we can just basically print all those",
      "offset": 2381.68,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "individual rows so I'm getting all the",
      "offset": 2383.88,
      "duration": 4.439
    },
    {
      "lang": "en",
      "text": "rows I'm getting all the tokens that",
      "offset": 2386.16,
      "duration": 4.28
    },
    {
      "lang": "en",
      "text": "were sampled and I'm using the decode",
      "offset": 2388.319,
      "duration": 4.121
    },
    {
      "lang": "en",
      "text": "function from Tik tokenizer to get back",
      "offset": 2390.44,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "the string which we can print and so",
      "offset": 2392.44,
      "duration": 6.879
    },
    {
      "lang": "en",
      "text": "terminal new terminal",
      "offset": 2395.96,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "and let me python train",
      "offset": 2399.44,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "gpt2 okay so these are the generations",
      "offset": 2408.96,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "that we're getting hello I'm a language",
      "offset": 2411.52,
      "duration": 3.48
    },
    {
      "lang": "en",
      "text": "model not a",
      "offset": 2413.52,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "program um new line new line Etc hello",
      "offset": 2415,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "I'm a language model and one of the main",
      "offset": 2419.2,
      "duration": 3.159
    },
    {
      "lang": "en",
      "text": "things that bothers me when they create",
      "offset": 2421,
      "duration": 2.839
    },
    {
      "lang": "en",
      "text": "languages is how easy it becomes to",
      "offset": 2422.359,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "create something that I me so this will",
      "offset": 2423.839,
      "duration": 3.881
    },
    {
      "lang": "en",
      "text": "just like blabber on right in all these",
      "offset": 2426.2,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "cases now one thing you will notice is",
      "offset": 2427.72,
      "duration": 3.879
    },
    {
      "lang": "en",
      "text": "that these Generations are not the",
      "offset": 2429.8,
      "duration": 5.64
    },
    {
      "lang": "en",
      "text": "generations of hugging face here and I",
      "offset": 2431.599,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "can't find the discrepancy to be honest",
      "offset": 2435.44,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "and I didn't fully go through all these",
      "offset": 2437.599,
      "duration": 2.961
    },
    {
      "lang": "en",
      "text": "options but probably there's something",
      "offset": 2439.359,
      "duration": 3.96
    },
    {
      "lang": "en",
      "text": "else hiding in on addition to the top P",
      "offset": 2440.56,
      "duration": 4.44
    },
    {
      "lang": "en",
      "text": "so I'm not able to match it up but just",
      "offset": 2443.319,
      "duration": 4.601
    },
    {
      "lang": "en",
      "text": "for correctness um down here Below in",
      "offset": 2445,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "the juper notebook and using the hugging",
      "offset": 2447.92,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "face model so this is the hugging face",
      "offset": 2449.96,
      "duration": 6.359
    },
    {
      "lang": "en",
      "text": "model here I was I replicated the code",
      "offset": 2452.24,
      "duration": 7.4
    },
    {
      "lang": "en",
      "text": "and if I do this and I run that then I",
      "offset": 2456.319,
      "duration": 6.881
    },
    {
      "lang": "en",
      "text": "am getting the same results so basically",
      "offset": 2459.64,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "the model internals are not wrong it's",
      "offset": 2463.2,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "just I'm not 100% sure what the pipeline",
      "offset": 2465.4,
      "duration": 4.199
    },
    {
      "lang": "en",
      "text": "does in hugging face and that's why",
      "offset": 2468.2,
      "duration": 3.48
    },
    {
      "lang": "en",
      "text": "we're not able to match them up but",
      "offset": 2469.599,
      "duration": 3.72
    },
    {
      "lang": "en",
      "text": "otherwise the code is correct and we've",
      "offset": 2471.68,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "loaded all the um tensors correctly so",
      "offset": 2473.319,
      "duration": 4.681
    },
    {
      "lang": "en",
      "text": "we're initializing the model correctly",
      "offset": 2476.72,
      "duration": 3.399
    },
    {
      "lang": "en",
      "text": "and everything here works so long story",
      "offset": 2478,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "short uh We've Port it all the weights",
      "offset": 2480.119,
      "duration": 5.161
    },
    {
      "lang": "en",
      "text": "we initialize the gpt2 this is the exact",
      "offset": 2482.48,
      "duration": 5.08
    },
    {
      "lang": "en",
      "text": "opening gpt2 and it can generate",
      "offset": 2485.28,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "sequences and they look sensible and now",
      "offset": 2487.56,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "here of course we're initializing with",
      "offset": 2490.96,
      "duration": 3.96
    },
    {
      "lang": "en",
      "text": "gbt2 model weights but now we want to",
      "offset": 2492.28,
      "duration": 4.28
    },
    {
      "lang": "en",
      "text": "initialize from scratch from random",
      "offset": 2494.92,
      "duration": 3.56
    },
    {
      "lang": "en",
      "text": "numbers and we want to actually train a",
      "offset": 2496.56,
      "duration": 4.279
    },
    {
      "lang": "en",
      "text": "model that will give us sequences as",
      "offset": 2498.48,
      "duration": 5.839
    },
    {
      "lang": "en",
      "text": "good as or better than these ones in",
      "offset": 2500.839,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "quality and so that's what we turn to",
      "offset": 2504.319,
      "duration": 4.04
    },
    {
      "lang": "en",
      "text": "next so it turns out that using the",
      "offset": 2506.599,
      "duration": 3.081
    },
    {
      "lang": "en",
      "text": "random model is actually fairly",
      "offset": 2508.359,
      "duration": 3.321
    },
    {
      "lang": "en",
      "text": "straightforward because pytorch already",
      "offset": 2509.68,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "initializes our model randomly and by",
      "offset": 2511.68,
      "duration": 6.56
    },
    {
      "lang": "en",
      "text": "default so when we create the GPT model",
      "offset": 2513.68,
      "duration": 7.28
    },
    {
      "lang": "en",
      "text": "and the Constructor this is all um all",
      "offset": 2518.24,
      "duration": 5.359
    },
    {
      "lang": "en",
      "text": "of these layers and modules have random",
      "offset": 2520.96,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "initializers that are there by default",
      "offset": 2523.599,
      "duration": 4.361
    },
    {
      "lang": "en",
      "text": "so when these linear layers get created",
      "offset": 2525.92,
      "duration": 4.28
    },
    {
      "lang": "en",
      "text": "and so on there's default Constructors",
      "offset": 2527.96,
      "duration": 3.48
    },
    {
      "lang": "en",
      "text": "for example using the Javier",
      "offset": 2530.2,
      "duration": 3.32
    },
    {
      "lang": "en",
      "text": "initialization that we saw in the past",
      "offset": 2531.44,
      "duration": 4.12
    },
    {
      "lang": "en",
      "text": "uh to construct the weights of these",
      "offset": 2533.52,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "layers and so creating a random model",
      "offset": 2535.56,
      "duration": 5.4
    },
    {
      "lang": "en",
      "text": "instead of a gpt2 model is actually",
      "offset": 2538.319,
      "duration": 4.28
    },
    {
      "lang": "en",
      "text": "fairly straightforward and we would just",
      "offset": 2540.96,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "come here and instead we would create",
      "offset": 2542.599,
      "duration": 5.401
    },
    {
      "lang": "en",
      "text": "model equals GPT and then we want to use",
      "offset": 2544.8,
      "duration": 6.279
    },
    {
      "lang": "en",
      "text": "the default config GPT config and the",
      "offset": 2548,
      "duration": 5.72
    },
    {
      "lang": "en",
      "text": "default config uses the 124 M parameters",
      "offset": 2551.079,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "so this is the random model",
      "offset": 2553.72,
      "duration": 5.96
    },
    {
      "lang": "en",
      "text": "initialization and we can run",
      "offset": 2555.96,
      "duration": 3.72
    },
    {
      "lang": "en",
      "text": "it and we should be able to get uh",
      "offset": 2562.24,
      "duration": 5.8
    },
    {
      "lang": "en",
      "text": "results now the results here of course",
      "offset": 2566.319,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "are total garbage carbal and that's",
      "offset": 2568.04,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "because this is random model and so",
      "offset": 2570.16,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "we're just getting all these random",
      "offset": 2571.8,
      "duration": 3.64
    },
    {
      "lang": "en",
      "text": "token string pieces chunked up totally",
      "offset": 2573.04,
      "duration": 4.68
    },
    {
      "lang": "en",
      "text": "at random so that's what we have right",
      "offset": 2575.44,
      "duration": 4.28
    },
    {
      "lang": "en",
      "text": "now uh now one more thing I wanted to",
      "offset": 2577.72,
      "duration": 3.72
    },
    {
      "lang": "en",
      "text": "point out by the way is in case you do",
      "offset": 2579.72,
      "duration": 3.399
    },
    {
      "lang": "en",
      "text": "not have Cuda available because you",
      "offset": 2581.44,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "don't have a GPU you can still follow",
      "offset": 2583.119,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "along with uh with what we're doing here",
      "offset": 2584.88,
      "duration": 5.4
    },
    {
      "lang": "en",
      "text": "uh to some extent uh and probably not to",
      "offset": 2587.68,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "the very end because by the end we're",
      "offset": 2590.28,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "going to be using multiple gpus and",
      "offset": 2591.92,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "actually doing a serious training run uh",
      "offset": 2593.48,
      "duration": 3.16
    },
    {
      "lang": "en",
      "text": "but for now you can actually follow",
      "offset": 2595.52,
      "duration": 3.64
    },
    {
      "lang": "en",
      "text": "along decently okay uh so one thing that",
      "offset": 2596.64,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "I like to do in pytorch is I like to",
      "offset": 2599.16,
      "duration": 3.48
    },
    {
      "lang": "en",
      "text": "autod detect the device that is",
      "offset": 2600.96,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "available to you so in particular you",
      "offset": 2602.64,
      "duration": 5.64
    },
    {
      "lang": "en",
      "text": "could do that like this",
      "offset": 2604.72,
      "duration": 5.599
    },
    {
      "lang": "en",
      "text": "so here we are trying to detect a device",
      "offset": 2608.28,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "to run on that has the highest compute",
      "offset": 2610.319,
      "duration": 3.481
    },
    {
      "lang": "en",
      "text": "capability you can think about it that",
      "offset": 2612.2,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "way so by default we start with CPU",
      "offset": 2613.8,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "which of course is available everywhere",
      "offset": 2616.44,
      "duration": 2.919
    },
    {
      "lang": "en",
      "text": "because every single computer will have",
      "offset": 2617.8,
      "duration": 4.2
    },
    {
      "lang": "en",
      "text": "a CPU but then we can try to detect do",
      "offset": 2619.359,
      "duration": 5.321
    },
    {
      "lang": "en",
      "text": "you have a GPU you so use a Cuda and",
      "offset": 2622,
      "duration": 5.16
    },
    {
      "lang": "en",
      "text": "then if you don't have a Cuda uh do you",
      "offset": 2624.68,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "at least have MPS MPS is the back end",
      "offset": 2627.16,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "for Apple silicon so if you have a",
      "offset": 2629.48,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "Macbook that is fairly new you probably",
      "offset": 2631.72,
      "duration": 3.8
    },
    {
      "lang": "en",
      "text": "have apple silicon on the inside and",
      "offset": 2633.48,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "then that has a GPU that is actually",
      "offset": 2635.52,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "fairly capable uh depending on which",
      "offset": 2637.16,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "MacBook you have and so you can use MPS",
      "offset": 2639.2,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "which will be potentially faster than",
      "offset": 2641.079,
      "duration": 4.52
    },
    {
      "lang": "en",
      "text": "CPU and so we can print the device here",
      "offset": 2642.8,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "now once we have the device we can",
      "offset": 2645.599,
      "duration": 5.441
    },
    {
      "lang": "en",
      "text": "actually use it in place of Puda so we",
      "offset": 2647.119,
      "duration": 7.081
    },
    {
      "lang": "en",
      "text": "just swap it in and notice that here",
      "offset": 2651.04,
      "duration": 6.559
    },
    {
      "lang": "en",
      "text": "when we call model on X if this x here",
      "offset": 2654.2,
      "duration": 7
    },
    {
      "lang": "en",
      "text": "is on CPU instead of GPU then it will",
      "offset": 2657.599,
      "duration": 5.801
    },
    {
      "lang": "en",
      "text": "work fine because here in the forward",
      "offset": 2661.2,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "which is where P to will come when we",
      "offset": 2663.4,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "create a pose we were careful to use the",
      "offset": 2666.16,
      "duration": 5.48
    },
    {
      "lang": "en",
      "text": "device of idx to create this tensor as",
      "offset": 2668.44,
      "duration": 5.24
    },
    {
      "lang": "en",
      "text": "well and so there won't be any mismatch",
      "offset": 2671.64,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "where one tensor is on CPU one is on GPU",
      "offset": 2673.68,
      "duration": 4.84
    },
    {
      "lang": "en",
      "text": "and uh that you can't combine those but",
      "offset": 2676.119,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "here we are um carefully initializing on",
      "offset": 2678.52,
      "duration": 5.24
    },
    {
      "lang": "en",
      "text": "the correct device as indicated by the",
      "offset": 2681,
      "duration": 6.04
    },
    {
      "lang": "en",
      "text": "input to this model so this will autod",
      "offset": 2683.76,
      "duration": 5.4
    },
    {
      "lang": "en",
      "text": "detect device for me this will be of",
      "offset": 2687.04,
      "duration": 3.279
    },
    {
      "lang": "en",
      "text": "course",
      "offset": 2689.16,
      "duration": 5.439
    },
    {
      "lang": "en",
      "text": "GPU so using device",
      "offset": 2690.319,
      "duration": 8.04
    },
    {
      "lang": "en",
      "text": "Cuda uh but uh you can also run with um",
      "offset": 2694.599,
      "duration": 5.401
    },
    {
      "lang": "en",
      "text": "as I mentioned another device and it's",
      "offset": 2698.359,
      "duration": 3.041
    },
    {
      "lang": "en",
      "text": "not going to be too much slower so if I",
      "offset": 2700,
      "duration": 3.64
    },
    {
      "lang": "en",
      "text": "override device here",
      "offset": 2701.4,
      "duration": 5.679
    },
    {
      "lang": "en",
      "text": "oops if I override device equals",
      "offset": 2703.64,
      "duration": 5.24
    },
    {
      "lang": "en",
      "text": "CPU",
      "offset": 2707.079,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "then we'll still print Cuda of course",
      "offset": 2708.88,
      "duration": 7.4
    },
    {
      "lang": "en",
      "text": "but now we're actually using CPU one 2 3",
      "offset": 2711.319,
      "duration": 10.081
    },
    {
      "lang": "en",
      "text": "4 5 6 okay about 6 seconds and actually",
      "offset": 2716.28,
      "duration": 6.68
    },
    {
      "lang": "en",
      "text": "we're not using torch compile and stuff",
      "offset": 2721.4,
      "duration": 2.919
    },
    {
      "lang": "en",
      "text": "like that which will speed up everything",
      "offset": 2722.96,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "a lot faster as well but you can follow",
      "offset": 2724.319,
      "duration": 6.04
    },
    {
      "lang": "en",
      "text": "even on a CPU I think to a decent extent",
      "offset": 2727.2,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "um so that's note on that okay so I do",
      "offset": 2730.359,
      "duration": 4.76
    },
    {
      "lang": "en",
      "text": "want to loop around eventually into what",
      "offset": 2732.64,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "it means to have different devices in",
      "offset": 2735.119,
      "duration": 3.281
    },
    {
      "lang": "en",
      "text": "pytorch and what it is exactly that",
      "offset": 2736.559,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "pytorch does in the background for you",
      "offset": 2738.4,
      "duration": 4.76
    },
    {
      "lang": "en",
      "text": "when you do something like module. 2",
      "offset": 2740.559,
      "duration": 4.841
    },
    {
      "lang": "en",
      "text": "device or where you take a torch tensor",
      "offset": 2743.16,
      "duration": 4.919
    },
    {
      "lang": "en",
      "text": "and do A2 device and what exactly",
      "offset": 2745.4,
      "duration": 4.52
    },
    {
      "lang": "en",
      "text": "happens and how that works but for now",
      "offset": 2748.079,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "I'd like to get to training and I'd like",
      "offset": 2749.92,
      "duration": 3.96
    },
    {
      "lang": "en",
      "text": "to start training the model and for now",
      "offset": 2751.599,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "let's just say the device makes code go",
      "offset": 2753.88,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "fast um and let's go into how we can",
      "offset": 2755.839,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "actually train the model so to train the",
      "offset": 2758.599,
      "duration": 3.801
    },
    {
      "lang": "en",
      "text": "model we're going to need some data set",
      "offset": 2760.72,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "and for me the best debugging simplest",
      "offset": 2762.4,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "data set that I like to use is the tiny",
      "offset": 2764.48,
      "duration": 4.52
    },
    {
      "lang": "en",
      "text": "Shakespeare data set um and it's",
      "offset": 2766.319,
      "duration": 4.841
    },
    {
      "lang": "en",
      "text": "available at this URL so you can W get",
      "offset": 2769,
      "duration": 3.319
    },
    {
      "lang": "en",
      "text": "it or you can just search tiny",
      "offset": 2771.16,
      "duration": 2.32
    },
    {
      "lang": "en",
      "text": "Shakespeare data",
      "offset": 2772.319,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "set and so um I have in my file system",
      "offset": 2773.48,
      "duration": 5.32
    },
    {
      "lang": "en",
      "text": "as just LS input.txt",
      "offset": 2776.559,
      "duration": 5.481
    },
    {
      "lang": "en",
      "text": "so I already downloaded it and here I'm",
      "offset": 2778.8,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "reading the data set getting the first",
      "offset": 2782.04,
      "duration": 4.279
    },
    {
      "lang": "en",
      "text": "1,000 characters and printing the first",
      "offset": 2783.76,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "100",
      "offset": 2786.319,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "now remember that gpt2 has uh roughly a",
      "offset": 2787.44,
      "duration": 5.08
    },
    {
      "lang": "en",
      "text": "compression ratio the tokenizer has a",
      "offset": 2790.24,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "compression ratio of rly 3 to1 so th000",
      "offset": 2792.52,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "characters is roughly 300 tokens here uh",
      "offset": 2795.119,
      "duration": 4.681
    },
    {
      "lang": "en",
      "text": "that will come out of this in the slice",
      "offset": 2797.88,
      "duration": 4.12
    },
    {
      "lang": "en",
      "text": "that we're currently getting so this is",
      "offset": 2799.8,
      "duration": 4.759
    },
    {
      "lang": "en",
      "text": "the first few uh",
      "offset": 2802,
      "duration": 4.839
    },
    {
      "lang": "en",
      "text": "characters and uh if you want to get a",
      "offset": 2804.559,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "few more statistics on this we can do",
      "offset": 2806.839,
      "duration": 3.561
    },
    {
      "lang": "en",
      "text": "work count on input.txt",
      "offset": 2808.4,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "so we can see that this is uh 40,000",
      "offset": 2810.4,
      "duration": 5.919
    },
    {
      "lang": "en",
      "text": "lines about 200,000 words in this data",
      "offset": 2813.92,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "set and about 1 million bytes in this",
      "offset": 2816.319,
      "duration": 5.561
    },
    {
      "lang": "en",
      "text": "file and knowing that this file is only",
      "offset": 2819.52,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "asky characters there's no crazy unic",
      "offset": 2821.88,
      "duration": 4.04
    },
    {
      "lang": "en",
      "text": "code here as far as I know and so every",
      "offset": 2823.599,
      "duration": 4.601
    },
    {
      "lang": "en",
      "text": "asky character is encoded with one bite",
      "offset": 2825.92,
      "duration": 4.12
    },
    {
      "lang": "en",
      "text": "and so this is uh the same number",
      "offset": 2828.2,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "roughly a million characters inside this",
      "offset": 2830.04,
      "duration": 5.559
    },
    {
      "lang": "en",
      "text": "data set so that's the data set size uh",
      "offset": 2832.68,
      "duration": 4.679
    },
    {
      "lang": "en",
      "text": "by default very small and minimal data",
      "offset": 2835.599,
      "duration": 4.041
    },
    {
      "lang": "en",
      "text": "set for debugging to get us off the",
      "offset": 2837.359,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "ground in order to tokenize this data",
      "offset": 2839.64,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "set we're going to get Tik token",
      "offset": 2841.52,
      "duration": 6.12
    },
    {
      "lang": "en",
      "text": "encoding for gbt2 encode the data uh the",
      "offset": 2843.96,
      "duration": 6.96
    },
    {
      "lang": "en",
      "text": "first um 1,000 characters and then I'm",
      "offset": 2847.64,
      "duration": 6.04
    },
    {
      "lang": "en",
      "text": "only going to print the first 24 tokens",
      "offset": 2850.92,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "so these are the tokens as a list of",
      "offset": 2853.68,
      "duration": 5.24
    },
    {
      "lang": "en",
      "text": "integers and if you can read gpt2 tokens",
      "offset": 2856.119,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "you will see that 198 here you'll",
      "offset": 2858.92,
      "duration": 4.04
    },
    {
      "lang": "en",
      "text": "recognize that as the slashing character",
      "offset": 2860.92,
      "duration": 4.12
    },
    {
      "lang": "en",
      "text": "so that is a new line and then here for",
      "offset": 2862.96,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "example we have two new lines so that's",
      "offset": 2865.04,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "198 twice here uh so this is just a",
      "offset": 2866.64,
      "duration": 5.919
    },
    {
      "lang": "en",
      "text": "tokenization of the first 24 tokens so",
      "offset": 2869.52,
      "duration": 4.599
    },
    {
      "lang": "en",
      "text": "what we want to do now is we want to",
      "offset": 2872.559,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "actually process these token sequences",
      "offset": 2874.119,
      "duration": 5.321
    },
    {
      "lang": "en",
      "text": "and feed them into a Transformer and in",
      "offset": 2876.88,
      "duration": 4.92
    },
    {
      "lang": "en",
      "text": "particular we want them we want to",
      "offset": 2879.44,
      "duration": 5.679
    },
    {
      "lang": "en",
      "text": "rearrange these tokens into this idx",
      "offset": 2881.8,
      "duration": 4.519
    },
    {
      "lang": "en",
      "text": "variable that we're going to be feeding",
      "offset": 2885.119,
      "duration": 3.161
    },
    {
      "lang": "en",
      "text": "into the Transformer so we don't want a",
      "offset": 2886.319,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "single very long onedimensional sequence",
      "offset": 2888.28,
      "duration": 4.68
    },
    {
      "lang": "en",
      "text": "we want an entire batch where each",
      "offset": 2890.48,
      "duration": 5.8
    },
    {
      "lang": "en",
      "text": "sequence is up to uh is basically T",
      "offset": 2892.96,
      "duration": 5.359
    },
    {
      "lang": "en",
      "text": "tokens and T cannot be larger than the",
      "offset": 2896.28,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "maximum sequence length and then we have",
      "offset": 2898.319,
      "duration": 6.681
    },
    {
      "lang": "en",
      "text": "these t uh tlong uh sequences of tokens",
      "offset": 2901.16,
      "duration": 6.64
    },
    {
      "lang": "en",
      "text": "and we have B independent examples of",
      "offset": 2905,
      "duration": 5.359
    },
    {
      "lang": "en",
      "text": "sequences so how can we create a b BYT",
      "offset": 2907.8,
      "duration": 4.6
    },
    {
      "lang": "en",
      "text": "tensor that we can feed into the forward",
      "offset": 2910.359,
      "duration": 3.72
    },
    {
      "lang": "en",
      "text": "out of these onedimensional",
      "offset": 2912.4,
      "duration": 4.199
    },
    {
      "lang": "en",
      "text": "sequences so here's my favorite way to",
      "offset": 2914.079,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "to achieve this uh so if we take torch",
      "offset": 2916.599,
      "duration": 4.841
    },
    {
      "lang": "en",
      "text": "and then we create a tensor object out",
      "offset": 2919.599,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "of this list of integers and just the",
      "offset": 2921.44,
      "duration": 4.28
    },
    {
      "lang": "en",
      "text": "first 24 tokens my favorite way to do",
      "offset": 2922.96,
      "duration": 6.56
    },
    {
      "lang": "en",
      "text": "this is basically you do a do view of um",
      "offset": 2925.72,
      "duration": 7.16
    },
    {
      "lang": "en",
      "text": "of uh for example 4x6 which multiply to",
      "offset": 2929.52,
      "duration": 5.319
    },
    {
      "lang": "en",
      "text": "24 and so it's just a two-dimensional",
      "offset": 2932.88,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "rearrangement of these tokens and you'll",
      "offset": 2934.839,
      "duration": 2.921
    },
    {
      "lang": "en",
      "text": "is that when you view this",
      "offset": 2936.88,
      "duration": 1.919
    },
    {
      "lang": "en",
      "text": "onedimensional sequence as",
      "offset": 2937.76,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "two-dimensional 4x6 here the first six",
      "offset": 2938.799,
      "duration": 7.601
    },
    {
      "lang": "en",
      "text": "uh tokens uh up to here end up being the",
      "offset": 2943.04,
      "duration": 6.92
    },
    {
      "lang": "en",
      "text": "first row the next six tokens here end",
      "offset": 2946.4,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "up being the second row and so on and so",
      "offset": 2949.96,
      "duration": 4.76
    },
    {
      "lang": "en",
      "text": "basically it's just going to stack up",
      "offset": 2952.4,
      "duration": 6.199
    },
    {
      "lang": "en",
      "text": "this the um every six tokens in this",
      "offset": 2954.72,
      "duration": 6.04
    },
    {
      "lang": "en",
      "text": "case as independent rows and it creates",
      "offset": 2958.599,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "a batch of tokens in this case and so",
      "offset": 2960.76,
      "duration": 5.96
    },
    {
      "lang": "en",
      "text": "for example if we are token 25 in the",
      "offset": 2963.48,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "Transformer when we feed this in and",
      "offset": 2966.72,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "this becomes the idx this token is going",
      "offset": 2968.28,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "to see these three tokens and it's going",
      "offset": 2970.72,
      "duration": 5.24
    },
    {
      "lang": "en",
      "text": "to try to predict that 198 comes",
      "offset": 2973.24,
      "duration": 6.04
    },
    {
      "lang": "en",
      "text": "next so in this way we are able to",
      "offset": 2975.96,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "create this two-dimensional batch that's",
      "offset": 2979.28,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "that's quite nice now in terms of the",
      "offset": 2981.16,
      "duration": 4.52
    },
    {
      "lang": "en",
      "text": "label that we're going to need for the",
      "offset": 2984.16,
      "duration": 3.159
    },
    {
      "lang": "en",
      "text": "Target to calculate the loss function",
      "offset": 2985.68,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "how do we get that well we could write",
      "offset": 2987.319,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "some code inside the forward pass",
      "offset": 2989.52,
      "duration": 3.559
    },
    {
      "lang": "en",
      "text": "because we know that the next uh token",
      "offset": 2991.079,
      "duration": 4.121
    },
    {
      "lang": "en",
      "text": "in a sequence which is the label is just",
      "offset": 2993.079,
      "duration": 4.28
    },
    {
      "lang": "en",
      "text": "to the right of us but you'll notice",
      "offset": 2995.2,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "that actually we for this token at the",
      "offset": 2997.359,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "very end 13 we don't actually have the",
      "offset": 2999.92,
      "duration": 4.04
    },
    {
      "lang": "en",
      "text": "next correct token because we didn't",
      "offset": 3002.24,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "load it so uh we actually didn't get",
      "offset": 3003.96,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "enough information here so I'll show you",
      "offset": 3007.2,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "my favorite way of basically getting",
      "offset": 3009.96,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "these batches and I like to personally",
      "offset": 3011.92,
      "duration": 4.04
    },
    {
      "lang": "en",
      "text": "have not just the input to the",
      "offset": 3014.76,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "Transformer which I like to call X but I",
      "offset": 3015.96,
      "duration": 5.96
    },
    {
      "lang": "en",
      "text": "also like to create the labels uh tensor",
      "offset": 3018.44,
      "duration": 6.2
    },
    {
      "lang": "en",
      "text": "which is of the exact same size as X but",
      "offset": 3021.92,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "contains the targets at every single",
      "offset": 3024.64,
      "duration": 2.6
    },
    {
      "lang": "en",
      "text": "position",
      "offset": 3026.16,
      "duration": 2.439
    },
    {
      "lang": "en",
      "text": "and so here's the way that I like to do",
      "offset": 3027.24,
      "duration": 3.24
    },
    {
      "lang": "en",
      "text": "that I like to make sure that I fetch",
      "offset": 3028.599,
      "duration": 4.361
    },
    {
      "lang": "en",
      "text": "plus one uh token because we need the",
      "offset": 3030.48,
      "duration": 5.119
    },
    {
      "lang": "en",
      "text": "ground Truth for the very last token uh",
      "offset": 3032.96,
      "duration": 3.48
    },
    {
      "lang": "en",
      "text": "for",
      "offset": 3035.599,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "13 and then when we're creating the",
      "offset": 3036.44,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "input we take everything up to the last",
      "offset": 3039.119,
      "duration": 5.521
    },
    {
      "lang": "en",
      "text": "token not including and view it as 4x6",
      "offset": 3041.48,
      "duration": 5.639
    },
    {
      "lang": "en",
      "text": "and when we're creating targets we do",
      "offset": 3044.64,
      "duration": 5.719
    },
    {
      "lang": "en",
      "text": "the buffer but starting at index one not",
      "offset": 3047.119,
      "duration": 5.041
    },
    {
      "lang": "en",
      "text": "index zero so we're skipping the first",
      "offset": 3050.359,
      "duration": 3.881
    },
    {
      "lang": "en",
      "text": "element and we view it in the exact same",
      "offset": 3052.16,
      "duration": 5.959
    },
    {
      "lang": "en",
      "text": "size and then when I print this",
      "offset": 3054.24,
      "duration": 5.96
    },
    {
      "lang": "en",
      "text": "here's what happens where we see that",
      "offset": 3058.119,
      "duration": 3.96
    },
    {
      "lang": "en",
      "text": "basically as an example for this token",
      "offset": 3060.2,
      "duration": 4.919
    },
    {
      "lang": "en",
      "text": "25 its Target was 198 and that's now",
      "offset": 3062.079,
      "duration": 5.121
    },
    {
      "lang": "en",
      "text": "just stored at the exact same position",
      "offset": 3065.119,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "in the Target tensor which is 198 and",
      "offset": 3067.2,
      "duration": 6.56
    },
    {
      "lang": "en",
      "text": "also this last token 13 now has its",
      "offset": 3070.119,
      "duration": 6.081
    },
    {
      "lang": "en",
      "text": "label which is 198 and that's just",
      "offset": 3073.76,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "because we loaded this plus one here so",
      "offset": 3076.2,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "basically this is the way I like to do",
      "offset": 3079.44,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "it you take long sequences you uh view",
      "offset": 3080.68,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "them in two- dimensional terms so that",
      "offset": 3084.079,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "you get batch of time and then we make",
      "offset": 3086.04,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "sure to load one additional token so we",
      "offset": 3089.04,
      "duration": 5.519
    },
    {
      "lang": "en",
      "text": "basically load a buffer of tokens of B *",
      "offset": 3091.24,
      "duration": 5.879
    },
    {
      "lang": "en",
      "text": "t+ one and then we sort of offset things",
      "offset": 3094.559,
      "duration": 4.681
    },
    {
      "lang": "en",
      "text": "and view them and then we have two",
      "offset": 3097.119,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "tensors one of them is the input to the",
      "offset": 3099.24,
      "duration": 4.359
    },
    {
      "lang": "en",
      "text": "Transformer and the other exactly is the",
      "offset": 3101.04,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "labels and so let's now reorganize this",
      "offset": 3103.599,
      "duration": 6.641
    },
    {
      "lang": "en",
      "text": "code and um create a very simple data",
      "offset": 3106.48,
      "duration": 6.2
    },
    {
      "lang": "en",
      "text": "loader object that tries to basically",
      "offset": 3110.24,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "load these tokens and um feed them to",
      "offset": 3112.68,
      "duration": 4.52
    },
    {
      "lang": "en",
      "text": "the Transformer and calculate the loss",
      "offset": 3115.24,
      "duration": 4.119
    },
    {
      "lang": "en",
      "text": "okay so I reshuffled the code here uh",
      "offset": 3117.2,
      "duration": 4.44
    },
    {
      "lang": "en",
      "text": "accordingly so as you can see here I'm",
      "offset": 3119.359,
      "duration": 5.841
    },
    {
      "lang": "en",
      "text": "temporarily overwriting U to run a CPU",
      "offset": 3121.64,
      "duration": 5.32
    },
    {
      "lang": "en",
      "text": "and importing TI token and all of this",
      "offset": 3125.2,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "should look familiar we're loading a",
      "offset": 3126.96,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "th000 characters I'm setting BT to just",
      "offset": 3128.4,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "be 4 and 32 right now just because we're",
      "offset": 3130.72,
      "duration": 4.28
    },
    {
      "lang": "en",
      "text": "debugging we just want to have a single",
      "offset": 3133.28,
      "duration": 4.2
    },
    {
      "lang": "en",
      "text": "batch that's very small and all of this",
      "offset": 3135,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "should now look familiar and follows",
      "offset": 3137.48,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "what we did on the right and then here",
      "offset": 3139.16,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "we get the we create the model and get",
      "offset": 3141.72,
      "duration": 6.359
    },
    {
      "lang": "en",
      "text": "the lits and so so here as you see I",
      "offset": 3144.76,
      "duration": 5.24
    },
    {
      "lang": "en",
      "text": "already ran this only runs in a few",
      "offset": 3148.079,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "seconds but because we have a batch of",
      "offset": 3150,
      "duration": 8.44
    },
    {
      "lang": "en",
      "text": "uh 4X 32 our lits are now of size 4X 32x",
      "offset": 3152.799,
      "duration": 8.081
    },
    {
      "lang": "en",
      "text": "50257 so those are the lit for what",
      "offset": 3158.44,
      "duration": 5.24
    },
    {
      "lang": "en",
      "text": "comes next at every position and now we",
      "offset": 3160.88,
      "duration": 5.4
    },
    {
      "lang": "en",
      "text": "have the labels which are stored in y so",
      "offset": 3163.68,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "now is the time to calculate the loss",
      "offset": 3166.28,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "and then do the backward pass and then",
      "offset": 3168.24,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "the optimization so let's first",
      "offset": 3169.88,
      "duration": 2.959
    },
    {
      "lang": "en",
      "text": "calculate the",
      "offset": 3171.839,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "loss okay so to calculate the loss we're",
      "offset": 3172.839,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "going to adjust the forward function of",
      "offset": 3175.28,
      "duration": 4.12
    },
    {
      "lang": "en",
      "text": "this NN module in the model and in",
      "offset": 3176.92,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "particular we're not just going to be",
      "offset": 3179.4,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "returning logits but also we're going to",
      "offset": 3180.839,
      "duration": 4.041
    },
    {
      "lang": "en",
      "text": "return the loss uh and we're going to",
      "offset": 3182.52,
      "duration": 4.039
    },
    {
      "lang": "en",
      "text": "not just pass in the input in thees but",
      "offset": 3184.88,
      "duration": 7.239
    },
    {
      "lang": "en",
      "text": "also the targets uh in y and now we will",
      "offset": 3186.559,
      "duration": 7.481
    },
    {
      "lang": "en",
      "text": "print not Lo just. shape anymore we're",
      "offset": 3192.119,
      "duration": 2.801
    },
    {
      "lang": "en",
      "text": "actually going to print the loss",
      "offset": 3194.04,
      "duration": 3.64
    },
    {
      "lang": "en",
      "text": "function and then c. exit of zero so",
      "offset": 3194.92,
      "duration": 5.24
    },
    {
      "lang": "en",
      "text": "that we skip some of the sampling logic",
      "offset": 3197.68,
      "duration": 4.2
    },
    {
      "lang": "en",
      "text": "so now let's swing up to the forward",
      "offset": 3200.16,
      "duration": 5.399
    },
    {
      "lang": "en",
      "text": "function which gets called there because",
      "offset": 3201.88,
      "duration": 6.36
    },
    {
      "lang": "en",
      "text": "now we also have these optional",
      "offset": 3205.559,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "targets and when we get the targets we",
      "offset": 3208.24,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "can also calculate uh the loss and",
      "offset": 3210.599,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "remember that we want to basically",
      "offset": 3212.96,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "return uh log just loss and loss by",
      "offset": 3214.2,
      "duration": 5.08
    },
    {
      "lang": "en",
      "text": "default is none",
      "offset": 3216.88,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "but",
      "offset": 3219.28,
      "duration": 6.6
    },
    {
      "lang": "en",
      "text": "um let's put this here if uh targets is",
      "offset": 3220.88,
      "duration": 8.719
    },
    {
      "lang": "en",
      "text": "not none then we want to calculate loss",
      "offset": 3225.88,
      "duration": 5.8
    },
    {
      "lang": "en",
      "text": "and co-pilot is already getting excited",
      "offset": 3229.599,
      "duration": 4.121
    },
    {
      "lang": "en",
      "text": "here and calculating the what looks to",
      "offset": 3231.68,
      "duration": 4.28
    },
    {
      "lang": "en",
      "text": "be correct loss it is using the cross",
      "offset": 3233.72,
      "duration": 6.72
    },
    {
      "lang": "en",
      "text": "entropy loss as is documented here uh so",
      "offset": 3235.96,
      "duration": 7.72
    },
    {
      "lang": "en",
      "text": "this is a function in pytorch under the",
      "offset": 3240.44,
      "duration": 5.359
    },
    {
      "lang": "en",
      "text": "functional now what is actually",
      "offset": 3243.68,
      "duration": 3.24
    },
    {
      "lang": "en",
      "text": "happening here because it looks a little",
      "offset": 3245.799,
      "duration": 3.8
    },
    {
      "lang": "en",
      "text": "bit scary uh basically uh the F that",
      "offset": 3246.92,
      "duration": 3.96
    },
    {
      "lang": "en",
      "text": "cross entropy does not like",
      "offset": 3249.599,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "multi-dimensional inputs it can't take a",
      "offset": 3250.88,
      "duration": 4.439
    },
    {
      "lang": "en",
      "text": "b BYT by vocap size so what's happening",
      "offset": 3252.799,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "here is that we are flattening out this",
      "offset": 3255.319,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "three-dimensional tensor into just two",
      "offset": 3257.76,
      "duration": 3.559
    },
    {
      "lang": "en",
      "text": "Dimensions the First Dimension is going",
      "offset": 3259.559,
      "duration": 3.641
    },
    {
      "lang": "en",
      "text": "to be calculated automatically and it's",
      "offset": 3261.319,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "going to be B * T and then the last",
      "offset": 3263.2,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "Dimension is vocap size so basically",
      "offset": 3266.52,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "this is uh flattening out this",
      "offset": 3268.64,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "three-dimensional tensor of logits to",
      "offset": 3270.28,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "just be two- dimensional B * T all",
      "offset": 3272.64,
      "duration": 6.959
    },
    {
      "lang": "en",
      "text": "individual examples and vocap size on uh",
      "offset": 3275.16,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "in terms of the length of each row and",
      "offset": 3279.599,
      "duration": 3.321
    },
    {
      "lang": "en",
      "text": "then it's also flattening out the",
      "offset": 3281.64,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "targets which are also two- dimensional",
      "offset": 3282.92,
      "duration": 3.8
    },
    {
      "lang": "en",
      "text": "at this stage but we're going to just",
      "offset": 3284.64,
      "duration": 3.56
    },
    {
      "lang": "en",
      "text": "flatten them out so they're just a",
      "offset": 3286.72,
      "duration": 4.359
    },
    {
      "lang": "en",
      "text": "single tensor of B * T and this can then",
      "offset": 3288.2,
      "duration": 4.52
    },
    {
      "lang": "en",
      "text": "pass into cross entropy to calculate a",
      "offset": 3291.079,
      "duration": 4.28
    },
    {
      "lang": "en",
      "text": "loss which we return so this should",
      "offset": 3292.72,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "basically at this point run because this",
      "offset": 3295.359,
      "duration": 4.521
    },
    {
      "lang": "en",
      "text": "is not too complicated",
      "offset": 3297.76,
      "duration": 5.599
    },
    {
      "lang": "en",
      "text": "so let's run it and let's see if we",
      "offset": 3299.88,
      "duration": 6.199
    },
    {
      "lang": "en",
      "text": "should be printing the",
      "offset": 3303.359,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "loss and here we see that we printed 11",
      "offset": 3309.52,
      "duration": 6.52
    },
    {
      "lang": "en",
      "text": "uh roughly and so",
      "offset": 3312.96,
      "duration": 5.8
    },
    {
      "lang": "en",
      "text": "um and notice that this is the tensor of",
      "offset": 3316.04,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "a single element which is this number 11",
      "offset": 3318.76,
      "duration": 4.44
    },
    {
      "lang": "en",
      "text": "now we also want to be able to calculate",
      "offset": 3321.64,
      "duration": 4.12
    },
    {
      "lang": "en",
      "text": "a reasonable uh kind of starting point",
      "offset": 3323.2,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "for a random rationalized Network so we",
      "offset": 3325.76,
      "duration": 4.12
    },
    {
      "lang": "en",
      "text": "covered this in previous videos but our",
      "offset": 3327.92,
      "duration": 3.96
    },
    {
      "lang": "en",
      "text": "vocabulary size is",
      "offset": 3329.88,
      "duration": 4.52
    },
    {
      "lang": "en",
      "text": "50257 at initialization of the network",
      "offset": 3331.88,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "you would hope that um every vocab",
      "offset": 3334.4,
      "duration": 6.28
    },
    {
      "lang": "en",
      "text": "element is getting roughly a uniform",
      "offset": 3337.88,
      "duration": 4.679
    },
    {
      "lang": "en",
      "text": "probability uh so that we're not",
      "offset": 3340.68,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "favoring at initialization any token way",
      "offset": 3342.559,
      "duration": 5.24
    },
    {
      "lang": "en",
      "text": "too much we're not confidently wrong at",
      "offset": 3345.559,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "initialization so what we're hoping is",
      "offset": 3347.799,
      "duration": 3.24
    },
    {
      "lang": "en",
      "text": "that the probability of any arbitrary",
      "offset": 3349.559,
      "duration": 5.8
    },
    {
      "lang": "en",
      "text": "token is roughly 1 over 50,2 57 and now",
      "offset": 3351.039,
      "duration": 6.52
    },
    {
      "lang": "en",
      "text": "we can sanity check the loss because",
      "offset": 3355.359,
      "duration": 3.96
    },
    {
      "lang": "en",
      "text": "remember that the cross entropy loss is",
      "offset": 3357.559,
      "duration": 4.04
    },
    {
      "lang": "en",
      "text": "just basically the negative um log",
      "offset": 3359.319,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "likelihood so if we now take this",
      "offset": 3361.599,
      "duration": 4.76
    },
    {
      "lang": "en",
      "text": "probability and we take it through the",
      "offset": 3364.52,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "natural logarithm and then we do the",
      "offset": 3366.359,
      "duration": 5.24
    },
    {
      "lang": "en",
      "text": "negative that is the loss we expect at",
      "offset": 3368.92,
      "duration": 4.36
    },
    {
      "lang": "en",
      "text": "initialization and we covered this in",
      "offset": 3371.599,
      "duration": 3.801
    },
    {
      "lang": "en",
      "text": "previous videos so I would expect",
      "offset": 3373.28,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "something around 10.82 and we're seeing",
      "offset": 3375.4,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "something around 11 so it's not way off",
      "offset": 3377.68,
      "duration": 4.2
    },
    {
      "lang": "en",
      "text": "this is roughly the probability I expect",
      "offset": 3380.039,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "at initialization so that tells me that",
      "offset": 3381.88,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "the at initialization or probability",
      "offset": 3384.359,
      "duration": 3.561
    },
    {
      "lang": "en",
      "text": "distribtion is roughly diffused it's a",
      "offset": 3386.28,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "good starting point and we can now uh",
      "offset": 3387.92,
      "duration": 4.679
    },
    {
      "lang": "en",
      "text": "perform the optimization and tell the",
      "offset": 3390.68,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "network which elements you know should",
      "offset": 3392.599,
      "duration": 4.681
    },
    {
      "lang": "en",
      "text": "follow correctly in what order so at",
      "offset": 3394.599,
      "duration": 4.681
    },
    {
      "lang": "en",
      "text": "this point we can do a l step backward",
      "offset": 3397.28,
      "duration": 3.16
    },
    {
      "lang": "en",
      "text": "calculate the gradients and do an",
      "offset": 3399.28,
      "duration": 3.799
    },
    {
      "lang": "en",
      "text": "optimization so let's get to that okay",
      "offset": 3400.44,
      "duration": 6.2
    },
    {
      "lang": "en",
      "text": "so let's do the optimization now um so",
      "offset": 3403.079,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "here we",
      "offset": 3406.64,
      "duration": 4.52
    },
    {
      "lang": "en",
      "text": "have the loss is this is how we get the",
      "offset": 3407.96,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "loss but now basically we want a load",
      "offset": 3411.16,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "for Loop here so 4 I in range let's do",
      "offset": 3413.48,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "50 steps or something like that uh let's",
      "offset": 3415.88,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "create an Optimizer object in",
      "offset": 3418.2,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "pytorch um and so here we are using the",
      "offset": 3420.599,
      "duration": 6.72
    },
    {
      "lang": "en",
      "text": "atom um Optimizer which is an",
      "offset": 3424.44,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "alternative to the stochastic radian",
      "offset": 3427.319,
      "duration": 4.04
    },
    {
      "lang": "en",
      "text": "descent Optimizer SGD that we were using",
      "offset": 3428.839,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "so SGD is a lot simpler atom is a bit",
      "offset": 3431.359,
      "duration": 3.121
    },
    {
      "lang": "en",
      "text": "more involved and I actually",
      "offset": 3433.16,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "specifically like the atom W variation",
      "offset": 3434.48,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "because in my opinion it kind of just",
      "offset": 3437.319,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "like fixes a bug um so adom w is a bug",
      "offset": 3439.119,
      "duration": 6.041
    },
    {
      "lang": "en",
      "text": "fix of atom is what I would say when we",
      "offset": 3442.52,
      "duration": 4.599
    },
    {
      "lang": "en",
      "text": "go to the documentation for atom",
      "offset": 3445.16,
      "duration": 4.12
    },
    {
      "lang": "en",
      "text": "W oh my",
      "offset": 3447.119,
      "duration": 5.521
    },
    {
      "lang": "en",
      "text": "gosh we see um that it takes a bunch of",
      "offset": 3449.28,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "hyper parameters and it's a little bit",
      "offset": 3452.64,
      "duration": 2.84
    },
    {
      "lang": "en",
      "text": "more complicated than the SGD we were",
      "offset": 3454,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "looking at before uh because in addition",
      "offset": 3455.48,
      "duration": 4.28
    },
    {
      "lang": "en",
      "text": "to basically updating the parameters",
      "offset": 3457.76,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "with the gradient uh scaled by the",
      "offset": 3459.76,
      "duration": 3.799
    },
    {
      "lang": "en",
      "text": "Learning rate it keeps these buffers",
      "offset": 3461.359,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "around and it keeps two buffers the m",
      "offset": 3463.559,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "and the V which it calls the first and",
      "offset": 3466.24,
      "duration": 3.72
    },
    {
      "lang": "en",
      "text": "the second moment so something that",
      "offset": 3468.119,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "looks a bit like momentum and something",
      "offset": 3469.96,
      "duration": 3.639
    },
    {
      "lang": "en",
      "text": "that looks a bit like RMS prop if you're",
      "offset": 3471.559,
      "duration": 3.8
    },
    {
      "lang": "en",
      "text": "familiar with it but you don't have to",
      "offset": 3473.599,
      "duration": 3.72
    },
    {
      "lang": "en",
      "text": "be it's just kind of a normalization",
      "offset": 3475.359,
      "duration": 3.641
    },
    {
      "lang": "en",
      "text": "that happens on each gradient element",
      "offset": 3477.319,
      "duration": 3.641
    },
    {
      "lang": "en",
      "text": "individually and speeds up the",
      "offset": 3479,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "optimization especially for language",
      "offset": 3480.96,
      "duration": 3.32
    },
    {
      "lang": "en",
      "text": "models but I'm not going to go into the",
      "offset": 3482.599,
      "duration": 3.561
    },
    {
      "lang": "en",
      "text": "detail right here we're going to treat",
      "offset": 3484.28,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "it as a bit of a black box and it just",
      "offset": 3486.16,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "optimizes um the objective faster than",
      "offset": 3488.92,
      "duration": 4.84
    },
    {
      "lang": "en",
      "text": "SGD which is what we've seen in the",
      "offset": 3492,
      "duration": 3.88
    },
    {
      "lang": "en",
      "text": "previous lectures so let's use it as a",
      "offset": 3493.76,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "black box in our case uh create the",
      "offset": 3495.88,
      "duration": 5.719
    },
    {
      "lang": "en",
      "text": "optimizer object and",
      "offset": 3498.48,
      "duration": 7.639
    },
    {
      "lang": "en",
      "text": "then go through the optimization",
      "offset": 3501.599,
      "duration": 4.52
    },
    {
      "lang": "en",
      "text": "the first thing to always make sure the",
      "offset": 3508.76,
      "duration": 3.799
    },
    {
      "lang": "en",
      "text": "co-pilot did not forget to zero the",
      "offset": 3510.64,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "gradients so um always remember that you",
      "offset": 3512.559,
      "duration": 5.56
    },
    {
      "lang": "en",
      "text": "have to start with a zero gradient then",
      "offset": 3515.44,
      "duration": 4.119
    },
    {
      "lang": "en",
      "text": "when you get your loss and you do a DOT",
      "offset": 3518.119,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "backward dot backward adds to gradients",
      "offset": 3519.559,
      "duration": 5.321
    },
    {
      "lang": "en",
      "text": "so it deposits gradients it it always",
      "offset": 3522.76,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "does a plus equals on whatever the",
      "offset": 3524.88,
      "duration": 3.239
    },
    {
      "lang": "en",
      "text": "gradients are which is why you must set",
      "offset": 3526.359,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "them to zero so this accumulates the",
      "offset": 3528.119,
      "duration": 4.521
    },
    {
      "lang": "en",
      "text": "gradient from this loss and then we call",
      "offset": 3530.28,
      "duration": 6.36
    },
    {
      "lang": "en",
      "text": "the step function on the optimizer to um",
      "offset": 3532.64,
      "duration": 7.52
    },
    {
      "lang": "en",
      "text": "update the parameters and to um decrease",
      "offset": 3536.64,
      "duration": 4.199
    },
    {
      "lang": "en",
      "text": "the",
      "offset": 3540.16,
      "duration": 3.399
    },
    {
      "lang": "en",
      "text": "loss and then we print a step and the",
      "offset": 3540.839,
      "duration": 5.361
    },
    {
      "lang": "en",
      "text": "loss do item is used here because loss",
      "offset": 3543.559,
      "duration": 4.681
    },
    {
      "lang": "en",
      "text": "is a tensor with a single element do",
      "offset": 3546.2,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "item will actually uh convert that to a",
      "offset": 3548.24,
      "duration": 5.4
    },
    {
      "lang": "en",
      "text": "single float and this float will live",
      "offset": 3551.24,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "not will will live on the CPU so this",
      "offset": 3553.64,
      "duration": 4.12
    },
    {
      "lang": "en",
      "text": "gets to some of the internals again of",
      "offset": 3556.52,
      "duration": 4.2
    },
    {
      "lang": "en",
      "text": "the devices but loss is a is a tensor",
      "offset": 3557.76,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "with a single element and it lifts on",
      "offset": 3560.72,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "GPU for me because I'm using gpus when",
      "offset": 3562.88,
      "duration": 5.719
    },
    {
      "lang": "en",
      "text": "you call item P torch behind the scenes",
      "offset": 3565.52,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "will take that one-dimensional tensor",
      "offset": 3568.599,
      "duration": 4.361
    },
    {
      "lang": "en",
      "text": "ship it back to the CPU uh memory and",
      "offset": 3570.319,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "convert it into a float that we can just",
      "offset": 3572.96,
      "duration": 5.399
    },
    {
      "lang": "en",
      "text": "print so this is the optimization and",
      "offset": 3575.319,
      "duration": 6.681
    },
    {
      "lang": "en",
      "text": "this should probably just",
      "offset": 3578.359,
      "duration": 6.801
    },
    {
      "lang": "en",
      "text": "work let's see what",
      "offset": 3582,
      "duration": 5.64
    },
    {
      "lang": "en",
      "text": "happens actually sorry let me instead of",
      "offset": 3585.16,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "using CPU override let me delete that so",
      "offset": 3587.64,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "this is a bit faster for me and it runs",
      "offset": 3590.44,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "on Cuda",
      "offset": 3592.119,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "oh expected all tensors to be on the",
      "offset": 3598.68,
      "duration": 3.56
    },
    {
      "lang": "en",
      "text": "same device but found at least two",
      "offset": 3600.92,
      "duration": 5.119
    },
    {
      "lang": "en",
      "text": "devices Cuda zero and CPU so Cuda zero",
      "offset": 3602.24,
      "duration": 5.64
    },
    {
      "lang": "en",
      "text": "is the zeroth GPU because I actually",
      "offset": 3606.039,
      "duration": 4.76
    },
    {
      "lang": "en",
      "text": "have eight gpus on this box uh so the",
      "offset": 3607.88,
      "duration": 6.959
    },
    {
      "lang": "en",
      "text": "zeroth GPU in my box and CPU and model",
      "offset": 3610.799,
      "duration": 6.401
    },
    {
      "lang": "en",
      "text": "we have moved to device but when I was",
      "offset": 3614.839,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "writing this code I actually introduced",
      "offset": 3617.2,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "a bug because buff we never moved to",
      "offset": 3618.52,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "device and you have to be careful",
      "offset": 3621.2,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "because you can't just do buff dot two",
      "offset": 3623.16,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "of",
      "offset": 3625.68,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "device um it's not stateful it doesn't",
      "offset": 3626.92,
      "duration": 6.439
    },
    {
      "lang": "en",
      "text": "convert it to be a device it instead uh",
      "offset": 3630,
      "duration": 5.64
    },
    {
      "lang": "en",
      "text": "returns pointer to a new memory which is",
      "offset": 3633.359,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "on the device so you see how we can just",
      "offset": 3635.64,
      "duration": 3.959
    },
    {
      "lang": "en",
      "text": "do model that two a device that does not",
      "offset": 3637.68,
      "duration": 4.439
    },
    {
      "lang": "en",
      "text": "apply to tensors you have to do buff",
      "offset": 3639.599,
      "duration": 4.921
    },
    {
      "lang": "en",
      "text": "equals",
      "offset": 3642.119,
      "duration": 7.841
    },
    {
      "lang": "en",
      "text": "um b.2 device and then this should work",
      "offset": 3644.52,
      "duration": 7.64
    },
    {
      "lang": "en",
      "text": "okay so what do we expect to see we",
      "offset": 3649.96,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "expect to see a reasonable loss in the",
      "offset": 3652.16,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "beginning and then we continue to",
      "offset": 3653.799,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "optimize just the single batch and so we",
      "offset": 3655.28,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "want to see that we can overfit this",
      "offset": 3657.4,
      "duration": 3.639
    },
    {
      "lang": "en",
      "text": "single batch we can we can crush this",
      "offset": 3658.88,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "little batch and we can perfectly",
      "offset": 3661.039,
      "duration": 3.481
    },
    {
      "lang": "en",
      "text": "predict the indices on just this little",
      "offset": 3662.799,
      "duration": 3.961
    },
    {
      "lang": "en",
      "text": "batch and indeed that is roughly what",
      "offset": 3664.52,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "we're seeing here",
      "offset": 3666.76,
      "duration": 5.88
    },
    {
      "lang": "en",
      "text": "so um we started off at roughly 10.82 11",
      "offset": 3668.2,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "in this case and then as we continue",
      "offset": 3672.64,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "optimizing on this single batch without",
      "offset": 3674.44,
      "duration": 3.399
    },
    {
      "lang": "en",
      "text": "loading new examples we are making sure",
      "offset": 3676,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "that we can overfit a single batch and",
      "offset": 3677.839,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "we are getting to very very low loss so",
      "offset": 3680,
      "duration": 4.359
    },
    {
      "lang": "en",
      "text": "the Transformer is memorizing this",
      "offset": 3681.92,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "single individual batch and one more",
      "offset": 3684.359,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "thing I didn't mention is uh the",
      "offset": 3686.64,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "learning rate here is 3 E4 which is a",
      "offset": 3688.119,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "pretty good default for most uh",
      "offset": 3690.72,
      "duration": 4.44
    },
    {
      "lang": "en",
      "text": "optimizations that you want to run at a",
      "offset": 3693.319,
      "duration": 5.081
    },
    {
      "lang": "en",
      "text": "very early debugging stage so this is",
      "offset": 3695.16,
      "duration": 6.199
    },
    {
      "lang": "en",
      "text": "our simple inter Loop and uh we are",
      "offset": 3698.4,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "overfitting a single batch and this",
      "offset": 3701.359,
      "duration": 4.121
    },
    {
      "lang": "en",
      "text": "looks good so now what uh what comes",
      "offset": 3702.96,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "next is we don't just want to overfit a",
      "offset": 3705.48,
      "duration": 2.879
    },
    {
      "lang": "en",
      "text": "single batch we actually want to do an",
      "offset": 3706.799,
      "duration": 3.401
    },
    {
      "lang": "en",
      "text": "optimization so we actually need to",
      "offset": 3708.359,
      "duration": 4.2
    },
    {
      "lang": "en",
      "text": "iterate these XY batches and create a",
      "offset": 3710.2,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "little data loader uh that makes sure",
      "offset": 3712.559,
      "duration": 4.121
    },
    {
      "lang": "en",
      "text": "that we're always getting a fresh batch",
      "offset": 3714.52,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "and that we're actually optimizing a",
      "offset": 3716.68,
      "duration": 2.919
    },
    {
      "lang": "en",
      "text": "reasonable objective so let's do that",
      "offset": 3717.88,
      "duration": 3.88
    },
    {
      "lang": "en",
      "text": "next okay so this is what I came up with",
      "offset": 3719.599,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "and I wrote a little data loader",
      "offset": 3721.76,
      "duration": 4.92
    },
    {
      "lang": "en",
      "text": "light um so what this data loader does",
      "offset": 3723.76,
      "duration": 5.16
    },
    {
      "lang": "en",
      "text": "is we're importing the token up here",
      "offset": 3726.68,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "we're reading the entire text file from",
      "offset": 3728.92,
      "duration": 3.879
    },
    {
      "lang": "en",
      "text": "this single input.txt",
      "offset": 3730.599,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "tokenizing it and then we're just",
      "offset": 3732.799,
      "duration": 4.601
    },
    {
      "lang": "en",
      "text": "printing the number of tokens in total",
      "offset": 3734.839,
      "duration": 4.601
    },
    {
      "lang": "en",
      "text": "and the number of batches in a single",
      "offset": 3737.4,
      "duration": 4.84
    },
    {
      "lang": "en",
      "text": "Epoch of iterating over this data set so",
      "offset": 3739.44,
      "duration": 5.24
    },
    {
      "lang": "en",
      "text": "how many unique batches do we output",
      "offset": 3742.24,
      "duration": 4.039
    },
    {
      "lang": "en",
      "text": "before we loop back around the beginning",
      "offset": 3744.68,
      "duration": 3.639
    },
    {
      "lang": "en",
      "text": "of the document and start reading it",
      "offset": 3746.279,
      "duration": 5.161
    },
    {
      "lang": "en",
      "text": "again so we start off at position zero",
      "offset": 3748.319,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "and then we simply walk the document in",
      "offset": 3751.44,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "batches of B * T so we take chunks of B",
      "offset": 3753.52,
      "duration": 7.36
    },
    {
      "lang": "en",
      "text": "* T and then always Advance by B * T and",
      "offset": 3756.4,
      "duration": 6.399
    },
    {
      "lang": "en",
      "text": "um it's important to note that we're",
      "offset": 3760.88,
      "duration": 3.959
    },
    {
      "lang": "en",
      "text": "always advancing our position by exactly",
      "offset": 3762.799,
      "duration": 4.841
    },
    {
      "lang": "en",
      "text": "B * T but when we're fetching the tokens",
      "offset": 3764.839,
      "duration": 4.681
    },
    {
      "lang": "en",
      "text": "we're actually fetching from current",
      "offset": 3767.64,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "position to B * t + 1 and we need that",
      "offset": 3769.52,
      "duration": 5.559
    },
    {
      "lang": "en",
      "text": "plus one because remember uh we need the",
      "offset": 3772.76,
      "duration": 3.88
    },
    {
      "lang": "en",
      "text": "target token",
      "offset": 3775.079,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "um for the last token in the current",
      "offset": 3776.64,
      "duration": 6.199
    },
    {
      "lang": "en",
      "text": "batch and so that way we can do um the",
      "offset": 3778.76,
      "duration": 8.319
    },
    {
      "lang": "en",
      "text": "XY exactly as we did it before and if we",
      "offset": 3782.839,
      "duration": 6.52
    },
    {
      "lang": "en",
      "text": "are to um run out of data we'll just",
      "offset": 3787.079,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "loop back around to zero so this is one",
      "offset": 3789.359,
      "duration": 4.44
    },
    {
      "lang": "en",
      "text": "way to write a very very simple data",
      "offset": 3792.119,
      "duration": 4.44
    },
    {
      "lang": "en",
      "text": "loader um that simply just goes through",
      "offset": 3793.799,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "the file in chunks and is good enough",
      "offset": 3796.559,
      "duration": 5.401
    },
    {
      "lang": "en",
      "text": "for us uh for current purposes and we're",
      "offset": 3799.319,
      "duration": 5.161
    },
    {
      "lang": "en",
      "text": "going to complexify it later and now",
      "offset": 3801.96,
      "duration": 4.359
    },
    {
      "lang": "en",
      "text": "we'd like to come back around here and",
      "offset": 3804.48,
      "duration": 3.079
    },
    {
      "lang": "en",
      "text": "we'd like to actually use our data",
      "offset": 3806.319,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "loader so the import Tik token has moved",
      "offset": 3807.559,
      "duration": 4.76
    },
    {
      "lang": "en",
      "text": "up and actually all of this is now",
      "offset": 3809.839,
      "duration": 5.161
    },
    {
      "lang": "en",
      "text": "useless so instead we just want a train",
      "offset": 3812.319,
      "duration": 6.161
    },
    {
      "lang": "en",
      "text": "loader for the training data and we want",
      "offset": 3815,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "to use the same hyper parameters for",
      "offset": 3818.48,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "four so B size was four and time was",
      "offset": 3819.88,
      "duration": 7.199
    },
    {
      "lang": "en",
      "text": "32 and then here we need to get the XY",
      "offset": 3823.48,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "for the current batch so let's see if",
      "offset": 3827.079,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "copal gets it because this is simple",
      "offset": 3829.24,
      "duration": 4.68
    },
    {
      "lang": "en",
      "text": "enough uh so we call the next batch and",
      "offset": 3831.24,
      "duration": 6.359
    },
    {
      "lang": "en",
      "text": "then we um make sure that we have to",
      "offset": 3833.92,
      "duration": 8.84
    },
    {
      "lang": "en",
      "text": "move our tensors from CPU to the device",
      "offset": 3837.599,
      "duration": 7.52
    },
    {
      "lang": "en",
      "text": "so here when I converted the tokens",
      "offset": 3842.76,
      "duration": 4.2
    },
    {
      "lang": "en",
      "text": "notice that I didn't actually move these",
      "offset": 3845.119,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "tokens to the GPU I left them on CPU",
      "offset": 3846.96,
      "duration": 5.72
    },
    {
      "lang": "en",
      "text": "which is the default um and that's just",
      "offset": 3850.119,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "because I'm trying not to waste too much",
      "offset": 3852.68,
      "duration": 4.119
    },
    {
      "lang": "en",
      "text": "memory on the GPU in this case this is a",
      "offset": 3854.68,
      "duration": 4.439
    },
    {
      "lang": "en",
      "text": "tiny data set and it would fit uh but",
      "offset": 3856.799,
      "duration": 4.52
    },
    {
      "lang": "en",
      "text": "it's fine to just uh ship it to GPU",
      "offset": 3859.119,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "right now for for our purposes right now",
      "offset": 3861.319,
      "duration": 4.681
    },
    {
      "lang": "en",
      "text": "so we get the next batch we keep the",
      "offset": 3864.319,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "data loader simple CPU class and then",
      "offset": 3866,
      "duration": 5.359
    },
    {
      "lang": "en",
      "text": "here we actually ship it to the GPU and",
      "offset": 3869.039,
      "duration": 5.8
    },
    {
      "lang": "en",
      "text": "do all the computation and uh let's see",
      "offset": 3871.359,
      "duration": 8.361
    },
    {
      "lang": "en",
      "text": "if this runs so python train gbt2 pi and",
      "offset": 3874.839,
      "duration": 6.361
    },
    {
      "lang": "en",
      "text": "what do we expect to see before this",
      "offset": 3879.72,
      "duration": 3.399
    },
    {
      "lang": "en",
      "text": "actually happens what we expect to see",
      "offset": 3881.2,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "is now we're actually getting the next",
      "offset": 3883.119,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "batch so we expect to not overfit a",
      "offset": 3884.72,
      "duration": 5.839
    },
    {
      "lang": "en",
      "text": "single batch and so I expect our loss to",
      "offset": 3887.44,
      "duration": 6.639
    },
    {
      "lang": "en",
      "text": "come down but not too much and that's",
      "offset": 3890.559,
      "duration": 5.361
    },
    {
      "lang": "en",
      "text": "because I still expect it to come down",
      "offset": 3894.079,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "because in the",
      "offset": 3895.92,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "50257 tokens many of those tokens never",
      "offset": 3897.279,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "occur in our data set so there are some",
      "offset": 3900.24,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "very easy gains to be made here in the",
      "offset": 3902.279,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "optimization by for example taking the",
      "offset": 3904.24,
      "duration": 4.599
    },
    {
      "lang": "en",
      "text": "biases of all the loits that never occur",
      "offset": 3906.359,
      "duration": 4.841
    },
    {
      "lang": "en",
      "text": "and driving them to negative infinity",
      "offset": 3908.839,
      "duration": 4.041
    },
    {
      "lang": "en",
      "text": "and that would basically just it's just",
      "offset": 3911.2,
      "duration": 3.399
    },
    {
      "lang": "en",
      "text": "that all of these crazy unic codes or",
      "offset": 3912.88,
      "duration": 3.56
    },
    {
      "lang": "en",
      "text": "different languages those tokens never",
      "offset": 3914.599,
      "duration": 3.321
    },
    {
      "lang": "en",
      "text": "occur so their probability should be",
      "offset": 3916.44,
      "duration": 3.399
    },
    {
      "lang": "en",
      "text": "very low and so the gains that we should",
      "offset": 3917.92,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "be seeing are along the lines of",
      "offset": 3919.839,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "basically deleting the usage of tokens",
      "offset": 3922.319,
      "duration": 4.201
    },
    {
      "lang": "en",
      "text": "that never occur that's probably most of",
      "offset": 3924.48,
      "duration": 3.639
    },
    {
      "lang": "en",
      "text": "the loss gain that we're going to see at",
      "offset": 3926.52,
      "duration": 4.039
    },
    {
      "lang": "en",
      "text": "this scale right now uh but we shouldn't",
      "offset": 3928.119,
      "duration": 7
    },
    {
      "lang": "en",
      "text": "come to a zero uh because um we are only",
      "offset": 3930.559,
      "duration": 6.681
    },
    {
      "lang": "en",
      "text": "doing 50 iterations and I don't think",
      "offset": 3935.119,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "that's enough to do an eoch right now so",
      "offset": 3937.24,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "let's see what we",
      "offset": 3939.359,
      "duration": 5.48
    },
    {
      "lang": "en",
      "text": "got we um we have 338,000",
      "offset": 3940.599,
      "duration": 6.68
    },
    {
      "lang": "en",
      "text": "tokens which makes sense with our 3:1",
      "offset": 3944.839,
      "duration": 4.041
    },
    {
      "lang": "en",
      "text": "compression ratio because there are 1",
      "offset": 3947.279,
      "duration": 5.441
    },
    {
      "lang": "en",
      "text": "million uh characters so one Epoch with",
      "offset": 3948.88,
      "duration": 6.32
    },
    {
      "lang": "en",
      "text": "the current setting of B and T will take",
      "offset": 3952.72,
      "duration": 5.839
    },
    {
      "lang": "en",
      "text": "2, 600 batches and we're only doing 50",
      "offset": 3955.2,
      "duration": 5.8
    },
    {
      "lang": "en",
      "text": "batches of optimization in",
      "offset": 3958.559,
      "duration": 4.681
    },
    {
      "lang": "en",
      "text": "here so we start off in a familiar",
      "offset": 3961,
      "duration": 4.68
    },
    {
      "lang": "en",
      "text": "territory as expected and then we seem",
      "offset": 3963.24,
      "duration": 6.559
    },
    {
      "lang": "en",
      "text": "to come down to about 6.6 so basically",
      "offset": 3965.68,
      "duration": 6.2
    },
    {
      "lang": "en",
      "text": "things seem to be working okay right now",
      "offset": 3969.799,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "with respect to our expectations so",
      "offset": 3971.88,
      "duration": 4.12
    },
    {
      "lang": "en",
      "text": "that's good okay next I want to actually",
      "offset": 3973.799,
      "duration": 5.081
    },
    {
      "lang": "en",
      "text": "fix a bug that we have in our code um",
      "offset": 3976,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "it's not a major bug but it is a bug",
      "offset": 3978.88,
      "duration": 3.8
    },
    {
      "lang": "en",
      "text": "with respect to how gpt2 training uh",
      "offset": 3980.72,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "should",
      "offset": 3982.68,
      "duration": 3.639
    },
    {
      "lang": "en",
      "text": "happen um",
      "offset": 3984.079,
      "duration": 4.841
    },
    {
      "lang": "en",
      "text": "so the buck is the following we were not",
      "offset": 3986.319,
      "duration": 3.641
    },
    {
      "lang": "en",
      "text": "being careful enough when we were",
      "offset": 3988.92,
      "duration": 2.199
    },
    {
      "lang": "en",
      "text": "loading the weights from hugging face",
      "offset": 3989.96,
      "duration": 3.24
    },
    {
      "lang": "en",
      "text": "and we actually missed a little detail",
      "offset": 3991.119,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "so if we come",
      "offset": 3993.2,
      "duration": 5.639
    },
    {
      "lang": "en",
      "text": "here notice that um the shape of these",
      "offset": 3995.279,
      "duration": 6.961
    },
    {
      "lang": "en",
      "text": "two tensors is the same so this one here",
      "offset": 3998.839,
      "duration": 5.321
    },
    {
      "lang": "en",
      "text": "is the token embedding at the bottom of",
      "offset": 4002.24,
      "duration": 2.879
    },
    {
      "lang": "en",
      "text": "the",
      "offset": 4004.16,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "Transformer right so and this one here",
      "offset": 4005.119,
      "duration": 5.321
    },
    {
      "lang": "en",
      "text": "is the language modeling head at the top",
      "offset": 4008.319,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "of the",
      "offset": 4010.44,
      "duration": 3.08
    },
    {
      "lang": "en",
      "text": "Transformer and both of these are",
      "offset": 4011.319,
      "duration": 4.04
    },
    {
      "lang": "en",
      "text": "basically two-dimensional tensors and",
      "offset": 4013.52,
      "duration": 5.96
    },
    {
      "lang": "en",
      "text": "they shape is identical so here the",
      "offset": 4015.359,
      "duration": 5.401
    },
    {
      "lang": "en",
      "text": "first one is the output embedding the",
      "offset": 4019.48,
      "duration": 3.319
    },
    {
      "lang": "en",
      "text": "token embedding and the second one is",
      "offset": 4020.76,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "this linear layer at the very top the",
      "offset": 4022.799,
      "duration": 4.201
    },
    {
      "lang": "en",
      "text": "classifier layer both of them are of",
      "offset": 4024.68,
      "duration": 3.56
    },
    {
      "lang": "en",
      "text": "shape",
      "offset": 4027,
      "duration": 2.92
    },
    {
      "lang": "en",
      "text": "50257 X",
      "offset": 4028.24,
      "duration": 5.4
    },
    {
      "lang": "en",
      "text": "768 um this one here is giving us our",
      "offset": 4029.92,
      "duration": 6.119
    },
    {
      "lang": "en",
      "text": "token embeddings at the bottom and this",
      "offset": 4033.64,
      "duration": 5.24
    },
    {
      "lang": "en",
      "text": "one here is taking the 768 channels of",
      "offset": 4036.039,
      "duration": 5.161
    },
    {
      "lang": "en",
      "text": "the Transformer and trying to upscale",
      "offset": 4038.88,
      "duration": 5.56
    },
    {
      "lang": "en",
      "text": "that to 50, 257 to get the Lis for the",
      "offset": 4041.2,
      "duration": 5.8
    },
    {
      "lang": "en",
      "text": "next token so they're both the same",
      "offset": 4044.44,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "shape but more than that actually if you",
      "offset": 4047,
      "duration": 6.599
    },
    {
      "lang": "en",
      "text": "look at um comparing their elements um",
      "offset": 4049.48,
      "duration": 6.079
    },
    {
      "lang": "en",
      "text": "in pytorch this is an element wise",
      "offset": 4053.599,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "equality so then we use do all and we",
      "offset": 4055.559,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "see that every single element is",
      "offset": 4057.76,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "identical and more than that we see that",
      "offset": 4059.319,
      "duration": 5.601
    },
    {
      "lang": "en",
      "text": "if we actually look at the data pointer",
      "offset": 4062.559,
      "duration": 4.441
    },
    {
      "lang": "en",
      "text": "uh this is what this is a way in pytorch",
      "offset": 4064.92,
      "duration": 4.439
    },
    {
      "lang": "en",
      "text": "to get the actual pointer to the uh data",
      "offset": 4067,
      "duration": 4.44
    },
    {
      "lang": "en",
      "text": "and the storage we see that actually the",
      "offset": 4069.359,
      "duration": 4.361
    },
    {
      "lang": "en",
      "text": "pointer is identical so not only are",
      "offset": 4071.44,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "these two separate tensors that happen",
      "offset": 4073.72,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "to have the same shape and elements",
      "offset": 4075.52,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "they're actually pointing to the",
      "offset": 4077.4,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "identical tensor so what's happening",
      "offset": 4078.64,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "here is that this is a common weight",
      "offset": 4082.119,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "tying scheme uh that actually comes from",
      "offset": 4083.76,
      "duration": 4.599
    },
    {
      "lang": "en",
      "text": "the original",
      "offset": 4086.52,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "um from the original attention is all",
      "offset": 4088.359,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "you need paper and actually even the",
      "offset": 4090.68,
      "duration": 5.559
    },
    {
      "lang": "en",
      "text": "reference before it so if we come",
      "offset": 4092.279,
      "duration": 6.96
    },
    {
      "lang": "en",
      "text": "here",
      "offset": 4096.239,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "um eddings and softmax in the attention",
      "offset": 4099.96,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "is all you need paper they mentioned",
      "offset": 4102.48,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "that in our model we shared the same",
      "offset": 4104.92,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "weight Matrix between the two embedding",
      "offset": 4106.96,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "layers and the pre softmax linear",
      "offset": 4108.52,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "transformation similar to 30 um so this",
      "offset": 4110.799,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "is an awkward way to phrase that these",
      "offset": 4114.52,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "two are shared and they're tied and",
      "offset": 4116.799,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "they're the same Matrix and the 30",
      "offset": 4118.359,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "reference is this",
      "offset": 4120.96,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "paper um so this came out in",
      "offset": 4122.52,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "2017 and you can read the full paper but",
      "offset": 4125.199,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "basically it argues for this weight",
      "offset": 4127.719,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "tying scheme and I think intuitively the",
      "offset": 4129.759,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "idea for why you might want to do this",
      "offset": 4133.159,
      "duration": 4.841
    },
    {
      "lang": "en",
      "text": "comes from from this paragraph here and",
      "offset": 4134.96,
      "duration": 6.44
    },
    {
      "lang": "en",
      "text": "basically you you can observe",
      "offset": 4138,
      "duration": 6.679
    },
    {
      "lang": "en",
      "text": "that um you actually want these two",
      "offset": 4141.4,
      "duration": 5.759
    },
    {
      "lang": "en",
      "text": "matrices to behave similar in the",
      "offset": 4144.679,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "following sense if two tokens are very",
      "offset": 4147.159,
      "duration": 5.64
    },
    {
      "lang": "en",
      "text": "similar semantically like maybe one of",
      "offset": 4150.679,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "them is all lowercase and the other one",
      "offset": 4152.799,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "is all uppercase or it's the same token",
      "offset": 4154.279,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "in a different language or something",
      "offset": 4156.48,
      "duration": 2.92
    },
    {
      "lang": "en",
      "text": "like that if you have similarity between",
      "offset": 4157.799,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "two tokens presumably you would expect",
      "offset": 4159.4,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "that they are uh nearby in the token",
      "offset": 4161.4,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "embedding space but in the exact same",
      "offset": 4163.4,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "way you'd expect that if you have two",
      "offset": 4166.199,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "tokens that are similar semantically",
      "offset": 4167.88,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "you'd expect them to get the same",
      "offset": 4170.44,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "probabilities at the output of a",
      "offset": 4172.199,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "transformer because they are",
      "offset": 4173.96,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "semantically similar and so both",
      "offset": 4175.239,
      "duration": 6.281
    },
    {
      "lang": "en",
      "text": "positions in the Transformer at the very",
      "offset": 4179.64,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "bottom and at the top have this property",
      "offset": 4181.52,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "that similar tokens should have similar",
      "offset": 4183.799,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "embeddings or similar weights and so",
      "offset": 4186.08,
      "duration": 5.239
    },
    {
      "lang": "en",
      "text": "this is what motivates their exploration",
      "offset": 4189.239,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "here and they they kind of you know I",
      "offset": 4191.319,
      "duration": 2.84
    },
    {
      "lang": "en",
      "text": "don't want to go through the entire",
      "offset": 4193.159,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "paper and and uh you can go through it",
      "offset": 4194.159,
      "duration": 5.161
    },
    {
      "lang": "en",
      "text": "but this is what they observe they also",
      "offset": 4197.08,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "observe that if you look at the output",
      "offset": 4199.32,
      "duration": 3.32
    },
    {
      "lang": "en",
      "text": "embeddings they also behave like word",
      "offset": 4200.6,
      "duration": 5.559
    },
    {
      "lang": "en",
      "text": "embeddings um if you um if you just kind",
      "offset": 4202.64,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "of try to use those weights as word",
      "offset": 4206.159,
      "duration": 4.441
    },
    {
      "lang": "en",
      "text": "embeddings um so they kind of observe",
      "offset": 4208,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "this similarity they try to tie them and",
      "offset": 4210.6,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "they observe that they can get much",
      "offset": 4213,
      "duration": 4.199
    },
    {
      "lang": "en",
      "text": "better performance in that way and so",
      "offset": 4214.12,
      "duration": 4.76
    },
    {
      "lang": "en",
      "text": "this was adopted and the attention is",
      "offset": 4217.199,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "all need paper and then it was used",
      "offset": 4218.88,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "again in gpt2 as well",
      "offset": 4220.96,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "so I couldn't find it in the",
      "offset": 4224.32,
      "duration": 4.04
    },
    {
      "lang": "en",
      "text": "Transformers implementation I'm not sure",
      "offset": 4226.159,
      "duration": 4.361
    },
    {
      "lang": "en",
      "text": "where they tie those embeddings but I",
      "offset": 4228.36,
      "duration": 5.799
    },
    {
      "lang": "en",
      "text": "can find it in the original gpt2 code U",
      "offset": 4230.52,
      "duration": 6.4
    },
    {
      "lang": "en",
      "text": "introduced by open aai so this is um",
      "offset": 4234.159,
      "duration": 6.281
    },
    {
      "lang": "en",
      "text": "openai gpt2 Source model and here where",
      "offset": 4236.92,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "they are forwarding this model and this",
      "offset": 4240.44,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "is in tensorflow but uh that's okay we",
      "offset": 4241.96,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "see that they get the wte token",
      "offset": 4244.679,
      "duration": 5.56
    },
    {
      "lang": "en",
      "text": "embeddings and then here is the incoder",
      "offset": 4246.6,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "of the token embeddings and the",
      "offset": 4250.239,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "position and then here at the bottom",
      "offset": 4252.28,
      "duration": 6.28
    },
    {
      "lang": "en",
      "text": "they Ed the WT again to do the lits so",
      "offset": 4254.32,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "when they get the loits it's a math Mo",
      "offset": 4258.56,
      "duration": 4.36
    },
    {
      "lang": "en",
      "text": "of uh this output from the Transformer",
      "offset": 4260.48,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "and the wte tensor is",
      "offset": 4262.92,
      "duration": 5.4
    },
    {
      "lang": "en",
      "text": "reused um and so the wte tensor",
      "offset": 4265.36,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "basically is used twice on the bottom of",
      "offset": 4268.32,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "the Transformer and on the top of the",
      "offset": 4270.239,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "Transformer and in the backward pass",
      "offset": 4272.159,
      "duration": 4.921
    },
    {
      "lang": "en",
      "text": "we'll get gradients contributions from",
      "offset": 4274.64,
      "duration": 4.36
    },
    {
      "lang": "en",
      "text": "both branches right and these gradients",
      "offset": 4277.08,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "will add up um on the wte tensor um so",
      "offset": 4279,
      "duration": 5.64
    },
    {
      "lang": "en",
      "text": "we'll get a contribution from the",
      "offset": 4283.56,
      "duration": 2.32
    },
    {
      "lang": "en",
      "text": "classifier list",
      "offset": 4284.64,
      "duration": 2.36
    },
    {
      "lang": "en",
      "text": "and then at the very end of the",
      "offset": 4285.88,
      "duration": 3.08
    },
    {
      "lang": "en",
      "text": "Transformer we'll get a contribution at",
      "offset": 4287,
      "duration": 4.44
    },
    {
      "lang": "en",
      "text": "the at the bottom of it float floating",
      "offset": 4288.96,
      "duration": 6.32
    },
    {
      "lang": "en",
      "text": "again into the wte uh tensor so we want",
      "offset": 4291.44,
      "duration": 6.88
    },
    {
      "lang": "en",
      "text": "to we are currently not sharing WT and",
      "offset": 4295.28,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "our code but we want to do",
      "offset": 4298.32,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "that um",
      "offset": 4300.88,
      "duration": 7.44
    },
    {
      "lang": "en",
      "text": "so weight sharing scheme um and one way",
      "offset": 4304.32,
      "duration": 6.64
    },
    {
      "lang": "en",
      "text": "to do this let's see if goil gets it oh",
      "offset": 4308.32,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "it does okay uh so this is one way to do",
      "offset": 4310.96,
      "duration": 5.279
    },
    {
      "lang": "en",
      "text": "it",
      "offset": 4314.56,
      "duration": 2.4
    },
    {
      "lang": "en",
      "text": "uh",
      "offset": 4316.239,
      "duration": 2.761
    },
    {
      "lang": "en",
      "text": "basically relatively straightforward",
      "offset": 4316.96,
      "duration": 3.32
    },
    {
      "lang": "en",
      "text": "what we're doing here is we're taking",
      "offset": 4319,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "the wte do weight and we're simply uh",
      "offset": 4320.28,
      "duration": 8.24
    },
    {
      "lang": "en",
      "text": "redirecting it to point to the LM head",
      "offset": 4324.6,
      "duration": 7.36
    },
    {
      "lang": "en",
      "text": "so um this basically copies the data",
      "offset": 4328.52,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "pointer right it copies the reference",
      "offset": 4331.96,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "and now the wte weight becomes orphaned",
      "offset": 4334.36,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "uh the old value of it and uh pytorch",
      "offset": 4337.88,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "will clean it up python will clean it up",
      "offset": 4340.84,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "and so we are only left with a single",
      "offset": 4343.32,
      "duration": 5.319
    },
    {
      "lang": "en",
      "text": "tensor and it's going to be used twice",
      "offset": 4346.28,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "in the forward pass and uh this is to my",
      "offset": 4348.639,
      "duration": 5.441
    },
    {
      "lang": "en",
      "text": "knowledge all that's required so we",
      "offset": 4351.96,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "should be able to use this and this",
      "offset": 4354.08,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "should probably train uh we're just",
      "offset": 4356.52,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "going to basically be using this exact",
      "offset": 4359.04,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "same sensor twice and",
      "offset": 4360.84,
      "duration": 5.399
    },
    {
      "lang": "en",
      "text": "um we weren't being careful with",
      "offset": 4364,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "tracking the likelihoods but uh",
      "offset": 4366.239,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "according to the paper and according to",
      "offset": 4368.56,
      "duration": 2.44
    },
    {
      "lang": "en",
      "text": "the results you'd actually expect",
      "offset": 4370,
      "duration": 3.239
    },
    {
      "lang": "en",
      "text": "slightly better results doing this and",
      "offset": 4371,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "in addition to that one other reason",
      "offset": 4373.239,
      "duration": 3.881
    },
    {
      "lang": "en",
      "text": "that this is very very nice for us is",
      "offset": 4374.84,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "that this is a ton of parameters right",
      "offset": 4377.12,
      "duration": 6.84
    },
    {
      "lang": "en",
      "text": "uh what is the size here it's 768 *",
      "offset": 4379.639,
      "duration": 7.681
    },
    {
      "lang": "en",
      "text": "50257 so This Is 40 million parameters",
      "offset": 4383.96,
      "duration": 5.8
    },
    {
      "lang": "en",
      "text": "and this is a 124 million parameter",
      "offset": 4387.32,
      "duration": 5.56
    },
    {
      "lang": "en",
      "text": "model so 40 divide 124 so this is like",
      "offset": 4389.76,
      "duration": 6.2
    },
    {
      "lang": "en",
      "text": "30% of the parameters are being saved",
      "offset": 4392.88,
      "duration": 5.88
    },
    {
      "lang": "en",
      "text": "using this weight time scheme and so",
      "offset": 4395.96,
      "duration": 4.04
    },
    {
      "lang": "en",
      "text": "this might be one of the reasons that",
      "offset": 4398.76,
      "duration": 2.919
    },
    {
      "lang": "en",
      "text": "this is working slightly better if",
      "offset": 4400,
      "duration": 2.92
    },
    {
      "lang": "en",
      "text": "you're not training the model long",
      "offset": 4401.679,
      "duration": 3.721
    },
    {
      "lang": "en",
      "text": "enough because of the weight tying uh",
      "offset": 4402.92,
      "duration": 3.56
    },
    {
      "lang": "en",
      "text": "you don't have to train as many",
      "offset": 4405.4,
      "duration": 2.52
    },
    {
      "lang": "en",
      "text": "parameters and so you become more",
      "offset": 4406.48,
      "duration": 4.199
    },
    {
      "lang": "en",
      "text": "efficient um in terms of the training",
      "offset": 4407.92,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "process uh because you have fewer",
      "offset": 4410.679,
      "duration": 3.56
    },
    {
      "lang": "en",
      "text": "parameters and you're putting in this",
      "offset": 4412.56,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "inductive bias that these two embeddings",
      "offset": 4414.239,
      "duration": 5.881
    },
    {
      "lang": "en",
      "text": "should share similarities between tokens",
      "offset": 4416.719,
      "duration": 5.681
    },
    {
      "lang": "en",
      "text": "so this is the way time scheme and we've",
      "offset": 4420.12,
      "duration": 4.36
    },
    {
      "lang": "en",
      "text": "saved a ton of parameters and we expect",
      "offset": 4422.4,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "our model to work slightly better",
      "offset": 4424.48,
      "duration": 3.48
    },
    {
      "lang": "en",
      "text": "because of the scheme okay next I would",
      "offset": 4425.84,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "like us to be a bit more careful with",
      "offset": 4427.96,
      "duration": 2.92
    },
    {
      "lang": "en",
      "text": "the initialization and to try to follow",
      "offset": 4429.199,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "the way gpt2 initialized their model now",
      "offset": 4430.88,
      "duration": 5.08
    },
    {
      "lang": "en",
      "text": "unfortunately the gpt2 paper and the",
      "offset": 4434,
      "duration": 4.76
    },
    {
      "lang": "en",
      "text": "gpt3 paper are not very explicit about",
      "offset": 4435.96,
      "duration": 4.199
    },
    {
      "lang": "en",
      "text": "initialization so we kind of have to",
      "offset": 4438.76,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "read between the lines uh and instead of",
      "offset": 4440.159,
      "duration": 4.761
    },
    {
      "lang": "en",
      "text": "going to the paper which is quite vague",
      "offset": 4442.679,
      "duration": 4.761
    },
    {
      "lang": "en",
      "text": "um there's a bit of information in the",
      "offset": 4444.92,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "code that open I released so when we go",
      "offset": 4447.44,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "to the model.py we see that when they",
      "offset": 4449.159,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "initialize their weights they are using",
      "offset": 4451.28,
      "duration": 4.6
    },
    {
      "lang": "en",
      "text": "the standard deviation of",
      "offset": 4453.96,
      "duration": 5.32
    },
    {
      "lang": "en",
      "text": "0.02 and that's how they they so this is",
      "offset": 4455.88,
      "duration": 5.839
    },
    {
      "lang": "en",
      "text": "a normal distribution for the weights",
      "offset": 4459.28,
      "duration": 3.959
    },
    {
      "lang": "en",
      "text": "and the standard deviation is",
      "offset": 4461.719,
      "duration": 4.201
    },
    {
      "lang": "en",
      "text": "0.02 for the bias they initialize that",
      "offset": 4463.239,
      "duration": 3.641
    },
    {
      "lang": "en",
      "text": "with",
      "offset": 4465.92,
      "duration": 4.68
    },
    {
      "lang": "en",
      "text": "zero and then when we scroll down",
      "offset": 4466.88,
      "duration": 7.04
    },
    {
      "lang": "en",
      "text": "here why is this not scrolling",
      "offset": 4470.6,
      "duration": 5.8
    },
    {
      "lang": "en",
      "text": "um the token embeddings are initialized",
      "offset": 4473.92,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "at",
      "offset": 4476.4,
      "duration": 3.88
    },
    {
      "lang": "en",
      "text": "0.02 and position embeddings at 0.01 for",
      "offset": 4477.04,
      "duration": 5.4
    },
    {
      "lang": "en",
      "text": "some reason so those are the",
      "offset": 4480.28,
      "duration": 3.959
    },
    {
      "lang": "en",
      "text": "initializations and we'd like to mirror",
      "offset": 4482.44,
      "duration": 2.64
    },
    {
      "lang": "en",
      "text": "that in",
      "offset": 4484.239,
      "duration": 4.041
    },
    {
      "lang": "en",
      "text": "gpt2 uh in our module here so here's a",
      "offset": 4485.08,
      "duration": 5.8
    },
    {
      "lang": "en",
      "text": "snippet of code that I sort of came up",
      "offset": 4488.28,
      "duration": 4.439
    },
    {
      "lang": "en",
      "text": "with very",
      "offset": 4490.88,
      "duration": 4.68
    },
    {
      "lang": "en",
      "text": "quickly so what's happening here is at",
      "offset": 4492.719,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "the end of our initializer for the GPT",
      "offset": 4495.56,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "module we're calling the apply function",
      "offset": 4497.719,
      "duration": 4.761
    },
    {
      "lang": "en",
      "text": "of NN module and that iterates all the",
      "offset": 4499.8,
      "duration": 6.04
    },
    {
      "lang": "en",
      "text": "sub modules of this module and uh",
      "offset": 4502.48,
      "duration": 6.36
    },
    {
      "lang": "en",
      "text": "applies in it weights function on them",
      "offset": 4505.84,
      "duration": 5.16
    },
    {
      "lang": "en",
      "text": "and so what's happening here is that",
      "offset": 4508.84,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "we're in we're iterating all the modules",
      "offset": 4511,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "here and if they are an nn. linear",
      "offset": 4513.159,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "module then we're going to make sure to",
      "offset": 4516.04,
      "duration": 3.88
    },
    {
      "lang": "en",
      "text": "initialize the weight using a normal",
      "offset": 4517.8,
      "duration": 3.72
    },
    {
      "lang": "en",
      "text": "with the standard deviation of",
      "offset": 4519.92,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "0.02 if there's a bias in this layer we",
      "offset": 4521.52,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "will make sure to initialize that to",
      "offset": 4524,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "zero note that zero initialization for",
      "offset": 4525.44,
      "duration": 4.52
    },
    {
      "lang": "en",
      "text": "the bias is not actually the pyto",
      "offset": 4528.32,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "default um by default the bias here is",
      "offset": 4529.96,
      "duration": 6.12
    },
    {
      "lang": "en",
      "text": "initialized with a uniform so uh that's",
      "offset": 4533.44,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "interesting so we make sure to use zero",
      "offset": 4536.08,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "and for the embedding we're just going",
      "offset": 4538.96,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "to use 0.02 and um keep it the same um",
      "offset": 4540,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "so we're not going to change it to 0.01",
      "offset": 4543.84,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "for positional because it's about the",
      "offset": 4545.6,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "same and then if you look through our",
      "offset": 4547.44,
      "duration": 3.96
    },
    {
      "lang": "en",
      "text": "model the only other layer that requires",
      "offset": 4549.44,
      "duration": 4.12
    },
    {
      "lang": "en",
      "text": "initialization and that has parameters",
      "offset": 4551.4,
      "duration": 4.36
    },
    {
      "lang": "en",
      "text": "is the layer norm and the fighter defer",
      "offset": 4553.56,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "initialization sets the scale in the",
      "offset": 4555.76,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "layer Norm to be one and the offset in",
      "offset": 4557.8,
      "duration": 3.879
    },
    {
      "lang": "en",
      "text": "the layer Norm to be zero so that's",
      "offset": 4560,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "exactly what we want and so we're just",
      "offset": 4561.679,
      "duration": 4.441
    },
    {
      "lang": "en",
      "text": "going to uh keep it that way and so this",
      "offset": 4563.36,
      "duration": 5.839
    },
    {
      "lang": "en",
      "text": "is the default initialization if we are",
      "offset": 4566.12,
      "duration": 8.24
    },
    {
      "lang": "en",
      "text": "following the um where is it the uh gpt2",
      "offset": 4569.199,
      "duration": 7.921
    },
    {
      "lang": "en",
      "text": "uh source code that they released I",
      "offset": 4574.36,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "would like to point out by the way that",
      "offset": 4577.12,
      "duration": 4.36
    },
    {
      "lang": "en",
      "text": "um typically the standard deviation here",
      "offset": 4579.36,
      "duration": 3.799
    },
    {
      "lang": "en",
      "text": "on this initialization if you follow the",
      "offset": 4581.48,
      "duration": 3.48
    },
    {
      "lang": "en",
      "text": "Javier initialization would be one of",
      "offset": 4583.159,
      "duration": 4.04
    },
    {
      "lang": "en",
      "text": "over the square root of the number of",
      "offset": 4584.96,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "features that are incoming into this",
      "offset": 4587.199,
      "duration": 4.841
    },
    {
      "lang": "en",
      "text": "layer but if you'll notice actually 0.02",
      "offset": 4588.8,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "is basically consistent with that",
      "offset": 4592.04,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "because the the model sizes inside these",
      "offset": 4594.08,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "Transformers for gpt2 are roughly 768",
      "offset": 4596.28,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "1600 Etc so 1 over the square root of",
      "offset": 4599.08,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "for example 768 gives us",
      "offset": 4601.96,
      "duration": 7.759
    },
    {
      "lang": "en",
      "text": "0.03 if we plug in 600 1,600 we get",
      "offset": 4604.52,
      "duration": 8.08
    },
    {
      "lang": "en",
      "text": "0.02 if we plug in three times that",
      "offset": 4609.719,
      "duration": 6.44
    },
    {
      "lang": "en",
      "text": "0.014 Etc so basically 0.02 is roughly",
      "offset": 4612.6,
      "duration": 7.039
    },
    {
      "lang": "en",
      "text": "in the vicinity of reasonable values for",
      "offset": 4616.159,
      "duration": 6.401
    },
    {
      "lang": "en",
      "text": "the for um for these initializations",
      "offset": 4619.639,
      "duration": 5.881
    },
    {
      "lang": "en",
      "text": "anyway so so it's not uh completely",
      "offset": 4622.56,
      "duration": 5.599
    },
    {
      "lang": "en",
      "text": "crazy to be hard coding 0.02 here uh but",
      "offset": 4625.52,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "you'd like typically uh some something",
      "offset": 4628.159,
      "duration": 5.56
    },
    {
      "lang": "en",
      "text": "that grows with the model size instead",
      "offset": 4631.44,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "but we will keep this because that is",
      "offset": 4633.719,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "the gpt2 initialization per their source",
      "offset": 4635.199,
      "duration": 4.281
    },
    {
      "lang": "en",
      "text": "code but we are not fully done yet on",
      "offset": 4637.239,
      "duration": 3.561
    },
    {
      "lang": "en",
      "text": "initialization because there's one more",
      "offset": 4639.48,
      "duration": 3.96
    },
    {
      "lang": "en",
      "text": "caveat here so",
      "offset": 4640.8,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "here a mod initialization which accounts",
      "offset": 4643.44,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "for the accumulation on the residual",
      "offset": 4646.08,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "path with model depth is used we scale",
      "offset": 4647.679,
      "duration": 4.04
    },
    {
      "lang": "en",
      "text": "the weight of residual layers of",
      "offset": 4650.4,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "initialization by factor of one over squ",
      "offset": 4651.719,
      "duration": 3.721
    },
    {
      "lang": "en",
      "text": "of n where n is the number of residual",
      "offset": 4653.52,
      "duration": 5.24
    },
    {
      "lang": "en",
      "text": "layers so this is what gbt2 paper says",
      "offset": 4655.44,
      "duration": 5.56
    },
    {
      "lang": "en",
      "text": "so we have not implemented that yet and",
      "offset": 4658.76,
      "duration": 4.28
    },
    {
      "lang": "en",
      "text": "uh we can do so now now I'd like to",
      "offset": 4661,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "actually kind of like motivate a little",
      "offset": 4663.04,
      "duration": 4.52
    },
    {
      "lang": "en",
      "text": "bit what they mean here I think um so",
      "offset": 4664.28,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "here's roughly what they",
      "offset": 4667.56,
      "duration": 4.84
    },
    {
      "lang": "en",
      "text": "mean if you start out with zeros in your",
      "offset": 4669.4,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "residual stream remember that each",
      "offset": 4672.4,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "residual stream is a is of this form",
      "offset": 4674.84,
      "duration": 5.359
    },
    {
      "lang": "en",
      "text": "where we continue adding to it X is X",
      "offset": 4677.92,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "plus something some kind of contribution",
      "offset": 4680.199,
      "duration": 5.081
    },
    {
      "lang": "en",
      "text": "so every single block of the residual uh",
      "offset": 4682.92,
      "duration": 6.2
    },
    {
      "lang": "en",
      "text": "Network contributes some uh amount and",
      "offset": 4685.28,
      "duration": 6.399
    },
    {
      "lang": "en",
      "text": "it gets added and so what ends up",
      "offset": 4689.12,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "happening is that the variance of the",
      "offset": 4691.679,
      "duration": 6.321
    },
    {
      "lang": "en",
      "text": "activations in the residual stream grows",
      "offset": 4695.04,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "so here's a small example if we start at",
      "offset": 4698,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "zero and then we for 100 times uh we",
      "offset": 4699.76,
      "duration": 5.399
    },
    {
      "lang": "en",
      "text": "have sort of this residual stream of of",
      "offset": 4703.36,
      "duration": 6.76
    },
    {
      "lang": "en",
      "text": "768 uh zeros and then 100 times we add",
      "offset": 4705.159,
      "duration": 7.921
    },
    {
      "lang": "en",
      "text": "um random which is a normal distribution",
      "offset": 4710.12,
      "duration": 6.28
    },
    {
      "lang": "en",
      "text": "zero mean one standard deviation if we",
      "offset": 4713.08,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "add to it then by the end the residual",
      "offset": 4716.4,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "stream has grown to have standard",
      "offset": 4717.96,
      "duration": 4.44
    },
    {
      "lang": "en",
      "text": "deviation of 10 and that's just because",
      "offset": 4719.36,
      "duration": 8.2
    },
    {
      "lang": "en",
      "text": "um we're always adding um these numbers",
      "offset": 4722.4,
      "duration": 8.16
    },
    {
      "lang": "en",
      "text": "and so this scaling factor that they use",
      "offset": 4727.56,
      "duration": 6.079
    },
    {
      "lang": "en",
      "text": "here exactly compensates for that growth",
      "offset": 4730.56,
      "duration": 7.079
    },
    {
      "lang": "en",
      "text": "so if we take n and we basically um",
      "offset": 4733.639,
      "duration": 5.361
    },
    {
      "lang": "en",
      "text": "scale down every one of these",
      "offset": 4737.639,
      "duration": 3.08
    },
    {
      "lang": "en",
      "text": "contributions into the residual stream",
      "offset": 4739,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "by one over theare Ro of n so 1 over",
      "offset": 4740.719,
      "duration": 6.681
    },
    {
      "lang": "en",
      "text": "theun of n is n to the 0.5",
      "offset": 4743.48,
      "duration": 7.8
    },
    {
      "lang": "en",
      "text": "right because n the5 is the square root",
      "offset": 4747.4,
      "duration": 6.759
    },
    {
      "lang": "en",
      "text": "and then one over the square root is n.5",
      "offset": 4751.28,
      "duration": 5.48
    },
    {
      "lang": "en",
      "text": "if we scale it in this way then we see",
      "offset": 4754.159,
      "duration": 5.841
    },
    {
      "lang": "en",
      "text": "that we actually get um",
      "offset": 4756.76,
      "duration": 4.959
    },
    {
      "lang": "en",
      "text": "one",
      "offset": 4760,
      "duration": 4.52
    },
    {
      "lang": "en",
      "text": "so this is a way to control the growth",
      "offset": 4761.719,
      "duration": 4.92
    },
    {
      "lang": "en",
      "text": "of of activations inside the residual",
      "offset": 4764.52,
      "duration": 4.679
    },
    {
      "lang": "en",
      "text": "stream in the forward pass and so we'd",
      "offset": 4766.639,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "like to initialize in the same way where",
      "offset": 4769.199,
      "duration": 4.601
    },
    {
      "lang": "en",
      "text": "these weights that are at the end of",
      "offset": 4771.6,
      "duration": 7.16
    },
    {
      "lang": "en",
      "text": "each block so this C uh layer uh the gbt",
      "offset": 4773.8,
      "duration": 6.72
    },
    {
      "lang": "en",
      "text": "paper proposes to scale down those",
      "offset": 4778.76,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "weights by one over the square root of",
      "offset": 4780.52,
      "duration": 3.119
    },
    {
      "lang": "en",
      "text": "the number of residual",
      "offset": 4782.28,
      "duration": 4.04
    },
    {
      "lang": "en",
      "text": "layers so one crude way to implement",
      "offset": 4783.639,
      "duration": 4.761
    },
    {
      "lang": "en",
      "text": "this is the following I don't know if",
      "offset": 4786.32,
      "duration": 4.44
    },
    {
      "lang": "en",
      "text": "this is uh pyro sanctioned but it works",
      "offset": 4788.4,
      "duration": 5.239
    },
    {
      "lang": "en",
      "text": "for me is we'll do in the",
      "offset": 4790.76,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "initialization see that s that do",
      "offset": 4793.639,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "special nanog",
      "offset": 4796.52,
      "duration": 7.88
    },
    {
      "lang": "en",
      "text": "GPT uh scale in it is one so we're",
      "offset": 4798.199,
      "duration": 8.601
    },
    {
      "lang": "en",
      "text": "setting um kind of like a flag for this",
      "offset": 4804.4,
      "duration": 4.44
    },
    {
      "lang": "en",
      "text": "module there must be a better way in py",
      "offset": 4806.8,
      "duration": 4.68
    },
    {
      "lang": "en",
      "text": "torch right but I don't",
      "offset": 4808.84,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "know okay so we're basically attaching",
      "offset": 4811.48,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "this flag and trying to make sure that",
      "offset": 4813.8,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "it doesn't conflict with anything",
      "offset": 4816.36,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "previously and then when we come down",
      "offset": 4817.96,
      "duration": 7.12
    },
    {
      "lang": "en",
      "text": "here this STD should be 0.02 by default",
      "offset": 4820.36,
      "duration": 6.72
    },
    {
      "lang": "en",
      "text": "but then if",
      "offset": 4825.08,
      "duration": 6.32
    },
    {
      "lang": "en",
      "text": "haat um module of this thing",
      "offset": 4827.08,
      "duration": 7.159
    },
    {
      "lang": "en",
      "text": "then STD *",
      "offset": 4831.4,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "equals",
      "offset": 4834.239,
      "duration": 5.321
    },
    {
      "lang": "en",
      "text": "um copal is not guessing correctly uh so",
      "offset": 4836.6,
      "duration": 4.44
    },
    {
      "lang": "en",
      "text": "we want one over the square root of the",
      "offset": 4839.56,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "number of layers so",
      "offset": 4841.04,
      "duration": 6.199
    },
    {
      "lang": "en",
      "text": "um the number of residual layers here is",
      "offset": 4844.12,
      "duration": 4.44
    },
    {
      "lang": "en",
      "text": "twice",
      "offset": 4847.239,
      "duration": 5.681
    },
    {
      "lang": "en",
      "text": "times Salt out config layers and then",
      "offset": 4848.56,
      "duration": 8.48
    },
    {
      "lang": "en",
      "text": "this times .5 so we want to scale down",
      "offset": 4852.92,
      "duration": 6.68
    },
    {
      "lang": "en",
      "text": "that standard deviation and this should",
      "offset": 4857.04,
      "duration": 5.96
    },
    {
      "lang": "en",
      "text": "be um correct and Implement that I",
      "offset": 4859.6,
      "duration": 4.92
    },
    {
      "lang": "en",
      "text": "should clarify by the way that the two",
      "offset": 4863,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "times number of layers comes from the",
      "offset": 4864.52,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "fact that every single one of our layers",
      "offset": 4866.12,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "in the Transformer actually has two",
      "offset": 4867.8,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "blocks that add to the ridal pathway",
      "offset": 4869.56,
      "duration": 3.96
    },
    {
      "lang": "en",
      "text": "right we have the attention and then the",
      "offset": 4871.88,
      "duration": 4.12
    },
    {
      "lang": "en",
      "text": "MLP so that's where the two times comes",
      "offset": 4873.52,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "from and the other thing to mention is",
      "offset": 4876,
      "duration": 5.08
    },
    {
      "lang": "en",
      "text": "that uh what's slightly awkward but",
      "offset": 4878.8,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "we're not going to fix it is that um",
      "offset": 4881.08,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "because we are weight sharing the wte",
      "offset": 4883.92,
      "duration": 5.48
    },
    {
      "lang": "en",
      "text": "and the LM head in this iteration of our",
      "offset": 4886.52,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "old subm modules we're going to actually",
      "offset": 4889.4,
      "duration": 3.96
    },
    {
      "lang": "en",
      "text": "come around to that tensor twice so",
      "offset": 4891.08,
      "duration": 3.8
    },
    {
      "lang": "en",
      "text": "we're going to first initialize it as an",
      "offset": 4893.36,
      "duration": 4.04
    },
    {
      "lang": "en",
      "text": "embedding with 0.02 and then we're going",
      "offset": 4894.88,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "to come back around it again in a linear",
      "offset": 4897.4,
      "duration": 5.16
    },
    {
      "lang": "en",
      "text": "and initialize it again using 0.02 and",
      "offset": 4899.6,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "it's going to be 0.02 because the LM",
      "offset": 4902.56,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "head is of course not not scaled so it's",
      "offset": 4904.4,
      "duration": 4.279
    },
    {
      "lang": "en",
      "text": "not going to come here it's just it's",
      "offset": 4906.32,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "going to be basically initialized twice",
      "offset": 4908.679,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "using the identical same initialization",
      "offset": 4910.32,
      "duration": 6.28
    },
    {
      "lang": "en",
      "text": "but that's okay and then scrolling over",
      "offset": 4912.84,
      "duration": 6.879
    },
    {
      "lang": "en",
      "text": "here I added uh some code here so that",
      "offset": 4916.6,
      "duration": 4.039
    },
    {
      "lang": "en",
      "text": "we have",
      "offset": 4919.719,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "reproducibility um to set the seeds and",
      "offset": 4920.639,
      "duration": 4.921
    },
    {
      "lang": "en",
      "text": "now we should be able to python train",
      "offset": 4923.639,
      "duration": 5.441
    },
    {
      "lang": "en",
      "text": "gpt2 pi and let this running and as far",
      "offset": 4925.56,
      "duration": 5.599
    },
    {
      "lang": "en",
      "text": "as I know this is the gpt2",
      "offset": 4929.08,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "initialization uh in the way we've",
      "offset": 4931.159,
      "duration": 5.161
    },
    {
      "lang": "en",
      "text": "implemented it right now so this",
      "offset": 4932.84,
      "duration": 6.92
    },
    {
      "lang": "en",
      "text": "looks uh reasonable to me okay so at",
      "offset": 4936.32,
      "duration": 5.399
    },
    {
      "lang": "en",
      "text": "this point we have the gpt2 model we",
      "offset": 4939.76,
      "duration": 3.32
    },
    {
      "lang": "en",
      "text": "have some confidence that it's correctly",
      "offset": 4941.719,
      "duration": 3.121
    },
    {
      "lang": "en",
      "text": "implemented we've initialized it",
      "offset": 4943.08,
      "duration": 3.32
    },
    {
      "lang": "en",
      "text": "properly and we have a data loader",
      "offset": 4944.84,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "that's iterating through data batches",
      "offset": 4946.4,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "and we can train so now comes the fun",
      "offset": 4947.96,
      "duration": 3.8
    },
    {
      "lang": "en",
      "text": "part I'd like us to speed up the",
      "offset": 4950.239,
      "duration": 3.48
    },
    {
      "lang": "en",
      "text": "training by a lot so we're getting our",
      "offset": 4951.76,
      "duration": 3.16
    },
    {
      "lang": "en",
      "text": "money's worth with respect to the",
      "offset": 4953.719,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "hardware that we are uh using here and",
      "offset": 4954.92,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "uh we're going to speed up the training",
      "offset": 4958.04,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "by quite a bit uh now you always want to",
      "offset": 4959.159,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "start with what Hardware do you have",
      "offset": 4962.52,
      "duration": 3.4
    },
    {
      "lang": "en",
      "text": "what does it offer and are you fully",
      "offset": 4964.12,
      "duration": 4.44
    },
    {
      "lang": "en",
      "text": "utilizing it so in my case if we go to",
      "offset": 4965.92,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "Nvidia",
      "offset": 4968.56,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "SMI we can see",
      "offset": 4969.92,
      "duration": 7.08
    },
    {
      "lang": "en",
      "text": "that I have eight gpus and each one of",
      "offset": 4973.12,
      "duration": 8.76
    },
    {
      "lang": "en",
      "text": "those gpus is an a100 sxm 80 gb so this",
      "offset": 4977,
      "duration": 6.84
    },
    {
      "lang": "en",
      "text": "is the GPU that I have available to me",
      "offset": 4981.88,
      "duration": 5.64
    },
    {
      "lang": "en",
      "text": "in this box now when I look when I use",
      "offset": 4983.84,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "um to spin up these kinds of Boxes by",
      "offset": 4987.52,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "the way my favorite place to go to is",
      "offset": 4989.28,
      "duration": 5.64
    },
    {
      "lang": "en",
      "text": "Lambda Labs um they do sponsor my",
      "offset": 4991.84,
      "duration": 6.04
    },
    {
      "lang": "en",
      "text": "development and that of my projects uh",
      "offset": 4994.92,
      "duration": 5.08
    },
    {
      "lang": "en",
      "text": "but I this is my favorite place to go",
      "offset": 4997.88,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "and this is where you can spin up one of",
      "offset": 5000,
      "duration": 3.32
    },
    {
      "lang": "en",
      "text": "these machines and you pay per hour and",
      "offset": 5001.48,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "it's very very simple",
      "offset": 5003.32,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "so I like to spin them up and then",
      "offset": 5005,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "connect vsod to it and that's how I",
      "offset": 5006.28,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "develop now when we look at the A1 100s",
      "offset": 5008.36,
      "duration": 6.64
    },
    {
      "lang": "en",
      "text": "that are available here a100 80 GB sxm",
      "offset": 5010.84,
      "duration": 8.16
    },
    {
      "lang": "en",
      "text": "is the um GPU that I have here and we",
      "offset": 5015,
      "duration": 6.92
    },
    {
      "lang": "en",
      "text": "have a bunch of numbers here for um how",
      "offset": 5019,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "many calculations you can expect out of",
      "offset": 5021.92,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "this GPU so when I come over here",
      "offset": 5023.32,
      "duration": 7.08
    },
    {
      "lang": "en",
      "text": "and I break in right after here so",
      "offset": 5026.48,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "python",
      "offset": 5030.4,
      "duration": 3.239
    },
    {
      "lang": "en",
      "text": "trity so I'm breaking in right after we",
      "offset": 5031.44,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "calculate the loit and",
      "offset": 5033.639,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "laws and the interesting thing I'd like",
      "offset": 5035.679,
      "duration": 7.04
    },
    {
      "lang": "en",
      "text": "you to note is when I do lit. dtype this",
      "offset": 5037.8,
      "duration": 8.68
    },
    {
      "lang": "en",
      "text": "prints a torch. FL 32 so by default iny",
      "offset": 5042.719,
      "duration": 6.241
    },
    {
      "lang": "en",
      "text": "torch when you create tensors um and",
      "offset": 5046.48,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "this is the case for all the activations",
      "offset": 5048.96,
      "duration": 2.759
    },
    {
      "lang": "en",
      "text": "and for the parameters of the network",
      "offset": 5050.4,
      "duration": 3.319
    },
    {
      "lang": "en",
      "text": "and so on by default everything is in",
      "offset": 5051.719,
      "duration": 5.641
    },
    {
      "lang": "en",
      "text": "float 32 that means that every single",
      "offset": 5053.719,
      "duration": 6.761
    },
    {
      "lang": "en",
      "text": "number activation or weight and so on is",
      "offset": 5057.36,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "using a float representation that has 32",
      "offset": 5060.48,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "bits and uh that's actually quite a bit",
      "offset": 5063.28,
      "duration": 4.359
    },
    {
      "lang": "en",
      "text": "of memory and it turns out empirically",
      "offset": 5066.08,
      "duration": 2.639
    },
    {
      "lang": "en",
      "text": "that for deep learning as a",
      "offset": 5067.639,
      "duration": 2.921
    },
    {
      "lang": "en",
      "text": "computational workload this is way too",
      "offset": 5068.719,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "much and deep learning and the training",
      "offset": 5070.56,
      "duration": 4.04
    },
    {
      "lang": "en",
      "text": "of these networks can tolerate",
      "offset": 5072.88,
      "duration": 4.52
    },
    {
      "lang": "en",
      "text": "significantly lower precisions um not",
      "offset": 5074.6,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "all computational workflows can tolerate",
      "offset": 5077.4,
      "duration": 6.04
    },
    {
      "lang": "en",
      "text": "small Precision so for example um if we",
      "offset": 5079.56,
      "duration": 6.36
    },
    {
      "lang": "en",
      "text": "go back to to the data sheet you'll see",
      "offset": 5083.44,
      "duration": 4.84
    },
    {
      "lang": "en",
      "text": "that actually these gpus support up to",
      "offset": 5085.92,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "fp64 and this is quite useful I",
      "offset": 5088.28,
      "duration": 4.359
    },
    {
      "lang": "en",
      "text": "understand for a lot of um scientific",
      "offset": 5090.719,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "Computing applications and there really",
      "offset": 5092.639,
      "duration": 4.201
    },
    {
      "lang": "en",
      "text": "need this uh but we don't need that much",
      "offset": 5094.719,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "Precision for deep learning training So",
      "offset": 5096.84,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "currently we are here",
      "offset": 5099.199,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "fp32 and with this code as it is right",
      "offset": 5101.159,
      "duration": 6.841
    },
    {
      "lang": "en",
      "text": "now we expect to get at at most 19.5",
      "offset": 5104.199,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "Tera flops of performance that means",
      "offset": 5108,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "we're doing 19.5 trillion operations",
      "offset": 5110.679,
      "duration": 4.601
    },
    {
      "lang": "en",
      "text": "floating Point operations so this is",
      "offset": 5113.36,
      "duration": 7.04
    },
    {
      "lang": "en",
      "text": "floating Point multiply add most um most",
      "offset": 5115.28,
      "duration": 7.8
    },
    {
      "lang": "en",
      "text": "likely and so these are the floating",
      "offset": 5120.4,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "Point operations",
      "offset": 5123.08,
      "duration": 4.599
    },
    {
      "lang": "en",
      "text": "uh now notice that if we are willing to",
      "offset": 5125.44,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "go down in Precision so tf32 is a lower",
      "offset": 5127.679,
      "duration": 5.241
    },
    {
      "lang": "en",
      "text": "Precision format we're going to see in a",
      "offset": 5131.6,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "second you can actually get an 8X",
      "offset": 5132.92,
      "duration": 3.719
    },
    {
      "lang": "en",
      "text": "Improvement here and if you're willing",
      "offset": 5134.96,
      "duration": 4.679
    },
    {
      "lang": "en",
      "text": "to go down to float 16 or B float 16 you",
      "offset": 5136.639,
      "duration": 5.841
    },
    {
      "lang": "en",
      "text": "can actually get time 16x performance",
      "offset": 5139.639,
      "duration": 6.321
    },
    {
      "lang": "en",
      "text": "all the way to 312 Tera flops you see",
      "offset": 5142.48,
      "duration": 5.4
    },
    {
      "lang": "en",
      "text": "here that Nvidia likes to site numbers",
      "offset": 5145.96,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "that have an asterisk here this asterisk",
      "offset": 5147.88,
      "duration": 4.839
    },
    {
      "lang": "en",
      "text": "uh says with sparsity uh but we are not",
      "offset": 5150.44,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "going to be using sparsity in R code and",
      "offset": 5152.719,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "I don't know that this is very widely",
      "offset": 5155.159,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "used in the industry right now so most",
      "offset": 5156.96,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "people look at this number here uh",
      "offset": 5158.92,
      "duration": 4.52
    },
    {
      "lang": "en",
      "text": "without sparcity and you'll notice that",
      "offset": 5161.04,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "we could have got even more here but",
      "offset": 5163.44,
      "duration": 5.32
    },
    {
      "lang": "en",
      "text": "this is int 8 and int 8 is used for",
      "offset": 5165.52,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "inference not for training uh because",
      "offset": 5168.76,
      "duration": 8.32
    },
    {
      "lang": "en",
      "text": "int 8 has a um it basically has um",
      "offset": 5171.6,
      "duration": 6.8
    },
    {
      "lang": "en",
      "text": "uniform",
      "offset": 5177.08,
      "duration": 4.84
    },
    {
      "lang": "en",
      "text": "spacing um and uh we actually require a",
      "offset": 5178.4,
      "duration": 6.239
    },
    {
      "lang": "en",
      "text": "float so that we get a better match to",
      "offset": 5181.92,
      "duration": 6.12
    },
    {
      "lang": "en",
      "text": "the uh normal distributions that occur",
      "offset": 5184.639,
      "duration": 5.241
    },
    {
      "lang": "en",
      "text": "during training of neural networks where",
      "offset": 5188.04,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "both activations and weights are",
      "offset": 5189.88,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "distributed as a normal distribution and",
      "offset": 5191.48,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "so uh floating points are really",
      "offset": 5193.88,
      "duration": 4.52
    },
    {
      "lang": "en",
      "text": "important to to match that uh",
      "offset": 5195.56,
      "duration": 4.84
    },
    {
      "lang": "en",
      "text": "representation so we're not typically",
      "offset": 5198.4,
      "duration": 4.36
    },
    {
      "lang": "en",
      "text": "using int 8 uh for training but we are",
      "offset": 5200.4,
      "duration": 5.16
    },
    {
      "lang": "en",
      "text": "using it for inference and if we bring",
      "offset": 5202.76,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "down the Precision we can get a lot more",
      "offset": 5205.56,
      "duration": 4.119
    },
    {
      "lang": "en",
      "text": "Terra flops out of the tensor course",
      "offset": 5207.76,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "available in the gpus we'll talk about",
      "offset": 5209.679,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "that in a second but in addition to that",
      "offset": 5211.44,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "if all of these numbers have fewer bits",
      "offset": 5213.6,
      "duration": 4.92
    },
    {
      "lang": "en",
      "text": "of representation it's going to be much",
      "offset": 5216.56,
      "duration": 4.04
    },
    {
      "lang": "en",
      "text": "easier to move them around and that's",
      "offset": 5218.52,
      "duration": 3.639
    },
    {
      "lang": "en",
      "text": "where we start to get into the memory",
      "offset": 5220.6,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "bandwidth and the memory of the model so",
      "offset": 5222.159,
      "duration": 4.52
    },
    {
      "lang": "en",
      "text": "not only do we have a finite capacity of",
      "offset": 5224.76,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "the number of bits that our GPU can",
      "offset": 5226.679,
      "duration": 4.601
    },
    {
      "lang": "en",
      "text": "store but in addition to that there's a",
      "offset": 5228.76,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "speed with which you can access this",
      "offset": 5231.28,
      "duration": 4.959
    },
    {
      "lang": "en",
      "text": "memory um and you have a certain memory",
      "offset": 5233.239,
      "duration": 5.801
    },
    {
      "lang": "en",
      "text": "bandwidth it's a very precious resource",
      "offset": 5236.239,
      "duration": 5.041
    },
    {
      "lang": "en",
      "text": "and in fact many of the deep learning uh",
      "offset": 5239.04,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "work workloads for training are memory",
      "offset": 5241.28,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "bound and what that means is actually",
      "offset": 5243.28,
      "duration": 4.359
    },
    {
      "lang": "en",
      "text": "that the tensor cores that do all these",
      "offset": 5245.199,
      "duration": 4.52
    },
    {
      "lang": "en",
      "text": "extremely fast multiplications most of",
      "offset": 5247.639,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "the time they're waiting around they're",
      "offset": 5249.719,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "idle um because we can't feed them with",
      "offset": 5251.239,
      "duration": 5.761
    },
    {
      "lang": "en",
      "text": "data fast enough we can't load the data",
      "offset": 5254.52,
      "duration": 4.199
    },
    {
      "lang": "en",
      "text": "fast enough from memory so typical",
      "offset": 5257,
      "duration": 3.639
    },
    {
      "lang": "en",
      "text": "utilizations of your Hardware if you're",
      "offset": 5258.719,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "getting 60% uh utilization you're",
      "offset": 5260.639,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "actually doing extremely well um so half",
      "offset": 5263.119,
      "duration": 5.401
    },
    {
      "lang": "en",
      "text": "of the time in a well-tuned application",
      "offset": 5266.159,
      "duration": 4.04
    },
    {
      "lang": "en",
      "text": "your tensor cores are not doing",
      "offset": 5268.52,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "multiplies because the data is not",
      "offset": 5270.199,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "available so the memory bandwidth here",
      "offset": 5271.56,
      "duration": 4.2
    },
    {
      "lang": "en",
      "text": "is extremely important as well and if we",
      "offset": 5273.56,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "come down in the Precision for all the",
      "offset": 5275.76,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "floats all the numbers weights and",
      "offset": 5278.119,
      "duration": 4.841
    },
    {
      "lang": "en",
      "text": "activations suddenly require less memory",
      "offset": 5280.239,
      "duration": 4.92
    },
    {
      "lang": "en",
      "text": "so we can store more and we can access",
      "offset": 5282.96,
      "duration": 4.52
    },
    {
      "lang": "en",
      "text": "it faster so everything speeds up and",
      "offset": 5285.159,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "it's amazing and now let's reap the",
      "offset": 5287.48,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "benefits of it um and let's first look",
      "offset": 5289.88,
      "duration": 4.52
    },
    {
      "lang": "en",
      "text": "at the tensor float 32",
      "offset": 5292.199,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "format okay so first of all what are",
      "offset": 5294.4,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "tensor cores well tensor course tensor",
      "offset": 5296.44,
      "duration": 5.799
    },
    {
      "lang": "en",
      "text": "core is just an instruction in the a100",
      "offset": 5299.4,
      "duration": 5.759
    },
    {
      "lang": "en",
      "text": "architecture right so so what it does is",
      "offset": 5302.239,
      "duration": 5.48
    },
    {
      "lang": "en",
      "text": "it does basically a little 4x4 Matrix",
      "offset": 5305.159,
      "duration": 5.601
    },
    {
      "lang": "en",
      "text": "multiply so uh this is just matrix",
      "offset": 5307.719,
      "duration": 7.561
    },
    {
      "lang": "en",
      "text": "multiplication here of 4x4 matrices and",
      "offset": 5310.76,
      "duration": 7.24
    },
    {
      "lang": "en",
      "text": "there are multiple configurations as to",
      "offset": 5315.28,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "what Precision any of these matrices are",
      "offset": 5318,
      "duration": 4.679
    },
    {
      "lang": "en",
      "text": "it in what Precision the internal",
      "offset": 5320.8,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "accumulate happens and then what is the",
      "offset": 5322.679,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "output Precision input precisions Etc so",
      "offset": 5325.119,
      "duration": 3.801
    },
    {
      "lang": "en",
      "text": "there's a few switches but it's",
      "offset": 5327.239,
      "duration": 3.801
    },
    {
      "lang": "en",
      "text": "basically a 4x4 multiply and then",
      "offset": 5328.92,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "anytime we have any operations that",
      "offset": 5331.04,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "require Magic multiplication uh they get",
      "offset": 5333.4,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "broken up into these into this",
      "offset": 5335.84,
      "duration": 5.08
    },
    {
      "lang": "en",
      "text": "instruction of little 4x4 multiply and",
      "offset": 5338.4,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "so everything gets broken up into this",
      "offset": 5340.92,
      "duration": 3.16
    },
    {
      "lang": "en",
      "text": "instruction because it's the fastest way",
      "offset": 5342.239,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "to multiply matrices and it turns out",
      "offset": 5344.08,
      "duration": 4.039
    },
    {
      "lang": "en",
      "text": "that most of the computational work that",
      "offset": 5346.4,
      "duration": 4.04
    },
    {
      "lang": "en",
      "text": "we're doing up above uh all of it really",
      "offset": 5348.119,
      "duration": 4.681
    },
    {
      "lang": "en",
      "text": "is matrix multiplication most of the",
      "offset": 5350.44,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "work computationally happens in the",
      "offset": 5352.8,
      "duration": 7.28
    },
    {
      "lang": "en",
      "text": "linear layers um linear linear Etc",
      "offset": 5354.28,
      "duration": 7.399
    },
    {
      "lang": "en",
      "text": "there's a few things sandwiched in",
      "offset": 5360.08,
      "duration": 3.24
    },
    {
      "lang": "en",
      "text": "between so there's some additions in",
      "offset": 5361.679,
      "duration": 4.121
    },
    {
      "lang": "en",
      "text": "residuals there's some G nonlinearities",
      "offset": 5363.32,
      "duration": 4.919
    },
    {
      "lang": "en",
      "text": "there's some layer Norms Etc but if you",
      "offset": 5365.8,
      "duration": 4.52
    },
    {
      "lang": "en",
      "text": "just time them you'll see that these are",
      "offset": 5368.239,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "nothing like basically the in",
      "offset": 5370.32,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "Transformer is just a bunch of Matrix",
      "offset": 5372.32,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "multiplications really um and especially",
      "offset": 5374.719,
      "duration": 4.92
    },
    {
      "lang": "en",
      "text": "at this small scale 124 million",
      "offset": 5377.76,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "parameter model actually the biggest",
      "offset": 5379.639,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "matrix multiplication by far is the",
      "offset": 5382.4,
      "duration": 4.279
    },
    {
      "lang": "en",
      "text": "classifier layer at the top that is a",
      "offset": 5384.28,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "massive Matrix multiply of going from",
      "offset": 5386.679,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "768 to",
      "offset": 5389,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "50257 and that Matrix multiply dominates",
      "offset": 5390.76,
      "duration": 4.52
    },
    {
      "lang": "en",
      "text": "anything else that happens in that",
      "offset": 5393.639,
      "duration": 5.121
    },
    {
      "lang": "en",
      "text": "Network roughly speaking so it's Matrix",
      "offset": 5395.28,
      "duration": 5.56
    },
    {
      "lang": "en",
      "text": "multiplies that become a lot faster",
      "offset": 5398.76,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "which are hidden inside our linear",
      "offset": 5400.84,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "layers and they're accelerated through",
      "offset": 5402.679,
      "duration": 4.921
    },
    {
      "lang": "en",
      "text": "tensor course now the best reference I",
      "offset": 5405.08,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "would say for tensor course is basically",
      "offset": 5407.6,
      "duration": 5.4
    },
    {
      "lang": "en",
      "text": "just go to the um a 100 architecture",
      "offset": 5409.239,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "white paper and then it's pretty",
      "offset": 5413,
      "duration": 5.08
    },
    {
      "lang": "en",
      "text": "detailed and but I think people it's",
      "offset": 5415.239,
      "duration": 5.081
    },
    {
      "lang": "en",
      "text": "like relatively readable mostly if you",
      "offset": 5418.08,
      "duration": 5.32
    },
    {
      "lang": "en",
      "text": "half understand what's happening um so",
      "offset": 5420.32,
      "duration": 5.799
    },
    {
      "lang": "en",
      "text": "figure 9 tensor float",
      "offset": 5423.4,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "32 so this is the explanation basically",
      "offset": 5426.119,
      "duration": 5.361
    },
    {
      "lang": "en",
      "text": "for tf32 and what happens here and you",
      "offset": 5428.36,
      "duration": 4.279
    },
    {
      "lang": "en",
      "text": "see that there's many configuration",
      "offset": 5431.48,
      "duration": 3.8
    },
    {
      "lang": "en",
      "text": "options here available so the input",
      "offset": 5432.639,
      "duration": 5.161
    },
    {
      "lang": "en",
      "text": "operands and what precisions are they in",
      "offset": 5435.28,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "the accumulator and um what um basically",
      "offset": 5437.8,
      "duration": 6.76
    },
    {
      "lang": "en",
      "text": "the um the internal representation",
      "offset": 5441.76,
      "duration": 4.84
    },
    {
      "lang": "en",
      "text": "within the instruction when you do the",
      "offset": 5444.56,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "accumulate of this matrix",
      "offset": 5446.6,
      "duration": 4.599
    },
    {
      "lang": "en",
      "text": "multiplication so the intermediate plus",
      "offset": 5448.639,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "equals um of the intermediate little",
      "offset": 5451.199,
      "duration": 4.601
    },
    {
      "lang": "en",
      "text": "vector multiplies here that all happens",
      "offset": 5453.639,
      "duration": 3.721
    },
    {
      "lang": "en",
      "text": "in",
      "offset": 5455.8,
      "duration": 4.359
    },
    {
      "lang": "en",
      "text": "fp32 and then uh this is an aex",
      "offset": 5457.36,
      "duration": 4.6
    },
    {
      "lang": "en",
      "text": "improvement as I mentioned to the Ops",
      "offset": 5460.159,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "that we get so tf32 specifically we're",
      "offset": 5461.96,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "looking at this row here and the way",
      "offset": 5464.48,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "this works",
      "offset": 5466.36,
      "duration": 3.879
    },
    {
      "lang": "en",
      "text": "is",
      "offset": 5467.44,
      "duration": 7.239
    },
    {
      "lang": "en",
      "text": "um normally fp32 has 32 bits",
      "offset": 5470.239,
      "duration": 8
    },
    {
      "lang": "en",
      "text": "tf32 is the exact same bits we have one",
      "offset": 5474.679,
      "duration": 6.96
    },
    {
      "lang": "en",
      "text": "sign bit we have eight exponent bits",
      "offset": 5478.239,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "except the mantisa bits get cropped in",
      "offset": 5481.639,
      "duration": 5.48
    },
    {
      "lang": "en",
      "text": "the float and so basically um we end up",
      "offset": 5484.239,
      "duration": 6.641
    },
    {
      "lang": "en",
      "text": "with just 19 bits instead of 32 bits",
      "offset": 5487.119,
      "duration": 6.56
    },
    {
      "lang": "en",
      "text": "because the last 133 bits get truncated",
      "offset": 5490.88,
      "duration": 5.56
    },
    {
      "lang": "en",
      "text": "they get dropped um and all this is",
      "offset": 5493.679,
      "duration": 5.241
    },
    {
      "lang": "en",
      "text": "internal to the instruction so none of",
      "offset": 5496.44,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "it is visible to anything in our pytorch",
      "offset": 5498.92,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "uh none of our pytorch code will change",
      "offset": 5501.44,
      "duration": 4.199
    },
    {
      "lang": "en",
      "text": "all of the numbers will look identical",
      "offset": 5503.639,
      "duration": 3.641
    },
    {
      "lang": "en",
      "text": "it's just that when you call the tensor",
      "offset": 5505.639,
      "duration": 4.921
    },
    {
      "lang": "en",
      "text": "core um instruction internally in the",
      "offset": 5507.28,
      "duration": 7
    },
    {
      "lang": "en",
      "text": "hardware it will crop out these 13 bits",
      "offset": 5510.56,
      "duration": 6.8
    },
    {
      "lang": "en",
      "text": "and that allows it to uh calculate this",
      "offset": 5514.28,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "little Matrix multiply significantly",
      "offset": 5517.36,
      "duration": 4.759
    },
    {
      "lang": "en",
      "text": "faster 8X faster now of course this",
      "offset": 5519.08,
      "duration": 5.88
    },
    {
      "lang": "en",
      "text": "speed up comes at a cost and the cost is",
      "offset": 5522.119,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "that we are reducing the Precision our",
      "offset": 5524.96,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "accumulate is still an fp32 our output",
      "offset": 5527.32,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "is fp32 our inputs are fp32 but",
      "offset": 5529.44,
      "duration": 5.48
    },
    {
      "lang": "en",
      "text": "internally things get truncated in the",
      "offset": 5532.6,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "operand to perform the operation faster",
      "offset": 5534.92,
      "duration": 4.12
    },
    {
      "lang": "en",
      "text": "and so our results are starting to be a",
      "offset": 5537.6,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "bit more approximate but empirically",
      "offset": 5539.04,
      "duration": 3.4
    },
    {
      "lang": "en",
      "text": "when you actually train with this you",
      "offset": 5541.04,
      "duration": 3.32
    },
    {
      "lang": "en",
      "text": "basically can't tell the difference",
      "offset": 5542.44,
      "duration": 4.52
    },
    {
      "lang": "en",
      "text": "so the reason I like tf32 is because if",
      "offset": 5544.36,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "you can tolerate a little bit of a",
      "offset": 5546.96,
      "duration": 5.279
    },
    {
      "lang": "en",
      "text": "Precision fudge um then this is free",
      "offset": 5548.44,
      "duration": 6.12
    },
    {
      "lang": "en",
      "text": "like none of your codes sees this it's",
      "offset": 5552.239,
      "duration": 4.521
    },
    {
      "lang": "en",
      "text": "fully internal to the operation and the",
      "offset": 5554.56,
      "duration": 4.84
    },
    {
      "lang": "en",
      "text": "operation to you just go 8X faster and",
      "offset": 5556.76,
      "duration": 5.24
    },
    {
      "lang": "en",
      "text": "it's a bit more approximate and so it's",
      "offset": 5559.4,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "a pretty sweet spot I would say in",
      "offset": 5562,
      "duration": 4.04
    },
    {
      "lang": "en",
      "text": "optimization and uh let's see what that",
      "offset": 5563.719,
      "duration": 4.361
    },
    {
      "lang": "en",
      "text": "looks like first so I've set up our Cod",
      "offset": 5566.04,
      "duration": 5.56
    },
    {
      "lang": "en",
      "text": "to just time the uh iterations so import",
      "offset": 5568.08,
      "duration": 6.119
    },
    {
      "lang": "en",
      "text": "time I changed the hyper parameters so",
      "offset": 5571.6,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "that we have something a bit more that",
      "offset": 5574.199,
      "duration": 3.281
    },
    {
      "lang": "en",
      "text": "reflects uh kind of workload that we",
      "offset": 5575.52,
      "duration": 4.04
    },
    {
      "lang": "en",
      "text": "want to run uh because we want to do a",
      "offset": 5577.48,
      "duration": 3.88
    },
    {
      "lang": "en",
      "text": "fairly large run at the end of this so",
      "offset": 5579.56,
      "duration": 4.599
    },
    {
      "lang": "en",
      "text": "let's use batch size 16 and let's now",
      "offset": 5581.36,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "use the actual gpt2 um maximum sequence",
      "offset": 5584.159,
      "duration": 4.52
    },
    {
      "lang": "en",
      "text": "length of 10,24",
      "offset": 5587.04,
      "duration": 4.44
    },
    {
      "lang": "en",
      "text": "tokens uh so this is the",
      "offset": 5588.679,
      "duration": 6.841
    },
    {
      "lang": "en",
      "text": "configuration and then for 50 iterations",
      "offset": 5591.48,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "I'm just doing something very lazy here",
      "offset": 5595.52,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "I'm doing time. time to get the current",
      "offset": 5597.48,
      "duration": 4.6
    },
    {
      "lang": "en",
      "text": "time and then this is the optimization",
      "offset": 5599.76,
      "duration": 4.76
    },
    {
      "lang": "en",
      "text": "Loop and now I want to time how long",
      "offset": 5602.08,
      "duration": 6.559
    },
    {
      "lang": "en",
      "text": "this takes now one issue with working",
      "offset": 5604.52,
      "duration": 7.48
    },
    {
      "lang": "en",
      "text": "with gpus is that as your",
      "offset": 5608.639,
      "duration": 6.841
    },
    {
      "lang": "en",
      "text": "CPU um when your CPU runs it's just",
      "offset": 5612,
      "duration": 6.159
    },
    {
      "lang": "en",
      "text": "scheduling work on GPU it's ordering",
      "offset": 5615.48,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "some work right and so it send a request",
      "offset": 5618.159,
      "duration": 5.161
    },
    {
      "lang": "en",
      "text": "and then it continues running and so we",
      "offset": 5620.6,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "can actually it can happen sometimes",
      "offset": 5623.32,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "that we sort of um speed through this",
      "offset": 5624.52,
      "duration": 5.88
    },
    {
      "lang": "en",
      "text": "and we queue up a lot of kernels to run",
      "offset": 5628.199,
      "duration": 4.52
    },
    {
      "lang": "en",
      "text": "on the GPU and then the CPU sort of like",
      "offset": 5630.4,
      "duration": 4.2
    },
    {
      "lang": "en",
      "text": "gets here and takes time at time but",
      "offset": 5632.719,
      "duration": 3.561
    },
    {
      "lang": "en",
      "text": "actually the GPU is still running",
      "offset": 5634.6,
      "duration": 3.24
    },
    {
      "lang": "en",
      "text": "because it takes it time to actually",
      "offset": 5636.28,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "work through the work that was scheduled",
      "offset": 5637.84,
      "duration": 5.24
    },
    {
      "lang": "en",
      "text": "to run and so you're just building up a",
      "offset": 5640.199,
      "duration": 5.361
    },
    {
      "lang": "en",
      "text": "queue for the GPU and so actually if you",
      "offset": 5643.08,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "need to you want to wait toat data",
      "offset": 5645.56,
      "duration": 4.52
    },
    {
      "lang": "en",
      "text": "synchronize and this will wait for the",
      "offset": 5647.96,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "GPU to finish all the work that was",
      "offset": 5650.08,
      "duration": 5.48
    },
    {
      "lang": "en",
      "text": "scheduled to run up above here and then",
      "offset": 5652.28,
      "duration": 5.24
    },
    {
      "lang": "en",
      "text": "we can actually take the time so",
      "offset": 5655.56,
      "duration": 3.48
    },
    {
      "lang": "en",
      "text": "basically we're waiting for the GPU to",
      "offset": 5657.52,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "stop this iteration take time and then",
      "offset": 5659.04,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "we're going to just print it so",
      "offset": 5662.239,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "so here I'm going to run the training",
      "offset": 5664.48,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "Loop and here on the right I'm watching",
      "offset": 5666.8,
      "duration": 6.28
    },
    {
      "lang": "en",
      "text": "Nvidia SMI so we start off at zero um",
      "offset": 5669.52,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "we're not using the GPU and then by",
      "offset": 5673.08,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "default P will use gpu0 so we see that",
      "offset": 5675.04,
      "duration": 5.24
    },
    {
      "lang": "en",
      "text": "it gets filled up and we're using 35 GB",
      "offset": 5677.239,
      "duration": 5.321
    },
    {
      "lang": "en",
      "text": "out of 80 gabt",
      "offset": 5680.28,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "available and then here on the left we",
      "offset": 5682.56,
      "duration": 5.24
    },
    {
      "lang": "en",
      "text": "see that because we've cranked up the",
      "offset": 5685,
      "duration": 3.96
    },
    {
      "lang": "en",
      "text": "batch",
      "offset": 5687.8,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "size now it's only 20 batches to do a",
      "offset": 5688.96,
      "duration": 5.159
    },
    {
      "lang": "en",
      "text": "single Epoch on our tiny Shakespeare",
      "offset": 5691.48,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "and we see that we're seeing roughly a",
      "offset": 5694.119,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "th000 milliseconds per iteration here",
      "offset": 5695.96,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "right",
      "offset": 5698.199,
      "duration": 4.681
    },
    {
      "lang": "en",
      "text": "so the first iteration sometimes is",
      "offset": 5700.199,
      "duration": 4.52
    },
    {
      "lang": "en",
      "text": "slower and that's because pytorch might",
      "offset": 5702.88,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "be doing a lot of initializations here",
      "offset": 5704.719,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "on the very first iteration and so it's",
      "offset": 5706.08,
      "duration": 3.72
    },
    {
      "lang": "en",
      "text": "probably initializing all these uh",
      "offset": 5708.32,
      "duration": 3.16
    },
    {
      "lang": "en",
      "text": "tensors and buffers to hold all the",
      "offset": 5709.8,
      "duration": 3.879
    },
    {
      "lang": "en",
      "text": "gradients and I'm not 100% sure all the",
      "offset": 5711.48,
      "duration": 4.6
    },
    {
      "lang": "en",
      "text": "work that happens here but uh this could",
      "offset": 5713.679,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "be a slower iteration when you're timing",
      "offset": 5716.08,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "your logic you always want to be careful",
      "offset": 5718.08,
      "duration": 3.24
    },
    {
      "lang": "en",
      "text": "with that but basically we're seeing a",
      "offset": 5719.52,
      "duration": 4.84
    },
    {
      "lang": "en",
      "text": "th000 milliseconds per iteration",
      "offset": 5721.32,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "um and so this will run for roughly 50",
      "offset": 5724.36,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "seconds as we have it right now so",
      "offset": 5726.76,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "that's our Baseline in flo 32 one more",
      "offset": 5729.4,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "thing I wanted to mention is that if",
      "offset": 5732.6,
      "duration": 4.039
    },
    {
      "lang": "en",
      "text": "this doesn't fit into your GPU and",
      "offset": 5735.08,
      "duration": 3.32
    },
    {
      "lang": "en",
      "text": "you're getting out of memory errors then",
      "offset": 5736.639,
      "duration": 3.48
    },
    {
      "lang": "en",
      "text": "start decreasing your batch size until",
      "offset": 5738.4,
      "duration": 4.36
    },
    {
      "lang": "en",
      "text": "things fit so instead of 16 try eight or",
      "offset": 5740.119,
      "duration": 5.881
    },
    {
      "lang": "en",
      "text": "four or whatever you need to fit um the",
      "offset": 5742.76,
      "duration": 5.439
    },
    {
      "lang": "en",
      "text": "batch into your GPU and if you have a",
      "offset": 5746,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "bigger GPU you can actually potentially",
      "offset": 5748.199,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "get away with 32 and so on uh by default",
      "offset": 5749.6,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "you want to basically max out has Max",
      "offset": 5752.6,
      "duration": 3.96
    },
    {
      "lang": "en",
      "text": "Max out the batch size that fits on your",
      "offset": 5754.4,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "GPU and you want to keep it nice numbers",
      "offset": 5756.56,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "so use numbers that have lots of powers",
      "offset": 5759.52,
      "duration": 5.8
    },
    {
      "lang": "en",
      "text": "of two in them so 16 is a good number 8",
      "offset": 5761.84,
      "duration": 7.319
    },
    {
      "lang": "en",
      "text": "24 32 48 These are nice numbers but",
      "offset": 5765.32,
      "duration": 5.879
    },
    {
      "lang": "en",
      "text": "don't use something like 17 uh because",
      "offset": 5769.159,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "that will run very inefficiently on a",
      "offset": 5771.199,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "GPU uh and we're going to see that a bit",
      "offset": 5772.6,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "later as well so for now let's just",
      "offset": 5774.719,
      "duration": 4.201
    },
    {
      "lang": "en",
      "text": "stick with",
      "offset": 5777.239,
      "duration": 4.92
    },
    {
      "lang": "en",
      "text": "16124 and uh the one thing that I added",
      "offset": 5778.92,
      "duration": 6.6
    },
    {
      "lang": "en",
      "text": "also here and I ran it again is I'm",
      "offset": 5782.159,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "calculating a tokens per second",
      "offset": 5785.52,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "throughput during training",
      "offset": 5787.04,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "because we might end up changing the",
      "offset": 5789.76,
      "duration": 4.359
    },
    {
      "lang": "en",
      "text": "backat size around over time but tokens",
      "offset": 5791.84,
      "duration": 3.72
    },
    {
      "lang": "en",
      "text": "per second is the objective measure that",
      "offset": 5794.119,
      "duration": 2.961
    },
    {
      "lang": "en",
      "text": "we actually really care about how many",
      "offset": 5795.56,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "tokens of data are we training on and",
      "offset": 5797.08,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "what is the throughput of tokens that",
      "offset": 5799.48,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "we're getting in our optimization so",
      "offset": 5801.08,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "right now we're processing and training",
      "offset": 5803.08,
      "duration": 5.4
    },
    {
      "lang": "en",
      "text": "on 163,000 tokens per second roughly and",
      "offset": 5804.76,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "that's a bit more objective",
      "offset": 5808.48,
      "duration": 5.32
    },
    {
      "lang": "en",
      "text": "metric okay so let's now enable tf32 now",
      "offset": 5810.44,
      "duration": 5.719
    },
    {
      "lang": "en",
      "text": "luckily pytorch makes this fairly easy",
      "offset": 5813.8,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "for us and uh to enable tf32 you just",
      "offset": 5816.159,
      "duration": 6.721
    },
    {
      "lang": "en",
      "text": "need to do a single line and is this and",
      "offset": 5819.56,
      "duration": 5.24
    },
    {
      "lang": "en",
      "text": "when we go to the py documentation here",
      "offset": 5822.88,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "for this function basically this tells",
      "offset": 5824.8,
      "duration": 5.56
    },
    {
      "lang": "en",
      "text": "pych what kind of kernels to run and by",
      "offset": 5827.28,
      "duration": 5.72
    },
    {
      "lang": "en",
      "text": "default I believe it is highest highest",
      "offset": 5830.36,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "Precision for mat M and that means that",
      "offset": 5833,
      "duration": 5.119
    },
    {
      "lang": "en",
      "text": "everything happens in float 32 just like",
      "offset": 5835.96,
      "duration": 4.679
    },
    {
      "lang": "en",
      "text": "it did before but if we set it to high",
      "offset": 5838.119,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "as we do right now Matrix",
      "offset": 5840.639,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "multiplications will not use tensor flow",
      "offset": 5842.6,
      "duration": 4.36
    },
    {
      "lang": "en",
      "text": "32 when it's",
      "offset": 5844.88,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "available my GPU is a100 so it's an",
      "offset": 5846.96,
      "duration": 6.159
    },
    {
      "lang": "en",
      "text": "ampere series and therefore tf32 is",
      "offset": 5850.4,
      "duration": 5.16
    },
    {
      "lang": "en",
      "text": "available if you have an older GPU this",
      "offset": 5853.119,
      "duration": 4.921
    },
    {
      "lang": "en",
      "text": "might not be available for you but for",
      "offset": 5855.56,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "my GPU it's available and so what I",
      "offset": 5858.04,
      "duration": 3.88
    },
    {
      "lang": "en",
      "text": "expect P to do is that every single",
      "offset": 5859.96,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "place where we see an nn. linear inside",
      "offset": 5861.92,
      "duration": 4.52
    },
    {
      "lang": "en",
      "text": "there there's a matrix multiplication",
      "offset": 5864.76,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "and I expect that matrix multiplication",
      "offset": 5866.44,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "now to be um running on tensor course",
      "offset": 5868.04,
      "duration": 7.4
    },
    {
      "lang": "en",
      "text": "utilizing the TF 32%",
      "offset": 5871.32,
      "duration": 4.12
    },
    {
      "lang": "en",
      "text": "so this is the single line of change",
      "offset": 5875.639,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "that is I believe necessary and let's",
      "offset": 5878.04,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "rerun this now we saw that um in terms",
      "offset": 5879.56,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "of the throughput that is promised to us",
      "offset": 5883,
      "duration": 5.159
    },
    {
      "lang": "en",
      "text": "we're supposed to be getting 8X roughly",
      "offset": 5885.08,
      "duration": 5.72
    },
    {
      "lang": "en",
      "text": "so let's see what",
      "offset": 5888.159,
      "duration": 7.601
    },
    {
      "lang": "en",
      "text": "happens and that 8X came from here right",
      "offset": 5890.8,
      "duration": 9.399
    },
    {
      "lang": "en",
      "text": "um 8X and it also came from looking at",
      "offset": 5895.76,
      "duration": 8.839
    },
    {
      "lang": "en",
      "text": "it um here 156 T flops instead of of",
      "offset": 5900.199,
      "duration": 7.4
    },
    {
      "lang": "en",
      "text": "19.5 okay so what actually happened uh",
      "offset": 5904.599,
      "duration": 5.361
    },
    {
      "lang": "en",
      "text": "so we're seeing that our throughput",
      "offset": 5907.599,
      "duration": 7.481
    },
    {
      "lang": "en",
      "text": "roughly 3x not aex so we are going we're",
      "offset": 5909.96,
      "duration": 7.199
    },
    {
      "lang": "en",
      "text": "from 1,000 milliseconds we're going down",
      "offset": 5915.08,
      "duration": 4.119
    },
    {
      "lang": "en",
      "text": "to 300 milliseconds and our throughput",
      "offset": 5917.159,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "is now about 50,000 tokens per second so",
      "offset": 5919.199,
      "duration": 4.681
    },
    {
      "lang": "en",
      "text": "we have a roughly 3x instead of 8X so",
      "offset": 5921.639,
      "duration": 4.361
    },
    {
      "lang": "en",
      "text": "what happened and basically What's",
      "offset": 5923.88,
      "duration": 4.2
    },
    {
      "lang": "en",
      "text": "Happening Here is again a lot of these",
      "offset": 5926,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "workloads are memory bound and so even",
      "offset": 5928.08,
      "duration": 4.36
    },
    {
      "lang": "en",
      "text": "though the",
      "offset": 5931.28,
      "duration": 5.839
    },
    {
      "lang": "en",
      "text": "tf32 offers in principle a lot faster",
      "offset": 5932.44,
      "duration": 6.64
    },
    {
      "lang": "en",
      "text": "throughput all of these numbers",
      "offset": 5937.119,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "everywhere are still float 32s and it's",
      "offset": 5939.08,
      "duration": 4.519
    },
    {
      "lang": "en",
      "text": "float 32 numbers that are being shipped",
      "offset": 5941.92,
      "duration": 3.16
    },
    {
      "lang": "en",
      "text": "all over the place through the memory",
      "offset": 5943.599,
      "duration": 3.801
    },
    {
      "lang": "en",
      "text": "system and is just costing us way too",
      "offset": 5945.08,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "much time to shuttle around all this",
      "offset": 5947.4,
      "duration": 3.239
    },
    {
      "lang": "en",
      "text": "data and so even though we've made the",
      "offset": 5948.92,
      "duration": 4.279
    },
    {
      "lang": "en",
      "text": "multiply itself much faster uh we are",
      "offset": 5950.639,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "memory bound and we're not actually",
      "offset": 5953.199,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "seeing the full benefit uh that would",
      "offset": 5954.32,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "come from uh this napkin math here uh",
      "offset": 5956.239,
      "duration": 5.841
    },
    {
      "lang": "en",
      "text": "that said we are getting one a 3X faster",
      "offset": 5959.84,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "throughput and this is free um single",
      "offset": 5962.08,
      "duration": 6.039
    },
    {
      "lang": "en",
      "text": "line of code in P torch all your",
      "offset": 5966.08,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "variables are still float 32 everywhere",
      "offset": 5968.119,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "it just runs faster and it's slightly",
      "offset": 5970.48,
      "duration": 3.96
    },
    {
      "lang": "en",
      "text": "more approximate but we're not going to",
      "offset": 5972.36,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "notice it basically uh so that's",
      "offset": 5974.44,
      "duration": 6.759
    },
    {
      "lang": "en",
      "text": "tf32 okay so let's now continue so we've",
      "offset": 5977.4,
      "duration": 7.4
    },
    {
      "lang": "en",
      "text": "exercised this row and um we saw that we",
      "offset": 5981.199,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "can crop out some of the Precision",
      "offset": 5984.8,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "inside the operation itself but we saw",
      "offset": 5986.4,
      "duration": 3.88
    },
    {
      "lang": "en",
      "text": "that we're still memory bound we're",
      "offset": 5989.119,
      "duration": 2.881
    },
    {
      "lang": "en",
      "text": "still moving around all these floats",
      "offset": 5990.28,
      "duration": 3.319
    },
    {
      "lang": "en",
      "text": "right otherwise and we're paying that",
      "offset": 5992,
      "duration": 4.04
    },
    {
      "lang": "en",
      "text": "cost because of this so let's now",
      "offset": 5993.599,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "decrease the amount of stuff that we're",
      "offset": 5996.04,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "going to be moving around and we're",
      "offset": 5997.76,
      "duration": 4.2
    },
    {
      "lang": "en",
      "text": "going to do that by dropping down to B",
      "offset": 5999.8,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "float 16 so we're only going to be",
      "offset": 6001.96,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "maintaining 16 bits per float and we're",
      "offset": 6004.679,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "going to use the B flat 16 and I'll",
      "offset": 6007.4,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "explain in a bit uh fp16 difference and",
      "offset": 6008.92,
      "duration": 5.96
    },
    {
      "lang": "en",
      "text": "uh we're going to be in this row so when",
      "offset": 6012.04,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "we go back to the documentation here for",
      "offset": 6014.88,
      "duration": 4.04
    },
    {
      "lang": "en",
      "text": "the a",
      "offset": 6017,
      "duration": 6.199
    },
    {
      "lang": "en",
      "text": "100 um we see here the precisions that",
      "offset": 6018.92,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "are are available and this is the",
      "offset": 6023.199,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "original fp32 the tf32 crops out the",
      "offset": 6025,
      "duration": 5.48
    },
    {
      "lang": "en",
      "text": "Precision and then here in",
      "offset": 6028.239,
      "duration": 5.121
    },
    {
      "lang": "en",
      "text": "bf16 you see that it is very similar to",
      "offset": 6030.48,
      "duration": 5.8
    },
    {
      "lang": "en",
      "text": "tf32 but it's even more aggressive in",
      "offset": 6033.36,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "cropping off of the Precision the",
      "offset": 6036.28,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "mantisa of this float so the important",
      "offset": 6038.4,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "thing with B float 16 is that the",
      "offset": 6040.92,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "exponent bits and the sign bit of course",
      "offset": 6042.56,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "remain unchanged so if you're familiar",
      "offset": 6045.08,
      "duration": 4.76
    },
    {
      "lang": "en",
      "text": "with your float numbers and I think this",
      "offset": 6047.52,
      "duration": 4.679
    },
    {
      "lang": "en",
      "text": "should should probably be an entire",
      "offset": 6049.84,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "video by itself",
      "offset": 6052.199,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "the exponent sets the range that you can",
      "offset": 6053.92,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "represent of your numbers and the",
      "offset": 6056.44,
      "duration": 4.44
    },
    {
      "lang": "en",
      "text": "Precision is how much Precision you have",
      "offset": 6058.159,
      "duration": 6.121
    },
    {
      "lang": "en",
      "text": "for your numbers and so the range of",
      "offset": 6060.88,
      "duration": 6.56
    },
    {
      "lang": "en",
      "text": "numbers is identical but we can we have",
      "offset": 6064.28,
      "duration": 6.64
    },
    {
      "lang": "en",
      "text": "fewer possibilities within that range",
      "offset": 6067.44,
      "duration": 5.48
    },
    {
      "lang": "en",
      "text": "because we are truncating the Mena so we",
      "offset": 6070.92,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "have less Precision in that",
      "offset": 6072.92,
      "duration": 4.84
    },
    {
      "lang": "en",
      "text": "range what that means is that things are",
      "offset": 6074.679,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "actually fairly nice because we have the",
      "offset": 6077.76,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "original range of numbers that are",
      "offset": 6079.719,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "representable in float but we just have",
      "offset": 6081.36,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "less Precision for it and the difference",
      "offset": 6084.119,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "with fp16 is that they actually touch",
      "offset": 6087.04,
      "duration": 5.72
    },
    {
      "lang": "en",
      "text": "and change the range so fp16 cannot",
      "offset": 6089.159,
      "duration": 6.201
    },
    {
      "lang": "en",
      "text": "represent the full range of fp32 it has",
      "offset": 6092.76,
      "duration": 4.76
    },
    {
      "lang": "en",
      "text": "a reduced range and that's where you",
      "offset": 6095.36,
      "duration": 4.2
    },
    {
      "lang": "en",
      "text": "start to actually run into issues",
      "offset": 6097.52,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "because now you need uh these gradient",
      "offset": 6099.56,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "scalers and things like that and I'm not",
      "offset": 6101.679,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "going to go into the detail of that in",
      "offset": 6103.56,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "this video because that's a whole video",
      "offset": 6105.76,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "by itself but fb16 actually historically",
      "offset": 6108.199,
      "duration": 4.601
    },
    {
      "lang": "en",
      "text": "came first that was available in the",
      "offset": 6110.88,
      "duration": 5.56
    },
    {
      "lang": "en",
      "text": "Volta series before Amper and so fp16",
      "offset": 6112.8,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "came first and everyone started to train",
      "offset": 6116.44,
      "duration": 3.719
    },
    {
      "lang": "en",
      "text": "in fp16 but everyone had to use all",
      "offset": 6118,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "these gradient scaling operations which",
      "offset": 6120.159,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "are kind of annoying and it's an",
      "offset": 6122.159,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "additional source of state and",
      "offset": 6123.76,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "complexity and the reason for that was",
      "offset": 6125.159,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "because the exponent range was reduced",
      "offset": 6127.44,
      "duration": 6.4
    },
    {
      "lang": "en",
      "text": "in fp16 so that's the i e fp16 spec and",
      "offset": 6129.4,
      "duration": 6.56
    },
    {
      "lang": "en",
      "text": "then they came out with bf16 and the",
      "offset": 6133.84,
      "duration": 4.52
    },
    {
      "lang": "en",
      "text": "Ampere and they made it much simpler",
      "offset": 6135.96,
      "duration": 4.199
    },
    {
      "lang": "en",
      "text": "because we're just truncating manessa we",
      "offset": 6138.36,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "have the exact same range and we do not",
      "offset": 6140.159,
      "duration": 3.96
    },
    {
      "lang": "en",
      "text": "need gradient scalers so everything is",
      "offset": 6141.96,
      "duration": 4.6
    },
    {
      "lang": "en",
      "text": "much much simpler now when we do use",
      "offset": 6144.119,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "bf16 though we are impacting the numbers",
      "offset": 6146.56,
      "duration": 5.559
    },
    {
      "lang": "en",
      "text": "that we might be seeing in our pytorch",
      "offset": 6150.199,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "code these this change is not just local",
      "offset": 6152.119,
      "duration": 5.681
    },
    {
      "lang": "en",
      "text": "to the operation itself so let's see how",
      "offset": 6155.159,
      "duration": 4.761
    },
    {
      "lang": "en",
      "text": "that works",
      "offset": 6157.8,
      "duration": 5.319
    },
    {
      "lang": "en",
      "text": "um there's some documentation here that",
      "offset": 6159.92,
      "duration": 4.679
    },
    {
      "lang": "en",
      "text": "so I think this is probably the best",
      "offset": 6163.119,
      "duration": 2.961
    },
    {
      "lang": "en",
      "text": "best page to explain how to use mixed",
      "offset": 6164.599,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "Precision in pytorch um because there",
      "offset": 6166.08,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "are many other tutorials and so on even",
      "offset": 6169.08,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "within pitor documentation that are a",
      "offset": 6171.52,
      "duration": 4.04
    },
    {
      "lang": "en",
      "text": "lot more confusing and so I recommend",
      "offset": 6173.48,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "specifically this one because there's",
      "offset": 6175.56,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "five other copies that I would not",
      "offset": 6177.8,
      "duration": 4.359
    },
    {
      "lang": "en",
      "text": "recommend and then when we come",
      "offset": 6179.719,
      "duration": 5.281
    },
    {
      "lang": "en",
      "text": "here ignore everything about everything",
      "offset": 6182.159,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "ignore everything about gradient",
      "offset": 6185,
      "duration": 5.96
    },
    {
      "lang": "en",
      "text": "scalers and only look at torch.",
      "offset": 6187.04,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "AutoCast and basically also this comes",
      "offset": 6190.96,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "to a single line of code at the end so",
      "offset": 6193.52,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "this is the context manager that we",
      "offset": 6195.92,
      "duration": 5.759
    },
    {
      "lang": "en",
      "text": "want and we want to use that in our",
      "offset": 6198.52,
      "duration": 6.84
    },
    {
      "lang": "en",
      "text": "Network when you click into the torch.",
      "offset": 6201.679,
      "duration": 7.161
    },
    {
      "lang": "en",
      "text": "AutoCast autocasting it has a few more",
      "offset": 6205.36,
      "duration": 5.4
    },
    {
      "lang": "en",
      "text": "uh a bit more guideline for you so it's",
      "offset": 6208.84,
      "duration": 5.359
    },
    {
      "lang": "en",
      "text": "telling you do not call B flat 16 on any",
      "offset": 6210.76,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "of your tensors just use AutoCast and",
      "offset": 6214.199,
      "duration": 5.4
    },
    {
      "lang": "en",
      "text": "only surround the uh forward pass of the",
      "offset": 6216.92,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "model and the loss calculation and",
      "offset": 6219.599,
      "duration": 3.56
    },
    {
      "lang": "en",
      "text": "that's the only two things that you",
      "offset": 6221.88,
      "duration": 3.239
    },
    {
      "lang": "en",
      "text": "should be surrounding leave the backward",
      "offset": 6223.159,
      "duration": 4.361
    },
    {
      "lang": "en",
      "text": "and the optimizer step alone so that's",
      "offset": 6225.119,
      "duration": 4.841
    },
    {
      "lang": "en",
      "text": "the guidance that comes from the P team",
      "offset": 6227.52,
      "duration": 3.96
    },
    {
      "lang": "en",
      "text": "so we're going to follow that guidance",
      "offset": 6229.96,
      "duration": 3.96
    },
    {
      "lang": "en",
      "text": "and for us because the L calculation is",
      "offset": 6231.48,
      "duration": 4.92
    },
    {
      "lang": "en",
      "text": "inside of the model forward pass for us",
      "offset": 6233.92,
      "duration": 4.6
    },
    {
      "lang": "en",
      "text": "we are going to be doing",
      "offset": 6236.4,
      "duration": 4.12
    },
    {
      "lang": "en",
      "text": "this and then we don't want to be using",
      "offset": 6238.52,
      "duration": 4.04
    },
    {
      "lang": "en",
      "text": "torch Flo 16 because if we do that we",
      "offset": 6240.52,
      "duration": 3.8
    },
    {
      "lang": "en",
      "text": "need to start using gradient scalers as",
      "offset": 6242.56,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "well so we are going to be using B float",
      "offset": 6244.32,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "16 this is only possible to do an ampere",
      "offset": 6246.32,
      "duration": 5.64
    },
    {
      "lang": "en",
      "text": "uh but this means that the changes are",
      "offset": 6249.92,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "extremely minimal like basically just",
      "offset": 6251.96,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "this one line of",
      "offset": 6253.679,
      "duration": 5.361
    },
    {
      "lang": "en",
      "text": "code um let me first break",
      "offset": 6254.96,
      "duration": 7.6
    },
    {
      "lang": "en",
      "text": "in to here before we actually run this",
      "offset": 6259.04,
      "duration": 6.199
    },
    {
      "lang": "en",
      "text": "so right after logits I'd like to show",
      "offset": 6262.56,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "you that different from the tf32 that we",
      "offset": 6265.239,
      "duration": 5.841
    },
    {
      "lang": "en",
      "text": "saw this is actually going to impact our",
      "offset": 6268.32,
      "duration": 4.6
    },
    {
      "lang": "en",
      "text": "tensors",
      "offset": 6271.08,
      "duration": 5.599
    },
    {
      "lang": "en",
      "text": "so this Lis tensor if we now look at",
      "offset": 6272.92,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "this and we look at the dtype we",
      "offset": 6276.679,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "suddenly see that this is now B float",
      "offset": 6278.44,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "16 uh it's not float 32 anymore so our",
      "offset": 6280.84,
      "duration": 4.68
    },
    {
      "lang": "en",
      "text": "activations have been changed the",
      "offset": 6283.88,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "activations tensor is now B FL 16 but",
      "offset": 6285.52,
      "duration": 5.8
    },
    {
      "lang": "en",
      "text": "not everything has changed so model.",
      "offset": 6288.52,
      "duration": 5.8
    },
    {
      "lang": "en",
      "text": "Transformer",
      "offset": 6291.32,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "wte uh this is the weight uh token",
      "offset": 6295.48,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "embedding table it has a weight inside",
      "offset": 6297.84,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "it and the dtype of this weight this",
      "offset": 6300.28,
      "duration": 6.28
    },
    {
      "lang": "en",
      "text": "parameter is still torch float 32 so our",
      "offset": 6302.96,
      "duration": 6.12
    },
    {
      "lang": "en",
      "text": "parameters seem to still be in float 32",
      "offset": 6306.56,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "but our activations the loits are now in",
      "offset": 6309.08,
      "duration": 5.119
    },
    {
      "lang": "en",
      "text": "P 16 so clearly this is why we get the",
      "offset": 6311.119,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "mixed Precision some things pytorch is",
      "offset": 6314.199,
      "duration": 5.241
    },
    {
      "lang": "en",
      "text": "keeping inlow 32 some things pytorch is",
      "offset": 6316.719,
      "duration": 6.721
    },
    {
      "lang": "en",
      "text": "converting to lower Precision um and",
      "offset": 6319.44,
      "duration": 6.679
    },
    {
      "lang": "en",
      "text": "what gets converted at what point is not",
      "offset": 6323.44,
      "duration": 6.96
    },
    {
      "lang": "en",
      "text": "super clear I remember scrolling",
      "offset": 6326.119,
      "duration": 7.681
    },
    {
      "lang": "en",
      "text": "down is it",
      "offset": 6330.4,
      "duration": 3.4
    },
    {
      "lang": "en",
      "text": "here okay I can't find",
      "offset": 6334.679,
      "duration": 6.401
    },
    {
      "lang": "en",
      "text": "it I I thought it was here okay there we",
      "offset": 6337.36,
      "duration": 7
    },
    {
      "lang": "en",
      "text": "go so there are a few docks on when",
      "offset": 6341.08,
      "duration": 5.639
    },
    {
      "lang": "en",
      "text": "you're using this AutoCast what gets",
      "offset": 6344.36,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "converted to B FL 16 and and when so for",
      "offset": 6346.719,
      "duration": 5.041
    },
    {
      "lang": "en",
      "text": "example only these Matrix multiply like",
      "offset": 6349.48,
      "duration": 5.08
    },
    {
      "lang": "en",
      "text": "operations get converted to float 16 but",
      "offset": 6351.76,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "a lot of operations remain in float 32",
      "offset": 6354.56,
      "duration": 4.039
    },
    {
      "lang": "en",
      "text": "so in particular a lot of normalizations",
      "offset": 6356.76,
      "duration": 3.72
    },
    {
      "lang": "en",
      "text": "like layer norms and things like that",
      "offset": 6358.599,
      "duration": 3.321
    },
    {
      "lang": "en",
      "text": "not all of those layers might be",
      "offset": 6360.48,
      "duration": 4.52
    },
    {
      "lang": "en",
      "text": "converted um so only some layers",
      "offset": 6361.92,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "selectively would be running B flat 16",
      "offset": 6365,
      "duration": 5.56
    },
    {
      "lang": "en",
      "text": "but things like softmax uh layer Norms",
      "offset": 6367.36,
      "duration": 6.72
    },
    {
      "lang": "en",
      "text": "uh log um log soft Max so loss function",
      "offset": 6370.56,
      "duration": 5.24
    },
    {
      "lang": "en",
      "text": "calculations a lot of those things might",
      "offset": 6374.08,
      "duration": 3.72
    },
    {
      "lang": "en",
      "text": "remain in float 32 because they are more",
      "offset": 6375.8,
      "duration": 4.52
    },
    {
      "lang": "en",
      "text": "susceptible to Precision changes major",
      "offset": 6377.8,
      "duration": 5.72
    },
    {
      "lang": "en",
      "text": "multiplies are fairly um",
      "offset": 6380.32,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "robust to Precision changes uh so some",
      "offset": 6383.52,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "parts of the network are um impacted",
      "offset": 6386.8,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "more or less by the Precision",
      "offset": 6389.04,
      "duration": 5.48
    },
    {
      "lang": "en",
      "text": "change um so basically only some parts",
      "offset": 6391.8,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "of the of the model are running in",
      "offset": 6394.52,
      "duration": 3.639
    },
    {
      "lang": "en",
      "text": "reduced Precision let's take it for a",
      "offset": 6395.96,
      "duration": 5.719
    },
    {
      "lang": "en",
      "text": "spin and let's actually see what kind of",
      "offset": 6398.159,
      "duration": 6.56
    },
    {
      "lang": "en",
      "text": "improvement we achieve",
      "offset": 6401.679,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "here okay so we used to be 333",
      "offset": 6408.28,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "milliseconds we're now 300",
      "offset": 6411.4,
      "duration": 3.4
    },
    {
      "lang": "en",
      "text": "and we used to be somewhere around",
      "offset": 6413.8,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "50,000 tokens per second we're now at 55",
      "offset": 6414.8,
      "duration": 5.16
    },
    {
      "lang": "en",
      "text": "so we're definitely running faster but",
      "offset": 6417.639,
      "duration": 4.841
    },
    {
      "lang": "en",
      "text": "maybe not a lot faster and that's",
      "offset": 6419.96,
      "duration": 3.719
    },
    {
      "lang": "en",
      "text": "because there are still many many",
      "offset": 6422.48,
      "duration": 3.239
    },
    {
      "lang": "en",
      "text": "bottlenecks in our gbt2 we're just",
      "offset": 6423.679,
      "duration": 4.201
    },
    {
      "lang": "en",
      "text": "getting started but we have dropped down",
      "offset": 6425.719,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "the precision as far as we can with my",
      "offset": 6427.88,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "current GPU which is a100 we're using",
      "offset": 6429.8,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "pytorch AutoCast unfortunately I don't",
      "offset": 6432.76,
      "duration": 4.52
    },
    {
      "lang": "en",
      "text": "actually exactly know what pytorch",
      "offset": 6435.32,
      "duration": 4.52
    },
    {
      "lang": "en",
      "text": "AutoCast do uh does I don't actually",
      "offset": 6437.28,
      "duration": 4.76
    },
    {
      "lang": "en",
      "text": "know exactly what's in B flat 16 what's",
      "offset": 6439.84,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "in float 32",
      "offset": 6442.04,
      "duration": 2.48
    },
    {
      "lang": "en",
      "text": "we could go in and we could start to",
      "offset": 6443.36,
      "duration": 3.879
    },
    {
      "lang": "en",
      "text": "scrutinize it um but these are the kinds",
      "offset": 6444.52,
      "duration": 4.84
    },
    {
      "lang": "en",
      "text": "of rules that pytorch has internally and",
      "offset": 6447.239,
      "duration": 4.121
    },
    {
      "lang": "en",
      "text": "unfortunately they don't documented very",
      "offset": 6449.36,
      "duration": 4.839
    },
    {
      "lang": "en",
      "text": "well uh so we're not going to go into",
      "offset": 6451.36,
      "duration": 5.239
    },
    {
      "lang": "en",
      "text": "that into in too much detail but for now",
      "offset": 6454.199,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "we are training in B flow 16 we do not",
      "offset": 6456.599,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "need a gradient scaler and the reason",
      "offset": 6459,
      "duration": 5.639
    },
    {
      "lang": "en",
      "text": "things are running faster is because um",
      "offset": 6460.92,
      "duration": 6.52
    },
    {
      "lang": "en",
      "text": "we are able to run tensor course in B FL",
      "offset": 6464.639,
      "duration": 7.361
    },
    {
      "lang": "en",
      "text": "16 now that means we are in this row but",
      "offset": 6467.44,
      "duration": 6.44
    },
    {
      "lang": "en",
      "text": "uh we are also paying in Precision for",
      "offset": 6472,
      "duration": 5.56
    },
    {
      "lang": "en",
      "text": "this uh so um we expect slightly less",
      "offset": 6473.88,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "accurate results with respect to the",
      "offset": 6477.56,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "original fp32 but empirically in many",
      "offset": 6478.88,
      "duration": 5.359
    },
    {
      "lang": "en",
      "text": "cases this is a worth it uh kind of",
      "offset": 6481.88,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "tradeoff because it allows you to run",
      "offset": 6484.239,
      "duration": 3.641
    },
    {
      "lang": "en",
      "text": "faster and you could for example train",
      "offset": 6486.199,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "longer and make up for the uh for that",
      "offset": 6487.88,
      "duration": 7.16
    },
    {
      "lang": "en",
      "text": "Precision decrease so um that's b46 for",
      "offset": 6490.84,
      "duration": 6.68
    },
    {
      "lang": "en",
      "text": "now okay so as we can see we are",
      "offset": 6495.04,
      "duration": 4.76
    },
    {
      "lang": "en",
      "text": "currently at about 300 milliseconds uh",
      "offset": 6497.52,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "per iteration and we're now going to",
      "offset": 6499.8,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "reach for some really heavy weapons in",
      "offset": 6501.36,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "the pie torch Arsenal and in particular",
      "offset": 6503.08,
      "duration": 4.599
    },
    {
      "lang": "en",
      "text": "we're going to introduce torch. compile",
      "offset": 6505.04,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "so torch. compile is really quite",
      "offset": 6507.679,
      "duration": 3.321
    },
    {
      "lang": "en",
      "text": "incredible infrastructure from the",
      "offset": 6509.599,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "pytorch team and it's basically a",
      "offset": 6511,
      "duration": 4.04
    },
    {
      "lang": "en",
      "text": "compiler for neural networks like it's",
      "offset": 6512.719,
      "duration": 5.601
    },
    {
      "lang": "en",
      "text": "almost like GCC for CN C++ code this is",
      "offset": 6515.04,
      "duration": 7.8
    },
    {
      "lang": "en",
      "text": "just this GCC of neural nuts so came out",
      "offset": 6518.32,
      "duration": 8
    },
    {
      "lang": "en",
      "text": "a while ago and extremely simple to use",
      "offset": 6522.84,
      "duration": 5.799
    },
    {
      "lang": "en",
      "text": "um the way to use torch compile is to do",
      "offset": 6526.32,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "this it's a single line of code to",
      "offset": 6528.639,
      "duration": 5.48
    },
    {
      "lang": "en",
      "text": "compile your model and return it now",
      "offset": 6530.8,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "this line of code will cost you",
      "offset": 6534.119,
      "duration": 3.48
    },
    {
      "lang": "en",
      "text": "compilation time but as you might guess",
      "offset": 6535.44,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "it's going to make the code a lot faster",
      "offset": 6537.599,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "so let's actually run that because this",
      "offset": 6539.679,
      "duration": 3.96
    },
    {
      "lang": "en",
      "text": "will take some time to run but currently",
      "offset": 6541.84,
      "duration": 3.72
    },
    {
      "lang": "en",
      "text": "remember we're at 300 milliseconds and",
      "offset": 6543.639,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "we'll see what happens now while this is",
      "offset": 6545.56,
      "duration": 4.679
    },
    {
      "lang": "en",
      "text": "running I'd like to explain a little bit",
      "offset": 6548.04,
      "duration": 3.8
    },
    {
      "lang": "en",
      "text": "of what torch. compile does under the",
      "offset": 6550.239,
      "duration": 4.761
    },
    {
      "lang": "en",
      "text": "hood uh so feel free to read this page",
      "offset": 6551.84,
      "duration": 5.799
    },
    {
      "lang": "en",
      "text": "of P torch but basically there's no real",
      "offset": 6555,
      "duration": 4.52
    },
    {
      "lang": "en",
      "text": "good reason for you to not use torch",
      "offset": 6557.639,
      "duration": 3.881
    },
    {
      "lang": "en",
      "text": "compile in your pie torch I kind of feel",
      "offset": 6559.52,
      "duration": 3.719
    },
    {
      "lang": "en",
      "text": "like you should be using almost by",
      "offset": 6561.52,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "default if you're not uh unless you're",
      "offset": 6563.239,
      "duration": 3.321
    },
    {
      "lang": "en",
      "text": "debugging and you want your code to run",
      "offset": 6565.04,
      "duration": 4.599
    },
    {
      "lang": "en",
      "text": "really fast and there's one line here in",
      "offset": 6566.56,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "torch compile that I found that actually",
      "offset": 6569.639,
      "duration": 4.201
    },
    {
      "lang": "en",
      "text": "kind of like gets to why this is faster",
      "offset": 6571.04,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "speed up mainly comes from reducing",
      "offset": 6573.84,
      "duration": 4.92
    },
    {
      "lang": "en",
      "text": "python overhead and GPU read wrs so let",
      "offset": 6575.599,
      "duration": 6.201
    },
    {
      "lang": "en",
      "text": "me unpack that a little bit um okay here",
      "offset": 6578.76,
      "duration": 4.919
    },
    {
      "lang": "en",
      "text": "we are okay so we went from 300",
      "offset": 6581.8,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "milliseconds we're now running at 129",
      "offset": 6583.679,
      "duration": 7.48
    },
    {
      "lang": "en",
      "text": "milliseconds so this is uh 300 129 about",
      "offset": 6586.92,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "2.3x Improvement from a single line of",
      "offset": 6591.159,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "code in py torch uh so quite incredible",
      "offset": 6593.4,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "so what is happening what's happening",
      "offset": 6596.679,
      "duration": 3.081
    },
    {
      "lang": "en",
      "text": "under the hood well when you pass the",
      "offset": 6597.88,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "model to torch",
      "offset": 6599.76,
      "duration": 4.439
    },
    {
      "lang": "en",
      "text": "compile what we have here in this NN",
      "offset": 6601.4,
      "duration": 4.44
    },
    {
      "lang": "en",
      "text": "module this is really just the",
      "offset": 6604.199,
      "duration": 4.201
    },
    {
      "lang": "en",
      "text": "algorithmic description of what we'd",
      "offset": 6605.84,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "like to happen in our Network and torch",
      "offset": 6608.4,
      "duration": 5.88
    },
    {
      "lang": "en",
      "text": "compile will analyze the entire thing",
      "offset": 6611.04,
      "duration": 4.92
    },
    {
      "lang": "en",
      "text": "and it will look at what operations You'",
      "offset": 6614.28,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "like to use and with the benefit of",
      "offset": 6615.96,
      "duration": 4.84
    },
    {
      "lang": "en",
      "text": "knowing exactly what's going to happen",
      "offset": 6618.04,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "it doesn't have to run in What's called",
      "offset": 6620.8,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "the e mode it doesn't have to just kind",
      "offset": 6622.28,
      "duration": 4.439
    },
    {
      "lang": "en",
      "text": "of like go layer by layer like the",
      "offset": 6624.48,
      "duration": 5.32
    },
    {
      "lang": "en",
      "text": "python interpreter normally would start",
      "offset": 6626.719,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "at the",
      "offset": 6629.8,
      "duration": 3.799
    },
    {
      "lang": "en",
      "text": "forward and the python interpreter will",
      "offset": 6631.199,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "go okay let's do this operation and then",
      "offset": 6633.599,
      "duration": 4.441
    },
    {
      "lang": "en",
      "text": "let's do that operation and it kind of",
      "offset": 6636.08,
      "duration": 4.119
    },
    {
      "lang": "en",
      "text": "materializes all the operations as it",
      "offset": 6638.04,
      "duration": 5.079
    },
    {
      "lang": "en",
      "text": "goes through uh so these um calculations",
      "offset": 6640.199,
      "duration": 5.641
    },
    {
      "lang": "en",
      "text": "are dispatched and run in this order and",
      "offset": 6643.119,
      "duration": 4.681
    },
    {
      "lang": "en",
      "text": "the python interpreter and this code",
      "offset": 6645.84,
      "duration": 3.48
    },
    {
      "lang": "en",
      "text": "doesn't know what kind of operations are",
      "offset": 6647.8,
      "duration": 3.64
    },
    {
      "lang": "en",
      "text": "going to happen later but torch compile",
      "offset": 6649.32,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "sees your entire code at the same time",
      "offset": 6651.44,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "and it's able to know what operations",
      "offset": 6653.719,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "you intend to run and it will kind of",
      "offset": 6656,
      "duration": 4.44
    },
    {
      "lang": "en",
      "text": "optimize that process the first thing it",
      "offset": 6658.28,
      "duration": 3.56
    },
    {
      "lang": "en",
      "text": "will do is will it will take out the",
      "offset": 6660.44,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "python interpreter from the forward pass",
      "offset": 6661.84,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "entirely and it will kind of compile",
      "offset": 6663.8,
      "duration": 3.64
    },
    {
      "lang": "en",
      "text": "this entire neural net as a single",
      "offset": 6665.84,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "object with no python interpreter",
      "offset": 6667.44,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "involved so it knows exactly what's",
      "offset": 6669.599,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "going to run and we'll just run that and",
      "offset": 6671.36,
      "duration": 2.799
    },
    {
      "lang": "en",
      "text": "it's all going to be running in",
      "offset": 6672.96,
      "duration": 2.679
    },
    {
      "lang": "en",
      "text": "efficient",
      "offset": 6674.159,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "code uh the second thing that happens is",
      "offset": 6675.639,
      "duration": 6.321
    },
    {
      "lang": "en",
      "text": "uh this read write that they mentioned",
      "offset": 6678.96,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "very briefly so a good example of that I",
      "offset": 6681.96,
      "duration": 3.8
    },
    {
      "lang": "en",
      "text": "think is the G nonlinearity that we've",
      "offset": 6683.96,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "been looking at so here we use the n and",
      "offset": 6685.76,
      "duration": 6.6
    },
    {
      "lang": "en",
      "text": "G now this here is me uh basically just",
      "offset": 6688.119,
      "duration": 7.361
    },
    {
      "lang": "en",
      "text": "breaking up the inang Galu uh which you",
      "offset": 6692.36,
      "duration": 5.56
    },
    {
      "lang": "en",
      "text": "remember has this formula so this here",
      "offset": 6695.48,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "is the equivalent implementation to",
      "offset": 6697.92,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "what's happening inside g algorithmic l",
      "offset": 6699.719,
      "duration": 2.601
    },
    {
      "lang": "en",
      "text": "it's",
      "offset": 6701.679,
      "duration": 4.361
    },
    {
      "lang": "en",
      "text": "identical Now by default if uh we just",
      "offset": 6702.32,
      "duration": 6.56
    },
    {
      "lang": "en",
      "text": "we using this instead of ending. G here",
      "offset": 6706.04,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "what would happen without torch compile",
      "offset": 6708.88,
      "duration": 3.96
    },
    {
      "lang": "en",
      "text": "well the python interpreter would make",
      "offset": 6711.32,
      "duration": 3.399
    },
    {
      "lang": "en",
      "text": "its way here and then it would be okay",
      "offset": 6712.84,
      "duration": 5.279
    },
    {
      "lang": "en",
      "text": "well there's an input well let me first",
      "offset": 6714.719,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "let me raise this input to the third",
      "offset": 6718.119,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "power and it's going to dispatch a",
      "offset": 6719.679,
      "duration": 3.641
    },
    {
      "lang": "en",
      "text": "kernel that takes your input and raises",
      "offset": 6721.239,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "it to the third power and that kernel",
      "offset": 6723.32,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "will run and when this kernel runs what",
      "offset": 6725.88,
      "duration": 5.56
    },
    {
      "lang": "en",
      "text": "ends up happening is this input is",
      "offset": 6728.92,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "stored in the memory of the GPU so",
      "offset": 6731.44,
      "duration": 5.239
    },
    {
      "lang": "en",
      "text": "here's a helpful example of the layout",
      "offset": 6733.88,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "of what's happening right you have your",
      "offset": 6736.679,
      "duration": 4.361
    },
    {
      "lang": "en",
      "text": "CPU this is in every single computer",
      "offset": 6738.679,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "there's a few cores in there and you",
      "offset": 6741.04,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "have your uh Ram uh your memory and the",
      "offset": 6743,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "CPU can talk to the memory and this is",
      "offset": 6746.48,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "all well known but now we've added the",
      "offset": 6748.36,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "GPU and the GPU is a slightly different",
      "offset": 6750.079,
      "duration": 3.481
    },
    {
      "lang": "en",
      "text": "architecture of course they can",
      "offset": 6752.199,
      "duration": 3.281
    },
    {
      "lang": "en",
      "text": "communicate and it's different in that",
      "offset": 6753.56,
      "duration": 5.079
    },
    {
      "lang": "en",
      "text": "it's got a lot more course than a CPU",
      "offset": 6755.48,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "all of those cores are individually a",
      "offset": 6758.639,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "lot simpler too but it also has memory",
      "offset": 6760.28,
      "duration": 7.319
    },
    {
      "lang": "en",
      "text": "right this high bandwidth memory I'm",
      "offset": 6763.44,
      "duration": 6.279
    },
    {
      "lang": "en",
      "text": "sorry if I'm botching it hbm I don't",
      "offset": 6767.599,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "even know what that stands for I'm just",
      "offset": 6769.719,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "realizing that",
      "offset": 6771.92,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "but uh this is the memory and it's very",
      "offset": 6773.239,
      "duration": 5.241
    },
    {
      "lang": "en",
      "text": "equivalent to uh RAM basically in the",
      "offset": 6774.92,
      "duration": 5.319
    },
    {
      "lang": "en",
      "text": "computer and what's happening is that",
      "offset": 6778.48,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "input is living in the memory and when",
      "offset": 6780.239,
      "duration": 4.92
    },
    {
      "lang": "en",
      "text": "you do input",
      "offset": 6782.56,
      "duration": 7.24
    },
    {
      "lang": "en",
      "text": "cubed this has to travel to the GPU to",
      "offset": 6785.159,
      "duration": 6.96
    },
    {
      "lang": "en",
      "text": "the course and to all the caches and",
      "offset": 6789.8,
      "duration": 5.359
    },
    {
      "lang": "en",
      "text": "registers on the actual chip of this",
      "offset": 6792.119,
      "duration": 5.801
    },
    {
      "lang": "en",
      "text": "GPU and it has to calculate the all the",
      "offset": 6795.159,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "elements to the third and then it saves",
      "offset": 6797.92,
      "duration": 4.759
    },
    {
      "lang": "en",
      "text": "the result back to the memory and it's",
      "offset": 6799.8,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "this uh travel time that actually causes",
      "offset": 6802.679,
      "duration": 5.56
    },
    {
      "lang": "en",
      "text": "a lot of issues so here remember this",
      "offset": 6805,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "memory bandwidth we can communicate",
      "offset": 6808.239,
      "duration": 3.721
    },
    {
      "lang": "en",
      "text": "about 2 terabytes per second which is a",
      "offset": 6810.28,
      "duration": 5.359
    },
    {
      "lang": "en",
      "text": "lot but also we have to Traverse this",
      "offset": 6811.96,
      "duration": 5.719
    },
    {
      "lang": "en",
      "text": "link and it's very slow so here on the",
      "offset": 6815.639,
      "duration": 3.96
    },
    {
      "lang": "en",
      "text": "GPU we're on chip and everything is",
      "offset": 6817.679,
      "duration": 3.96
    },
    {
      "lang": "en",
      "text": "super fast within the chip but going to",
      "offset": 6819.599,
      "duration": 3.881
    },
    {
      "lang": "en",
      "text": "the memory is extremely expensive takes",
      "offset": 6821.639,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "extremely long amount of time and so we",
      "offset": 6823.48,
      "duration": 5.32
    },
    {
      "lang": "en",
      "text": "load the input do the calculations and",
      "offset": 6826.599,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "load back the output and this round trip",
      "offset": 6828.8,
      "duration": 4.2
    },
    {
      "lang": "en",
      "text": "takes a lot of time",
      "offset": 6831.56,
      "duration": 3.32
    },
    {
      "lang": "en",
      "text": "and now right after we do that we",
      "offset": 6833,
      "duration": 4.28
    },
    {
      "lang": "en",
      "text": "multiply by this constant so what",
      "offset": 6834.88,
      "duration": 4.6
    },
    {
      "lang": "en",
      "text": "happens then is we dispatch another",
      "offset": 6837.28,
      "duration": 5.439
    },
    {
      "lang": "en",
      "text": "kernel and then the result travels back",
      "offset": 6839.48,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "all the elements get multiplied by a",
      "offset": 6842.719,
      "duration": 3.641
    },
    {
      "lang": "en",
      "text": "constant and then the results travel",
      "offset": 6843.96,
      "duration": 5.159
    },
    {
      "lang": "en",
      "text": "back to the memory and then we take the",
      "offset": 6846.36,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "result and we add back input and so this",
      "offset": 6849.119,
      "duration": 6.201
    },
    {
      "lang": "en",
      "text": "entire thing again travels to the GPU",
      "offset": 6852.44,
      "duration": 5.759
    },
    {
      "lang": "en",
      "text": "adds the inputs and gets written back so",
      "offset": 6855.32,
      "duration": 5.279
    },
    {
      "lang": "en",
      "text": "we're making all these round trips from",
      "offset": 6858.199,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "the memory to actually where the comput",
      "offset": 6860.599,
      "duration": 4.281
    },
    {
      "lang": "en",
      "text": "happens because all the tensor cores and",
      "offset": 6862.679,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "alus and everything like that is all",
      "offset": 6864.88,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "stored on the chip in the GPU so we're",
      "offset": 6866.76,
      "duration": 4.959
    },
    {
      "lang": "en",
      "text": "doing a ton of round trips and pytorch",
      "offset": 6868.88,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "uh without using torch compile doesn't",
      "offset": 6871.719,
      "duration": 4.601
    },
    {
      "lang": "en",
      "text": "know to optimize this because it doesn't",
      "offset": 6873.92,
      "duration": 3.56
    },
    {
      "lang": "en",
      "text": "know what kind of operations you're",
      "offset": 6876.32,
      "duration": 3.64
    },
    {
      "lang": "en",
      "text": "running later you're just telling it",
      "offset": 6877.48,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "raise the power to the third then do",
      "offset": 6879.96,
      "duration": 3.279
    },
    {
      "lang": "en",
      "text": "this then do that and it will just do",
      "offset": 6881.88,
      "duration": 3.319
    },
    {
      "lang": "en",
      "text": "that in that sequence but torch compile",
      "offset": 6883.239,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "sees your entire code it will come here",
      "offset": 6885.199,
      "duration": 3.801
    },
    {
      "lang": "en",
      "text": "and it will realize wait all of these",
      "offset": 6887.4,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "are elementwise operations and actually",
      "offset": 6889,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "what I'm going to do is I'm going to do",
      "offset": 6892.44,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "a single trip of input to the GPU then",
      "offset": 6893.719,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "for every single element I'm going to do",
      "offset": 6896.599,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "all of these operations while that",
      "offset": 6898.599,
      "duration": 5.721
    },
    {
      "lang": "en",
      "text": "memory is on the GPU or chunks of it",
      "offset": 6900.76,
      "duration": 5.319
    },
    {
      "lang": "en",
      "text": "rather and then I'm going to write back",
      "offset": 6904.32,
      "duration": 3.48
    },
    {
      "lang": "en",
      "text": "a single time so we're not going to have",
      "offset": 6906.079,
      "duration": 3.56
    },
    {
      "lang": "en",
      "text": "these round trips and that's one example",
      "offset": 6907.8,
      "duration": 4.12
    },
    {
      "lang": "en",
      "text": "of what's called kernel fusion and is a",
      "offset": 6909.639,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "major way in which everything is sped up",
      "offset": 6911.92,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "so basically if you have your benefit of",
      "offset": 6914.44,
      "duration": 2.759
    },
    {
      "lang": "en",
      "text": "onet and you know exactly what you're",
      "offset": 6915.84,
      "duration": 3.48
    },
    {
      "lang": "en",
      "text": "going to compute you can optimize your",
      "offset": 6917.199,
      "duration": 4.681
    },
    {
      "lang": "en",
      "text": "round trips to the memory and you're not",
      "offset": 6919.32,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "going to pay the the memory bandwidth",
      "offset": 6921.88,
      "duration": 3.4
    },
    {
      "lang": "en",
      "text": "cost and that's fundamentally what makes",
      "offset": 6923.079,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "some of these operations a lot faster",
      "offset": 6925.28,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "and what they mean by read writes",
      "offset": 6927.32,
      "duration": 5.359
    },
    {
      "lang": "en",
      "text": "here so let me erase this because we are",
      "offset": 6930.079,
      "duration": 6.52
    },
    {
      "lang": "en",
      "text": "not using it and yeah we should be using",
      "offset": 6932.679,
      "duration": 6.601
    },
    {
      "lang": "en",
      "text": "torch compile and our code is now",
      "offset": 6936.599,
      "duration": 4.12
    },
    {
      "lang": "en",
      "text": "significantly faster and we're doing",
      "offset": 6939.28,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "about",
      "offset": 6940.719,
      "duration": 4.281
    },
    {
      "lang": "en",
      "text": "125,000 tokens per second but we still",
      "offset": 6942,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "have a long way to go before we move on",
      "offset": 6945,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "I wanted to supplement the discussion a",
      "offset": 6947.199,
      "duration": 4.281
    },
    {
      "lang": "en",
      "text": "little bit with a few more figures uh",
      "offset": 6949.4,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "because this is a complic topic but it's",
      "offset": 6951.48,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "worth understanding on a high level uh",
      "offset": 6953.239,
      "duration": 3.641
    },
    {
      "lang": "en",
      "text": "what's happening here and I could",
      "offset": 6955.239,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "probably spend an entire video of like",
      "offset": 6956.88,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "two hours on this but just the preview",
      "offset": 6958.44,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "of that basically so this chip here that",
      "offset": 6960.239,
      "duration": 6.721
    },
    {
      "lang": "en",
      "text": "is uh the GPU this chip is where all the",
      "offset": 6963.639,
      "duration": 6.321
    },
    {
      "lang": "en",
      "text": "calculations happen mostly but this chip",
      "offset": 6966.96,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "also does have some memory in it but",
      "offset": 6969.96,
      "duration": 5.639
    },
    {
      "lang": "en",
      "text": "most of the memory by far is here in the",
      "offset": 6972.96,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "high bandwidth memory hbm and is",
      "offset": 6975.599,
      "duration": 5.08
    },
    {
      "lang": "en",
      "text": "connected they're connected um but these",
      "offset": 6978.159,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "are two separate chips basically",
      "offset": 6980.679,
      "duration": 6.04
    },
    {
      "lang": "en",
      "text": "now here this is a zoom in of kind of",
      "offset": 6983.119,
      "duration": 7.361
    },
    {
      "lang": "en",
      "text": "this cartoon diagram of a GPU and what",
      "offset": 6986.719,
      "duration": 5.041
    },
    {
      "lang": "en",
      "text": "we're seeing here is number one you see",
      "offset": 6990.48,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "this hbm I I realize it's probably very",
      "offset": 6991.76,
      "duration": 4.12
    },
    {
      "lang": "en",
      "text": "small for you but on the sides here it",
      "offset": 6994.159,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "says hbm and so that that's the links to",
      "offset": 6995.88,
      "duration": 6.6
    },
    {
      "lang": "en",
      "text": "the hbm now the hbm is again off chip on",
      "offset": 6998.48,
      "duration": 6.52
    },
    {
      "lang": "en",
      "text": "the chip there are a large number of",
      "offset": 7002.48,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "these streaming",
      "offset": 7005,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "multiprocessors uh every one of these is",
      "offset": 7006.48,
      "duration": 5.159
    },
    {
      "lang": "en",
      "text": "an SM there's 120 of them in total and",
      "offset": 7008.28,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "this is where the a lot of the",
      "offset": 7011.639,
      "duration": 3.08
    },
    {
      "lang": "en",
      "text": "calculations happen and this is a zoom",
      "offset": 7012.599,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "in of a single individual as it has",
      "offset": 7014.719,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "these four quadrants and see for example",
      "offset": 7017.4,
      "duration": 3.48
    },
    {
      "lang": "en",
      "text": "tensor core this is where a lot of the",
      "offset": 7019.44,
      "duration": 3.48
    },
    {
      "lang": "en",
      "text": "Matrix multiply stuff happens but",
      "offset": 7020.88,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "there's all these other units to do all",
      "offset": 7022.92,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "different kinds of calculations for fp64",
      "offset": 7024.8,
      "duration": 6.359
    },
    {
      "lang": "en",
      "text": "fp32 and for integers and so on now so",
      "offset": 7027.239,
      "duration": 6.44
    },
    {
      "lang": "en",
      "text": "we have all this uh logic here to do the",
      "offset": 7031.159,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "calculations but in addition to that on",
      "offset": 7033.679,
      "duration": 4.04
    },
    {
      "lang": "en",
      "text": "the chip there is memory sprinkled",
      "offset": 7035.719,
      "duration": 5.641
    },
    {
      "lang": "en",
      "text": "throughout the chip so L2 cache is some",
      "offset": 7037.719,
      "duration": 6.201
    },
    {
      "lang": "en",
      "text": "amount of memory that lives on the chip",
      "offset": 7041.36,
      "duration": 4.52
    },
    {
      "lang": "en",
      "text": "and then on the SMS themselves there's",
      "offset": 7043.92,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "L1 cache I realized it's probably very",
      "offset": 7045.88,
      "duration": 5.239
    },
    {
      "lang": "en",
      "text": "small for you but this blue bar is L1",
      "offset": 7048.32,
      "duration": 6.2
    },
    {
      "lang": "en",
      "text": "and there's also registers um and so",
      "offset": 7051.119,
      "duration": 5.761
    },
    {
      "lang": "en",
      "text": "there is memory stored here but the way",
      "offset": 7054.52,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "this memory is stored is very different",
      "offset": 7056.88,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "from the way memory is stored in hbm uh",
      "offset": 7058.599,
      "duration": 5.401
    },
    {
      "lang": "en",
      "text": "this is a very different implementation",
      "offset": 7061.599,
      "duration": 5.961
    },
    {
      "lang": "en",
      "text": "uh using um just in terms of like what",
      "offset": 7064,
      "duration": 4.76
    },
    {
      "lang": "en",
      "text": "the Silicon looks like it's a very",
      "offset": 7067.56,
      "duration": 2.159
    },
    {
      "lang": "en",
      "text": "different",
      "offset": 7068.76,
      "duration": 3.56
    },
    {
      "lang": "en",
      "text": "implementation um so here you would",
      "offset": 7069.719,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "using transistors and capacitors and",
      "offset": 7072.32,
      "duration": 3.08
    },
    {
      "lang": "en",
      "text": "here it's a very different",
      "offset": 7074.199,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "implementation uh with SRAM and what",
      "offset": 7075.4,
      "duration": 6.04
    },
    {
      "lang": "en",
      "text": "that looks like but long story short is",
      "offset": 7077.639,
      "duration": 7.96
    },
    {
      "lang": "en",
      "text": "um there is um memory inside the chip",
      "offset": 7081.44,
      "duration": 5.799
    },
    {
      "lang": "en",
      "text": "but it's not a lot of memory that's the",
      "offset": 7085.599,
      "duration": 4.201
    },
    {
      "lang": "en",
      "text": "critical point so this is some C this is",
      "offset": 7087.239,
      "duration": 4.681
    },
    {
      "lang": "en",
      "text": "a example diagram of a slightly",
      "offset": 7089.8,
      "duration": 4.68
    },
    {
      "lang": "en",
      "text": "different GPU just like here where it",
      "offset": 7091.92,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "shows that for example typical numbers",
      "offset": 7094.48,
      "duration": 4.92
    },
    {
      "lang": "en",
      "text": "for CPU Dam memory which is this thing",
      "offset": 7096.239,
      "duration": 6.201
    },
    {
      "lang": "en",
      "text": "here you might have one tab of this",
      "offset": 7099.4,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "right but it would be extremely",
      "offset": 7102.44,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "expensive to access especially for a GPU",
      "offset": 7103.639,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "you have to go through the CPU here now",
      "offset": 7105.56,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "next we have the hbm so we have tens of",
      "offset": 7108.199,
      "duration": 4.92
    },
    {
      "lang": "en",
      "text": "gigabytes of hbm memory on a typical GPU",
      "offset": 7110.56,
      "duration": 4.519
    },
    {
      "lang": "en",
      "text": "here but it's as I mentioned very",
      "offset": 7113.119,
      "duration": 5.48
    },
    {
      "lang": "en",
      "text": "expensive to access and then on the chip",
      "offset": 7115.079,
      "duration": 5.681
    },
    {
      "lang": "en",
      "text": "itself everything is extremely fast",
      "offset": 7118.599,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "within the chip but we only have couple",
      "offset": 7120.76,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "10 megabytes of memory collectively",
      "offset": 7122.84,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "throughout the Chip And so there's just",
      "offset": 7125.8,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "not enough space because the memory is",
      "offset": 7128.04,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "very expensive on the chip and so",
      "offset": 7130.119,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "there's not a lot of it but it is",
      "offset": 7132.28,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "lightning fast to access in relative",
      "offset": 7133.639,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "terms and so basically whenever we have",
      "offset": 7135.8,
      "duration": 5.24
    },
    {
      "lang": "en",
      "text": "these kernels um the more accurate",
      "offset": 7138.04,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "picture of what's Happening Here is that",
      "offset": 7141.04,
      "duration": 4.599
    },
    {
      "lang": "en",
      "text": "we take these inputs which live by",
      "offset": 7143.239,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "default on the global memory and now we",
      "offset": 7145.639,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "need to perform some calculation so we",
      "offset": 7148.119,
      "duration": 4.681
    },
    {
      "lang": "en",
      "text": "start streaming the data from the um",
      "offset": 7150.119,
      "duration": 6.241
    },
    {
      "lang": "en",
      "text": "Global memory to the uh chip we perform",
      "offset": 7152.8,
      "duration": 5.359
    },
    {
      "lang": "en",
      "text": "the calculations on the chip and then",
      "offset": 7156.36,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "stream it back and store it back to the",
      "offset": 7158.159,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "global memory right and so if we are if",
      "offset": 7159.96,
      "duration": 4.679
    },
    {
      "lang": "en",
      "text": "we don't have torch compile we are",
      "offset": 7163.159,
      "duration": 2.881
    },
    {
      "lang": "en",
      "text": "streaming the data through the chip",
      "offset": 7164.639,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "doing the calculations and saving to the",
      "offset": 7166.04,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "memory and we're doing those round trips",
      "offset": 7167.679,
      "duration": 2.761
    },
    {
      "lang": "en",
      "text": "many many",
      "offset": 7169.4,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "times but uh if it's torch compiled then",
      "offset": 7170.44,
      "duration": 4.92
    },
    {
      "lang": "en",
      "text": "we start streaming the memory as before",
      "offset": 7173.4,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "but then while we're on the chip we're",
      "offset": 7175.36,
      "duration": 5.279
    },
    {
      "lang": "en",
      "text": "we're we have a chunk of the uh data",
      "offset": 7177.4,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "that we're trying to process so that",
      "offset": 7180.639,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "chunk now lives on the chip while it's",
      "offset": 7182.599,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "on the chip it's extremely fast to",
      "offset": 7184.56,
      "duration": 3.8
    },
    {
      "lang": "en",
      "text": "operate on so if we have kernel Fusion",
      "offset": 7186.04,
      "duration": 3.88
    },
    {
      "lang": "en",
      "text": "we can do all the operations right there",
      "offset": 7188.36,
      "duration": 4.359
    },
    {
      "lang": "en",
      "text": "in an element-wise fashion and those are",
      "offset": 7189.92,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "very cheap and then we do a single round",
      "offset": 7192.719,
      "duration": 5.561
    },
    {
      "lang": "en",
      "text": "trip back to the global memory so",
      "offset": 7194.96,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "operator Fusion basically allows you to",
      "offset": 7198.28,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "keep your chunk of data on the Chip And",
      "offset": 7200,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "do lots of calculations on it before you",
      "offset": 7202.28,
      "duration": 4.439
    },
    {
      "lang": "en",
      "text": "write it back and that gives huge",
      "offset": 7204.079,
      "duration": 4.921
    },
    {
      "lang": "en",
      "text": "savings and that's why torch compile",
      "offset": 7206.719,
      "duration": 4.681
    },
    {
      "lang": "en",
      "text": "ends up being a lot faster or that's one",
      "offset": 7209,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "of the major",
      "offset": 7211.4,
      "duration": 3.319
    },
    {
      "lang": "en",
      "text": "reasons uh so again just a very brief",
      "offset": 7212.36,
      "duration": 4.2
    },
    {
      "lang": "en",
      "text": "intro to the memory hierarchy and",
      "offset": 7214.719,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "roughly what torch compile does for you",
      "offset": 7216.56,
      "duration": 4.519
    },
    {
      "lang": "en",
      "text": "now torch compile is amazing but there",
      "offset": 7219.119,
      "duration": 4.361
    },
    {
      "lang": "en",
      "text": "are operations torch compile will not",
      "offset": 7221.079,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "find and an amazing example of that is",
      "offset": 7223.48,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "Flash attention to which we turn next so",
      "offset": 7226.199,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "flash attention comes from this paper",
      "offset": 7229,
      "duration": 4.44
    },
    {
      "lang": "en",
      "text": "from uh Stanford in",
      "offset": 7230.76,
      "duration": 5.439
    },
    {
      "lang": "en",
      "text": "2022 and it's this incredible algorithm",
      "offset": 7233.44,
      "duration": 5.759
    },
    {
      "lang": "en",
      "text": "for performing attention so um and",
      "offset": 7236.199,
      "duration": 5.281
    },
    {
      "lang": "en",
      "text": "running it a lot faster so flash",
      "offset": 7239.199,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "attention will come here and we will",
      "offset": 7241.48,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "take out these four",
      "offset": 7244.719,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "lines and Flash attention implements",
      "offset": 7246.44,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "these four lines really really quickly",
      "offset": 7248.8,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "and how does it do that well flash",
      "offset": 7251.56,
      "duration": 5.639
    },
    {
      "lang": "en",
      "text": "attention is a kernel Fusion operation",
      "offset": 7253.36,
      "duration": 6.359
    },
    {
      "lang": "en",
      "text": "so you see here we have um in this",
      "offset": 7257.199,
      "duration": 5.241
    },
    {
      "lang": "en",
      "text": "diagram they're showing P torch and you",
      "offset": 7259.719,
      "duration": 5.161
    },
    {
      "lang": "en",
      "text": "have these four operations uh they're",
      "offset": 7262.44,
      "duration": 4.04
    },
    {
      "lang": "en",
      "text": "including Dropout but we are not using",
      "offset": 7264.88,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "Dropout here so we just have these four",
      "offset": 7266.48,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "lines of code here and instead of those",
      "offset": 7268.8,
      "duration": 5.16
    },
    {
      "lang": "en",
      "text": "we are fusing them into a single fused",
      "offset": 7271.84,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "kernel of flash attention so it's an",
      "offset": 7273.96,
      "duration": 5.32
    },
    {
      "lang": "en",
      "text": "it's a it's a kernel Fusion algorithm",
      "offset": 7276.639,
      "duration": 3.96
    },
    {
      "lang": "en",
      "text": "but it's a kernel Fusion that torch",
      "offset": 7279.28,
      "duration": 3.319
    },
    {
      "lang": "en",
      "text": "compile cannot find",
      "offset": 7280.599,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "and the reason that it cannot find it is",
      "offset": 7282.599,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "that it um requires an algorithmic",
      "offset": 7284.36,
      "duration": 4.52
    },
    {
      "lang": "en",
      "text": "rewrite of how attention is actually",
      "offset": 7286.76,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "implemented here in this case and what's",
      "offset": 7288.88,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "remarkable about it is that uh flash",
      "offset": 7291.32,
      "duration": 3.799
    },
    {
      "lang": "en",
      "text": "attention actually if you just count the",
      "offset": 7293.52,
      "duration": 3.559
    },
    {
      "lang": "en",
      "text": "number of flops flash attention does",
      "offset": 7295.119,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "more flops than this attention here but",
      "offset": 7297.079,
      "duration": 5.241
    },
    {
      "lang": "en",
      "text": "flash attention is actually",
      "offset": 7301.199,
      "duration": 4.201
    },
    {
      "lang": "en",
      "text": "significantly faster in fact they site",
      "offset": 7302.32,
      "duration": 6.359
    },
    {
      "lang": "en",
      "text": "7. six times faster potentially and",
      "offset": 7305.4,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "that's because it is very mindful of the",
      "offset": 7308.679,
      "duration": 5.121
    },
    {
      "lang": "en",
      "text": "memory hierarchy as I described it just",
      "offset": 7311.48,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "now and so it's very mindful about",
      "offset": 7313.8,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "what's in high bandwidth memory what's",
      "offset": 7315.96,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "in the shared memory and it is very",
      "offset": 7317.88,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "careful with how it orchestrates the",
      "offset": 7320.599,
      "duration": 4.201
    },
    {
      "lang": "en",
      "text": "computation such that we have fewer",
      "offset": 7322.04,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "reads and writes to the high bandwidth",
      "offset": 7324.8,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "memory and so even though we're doing",
      "offset": 7326.92,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "more flops the expensive part is they",
      "offset": 7328.4,
      "duration": 4.279
    },
    {
      "lang": "en",
      "text": "load and store into hbm and that's what",
      "offset": 7330.44,
      "duration": 4.84
    },
    {
      "lang": "en",
      "text": "they avoid and so in particular they do",
      "offset": 7332.679,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "not ever materialize this end byend",
      "offset": 7335.28,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "attention Matrix this ATT here a flash",
      "offset": 7337.88,
      "duration": 5.4
    },
    {
      "lang": "en",
      "text": "attention is designed such that this",
      "offset": 7341.52,
      "duration": 3.88
    },
    {
      "lang": "en",
      "text": "Matrix never gets materialized at any",
      "offset": 7343.28,
      "duration": 4.919
    },
    {
      "lang": "en",
      "text": "point and it never gets read or written",
      "offset": 7345.4,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "to the hbm and this is a very large",
      "offset": 7348.199,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "Matrix right so um because this is where",
      "offset": 7350.36,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "all the queries and keys interact and",
      "offset": 7352.92,
      "duration": 3.96
    },
    {
      "lang": "en",
      "text": "we're sort of getting",
      "offset": 7354.679,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "um for each head for each batch element",
      "offset": 7356.88,
      "duration": 5.56
    },
    {
      "lang": "en",
      "text": "we're getting a t BYT Matrix of",
      "offset": 7360.679,
      "duration": 4.761
    },
    {
      "lang": "en",
      "text": "attention which is a Million numbers",
      "offset": 7362.44,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "even for a single head at a single batch",
      "offset": 7365.44,
      "duration": 5.159
    },
    {
      "lang": "en",
      "text": "index at like so so basically this is a",
      "offset": 7367.32,
      "duration": 5.359
    },
    {
      "lang": "en",
      "text": "ton of memory and and this is never",
      "offset": 7370.599,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "materialized and the way that this is",
      "offset": 7372.679,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "achieved is that basically the",
      "offset": 7374.679,
      "duration": 4.281
    },
    {
      "lang": "en",
      "text": "fundamental algorithmic rewrite here",
      "offset": 7377.239,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "relies on this online softmax trick",
      "offset": 7378.96,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "which was proposed previously and I'll",
      "offset": 7382.119,
      "duration": 3.56
    },
    {
      "lang": "en",
      "text": "show you the paper in a bit and the",
      "offset": 7383.44,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "online softmax trick coming from a",
      "offset": 7385.679,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "previous paper um shows how you can",
      "offset": 7387.92,
      "duration": 6.159
    },
    {
      "lang": "en",
      "text": "incrementally evaluate a soft Max",
      "offset": 7390.679,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "without having to sort of realize all of",
      "offset": 7394.079,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "the inputs to the softmax to do the",
      "offset": 7396.199,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "normalization and you do that by having",
      "offset": 7398,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "these intermediate variables M and L and",
      "offset": 7399.88,
      "duration": 4.12
    },
    {
      "lang": "en",
      "text": "there's an update to them that allows",
      "offset": 7402.32,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "you to evaluate the softmax in an online",
      "offset": 7404,
      "duration": 6.96
    },
    {
      "lang": "en",
      "text": "manner um now flash attention actually",
      "offset": 7406.8,
      "duration": 5.879
    },
    {
      "lang": "en",
      "text": "so recently flash attention 2 came out",
      "offset": 7410.96,
      "duration": 3.119
    },
    {
      "lang": "en",
      "text": "as well so I have that paper up here as",
      "offset": 7412.679,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "well uh that has additional gains to how",
      "offset": 7414.079,
      "duration": 4.841
    },
    {
      "lang": "en",
      "text": "it calculates flash attention and the",
      "offset": 7416.44,
      "duration": 4.04
    },
    {
      "lang": "en",
      "text": "original paper that this is based on",
      "offset": 7418.92,
      "duration": 3.96
    },
    {
      "lang": "en",
      "text": "basically is this online normalizer",
      "offset": 7420.48,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "calculation for softmax and remarkably",
      "offset": 7422.88,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "it came out of Nvidia and it came out of",
      "offset": 7425.04,
      "duration": 5.32
    },
    {
      "lang": "en",
      "text": "it like really early 2018 so this is 4",
      "offset": 7426.88,
      "duration": 5.96
    },
    {
      "lang": "en",
      "text": "years before flash attention",
      "offset": 7430.36,
      "duration": 5.359
    },
    {
      "lang": "en",
      "text": "and this paper says that we propose a",
      "offset": 7432.84,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "way to compute the classical softmax",
      "offset": 7435.719,
      "duration": 3.321
    },
    {
      "lang": "en",
      "text": "with fewer memory accesses and",
      "offset": 7437.239,
      "duration": 3.121
    },
    {
      "lang": "en",
      "text": "hypothesize that this reduction in",
      "offset": 7439.04,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "memory accesses should improve softmax",
      "offset": 7440.36,
      "duration": 5.359
    },
    {
      "lang": "en",
      "text": "performance on actual hardware and so",
      "offset": 7442.4,
      "duration": 5.839
    },
    {
      "lang": "en",
      "text": "they are extremely correct in this",
      "offset": 7445.719,
      "duration": 4.36
    },
    {
      "lang": "en",
      "text": "hypothesis but it's really fascinating",
      "offset": 7448.239,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "to me that they're from Nvidia and that",
      "offset": 7450.079,
      "duration": 3.64
    },
    {
      "lang": "en",
      "text": "they had this realization but they",
      "offset": 7452,
      "duration": 3.239
    },
    {
      "lang": "en",
      "text": "didn't actually take it to the actual",
      "offset": 7453.719,
      "duration": 4.281
    },
    {
      "lang": "en",
      "text": "flash attention that had to come four",
      "offset": 7455.239,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "years later from Stanford so I don't",
      "offset": 7458,
      "duration": 4.36
    },
    {
      "lang": "en",
      "text": "fully understand the historical how this",
      "offset": 7460.04,
      "duration": 4.84
    },
    {
      "lang": "en",
      "text": "happened historically um but they do",
      "offset": 7462.36,
      "duration": 4.279
    },
    {
      "lang": "en",
      "text": "basically propose this online update to",
      "offset": 7464.88,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "the softmax uh right here and this is",
      "offset": 7466.639,
      "duration": 5.321
    },
    {
      "lang": "en",
      "text": "fundamentally what they reuse here to",
      "offset": 7469.52,
      "duration": 3.96
    },
    {
      "lang": "en",
      "text": "calculate the softmax in a streaming",
      "offset": 7471.96,
      "duration": 3.4
    },
    {
      "lang": "en",
      "text": "Manner and then they realize they can",
      "offset": 7473.48,
      "duration": 4.36
    },
    {
      "lang": "en",
      "text": "actually fuse all the other operations",
      "offset": 7475.36,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "with the online sofx calculation into a",
      "offset": 7477.84,
      "duration": 4.759
    },
    {
      "lang": "en",
      "text": "single fused kernel flash attention and",
      "offset": 7480.159,
      "duration": 5.721
    },
    {
      "lang": "en",
      "text": "that's what we are about to use so great",
      "offset": 7482.599,
      "duration": 5.361
    },
    {
      "lang": "en",
      "text": "example I think of being aware of um",
      "offset": 7485.88,
      "duration": 4.04
    },
    {
      "lang": "en",
      "text": "memory hierarchy the fact that flops",
      "offset": 7487.96,
      "duration": 4.44
    },
    {
      "lang": "en",
      "text": "don't matter uh the entire memory access",
      "offset": 7489.92,
      "duration": 4.759
    },
    {
      "lang": "en",
      "text": "pattern matters and that torch compile",
      "offset": 7492.4,
      "duration": 3.279
    },
    {
      "lang": "en",
      "text": "is amazing but there are many",
      "offset": 7494.679,
      "duration": 2.361
    },
    {
      "lang": "en",
      "text": "optimizations that are still available",
      "offset": 7495.679,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "to us that potentially torch compile",
      "offset": 7497.04,
      "duration": 4.599
    },
    {
      "lang": "en",
      "text": "cannot find maybe maybe one day it could",
      "offset": 7499.119,
      "duration": 5.56
    },
    {
      "lang": "en",
      "text": "but right now it seems like a lot to ask",
      "offset": 7501.639,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "so here's what we're going to do we're",
      "offset": 7504.679,
      "duration": 4.681
    },
    {
      "lang": "en",
      "text": "going to use Flash attention and the way",
      "offset": 7505.8,
      "duration": 5.96
    },
    {
      "lang": "en",
      "text": "to do that basically in pytorch is we",
      "offset": 7509.36,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "are going to comment out these four",
      "offset": 7511.76,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "lines and we're going to replace them",
      "offset": 7514,
      "duration": 4.36
    },
    {
      "lang": "en",
      "text": "with a single line and here we are",
      "offset": 7515.92,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "calling this compound operation in",
      "offset": 7518.36,
      "duration": 4.6
    },
    {
      "lang": "en",
      "text": "pytorch called scale that product",
      "offset": 7520.32,
      "duration": 7.16
    },
    {
      "lang": "en",
      "text": "attention and uh pytorch will call flash",
      "offset": 7522.96,
      "duration": 8.04
    },
    {
      "lang": "en",
      "text": "attention when you use it in this way",
      "offset": 7527.48,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "I'm not actually 100% sure why torch",
      "offset": 7531,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "compile doesn't realize that these four",
      "offset": 7532.679,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "lines should just call flash attention",
      "offset": 7534.199,
      "duration": 4.121
    },
    {
      "lang": "en",
      "text": "in this exact way we have to do it again",
      "offset": 7536.04,
      "duration": 4.76
    },
    {
      "lang": "en",
      "text": "for it which in my opinion is a little",
      "offset": 7538.32,
      "duration": 8
    },
    {
      "lang": "en",
      "text": "bit odd but um here we are so you have",
      "offset": 7540.8,
      "duration": 8.2
    },
    {
      "lang": "en",
      "text": "to use this compound up and uh let's",
      "offset": 7546.32,
      "duration": 5.319
    },
    {
      "lang": "en",
      "text": "wait for a few moments before torch comp",
      "offset": 7549,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "compile gets around to it and then let's",
      "offset": 7551.639,
      "duration": 7.04
    },
    {
      "lang": "en",
      "text": "remember that we achieved 6.05 661 I",
      "offset": 7553.639,
      "duration": 6.401
    },
    {
      "lang": "en",
      "text": "have it here that's the loss we were",
      "offset": 7558.679,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "expecting to see and we took 130",
      "offset": 7560.04,
      "duration": 5.679
    },
    {
      "lang": "en",
      "text": "milliseconds uh before this change so",
      "offset": 7563.239,
      "duration": 4.121
    },
    {
      "lang": "en",
      "text": "we're expecting to see the exact same",
      "offset": 7565.719,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "result by iteration 49 but we expect to",
      "offset": 7567.36,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "see faster runtime because Flash",
      "offset": 7570.599,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "attention is just a an algorithmic",
      "offset": 7573.28,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "rewrite and it's a faster kernel but it",
      "offset": 7574.92,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "doesn't actually change any of the",
      "offset": 7576.639,
      "duration": 2.721
    },
    {
      "lang": "en",
      "text": "computation and we should have the exact",
      "offset": 7577.92,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "same optimization so okay so we're a lot",
      "offset": 7579.36,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "faster we're at about 95 milliseconds",
      "offset": 7581.84,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "and we achiev",
      "offset": 7584.88,
      "duration": 6.4
    },
    {
      "lang": "en",
      "text": "6.58 okay so they're basically identical",
      "offset": 7588,
      "duration": 6.36
    },
    {
      "lang": "en",
      "text": "up to a floating Point fudge Factor so",
      "offset": 7591.28,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "it's the identical computation but it's",
      "offset": 7594.36,
      "duration": 4.839
    },
    {
      "lang": "en",
      "text": "significantly faster going from 130 to",
      "offset": 7596.4,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "roughly 90",
      "offset": 7599.199,
      "duration": 5.601
    },
    {
      "lang": "en",
      "text": "96 and so this is um 96 divide",
      "offset": 7600.639,
      "duration": 8.241
    },
    {
      "lang": "en",
      "text": "130ish so this is maybe 27 is%",
      "offset": 7604.8,
      "duration": 7.68
    },
    {
      "lang": "en",
      "text": "Improvement um so uh really interesting",
      "offset": 7608.88,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "and that is Flash retention okay we are",
      "offset": 7612.48,
      "duration": 4.52
    },
    {
      "lang": "en",
      "text": "now getting to one of my favorite",
      "offset": 7614.96,
      "duration": 4.599
    },
    {
      "lang": "en",
      "text": "optimizations and it is simultaneously",
      "offset": 7617,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "the dumbest and the most brilliant",
      "offset": 7619.559,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "optimization and it's always a little",
      "offset": 7622.04,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "bit surprising to me um anyway so",
      "offset": 7623.96,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "basically I mentioned a few minutes ago",
      "offset": 7626.28,
      "duration": 4.359
    },
    {
      "lang": "en",
      "text": "that there are some numbers that are",
      "offset": 7628.199,
      "duration": 5.121
    },
    {
      "lang": "en",
      "text": "nice and some numbers that are ugly so",
      "offset": 7630.639,
      "duration": 6.44
    },
    {
      "lang": "en",
      "text": "64 is a beautiful nice number 128 is",
      "offset": 7633.32,
      "duration": 6.799
    },
    {
      "lang": "en",
      "text": "even nicer 256 is beautiful what makes",
      "offset": 7637.079,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "these numbers beautiful is that there",
      "offset": 7640.119,
      "duration": 3.08
    },
    {
      "lang": "en",
      "text": "are many powers of two inside them you",
      "offset": 7641.159,
      "duration": 5.161
    },
    {
      "lang": "en",
      "text": "can divide by two many times and uh",
      "offset": 7643.199,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "examples of ugly numbers are like 13 and",
      "offset": 7646.32,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "17 and something like that prime numbers",
      "offset": 7648.4,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "numbers that are not even and so on and",
      "offset": 7650.639,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "so pretty much you always want to use",
      "offset": 7652.8,
      "duration": 3.56
    },
    {
      "lang": "en",
      "text": "nice numbers in all of your code that",
      "offset": 7654.559,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "deals with neural networks or Cuda",
      "offset": 7656.36,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "because everything in Cuda Works in sort",
      "offset": 7658.32,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "of like powers of two and lots of",
      "offset": 7660.84,
      "duration": 4.279
    },
    {
      "lang": "en",
      "text": "kernels are written in terms of powers",
      "offset": 7662.88,
      "duration": 4.12
    },
    {
      "lang": "en",
      "text": "of Two And there are lots of blocks of",
      "offset": 7665.119,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "sizes 16 and uh 64 and so on so",
      "offset": 7667,
      "duration": 5.559
    },
    {
      "lang": "en",
      "text": "everything is written in those terms and",
      "offset": 7670.559,
      "duration": 3.721
    },
    {
      "lang": "en",
      "text": "you always have special case handling",
      "offset": 7672.559,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "for all kinds of uh logic that U when",
      "offset": 7674.28,
      "duration": 6.04
    },
    {
      "lang": "en",
      "text": "your inputs are not made of nice numbers",
      "offset": 7677.28,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "so let's see what that looks like",
      "offset": 7680.32,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "basically scan your code and look for",
      "offset": 7681.84,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "ugly numbers is roughly theistic so",
      "offset": 7683.52,
      "duration": 7.24
    },
    {
      "lang": "en",
      "text": "three times is kind of ugly um I'm not",
      "offset": 7686.84,
      "duration": 6.04
    },
    {
      "lang": "en",
      "text": "100% sure maybe this can be improved but",
      "offset": 7690.76,
      "duration": 4.839
    },
    {
      "lang": "en",
      "text": "this is uh this is ugly and not",
      "offset": 7692.88,
      "duration": 7.64
    },
    {
      "lang": "en",
      "text": "ideal um four times is nice so that's uh",
      "offset": 7695.599,
      "duration": 7
    },
    {
      "lang": "en",
      "text": "that's nice",
      "offset": 7700.52,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "1024 is very nice that's a power of two",
      "offset": 7702.599,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "12 is a little bit suspicious um not too",
      "offset": 7705.559,
      "duration": 6.68
    },
    {
      "lang": "en",
      "text": "many powers of two 768 is great 50, 257",
      "offset": 7708.599,
      "duration": 7.441
    },
    {
      "lang": "en",
      "text": "is a really really ugly number um it's",
      "offset": 7712.239,
      "duration": 6.44
    },
    {
      "lang": "en",
      "text": "first of all it's odd so uh and there's",
      "offset": 7716.04,
      "duration": 4.76
    },
    {
      "lang": "en",
      "text": "no not too many powers of two in there",
      "offset": 7718.679,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "so this is a very ugly number and it's",
      "offset": 7720.8,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "highly suspicious and then when we",
      "offset": 7723.559,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "scroll down all these numbers are nice",
      "offset": 7725.36,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "and then here we have mostly nice",
      "offset": 7728.199,
      "duration": 5.121
    },
    {
      "lang": "en",
      "text": "numbers except for 25 so in this",
      "offset": 7730.36,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "configuration of gpt2 XL a number of",
      "offset": 7733.32,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "heads is 25 uh that's a really ugly",
      "offset": 7735.239,
      "duration": 5.041
    },
    {
      "lang": "en",
      "text": "number that's an odd number and um",
      "offset": 7737.4,
      "duration": 4.04
    },
    {
      "lang": "en",
      "text": "actually this did cause a lot of",
      "offset": 7740.28,
      "duration": 2.48
    },
    {
      "lang": "en",
      "text": "headaches for us recently when we're",
      "offset": 7741.44,
      "duration": 3.159
    },
    {
      "lang": "en",
      "text": "trying to optimize some kernels uh to",
      "offset": 7742.76,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "run this fast um and required a bunch of",
      "offset": 7744.599,
      "duration": 5.441
    },
    {
      "lang": "en",
      "text": "special case handling so basically these",
      "offset": 7747.48,
      "duration": 5.159
    },
    {
      "lang": "en",
      "text": "numbers are we have some ugly numbers",
      "offset": 7750.04,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "and some of them are easier to fix than",
      "offset": 7752.639,
      "duration": 3.281
    },
    {
      "lang": "en",
      "text": "others and in particular the voap size",
      "offset": 7753.96,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "being 50257 that's a very ugly number",
      "offset": 7755.92,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "very suspicious and we want to fix it",
      "offset": 7758.84,
      "duration": 4.2
    },
    {
      "lang": "en",
      "text": "now when you when you fix these things",
      "offset": 7760.96,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "uh one of the easy ways to do that is",
      "offset": 7763.04,
      "duration": 4.599
    },
    {
      "lang": "en",
      "text": "you basically um increase the number",
      "offset": 7764.48,
      "duration": 5.4
    },
    {
      "lang": "en",
      "text": "until it's the nearest power of two that",
      "offset": 7767.639,
      "duration": 4.681
    },
    {
      "lang": "en",
      "text": "you like so here's a much nicer number",
      "offset": 7769.88,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "it's",
      "offset": 7772.32,
      "duration": 5.16
    },
    {
      "lang": "en",
      "text": "50304 and why is that because 50304 can",
      "offset": 7773.559,
      "duration": 9.6
    },
    {
      "lang": "en",
      "text": "be divided by 8 or by 16 or by 32",
      "offset": 7777.48,
      "duration": 8.84
    },
    {
      "lang": "en",
      "text": "64 it can even be divided by 128 I think",
      "offset": 7783.159,
      "duration": 6.681
    },
    {
      "lang": "en",
      "text": "yeah so it's a very nice number um so",
      "offset": 7786.32,
      "duration": 5.64
    },
    {
      "lang": "en",
      "text": "what we're going to do here is the GPT",
      "offset": 7789.84,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "config and you see that we initialized B",
      "offset": 7791.96,
      "duration": 2.92
    },
    {
      "lang": "en",
      "text": "cap size to",
      "offset": 7793.679,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "50257 Let's override just",
      "offset": 7794.88,
      "duration": 6.96
    },
    {
      "lang": "en",
      "text": "that um element to be",
      "offset": 7798.079,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "50304 okay so everything else stays the",
      "offset": 7801.88,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "same we're just increasing our",
      "offset": 7805.239,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "vocabulary size so we're adding it's",
      "offset": 7806.76,
      "duration": 5.439
    },
    {
      "lang": "en",
      "text": "almost like we're adding fake tokens uh",
      "offset": 7809.719,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "so that book up size has powers of two",
      "offset": 7812.199,
      "duration": 4.52
    },
    {
      "lang": "en",
      "text": "inside it now actually what I'm doing",
      "offset": 7814.119,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "here by the way is I'm increasing the",
      "offset": 7816.719,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "amount of computation that our network",
      "offset": 7818.119,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "will be doing if you just count the the",
      "offset": 7819.92,
      "duration": 3.719
    },
    {
      "lang": "en",
      "text": "flops on like do the math of how many",
      "offset": 7821.48,
      "duration": 3.639
    },
    {
      "lang": "en",
      "text": "flops we're doing we're going to be",
      "offset": 7823.639,
      "duration": 3.641
    },
    {
      "lang": "en",
      "text": "doing more flops and we still have to",
      "offset": 7825.119,
      "duration": 5.241
    },
    {
      "lang": "en",
      "text": "think through whether this doesn't break",
      "offset": 7827.28,
      "duration": 5.839
    },
    {
      "lang": "en",
      "text": "anything but if I just run this uh let's",
      "offset": 7830.36,
      "duration": 5.56
    },
    {
      "lang": "en",
      "text": "see what we get uh currently this ran in",
      "offset": 7833.119,
      "duration": 5.241
    },
    {
      "lang": "en",
      "text": "maybe",
      "offset": 7835.92,
      "duration": 5.799
    },
    {
      "lang": "en",
      "text": "96.5 milliseconds per step I'm just kind",
      "offset": 7838.36,
      "duration": 5.319
    },
    {
      "lang": "en",
      "text": "of like eyeballing it and let's see what",
      "offset": 7841.719,
      "duration": 4.601
    },
    {
      "lang": "en",
      "text": "kind of a result we're going to",
      "offset": 7843.679,
      "duration": 5.721
    },
    {
      "lang": "en",
      "text": "get uh while this is compiling let's",
      "offset": 7846.32,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "think through whether our code actually",
      "offset": 7849.4,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "works okay when we increase the vocap",
      "offset": 7851.44,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "size like this let's look at where vocap",
      "offset": 7853.239,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "size is actually",
      "offset": 7855.28,
      "duration": 4.919
    },
    {
      "lang": "en",
      "text": "used so we swing up to the inet and we",
      "offset": 7857.239,
      "duration": 4.601
    },
    {
      "lang": "en",
      "text": "see that it's used inside the embedding",
      "offset": 7860.199,
      "duration": 3.081
    },
    {
      "lang": "en",
      "text": "table of course so all the way at the",
      "offset": 7861.84,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "bottom of the Transformer and it's used",
      "offset": 7863.28,
      "duration": 3.48
    },
    {
      "lang": "en",
      "text": "at the classifier layer all the way at",
      "offset": 7865.199,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "the top of the Transformer so in two",
      "offset": 7866.76,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "places and let's take a look and we're",
      "offset": 7868.88,
      "duration": 5.56
    },
    {
      "lang": "en",
      "text": "running at 93 so 93 milliseconds instead",
      "offset": 7871.079,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "of",
      "offset": 7874.44,
      "duration": 5.32
    },
    {
      "lang": "en",
      "text": "96.5 so we are seeing a roughly yeah 4%",
      "offset": 7875.239,
      "duration": 7.681
    },
    {
      "lang": "en",
      "text": "Improvement here uh by doing more",
      "offset": 7879.76,
      "duration": 5.439
    },
    {
      "lang": "en",
      "text": "calculations and the reason for this is",
      "offset": 7882.92,
      "duration": 5.279
    },
    {
      "lang": "en",
      "text": "we fixed we've made an ugly number into",
      "offset": 7885.199,
      "duration": 5.601
    },
    {
      "lang": "en",
      "text": "a nice number let's I'm going to come",
      "offset": 7888.199,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "into the explanation for that a little",
      "offset": 7890.8,
      "duration": 3.399
    },
    {
      "lang": "en",
      "text": "bit again but for now let's just",
      "offset": 7892.44,
      "duration": 2.759
    },
    {
      "lang": "en",
      "text": "convince ourselves that we're not",
      "offset": 7894.199,
      "duration": 2.761
    },
    {
      "lang": "en",
      "text": "breaking anything when we do this so",
      "offset": 7895.199,
      "duration": 4.761
    },
    {
      "lang": "en",
      "text": "first of all we've made the the wte the",
      "offset": 7896.96,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "embedding table for the tokens we've",
      "offset": 7899.96,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "made it larger it's almost like we",
      "offset": 7901.36,
      "duration": 5.08
    },
    {
      "lang": "en",
      "text": "introduced more tokens at the bottom and",
      "offset": 7903.159,
      "duration": 5.601
    },
    {
      "lang": "en",
      "text": "these tokens are never used because the",
      "offset": 7906.44,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "gbt tokenizer only has tokens up to",
      "offset": 7908.76,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "$50,000",
      "offset": 7910.599,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "256 and so we'll never index into the",
      "offset": 7911.88,
      "duration": 5.799
    },
    {
      "lang": "en",
      "text": "rows that we've added so we're wasting a",
      "offset": 7915.079,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "little bit of space here by creating",
      "offset": 7917.679,
      "duration": 3.56
    },
    {
      "lang": "en",
      "text": "memory that's never going to be accessed",
      "offset": 7919.48,
      "duration": 4.36
    },
    {
      "lang": "en",
      "text": "never going to be used Etc now that's",
      "offset": 7921.239,
      "duration": 5.121
    },
    {
      "lang": "en",
      "text": "not fully correct because this wte",
      "offset": 7923.84,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "weight ends up being shared and ends up",
      "offset": 7926.36,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "being used in the classifier here at the",
      "offset": 7928.48,
      "duration": 4.52
    },
    {
      "lang": "en",
      "text": "end so what is that doing to the",
      "offset": 7930.44,
      "duration": 4.92
    },
    {
      "lang": "en",
      "text": "classifier right here well what what",
      "offset": 7933,
      "duration": 3.4
    },
    {
      "lang": "en",
      "text": "that's doing is we're predicting",
      "offset": 7935.36,
      "duration": 2.879
    },
    {
      "lang": "en",
      "text": "additional Dimensions at the classifier",
      "offset": 7936.4,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "now and we're predicting probabilities",
      "offset": 7938.239,
      "duration": 3.721
    },
    {
      "lang": "en",
      "text": "for tokens that will of course never be",
      "offset": 7940.4,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "present in the training set um and so",
      "offset": 7941.96,
      "duration": 5.279
    },
    {
      "lang": "en",
      "text": "therefore the network has to learn that",
      "offset": 7945.4,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "these probabilities uh have to be driven",
      "offset": 7947.239,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "to zero and so the logits that the",
      "offset": 7949.32,
      "duration": 4.359
    },
    {
      "lang": "en",
      "text": "network produces have to drive those",
      "offset": 7951.88,
      "duration": 3.799
    },
    {
      "lang": "en",
      "text": "dimensions of the output to negative",
      "offset": 7953.679,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "Infinity but it that's no different from",
      "offset": 7955.679,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "all the other tokens that are already in",
      "offset": 7958.079,
      "duration": 4.681
    },
    {
      "lang": "en",
      "text": "our data set um or rather that are not",
      "offset": 7959.44,
      "duration": 5.719
    },
    {
      "lang": "en",
      "text": "in our data set so Shakespeare only",
      "offset": 7962.76,
      "duration": 4.2
    },
    {
      "lang": "en",
      "text": "probably uses let's say a th000 tokens",
      "offset": 7965.159,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "out of 50,000 to 57 tokens so most of",
      "offset": 7966.96,
      "duration": 4.36
    },
    {
      "lang": "en",
      "text": "the tokens are already being driven to",
      "offset": 7969.8,
      "duration": 3.64
    },
    {
      "lang": "en",
      "text": "zero probability by the optimization we'",
      "offset": 7971.32,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "just introduced a few more tokens now",
      "offset": 7973.44,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "that in a similar manner will never be",
      "offset": 7975.48,
      "duration": 4.36
    },
    {
      "lang": "en",
      "text": "used and have to be driven to zero in",
      "offset": 7977.199,
      "duration": 5.601
    },
    {
      "lang": "en",
      "text": "probability um so functionally though",
      "offset": 7979.84,
      "duration": 5.56
    },
    {
      "lang": "en",
      "text": "nothing breaks we're using a bit more",
      "offset": 7982.8,
      "duration": 5.919
    },
    {
      "lang": "en",
      "text": "extra um memory but otherwise this is a",
      "offset": 7985.4,
      "duration": 5.719
    },
    {
      "lang": "en",
      "text": "harmless operation as far as I can tell",
      "offset": 7988.719,
      "duration": 4.201
    },
    {
      "lang": "en",
      "text": "but and we're adding calculation but",
      "offset": 7991.119,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "it's running faster and it's running",
      "offset": 7992.92,
      "duration": 4.84
    },
    {
      "lang": "en",
      "text": "faster because as I mentioned in Cuda so",
      "offset": 7994.96,
      "duration": 6.32
    },
    {
      "lang": "en",
      "text": "many kernels use uh block tiles and",
      "offset": 7997.76,
      "duration": 5.16
    },
    {
      "lang": "en",
      "text": "these block towels are usually nice",
      "offset": 8001.28,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "numbers uh so powers of two so",
      "offset": 8002.92,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "calculations are done in like chunks of",
      "offset": 8005.28,
      "duration": 5.799
    },
    {
      "lang": "en",
      "text": "64 or chunks of 32 and when your um when",
      "offset": 8006.679,
      "duration": 6.241
    },
    {
      "lang": "en",
      "text": "your desired calculation doesn't neatly",
      "offset": 8011.079,
      "duration": 5.241
    },
    {
      "lang": "en",
      "text": "fit into those block tiles um there are",
      "offset": 8012.92,
      "duration": 5.799
    },
    {
      "lang": "en",
      "text": "all kinds of boundary kernels that can",
      "offset": 8016.32,
      "duration": 5.879
    },
    {
      "lang": "en",
      "text": "kick in to like do the last part so",
      "offset": 8018.719,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "basically in a lot of kernels they will",
      "offset": 8022.199,
      "duration": 4.04
    },
    {
      "lang": "en",
      "text": "chunk at up your input and they will do",
      "offset": 8024.559,
      "duration": 3.401
    },
    {
      "lang": "en",
      "text": "the nice part first and then they have a",
      "offset": 8026.239,
      "duration": 3.88
    },
    {
      "lang": "en",
      "text": "whole second second phase where they",
      "offset": 8027.96,
      "duration": 6.04
    },
    {
      "lang": "en",
      "text": "come back to any that like uh remains uh",
      "offset": 8030.119,
      "duration": 5.96
    },
    {
      "lang": "en",
      "text": "and then they process the remaining part",
      "offset": 8034,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "and the kernels for that could be very",
      "offset": 8036.079,
      "duration": 3.961
    },
    {
      "lang": "en",
      "text": "inefficient and so you're basically um",
      "offset": 8037.28,
      "duration": 5.319
    },
    {
      "lang": "en",
      "text": "spinning up all this extra compute and",
      "offset": 8040.04,
      "duration": 4.28
    },
    {
      "lang": "en",
      "text": "is extremely inefficient so you might as",
      "offset": 8042.599,
      "duration": 5.281
    },
    {
      "lang": "en",
      "text": "well pad your inputs and um make it fit",
      "offset": 8044.32,
      "duration": 5.96
    },
    {
      "lang": "en",
      "text": "nicely and usually that empiric lens up",
      "offset": 8047.88,
      "duration": 5.719
    },
    {
      "lang": "en",
      "text": "actually running faster um so this is",
      "offset": 8050.28,
      "duration": 6.56
    },
    {
      "lang": "en",
      "text": "another example of a 4% Improvement that",
      "offset": 8053.599,
      "duration": 5.161
    },
    {
      "lang": "en",
      "text": "we've added and this is something that",
      "offset": 8056.84,
      "duration": 4.6
    },
    {
      "lang": "en",
      "text": "also torch compile did not find for us",
      "offset": 8058.76,
      "duration": 4.12
    },
    {
      "lang": "en",
      "text": "you would hope that torch compile at",
      "offset": 8061.44,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "some point could figure an optimization",
      "offset": 8062.88,
      "duration": 4.359
    },
    {
      "lang": "en",
      "text": "like this out uh but for now uh this is",
      "offset": 8064.48,
      "duration": 4.44
    },
    {
      "lang": "en",
      "text": "it and I also have to point out that",
      "offset": 8067.239,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "we're using pytorch nightly so that's",
      "offset": 8068.92,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "why we're only seeing 4% if you're using",
      "offset": 8070.92,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "pytorch 2.3.1 or earlier you would",
      "offset": 8073,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "actually see something like 30%",
      "offset": 8076.04,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "Improvement just from this change from",
      "offset": 8077.48,
      "duration": 6.4
    },
    {
      "lang": "en",
      "text": "changing it to from 50,000 to 57 to",
      "offset": 8079.96,
      "duration": 7.08
    },
    {
      "lang": "en",
      "text": "50304 so again one of my favorite",
      "offset": 8083.88,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "examples also of having to understand",
      "offset": 8087.04,
      "duration": 4.199
    },
    {
      "lang": "en",
      "text": "the under the hood and how it all works",
      "offset": 8089.079,
      "duration": 3.321
    },
    {
      "lang": "en",
      "text": "and to know what kinds of things to",
      "offset": 8091.239,
      "duration": 2.761
    },
    {
      "lang": "en",
      "text": "Tinker with to push the performance of",
      "offset": 8092.4,
      "duration": 4.279
    },
    {
      "lang": "en",
      "text": "your code okay so at this point we have",
      "offset": 8094,
      "duration": 4.92
    },
    {
      "lang": "en",
      "text": "improved the performance by about 11x",
      "offset": 8096.679,
      "duration": 4.04
    },
    {
      "lang": "en",
      "text": "right because we started at about 1,000",
      "offset": 8098.92,
      "duration": 3.719
    },
    {
      "lang": "en",
      "text": "milliseconds per step and we're now down",
      "offset": 8100.719,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "to like 93 milliseconds so that's uh",
      "offset": 8102.639,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "quite good and we're uh doing a much",
      "offset": 8105.92,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "better job of utilizing our GPU",
      "offset": 8108.159,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "resources so I'm going to now turn to",
      "offset": 8109.76,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "more algorithmic changes uh and",
      "offset": 8112.559,
      "duration": 3.801
    },
    {
      "lang": "en",
      "text": "improvements to the actual optimization",
      "offset": 8114.8,
      "duration": 3.48
    },
    {
      "lang": "en",
      "text": "itself and what we would like to do is",
      "offset": 8116.36,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "we would like to follow the hyper",
      "offset": 8118.28,
      "duration": 2.68
    },
    {
      "lang": "en",
      "text": "parameters that are mentioned in the GP",
      "offset": 8119.48,
      "duration": 6.88
    },
    {
      "lang": "en",
      "text": "G2 or gpt2 gpt3 paper now sadly gpt2 is",
      "offset": 8120.96,
      "duration": 7.92
    },
    {
      "lang": "en",
      "text": "uh doesn't actually say too much it's",
      "offset": 8126.36,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "very nice of them that they released the",
      "offset": 8128.88,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "model weights and the code but the paper",
      "offset": 8130.28,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "itself is extremely vague as to the",
      "offset": 8132.4,
      "duration": 3.88
    },
    {
      "lang": "en",
      "text": "optimization details uh the code itself",
      "offset": 8133.8,
      "duration": 5.08
    },
    {
      "lang": "en",
      "text": "that they released as well the code",
      "offset": 8136.28,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "we've been looking at this is just the",
      "offset": 8138.88,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "inference code so there's no training",
      "offset": 8140.28,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "code here and very few hyp parameters so",
      "offset": 8141.92,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "this doesn't also tell us too much so",
      "offset": 8144.28,
      "duration": 3.879
    },
    {
      "lang": "en",
      "text": "for that we have to turn to the gpt3",
      "offset": 8146.239,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "paper and um in the depending of the",
      "offset": 8148.159,
      "duration": 6.92
    },
    {
      "lang": "en",
      "text": "gpt3 paper um they have a lot more hyper",
      "offset": 8151.44,
      "duration": 5.56
    },
    {
      "lang": "en",
      "text": "parameters here for us to use and the",
      "offset": 8155.079,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "gpt3 paper in general is a lot more",
      "offset": 8157,
      "duration": 5.96
    },
    {
      "lang": "en",
      "text": "detailed as to uh all of the you know",
      "offset": 8159.639,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "small details that go into the model",
      "offset": 8162.96,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "training but gpt3 U models were never",
      "offset": 8164.28,
      "duration": 5.72
    },
    {
      "lang": "en",
      "text": "released so gbt2 we have the weights but",
      "offset": 8167.199,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "no details and gpt3 we have lots of",
      "offset": 8170,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "details but no weights so um but roughly",
      "offset": 8171.92,
      "duration": 5.799
    },
    {
      "lang": "en",
      "text": "speaking gpt2 and gpt3 architectures are",
      "offset": 8175.52,
      "duration": 5.96
    },
    {
      "lang": "en",
      "text": "very very similar and um basically there",
      "offset": 8177.719,
      "duration": 5.321
    },
    {
      "lang": "en",
      "text": "are very few changes the context length",
      "offset": 8181.48,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "was expanded from 1024 to 2048 and",
      "offset": 8183.04,
      "duration": 5.119
    },
    {
      "lang": "en",
      "text": "that's kind of like the major change uh",
      "offset": 8185.96,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "and some of the hyper parameters around",
      "offset": 8188.159,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "the Transformer have changed but",
      "offset": 8189.48,
      "duration": 2.839
    },
    {
      "lang": "en",
      "text": "otherwise they're pretty much the same",
      "offset": 8191.119,
      "duration": 3.041
    },
    {
      "lang": "en",
      "text": "model it's just that gpt3 was trained",
      "offset": 8192.319,
      "duration": 4.2
    },
    {
      "lang": "en",
      "text": "for a lot longer on a bigger data set",
      "offset": 8194.16,
      "duration": 4.6
    },
    {
      "lang": "en",
      "text": "and uh has a lot more thorough",
      "offset": 8196.519,
      "duration": 5.521
    },
    {
      "lang": "en",
      "text": "evaluations uh and the gpt3 model is 175",
      "offset": 8198.76,
      "duration": 7.839
    },
    {
      "lang": "en",
      "text": "billion instead of 1.6 billion um in the",
      "offset": 8202.04,
      "duration": 7.12
    },
    {
      "lang": "en",
      "text": "gpt2 so long story short we're going to",
      "offset": 8206.599,
      "duration": 4.441
    },
    {
      "lang": "en",
      "text": "go to gp3 paper to follow along some the",
      "offset": 8209.16,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "hyper parameters so to train all the",
      "offset": 8211.04,
      "duration": 5.599
    },
    {
      "lang": "en",
      "text": "versions of gpt3 we use atom with beta 1",
      "offset": 8214.28,
      "duration": 6.119
    },
    {
      "lang": "en",
      "text": "beta 2 of9 and .95 so let's swing over",
      "offset": 8216.639,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "here and make sure that the betas",
      "offset": 8220.399,
      "duration": 4.12
    },
    {
      "lang": "en",
      "text": "parameter which you can see here",
      "offset": 8222.639,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "defaults to 0.9 and",
      "offset": 8224.519,
      "duration": 6.92
    },
    {
      "lang": "en",
      "text": "999 is actually set to 0.9 and",
      "offset": 8226.639,
      "duration": 8.241
    },
    {
      "lang": "en",
      "text": ".95 and then the Epsilon parameter uh",
      "offset": 8231.439,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "you can see is the default is 1 in8 and",
      "offset": 8234.88,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "this is also one in8 let's just uh put",
      "offset": 8237.679,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "it in so that works",
      "offset": 8239.92,
      "duration": 5.32
    },
    {
      "lang": "en",
      "text": "expit uh now next up they say we clip",
      "offset": 8242.08,
      "duration": 5.8
    },
    {
      "lang": "en",
      "text": "the gra Global Norm of the gradient at",
      "offset": 8245.24,
      "duration": 5.399
    },
    {
      "lang": "en",
      "text": "1.0 so what this is referring to is that",
      "offset": 8247.88,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "once we calculate the gradients right",
      "offset": 8250.639,
      "duration": 5.161
    },
    {
      "lang": "en",
      "text": "after l. backward um we basically have",
      "offset": 8252.439,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "the gradients at all the parameter",
      "offset": 8255.8,
      "duration": 5.16
    },
    {
      "lang": "en",
      "text": "tensors and what people like to do is",
      "offset": 8257.319,
      "duration": 5.4
    },
    {
      "lang": "en",
      "text": "basically uh clip them to have some kind",
      "offset": 8260.96,
      "duration": 4.84
    },
    {
      "lang": "en",
      "text": "of a maximum Norm so in pytor this is",
      "offset": 8262.719,
      "duration": 5.521
    },
    {
      "lang": "en",
      "text": "fairly easy to do uh it's one line of",
      "offset": 8265.8,
      "duration": 4.36
    },
    {
      "lang": "en",
      "text": "code here that we have to insert right",
      "offset": 8268.24,
      "duration": 4.279
    },
    {
      "lang": "en",
      "text": "after we calcul Cal the gradients and",
      "offset": 8270.16,
      "duration": 5.319
    },
    {
      "lang": "en",
      "text": "what this utility function is doing is",
      "offset": 8272.519,
      "duration": 5.721
    },
    {
      "lang": "en",
      "text": "um it's calculating the global Norm of",
      "offset": 8275.479,
      "duration": 5.641
    },
    {
      "lang": "en",
      "text": "the parameters so every single par um",
      "offset": 8278.24,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "gradient on all the parameters you",
      "offset": 8281.12,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "square it and you add it all up and you",
      "offset": 8283.12,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "take a big square root of that and",
      "offset": 8285.04,
      "duration": 5.16
    },
    {
      "lang": "en",
      "text": "that's the norm of the parameter V",
      "offset": 8287.2,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "Vector basically it's the it's the",
      "offset": 8290.2,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "length of it if you if you'd like to",
      "offset": 8292.96,
      "duration": 3.32
    },
    {
      "lang": "en",
      "text": "look at it that way and we are basically",
      "offset": 8294.519,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "making sure that its length is no more",
      "offset": 8296.28,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "than 1.0 and we're going to clip it",
      "offset": 8298.519,
      "duration": 4.681
    },
    {
      "lang": "en",
      "text": "and the reason that people like to use",
      "offset": 8301.319,
      "duration": 3.961
    },
    {
      "lang": "en",
      "text": "this is that uh sometimes you can get",
      "offset": 8303.2,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "unlucky during your optimization maybe",
      "offset": 8305.28,
      "duration": 3.399
    },
    {
      "lang": "en",
      "text": "it's a bad data batch or something like",
      "offset": 8307.04,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "that and if you get very unlucky in the",
      "offset": 8308.679,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "batch you might get really high loss and",
      "offset": 8311.2,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "really high loss could lead to a really",
      "offset": 8313.559,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "high gradient and this could basically",
      "offset": 8315.2,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "uh shock your model and shock the",
      "offset": 8318.2,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "optimization so people like to use a",
      "offset": 8320.24,
      "duration": 5.479
    },
    {
      "lang": "en",
      "text": "gradient Norm clipping uh to prevent the",
      "offset": 8322.439,
      "duration": 6.601
    },
    {
      "lang": "en",
      "text": "model from um basically getting too big",
      "offset": 8325.719,
      "duration": 4.68
    },
    {
      "lang": "en",
      "text": "of shocks in terms of the gradient",
      "offset": 8329.04,
      "duration": 4.439
    },
    {
      "lang": "en",
      "text": "magnet ude and uh the upper bound it in",
      "offset": 8330.399,
      "duration": 5.08
    },
    {
      "lang": "en",
      "text": "this way it's a bit of a hacky solution",
      "offset": 8333.479,
      "duration": 4.441
    },
    {
      "lang": "en",
      "text": "it's about like a patch on top of like",
      "offset": 8335.479,
      "duration": 5.16
    },
    {
      "lang": "en",
      "text": "deeper issues uh but uh people still do",
      "offset": 8337.92,
      "duration": 5.479
    },
    {
      "lang": "en",
      "text": "it fairly frequently now the clip grad",
      "offset": 8340.639,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "Norm Returns the norm of the gradient",
      "offset": 8343.399,
      "duration": 5.521
    },
    {
      "lang": "en",
      "text": "which I like to always visualize uh",
      "offset": 8345.84,
      "duration": 5.879
    },
    {
      "lang": "en",
      "text": "because um it is useful information and",
      "offset": 8348.92,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "sometimes you can look at the norm of",
      "offset": 8351.719,
      "duration": 3.8
    },
    {
      "lang": "en",
      "text": "the gradient and if it's well behaved",
      "offset": 8353.319,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "things are good if it's climbing things",
      "offset": 8355.519,
      "duration": 3.481
    },
    {
      "lang": "en",
      "text": "are bad and they're destabilizing during",
      "offset": 8357.399,
      "duration": 3.721
    },
    {
      "lang": "en",
      "text": "training sometimes you could get a spike",
      "offset": 8359,
      "duration": 3.719
    },
    {
      "lang": "en",
      "text": "in the norm and that means there's some",
      "offset": 8361.12,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "kind of an issue or an instability so",
      "offset": 8362.719,
      "duration": 6.241
    },
    {
      "lang": "en",
      "text": "the norm here will be a",
      "offset": 8365.04,
      "duration": 8.319
    },
    {
      "lang": "en",
      "text": "norm uh and let's do a uh 4f or",
      "offset": 8368.96,
      "duration": 5.639
    },
    {
      "lang": "en",
      "text": "something like",
      "offset": 8373.359,
      "duration": 4.441
    },
    {
      "lang": "en",
      "text": "that and I believe this is just a float",
      "offset": 8374.599,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "and so we should be able to uh print",
      "offset": 8377.8,
      "duration": 6.639
    },
    {
      "lang": "en",
      "text": "that uh so that's Global gradient",
      "offset": 8380.599,
      "duration": 6.281
    },
    {
      "lang": "en",
      "text": "clipping now they go into the details of",
      "offset": 8384.439,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "the learning rate uh scheduler so they",
      "offset": 8386.88,
      "duration": 4.36
    },
    {
      "lang": "en",
      "text": "don't just use a fixed learning rate",
      "offset": 8389.64,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "like we do here for 3 E4 but there's",
      "offset": 8391.24,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "actually basically a cosine DK learning",
      "offset": 8394.04,
      "duration": 6.84
    },
    {
      "lang": "en",
      "text": "rate schedule um it's got a warm-up and",
      "offset": 8397,
      "duration": 7.96
    },
    {
      "lang": "en",
      "text": "it's got a cosine DEC to 10% over some",
      "offset": 8400.88,
      "duration": 5.64
    },
    {
      "lang": "en",
      "text": "Horizon",
      "offset": 8404.96,
      "duration": 4.6
    },
    {
      "lang": "en",
      "text": "um and so we're going to implement uh",
      "offset": 8406.52,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "this in a second I just like to see Norm",
      "offset": 8409.56,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "printed here okay there we go so what",
      "offset": 8411.88,
      "duration": 4.84
    },
    {
      "lang": "en",
      "text": "happened here is the norm is actually",
      "offset": 8414.68,
      "duration": 4.6
    },
    {
      "lang": "en",
      "text": "really high in the beginning 30 or so",
      "offset": 8416.72,
      "duration": 4.84
    },
    {
      "lang": "en",
      "text": "and you see that as we continue training",
      "offset": 8419.28,
      "duration": 3.48
    },
    {
      "lang": "en",
      "text": "it kind of like",
      "offset": 8421.56,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "stabilizes um at values below one um and",
      "offset": 8422.76,
      "duration": 7.32
    },
    {
      "lang": "en",
      "text": "this is not that crazy uncommon for the",
      "offset": 8427.56,
      "duration": 4.28
    },
    {
      "lang": "en",
      "text": "norm to be high in the very first few",
      "offset": 8430.08,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "stages basically What's Happening Here",
      "offset": 8431.84,
      "duration": 3.559
    },
    {
      "lang": "en",
      "text": "is the model is completely random and so",
      "offset": 8433.6,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "there's a ton of learning happening very",
      "offset": 8435.399,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "early in the network but that learning",
      "offset": 8437.28,
      "duration": 4.52
    },
    {
      "lang": "en",
      "text": "is kind of like um you know it's mostly",
      "offset": 8439,
      "duration": 5.319
    },
    {
      "lang": "en",
      "text": "learning the biases of the output tokens",
      "offset": 8441.8,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "and so it's a bit of an unstable time uh",
      "offset": 8444.319,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "but the network usually stabilizes in a",
      "offset": 8446.76,
      "duration": 3.88
    },
    {
      "lang": "en",
      "text": "very few iterations so this looks very",
      "offset": 8448.399,
      "duration": 3.96
    },
    {
      "lang": "en",
      "text": "relatively reasonable to me except",
      "offset": 8450.64,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "usually I would expect this looks a",
      "offset": 8452.359,
      "duration": 3.881
    },
    {
      "lang": "en",
      "text": "little bit funky that we go from 28 to 6",
      "offset": 8454.08,
      "duration": 5.359
    },
    {
      "lang": "en",
      "text": "to 2 and then to 10 um it's not",
      "offset": 8456.24,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "completely insane but it's just kind of",
      "offset": 8459.439,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "a little bit",
      "offset": 8461.439,
      "duration": 3.801
    },
    {
      "lang": "en",
      "text": "funky um okay so let's now get to the",
      "offset": 8462.479,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "learning rate schuer so the learning",
      "offset": 8465.24,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "rate schedule that's used here in gpt3",
      "offset": 8467.359,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "is what's called a cosine Decay learning",
      "offset": 8469.479,
      "duration": 5.481
    },
    {
      "lang": "en",
      "text": "schedule with warmup and the way this",
      "offset": 8472.399,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "looks is that the learning rate is",
      "offset": 8474.96,
      "duration": 4.92
    },
    {
      "lang": "en",
      "text": "basically starts right at around zero",
      "offset": 8477.12,
      "duration": 4.76
    },
    {
      "lang": "en",
      "text": "linearly rank s up over some amount of",
      "offset": 8479.88,
      "duration": 4.519
    },
    {
      "lang": "en",
      "text": "time and then comes down with this",
      "offset": 8481.88,
      "duration": 5.8
    },
    {
      "lang": "en",
      "text": "cosine sort of form and comes down to",
      "offset": 8484.399,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "some kind of a minimum learning rate",
      "offset": 8487.68,
      "duration": 2.84
    },
    {
      "lang": "en",
      "text": "that's up to you so here the minimum",
      "offset": 8488.96,
      "duration": 4.439
    },
    {
      "lang": "en",
      "text": "learning rate is zero but uh here in the",
      "offset": 8490.52,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "paper they said that they use cosine",
      "offset": 8493.399,
      "duration": 4.04
    },
    {
      "lang": "en",
      "text": "Decay for learning rate down to 10% of",
      "offset": 8495.72,
      "duration": 5.16
    },
    {
      "lang": "en",
      "text": "its value over the first 260 billion",
      "offset": 8497.439,
      "duration": 6.481
    },
    {
      "lang": "en",
      "text": "tokens and then training continues 10%",
      "offset": 8500.88,
      "duration": 5.559
    },
    {
      "lang": "en",
      "text": "after and there's a linear warmup over",
      "offset": 8503.92,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "the first 375 million tokens so that's",
      "offset": 8506.439,
      "duration": 5.601
    },
    {
      "lang": "en",
      "text": "about the learn R so let's now implement",
      "offset": 8510.08,
      "duration": 5.399
    },
    {
      "lang": "en",
      "text": "this uh so I already implemented it here",
      "offset": 8512.04,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "and the way this works is let me scroll",
      "offset": 8515.479,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "down first here I changed our training",
      "offset": 8518.2,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "Loop a little bit so this was a 4i in",
      "offset": 8520.12,
      "duration": 4.76
    },
    {
      "lang": "en",
      "text": "Max steps I just change it to step now",
      "offset": 8522.6,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "so that we have the notion of a step is",
      "offset": 8524.88,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "a single optimization step in the in the",
      "offset": 8527,
      "duration": 6.479
    },
    {
      "lang": "en",
      "text": "for Loop and then here I get the LR for",
      "offset": 8529.92,
      "duration": 5.479
    },
    {
      "lang": "en",
      "text": "this step of the optimization using a",
      "offset": 8533.479,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "new function I call get LR and then in",
      "offset": 8535.399,
      "duration": 4.841
    },
    {
      "lang": "en",
      "text": "pytorch to set the learning rate I think",
      "offset": 8538.12,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "this is is the way to set the learning",
      "offset": 8540.24,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "rate it's a little bit gnarly um because",
      "offset": 8541.64,
      "duration": 3.88
    },
    {
      "lang": "en",
      "text": "you have to basically there's a notion",
      "offset": 8544,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "of different par parameter groups that",
      "offset": 8545.52,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "could exist in the optimizer and so you",
      "offset": 8547.2,
      "duration": 3.119
    },
    {
      "lang": "en",
      "text": "actually have to iterate over them even",
      "offset": 8548.96,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "though we currently have a single param",
      "offset": 8550.319,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "group only um and you have to set the LR",
      "offset": 8552.399,
      "duration": 5.121
    },
    {
      "lang": "en",
      "text": "in this for Loop kind of style is is my",
      "offset": 8554.88,
      "duration": 4.92
    },
    {
      "lang": "en",
      "text": "impression right now so we have this",
      "offset": 8557.52,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "look of LR we set the learning rate and",
      "offset": 8559.8,
      "duration": 5.599
    },
    {
      "lang": "en",
      "text": "then on the bottom I'm also printing it",
      "offset": 8562.16,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "uh so that's all the changes I made to",
      "offset": 8565.399,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "this Loop and then of course the get LR",
      "offset": 8567.04,
      "duration": 4.279
    },
    {
      "lang": "en",
      "text": "is my scheduler now it's worth pointing",
      "offset": 8569.16,
      "duration": 4.199
    },
    {
      "lang": "en",
      "text": "out that pytorch actually has learning",
      "offset": 8571.319,
      "duration": 4.12
    },
    {
      "lang": "en",
      "text": "rate schedulers and you can use them and",
      "offset": 8573.359,
      "duration": 3.641
    },
    {
      "lang": "en",
      "text": "I believe there's a cosine learning rate",
      "offset": 8575.439,
      "duration": 4.521
    },
    {
      "lang": "en",
      "text": "schedule in pytorch I just don't really",
      "offset": 8577,
      "duration": 5.72
    },
    {
      "lang": "en",
      "text": "love using that code because honestly",
      "offset": 8579.96,
      "duration": 6.2
    },
    {
      "lang": "en",
      "text": "it's like five lines of code and I fully",
      "offset": 8582.72,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "understand what's happening inside these",
      "offset": 8586.16,
      "duration": 3.4
    },
    {
      "lang": "en",
      "text": "lines so I don't love to use",
      "offset": 8587.28,
      "duration": 4.52
    },
    {
      "lang": "en",
      "text": "abstractions where they're kind of in",
      "offset": 8589.56,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "screwable and then I don't know what",
      "offset": 8591.8,
      "duration": 4.519
    },
    {
      "lang": "en",
      "text": "they're doing so personal style so the",
      "offset": 8593.16,
      "duration": 6.4
    },
    {
      "lang": "en",
      "text": "max learning rate here is let's say 3 E4",
      "offset": 8596.319,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "but we're going to see that in gpt3",
      "offset": 8599.56,
      "duration": 5.839
    },
    {
      "lang": "en",
      "text": "here they have a table of what the",
      "offset": 8602.399,
      "duration": 5.681
    },
    {
      "lang": "en",
      "text": "maximum learning rate is for every model",
      "offset": 8605.399,
      "duration": 8.801
    },
    {
      "lang": "en",
      "text": "size so um for for this one basically 12",
      "offset": 8608.08,
      "duration": 9.84
    },
    {
      "lang": "en",
      "text": "12 layer 768 gpt3 so the gpt3 small is",
      "offset": 8614.2,
      "duration": 6.04
    },
    {
      "lang": "en",
      "text": "roughly like a GPT",
      "offset": 8617.92,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "2124m we see that here they use a",
      "offset": 8620.24,
      "duration": 4.039
    },
    {
      "lang": "en",
      "text": "learning rate of 6 E4 so we could",
      "offset": 8622.319,
      "duration": 4.201
    },
    {
      "lang": "en",
      "text": "actually go higher um in fact we may",
      "offset": 8624.279,
      "duration": 3.961
    },
    {
      "lang": "en",
      "text": "want to try to follow that and just set",
      "offset": 8626.52,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "the max LR here at six",
      "offset": 8628.24,
      "duration": 4.84
    },
    {
      "lang": "en",
      "text": "uh then the that's the maximum learning",
      "offset": 8631.319,
      "duration": 4.601
    },
    {
      "lang": "en",
      "text": "rate the minum learning rate is uh 10%",
      "offset": 8633.08,
      "duration": 5.8
    },
    {
      "lang": "en",
      "text": "of that per description in the paper",
      "offset": 8635.92,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "some number of steps that we're going to",
      "offset": 8638.88,
      "duration": 3.96
    },
    {
      "lang": "en",
      "text": "warm up over and then the maximum steps",
      "offset": 8640.479,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "of the optimization which I now use also",
      "offset": 8642.84,
      "duration": 4.84
    },
    {
      "lang": "en",
      "text": "in the for Loop down here and then you",
      "offset": 8645.12,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "can go over this code if you like it's",
      "offset": 8647.68,
      "duration": 3.799
    },
    {
      "lang": "en",
      "text": "not U it's not terribly inside Flor",
      "offset": 8649.279,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "interesting I'm just uh modulating based",
      "offset": 8651.479,
      "duration": 4.84
    },
    {
      "lang": "en",
      "text": "on the iteration number which learning",
      "offset": 8653.92,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "rate uh there should be so this is the",
      "offset": 8656.319,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "warm-up region um",
      "offset": 8658.24,
      "duration": 4.52
    },
    {
      "lang": "en",
      "text": "this is the region after the",
      "offset": 8661.279,
      "duration": 3.281
    },
    {
      "lang": "en",
      "text": "optimization and then this is the region",
      "offset": 8662.76,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "sort of in between and this is where I",
      "offset": 8664.56,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "calculate the cosine learning rate",
      "offset": 8666.359,
      "duration": 3.481
    },
    {
      "lang": "en",
      "text": "schedule and you can step through this",
      "offset": 8668.399,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "in detail if you'd like uh but this is",
      "offset": 8669.84,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "basically implementing this",
      "offset": 8672,
      "duration": 6.319
    },
    {
      "lang": "en",
      "text": "curve and I ran this already and this is",
      "offset": 8673.84,
      "duration": 6.76
    },
    {
      "lang": "en",
      "text": "what that looks",
      "offset": 8678.319,
      "duration": 6.96
    },
    {
      "lang": "en",
      "text": "like um so when we now run we start at",
      "offset": 8680.6,
      "duration": 7.2
    },
    {
      "lang": "en",
      "text": "um some very low number now note that we",
      "offset": 8685.279,
      "duration": 4.2
    },
    {
      "lang": "en",
      "text": "don't start exactly at zero because that",
      "offset": 8687.8,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "would be not useful to update with a",
      "offset": 8689.479,
      "duration": 3.121
    },
    {
      "lang": "en",
      "text": "learning rate of zero that's why there's",
      "offset": 8690.8,
      "duration": 4.12
    },
    {
      "lang": "en",
      "text": "an it+ one so that on the zeroth",
      "offset": 8692.6,
      "duration": 4.759
    },
    {
      "lang": "en",
      "text": "iteration we are not using exactly zero",
      "offset": 8694.92,
      "duration": 4.76
    },
    {
      "lang": "en",
      "text": "we're using something very very low then",
      "offset": 8697.359,
      "duration": 4.841
    },
    {
      "lang": "en",
      "text": "we linearly warm up to maximum learning",
      "offset": 8699.68,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "rate which in this case was 34 when I",
      "offset": 8702.2,
      "duration": 5.4
    },
    {
      "lang": "en",
      "text": "ran it but now would be 6 E4 and then it",
      "offset": 8704.16,
      "duration": 7.68
    },
    {
      "lang": "en",
      "text": "starts to decay all the way down to um 3",
      "offset": 8707.6,
      "duration": 6.759
    },
    {
      "lang": "en",
      "text": "E5 which was at the time 10% of the",
      "offset": 8711.84,
      "duration": 4.76
    },
    {
      "lang": "en",
      "text": "original learning rate now one thing we",
      "offset": 8714.359,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "are not following exactly is that they",
      "offset": 8716.6,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "mentioned that um",
      "offset": 8718.279,
      "duration": 5.401
    },
    {
      "lang": "en",
      "text": "let me see if I can find it",
      "offset": 8721.319,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "again we're not exactly following what",
      "offset": 8723.68,
      "duration": 4.84
    },
    {
      "lang": "en",
      "text": "they did",
      "offset": 8726.04,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "because uh they mentioned that their",
      "offset": 8728.52,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "training Horizon is 300 billion tokens",
      "offset": 8730.92,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "and they come down to 10% of the initial",
      "offset": 8733.24,
      "duration": 4.44
    },
    {
      "lang": "en",
      "text": "learning rate of at 260 billion and then",
      "offset": 8735,
      "duration": 6.12
    },
    {
      "lang": "en",
      "text": "they train after 260 with 10% so",
      "offset": 8737.68,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "basically their Decay time is less than",
      "offset": 8741.12,
      "duration": 4.12
    },
    {
      "lang": "en",
      "text": "the max steps time whereas for us",
      "offset": 8743.04,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "they're exactly equal so it's not",
      "offset": 8745.24,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "exactly faithful but it's um it's an",
      "offset": 8747.52,
      "duration": 6.2
    },
    {
      "lang": "en",
      "text": "okay um this is okay for us and for our",
      "offset": 8751,
      "duration": 6.359
    },
    {
      "lang": "en",
      "text": "purposes right now and um we're just",
      "offset": 8753.72,
      "duration": 5.16
    },
    {
      "lang": "en",
      "text": "going to use this ourselves I don't",
      "offset": 8757.359,
      "duration": 2.761
    },
    {
      "lang": "en",
      "text": "think it makes too too big of a",
      "offset": 8758.88,
      "duration": 3.24
    },
    {
      "lang": "en",
      "text": "difference honestly I should point out",
      "offset": 8760.12,
      "duration": 4.199
    },
    {
      "lang": "en",
      "text": "that what learning rate schedule you use",
      "offset": 8762.12,
      "duration": 3.8
    },
    {
      "lang": "en",
      "text": "is totally up to you there's many",
      "offset": 8764.319,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "different types um coign learning rate",
      "offset": 8765.92,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "has been popularized a lot by gpt2 and",
      "offset": 8768.96,
      "duration": 3.519
    },
    {
      "lang": "en",
      "text": "gpt3 but people have come up with all",
      "offset": 8770.72,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "kinds of uh other learning rate",
      "offset": 8772.479,
      "duration": 3.641
    },
    {
      "lang": "en",
      "text": "schedules um and this is kind of like an",
      "offset": 8774.16,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "active area of uh research as to which",
      "offset": 8776.12,
      "duration": 4.6
    },
    {
      "lang": "en",
      "text": "one is the most effective at train these",
      "offset": 8778.16,
      "duration": 5.4
    },
    {
      "lang": "en",
      "text": "networks okay next up the paper talks",
      "offset": 8780.72,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "about the gradual batch size increase so",
      "offset": 8783.56,
      "duration": 5.719
    },
    {
      "lang": "en",
      "text": "there's a ramp on the batch size that is",
      "offset": 8786.88,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "linear and you start with very small",
      "offset": 8789.279,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "batch size and you ramp up to a big",
      "offset": 8791.2,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "batch size over time uh we're going to",
      "offset": 8792.96,
      "duration": 3.479
    },
    {
      "lang": "en",
      "text": "actually skip this and we're not going",
      "offset": 8795.2,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "to work with it and the reason I don't",
      "offset": 8796.439,
      "duration": 4.601
    },
    {
      "lang": "en",
      "text": "love to use it is that it complicates a",
      "offset": 8798.8,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "lot of the arithmetic because you are",
      "offset": 8801.04,
      "duration": 2.64
    },
    {
      "lang": "en",
      "text": "changing the number of tokens that",
      "offset": 8802.479,
      "duration": 2.761
    },
    {
      "lang": "en",
      "text": "you're processing at every single step",
      "offset": 8803.68,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "of the optimization and I like to keep",
      "offset": 8805.24,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "that math very very simple also my",
      "offset": 8807.6,
      "duration": 3.16
    },
    {
      "lang": "en",
      "text": "understanding is that that this is not",
      "offset": 8809.88,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "like a major um Improvement and also my",
      "offset": 8810.76,
      "duration": 4.76
    },
    {
      "lang": "en",
      "text": "understanding is that this is not like",
      "offset": 8814.279,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "an algorithmic optimization Improvement",
      "offset": 8815.52,
      "duration": 4.04
    },
    {
      "lang": "en",
      "text": "it's more of a systems and speed",
      "offset": 8817.72,
      "duration": 4.599
    },
    {
      "lang": "en",
      "text": "Improvement and roughly speaking this is",
      "offset": 8819.56,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "because uh in the early stages of the",
      "offset": 8822.319,
      "duration": 5.321
    },
    {
      "lang": "en",
      "text": "optimization uh again the model is in a",
      "offset": 8825.56,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "very atypical setting and mostly what",
      "offset": 8827.64,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "you're learning is that um you're mostly",
      "offset": 8830.68,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "learning to ignore the tokens uh that",
      "offset": 8833.16,
      "duration": 3.64
    },
    {
      "lang": "en",
      "text": "don't come up in your training set very",
      "offset": 8835.399,
      "duration": 3.88
    },
    {
      "lang": "en",
      "text": "often you're learning very simple biases",
      "offset": 8836.8,
      "duration": 6.479
    },
    {
      "lang": "en",
      "text": "and and that kind of a thing and so",
      "offset": 8839.279,
      "duration": 5.241
    },
    {
      "lang": "en",
      "text": "every single example that you put",
      "offset": 8843.279,
      "duration": 2.961
    },
    {
      "lang": "en",
      "text": "through your network is basically just",
      "offset": 8844.52,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "telling you use these tokens and don't",
      "offset": 8846.24,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "use these tokens and so the gradients",
      "offset": 8848.359,
      "duration": 3.241
    },
    {
      "lang": "en",
      "text": "from every single example are actually",
      "offset": 8850.16,
      "duration": 3.48
    },
    {
      "lang": "en",
      "text": "extremely highly correlated they all",
      "offset": 8851.6,
      "duration": 4.759
    },
    {
      "lang": "en",
      "text": "look roughly the same in the in the OR",
      "offset": 8853.64,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "original parts of the optimization",
      "offset": 8856.359,
      "duration": 2.641
    },
    {
      "lang": "en",
      "text": "because they're all just telling you",
      "offset": 8858.04,
      "duration": 2.48
    },
    {
      "lang": "en",
      "text": "that these tokens don't appear and these",
      "offset": 8859,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "tokens do appear and so because the",
      "offset": 8860.52,
      "duration": 4.6
    },
    {
      "lang": "en",
      "text": "gradients are all very similar and",
      "offset": 8863,
      "duration": 3.8
    },
    {
      "lang": "en",
      "text": "they're highly correlated then why are",
      "offset": 8865.12,
      "duration": 4.279
    },
    {
      "lang": "en",
      "text": "you doing batch sizes of like Millions",
      "offset": 8866.8,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "when if you do a batch size of 32k",
      "offset": 8869.399,
      "duration": 3.96
    },
    {
      "lang": "en",
      "text": "you're basically getting the exact same",
      "offset": 8871.92,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "gradient early on in the training and",
      "offset": 8873.359,
      "duration": 4.521
    },
    {
      "lang": "en",
      "text": "then later in the optimization once",
      "offset": 8875.92,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "you've learned all the simple stuff",
      "offset": 8877.88,
      "duration": 3.8
    },
    {
      "lang": "en",
      "text": "that's where the actual work starts and",
      "offset": 8880.08,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "that's where the gradients become more",
      "offset": 8881.68,
      "duration": 3.24
    },
    {
      "lang": "en",
      "text": "decorrelated per examples and that's",
      "offset": 8882.88,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "where they actually offer you sort of",
      "offset": 8884.92,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "statistical power in some sense um so",
      "offset": 8887.04,
      "duration": 5.279
    },
    {
      "lang": "en",
      "text": "we're going to skip this just because it",
      "offset": 8890.84,
      "duration": 3.72
    },
    {
      "lang": "en",
      "text": "kind of complicates things and we're",
      "offset": 8892.319,
      "duration": 3.281
    },
    {
      "lang": "en",
      "text": "going to go",
      "offset": 8894.56,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "to uh data are sampled without",
      "offset": 8895.6,
      "duration": 6.04
    },
    {
      "lang": "en",
      "text": "replacement during training um so until",
      "offset": 8898.479,
      "duration": 5.161
    },
    {
      "lang": "en",
      "text": "an Epoch boundary is reached so without",
      "offset": 8901.64,
      "duration": 3.32
    },
    {
      "lang": "en",
      "text": "replacement means that they're not",
      "offset": 8903.64,
      "duration": 4.04
    },
    {
      "lang": "en",
      "text": "sampling from some fixed pool and then",
      "offset": 8904.96,
      "duration": 6.12
    },
    {
      "lang": "en",
      "text": "uh take a sequence train on it but then",
      "offset": 8907.68,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "also like return the sequence to the",
      "offset": 8911.08,
      "duration": 3.72
    },
    {
      "lang": "en",
      "text": "pool they are exhausting a pool so when",
      "offset": 8912.56,
      "duration": 4.839
    },
    {
      "lang": "en",
      "text": "they draw a sequence it's it's gone",
      "offset": 8914.8,
      "duration": 4.84
    },
    {
      "lang": "en",
      "text": "until the next Epoch of training uh so",
      "offset": 8917.399,
      "duration": 3.721
    },
    {
      "lang": "en",
      "text": "we're already doing that because our",
      "offset": 8919.64,
      "duration": 5.08
    },
    {
      "lang": "en",
      "text": "data loader um iterates over chunks of",
      "offset": 8921.12,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "data so there's no replacement they",
      "offset": 8924.72,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "don't become eligible to be drawn again",
      "offset": 8927.04,
      "duration": 4.6
    },
    {
      "lang": "en",
      "text": "until the next P so we're basically",
      "offset": 8929.279,
      "duration": 3.801
    },
    {
      "lang": "en",
      "text": "already doing",
      "offset": 8931.64,
      "duration": 5.16
    },
    {
      "lang": "en",
      "text": "that um all models use a weight decay of",
      "offset": 8933.08,
      "duration": 6.12
    },
    {
      "lang": "en",
      "text": "0.1 to provide a small amount of",
      "offset": 8936.8,
      "duration": 4.36
    },
    {
      "lang": "en",
      "text": "regularization so let's Implement a",
      "offset": 8939.2,
      "duration": 3.8
    },
    {
      "lang": "en",
      "text": "weight Decay and you see here that I've",
      "offset": 8941.16,
      "duration": 3.8
    },
    {
      "lang": "en",
      "text": "already kind of made the changes and in",
      "offset": 8943,
      "duration": 3.56
    },
    {
      "lang": "en",
      "text": "particular instead of creating the",
      "offset": 8944.96,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "optimizer right here um I I'm creating a",
      "offset": 8946.56,
      "duration": 6.32
    },
    {
      "lang": "en",
      "text": "new configure optimizers function inside",
      "offset": 8950.24,
      "duration": 4.6
    },
    {
      "lang": "en",
      "text": "the model and I'm passing in some of the",
      "offset": 8952.88,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "hyper parameters instead so let's look",
      "offset": 8954.84,
      "duration": 3.72
    },
    {
      "lang": "en",
      "text": "at the configure optimizers which is",
      "offset": 8957.12,
      "duration": 4.92
    },
    {
      "lang": "en",
      "text": "supposed to return the optimizer",
      "offset": 8958.56,
      "duration": 3.48
    },
    {
      "lang": "en",
      "text": "object okay so it looks complicated but",
      "offset": 8964.88,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "it's actually really simple and it's",
      "offset": 8967.399,
      "duration": 4.04
    },
    {
      "lang": "en",
      "text": "just um we're just being very careful",
      "offset": 8969.2,
      "duration": 3.56
    },
    {
      "lang": "en",
      "text": "and there's a few settings here to go",
      "offset": 8971.439,
      "duration": 3.161
    },
    {
      "lang": "en",
      "text": "through the most important thing with",
      "offset": 8972.76,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "respect to this line is that you see",
      "offset": 8974.6,
      "duration": 4.36
    },
    {
      "lang": "en",
      "text": "there's a weight Decay parameter here",
      "offset": 8976.52,
      "duration": 4.959
    },
    {
      "lang": "en",
      "text": "and I'm passing that",
      "offset": 8978.96,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "into um well I'm passing that into",
      "offset": 8981.479,
      "duration": 4.92
    },
    {
      "lang": "en",
      "text": "something called optim groups that",
      "offset": 8984.8,
      "duration": 3.08
    },
    {
      "lang": "en",
      "text": "eventually ends up going into the addom",
      "offset": 8986.399,
      "duration": 4.441
    },
    {
      "lang": "en",
      "text": "W Optimizer um and the weight Decay",
      "offset": 8987.88,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "that's by default used in Addam W here",
      "offset": 8990.84,
      "duration": 6.32
    },
    {
      "lang": "en",
      "text": "is 0.01 so it's it's u 10 times lower",
      "offset": 8993,
      "duration": 8.359
    },
    {
      "lang": "en",
      "text": "than what's used in gpt3 paper here um",
      "offset": 8997.16,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "so the weight dek basically ends up",
      "offset": 9001.359,
      "duration": 2.641
    },
    {
      "lang": "en",
      "text": "making its way into the ADD and W",
      "offset": 9002.6,
      "duration": 3.16
    },
    {
      "lang": "en",
      "text": "through the optimizer groups now what",
      "offset": 9004,
      "duration": 3.479
    },
    {
      "lang": "en",
      "text": "else is going on here in this uh",
      "offset": 9005.76,
      "duration": 3.32
    },
    {
      "lang": "en",
      "text": "function so the two things that are",
      "offset": 9007.479,
      "duration": 3.041
    },
    {
      "lang": "en",
      "text": "happening here that are important is",
      "offset": 9009.08,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "that I'm splitting up the parameters",
      "offset": 9010.52,
      "duration": 4.04
    },
    {
      "lang": "en",
      "text": "into those that should be weight decayed",
      "offset": 9012.68,
      "duration": 3.16
    },
    {
      "lang": "en",
      "text": "and those that should not be weight",
      "offset": 9014.56,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "decayed so in particular it is common to",
      "offset": 9015.84,
      "duration": 7.04
    },
    {
      "lang": "en",
      "text": "not weight decay uh biases and any other",
      "offset": 9018.56,
      "duration": 6.759
    },
    {
      "lang": "en",
      "text": "sort of one-dimensional tensors so the",
      "offset": 9022.88,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "one-dimensional tensors are in the no",
      "offset": 9025.319,
      "duration": 4.761
    },
    {
      "lang": "en",
      "text": "Decay prams and these are also things",
      "offset": 9027.439,
      "duration": 5.88
    },
    {
      "lang": "en",
      "text": "like uh layer Norm scales and biases it",
      "offset": 9030.08,
      "duration": 4.44
    },
    {
      "lang": "en",
      "text": "doesn't really make sense to weight",
      "offset": 9033.319,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "Decay those you mostly want to weight",
      "offset": 9034.52,
      "duration": 4.68
    },
    {
      "lang": "en",
      "text": "Decay uh the weights that participate in",
      "offset": 9036.68,
      "duration": 4.92
    },
    {
      "lang": "en",
      "text": "Matrix multiplications and you want to",
      "offset": 9039.2,
      "duration": 4.279
    },
    {
      "lang": "en",
      "text": "potentially weight Decay the",
      "offset": 9041.6,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "embeddings and uh We've covered in",
      "offset": 9043.479,
      "duration": 3.96
    },
    {
      "lang": "en",
      "text": "previous video why it makes sense to",
      "offset": 9046,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "Decay the weights because you can sort",
      "offset": 9047.439,
      "duration": 4.04
    },
    {
      "lang": "en",
      "text": "of the it as a regularization because",
      "offset": 9049.439,
      "duration": 3.721
    },
    {
      "lang": "en",
      "text": "when you're pulling down all the weights",
      "offset": 9051.479,
      "duration": 3.8
    },
    {
      "lang": "en",
      "text": "you're forcing the optimization to use",
      "offset": 9053.16,
      "duration": 4.52
    },
    {
      "lang": "en",
      "text": "more of the weights um and you're not",
      "offset": 9055.279,
      "duration": 3.961
    },
    {
      "lang": "en",
      "text": "allowing any one of the weights",
      "offset": 9057.68,
      "duration": 4.84
    },
    {
      "lang": "en",
      "text": "individually to be way too large um",
      "offset": 9059.24,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "you're forcing you're forcing the",
      "offset": 9062.52,
      "duration": 2.52
    },
    {
      "lang": "en",
      "text": "network to kind of like distribute the",
      "offset": 9063.72,
      "duration": 3.559
    },
    {
      "lang": "en",
      "text": "work across more channels because",
      "offset": 9065.04,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "there's sort of like a pull of gravity",
      "offset": 9067.279,
      "duration": 3.801
    },
    {
      "lang": "en",
      "text": "on the weights",
      "offset": 9069.359,
      "duration": 4.361
    },
    {
      "lang": "en",
      "text": "themselves um so that's why we are",
      "offset": 9071.08,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "separating it in those ways here we're",
      "offset": 9073.72,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "only decaying the embeddings and the",
      "offset": 9076.08,
      "duration": 4.92
    },
    {
      "lang": "en",
      "text": "mmal participating ways",
      "offset": 9078.279,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "uh we're printing the number of uh",
      "offset": 9081,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "parameters that we decaying and not most",
      "offset": 9082.84,
      "duration": 3.96
    },
    {
      "lang": "en",
      "text": "of the parameters will be decayed and",
      "offset": 9084.6,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "then one more thing that we're doing",
      "offset": 9086.8,
      "duration": 4.36
    },
    {
      "lang": "en",
      "text": "here is I'm doing another optimization",
      "offset": 9087.8,
      "duration": 6.76
    },
    {
      "lang": "en",
      "text": "here and previous add and W did not have",
      "offset": 9091.16,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "this option but later parts of pytorch",
      "offset": 9094.56,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "introduced it and that's why I'm",
      "offset": 9097.08,
      "duration": 4.199
    },
    {
      "lang": "en",
      "text": "guarding it with an inspect do signature",
      "offset": 9098.88,
      "duration": 4.2
    },
    {
      "lang": "en",
      "text": "which is basically checking if this",
      "offset": 9101.279,
      "duration": 7.04
    },
    {
      "lang": "en",
      "text": "fused um quar is present inside atom W",
      "offset": 9103.08,
      "duration": 6.96
    },
    {
      "lang": "en",
      "text": "and then if it is present I'm going to",
      "offset": 9108.319,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "end up using it and passing it in here",
      "offset": 9110.04,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "because some earlier versions do not",
      "offset": 9113.52,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "have fused equals so here's adamw fused",
      "offset": 9115,
      "duration": 5.64
    },
    {
      "lang": "en",
      "text": "equals it did not used to exist and it",
      "offset": 9118.8,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "was added later and there's some docks",
      "offset": 9120.64,
      "duration": 5.24
    },
    {
      "lang": "en",
      "text": "here for what's happening and basically",
      "offset": 9123.04,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "they say that by default they do not use",
      "offset": 9125.88,
      "duration": 4.519
    },
    {
      "lang": "en",
      "text": "fused because it is relatively new and",
      "offset": 9127.84,
      "duration": 4.439
    },
    {
      "lang": "en",
      "text": "we want to give it sufficient big time",
      "offset": 9130.399,
      "duration": 3.521
    },
    {
      "lang": "en",
      "text": "so by default they don't use fused but",
      "offset": 9132.279,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "fused is a lot faster when it is",
      "offset": 9133.92,
      "duration": 3.519
    },
    {
      "lang": "en",
      "text": "available and when you're running on",
      "offset": 9135.72,
      "duration": 4.52
    },
    {
      "lang": "en",
      "text": "Cuda and what that does is in instead of",
      "offset": 9137.439,
      "duration": 5.321
    },
    {
      "lang": "en",
      "text": "iterating in a for Loop over all the",
      "offset": 9140.24,
      "duration": 5.159
    },
    {
      "lang": "en",
      "text": "parameter tensors and updating them that",
      "offset": 9142.76,
      "duration": 4.599
    },
    {
      "lang": "en",
      "text": "would launch a lot of kernels right and",
      "offset": 9145.399,
      "duration": 4.601
    },
    {
      "lang": "en",
      "text": "so a fused just means that it's a um all",
      "offset": 9147.359,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "those kernels are fused into a single",
      "offset": 9150,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "kernel you get rid of a lot of overhead",
      "offset": 9151.68,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "and you a single time on all the",
      "offset": 9154.16,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "parameters call a uh kernel that updates",
      "offset": 9156.319,
      "duration": 6.12
    },
    {
      "lang": "en",
      "text": "them and so it's just basically a kernel",
      "offset": 9159.76,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "Fusion for the atom W update instead of",
      "offset": 9162.439,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "iterating over all the",
      "offset": 9164.96,
      "duration": 3.96
    },
    {
      "lang": "en",
      "text": "tensors so that's the configure",
      "offset": 9167.16,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "optimizers function that I like to use",
      "offset": 9168.92,
      "duration": 4.84
    },
    {
      "lang": "en",
      "text": "and we can rerun and we're not going to",
      "offset": 9171.8,
      "duration": 3.72
    },
    {
      "lang": "en",
      "text": "see any major differences from what we",
      "offset": 9173.76,
      "duration": 3.559
    },
    {
      "lang": "en",
      "text": "saw before but we are going to see some",
      "offset": 9175.52,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "prints uh coming from here so let's just",
      "offset": 9177.319,
      "duration": 4.441
    },
    {
      "lang": "en",
      "text": "take a look at what they look",
      "offset": 9180.24,
      "duration": 4.28
    },
    {
      "lang": "en",
      "text": "like so we see that number of Decay",
      "offset": 9181.76,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "tensors is 50 and it's most of the",
      "offset": 9184.52,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "parameters and number of non- deay",
      "offset": 9186.72,
      "duration": 3.88
    },
    {
      "lang": "en",
      "text": "tensors is 98 and these are the biases",
      "offset": 9188.52,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "and the layer Norm parameters mostly and",
      "offset": 9190.6,
      "duration": 5.16
    },
    {
      "lang": "en",
      "text": "that's there's only 100,000 of those so",
      "offset": 9193.24,
      "duration": 5.4
    },
    {
      "lang": "en",
      "text": "most of it is decayed and then we are",
      "offset": 9195.76,
      "duration": 4.84
    },
    {
      "lang": "en",
      "text": "using the fused implementation of ATM W",
      "offset": 9198.64,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "which will be a lot faster so if you",
      "offset": 9200.6,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "have it available I would advise you to",
      "offset": 9202.56,
      "duration": 4.12
    },
    {
      "lang": "en",
      "text": "use it I'm not actually 100% sure why",
      "offset": 9204.84,
      "duration": 3.72
    },
    {
      "lang": "en",
      "text": "they don't default to it it seems fairly",
      "offset": 9206.68,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "benign and",
      "offset": 9208.56,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "harmless and also because we are using",
      "offset": 9209.56,
      "duration": 4.44
    },
    {
      "lang": "en",
      "text": "the fused implementation I think this is",
      "offset": 9211.6,
      "duration": 5.879
    },
    {
      "lang": "en",
      "text": "why we have dropped um notice that the",
      "offset": 9214,
      "duration": 5.279
    },
    {
      "lang": "en",
      "text": "running time used to be 93 milliseconds",
      "offset": 9217.479,
      "duration": 3.721
    },
    {
      "lang": "en",
      "text": "per step and we're now down to 90",
      "offset": 9219.279,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "milliseconds per step because of using",
      "offset": 9221.2,
      "duration": 5.079
    },
    {
      "lang": "en",
      "text": "the fused atom W Optimizer so in a",
      "offset": 9223.12,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "single commit here we are introducing",
      "offset": 9226.279,
      "duration": 5.521
    },
    {
      "lang": "en",
      "text": "fused atom getting improvements on the",
      "offset": 9228.88,
      "duration": 5.519
    },
    {
      "lang": "en",
      "text": "time and we're adding or changing the",
      "offset": 9231.8,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "weight Decay but we're only weight",
      "offset": 9234.399,
      "duration": 3.721
    },
    {
      "lang": "en",
      "text": "decaying the two dimensional parameters",
      "offset": 9236.2,
      "duration": 3.8
    },
    {
      "lang": "en",
      "text": "the embeddings and the matrices that",
      "offset": 9238.12,
      "duration": 5.48
    },
    {
      "lang": "en",
      "text": "participate in linear so that is this",
      "offset": 9240,
      "duration": 6.92
    },
    {
      "lang": "en",
      "text": "and we can take this out and uh yeah",
      "offset": 9243.6,
      "duration": 6.6
    },
    {
      "lang": "en",
      "text": "that is it for this line one more quick",
      "offset": 9246.92,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "note before we continue here I just want",
      "offset": 9250.2,
      "duration": 3.279
    },
    {
      "lang": "en",
      "text": "to point out that the relationship",
      "offset": 9251.96,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "between weight Decay learning rate batch",
      "offset": 9253.479,
      "duration": 5.041
    },
    {
      "lang": "en",
      "text": "size the atom parameters beta 1 beta 2",
      "offset": 9255.88,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "the Epsilon and so on these are very",
      "offset": 9258.52,
      "duration": 4.12
    },
    {
      "lang": "en",
      "text": "complicated uh mathematical",
      "offset": 9260.88,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "relationships in the optimization",
      "offset": 9262.64,
      "duration": 5.16
    },
    {
      "lang": "en",
      "text": "literature and um for the most part I'm",
      "offset": 9264.319,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "in this video I'm just trying to copy",
      "offset": 9267.8,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "paste the settings that open AI used but",
      "offset": 9269.279,
      "duration": 4.2
    },
    {
      "lang": "en",
      "text": "this is a complicated topic uh quite",
      "offset": 9271.399,
      "duration": 4.841
    },
    {
      "lang": "en",
      "text": "deep and um yeah in this video I just",
      "offset": 9273.479,
      "duration": 4.601
    },
    {
      "lang": "en",
      "text": "want to copy the parameters because it's",
      "offset": 9276.24,
      "duration": 3.32
    },
    {
      "lang": "en",
      "text": "a whole different video to really talk",
      "offset": 9278.08,
      "duration": 3.239
    },
    {
      "lang": "en",
      "text": "about that in detail and give it a",
      "offset": 9279.56,
      "duration": 3.08
    },
    {
      "lang": "en",
      "text": "proper Justice instead of just high",
      "offset": 9281.319,
      "duration": 2.361
    },
    {
      "lang": "en",
      "text": "level",
      "offset": 9282.64,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "intuitions uh now the next thing that I",
      "offset": 9283.68,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "want to move on to is that uh this",
      "offset": 9285.6,
      "duration": 4.12
    },
    {
      "lang": "en",
      "text": "paragraph here by the way we're going to",
      "offset": 9288.24,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "turn back around to when we improve our",
      "offset": 9289.72,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "data loader for now I want to swing back",
      "offset": 9291.92,
      "duration": 4.359
    },
    {
      "lang": "en",
      "text": "around",
      "offset": 9294.359,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "to this",
      "offset": 9296.279,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "table where you will notice that um for",
      "offset": 9301.12,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "different models we of course have",
      "offset": 9304.24,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "different U hyper parameters for the",
      "offset": 9306.56,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "Transformer that dictate the size of the",
      "offset": 9308.08,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "Transformer Network we also have a",
      "offset": 9310,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "different learning rate so we're seeing",
      "offset": 9312,
      "duration": 2.92
    },
    {
      "lang": "en",
      "text": "the pattern that the bigger networks are",
      "offset": 9313.359,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "trained with slightly lower learning",
      "offset": 9314.92,
      "duration": 5.399
    },
    {
      "lang": "en",
      "text": "rates and we also see this batch size",
      "offset": 9316.56,
      "duration": 5.64
    },
    {
      "lang": "en",
      "text": "where in in the small networks they use",
      "offset": 9320.319,
      "duration": 3.521
    },
    {
      "lang": "en",
      "text": "a smaller batch size and in the bigger",
      "offset": 9322.2,
      "duration": 4.199
    },
    {
      "lang": "en",
      "text": "networks they use a bigger batch size",
      "offset": 9323.84,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "now the problem with for us is we can't",
      "offset": 9326.399,
      "duration": 5.361
    },
    {
      "lang": "en",
      "text": "just use 0.5 million batch size because",
      "offset": 9328.72,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "uh if I just try to come in here and I",
      "offset": 9331.76,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "try to set uh this uh B where is my",
      "offset": 9333.52,
      "duration": 6.799
    },
    {
      "lang": "en",
      "text": "b",
      "offset": 9338,
      "duration": 5.56
    },
    {
      "lang": "en",
      "text": "um b",
      "offset": 9340.319,
      "duration": 3.241
    },
    {
      "lang": "en",
      "text": "equals where where do I call the DAT",
      "offset": 9344.16,
      "duration": 6.96
    },
    {
      "lang": "en",
      "text": "okay b equal 16 if I try to set um",
      "offset": 9346.76,
      "duration": 5.599
    },
    {
      "lang": "en",
      "text": "well well we have to be careful it's not",
      "offset": 9351.12,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "0.5 million because this is the badge",
      "offset": 9352.359,
      "duration": 4.601
    },
    {
      "lang": "en",
      "text": "size in the number of tokens every",
      "offset": 9354.24,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "single one of our rows is24 tokens so",
      "offset": 9356.96,
      "duration": 7.76
    },
    {
      "lang": "en",
      "text": "0.5 E6 1 million divide 1024 this would",
      "offset": 9360.24,
      "duration": 6.119
    },
    {
      "lang": "en",
      "text": "need about a",
      "offset": 9364.72,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "488 match size so the problem is I can't",
      "offset": 9366.359,
      "duration": 6.12
    },
    {
      "lang": "en",
      "text": "come in here and set this to 488 uh",
      "offset": 9369.6,
      "duration": 5.759
    },
    {
      "lang": "en",
      "text": "because my GPU would explode um this",
      "offset": 9372.479,
      "duration": 5.561
    },
    {
      "lang": "en",
      "text": "would not fit for sure and so but we",
      "offset": 9375.359,
      "duration": 5.481
    },
    {
      "lang": "en",
      "text": "still want to use this batch size",
      "offset": 9378.04,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "because again as I mentioned the batch",
      "offset": 9380.84,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "size is correlated with all the other",
      "offset": 9382.52,
      "duration": 3.799
    },
    {
      "lang": "en",
      "text": "optimization hyper parameters and the",
      "offset": 9384.6,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "learning rates and so on so we want to",
      "offset": 9386.319,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "have a faithful representation of all",
      "offset": 9388.279,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "the hyper parameters and therefore we",
      "offset": 9389.68,
      "duration": 5.24
    },
    {
      "lang": "en",
      "text": "need to uh use a bat size of .5 million",
      "offset": 9391.399,
      "duration": 5.721
    },
    {
      "lang": "en",
      "text": "roughly but the question is how do we",
      "offset": 9394.92,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "use .5 million if we only have a small",
      "offset": 9397.12,
      "duration": 4.6
    },
    {
      "lang": "en",
      "text": "GPU well for that we need to use what's",
      "offset": 9399.399,
      "duration": 4.841
    },
    {
      "lang": "en",
      "text": "called gradient accumulation uh so we're",
      "offset": 9401.72,
      "duration": 4.36
    },
    {
      "lang": "en",
      "text": "going to turn to that next and it allows",
      "offset": 9404.24,
      "duration": 4.52
    },
    {
      "lang": "en",
      "text": "us to simulate in a Serial way any",
      "offset": 9406.08,
      "duration": 5.56
    },
    {
      "lang": "en",
      "text": "arbitrary batch size that we set and so",
      "offset": 9408.76,
      "duration": 5.48
    },
    {
      "lang": "en",
      "text": "we can do a batch size of .5 million we",
      "offset": 9411.64,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "just have to run longer and we have to",
      "offset": 9414.24,
      "duration": 5.079
    },
    {
      "lang": "en",
      "text": "process multiple sequences and basically",
      "offset": 9416.439,
      "duration": 5.561
    },
    {
      "lang": "en",
      "text": "add up all the gradients from them to",
      "offset": 9419.319,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "simulate a batch size of .5 million so",
      "offset": 9422,
      "duration": 3.96
    },
    {
      "lang": "en",
      "text": "let's turn to that next okay so I",
      "offset": 9424.2,
      "duration": 3.239
    },
    {
      "lang": "en",
      "text": "started the implementation right here",
      "offset": 9425.96,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "just by adding these lines of code and",
      "offset": 9427.439,
      "duration": 4.84
    },
    {
      "lang": "en",
      "text": "basically what I did is first I set the",
      "offset": 9429.72,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "total batch size that we desire so this",
      "offset": 9432.279,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "is exactly .5 million and I used a nice",
      "offset": 9434.72,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "number a power of two uh because 2 to",
      "offset": 9437.479,
      "duration": 6.041
    },
    {
      "lang": "en",
      "text": "the 19 is 524 288 so it's roughly .5",
      "offset": 9439.72,
      "duration": 6.639
    },
    {
      "lang": "en",
      "text": "million it's a nice number now our micro",
      "offset": 9443.52,
      "duration": 6.2
    },
    {
      "lang": "en",
      "text": "batch size as we call it now is 16 so",
      "offset": 9446.359,
      "duration": 5.801
    },
    {
      "lang": "en",
      "text": "this is going to be we still have B BYT",
      "offset": 9449.72,
      "duration": 5.08
    },
    {
      "lang": "en",
      "text": "in the SE that go into the Transformer",
      "offset": 9452.16,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "and do forward backward but we're not",
      "offset": 9454.8,
      "duration": 3.519
    },
    {
      "lang": "en",
      "text": "going to do an update right we're going",
      "offset": 9456.8,
      "duration": 3.96
    },
    {
      "lang": "en",
      "text": "to do many forward backwards we're going",
      "offset": 9458.319,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "to and those gradients are all going to",
      "offset": 9460.76,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "plus equals on the parameter gradients",
      "offset": 9462.479,
      "duration": 3.721
    },
    {
      "lang": "en",
      "text": "they're all going to add up so we're",
      "offset": 9464.439,
      "duration": 4.121
    },
    {
      "lang": "en",
      "text": "going to do forward backward grad akum",
      "offset": 9466.2,
      "duration": 4.36
    },
    {
      "lang": "en",
      "text": "steps number of times and then we're",
      "offset": 9468.56,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "going to do a single update once all",
      "offset": 9470.56,
      "duration": 2.799
    },
    {
      "lang": "en",
      "text": "that is",
      "offset": 9472.399,
      "duration": 3.321
    },
    {
      "lang": "en",
      "text": "accumulated so in particular our micro",
      "offset": 9473.359,
      "duration": 4.681
    },
    {
      "lang": "en",
      "text": "batch size is just now controlling how",
      "offset": 9475.72,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "many tokens how many rows we're",
      "offset": 9478.04,
      "duration": 4.279
    },
    {
      "lang": "en",
      "text": "processing in a single go over a forward",
      "offset": 9479.8,
      "duration": 7
    },
    {
      "lang": "en",
      "text": "backward so um here we are doing 16 *",
      "offset": 9482.319,
      "duration": 7
    },
    {
      "lang": "en",
      "text": "124 we're doing 16",
      "offset": 9486.8,
      "duration": 7.36
    },
    {
      "lang": "en",
      "text": "384 um tokens per forward backward and",
      "offset": 9489.319,
      "duration": 8.441
    },
    {
      "lang": "en",
      "text": "we are supposed to be doing 2 to the 19",
      "offset": 9494.16,
      "duration": 6.4
    },
    {
      "lang": "en",
      "text": "whoops what am I doing 2 to the",
      "offset": 9497.76,
      "duration": 7.84
    },
    {
      "lang": "en",
      "text": "19 in total so the grat Aon will be",
      "offset": 9500.56,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "32 uh so therefore gr AUM here will work",
      "offset": 9506.16,
      "duration": 6.279
    },
    {
      "lang": "en",
      "text": "out to 32 and we have to do 32 forward",
      "offset": 9508.92,
      "duration": 6.76
    },
    {
      "lang": "en",
      "text": "backward um and then a single update now",
      "offset": 9512.439,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "we see that we have about 100",
      "offset": 9515.68,
      "duration": 3.08
    },
    {
      "lang": "en",
      "text": "milliseconds for a singer forward",
      "offset": 9517.439,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "backward so doing 32 of them will be",
      "offset": 9518.76,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "will make every step roughly 3 seconds",
      "offset": 9521.6,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "just napkin",
      "offset": 9524.6,
      "duration": 4.28
    },
    {
      "lang": "en",
      "text": "math so that's grum steps but now we",
      "offset": 9526.24,
      "duration": 4.6
    },
    {
      "lang": "en",
      "text": "actually have to Implement that so we're",
      "offset": 9528.88,
      "duration": 5.88
    },
    {
      "lang": "en",
      "text": "going to swing over to our training Loop",
      "offset": 9530.84,
      "duration": 5.88
    },
    {
      "lang": "en",
      "text": "because now this part",
      "offset": 9534.76,
      "duration": 4.519
    },
    {
      "lang": "en",
      "text": "here and this part here the forward and",
      "offset": 9536.72,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "the backward we have to now repeat this",
      "offset": 9539.279,
      "duration": 5.08
    },
    {
      "lang": "en",
      "text": "32 times before we do everything else",
      "offset": 9541.279,
      "duration": 5.241
    },
    {
      "lang": "en",
      "text": "that follows so let's uh see how we can",
      "offset": 9544.359,
      "duration": 4.761
    },
    {
      "lang": "en",
      "text": "Implement that so let's come over here",
      "offset": 9546.52,
      "duration": 4.04
    },
    {
      "lang": "en",
      "text": "and actually we do have to load a new",
      "offset": 9549.12,
      "duration": 3.159
    },
    {
      "lang": "en",
      "text": "batch every single time so let me move",
      "offset": 9550.56,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "that over here and now this is where we",
      "offset": 9552.279,
      "duration": 5.921
    },
    {
      "lang": "en",
      "text": "have the inner loop so for micro step in",
      "offset": 9554.24,
      "duration": 6.44
    },
    {
      "lang": "en",
      "text": "range graum",
      "offset": 9558.2,
      "duration": 6.239
    },
    {
      "lang": "en",
      "text": "steps we do this and remember that l.",
      "offset": 9560.68,
      "duration": 5.599
    },
    {
      "lang": "en",
      "text": "backward always deposits gradients so",
      "offset": 9564.439,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "we're doing inside losta backward",
      "offset": 9566.279,
      "duration": 3.321
    },
    {
      "lang": "en",
      "text": "there's always a plus equals on the",
      "offset": 9567.88,
      "duration": 4.04
    },
    {
      "lang": "en",
      "text": "gradients so in every single L of",
      "offset": 9569.6,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "backward gradients will add up on the",
      "offset": 9571.92,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "gradient",
      "offset": 9573.92,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "tensors um so we lost that backward and",
      "offset": 9575.04,
      "duration": 6.92
    },
    {
      "lang": "en",
      "text": "then we get all the gradients over there",
      "offset": 9578.72,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "and then we normalize and everything",
      "offset": 9581.96,
      "duration": 5.319
    },
    {
      "lang": "en",
      "text": "else should just follow um so we're very",
      "offset": 9583.76,
      "duration": 6.36
    },
    {
      "lang": "en",
      "text": "close but actually there's like subtle",
      "offset": 9587.279,
      "duration": 5.281
    },
    {
      "lang": "en",
      "text": "and deep issue here and this is actually",
      "offset": 9590.12,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "incorrect so invite I invite you to",
      "offset": 9592.56,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "think about why this is not yet",
      "offset": 9594.68,
      "duration": 5.08
    },
    {
      "lang": "en",
      "text": "sufficient um and uh let me fix it then",
      "offset": 9596.16,
      "duration": 4.84
    },
    {
      "lang": "en",
      "text": "okay so I brought back the jupyter",
      "offset": 9599.76,
      "duration": 2.84
    },
    {
      "lang": "en",
      "text": "notebook so we can think about this",
      "offset": 9601,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "carefully in a simple toy setting and",
      "offset": 9602.6,
      "duration": 4.92
    },
    {
      "lang": "en",
      "text": "see what's happening so let's create a",
      "offset": 9605.08,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "very simple neural nut that takes a 16",
      "offset": 9607.52,
      "duration": 4.28
    },
    {
      "lang": "en",
      "text": "Vector of 16 numbers and returns a",
      "offset": 9610.08,
      "duration": 2.68
    },
    {
      "lang": "en",
      "text": "single",
      "offset": 9611.8,
      "duration": 3.639
    },
    {
      "lang": "en",
      "text": "number and then here I'm creating some",
      "offset": 9612.76,
      "duration": 6.36
    },
    {
      "lang": "en",
      "text": "random uh examples X and some targets uh",
      "offset": 9615.439,
      "duration": 6.441
    },
    {
      "lang": "en",
      "text": "y Y and then we are using the mean",
      "offset": 9619.12,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "squared loss uh here to calculate the",
      "offset": 9621.88,
      "duration": 6.36
    },
    {
      "lang": "en",
      "text": "loss so basically what this is is four",
      "offset": 9625.12,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "individual examples and we're just doing",
      "offset": 9628.24,
      "duration": 3.72
    },
    {
      "lang": "en",
      "text": "Simple regression with the mean squared",
      "offset": 9630.24,
      "duration": 4.199
    },
    {
      "lang": "en",
      "text": "loss over those four",
      "offset": 9631.96,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "examples now when we calculate the loss",
      "offset": 9634.439,
      "duration": 3.96
    },
    {
      "lang": "en",
      "text": "and we lost that backward and look at",
      "offset": 9636.68,
      "duration": 3.639
    },
    {
      "lang": "en",
      "text": "the gradient this is the gradient that",
      "offset": 9638.399,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "we",
      "offset": 9640.319,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "achieve now the loss objective here",
      "offset": 9641.359,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "notice that in MSE loss the default for",
      "offset": 9644.24,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "the loss function is reduction is mean",
      "offset": 9646.479,
      "duration": 5.721
    },
    {
      "lang": "en",
      "text": "so we're we're calculating the average",
      "offset": 9649.84,
      "duration": 6.68
    },
    {
      "lang": "en",
      "text": "mean loss um the the mean loss here over",
      "offset": 9652.2,
      "duration": 7.159
    },
    {
      "lang": "en",
      "text": "the four examples so this is the exact",
      "offset": 9656.52,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "loss objective and this is the average",
      "offset": 9659.359,
      "duration": 4.12
    },
    {
      "lang": "en",
      "text": "the one over four because there are four",
      "offset": 9662.04,
      "duration": 4.12
    },
    {
      "lang": "en",
      "text": "independent examples here and then we",
      "offset": 9663.479,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "have the four examples and their mean",
      "offset": 9666.16,
      "duration": 5.08
    },
    {
      "lang": "en",
      "text": "squared error the squared error and then",
      "offset": 9668.68,
      "duration": 5.4
    },
    {
      "lang": "en",
      "text": "this makes it the mean squared error so",
      "offset": 9671.24,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "therefore uh we are we calculate the",
      "offset": 9674.08,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "squared error and then we normalize it",
      "offset": 9676.6,
      "duration": 3.56
    },
    {
      "lang": "en",
      "text": "to make it the mean over the examples",
      "offset": 9678.24,
      "duration": 4.119
    },
    {
      "lang": "en",
      "text": "and there's four examples here so now",
      "offset": 9680.16,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "when we come to the gradient",
      "offset": 9682.359,
      "duration": 6.601
    },
    {
      "lang": "en",
      "text": "accumulation version of it this uh this",
      "offset": 9684.56,
      "duration": 5.879
    },
    {
      "lang": "en",
      "text": "here is the gradient accumulation",
      "offset": 9688.96,
      "duration": 3.8
    },
    {
      "lang": "en",
      "text": "version of it where we have grad acum",
      "offset": 9690.439,
      "duration": 4.84
    },
    {
      "lang": "en",
      "text": "steps of four and I reset the gradient",
      "offset": 9692.76,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "we've grum steps of four and now I'm",
      "offset": 9695.279,
      "duration": 4.441
    },
    {
      "lang": "en",
      "text": "evaluating all the examples individually",
      "offset": 9698.04,
      "duration": 3.48
    },
    {
      "lang": "en",
      "text": "instead and calling L that backward on",
      "offset": 9699.72,
      "duration": 3.48
    },
    {
      "lang": "en",
      "text": "them many times and then we're looking",
      "offset": 9701.52,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "at the gradient that we achieve from",
      "offset": 9703.2,
      "duration": 4.04
    },
    {
      "lang": "en",
      "text": "that so basically now we forward our",
      "offset": 9704.64,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "function calculate the exact same loss",
      "offset": 9707.24,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "do a backward and we do that four times",
      "offset": 9709.64,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "and when we look at the gradient uh",
      "offset": 9712.279,
      "duration": 4.841
    },
    {
      "lang": "en",
      "text": "you'll notice that the gradients don't",
      "offset": 9714.6,
      "duration": 5.719
    },
    {
      "lang": "en",
      "text": "match so here we uh did a single batch",
      "offset": 9717.12,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "of four and here we did uh four gradient",
      "offset": 9720.319,
      "duration": 5.721
    },
    {
      "lang": "en",
      "text": "accumulation steps of batch size one and",
      "offset": 9723.2,
      "duration": 5.279
    },
    {
      "lang": "en",
      "text": "the gradients are not the same and",
      "offset": 9726.04,
      "duration": 3.72
    },
    {
      "lang": "en",
      "text": "basically the the reason that they're",
      "offset": 9728.479,
      "duration": 3.121
    },
    {
      "lang": "en",
      "text": "not the same is exactly because this",
      "offset": 9729.76,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "mean squared error gets lost this one",
      "offset": 9731.6,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "quarter in this loss gets lost because",
      "offset": 9734.24,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "what happens here is the loss of",
      "offset": 9736.56,
      "duration": 5.48
    },
    {
      "lang": "en",
      "text": "objective for every one of the loops is",
      "offset": 9739.2,
      "duration": 5.88
    },
    {
      "lang": "en",
      "text": "just a mean squ error um which in this",
      "offset": 9742.04,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "case because there's only a single",
      "offset": 9745.08,
      "duration": 3.72
    },
    {
      "lang": "en",
      "text": "example is just this term here so that",
      "offset": 9746.279,
      "duration": 4.521
    },
    {
      "lang": "en",
      "text": "was the loss in the zeroth eration same",
      "offset": 9748.8,
      "duration": 5.16
    },
    {
      "lang": "en",
      "text": "in the first third and so on and then",
      "offset": 9750.8,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "when you do the loss. backward we're",
      "offset": 9753.96,
      "duration": 4.28
    },
    {
      "lang": "en",
      "text": "accumulating gradients and what happens",
      "offset": 9755.439,
      "duration": 5.241
    },
    {
      "lang": "en",
      "text": "is that accumulation in the gradient is",
      "offset": 9758.24,
      "duration": 5.48
    },
    {
      "lang": "en",
      "text": "basically equivalent to doing a sum in",
      "offset": 9760.68,
      "duration": 4.44
    },
    {
      "lang": "en",
      "text": "the",
      "offset": 9763.72,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "loss so our loss actually here is this",
      "offset": 9765.12,
      "duration": 6.84
    },
    {
      "lang": "en",
      "text": "without the factor of one quarter",
      "offset": 9769.88,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "outside of it so we're missing the",
      "offset": 9771.96,
      "duration": 4.439
    },
    {
      "lang": "en",
      "text": "normalizer and therefore our gradients",
      "offset": 9774.439,
      "duration": 3.96
    },
    {
      "lang": "en",
      "text": "are off and so the way to fix this or",
      "offset": 9776.399,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "one of them is basically we can actually",
      "offset": 9778.399,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "come here and we can say loss equals",
      "offset": 9780.24,
      "duration": 4.68
    },
    {
      "lang": "en",
      "text": "loss divide",
      "offset": 9782.56,
      "duration": 4.68
    },
    {
      "lang": "en",
      "text": "4 and what happens now is that we're",
      "offset": 9784.92,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "introducing we're we're scaling our loss",
      "offset": 9787.24,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "we're introducing a one quarter in front",
      "offset": 9789.72,
      "duration": 4.52
    },
    {
      "lang": "en",
      "text": "of all of these",
      "offset": 9791.56,
      "duration": 2.68
    },
    {
      "lang": "en",
      "text": "places so all the individual losses are",
      "offset": 9794.96,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "now scaled by one quarter and and then",
      "offset": 9797.12,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "when we backward all of these accumulate",
      "offset": 9799.279,
      "duration": 5.401
    },
    {
      "lang": "en",
      "text": "with a sum but now there's a one quarter",
      "offset": 9802,
      "duration": 4.68
    },
    {
      "lang": "en",
      "text": "inside every one of these components and",
      "offset": 9804.68,
      "duration": 4.04
    },
    {
      "lang": "en",
      "text": "now our losses will be",
      "offset": 9806.68,
      "duration": 5.639
    },
    {
      "lang": "en",
      "text": "equivalent so when I run this you see",
      "offset": 9808.72,
      "duration": 6.759
    },
    {
      "lang": "en",
      "text": "that the U gradients are now identical",
      "offset": 9812.319,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "so long story short with this simple",
      "offset": 9815.479,
      "duration": 4.121
    },
    {
      "lang": "en",
      "text": "example uh when you step through it you",
      "offset": 9817.359,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "can see that basically the reason that",
      "offset": 9819.6,
      "duration": 4.44
    },
    {
      "lang": "en",
      "text": "this is not correct is because in the",
      "offset": 9821.439,
      "duration": 5.521
    },
    {
      "lang": "en",
      "text": "same way as here in the MSE loss the",
      "offset": 9824.04,
      "duration": 6.52
    },
    {
      "lang": "en",
      "text": "loss that we're calculating here in the",
      "offset": 9826.96,
      "duration": 7.96
    },
    {
      "lang": "en",
      "text": "model is using a reduction of mean as",
      "offset": 9830.56,
      "duration": 6.6
    },
    {
      "lang": "en",
      "text": "well uh so where's the loss after that",
      "offset": 9834.92,
      "duration": 3.399
    },
    {
      "lang": "en",
      "text": "cross",
      "offset": 9837.16,
      "duration": 4.36
    },
    {
      "lang": "en",
      "text": "entropy and by default the reduction uh",
      "offset": 9838.319,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "here in Cross entropy is also I don't",
      "offset": 9841.52,
      "duration": 3.56
    },
    {
      "lang": "en",
      "text": "know why they don't show it but it's the",
      "offset": 9843.439,
      "duration": 5.081
    },
    {
      "lang": "en",
      "text": "mean uh the mean uh loss at all the B",
      "offset": 9845.08,
      "duration": 5.88
    },
    {
      "lang": "en",
      "text": "BYT elements",
      "offset": 9848.52,
      "duration": 5.08
    },
    {
      "lang": "en",
      "text": "right so there's a reduction by mean in",
      "offset": 9850.96,
      "duration": 4.439
    },
    {
      "lang": "en",
      "text": "there and if we're just doing this",
      "offset": 9853.6,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "gradient accumulation here we're missing",
      "offset": 9855.399,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "that and so the way to fix this is to",
      "offset": 9856.96,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "simply compensate for the number of",
      "offset": 9859.16,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "gradient accumulation steps and we can",
      "offset": 9861.04,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "in the same way divide this loss so in",
      "offset": 9863,
      "duration": 3.8
    },
    {
      "lang": "en",
      "text": "particular here the number of steps that",
      "offset": 9865.359,
      "duration": 5.721
    },
    {
      "lang": "en",
      "text": "we're doing is loss equals loss divide",
      "offset": 9866.8,
      "duration": 6.72
    },
    {
      "lang": "en",
      "text": "gradient accumulation steps so even uh",
      "offset": 9871.08,
      "duration": 5.48
    },
    {
      "lang": "en",
      "text": "co-pilot s gets the modification but in",
      "offset": 9873.52,
      "duration": 5.24
    },
    {
      "lang": "en",
      "text": "the same way exactly we are scaling down",
      "offset": 9876.56,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "the loss so that when we do loss that",
      "offset": 9878.76,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "backward which basically corresponds to",
      "offset": 9880.72,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "a sum in the objective we are summing up",
      "offset": 9882.52,
      "duration": 3.959
    },
    {
      "lang": "en",
      "text": "the already",
      "offset": 9885.2,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "normalized um loss and and therefore",
      "offset": 9886.479,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "when we sum up the losses divided by",
      "offset": 9889.359,
      "duration": 4.361
    },
    {
      "lang": "en",
      "text": "grum steps we are recovering the",
      "offset": 9891.12,
      "duration": 5.48
    },
    {
      "lang": "en",
      "text": "additional normalizer uh and so now",
      "offset": 9893.72,
      "duration": 5.4
    },
    {
      "lang": "en",
      "text": "these two will be now this will be",
      "offset": 9896.6,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "equivalent to the original uh sort of",
      "offset": 9899.12,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "optimization because the gradient will",
      "offset": 9901.479,
      "duration": 4.201
    },
    {
      "lang": "en",
      "text": "come out the same okay so I had to do a",
      "offset": 9903.2,
      "duration": 4.199
    },
    {
      "lang": "en",
      "text": "few more touch-ups and I launched",
      "offset": 9905.68,
      "duration": 3.799
    },
    {
      "lang": "en",
      "text": "launched the optimization here so in",
      "offset": 9907.399,
      "duration": 3.521
    },
    {
      "lang": "en",
      "text": "particular one thing we want to do",
      "offset": 9909.479,
      "duration": 3.8
    },
    {
      "lang": "en",
      "text": "because we want to print things nicely",
      "offset": 9910.92,
      "duration": 4.359
    },
    {
      "lang": "en",
      "text": "is well first of all we need to create",
      "offset": 9913.279,
      "duration": 3.641
    },
    {
      "lang": "en",
      "text": "like an accumulator over the loss we",
      "offset": 9915.279,
      "duration": 3.08
    },
    {
      "lang": "en",
      "text": "can't just print the loss because we'd",
      "offset": 9916.92,
      "duration": 3.399
    },
    {
      "lang": "en",
      "text": "be printing only the final loss at the",
      "offset": 9918.359,
      "duration": 4.601
    },
    {
      "lang": "en",
      "text": "final micro step so instead we have loss",
      "offset": 9920.319,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "ofon which I initialize at zero and then",
      "offset": 9922.96,
      "duration": 5.479
    },
    {
      "lang": "en",
      "text": "I accumulate a uh the loss into it and",
      "offset": 9925.12,
      "duration": 6.239
    },
    {
      "lang": "en",
      "text": "I'm using detach so that um uh I'm",
      "offset": 9928.439,
      "duration": 6.681
    },
    {
      "lang": "en",
      "text": "detaching the tensor uh from the graph",
      "offset": 9931.359,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "and I'm just trying to keep track of the",
      "offset": 9935.12,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "values so I'm making these Leaf nodes",
      "offset": 9936.479,
      "duration": 5.601
    },
    {
      "lang": "en",
      "text": "when I add them so that's lakum and then",
      "offset": 9938.64,
      "duration": 5.32
    },
    {
      "lang": "en",
      "text": "we're printing that here instead of loss",
      "offset": 9942.08,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "and then in addition to that I had to",
      "offset": 9943.96,
      "duration": 4.04
    },
    {
      "lang": "en",
      "text": "account for the grum steps inside the",
      "offset": 9946,
      "duration": 4.359
    },
    {
      "lang": "en",
      "text": "tokens processed because now the tokens",
      "offset": 9948,
      "duration": 6.2
    },
    {
      "lang": "en",
      "text": "processed per step is B * T * gradient",
      "offset": 9950.359,
      "duration": 6.761
    },
    {
      "lang": "en",
      "text": "accumulation so long story short here we",
      "offset": 9954.2,
      "duration": 5.32
    },
    {
      "lang": "en",
      "text": "have the optimization it looks uh",
      "offset": 9957.12,
      "duration": 3.8
    },
    {
      "lang": "en",
      "text": "reasonable right we're starting at a",
      "offset": 9959.52,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "good spot we calculated the grum steps",
      "offset": 9960.92,
      "duration": 3.399
    },
    {
      "lang": "en",
      "text": "to be",
      "offset": 9963.52,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "32 and uh we're getting about 3 seconds",
      "offset": 9964.319,
      "duration": 4.04
    },
    {
      "lang": "en",
      "text": "here",
      "offset": 9967.2,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "right",
      "offset": 9968.359,
      "duration": 3.721
    },
    {
      "lang": "en",
      "text": "um",
      "offset": 9970,
      "duration": 4.359
    },
    {
      "lang": "en",
      "text": "and so this looks pretty good now if",
      "offset": 9972.08,
      "duration": 4.44
    },
    {
      "lang": "en",
      "text": "you'd like to verify that uh your",
      "offset": 9974.359,
      "duration": 3.96
    },
    {
      "lang": "en",
      "text": "optimization and the implementation here",
      "offset": 9976.52,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "is correct and your working on a side",
      "offset": 9978.319,
      "duration": 3.521
    },
    {
      "lang": "en",
      "text": "well now because we have the total patch",
      "offset": 9980.359,
      "duration": 3.641
    },
    {
      "lang": "en",
      "text": "size and the gradient accumulation steps",
      "offset": 9981.84,
      "duration": 4.84
    },
    {
      "lang": "en",
      "text": "our setting of B is purely a performance",
      "offset": 9984,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "optimization kind of setting so if you",
      "offset": 9986.68,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "have a big GPU you can actually increase",
      "offset": 9989.6,
      "duration": 3.56
    },
    {
      "lang": "en",
      "text": "this to 32 and you'll probably go a bit",
      "offset": 9991.16,
      "duration": 3.88
    },
    {
      "lang": "en",
      "text": "faster if you have a very small GPU you",
      "offset": 9993.16,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "can try eight or four but in any case",
      "offset": 9995.04,
      "duration": 3.88
    },
    {
      "lang": "en",
      "text": "you should be getting the exact same",
      "offset": 9997.56,
      "duration": 3.719
    },
    {
      "lang": "en",
      "text": "optimization and the same answers up to",
      "offset": 9998.92,
      "duration": 4.28
    },
    {
      "lang": "en",
      "text": "like a floating Point error because the",
      "offset": 10001.279,
      "duration": 4.761
    },
    {
      "lang": "en",
      "text": "gradient accumulation kicks in and um",
      "offset": 10003.2,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "and can um handle everything serially as",
      "offset": 10006.04,
      "duration": 3.319
    },
    {
      "lang": "en",
      "text": "an",
      "offset": 10008.56,
      "duration": 3.32
    },
    {
      "lang": "en",
      "text": "Neary so uh that's it for gradient",
      "offset": 10009.359,
      "duration": 4.601
    },
    {
      "lang": "en",
      "text": "accumulation I think okay so now is the",
      "offset": 10011.88,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "time to bring out the heavy weapons uh",
      "offset": 10013.96,
      "duration": 3.56
    },
    {
      "lang": "en",
      "text": "you've noticed that so far we've only",
      "offset": 10016.2,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "been using a single GPU for training but",
      "offset": 10017.52,
      "duration": 5.24
    },
    {
      "lang": "en",
      "text": "actually I am paying for eight gpus here",
      "offset": 10020.439,
      "duration": 4.281
    },
    {
      "lang": "en",
      "text": "and so uh we should be putting all of",
      "offset": 10022.76,
      "duration": 3.88
    },
    {
      "lang": "en",
      "text": "them to work and in particular they are",
      "offset": 10024.72,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "going to collaborate and uh you know",
      "offset": 10026.64,
      "duration": 5.799
    },
    {
      "lang": "en",
      "text": "optimize over tokens at the same time",
      "offset": 10029.76,
      "duration": 5.48
    },
    {
      "lang": "en",
      "text": "and communicate so that um uh they're",
      "offset": 10032.439,
      "duration": 4.361
    },
    {
      "lang": "en",
      "text": "all kind of collaborating on the",
      "offset": 10035.24,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "optimization for this we are going to be",
      "offset": 10036.8,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "using the distributed data parallel from",
      "offset": 10038.68,
      "duration": 3.799
    },
    {
      "lang": "en",
      "text": "pytorch there's also a legacy data",
      "offset": 10040.479,
      "duration": 4.041
    },
    {
      "lang": "en",
      "text": "parallel which I recommend you not use",
      "offset": 10042.479,
      "duration": 4.601
    },
    {
      "lang": "en",
      "text": "and that's kind of like you know Legacy",
      "offset": 10044.52,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "distributed data parallel Works in a",
      "offset": 10047.08,
      "duration": 4.44
    },
    {
      "lang": "en",
      "text": "very simple way we have eight gpus so",
      "offset": 10048.68,
      "duration": 6.32
    },
    {
      "lang": "en",
      "text": "we're going to uh launch eight processes",
      "offset": 10051.52,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "and each process is going to be assigned",
      "offset": 10055,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "to GPU and for each process the training",
      "offset": 10056.64,
      "duration": 4.679
    },
    {
      "lang": "en",
      "text": "Loop and everything we've worked on so",
      "offset": 10060.04,
      "duration": 2.6
    },
    {
      "lang": "en",
      "text": "far is going to look pretty much the",
      "offset": 10061.319,
      "duration": 4.281
    },
    {
      "lang": "en",
      "text": "same H GPU as far as it's concerned is",
      "offset": 10062.64,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "just working on exactly what we've built",
      "offset": 10065.6,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "so far but now Secret L there's eight of",
      "offset": 10067.2,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "them and they're all going to be",
      "offset": 10069.92,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "processing slightly different parts of",
      "offset": 10071.359,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "the data and we're going to add one more",
      "offset": 10072.92,
      "duration": 5.559
    },
    {
      "lang": "en",
      "text": "part where once they all calculate their",
      "offset": 10076.56,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "gradients there's one more part where we",
      "offset": 10078.479,
      "duration": 4.84
    },
    {
      "lang": "en",
      "text": "do a average of those",
      "offset": 10080.479,
      "duration": 4.681
    },
    {
      "lang": "en",
      "text": "gradients and so that's how they're",
      "offset": 10083.319,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "going to be collaborating on uh the",
      "offset": 10085.16,
      "duration": 5.159
    },
    {
      "lang": "en",
      "text": "computational workload here so to use",
      "offset": 10087.72,
      "duration": 4.599
    },
    {
      "lang": "en",
      "text": "all eight of them we're not going to be",
      "offset": 10090.319,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "launching our script anymore with just",
      "offset": 10092.319,
      "duration": 4.04
    },
    {
      "lang": "en",
      "text": "um pytorch train",
      "offset": 10094.399,
      "duration": 4.681
    },
    {
      "lang": "en",
      "text": "gbt2 piy we're going to be running it",
      "offset": 10096.359,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "with a special command called torrun in",
      "offset": 10099.08,
      "duration": 4.84
    },
    {
      "lang": "en",
      "text": "pytorch we'll see that in a bit and",
      "offset": 10101,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "torrun uh when it runs our python script",
      "offset": 10103.92,
      "duration": 4.76
    },
    {
      "lang": "en",
      "text": "we'll actually make sure to run eight",
      "offset": 10106.84,
      "duration": 5.559
    },
    {
      "lang": "en",
      "text": "eight of them in parallel and it creates",
      "offset": 10108.68,
      "duration": 5.799
    },
    {
      "lang": "en",
      "text": "these environmental variables where each",
      "offset": 10112.399,
      "duration": 5.441
    },
    {
      "lang": "en",
      "text": "of these processes can look up which uh",
      "offset": 10114.479,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "basically which one of the processes it",
      "offset": 10117.84,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "is so for example torron will set rank",
      "offset": 10120.319,
      "duration": 5.761
    },
    {
      "lang": "en",
      "text": "local Rank and World size environmental",
      "offset": 10123.68,
      "duration": 5.16
    },
    {
      "lang": "en",
      "text": "variables and so this is a bad way to",
      "offset": 10126.08,
      "duration": 5.239
    },
    {
      "lang": "en",
      "text": "detect whether uh DDP is running so if",
      "offset": 10128.84,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "we're using torch run if DDP is",
      "offset": 10131.319,
      "duration": 5.761
    },
    {
      "lang": "en",
      "text": "running then uh we have to make sure",
      "offset": 10134.92,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "that K is available because I don't know",
      "offset": 10137.08,
      "duration": 3.96
    },
    {
      "lang": "en",
      "text": "that you can run this on CPU anymore or",
      "offset": 10138.76,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "that that makes sense to do um this is",
      "offset": 10141.04,
      "duration": 6.52
    },
    {
      "lang": "en",
      "text": "some um setup code here the important",
      "offset": 10145,
      "duration": 5.359
    },
    {
      "lang": "en",
      "text": "part is that there's a world size which",
      "offset": 10147.56,
      "duration": 4.36
    },
    {
      "lang": "en",
      "text": "for us will be eight that's the total",
      "offset": 10150.359,
      "duration": 3.881
    },
    {
      "lang": "en",
      "text": "number of processes running there's a",
      "offset": 10151.92,
      "duration": 5.319
    },
    {
      "lang": "en",
      "text": "rank which is um each process will",
      "offset": 10154.24,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "basically run the ex exact same code at",
      "offset": 10157.239,
      "duration": 5.481
    },
    {
      "lang": "en",
      "text": "the exact same time roughly but all the",
      "offset": 10159.92,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "process the only difference between",
      "offset": 10162.72,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "these processes is that they all have a",
      "offset": 10164.239,
      "duration": 6.2
    },
    {
      "lang": "en",
      "text": "different dtp rank so the um gpu0 will",
      "offset": 10166,
      "duration": 7.479
    },
    {
      "lang": "en",
      "text": "have DDP rank of zero GPU 1 will have uh",
      "offset": 10170.439,
      "duration": 6.161
    },
    {
      "lang": "en",
      "text": "rank of one Etc so otherwise they're all",
      "offset": 10173.479,
      "duration": 5.121
    },
    {
      "lang": "en",
      "text": "running the exact same script it's just",
      "offset": 10176.6,
      "duration": 3.48
    },
    {
      "lang": "en",
      "text": "that DDP rank will be a slightly",
      "offset": 10178.6,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "different integer and that is the way",
      "offset": 10180.08,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "for us to coordinate that they don't for",
      "offset": 10182.04,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "example run on the same data we want to",
      "offset": 10184.319,
      "duration": 3.321
    },
    {
      "lang": "en",
      "text": "we want them to run on different parts",
      "offset": 10186.359,
      "duration": 3.521
    },
    {
      "lang": "en",
      "text": "of the data and so on",
      "offset": 10187.64,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "now local rank is something that is only",
      "offset": 10189.88,
      "duration": 4.599
    },
    {
      "lang": "en",
      "text": "used in a multi- node setting we only",
      "offset": 10192.04,
      "duration": 5.08
    },
    {
      "lang": "en",
      "text": "have a single node with ag gpus and so",
      "offset": 10194.479,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "local rank is the rank of the GPU on a",
      "offset": 10197.12,
      "duration": 7.119
    },
    {
      "lang": "en",
      "text": "single node so from 0 to seven as an",
      "offset": 10200.239,
      "duration": 6.04
    },
    {
      "lang": "en",
      "text": "example but for us we're mostly going to",
      "offset": 10204.239,
      "duration": 3.801
    },
    {
      "lang": "en",
      "text": "be running on a single box so the things",
      "offset": 10206.279,
      "duration": 4.281
    },
    {
      "lang": "en",
      "text": "we care about are Rank and World size",
      "offset": 10208.04,
      "duration": 4.76
    },
    {
      "lang": "en",
      "text": "this is eight and this will be whatever",
      "offset": 10210.56,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "it is depending on the GPU uh that uh",
      "offset": 10212.8,
      "duration": 4.599
    },
    {
      "lang": "en",
      "text": "that this particular instantiation of",
      "offset": 10215.2,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "the script runs on",
      "offset": 10217.399,
      "duration": 5.96
    },
    {
      "lang": "en",
      "text": "now here we make sure that according to",
      "offset": 10219.92,
      "duration": 7.319
    },
    {
      "lang": "en",
      "text": "the local rank we are setting the device",
      "offset": 10223.359,
      "duration": 7
    },
    {
      "lang": "en",
      "text": "to be Cuda colon and colon indicates",
      "offset": 10227.239,
      "duration": 5.441
    },
    {
      "lang": "en",
      "text": "which GPU to use if there are more than",
      "offset": 10230.359,
      "duration": 6.241
    },
    {
      "lang": "en",
      "text": "one gpus so depending on the local rank",
      "offset": 10232.68,
      "duration": 6.639
    },
    {
      "lang": "en",
      "text": "of this process it's going to use just",
      "offset": 10236.6,
      "duration": 4.28
    },
    {
      "lang": "en",
      "text": "the appropriate GPU so there's no",
      "offset": 10239.319,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "collisions on which GPU is being used by",
      "offset": 10240.88,
      "duration": 2.559
    },
    {
      "lang": "en",
      "text": "which",
      "offset": 10242.52,
      "duration": 3.16
    },
    {
      "lang": "en",
      "text": "process and finally there's a Boolean",
      "offset": 10243.439,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "variable that I like to create which is",
      "offset": 10245.68,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "the DDP rank equ equal Z so the master",
      "offset": 10247.2,
      "duration": 6.079
    },
    {
      "lang": "en",
      "text": "process is arbitrarily process number",
      "offset": 10250.96,
      "duration": 4.279
    },
    {
      "lang": "en",
      "text": "zero and it does a lot of the printing",
      "offset": 10253.279,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "logging checkpointing Etc and the other",
      "offset": 10255.239,
      "duration": 3.961
    },
    {
      "lang": "en",
      "text": "processes are thought of mostly as a",
      "offset": 10257.6,
      "duration": 3.96
    },
    {
      "lang": "en",
      "text": "compute processes that are assisting and",
      "offset": 10259.2,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "so Master process zero will have some",
      "offset": 10261.56,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "additional work to do all the other",
      "offset": 10263.52,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "processes will uh will mostly just be",
      "offset": 10265.239,
      "duration": 2.761
    },
    {
      "lang": "en",
      "text": "doing forward",
      "offset": 10266.96,
      "duration": 3.399
    },
    {
      "lang": "en",
      "text": "backwards and if we're not using DDP and",
      "offset": 10268,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "none of these variables are set we",
      "offset": 10270.359,
      "duration": 3.96
    },
    {
      "lang": "en",
      "text": "revert back to single GPU training so",
      "offset": 10272,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "that means that we only have rank zero",
      "offset": 10274.319,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "the world size is just one uh and and we",
      "offset": 10276.16,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "are the master process and we try to",
      "offset": 10279.04,
      "duration": 5.16
    },
    {
      "lang": "en",
      "text": "autodetect the device and this is world",
      "offset": 10281.16,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "as",
      "offset": 10284.2,
      "duration": 3.48
    },
    {
      "lang": "en",
      "text": "normal so so far all we've done is we've",
      "offset": 10285.319,
      "duration": 3.481
    },
    {
      "lang": "en",
      "text": "initialized",
      "offset": 10287.68,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "DDP and uh in the case where we're",
      "offset": 10288.8,
      "duration": 4.36
    },
    {
      "lang": "en",
      "text": "running with torrun which we'll see in a",
      "offset": 10291.359,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "bit there's going to be eight copies",
      "offset": 10293.16,
      "duration": 3.88
    },
    {
      "lang": "en",
      "text": "running in parallel each one of them",
      "offset": 10295.279,
      "duration": 4.04
    },
    {
      "lang": "en",
      "text": "will have a different Rank and now we",
      "offset": 10297.04,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "have to make sure that everything",
      "offset": 10299.319,
      "duration": 4.761
    },
    {
      "lang": "en",
      "text": "happens uh correctly afterwards so the",
      "offset": 10301.359,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "tricky thing with running multiple",
      "offset": 10304.08,
      "duration": 3.96
    },
    {
      "lang": "en",
      "text": "processes is you always have to imagine",
      "offset": 10305.68,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "that there's going to be eight processes",
      "offset": 10308.04,
      "duration": 4.92
    },
    {
      "lang": "en",
      "text": "running in parallel so as you read the",
      "offset": 10310.56,
      "duration": 4.04
    },
    {
      "lang": "en",
      "text": "code now you have to imagine there's",
      "offset": 10312.96,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "eight you know eight python interpreters",
      "offset": 10314.6,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "running down these lines of code and the",
      "offset": 10317.439,
      "duration": 3.96
    },
    {
      "lang": "en",
      "text": "only difference between them is that",
      "offset": 10319.72,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "they have a different DDP rank so they",
      "offset": 10321.399,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "all come here they all pick the exact",
      "offset": 10323.56,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "same seed they all make all of these",
      "offset": 10325.56,
      "duration": 4.52
    },
    {
      "lang": "en",
      "text": "calculations completely unaware of the",
      "offset": 10328.04,
      "duration": 3.96
    },
    {
      "lang": "en",
      "text": "other copies running roughly speaking",
      "offset": 10330.08,
      "duration": 4.6
    },
    {
      "lang": "en",
      "text": "right so they all make the exact same",
      "offset": 10332,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "calculations and now we have to adjust",
      "offset": 10334.68,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "these calculations to take into account",
      "offset": 10336.399,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "that there's actually like a certain",
      "offset": 10339.319,
      "duration": 5.241
    },
    {
      "lang": "en",
      "text": "world size and certain ranks so in",
      "offset": 10341.04,
      "duration": 5.439
    },
    {
      "lang": "en",
      "text": "particular these micro batches and",
      "offset": 10344.56,
      "duration": 3.56
    },
    {
      "lang": "en",
      "text": "sequence lengths these are all just per",
      "offset": 10346.479,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "GPU right so now there's going to be num",
      "offset": 10348.12,
      "duration": 6.52
    },
    {
      "lang": "en",
      "text": "processes of them running in parallel so",
      "offset": 10351.68,
      "duration": 5.24
    },
    {
      "lang": "en",
      "text": "we have to adjust this right because the",
      "offset": 10354.64,
      "duration": 4.52
    },
    {
      "lang": "en",
      "text": "grum steps now is going to be total B",
      "offset": 10356.92,
      "duration": 6.6
    },
    {
      "lang": "en",
      "text": "size divide B * T time U DDP R",
      "offset": 10359.16,
      "duration": 9.48
    },
    {
      "lang": "en",
      "text": "size because each um process will will",
      "offset": 10363.52,
      "duration": 8.08
    },
    {
      "lang": "en",
      "text": "do B * T and there's this many of",
      "offset": 10368.64,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "them and so in addition to that we we",
      "offset": 10371.6,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "want to make sure that this fits nicely",
      "offset": 10374.16,
      "duration": 4.6
    },
    {
      "lang": "en",
      "text": "into total batch size which for us it",
      "offset": 10376.88,
      "duration": 7.439
    },
    {
      "lang": "en",
      "text": "will because 16 * 124 * 8 8 gpus is",
      "offset": 10378.76,
      "duration": 9.32
    },
    {
      "lang": "en",
      "text": "131 uh K and so",
      "offset": 10384.319,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "524288 this means that our gratum will",
      "offset": 10388.08,
      "duration": 5.88
    },
    {
      "lang": "en",
      "text": "be four with the current settings right",
      "offset": 10390.479,
      "duration": 6.361
    },
    {
      "lang": "en",
      "text": "so there's going to be 16 * 124 process",
      "offset": 10393.96,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "on each GPU and then there's a GP pus so",
      "offset": 10396.84,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "we're going to be doing",
      "offset": 10398.92,
      "duration": 4.28
    },
    {
      "lang": "en",
      "text": "131,000 tokens in a single forward",
      "offset": 10400.52,
      "duration": 5.759
    },
    {
      "lang": "en",
      "text": "backward on the 8",
      "offset": 10403.2,
      "duration": 5.079
    },
    {
      "lang": "en",
      "text": "gpus so we want to make sure that this",
      "offset": 10406.279,
      "duration": 4.521
    },
    {
      "lang": "en",
      "text": "fits nicely so that we can derive a nice",
      "offset": 10408.279,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "gradient accumulation",
      "offset": 10410.8,
      "duration": 5.4
    },
    {
      "lang": "en",
      "text": "steps and uh yeah let's just adjust the",
      "offset": 10412.359,
      "duration": 8.761
    },
    {
      "lang": "en",
      "text": "comments here times uh DDP World size",
      "offset": 10416.2,
      "duration": 9.039
    },
    {
      "lang": "en",
      "text": "okay so each GPU calculates this now",
      "offset": 10421.12,
      "duration": 5.4
    },
    {
      "lang": "en",
      "text": "this is where we start to get run into",
      "offset": 10425.239,
      "duration": 4.12
    },
    {
      "lang": "en",
      "text": "issues right so we are each process is",
      "offset": 10426.52,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "going to come by a print and they're all",
      "offset": 10429.359,
      "duration": 3.88
    },
    {
      "lang": "en",
      "text": "going to print so we're going to have",
      "offset": 10431.8,
      "duration": 4.84
    },
    {
      "lang": "en",
      "text": "eight copies of these prints so one way",
      "offset": 10433.239,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "to deal with this is exactly this master",
      "offset": 10436.64,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "process variable that we have so if",
      "offset": 10438.359,
      "duration": 5.481
    },
    {
      "lang": "en",
      "text": "Master process then guard this and",
      "offset": 10440,
      "duration": 5.319
    },
    {
      "lang": "en",
      "text": "that's just so that we just print this a",
      "offset": 10443.84,
      "duration": 3.479
    },
    {
      "lang": "en",
      "text": "single time because otherwise all the",
      "offset": 10445.319,
      "duration": 3.521
    },
    {
      "lang": "en",
      "text": "processes would have computed the exact",
      "offset": 10447.319,
      "duration": 2.801
    },
    {
      "lang": "en",
      "text": "same variables and there's no need to",
      "offset": 10448.84,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "print this eight",
      "offset": 10450.12,
      "duration": 4.359
    },
    {
      "lang": "en",
      "text": "times um before getting into the data",
      "offset": 10451.84,
      "duration": 3.8
    },
    {
      "lang": "en",
      "text": "loader and we're going to have to",
      "offset": 10454.479,
      "duration": 3.561
    },
    {
      "lang": "en",
      "text": "refactor it obviously maybe at this",
      "offset": 10455.64,
      "duration": 5.839
    },
    {
      "lang": "en",
      "text": "point is uh we should do some prints and",
      "offset": 10458.04,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "uh just take it out for a spin and exit",
      "offset": 10461.479,
      "duration": 5.041
    },
    {
      "lang": "en",
      "text": "at this point so import",
      "offset": 10463.72,
      "duration": 9.24
    },
    {
      "lang": "en",
      "text": "sis and S start exit and print IM",
      "offset": 10466.52,
      "duration": 6.44
    },
    {
      "lang": "en",
      "text": "GPU um DDP",
      "offset": 10473.12,
      "duration": 10.239
    },
    {
      "lang": "en",
      "text": "rank IM GPU DDP Rank and that um",
      "offset": 10478.399,
      "duration": 7.761
    },
    {
      "lang": "en",
      "text": "print",
      "offset": 10483.359,
      "duration": 5.96
    },
    {
      "lang": "en",
      "text": "by so uh so now let's try to run this",
      "offset": 10486.16,
      "duration": 5.159
    },
    {
      "lang": "en",
      "text": "and just see how this works so let's",
      "offset": 10489.319,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "take it for a spin just so we see what",
      "offset": 10491.319,
      "duration": 3.401
    },
    {
      "lang": "en",
      "text": "it looks like so normally we use to",
      "offset": 10492.76,
      "duration": 4.92
    },
    {
      "lang": "en",
      "text": "launch python train gpd2 P like this now",
      "offset": 10494.72,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "we're going to run with torch run and",
      "offset": 10497.68,
      "duration": 4.36
    },
    {
      "lang": "en",
      "text": "this is what it looks like so torch run",
      "offset": 10499.359,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "Standalone number of processes for",
      "offset": 10502.04,
      "duration": 3.279
    },
    {
      "lang": "en",
      "text": "example is eight for us because we have",
      "offset": 10504,
      "duration": 5.359
    },
    {
      "lang": "en",
      "text": "eight gpus uh and then change of2 Pi so",
      "offset": 10505.319,
      "duration": 6.481
    },
    {
      "lang": "en",
      "text": "this is what the command would look like",
      "offset": 10509.359,
      "duration": 4.521
    },
    {
      "lang": "en",
      "text": "and torch run again we'll run eight of",
      "offset": 10511.8,
      "duration": 4.679
    },
    {
      "lang": "en",
      "text": "these so let's just see what happens so",
      "offset": 10513.88,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "first",
      "offset": 10516.479,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "it gets a little busy so there's a lot",
      "offset": 10518.6,
      "duration": 3.56
    },
    {
      "lang": "en",
      "text": "going on here so first of all there's",
      "offset": 10520.16,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "some warnings from distributed and I",
      "offset": 10522.16,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "don't actually know that these mean",
      "offset": 10524.08,
      "duration": 3.96
    },
    {
      "lang": "en",
      "text": "anything I think this is just like the",
      "offset": 10526.08,
      "duration": 3.48
    },
    {
      "lang": "en",
      "text": "code is setting up and the processes are",
      "offset": 10528.04,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "coming online and we're seeing some",
      "offset": 10529.56,
      "duration": 4.2
    },
    {
      "lang": "en",
      "text": "preliminary failure to collect while the",
      "offset": 10531.72,
      "duration": 4.36
    },
    {
      "lang": "en",
      "text": "processes come up I'm not 100% sure",
      "offset": 10533.76,
      "duration": 5.559
    },
    {
      "lang": "en",
      "text": "about that but we start to then get into",
      "offset": 10536.08,
      "duration": 5.319
    },
    {
      "lang": "en",
      "text": "actual prints",
      "offset": 10539.319,
      "duration": 4.841
    },
    {
      "lang": "en",
      "text": "so all the processes went down and then",
      "offset": 10541.399,
      "duration": 5.521
    },
    {
      "lang": "en",
      "text": "the first print actually comes from",
      "offset": 10544.16,
      "duration": 6.6
    },
    {
      "lang": "en",
      "text": "process 5 uh just by chance and then it",
      "offset": 10546.92,
      "duration": 5.88
    },
    {
      "lang": "en",
      "text": "printed so process 5 basically got here",
      "offset": 10550.76,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "first it said I'm process on GPU 5 buy",
      "offset": 10552.8,
      "duration": 7.32
    },
    {
      "lang": "en",
      "text": "and then this these prints come from the",
      "offset": 10556.84,
      "duration": 4.599
    },
    {
      "lang": "en",
      "text": "master",
      "offset": 10560.12,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "process so process 5 just finished first",
      "offset": 10561.439,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "for whatever reason it just depends on",
      "offset": 10564.279,
      "duration": 2.881
    },
    {
      "lang": "en",
      "text": "how the operating system scheduled the",
      "offset": 10565.76,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "processes to run uh then gpu0 ended then",
      "offset": 10567.16,
      "duration": 7.159
    },
    {
      "lang": "en",
      "text": "GPU 3 and two and then uh probably",
      "offset": 10570.56,
      "duration": 6.52
    },
    {
      "lang": "en",
      "text": "process 5 or something like that has uh",
      "offset": 10574.319,
      "duration": 5.441
    },
    {
      "lang": "en",
      "text": "exited and and DDP really doesn't like",
      "offset": 10577.08,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "that because we didn't properly dispose",
      "offset": 10579.76,
      "duration": 7.639
    },
    {
      "lang": "en",
      "text": "of uh the multi-gpus um setting and so",
      "offset": 10581.96,
      "duration": 6.8
    },
    {
      "lang": "en",
      "text": "process group has not been destroyed",
      "offset": 10587.399,
      "duration": 3.96
    },
    {
      "lang": "en",
      "text": "before we destruct uh so it really",
      "offset": 10588.76,
      "duration": 4.519
    },
    {
      "lang": "en",
      "text": "doesn't like that and in an actual",
      "offset": 10591.359,
      "duration": 3.481
    },
    {
      "lang": "en",
      "text": "application we would want to call",
      "offset": 10593.279,
      "duration": 4.04
    },
    {
      "lang": "en",
      "text": "destroy process group uh so that we",
      "offset": 10594.84,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "clean up DDP properly and so it doesn't",
      "offset": 10597.319,
      "duration": 4.521
    },
    {
      "lang": "en",
      "text": "like that too much and then the rest of",
      "offset": 10600.2,
      "duration": 5.119
    },
    {
      "lang": "en",
      "text": "the gpus finish and that's it so",
      "offset": 10601.84,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "basically we can't guarantee when these",
      "offset": 10605.319,
      "duration": 3.481
    },
    {
      "lang": "en",
      "text": "processes are running it's totally",
      "offset": 10606.64,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "but they are running in parallel we",
      "offset": 10608.8,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "don't want them to be printing um and",
      "offset": 10610.479,
      "duration": 6.561
    },
    {
      "lang": "en",
      "text": "next up let's erase",
      "offset": 10614.16,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "this next up we want to make sure that",
      "offset": 10617.04,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "when we create data loader light we need",
      "offset": 10619.2,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "to now make it aware of this",
      "offset": 10621.04,
      "duration": 5.08
    },
    {
      "lang": "en",
      "text": "multi-process um setting because we",
      "offset": 10623.12,
      "duration": 4.6
    },
    {
      "lang": "en",
      "text": "don't want all the processes to be",
      "offset": 10626.12,
      "duration": 3.96
    },
    {
      "lang": "en",
      "text": "loading the exact same data we want",
      "offset": 10627.72,
      "duration": 4.04
    },
    {
      "lang": "en",
      "text": "every process to get its own chunk of",
      "offset": 10630.08,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "data so that they're all working on",
      "offset": 10631.76,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "different parts of the data set of",
      "offset": 10633.439,
      "duration": 4.441
    },
    {
      "lang": "en",
      "text": "course so let's adjust that so one",
      "offset": 10634.96,
      "duration": 4.359
    },
    {
      "lang": "en",
      "text": "particular particularly simple and a",
      "offset": 10637.88,
      "duration": 3.72
    },
    {
      "lang": "en",
      "text": "naive way to do this is we have to make",
      "offset": 10639.319,
      "duration": 4.281
    },
    {
      "lang": "en",
      "text": "sure that we pass in the rank and the",
      "offset": 10641.6,
      "duration": 4.12
    },
    {
      "lang": "en",
      "text": "size to the data",
      "offset": 10643.6,
      "duration": 4.44
    },
    {
      "lang": "en",
      "text": "loader and then when we come up here we",
      "offset": 10645.72,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "see that we now take Rank and processes",
      "offset": 10648.04,
      "duration": 4.52
    },
    {
      "lang": "en",
      "text": "and we save them now the current",
      "offset": 10649.96,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "position will not be zero uh because",
      "offset": 10652.56,
      "duration": 5.24
    },
    {
      "lang": "en",
      "text": "what we want is we want to stride out",
      "offset": 10655.08,
      "duration": 5.359
    },
    {
      "lang": "en",
      "text": "all the processes so one way to do this",
      "offset": 10657.8,
      "duration": 6.12
    },
    {
      "lang": "en",
      "text": "is we basically take S.B times salt. T",
      "offset": 10660.439,
      "duration": 6.121
    },
    {
      "lang": "en",
      "text": "and then multiply it by the process",
      "offset": 10663.92,
      "duration": 5.479
    },
    {
      "lang": "en",
      "text": "rank so proc process rank 0 will start",
      "offset": 10666.56,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "at zero but process rank one now starts",
      "offset": 10669.399,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "at B * T process rank two is starts at 2",
      "offset": 10672.16,
      "duration": 6.84
    },
    {
      "lang": "en",
      "text": "* B * D Etc so that is the",
      "offset": 10675.479,
      "duration": 6.201
    },
    {
      "lang": "en",
      "text": "initialization now we still they still",
      "offset": 10679,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "do this identically but now when we",
      "offset": 10681.68,
      "duration": 5.08
    },
    {
      "lang": "en",
      "text": "advance we don't Advance by B * T we",
      "offset": 10684.04,
      "duration": 6.12
    },
    {
      "lang": "en",
      "text": "advance by B * T times number of",
      "offset": 10686.76,
      "duration": 7.36
    },
    {
      "lang": "en",
      "text": "processes right so basically um the",
      "offset": 10690.16,
      "duration": 6.319
    },
    {
      "lang": "en",
      "text": "total number of tokens that we're um",
      "offset": 10694.12,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "consuming is B * T * number processes",
      "offset": 10696.479,
      "duration": 6.84
    },
    {
      "lang": "en",
      "text": "and they all go off to a different Rank",
      "offset": 10699.319,
      "duration": 5.521
    },
    {
      "lang": "en",
      "text": "and the position has to advance by the",
      "offset": 10703.319,
      "duration": 3.08
    },
    {
      "lang": "en",
      "text": "entire",
      "offset": 10704.84,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "chunk and then here B * T time uh s. num",
      "offset": 10706.399,
      "duration": 7.241
    },
    {
      "lang": "en",
      "text": "processes + one would be to exceed",
      "offset": 10710.6,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "number of tokens then we're going to",
      "offset": 10713.64,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "Loop and when we Loop we want to of",
      "offset": 10715.16,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "course Loop in the exact same way so we",
      "offset": 10717.16,
      "duration": 5.72
    },
    {
      "lang": "en",
      "text": "sort of like reset back uh so this is",
      "offset": 10719.8,
      "duration": 5.72
    },
    {
      "lang": "en",
      "text": "the simplest change that I can uh find",
      "offset": 10722.88,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "for kind of a very simple distributed",
      "offset": 10725.52,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "data Lo light and um you can notice that",
      "offset": 10727.76,
      "duration": 4.76
    },
    {
      "lang": "en",
      "text": "if process rank is zero and non",
      "offset": 10730.96,
      "duration": 3.8
    },
    {
      "lang": "en",
      "text": "processes is one then uh the whole thing",
      "offset": 10732.52,
      "duration": 4.2
    },
    {
      "lang": "en",
      "text": "will be identical to what we had before",
      "offset": 10734.76,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "but now we can have actually multiple",
      "offset": 10736.72,
      "duration": 3.639
    },
    {
      "lang": "en",
      "text": "processes uh running and this should",
      "offset": 10738.359,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "work",
      "offset": 10740.359,
      "duration": 5.161
    },
    {
      "lang": "en",
      "text": "fine um so that's the data loader okay",
      "offset": 10741.359,
      "duration": 6.12
    },
    {
      "lang": "en",
      "text": "so next up once they've all initialized",
      "offset": 10745.52,
      "duration": 4.12
    },
    {
      "lang": "en",
      "text": "the data loader they come here and they",
      "offset": 10747.479,
      "duration": 5.8
    },
    {
      "lang": "en",
      "text": "all create a GPT model uh so we create",
      "offset": 10749.64,
      "duration": 6.12
    },
    {
      "lang": "en",
      "text": "eight GPT models on eight processes but",
      "offset": 10753.279,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "because the seeds are fixed here they",
      "offset": 10755.76,
      "duration": 4.599
    },
    {
      "lang": "en",
      "text": "all create the same identical model they",
      "offset": 10757.6,
      "duration": 5.08
    },
    {
      "lang": "en",
      "text": "all move it to the device of their Rank",
      "offset": 10760.359,
      "duration": 4.681
    },
    {
      "lang": "en",
      "text": "and they all compile the model and",
      "offset": 10762.68,
      "duration": 3.639
    },
    {
      "lang": "en",
      "text": "because the models are identical there",
      "offset": 10765.04,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "are eight identical compilations",
      "offset": 10766.319,
      "duration": 4.841
    },
    {
      "lang": "en",
      "text": "happening in parallel but that's okay",
      "offset": 10768.08,
      "duration": 4.92
    },
    {
      "lang": "en",
      "text": "now none of this uh changes because that",
      "offset": 10771.16,
      "duration": 3.72
    },
    {
      "lang": "en",
      "text": "is on a per step basis and we're",
      "offset": 10773,
      "duration": 3.64
    },
    {
      "lang": "en",
      "text": "currently working kind of within step",
      "offset": 10774.88,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "because we need to um just uh all the",
      "offset": 10776.64,
      "duration": 4.52
    },
    {
      "lang": "en",
      "text": "all the changes we're making are kind of",
      "offset": 10779.88,
      "duration": 2.84
    },
    {
      "lang": "en",
      "text": "like a within step",
      "offset": 10781.16,
      "duration": 3.8
    },
    {
      "lang": "en",
      "text": "changes now the important thing here is",
      "offset": 10782.72,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "when we construct the M model we",
      "offset": 10784.96,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "actually have a bit of work to to do",
      "offset": 10787.04,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "here get loits is deprecated so uh",
      "offset": 10788.319,
      "duration": 3.881
    },
    {
      "lang": "en",
      "text": "create",
      "offset": 10790.72,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "model we need to actually wrap the model",
      "offset": 10792.2,
      "duration": 6.32
    },
    {
      "lang": "en",
      "text": "into the distributed data parallel",
      "offset": 10795.52,
      "duration": 6.4
    },
    {
      "lang": "en",
      "text": "container so um this is how we wrap the",
      "offset": 10798.52,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "model into the DDP container and these",
      "offset": 10801.92,
      "duration": 5.24
    },
    {
      "lang": "en",
      "text": "are the docs for DDP and they're quite",
      "offset": 10804.68,
      "duration": 4.44
    },
    {
      "lang": "en",
      "text": "extensive and there's a lot of caveats",
      "offset": 10807.16,
      "duration": 3.239
    },
    {
      "lang": "en",
      "text": "and a lot of things to be careful with",
      "offset": 10809.12,
      "duration": 3.279
    },
    {
      "lang": "en",
      "text": "because everything complexifies times 10",
      "offset": 10810.399,
      "duration": 5.281
    },
    {
      "lang": "en",
      "text": "when multiple processes are involved but",
      "offset": 10812.399,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "roughly speaking this device IDs I",
      "offset": 10815.68,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "believe has to be passed in now",
      "offset": 10817.399,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "unfortunately the docs for what device",
      "offset": 10818.88,
      "duration": 5.599
    },
    {
      "lang": "en",
      "text": "IDs is is is extremely unclear uh so",
      "offset": 10820.76,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "when you actually like come here this",
      "offset": 10824.479,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "comment for what device IDs is is",
      "offset": 10826.84,
      "duration": 3.639
    },
    {
      "lang": "en",
      "text": "roughly",
      "offset": 10829.359,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "nonsensical um but I'm pretty sure it's",
      "offset": 10830.479,
      "duration": 5.041
    },
    {
      "lang": "en",
      "text": "supposed to be the DDP local rank so not",
      "offset": 10833.04,
      "duration": 6.199
    },
    {
      "lang": "en",
      "text": "the DDP rank the local rank uh so this",
      "offset": 10835.52,
      "duration": 5.759
    },
    {
      "lang": "en",
      "text": "is what you pass in here this wraps the",
      "offset": 10839.239,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "model and in particular what DDP does",
      "offset": 10841.279,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "for you is in a forward pass it actually",
      "offset": 10843.239,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "behaves identically so um my",
      "offset": 10845.52,
      "duration": 4.12
    },
    {
      "lang": "en",
      "text": "understanding of it is nothing should be",
      "offset": 10848.2,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "changed in the forward pass but in the",
      "offset": 10849.64,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "backward pass as you are doing the",
      "offset": 10851.64,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "backward pass um in the simpl setting",
      "offset": 10853.72,
      "duration": 5.4
    },
    {
      "lang": "en",
      "text": "once the backp passes over on each",
      "offset": 10856.84,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "independent GPU each independent GPU has",
      "offset": 10859.12,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "the gradient for all the parameters and",
      "offset": 10862.12,
      "duration": 4.68
    },
    {
      "lang": "en",
      "text": "what DDP does for you is once the",
      "offset": 10865.2,
      "duration": 3.96
    },
    {
      "lang": "en",
      "text": "backward pass is over it will call",
      "offset": 10866.8,
      "duration": 4.28
    },
    {
      "lang": "en",
      "text": "what's called all reduce and it",
      "offset": 10869.16,
      "duration": 5.159
    },
    {
      "lang": "en",
      "text": "basically does an average across all the",
      "offset": 10871.08,
      "duration": 7.08
    },
    {
      "lang": "en",
      "text": "uh ranks of their gradients and and then",
      "offset": 10874.319,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "it will deposit that average on every",
      "offset": 10878.16,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "single rank so every sing Single rank",
      "offset": 10880.319,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "will end up with the average on it and",
      "offset": 10882.64,
      "duration": 4.36
    },
    {
      "lang": "en",
      "text": "so basically that's the communication it",
      "offset": 10885.319,
      "duration": 3.241
    },
    {
      "lang": "en",
      "text": "just synchronizes and averages the",
      "offset": 10887,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "gradients and that's what DDP offers you",
      "offset": 10888.56,
      "duration": 5.679
    },
    {
      "lang": "en",
      "text": "now DDP actually is a little bit more um",
      "offset": 10891,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "it is a little bit more involved than",
      "offset": 10894.239,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "that because as you are doing the",
      "offset": 10895.64,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "backward pass through the layers of the",
      "offset": 10897.439,
      "duration": 3.8
    },
    {
      "lang": "en",
      "text": "Transformer it actually can dispatch",
      "offset": 10898.84,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "Communications for the gradient while",
      "offset": 10901.239,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "the backward pass is still happening so",
      "offset": 10903.479,
      "duration": 3.96
    },
    {
      "lang": "en",
      "text": "there's overlap of the uh communication",
      "offset": 10905.319,
      "duration": 3.641
    },
    {
      "lang": "en",
      "text": "of the gradient and the synchronization",
      "offset": 10907.439,
      "duration": 4.601
    },
    {
      "lang": "en",
      "text": "of them and uh the backward pass and uh",
      "offset": 10908.96,
      "duration": 6.279
    },
    {
      "lang": "en",
      "text": "this is just more efficient and um uh to",
      "offset": 10912.04,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "do it that way so that's what DDP does",
      "offset": 10915.239,
      "duration": 5.481
    },
    {
      "lang": "en",
      "text": "for you um forward is unchanged and",
      "offset": 10917.64,
      "duration": 4.839
    },
    {
      "lang": "en",
      "text": "backward is mostly unchanged and we're",
      "offset": 10920.72,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "tacking on this average as we'll see in",
      "offset": 10922.479,
      "duration": 5.8
    },
    {
      "lang": "en",
      "text": "a bit okay so now let's go to the uh",
      "offset": 10924.96,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "optimization nothing here changes let's",
      "offset": 10928.279,
      "duration": 4.441
    },
    {
      "lang": "en",
      "text": "go to the optimization here the inner",
      "offset": 10931.04,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "loop and think through the",
      "offset": 10932.72,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "synchronization of uh these gradients in",
      "offset": 10933.84,
      "duration": 4.599
    },
    {
      "lang": "en",
      "text": "the DP so basically by default what",
      "offset": 10935.92,
      "duration": 4.439
    },
    {
      "lang": "en",
      "text": "happens as I mentioned is when you do l.",
      "offset": 10938.439,
      "duration": 4.281
    },
    {
      "lang": "en",
      "text": "backward here it will do the backward",
      "offset": 10940.359,
      "duration": 4.361
    },
    {
      "lang": "en",
      "text": "pass and then it will synchronize the",
      "offset": 10942.72,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "gradients um the problem here is because",
      "offset": 10944.72,
      "duration": 5.8
    },
    {
      "lang": "en",
      "text": "of the gradient accumulation steps Loop",
      "offset": 10948.16,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "here we don't actually want to do the",
      "offset": 10950.52,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "synchronization after every single La",
      "offset": 10953.6,
      "duration": 3.719
    },
    {
      "lang": "en",
      "text": "step backward because we are just",
      "offset": 10955.64,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "depositing gradients and we're doing",
      "offset": 10957.319,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "that serially and we just want them",
      "offset": 10959.16,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "adding up and we don't want to",
      "offset": 10960.92,
      "duration": 3.16
    },
    {
      "lang": "en",
      "text": "synchronize every single time that would",
      "offset": 10962.6,
      "duration": 3.639
    },
    {
      "lang": "en",
      "text": "be extremely wasteful so basically we",
      "offset": 10964.08,
      "duration": 3.96
    },
    {
      "lang": "en",
      "text": "want to add them up and then on the the",
      "offset": 10966.239,
      "duration": 4.2
    },
    {
      "lang": "en",
      "text": "very last uh it's only on the very last",
      "offset": 10968.04,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "step when micro when micro step becomes",
      "offset": 10970.439,
      "duration": 5.361
    },
    {
      "lang": "en",
      "text": "gratak steps minus one only at that last",
      "offset": 10973.08,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "step do we want to actually do the",
      "offset": 10975.8,
      "duration": 6.4
    },
    {
      "lang": "en",
      "text": "alberu uh to average up the gradients so",
      "offset": 10978.68,
      "duration": 6.44
    },
    {
      "lang": "en",
      "text": "to do that we come here and um the",
      "offset": 10982.2,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "official sanctioned way by the way is to",
      "offset": 10985.12,
      "duration": 5.359
    },
    {
      "lang": "en",
      "text": "do this no sync context manager so",
      "offset": 10987,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "pytorch says this is a context manager",
      "offset": 10990.479,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "to disable gradient synchronization",
      "offset": 10993,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "across DDP processes So within this",
      "offset": 10994.479,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "context gradient will be",
      "offset": 10997,
      "duration": 4.92
    },
    {
      "lang": "en",
      "text": "accumulated and basically when you do no",
      "offset": 10999.2,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "sync there will be no communication so",
      "offset": 11001.92,
      "duration": 4.359
    },
    {
      "lang": "en",
      "text": "they are telling us to do with DDP no",
      "offset": 11004.08,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "sync uh do the gradient accumulation",
      "offset": 11006.279,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "accumulate grats and then they are",
      "offset": 11009.04,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "asking us to do DDP again with another",
      "offset": 11010.68,
      "duration": 4.679
    },
    {
      "lang": "en",
      "text": "input and that backward and I just",
      "offset": 11012.96,
      "duration": 4.6
    },
    {
      "lang": "en",
      "text": "really don't love this I I just really",
      "offset": 11015.359,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "don't like it uh the fact that you have",
      "offset": 11017.56,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "to copy paste your code here and use a",
      "offset": 11019.12,
      "duration": 3.56
    },
    {
      "lang": "en",
      "text": "context manager and this is just super",
      "offset": 11020.92,
      "duration": 4.359
    },
    {
      "lang": "en",
      "text": "ugly so when I went to this source code",
      "offset": 11022.68,
      "duration": 5.759
    },
    {
      "lang": "en",
      "text": "here you can see that when you enter",
      "offset": 11025.279,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "you simply toggle this variable this",
      "offset": 11028.439,
      "duration": 6.281
    },
    {
      "lang": "en",
      "text": "require backward grat sync and this is",
      "offset": 11031.439,
      "duration": 6.601
    },
    {
      "lang": "en",
      "text": "uh being toggled around and changed and",
      "offset": 11034.72,
      "duration": 6.36
    },
    {
      "lang": "en",
      "text": "this is the variable that basically uh",
      "offset": 11038.04,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "if you step through it is being toggled",
      "offset": 11041.08,
      "duration": 4.359
    },
    {
      "lang": "en",
      "text": "to determine if the gradient is going to",
      "offset": 11043.239,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "be synchronized so I actually just kind",
      "offset": 11045.439,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "of like to use that directly uh so",
      "offset": 11047.8,
      "duration": 5.639
    },
    {
      "lang": "en",
      "text": "instead what I like to do is the",
      "offset": 11050.479,
      "duration": 5.361
    },
    {
      "lang": "en",
      "text": "following right here before the L back",
      "offset": 11053.439,
      "duration": 7.241
    },
    {
      "lang": "en",
      "text": "backward if we are using the DDP then um",
      "offset": 11055.84,
      "duration": 7.16
    },
    {
      "lang": "en",
      "text": "then basically we only want to",
      "offset": 11060.68,
      "duration": 4.599
    },
    {
      "lang": "en",
      "text": "synchronize we only want this variable",
      "offset": 11063,
      "duration": 5.88
    },
    {
      "lang": "en",
      "text": "to be true when it is the final",
      "offset": 11065.279,
      "duration": 6.321
    },
    {
      "lang": "en",
      "text": "iteration in all the other iterations",
      "offset": 11068.88,
      "duration": 4.359
    },
    {
      "lang": "en",
      "text": "inside the micr steps we want to be",
      "offset": 11071.6,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "false so I just toggle it like this so",
      "offset": 11073.239,
      "duration": 5.281
    },
    {
      "lang": "en",
      "text": "required backward graph sync should only",
      "offset": 11076.479,
      "duration": 4.681
    },
    {
      "lang": "en",
      "text": "turn on when the micro step is the last",
      "offset": 11078.52,
      "duration": 5.759
    },
    {
      "lang": "en",
      "text": "step and so I'm toggling this variable",
      "offset": 11081.16,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "directly and I hope that that impacts",
      "offset": 11084.279,
      "duration": 4.04
    },
    {
      "lang": "en",
      "text": "last St backwards",
      "offset": 11087,
      "duration": 2.92
    },
    {
      "lang": "en",
      "text": "and this is a naughty thing to do",
      "offset": 11088.319,
      "duration": 2.801
    },
    {
      "lang": "en",
      "text": "because you know they could probably",
      "offset": 11089.92,
      "duration": 3.16
    },
    {
      "lang": "en",
      "text": "change the DDP and this variable will go",
      "offset": 11091.12,
      "duration": 3.88
    },
    {
      "lang": "en",
      "text": "away but for now I believe this this",
      "offset": 11093.08,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "works and it allows me to avoid the use",
      "offset": 11095,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "of context managers and code duplication",
      "offset": 11097.8,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "I'm just toggling the variable and then",
      "offset": 11100,
      "duration": 3.319
    },
    {
      "lang": "en",
      "text": "Lop backward will not synchronize most",
      "offset": 11101.56,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "of the steps and it will synchronize the",
      "offset": 11103.319,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "very last step and so once this is over",
      "offset": 11104.92,
      "duration": 8.519
    },
    {
      "lang": "en",
      "text": "uh and we come out every single um rank",
      "offset": 11108.439,
      "duration": 8.761
    },
    {
      "lang": "en",
      "text": "will suddenly magically have the average",
      "offset": 11113.439,
      "duration": 6.88
    },
    {
      "lang": "en",
      "text": "of all the gradients that were stored on",
      "offset": 11117.2,
      "duration": 5.279
    },
    {
      "lang": "en",
      "text": "all the ranks so now we have to think",
      "offset": 11120.319,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "through whether that is what we want and",
      "offset": 11122.479,
      "duration": 6.641
    },
    {
      "lang": "en",
      "text": "also um if this suffices and whether how",
      "offset": 11124.319,
      "duration": 6.761
    },
    {
      "lang": "en",
      "text": "it works with the loss and what is loss",
      "offset": 11129.12,
      "duration": 3.88
    },
    {
      "lang": "en",
      "text": "AUM so let's think through through that",
      "offset": 11131.08,
      "duration": 4.359
    },
    {
      "lang": "en",
      "text": "now and the problem I'm getting at is",
      "offset": 11133,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "that we've averaged the gradients which",
      "offset": 11135.439,
      "duration": 4.84
    },
    {
      "lang": "en",
      "text": "is great but the loss AUM has not been",
      "offset": 11137.56,
      "duration": 5.719
    },
    {
      "lang": "en",
      "text": "impacted yet and the and this is outside",
      "offset": 11140.279,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "of the DDP container so that is not",
      "offset": 11143.279,
      "duration": 4.441
    },
    {
      "lang": "en",
      "text": "being averaged um and so here when when",
      "offset": 11145.239,
      "duration": 4.521
    },
    {
      "lang": "en",
      "text": "we are printing Los AUM well presumably",
      "offset": 11147.72,
      "duration": 3.719
    },
    {
      "lang": "en",
      "text": "we're only going to be printing on the",
      "offset": 11149.76,
      "duration": 3.96
    },
    {
      "lang": "en",
      "text": "master process uh rank zero and it's",
      "offset": 11151.439,
      "duration": 3.641
    },
    {
      "lang": "en",
      "text": "just going to be printing the losses",
      "offset": 11153.72,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "that it saw on its process but instead",
      "offset": 11155.08,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "we want it to print the loss over all",
      "offset": 11157.8,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "the processes and the average of that",
      "offset": 11160.12,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "loss because we did average of gradients",
      "offset": 11162.04,
      "duration": 4.359
    },
    {
      "lang": "en",
      "text": "so we want the average of loss as well",
      "offset": 11164.2,
      "duration": 5.48
    },
    {
      "lang": "en",
      "text": "so simply here after this uh this is the",
      "offset": 11166.399,
      "duration": 6.721
    },
    {
      "lang": "en",
      "text": "code that I've used in the past um and",
      "offset": 11169.68,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "instead of LF we want",
      "offset": 11173.12,
      "duration": 5.08
    },
    {
      "lang": "en",
      "text": "Lum so if",
      "offset": 11175.84,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "DDP again then this is a p torch",
      "offset": 11178.2,
      "duration": 6.72
    },
    {
      "lang": "en",
      "text": "distributed I import it where do I",
      "offset": 11182,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "import",
      "offset": 11184.92,
      "duration": 5.359
    },
    {
      "lang": "en",
      "text": "it uh oh gosh so this file is starting",
      "offset": 11186.56,
      "duration": 7
    },
    {
      "lang": "en",
      "text": "to get out of control huh so if uh so",
      "offset": 11190.279,
      "duration": 6.04
    },
    {
      "lang": "en",
      "text": "import torch. distributed as dist",
      "offset": 11193.56,
      "duration": 4.919
    },
    {
      "lang": "en",
      "text": "so dist.",
      "offset": 11196.319,
      "duration": 5.761
    },
    {
      "lang": "en",
      "text": "ALU and we're doing the average on Lum",
      "offset": 11198.479,
      "duration": 5.96
    },
    {
      "lang": "en",
      "text": "and so this lakum tensor exists on all",
      "offset": 11202.08,
      "duration": 4.44
    },
    {
      "lang": "en",
      "text": "the ranks when we call all use of",
      "offset": 11204.439,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "average it creates the average of those",
      "offset": 11206.52,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "numbers and it deposits that average on",
      "offset": 11208.76,
      "duration": 4.84
    },
    {
      "lang": "en",
      "text": "all the ranks so all the ranks after",
      "offset": 11211,
      "duration": 6.96
    },
    {
      "lang": "en",
      "text": "this um call will now contain L AUM uh",
      "offset": 11213.6,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "averaged up and so when we print here on",
      "offset": 11217.96,
      "duration": 4.04
    },
    {
      "lang": "en",
      "text": "the master process the L AUM is",
      "offset": 11220.08,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "identical in all the other ranks as well",
      "offset": 11222,
      "duration": 5.399
    },
    {
      "lang": "en",
      "text": "so here if Master process",
      "offset": 11224,
      "duration": 6.88
    },
    {
      "lang": "en",
      "text": "oops we want to print like this okay and",
      "offset": 11227.399,
      "duration": 4.92
    },
    {
      "lang": "en",
      "text": "finally we have to be careful because",
      "offset": 11230.88,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "we're not processing even more tokens so",
      "offset": 11232.319,
      "duration": 5.96
    },
    {
      "lang": "en",
      "text": "times DDP World size",
      "offset": 11235.6,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "that's number of tokens that we've",
      "offset": 11238.279,
      "duration": 3.241
    },
    {
      "lang": "en",
      "text": "processed up",
      "offset": 11239.68,
      "duration": 4.84
    },
    {
      "lang": "en",
      "text": "above",
      "offset": 11241.52,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "and everything else should be fine uh",
      "offset": 11244.96,
      "duration": 4.279
    },
    {
      "lang": "en",
      "text": "the only other thing to be careful with",
      "offset": 11247.92,
      "duration": 3.319
    },
    {
      "lang": "en",
      "text": "is as I mentioned you want to destroy",
      "offset": 11249.239,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "the process group so that we are nice to",
      "offset": 11251.239,
      "duration": 4.521
    },
    {
      "lang": "en",
      "text": "nickel and it's not going to uh to uh to",
      "offset": 11253.08,
      "duration": 5.319
    },
    {
      "lang": "en",
      "text": "DDP and it's not going to complain to us",
      "offset": 11255.76,
      "duration": 4.44
    },
    {
      "lang": "en",
      "text": "uh when we exit",
      "offset": 11258.399,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "here so that should be it let's try to",
      "offset": 11260.2,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "take it for a spin okay so I launched",
      "offset": 11263.04,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "the script and it should be uh printing",
      "offset": 11264.84,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "here imminently we're now training with",
      "offset": 11266.8,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "8 gpus at the same time so the gradient",
      "offset": 11268.84,
      "duration": 4.439
    },
    {
      "lang": "en",
      "text": "accumulation steps is not 32 it is now",
      "offset": 11271.04,
      "duration": 6.96
    },
    {
      "lang": "en",
      "text": "divide 8 and it's just four uh so um",
      "offset": 11273.279,
      "duration": 6.601
    },
    {
      "lang": "en",
      "text": "otherwise this is what the optimization",
      "offset": 11278,
      "duration": 3.96
    },
    {
      "lang": "en",
      "text": "now looks like and wow we're going",
      "offset": 11279.88,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "really fast so we're processing 1.5",
      "offset": 11281.96,
      "duration": 7.439
    },
    {
      "lang": "en",
      "text": "million tokens uh per second now so",
      "offset": 11284.92,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "these are some serious numbers and the",
      "offset": 11289.399,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "tiny shakespare data set is so tiny that",
      "offset": 11291.08,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "we're just doing like so many Epoch over",
      "offset": 11292.84,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "it most likely but this is roughly what",
      "offset": 11295.399,
      "duration": 5.161
    },
    {
      "lang": "en",
      "text": "looks like um one thing that I had to",
      "offset": 11297.56,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "fix by the way is that this was model.",
      "offset": 11300.56,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "configure optimizers which Now doesn't",
      "offset": 11303.16,
      "duration": 4.6
    },
    {
      "lang": "en",
      "text": "work because model now is a DDP model so",
      "offset": 11305.2,
      "duration": 4.76
    },
    {
      "lang": "en",
      "text": "instead this has to become raw",
      "offset": 11307.76,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "model. configure optimizers where raw",
      "offset": 11309.96,
      "duration": 5.479
    },
    {
      "lang": "en",
      "text": "model is something I create here so",
      "offset": 11312.72,
      "duration": 5.519
    },
    {
      "lang": "en",
      "text": "right after I wrap the model into DDP uh",
      "offset": 11315.439,
      "duration": 5.081
    },
    {
      "lang": "en",
      "text": "I have to create the raw model which in",
      "offset": 11318.239,
      "duration": 5.361
    },
    {
      "lang": "en",
      "text": "the case of DDP is a model. module is",
      "offset": 11320.52,
      "duration": 6.12
    },
    {
      "lang": "en",
      "text": "where it stores the raw and then module",
      "offset": 11323.6,
      "duration": 6.2
    },
    {
      "lang": "en",
      "text": "of gpt2 as we have it which contains the",
      "offset": 11326.64,
      "duration": 4.679
    },
    {
      "lang": "en",
      "text": "uh configure optimizers function that we",
      "offset": 11329.8,
      "duration": 3.479
    },
    {
      "lang": "en",
      "text": "want to call so that's one thing that I",
      "offset": 11331.319,
      "duration": 4.761
    },
    {
      "lang": "en",
      "text": "have to fix otherwise this seems to run",
      "offset": 11333.279,
      "duration": 4.12
    },
    {
      "lang": "en",
      "text": "now one thing you'll notice is that when",
      "offset": 11336.08,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "you actually compare this run and the",
      "offset": 11337.399,
      "duration": 3.96
    },
    {
      "lang": "en",
      "text": "numbers in it to the just running a",
      "offset": 11339.52,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "single GPU you'll notice that this is",
      "offset": 11341.359,
      "duration": 5.561
    },
    {
      "lang": "en",
      "text": "single GPU run with 32 gratum the",
      "offset": 11344,
      "duration": 5.16
    },
    {
      "lang": "en",
      "text": "numbers won't exactly match",
      "offset": 11346.92,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "up and uh that's kind of a boring reason",
      "offset": 11349.16,
      "duration": 4.36
    },
    {
      "lang": "en",
      "text": "for why that happens uh the reason for",
      "offset": 11351.64,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "that is that in the data loader we're",
      "offset": 11353.52,
      "duration": 3.56
    },
    {
      "lang": "en",
      "text": "basically just iterating through batches",
      "offset": 11355.72,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "and slightly different way because now",
      "offset": 11357.08,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "we're looking for an entire page of data",
      "offset": 11358.84,
      "duration": 5.72
    },
    {
      "lang": "en",
      "text": "and if that page uh for all the gpus if",
      "offset": 11361.319,
      "duration": 5.601
    },
    {
      "lang": "en",
      "text": "that chunk exceeds the number of tokens",
      "offset": 11364.56,
      "duration": 4.52
    },
    {
      "lang": "en",
      "text": "we just Loop and so actually the single",
      "offset": 11366.92,
      "duration": 6.12
    },
    {
      "lang": "en",
      "text": "GPU and the H GPU process will end up um",
      "offset": 11369.08,
      "duration": 6.239
    },
    {
      "lang": "en",
      "text": "resetting in a slightly different Manner",
      "offset": 11373.04,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "and so our batches are slightly",
      "offset": 11375.319,
      "duration": 2.801
    },
    {
      "lang": "en",
      "text": "different and so we get slightly",
      "offset": 11376.56,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "different numbers but one way to",
      "offset": 11378.12,
      "duration": 4.6
    },
    {
      "lang": "en",
      "text": "convince yourself that this is okay it",
      "offset": 11379.92,
      "duration": 4.04
    },
    {
      "lang": "en",
      "text": "just make the total batch size much",
      "offset": 11382.72,
      "duration": 5.559
    },
    {
      "lang": "en",
      "text": "smaller and the b and a t and then um",
      "offset": 11383.96,
      "duration": 8.359
    },
    {
      "lang": "en",
      "text": "so I think I used uh 4 * 124 * 8 so I",
      "offset": 11388.279,
      "duration": 7.361
    },
    {
      "lang": "en",
      "text": "used 32768 as a total patch size and",
      "offset": 11392.319,
      "duration": 5.561
    },
    {
      "lang": "en",
      "text": "then um so I made sure that the single",
      "offset": 11395.64,
      "duration": 4.36
    },
    {
      "lang": "en",
      "text": "GPU will do eight creting accumulation",
      "offset": 11397.88,
      "duration": 4.76
    },
    {
      "lang": "en",
      "text": "steps and then the multi-gpu and then",
      "offset": 11400,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "you're reducing the boundary effects of",
      "offset": 11402.64,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "the data loader and you'll see that the",
      "offset": 11404.399,
      "duration": 4.281
    },
    {
      "lang": "en",
      "text": "numbers match up so long story short",
      "offset": 11406.16,
      "duration": 4.6
    },
    {
      "lang": "en",
      "text": "we're now going really really fast the",
      "offset": 11408.68,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "optimization is mostly consistent with",
      "offset": 11410.76,
      "duration": 5.559
    },
    {
      "lang": "en",
      "text": "gpt2 and three hyper parameters and uh",
      "offset": 11412.68,
      "duration": 5.559
    },
    {
      "lang": "en",
      "text": "we have outgrown our tiny Shakespeare",
      "offset": 11416.319,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "file and we want to upgrade it so let's",
      "offset": 11418.239,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "move to next to that next so let's now",
      "offset": 11420.56,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "take a look at what data sets were used",
      "offset": 11422.319,
      "duration": 5.321
    },
    {
      "lang": "en",
      "text": "by gpt2 and gpt3 so gbt2 used this web",
      "offset": 11423.76,
      "duration": 6.96
    },
    {
      "lang": "en",
      "text": "Text data set that was never released um",
      "offset": 11427.64,
      "duration": 4.759
    },
    {
      "lang": "en",
      "text": "there's an attempt at reproducing it",
      "offset": 11430.72,
      "duration": 3.8
    },
    {
      "lang": "en",
      "text": "called open web text uh so basically",
      "offset": 11432.399,
      "duration": 3.321
    },
    {
      "lang": "en",
      "text": "roughly speaking what they say here in",
      "offset": 11434.52,
      "duration": 3.24
    },
    {
      "lang": "en",
      "text": "the paper is that they scraped all",
      "offset": 11435.72,
      "duration": 5.4
    },
    {
      "lang": "en",
      "text": "outbound links from Reddit and then uh",
      "offset": 11437.76,
      "duration": 5.32
    },
    {
      "lang": "en",
      "text": "with at least three Karma and that was",
      "offset": 11441.12,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "kind of like their starting point and",
      "offset": 11443.08,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "they collected all the web P all the web",
      "offset": 11444.319,
      "duration": 3.801
    },
    {
      "lang": "en",
      "text": "pages and all the text in them and so",
      "offset": 11445.88,
      "duration": 4.439
    },
    {
      "lang": "en",
      "text": "this was 45 million links and this ended",
      "offset": 11448.12,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "up being 40 GB of text so uh so that's",
      "offset": 11450.319,
      "duration": 6.801
    },
    {
      "lang": "en",
      "text": "roughly what gpt2 says about its data",
      "offset": 11454.6,
      "duration": 3.96
    },
    {
      "lang": "en",
      "text": "set so it's basically outbound links",
      "offset": 11457.12,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "from Reddit now when we go over to gpt3",
      "offset": 11458.56,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "there's a training data set section and",
      "offset": 11461.6,
      "duration": 3.879
    },
    {
      "lang": "en",
      "text": "that's where they start to talk about um",
      "offset": 11463.56,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "common coll which is a lot more uh used",
      "offset": 11465.479,
      "duration": 6.361
    },
    {
      "lang": "en",
      "text": "actually I think even gpt2 talked about",
      "offset": 11469.72,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "common coll um but basically it's not a",
      "offset": 11471.84,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "very high quality data set all by itself",
      "offset": 11474.76,
      "duration": 3.639
    },
    {
      "lang": "en",
      "text": "because it is extremely noisy this is a",
      "offset": 11476.72,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "completely random subset of the internet",
      "offset": 11478.399,
      "duration": 4.521
    },
    {
      "lang": "en",
      "text": "and it's much worse than you think so",
      "offset": 11480.64,
      "duration": 3.88
    },
    {
      "lang": "en",
      "text": "people go into Great Lengths to filter",
      "offset": 11482.92,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "common craw because there's good stuff",
      "offset": 11484.52,
      "duration": 3.4
    },
    {
      "lang": "en",
      "text": "in it but most of it is just like ad",
      "offset": 11486.04,
      "duration": 4.6
    },
    {
      "lang": "en",
      "text": "spam random tables and numbers and stock",
      "offset": 11487.92,
      "duration": 8
    },
    {
      "lang": "en",
      "text": "tickers and uh it's just total mess",
      "offset": 11490.64,
      "duration": 8.08
    },
    {
      "lang": "en",
      "text": "so that's why people like to train on",
      "offset": 11495.92,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "these data mixtures that they curate and",
      "offset": 11498.72,
      "duration": 5.719
    },
    {
      "lang": "en",
      "text": "uh are careful with so a large chunk of",
      "offset": 11501.6,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "these data mixtures typically will be",
      "offset": 11504.439,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "common C like for example 50% of the",
      "offset": 11505.84,
      "duration": 4.599
    },
    {
      "lang": "en",
      "text": "tokens will be comic but then here in",
      "offset": 11507.88,
      "duration": 4.76
    },
    {
      "lang": "en",
      "text": "gpt3 they're also using web text to from",
      "offset": 11510.439,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "before so that's Reddit outbound but",
      "offset": 11512.64,
      "duration": 3.96
    },
    {
      "lang": "en",
      "text": "they're also adding for example books",
      "offset": 11514.6,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "and they're adding Wikipedia there's",
      "offset": 11516.6,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "many other things you can decide to add",
      "offset": 11518.2,
      "duration": 4.279
    },
    {
      "lang": "en",
      "text": "now this data set for gpt3 was also",
      "offset": 11520.68,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "never released so today some of the data",
      "offset": 11522.479,
      "duration": 3.8
    },
    {
      "lang": "en",
      "text": "sets that I'm familiar with that are",
      "offset": 11525.08,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "quite good and would be representative",
      "offset": 11526.279,
      "duration": 3.801
    },
    {
      "lang": "en",
      "text": "of something along these lines are",
      "offset": 11528.04,
      "duration": 4.76
    },
    {
      "lang": "en",
      "text": "number one the red pajama data set or",
      "offset": 11530.08,
      "duration": 4.6
    },
    {
      "lang": "en",
      "text": "more specifically for example the slim",
      "offset": 11532.8,
      "duration": 4.92
    },
    {
      "lang": "en",
      "text": "pajama subset of the red pajama data set",
      "offset": 11534.68,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "which is a cleaned and D duplicated",
      "offset": 11537.72,
      "duration": 3.88
    },
    {
      "lang": "en",
      "text": "version of it and just to give you a",
      "offset": 11539.8,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "sense again it's a bunch of common crawl",
      "offset": 11541.6,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "um C4 which is also as far as I know",
      "offset": 11544.279,
      "duration": 4.601
    },
    {
      "lang": "en",
      "text": "more common craw but processed",
      "offset": 11547.12,
      "duration": 3.8
    },
    {
      "lang": "en",
      "text": "differently and then we have GitHub",
      "offset": 11548.88,
      "duration": 4.84
    },
    {
      "lang": "en",
      "text": "books archive Wikipedia stack exchange",
      "offset": 11550.92,
      "duration": 4.12
    },
    {
      "lang": "en",
      "text": "these are the kinds of data sets that",
      "offset": 11553.72,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "would go into these data mixtures now",
      "offset": 11555.04,
      "duration": 3.72
    },
    {
      "lang": "en",
      "text": "specifically the one that I like that",
      "offset": 11557.319,
      "duration": 3.881
    },
    {
      "lang": "en",
      "text": "came out recently is called Fine web",
      "offset": 11558.76,
      "duration": 4.92
    },
    {
      "lang": "en",
      "text": "data set uh so this is an attempt to",
      "offset": 11561.2,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "basically collect really high quality",
      "offset": 11563.68,
      "duration": 5.24
    },
    {
      "lang": "en",
      "text": "common coll data and filter it in this",
      "offset": 11565.92,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "case to 15 trillion tokens and then in",
      "offset": 11568.92,
      "duration": 3.72
    },
    {
      "lang": "en",
      "text": "addition to that more recently",
      "offset": 11571.52,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "huggingface released this fine web edu",
      "offset": 11572.64,
      "duration": 5.599
    },
    {
      "lang": "en",
      "text": "subset which is 1.3 trillion of",
      "offset": 11575.279,
      "duration": 6.04
    },
    {
      "lang": "en",
      "text": "educational and 5.4 trillion of high",
      "offset": 11578.239,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "educational content so basically they're",
      "offset": 11581.319,
      "duration": 4.92
    },
    {
      "lang": "en",
      "text": "trying to filter common C to very high",
      "offset": 11583.439,
      "duration": 5.96
    },
    {
      "lang": "en",
      "text": "quality educational subsets and uh this",
      "offset": 11586.239,
      "duration": 5.16
    },
    {
      "lang": "en",
      "text": "is the one that we will use there's a",
      "offset": 11589.399,
      "duration": 5.161
    },
    {
      "lang": "en",
      "text": "long uh web page here on fine web and",
      "offset": 11591.399,
      "duration": 4.681
    },
    {
      "lang": "en",
      "text": "they go into a ton of detail about how",
      "offset": 11594.56,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "they process the data which is really",
      "offset": 11596.08,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "fascinating reading by the way and I",
      "offset": 11597.76,
      "duration": 2.92
    },
    {
      "lang": "en",
      "text": "would definitely recommend if you're",
      "offset": 11599.439,
      "duration": 3.281
    },
    {
      "lang": "en",
      "text": "interested into Data mixtures and so on",
      "offset": 11600.68,
      "duration": 3.559
    },
    {
      "lang": "en",
      "text": "and how data gets processed at these",
      "offset": 11602.72,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "scales a look at this uh page and more",
      "offset": 11604.239,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "specifically we'll be working with the",
      "offset": 11607.2,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "fine web edu I think and it's basically",
      "offset": 11608.479,
      "duration": 5.721
    },
    {
      "lang": "en",
      "text": "educational content from the",
      "offset": 11612.16,
      "duration": 4.119
    },
    {
      "lang": "en",
      "text": "internet uh they show that training on",
      "offset": 11614.2,
      "duration": 5.279
    },
    {
      "lang": "en",
      "text": "educational content in in their metrics",
      "offset": 11616.279,
      "duration": 7.321
    },
    {
      "lang": "en",
      "text": "um uh works really really well and we're",
      "offset": 11619.479,
      "duration": 6.92
    },
    {
      "lang": "en",
      "text": "going to use this sample 10 billion",
      "offset": 11623.6,
      "duration": 5.639
    },
    {
      "lang": "en",
      "text": "tokens subsample of it because we're not",
      "offset": 11626.399,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "going to be training on trillions of",
      "offset": 11629.239,
      "duration": 3.641
    },
    {
      "lang": "en",
      "text": "tokens uh we're just going to train on",
      "offset": 11630.319,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "uh 10 billion sample of the fine web edu",
      "offset": 11632.88,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "because empirically in my previous few",
      "offset": 11636.319,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "experiments this actually suffices to",
      "offset": 11638.08,
      "duration": 4.68
    },
    {
      "lang": "en",
      "text": "really get close to gpt2 Performance and",
      "offset": 11640.16,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "it's um simple enough to work with and",
      "offset": 11642.76,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "so let's work with the sample 10 uh BT",
      "offset": 11644.72,
      "duration": 5.96
    },
    {
      "lang": "en",
      "text": "so our goal will be to download it",
      "offset": 11647.72,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "process it and make sure that our data",
      "offset": 11650.68,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "loader can work with it so let's get to",
      "offset": 11652.72,
      "duration": 5.8
    },
    {
      "lang": "en",
      "text": "that okay so I introduced another um",
      "offset": 11655.08,
      "duration": 6.279
    },
    {
      "lang": "en",
      "text": "file here that will basically download",
      "offset": 11658.52,
      "duration": 5.48
    },
    {
      "lang": "en",
      "text": "Fine web edu from huging face data sets",
      "offset": 11661.359,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "it will pre-process and pre- tokenize",
      "offset": 11664,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "all of the data and it will save data",
      "offset": 11666.399,
      "duration": 7.96
    },
    {
      "lang": "en",
      "text": "shards to a uh folder on um local disk",
      "offset": 11668.88,
      "duration": 9.24
    },
    {
      "lang": "en",
      "text": "and so while this is running uh just",
      "offset": 11674.359,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "wanted to briefly mention that you can",
      "offset": 11678.12,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "kind of look through the data set viewer",
      "offset": 11680.359,
      "duration": 2.641
    },
    {
      "lang": "en",
      "text": "here just to get a sense of what's in",
      "offset": 11681.8,
      "duration": 3.32
    },
    {
      "lang": "en",
      "text": "here and it's kind of interesting I mean",
      "offset": 11683,
      "duration": 4.12
    },
    {
      "lang": "en",
      "text": "it's a it basically looks like it's",
      "offset": 11685.12,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "working fairly well like it's talking",
      "offset": 11687.12,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "about nuclear energy in France it's",
      "offset": 11688.8,
      "duration": 3.519
    },
    {
      "lang": "en",
      "text": "talking",
      "offset": 11691.439,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "about Mexican",
      "offset": 11692.319,
      "duration": 5.801
    },
    {
      "lang": "en",
      "text": "America some mac PJs Etc so actually it",
      "offset": 11694.399,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "seems like their filters are working",
      "offset": 11698.12,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "pretty well uh the filters here by the",
      "offset": 11699.6,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "way were applied automatically using um",
      "offset": 11701.8,
      "duration": 6.28
    },
    {
      "lang": "en",
      "text": "llama 370b I believe and so uh basically",
      "offset": 11704.56,
      "duration": 5.719
    },
    {
      "lang": "en",
      "text": "llms are judging which content is",
      "offset": 11708.08,
      "duration": 3.88
    },
    {
      "lang": "en",
      "text": "educational and that ends up making it",
      "offset": 11710.279,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "through the filter uh so that's pretty",
      "offset": 11711.96,
      "duration": 4.519
    },
    {
      "lang": "en",
      "text": "cool now in terms of the script itself",
      "offset": 11713.96,
      "duration": 3.64
    },
    {
      "lang": "en",
      "text": "I'm not going to go through the full",
      "offset": 11716.479,
      "duration": 3.161
    },
    {
      "lang": "en",
      "text": "script because it's not as interesting",
      "offset": 11717.6,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "and not as llm Centric but when you run",
      "offset": 11719.64,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "this basically number one we're going to",
      "offset": 11722.399,
      "duration": 4.281
    },
    {
      "lang": "en",
      "text": "load the data set uh which this is all",
      "offset": 11724.279,
      "duration": 4.12
    },
    {
      "lang": "en",
      "text": "huging face code running this you're",
      "offset": 11726.68,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "going to need to uh pip install data",
      "offset": 11728.399,
      "duration": 6.84
    },
    {
      "lang": "en",
      "text": "sets um so it's downloading the data set",
      "offset": 11731.16,
      "duration": 6.319
    },
    {
      "lang": "en",
      "text": "then it is tokenizing all of the",
      "offset": 11735.239,
      "duration": 4.521
    },
    {
      "lang": "en",
      "text": "documents inside this data set now when",
      "offset": 11737.479,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "we tokenize the documents you'll notice",
      "offset": 11739.76,
      "duration": 6.28
    },
    {
      "lang": "en",
      "text": "that um to tokenize a single document uh",
      "offset": 11742.199,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "we first",
      "offset": 11746.04,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "start the tokens with the end of text",
      "offset": 11747.479,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "token and this is a special token in the",
      "offset": 11749.72,
      "duration": 4.44
    },
    {
      "lang": "en",
      "text": "gpt2 tokenizer as you know so",
      "offset": 11751.72,
      "duration": 5.96
    },
    {
      "lang": "en",
      "text": "50256 is the ID of the end of text and",
      "offset": 11754.16,
      "duration": 5.119
    },
    {
      "lang": "en",
      "text": "this is what begins a document even",
      "offset": 11757.68,
      "duration": 3.519
    },
    {
      "lang": "en",
      "text": "though it's called end of text but this",
      "offset": 11759.279,
      "duration": 3.96
    },
    {
      "lang": "en",
      "text": "is uh the first token that begins a",
      "offset": 11761.199,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "document then we extend with all of the",
      "offset": 11763.239,
      "duration": 5.281
    },
    {
      "lang": "en",
      "text": "tokens of that document then we create a",
      "offset": 11766.08,
      "duration": 5.279
    },
    {
      "lang": "en",
      "text": "numpy array out of that we make sure",
      "offset": 11768.52,
      "duration": 6.4
    },
    {
      "lang": "en",
      "text": "that all the tokens are between",
      "offset": 11771.359,
      "duration": 6.201
    },
    {
      "lang": "en",
      "text": "oh okay let me debug this",
      "offset": 11774.92,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "okay so apologies for that uh it just",
      "offset": 11777.56,
      "duration": 3.639
    },
    {
      "lang": "en",
      "text": "had to do with me using a float division",
      "offset": 11779.319,
      "duration": 3.88
    },
    {
      "lang": "en",
      "text": "in Python it must be integer division so",
      "offset": 11781.199,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "that this is an INT and everything is",
      "offset": 11783.199,
      "duration": 5.361
    },
    {
      "lang": "en",
      "text": "nice um okay but basically the",
      "offset": 11785.12,
      "duration": 4.76
    },
    {
      "lang": "en",
      "text": "tokenization here is relatively",
      "offset": 11788.56,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "straightforward returns tokens in mp.",
      "offset": 11789.88,
      "duration": 5.96
    },
    {
      "lang": "en",
      "text": "un6 uh we're using .16 to save a little",
      "offset": 11792.88,
      "duration": 6.8
    },
    {
      "lang": "en",
      "text": "bit of space because 2 to the 16us 1 is",
      "offset": 11795.84,
      "duration": 7.2
    },
    {
      "lang": "en",
      "text": "65,000 so the gpt2 max token ID is well",
      "offset": 11799.68,
      "duration": 5.719
    },
    {
      "lang": "en",
      "text": "below that and then here there's a bunch",
      "offset": 11803.04,
      "duration": 4.04
    },
    {
      "lang": "en",
      "text": "of multiprocessing code and it's",
      "offset": 11805.399,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "honestly not that exciting so I'm not",
      "offset": 11807.08,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "going to step through it but we're",
      "offset": 11808.399,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "loading the data set we're tokenizing it",
      "offset": 11810.68,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "and we're saving everything to shards",
      "offset": 11812.8,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "and the shards are numpy files uh so",
      "offset": 11815.16,
      "duration": 6.6
    },
    {
      "lang": "en",
      "text": "just storing a numpy array and uh which",
      "offset": 11818.16,
      "duration": 5.4
    },
    {
      "lang": "en",
      "text": "is very very similar to torch",
      "offset": 11821.76,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "tensors and the first Shard 0000 is a",
      "offset": 11823.56,
      "duration": 5.639
    },
    {
      "lang": "en",
      "text": "Val a validation Shard and all the other",
      "offset": 11827.04,
      "duration": 5.319
    },
    {
      "lang": "en",
      "text": "shards are uh training shards and as I",
      "offset": 11829.199,
      "duration": 4.841
    },
    {
      "lang": "en",
      "text": "mentioned they all have 100 million",
      "offset": 11832.359,
      "duration": 5.361
    },
    {
      "lang": "en",
      "text": "tokens in them exactly um and and that",
      "offset": 11834.04,
      "duration": 6.399
    },
    {
      "lang": "en",
      "text": "just makes it easier to work with as to",
      "offset": 11837.72,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "Shard the files because if we just have",
      "offset": 11840.439,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "a single massive file sometimes they can",
      "offset": 11842.52,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "be hard to work with on the disk and so",
      "offset": 11844.04,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "sharting it is just kind of um nicer",
      "offset": 11846.76,
      "duration": 3.32
    },
    {
      "lang": "en",
      "text": "from that",
      "offset": 11848.84,
      "duration": 3.559
    },
    {
      "lang": "en",
      "text": "perspective and uh yeah so we'll just",
      "offset": 11850.08,
      "duration": 6.199
    },
    {
      "lang": "en",
      "text": "let this run this will be probably um",
      "offset": 11852.399,
      "duration": 5.721
    },
    {
      "lang": "en",
      "text": "30ish minutes or so and then we're going",
      "offset": 11856.279,
      "duration": 3.401
    },
    {
      "lang": "en",
      "text": "to come back to actually train on this",
      "offset": 11858.12,
      "duration": 3.119
    },
    {
      "lang": "en",
      "text": "data and we're going to be actually",
      "offset": 11859.68,
      "duration": 3.16
    },
    {
      "lang": "en",
      "text": "doing some legit pre-training in this",
      "offset": 11861.239,
      "duration": 4.281
    },
    {
      "lang": "en",
      "text": "case this is a good data set we're doing",
      "offset": 11862.84,
      "duration": 5.479
    },
    {
      "lang": "en",
      "text": "lots of tokens per second we have 8 gpus",
      "offset": 11865.52,
      "duration": 4.759
    },
    {
      "lang": "en",
      "text": "the code is ready and so we're actually",
      "offset": 11868.319,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "going to be doing a serious training run",
      "offset": 11870.279,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "so let's get P it back in a bit okay so",
      "offset": 11872,
      "duration": 6.92
    },
    {
      "lang": "en",
      "text": "we're back so uh if we LS edu fine web",
      "offset": 11874.76,
      "duration": 7.599
    },
    {
      "lang": "en",
      "text": "we see that there's now 100 charts in it",
      "offset": 11878.92,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "um and that makes sense because each",
      "offset": 11882.359,
      "duration": 3.96
    },
    {
      "lang": "en",
      "text": "chart is 100 million tokens so 100",
      "offset": 11883.8,
      "duration": 4.439
    },
    {
      "lang": "en",
      "text": "charts of that is 10 billion tokens in",
      "offset": 11886.319,
      "duration": 4.761
    },
    {
      "lang": "en",
      "text": "total now swinging over to the main file",
      "offset": 11888.239,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "I made some adjustments to our data",
      "offset": 11891.08,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "loader again and that's because we're",
      "offset": 11892.64,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "not running with uh Shakespeare anymore",
      "offset": 11894.68,
      "duration": 5.639
    },
    {
      "lang": "en",
      "text": "we want to use the fine web shards and",
      "offset": 11897.199,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "so you'll see some code here that",
      "offset": 11900.319,
      "duration": 3.08
    },
    {
      "lang": "en",
      "text": "additionally basically can load these",
      "offset": 11901.92,
      "duration": 6.12
    },
    {
      "lang": "en",
      "text": "shards uh we load the um un6 numpy file",
      "offset": 11903.399,
      "duration": 7.441
    },
    {
      "lang": "en",
      "text": "we convert it to a torch. long tensor",
      "offset": 11908.04,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "which is what a lot of the layers up top",
      "offset": 11910.84,
      "duration": 4.2
    },
    {
      "lang": "en",
      "text": "expect by default and then here we're",
      "offset": 11912.6,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "just enumerating all the shards I also",
      "offset": 11915.04,
      "duration": 5.239
    },
    {
      "lang": "en",
      "text": "added a split to data load of light so",
      "offset": 11918.12,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "we can uh load the split train but also",
      "offset": 11920.279,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "the split Val uh the zero",
      "offset": 11922.439,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "split and then we can load the shards",
      "offset": 11924.68,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "and then here we also have not just the",
      "offset": 11927.399,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "current position now but also the",
      "offset": 11929.72,
      "duration": 3.719
    },
    {
      "lang": "en",
      "text": "current Shard so we have a position",
      "offset": 11931,
      "duration": 4.76
    },
    {
      "lang": "en",
      "text": "inside A Shard and then when we uh run",
      "offset": 11933.439,
      "duration": 5.441
    },
    {
      "lang": "en",
      "text": "out of tokens in A Single Shard we first",
      "offset": 11935.76,
      "duration": 5.96
    },
    {
      "lang": "en",
      "text": "Advance The Shard and loop if we need to",
      "offset": 11938.88,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "and then we get the tokens and readjust",
      "offset": 11941.72,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "the position so this data loader will",
      "offset": 11943.76,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "now iterate all the shards as well so I",
      "offset": 11946.12,
      "duration": 5.4
    },
    {
      "lang": "en",
      "text": "Chang that and then the other thing that",
      "offset": 11949.92,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "I did while uh the data was processing",
      "offset": 11951.52,
      "duration": 5.679
    },
    {
      "lang": "en",
      "text": "is our train loader now has split train",
      "offset": 11954.64,
      "duration": 5.719
    },
    {
      "lang": "en",
      "text": "of course and down here I set up some I",
      "offset": 11957.199,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "set up some numbers",
      "offset": 11960.359,
      "duration": 4.441
    },
    {
      "lang": "en",
      "text": "so we are doing 2 to the",
      "offset": 11961.92,
      "duration": 9.76
    },
    {
      "lang": "en",
      "text": "9 uh tokens per uh per um per step and",
      "offset": 11964.8,
      "duration": 10.36
    },
    {
      "lang": "en",
      "text": "we want to do roughly 10 billion tokens",
      "offset": 11971.68,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "um because that's how many unique tokens",
      "offset": 11975.16,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "we have so if we did 10 billion tokens",
      "offset": 11976.72,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "then divide that by 29 we see that this",
      "offset": 11979.16,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "is 1973 steps so that's where that's",
      "offset": 11981.76,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "from and then the GPT three paper says",
      "offset": 11984.6,
      "duration": 4.839
    },
    {
      "lang": "en",
      "text": "that they warm up the learning rate over",
      "offset": 11987.52,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "375 million tokens so I came here and",
      "offset": 11989.439,
      "duration": 8.04
    },
    {
      "lang": "en",
      "text": "375 E6 tokens divide uh 2 to the",
      "offset": 11993.04,
      "duration": 8.439
    },
    {
      "lang": "en",
      "text": "19 is 715 steps so that's why warm-up",
      "offset": 11997.479,
      "duration": 6.92
    },
    {
      "lang": "en",
      "text": "steps is set to 715 so this will exactly",
      "offset": 12001.479,
      "duration": 6.161
    },
    {
      "lang": "en",
      "text": "match um the warm-up schedule that gpt3",
      "offset": 12004.399,
      "duration": 6.401
    },
    {
      "lang": "en",
      "text": "used and I think 715 by the way is very",
      "offset": 12007.64,
      "duration": 4.839
    },
    {
      "lang": "en",
      "text": "uh mild and this could be made",
      "offset": 12010.8,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "significantly more aggressive probably",
      "offset": 12012.479,
      "duration": 4.681
    },
    {
      "lang": "en",
      "text": "even like 100 is good enough um",
      "offset": 12013.92,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "but it's okay let's leave it for now so",
      "offset": 12017.16,
      "duration": 3.119
    },
    {
      "lang": "en",
      "text": "that we have the exact hyper parameters",
      "offset": 12018.88,
      "duration": 6.96
    },
    {
      "lang": "en",
      "text": "of gpt3 so I fix that and then um that's",
      "offset": 12020.279,
      "duration": 8.08
    },
    {
      "lang": "en",
      "text": "pretty much it we can we can run so we",
      "offset": 12025.84,
      "duration": 4.04
    },
    {
      "lang": "en",
      "text": "have our script",
      "offset": 12028.359,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "here and we can",
      "offset": 12029.88,
      "duration": 4.599
    },
    {
      "lang": "en",
      "text": "launch and actually sorry let me do one",
      "offset": 12032.439,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "more",
      "offset": 12034.479,
      "duration": 2.201
    },
    {
      "lang": "en",
      "text": "thing excuse",
      "offset": 12038,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "me for my GPU I can actually fit more",
      "offset": 12040.479,
      "duration": 5.481
    },
    {
      "lang": "en",
      "text": "batch size and I believe I can fat I can",
      "offset": 12043.92,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "fit 60 4 on my GPU as a micro bash size",
      "offset": 12045.96,
      "duration": 7.08
    },
    {
      "lang": "en",
      "text": "so let me try",
      "offset": 12050,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "that I could be misremembering but that",
      "offset": 12054.359,
      "duration": 5.88
    },
    {
      "lang": "en",
      "text": "means 64 * 124 per GPU and then we have",
      "offset": 12057.319,
      "duration": 5.321
    },
    {
      "lang": "en",
      "text": "a gpus so that means we would not even",
      "offset": 12060.239,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "be doing gradient accumulation if this",
      "offset": 12062.64,
      "duration": 4.2
    },
    {
      "lang": "en",
      "text": "fits because uh this just multi",
      "offset": 12064.319,
      "duration": 5.401
    },
    {
      "lang": "en",
      "text": "multiplies out to uh the full total bat",
      "offset": 12066.84,
      "duration": 5.32
    },
    {
      "lang": "en",
      "text": "size so no gradient",
      "offset": 12069.72,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "accumulation and that would run pretty",
      "offset": 12072.16,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "quickly if that fits",
      "offset": 12074.04,
      "duration": 3.64
    },
    {
      "lang": "en",
      "text": "let's go let's go I mean if this works",
      "offset": 12086.92,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "then this is basically a serious",
      "offset": 12089.8,
      "duration": 4.2
    },
    {
      "lang": "en",
      "text": "pre-training run um we're not logging",
      "offset": 12091.16,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "we're not evaluating the validation",
      "offset": 12094,
      "duration": 3.319
    },
    {
      "lang": "en",
      "text": "split we're not running any evaluations",
      "offset": 12095.399,
      "duration": 3.96
    },
    {
      "lang": "en",
      "text": "yet so it's not we haven't crossed our",
      "offset": 12097.319,
      "duration": 4.841
    },
    {
      "lang": "en",
      "text": "te's and dotted our eyes but uh if we",
      "offset": 12099.359,
      "duration": 4.84
    },
    {
      "lang": "en",
      "text": "let this run for a while we're going to",
      "offset": 12102.16,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "actually get a pretty good model and the",
      "offset": 12104.199,
      "duration": 4.841
    },
    {
      "lang": "en",
      "text": "model that might even be on par with or",
      "offset": 12106.8,
      "duration": 7.559
    },
    {
      "lang": "en",
      "text": "better than gpt2 124 M okay so it looks",
      "offset": 12109.04,
      "duration": 6.76
    },
    {
      "lang": "en",
      "text": "like everything is going great we're",
      "offset": 12114.359,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "processing 1.5 million tokens per",
      "offset": 12115.8,
      "duration": 7.599
    },
    {
      "lang": "en",
      "text": "second uh everything here looks good",
      "offset": 12118.76,
      "duration": 7.32
    },
    {
      "lang": "en",
      "text": "we're doing 330 milliseconds per",
      "offset": 12123.399,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "iteration and we have to do a total",
      "offset": 12126.08,
      "duration": 7.8
    },
    {
      "lang": "en",
      "text": "of uh where are we printing that 1973 so",
      "offset": 12129.319,
      "duration": 8.08
    },
    {
      "lang": "en",
      "text": "19073 times 0.33",
      "offset": 12133.88,
      "duration": 6.84
    },
    {
      "lang": "en",
      "text": "is this many seconds this many minutes",
      "offset": 12137.399,
      "duration": 7.04
    },
    {
      "lang": "en",
      "text": "so this will run for 1.7",
      "offset": 12140.72,
      "duration": 7.36
    },
    {
      "lang": "en",
      "text": "hours uh so one and a half hour run uh",
      "offset": 12144.439,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "like this and uh we don't even have to",
      "offset": 12148.08,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "use gradient accumulation which is nice",
      "offset": 12150.439,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "and you might not have that luxury in",
      "offset": 12151.92,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "your GPU in that case just start",
      "offset": 12153.399,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "decreasing the batch size until things",
      "offset": 12155.359,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "fit but keep it to nice",
      "offset": 12157,
      "duration": 5.359
    },
    {
      "lang": "en",
      "text": "numbers um so that's pretty exciting",
      "offset": 12159.359,
      "duration": 4.361
    },
    {
      "lang": "en",
      "text": "we're currently warming up the learning",
      "offset": 12162.359,
      "duration": 3.281
    },
    {
      "lang": "en",
      "text": "rate so you see that it's still very low",
      "offset": 12163.72,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "one4 so this will ramp up over the next",
      "offset": 12165.64,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "few steps all the way to 6 e",
      "offset": 12168.52,
      "duration": 5.16
    },
    {
      "lang": "en",
      "text": "Nega uh 4",
      "offset": 12170.84,
      "duration": 5.439
    },
    {
      "lang": "en",
      "text": "here very cool so now what I'd like to",
      "offset": 12173.68,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "do is uh let's cross the T and do our",
      "offset": 12176.279,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "eyes let's evaluate on the validation",
      "offset": 12178.319,
      "duration": 4.201
    },
    {
      "lang": "en",
      "text": "split and let's try to figure out how we",
      "offset": 12180.279,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "can run evals how we can do logging how",
      "offset": 12182.52,
      "duration": 4.679
    },
    {
      "lang": "en",
      "text": "we can visualize our losses and all the",
      "offset": 12185,
      "duration": 4.76
    },
    {
      "lang": "en",
      "text": "good stuff so let's get to that before",
      "offset": 12187.199,
      "duration": 4.441
    },
    {
      "lang": "en",
      "text": "we actually do the run okay so I've",
      "offset": 12189.76,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "adjusted the code so that we're",
      "offset": 12191.64,
      "duration": 3.48
    },
    {
      "lang": "en",
      "text": "evaluating on the validation split so",
      "offset": 12193.04,
      "duration": 3.96
    },
    {
      "lang": "en",
      "text": "creating the Val loader just by passing",
      "offset": 12195.12,
      "duration": 4.04
    },
    {
      "lang": "en",
      "text": "in Split equals Val that will basically",
      "offset": 12197,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "create a data loader just for the uh",
      "offset": 12199.16,
      "duration": 3.079
    },
    {
      "lang": "en",
      "text": "validation",
      "offset": 12201.16,
      "duration": 4.6
    },
    {
      "lang": "en",
      "text": "Shard um the other thing I did is in the",
      "offset": 12202.239,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "data loader I introduced a new function",
      "offset": 12205.76,
      "duration": 4.12
    },
    {
      "lang": "en",
      "text": "reset which is called at init and it",
      "offset": 12207.439,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "basically resets the data loader and",
      "offset": 12209.88,
      "duration": 4.28
    },
    {
      "lang": "en",
      "text": "that is very useful because when we come",
      "offset": 12211.92,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "to the main training Loop now so this is",
      "offset": 12214.16,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "the code that I've added and basically",
      "offset": 12217.04,
      "duration": 4.359
    },
    {
      "lang": "en",
      "text": "every 100th iteration including the",
      "offset": 12219.52,
      "duration": 4.52
    },
    {
      "lang": "en",
      "text": "zeroth iteration we put the model into",
      "offset": 12221.399,
      "duration": 6.04
    },
    {
      "lang": "en",
      "text": "evaluation mode we reset the Val loader",
      "offset": 12224.04,
      "duration": 6.76
    },
    {
      "lang": "en",
      "text": "and then um no gradients involved we're",
      "offset": 12227.439,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "going to",
      "offset": 12230.8,
      "duration": 4.04
    },
    {
      "lang": "en",
      "text": "basically accumulate the gradients over",
      "offset": 12232.64,
      "duration": 5.4
    },
    {
      "lang": "en",
      "text": "say 20 steps and then average it all up",
      "offset": 12234.84,
      "duration": 6.399
    },
    {
      "lang": "en",
      "text": "and print out the validation loss and so",
      "offset": 12238.04,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "that basically is the exact same logic",
      "offset": 12241.239,
      "duration": 5.08
    },
    {
      "lang": "en",
      "text": "as the training Loop roughly but there's",
      "offset": 12243.96,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "no loss that backward it's only",
      "offset": 12246.319,
      "duration": 3.16
    },
    {
      "lang": "en",
      "text": "inference we're just measuring the loss",
      "offset": 12247.88,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "we're adding it up everything else",
      "offset": 12249.479,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "otherwise applies and is exactly as",
      "offset": 12251.239,
      "duration": 3.96
    },
    {
      "lang": "en",
      "text": "we've seen it before and so this will",
      "offset": 12253.16,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "print the validation laws",
      "offset": 12255.199,
      "duration": 3.961
    },
    {
      "lang": "en",
      "text": "um every 100th iteration including on",
      "offset": 12256.92,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "the very first",
      "offset": 12259.16,
      "duration": 3.96
    },
    {
      "lang": "en",
      "text": "iteration uh so that's nice that will",
      "offset": 12260.52,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "tell us some amount some a little bit",
      "offset": 12263.12,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "about how much we're overfitting that",
      "offset": 12265,
      "duration": 4.359
    },
    {
      "lang": "en",
      "text": "said like uh we have roughly Infinity",
      "offset": 12267.199,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "data so we're mostly expecting our train",
      "offset": 12269.359,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "and Val loss to be about the same but",
      "offset": 12271.359,
      "duration": 3.641
    },
    {
      "lang": "en",
      "text": "the other reason I'm kind of interested",
      "offset": 12273.76,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "in this is because we can take the GPT",
      "offset": 12275,
      "duration": 4.68
    },
    {
      "lang": "en",
      "text": "2124m as openi released it we can",
      "offset": 12276.8,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "initialize from it and we can basically",
      "offset": 12279.68,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "see what kind of loss it achieves on the",
      "offset": 12281.68,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "validation loss as well and that gives",
      "offset": 12283.279,
      "duration": 4.601
    },
    {
      "lang": "en",
      "text": "us kind of an indication as to uh how",
      "offset": 12285.12,
      "duration": 4.44
    },
    {
      "lang": "en",
      "text": "much that model would generalize to 124",
      "offset": 12287.88,
      "duration": 5.08
    },
    {
      "lang": "en",
      "text": "M but it's not an sorry to fine web edu",
      "offset": 12289.56,
      "duration": 5.48
    },
    {
      "lang": "en",
      "text": "validation split that said it's not a",
      "offset": 12292.96,
      "duration": 4.04
    },
    {
      "lang": "en",
      "text": "super fair comparison to gpt2 because it",
      "offset": 12295.04,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "was trained on a very different data",
      "offset": 12297,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "distribution but it's still kind of like",
      "offset": 12298.239,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "an interesting data point and in any",
      "offset": 12300.04,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "case you would always want to have a",
      "offset": 12302.399,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "validation split in a training run like",
      "offset": 12303.88,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "this so that you can make sure that you",
      "offset": 12306.319,
      "duration": 5.441
    },
    {
      "lang": "en",
      "text": "are not um overfitting and this is",
      "offset": 12308.359,
      "duration": 5.561
    },
    {
      "lang": "en",
      "text": "especially a concern if we were to make",
      "offset": 12311.76,
      "duration": 5.16
    },
    {
      "lang": "en",
      "text": "more Epoch in our training data um so",
      "offset": 12313.92,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "for example right now we're just doing a",
      "offset": 12316.92,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "single Epoch but if we get to a point",
      "offset": 12318.319,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "where we want to train on 10 epochs or",
      "offset": 12320.359,
      "duration": 3.241
    },
    {
      "lang": "en",
      "text": "something like that we would be really",
      "offset": 12321.92,
      "duration": 4.2
    },
    {
      "lang": "en",
      "text": "careful with maybe we are memorizing",
      "offset": 12323.6,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "that data too much if we have a big",
      "offset": 12326.12,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "enough model and our validation split",
      "offset": 12328.08,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "would be one way to tell whether that is",
      "offset": 12330.6,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "happening okay and in addition to that",
      "offset": 12332.16,
      "duration": 3.96
    },
    {
      "lang": "en",
      "text": "if you remember at bottom of our script",
      "offset": 12334.439,
      "duration": 3.241
    },
    {
      "lang": "en",
      "text": "we had all of this orphaned code for",
      "offset": 12336.12,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "sampling from way back when so I deleted",
      "offset": 12337.68,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "that code and I moved it up um to here",
      "offset": 12340.439,
      "duration": 5.361
    },
    {
      "lang": "en",
      "text": "so once in a while we simply value",
      "offset": 12343.68,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "validation",
      "offset": 12345.8,
      "duration": 4.04
    },
    {
      "lang": "en",
      "text": "once in a while we sample we generate",
      "offset": 12346.8,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "samples and then uh we do that only",
      "offset": 12349.84,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "every 100 steps and we train on every",
      "offset": 12352.96,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "single step so that's how I have a",
      "offset": 12355.04,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "structure right now and I've been",
      "offset": 12356.72,
      "duration": 3.559
    },
    {
      "lang": "en",
      "text": "running this for 10,000 iterations so",
      "offset": 12358.239,
      "duration": 4.04
    },
    {
      "lang": "en",
      "text": "here are some samples on neration",
      "offset": 12360.279,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "1,000",
      "offset": 12362.279,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "um hello I'm a language model and I'm",
      "offset": 12365,
      "duration": 4.04
    },
    {
      "lang": "en",
      "text": "not able to get more",
      "offset": 12367.16,
      "duration": 3.72
    },
    {
      "lang": "en",
      "text": "creative I'm a language model and",
      "offset": 12369.04,
      "duration": 3.399
    },
    {
      "lang": "en",
      "text": "languages file you're learning about",
      "offset": 12370.88,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "here is or is the beginning of a",
      "offset": 12372.439,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "computer",
      "offset": 12374.16,
      "duration": 5.239
    },
    {
      "lang": "en",
      "text": "okay so this is all like pretty uh this",
      "offset": 12376.359,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "is still a garble uh but we're only at",
      "offset": 12379.399,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "ration 1,000 and we've only just barely",
      "offset": 12381.479,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "reached maximum learning rate uh so this",
      "offset": 12384.279,
      "duration": 4.441
    },
    {
      "lang": "en",
      "text": "is still learning uh we're about to get",
      "offset": 12386.279,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "some more samples coming up in",
      "offset": 12388.72,
      "duration": 3.479
    },
    {
      "lang": "en",
      "text": "1,00 okay",
      "offset": 12392.279,
      "duration": 5.96
    },
    {
      "lang": "en",
      "text": "um okay this is you know the model is",
      "offset": 12395.56,
      "duration": 6.52
    },
    {
      "lang": "en",
      "text": "still is still a young baby okay so uh",
      "offset": 12398.239,
      "duration": 6.281
    },
    {
      "lang": "en",
      "text": "basically all of this sampling code that",
      "offset": 12402.08,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "I've put here everything should be",
      "offset": 12404.52,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "familiar with to you and came from",
      "offset": 12405.92,
      "duration": 3.559
    },
    {
      "lang": "en",
      "text": "before the only thing that I did is I",
      "offset": 12407.72,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "created a generator object in pytorch so",
      "offset": 12409.479,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "that I have a direct control over the",
      "offset": 12412.199,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "sampling of the random numbers don't",
      "offset": 12414.439,
      "duration": 4.201
    },
    {
      "lang": "en",
      "text": "because I don't want to impact the RNG",
      "offset": 12416.68,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "state of the random number generator",
      "offset": 12418.64,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "that is the global one used for training",
      "offset": 12420.52,
      "duration": 3.879
    },
    {
      "lang": "en",
      "text": "I want this to be completely outside of",
      "offset": 12422.96,
      "duration": 4.12
    },
    {
      "lang": "en",
      "text": "the training Loop and so I'm using a",
      "offset": 12424.399,
      "duration": 5.281
    },
    {
      "lang": "en",
      "text": "special sampling RNG and then I make",
      "offset": 12427.08,
      "duration": 5.08
    },
    {
      "lang": "en",
      "text": "sure to seed it that every single rank",
      "offset": 12429.68,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "has a different seed and then I pass in",
      "offset": 12432.16,
      "duration": 4.84
    },
    {
      "lang": "en",
      "text": "here where we sort of consumer in the",
      "offset": 12434.88,
      "duration": 3.8
    },
    {
      "lang": "en",
      "text": "numbers in multinomial where the",
      "offset": 12437,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "sampling happens I make sure to pass in",
      "offset": 12438.68,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "the generator object there otherwise",
      "offset": 12440.84,
      "duration": 4.68
    },
    {
      "lang": "en",
      "text": "this is identical uh now the other thing",
      "offset": 12442.76,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "is um you'll notice that we're running a",
      "offset": 12445.52,
      "duration": 3.799
    },
    {
      "lang": "en",
      "text": "bit slower that's because I actually had",
      "offset": 12447.479,
      "duration": 4.521
    },
    {
      "lang": "en",
      "text": "to disable torch. compile to get this to",
      "offset": 12449.319,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "sample and um so we're running a bit",
      "offset": 12452,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "slower so for some reason it works with",
      "offset": 12454.52,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "no torch compile but when I torch",
      "offset": 12456.399,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "compile my model I get a really scary",
      "offset": 12457.72,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "error from pytorch and I have no idea",
      "offset": 12459.84,
      "duration": 3.639
    },
    {
      "lang": "en",
      "text": "how to resolve it right now so probably",
      "offset": 12461.479,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "by the time you see this code released",
      "offset": 12463.479,
      "duration": 3.8
    },
    {
      "lang": "en",
      "text": "or something like that maybe it's fixed",
      "offset": 12465.56,
      "duration": 3.56
    },
    {
      "lang": "en",
      "text": "but for now I'm just going to do end",
      "offset": 12467.279,
      "duration": 3.881
    },
    {
      "lang": "en",
      "text": "false um and I'm going to bring back",
      "offset": 12469.12,
      "duration": 5.159
    },
    {
      "lang": "en",
      "text": "toor compile and you're not going to get",
      "offset": 12471.16,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "samples and I I think I'll fix this",
      "offset": 12474.279,
      "duration": 4.841
    },
    {
      "lang": "en",
      "text": "later uh by the way um I will be",
      "offset": 12476.52,
      "duration": 4.679
    },
    {
      "lang": "en",
      "text": "releasing all this code and actually",
      "offset": 12479.12,
      "duration": 3.88
    },
    {
      "lang": "en",
      "text": "I've been very careful about making get",
      "offset": 12481.199,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "commits every time we add something and",
      "offset": 12483,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "so I'm going to release the entire repo",
      "offset": 12485.199,
      "duration": 4.04
    },
    {
      "lang": "en",
      "text": "that starts completely from scratch all",
      "offset": 12487.16,
      "duration": 5.48
    },
    {
      "lang": "en",
      "text": "the way to uh now and after this as well",
      "offset": 12489.239,
      "duration": 4.681
    },
    {
      "lang": "en",
      "text": "and so everything should be exactly",
      "offset": 12492.64,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "documented in the git commit history um",
      "offset": 12493.92,
      "duration": 5.08
    },
    {
      "lang": "en",
      "text": "um and so I think that will be nice so",
      "offset": 12496.239,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "hopefully by the time you go to GitHub",
      "offset": 12499,
      "duration": 3.479
    },
    {
      "lang": "en",
      "text": "uh this is removed and it's working and",
      "offset": 12500.72,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "I will have fixed the bug okay so I have",
      "offset": 12502.479,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "the optimization running here and it's",
      "offset": 12504.479,
      "duration": 4.441
    },
    {
      "lang": "en",
      "text": "stepping and we're on step 6,000 or so",
      "offset": 12506.88,
      "duration": 4.439
    },
    {
      "lang": "en",
      "text": "so we're about 30% through training now",
      "offset": 12508.92,
      "duration": 3.559
    },
    {
      "lang": "en",
      "text": "while this is training I would like to",
      "offset": 12511.319,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "introduce one evaluation that we're",
      "offset": 12512.479,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "going to use to supplement the",
      "offset": 12514.319,
      "duration": 4.601
    },
    {
      "lang": "en",
      "text": "validation set and that is the H swag",
      "offset": 12515.84,
      "duration": 6.68
    },
    {
      "lang": "en",
      "text": "eval so hos swag comes from this paper",
      "offset": 12518.92,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "back in 2019 so it's a 5-year-old eval",
      "offset": 12522.52,
      "duration": 4.959
    },
    {
      "lang": "en",
      "text": "now and the way H swag works is there is",
      "offset": 12524.68,
      "duration": 5.679
    },
    {
      "lang": "en",
      "text": "basically a sentence completion data set",
      "offset": 12527.479,
      "duration": 5.281
    },
    {
      "lang": "en",
      "text": "so it's a multiple choice for every one",
      "offset": 12530.359,
      "duration": 4.601
    },
    {
      "lang": "en",
      "text": "of these questions we have uh basically",
      "offset": 12532.76,
      "duration": 4.84
    },
    {
      "lang": "en",
      "text": "a shared context like a woman is outside",
      "offset": 12534.96,
      "duration": 4.6
    },
    {
      "lang": "en",
      "text": "with a bucket and a dog the dog is",
      "offset": 12537.6,
      "duration": 5.08
    },
    {
      "lang": "en",
      "text": "running around trying to avoid bath she",
      "offset": 12539.56,
      "duration": 5.4
    },
    {
      "lang": "en",
      "text": "a Rises the bucket off with soap and",
      "offset": 12542.68,
      "duration": 6.04
    },
    {
      "lang": "en",
      "text": "blow dry the dog's head B uses a hose to",
      "offset": 12544.96,
      "duration": 6.359
    },
    {
      "lang": "en",
      "text": "keep it from getting soapy C gets the",
      "offset": 12548.72,
      "duration": 5.519
    },
    {
      "lang": "en",
      "text": "dog wet and it runs away again or D gets",
      "offset": 12551.319,
      "duration": 5.441
    },
    {
      "lang": "en",
      "text": "into a bathtub with the dog",
      "offset": 12554.239,
      "duration": 5.16
    },
    {
      "lang": "en",
      "text": "and so basically the idea is that these",
      "offset": 12556.76,
      "duration": 5.32
    },
    {
      "lang": "en",
      "text": "multiple choice are constructed so that",
      "offset": 12559.399,
      "duration": 6.241
    },
    {
      "lang": "en",
      "text": "one of them is a natural continuation of",
      "offset": 12562.08,
      "duration": 7.96
    },
    {
      "lang": "en",
      "text": "the um sentence and the others are",
      "offset": 12565.64,
      "duration": 6.839
    },
    {
      "lang": "en",
      "text": "not and uh the others might not make",
      "offset": 12570.04,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "sense like uses the host to keep it from",
      "offset": 12572.479,
      "duration": 3.641
    },
    {
      "lang": "en",
      "text": "getting soaped that makes no sense and",
      "offset": 12574.12,
      "duration": 4.04
    },
    {
      "lang": "en",
      "text": "so what happens is that models that are",
      "offset": 12576.12,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "not trained very well are not able to",
      "offset": 12578.16,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "tell these apart but models that have a",
      "offset": 12580.279,
      "duration": 5.481
    },
    {
      "lang": "en",
      "text": "lot of World Knowledge and can tell uh",
      "offset": 12583.04,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "which um and can tell a lot about the",
      "offset": 12585.76,
      "duration": 4.439
    },
    {
      "lang": "en",
      "text": "world will be able to create these",
      "offset": 12588.04,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "completions and these sentences are",
      "offset": 12590.199,
      "duration": 5.481
    },
    {
      "lang": "en",
      "text": "sourced from activity net and from Wiki",
      "offset": 12592.76,
      "duration": 7.4
    },
    {
      "lang": "en",
      "text": "how and at the bottom of the uh",
      "offset": 12595.68,
      "duration": 7.679
    },
    {
      "lang": "en",
      "text": "paper there's kind of like a cool chart",
      "offset": 12600.16,
      "duration": 5.64
    },
    {
      "lang": "en",
      "text": "of the kinds of domains in Wiki house so",
      "offset": 12603.359,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "there's a lot of sentences from",
      "offset": 12605.8,
      "duration": 3.479
    },
    {
      "lang": "en",
      "text": "computers and electronics and Homes and",
      "offset": 12607.6,
      "duration": 3.48
    },
    {
      "lang": "en",
      "text": "Garden and it has kind of a broad",
      "offset": 12609.279,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "coverage of the kinds of things you need",
      "offset": 12611.08,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "to know about the world in order to find",
      "offset": 12613.359,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "the most likely completion and um the",
      "offset": 12615.56,
      "duration": 6.679
    },
    {
      "lang": "en",
      "text": "identity of that of that completion one",
      "offset": 12619.199,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "more thing that's kind of interesting",
      "offset": 12622.239,
      "duration": 2.801
    },
    {
      "lang": "en",
      "text": "about H swag is the way it was",
      "offset": 12623.199,
      "duration": 5.361
    },
    {
      "lang": "en",
      "text": "constructed is that the incorrect um",
      "offset": 12625.04,
      "duration": 7.279
    },
    {
      "lang": "en",
      "text": "options are deliberately um",
      "offset": 12628.56,
      "duration": 6.32
    },
    {
      "lang": "en",
      "text": "adversarially sourced so they're not",
      "offset": 12632.319,
      "duration": 4.681
    },
    {
      "lang": "en",
      "text": "just random sentences they're actually",
      "offset": 12634.88,
      "duration": 4.439
    },
    {
      "lang": "en",
      "text": "sentences generated by language models",
      "offset": 12637,
      "duration": 4.12
    },
    {
      "lang": "en",
      "text": "and they're generated in such a way that",
      "offset": 12639.319,
      "duration": 3.521
    },
    {
      "lang": "en",
      "text": "language models basically find them",
      "offset": 12641.12,
      "duration": 4.119
    },
    {
      "lang": "en",
      "text": "difficult but humans find them easy and",
      "offset": 12642.84,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "so they mentioned that humans have a 95%",
      "offset": 12645.239,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "accuracy on this set but at the time the",
      "offset": 12647.64,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "state-of-the-art language models had",
      "offset": 12649.96,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "only 48% and so at the time this was a",
      "offset": 12651.399,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "good Benchmark now you can read the",
      "offset": 12654.12,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "details of this paper to to learn more",
      "offset": 12657.479,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "um the thing to point out though is that",
      "offset": 12659.96,
      "duration": 3.8
    },
    {
      "lang": "en",
      "text": "this is 5 years ago and since then what",
      "offset": 12661.88,
      "duration": 3.479
    },
    {
      "lang": "en",
      "text": "happened to H swag is that it's been",
      "offset": 12663.76,
      "duration": 4.599
    },
    {
      "lang": "en",
      "text": "totally just uh",
      "offset": 12665.359,
      "duration": 6.361
    },
    {
      "lang": "en",
      "text": "um solved and so now the language models",
      "offset": 12668.359,
      "duration": 6.601
    },
    {
      "lang": "en",
      "text": "here are 96% so basically the 4% the",
      "offset": 12671.72,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "last 4% is probably errors in the data",
      "offset": 12674.96,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "set or the questions are really really",
      "offset": 12676.76,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "hard and so basically this data set is",
      "offset": 12678.8,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "kind of crushed with respect to language",
      "offset": 12680.84,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "models but back then the best language",
      "offset": 12682.239,
      "duration": 5.641
    },
    {
      "lang": "en",
      "text": "model was only at about 50% uh but this",
      "offset": 12683.96,
      "duration": 6.84
    },
    {
      "lang": "en",
      "text": "is how far things got but still the the",
      "offset": 12687.88,
      "duration": 5.519
    },
    {
      "lang": "en",
      "text": "reason people like H swag and it's not",
      "offset": 12690.8,
      "duration": 6.28
    },
    {
      "lang": "en",
      "text": "used by the way in gpt2 but in gpt3",
      "offset": 12693.399,
      "duration": 6.281
    },
    {
      "lang": "en",
      "text": "there is H swag eval and lots of people",
      "offset": 12697.08,
      "duration": 4.279
    },
    {
      "lang": "en",
      "text": "use H",
      "offset": 12699.68,
      "duration": 5.719
    },
    {
      "lang": "en",
      "text": "swag and so for gpt3 we have results",
      "offset": 12701.359,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "here",
      "offset": 12705.399,
      "duration": 3.241
    },
    {
      "lang": "en",
      "text": "that are cited so we know what percent",
      "offset": 12706.359,
      "duration": 5.561
    },
    {
      "lang": "en",
      "text": "accuracies gpt3 um attains at all these",
      "offset": 12708.64,
      "duration": 5.599
    },
    {
      "lang": "en",
      "text": "different model checkpoints for H swag",
      "offset": 12711.92,
      "duration": 4.68
    },
    {
      "lang": "en",
      "text": "eval and the reason people like it is",
      "offset": 12714.239,
      "duration": 5.481
    },
    {
      "lang": "en",
      "text": "because H swag is a smooth eval and it",
      "offset": 12716.6,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "is an eval that offers quote unquote",
      "offset": 12719.72,
      "duration": 4.92
    },
    {
      "lang": "en",
      "text": "early signal uh so early signal means",
      "offset": 12721.96,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "that even small language models are",
      "offset": 12724.64,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "going to start at the random chance of",
      "offset": 12726.6,
      "duration": 4.599
    },
    {
      "lang": "en",
      "text": "25% but they're going to slowly improve",
      "offset": 12728.96,
      "duration": 6.2
    },
    {
      "lang": "en",
      "text": "and you're going to see 25 26 27 Etc and",
      "offset": 12731.199,
      "duration": 6.361
    },
    {
      "lang": "en",
      "text": "uh you can see slow Improvement even",
      "offset": 12735.16,
      "duration": 4.039
    },
    {
      "lang": "en",
      "text": "when the models are very small and it's",
      "offset": 12737.56,
      "duration": 5.799
    },
    {
      "lang": "en",
      "text": "very early so it's smooth it has early",
      "offset": 12739.199,
      "duration": 7.361
    },
    {
      "lang": "en",
      "text": "signal and um it's been around for a",
      "offset": 12743.359,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "long time so that's why people kind of",
      "offset": 12746.56,
      "duration": 3.16
    },
    {
      "lang": "en",
      "text": "like this",
      "offset": 12748.56,
      "duration": 3.48
    },
    {
      "lang": "en",
      "text": "eval uh now the way that we're going to",
      "offset": 12749.72,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "evaluate this is as",
      "offset": 12752.04,
      "duration": 5.439
    },
    {
      "lang": "en",
      "text": "follows as I mentioned we have a shared",
      "offset": 12754.72,
      "duration": 4.44
    },
    {
      "lang": "en",
      "text": "context and this is kind of like a",
      "offset": 12757.479,
      "duration": 3.8
    },
    {
      "lang": "en",
      "text": "multiple choice task but instead of",
      "offset": 12759.16,
      "duration": 3.8
    },
    {
      "lang": "en",
      "text": "giving the model a multiple choice",
      "offset": 12761.279,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "question and asking it for A B C or D uh",
      "offset": 12762.96,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "we can't do that because these models",
      "offset": 12766.279,
      "duration": 3.641
    },
    {
      "lang": "en",
      "text": "when they are so small as we are seeing",
      "offset": 12767.92,
      "duration": 3.559
    },
    {
      "lang": "en",
      "text": "here the models can't actually do",
      "offset": 12769.92,
      "duration": 3.559
    },
    {
      "lang": "en",
      "text": "multiple choice they don't understand",
      "offset": 12771.479,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "the concept of associating a label to",
      "offset": 12773.479,
      "duration": 4.601
    },
    {
      "lang": "en",
      "text": "one of the options of multiple choice uh",
      "offset": 12775.8,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "they don't understand that so we have to",
      "offset": 12778.08,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "give it to them in a native form and the",
      "offset": 12779.72,
      "duration": 5.32
    },
    {
      "lang": "en",
      "text": "native form is a token completion so",
      "offset": 12781.68,
      "duration": 5.24
    },
    {
      "lang": "en",
      "text": "here's what we do we construct a batch",
      "offset": 12785.04,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "of four rows and uh T tokens whatever",
      "offset": 12786.92,
      "duration": 6.359
    },
    {
      "lang": "en",
      "text": "that t happens to be then the shared",
      "offset": 12790.64,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "context that is basically the context",
      "offset": 12793.279,
      "duration": 4.601
    },
    {
      "lang": "en",
      "text": "for the for choices the tokens of that",
      "offset": 12795.199,
      "duration": 5.641
    },
    {
      "lang": "en",
      "text": "are shared across all of the rows and",
      "offset": 12797.88,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "then we have the four options so we kind",
      "offset": 12800.84,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "of like lay them out and then only one",
      "offset": 12802.84,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "of the options is correct in this case",
      "offset": 12805.08,
      "duration": 5.279
    },
    {
      "lang": "en",
      "text": "label three option three and so um this",
      "offset": 12806.68,
      "duration": 5.519
    },
    {
      "lang": "en",
      "text": "is the correct option and option one two",
      "offset": 12810.359,
      "duration": 2.84
    },
    {
      "lang": "en",
      "text": "and for are",
      "offset": 12812.199,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "incorrect now these options might be of",
      "offset": 12813.199,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "different lengths so what we do is we",
      "offset": 12816.04,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "sort of like take the longest length and",
      "offset": 12818.08,
      "duration": 4.68
    },
    {
      "lang": "en",
      "text": "that's the size of the batch B BYT and",
      "offset": 12820.04,
      "duration": 5.359
    },
    {
      "lang": "en",
      "text": "then some of these uh here are going to",
      "offset": 12822.76,
      "duration": 4.679
    },
    {
      "lang": "en",
      "text": "be pded Dimensions so they're going to",
      "offset": 12825.399,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "be unused and so we need the tokens we",
      "offset": 12827.439,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "need the correct label and we need a",
      "offset": 12831.479,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "mask that tells us which tokens are",
      "offset": 12833.359,
      "duration": 5.161
    },
    {
      "lang": "en",
      "text": "active and the mask is then zero for",
      "offset": 12835.8,
      "duration": 6.04
    },
    {
      "lang": "en",
      "text": "these uh padded areas so that's how we",
      "offset": 12838.52,
      "duration": 5.719
    },
    {
      "lang": "en",
      "text": "construct these batches and then in",
      "offset": 12841.84,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "order to get the language model to",
      "offset": 12844.239,
      "duration": 4.681
    },
    {
      "lang": "en",
      "text": "predict A B C or D the way this works is",
      "offset": 12845.52,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "basically we're just going to look at",
      "offset": 12848.92,
      "duration": 3.8
    },
    {
      "lang": "en",
      "text": "the tokens their probabilities and we're",
      "offset": 12850.16,
      "duration": 5.72
    },
    {
      "lang": "en",
      "text": "going to pick the option that gets the",
      "offset": 12852.72,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "lowest or the highest average",
      "offset": 12855.88,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "probability for the token so for the",
      "offset": 12858.64,
      "duration": 6.639
    },
    {
      "lang": "en",
      "text": "tokens because that is the most likely",
      "offset": 12862.04,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "completion according to the language",
      "offset": 12865.279,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "model so we're just going to look at the",
      "offset": 12867,
      "duration": 6.12
    },
    {
      "lang": "en",
      "text": "um probabilities here and average them",
      "offset": 12869.84,
      "duration": 5.96
    },
    {
      "lang": "en",
      "text": "up across the options and pick the one",
      "offset": 12873.12,
      "duration": 4.92
    },
    {
      "lang": "en",
      "text": "with the highest probability roughly",
      "offset": 12875.8,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "speaking so this is how we're going to",
      "offset": 12878.04,
      "duration": 4.12
    },
    {
      "lang": "en",
      "text": "do H swag",
      "offset": 12880.279,
      "duration": 5.721
    },
    {
      "lang": "en",
      "text": "um and this is I believe also how uh",
      "offset": 12882.16,
      "duration": 8.52
    },
    {
      "lang": "en",
      "text": "gpt3 did it um this is how gpt3 did it",
      "offset": 12886,
      "duration": 6.68
    },
    {
      "lang": "en",
      "text": "as far as I know but you should note",
      "offset": 12890.68,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "that some of the other evals where you",
      "offset": 12892.68,
      "duration": 4.36
    },
    {
      "lang": "en",
      "text": "might see H swag may not do it this way",
      "offset": 12894.359,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "they may do it in a multiple choice",
      "offset": 12897.04,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "format where you sort of uh give the the",
      "offset": 12898.359,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "context a single time and then the four",
      "offset": 12900.88,
      "duration": 4.12
    },
    {
      "lang": "en",
      "text": "completions and so the model is able to",
      "offset": 12902.92,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "see all the four options before it picks",
      "offset": 12905,
      "duration": 3.96
    },
    {
      "lang": "en",
      "text": "the best possible option and that's",
      "offset": 12907,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "actually an easier task for a model",
      "offset": 12908.96,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "because you get to see the other options",
      "offset": 12911.08,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "when you're picking your choice um but",
      "offset": 12912.88,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "unfortunately models at our size can't",
      "offset": 12915.8,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "do that only models at a bigger size are",
      "offset": 12917.52,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "able to do that and so our models are",
      "offset": 12920.04,
      "duration": 3.64
    },
    {
      "lang": "en",
      "text": "actually slightly handicapped in this",
      "offset": 12922.239,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "way that they are not going to see the",
      "offset": 12923.68,
      "duration": 3.96
    },
    {
      "lang": "en",
      "text": "other options they're only going to see",
      "offset": 12925.6,
      "duration": 3.96
    },
    {
      "lang": "en",
      "text": "one option at a time and they just have",
      "offset": 12927.64,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "to assign probabilities and the correct",
      "offset": 12929.56,
      "duration": 4.6
    },
    {
      "lang": "en",
      "text": "option has to win out in this metric all",
      "offset": 12931.479,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "right so let's now implement this very",
      "offset": 12934.16,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "briefly and incorporate it into our",
      "offset": 12936.279,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "script okay so what I've done here is",
      "offset": 12938.239,
      "duration": 4.04
    },
    {
      "lang": "en",
      "text": "I've introduced a new file called hell",
      "offset": 12940.439,
      "duration": 4.601
    },
    {
      "lang": "en",
      "text": "swag. py that you can take a look into",
      "offset": 12942.279,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "and I'm not going to to step through all",
      "offset": 12945.04,
      "duration": 3.88
    },
    {
      "lang": "en",
      "text": "of it because uh this is not exactly",
      "offset": 12946.359,
      "duration": 5.241
    },
    {
      "lang": "en",
      "text": "like deep code deep code it's kind of",
      "offset": 12948.92,
      "duration": 4.76
    },
    {
      "lang": "en",
      "text": "like a little bit tedious honestly",
      "offset": 12951.6,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "because what's happening is I'm",
      "offset": 12953.68,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "downloading hsac from GitHub and I'm",
      "offset": 12954.56,
      "duration": 3.879
    },
    {
      "lang": "en",
      "text": "rendering all of its examples and there",
      "offset": 12956.88,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "are a total of 10,000 examples I am",
      "offset": 12958.439,
      "duration": 5.96
    },
    {
      "lang": "en",
      "text": "rendering them into this format um and",
      "offset": 12960.64,
      "duration": 6.559
    },
    {
      "lang": "en",
      "text": "so here at the end of this render",
      "offset": 12964.399,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "example function you can see that I'm",
      "offset": 12967.199,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "returning the",
      "offset": 12969.6,
      "duration": 6.839
    },
    {
      "lang": "en",
      "text": "tokens uh the tokens of this um 4xt",
      "offset": 12971.12,
      "duration": 8.04
    },
    {
      "lang": "en",
      "text": "uh array of Tokens The Mask which tells",
      "offset": 12976.439,
      "duration": 5.241
    },
    {
      "lang": "en",
      "text": "us which parts are the options and",
      "offset": 12979.16,
      "duration": 5.279
    },
    {
      "lang": "en",
      "text": "everything else is zero and the label",
      "offset": 12981.68,
      "duration": 5.08
    },
    {
      "lang": "en",
      "text": "that is the correct label and so that",
      "offset": 12984.439,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "allows us to then iterate the examples",
      "offset": 12986.76,
      "duration": 3.4
    },
    {
      "lang": "en",
      "text": "and render them and I have an evaluate",
      "offset": 12988.279,
      "duration": 5.601
    },
    {
      "lang": "en",
      "text": "function here which can load a um gpt2",
      "offset": 12990.16,
      "duration": 6.52
    },
    {
      "lang": "en",
      "text": "from huging face and it runs the eval",
      "offset": 12993.88,
      "duration": 6.72
    },
    {
      "lang": "en",
      "text": "here um and it basically just calculates",
      "offset": 12996.68,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "uh just as I described it predicts the",
      "offset": 13000.6,
      "duration": 4.44
    },
    {
      "lang": "en",
      "text": "option that has the lowest or the",
      "offset": 13002.76,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "highest prob ility and the way to do",
      "offset": 13005.04,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "that actually is we can basically",
      "offset": 13007.76,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "evaluate the cross entropy loss so we're",
      "offset": 13008.96,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "basically evaluating the loss of",
      "offset": 13011.6,
      "duration": 3.799
    },
    {
      "lang": "en",
      "text": "predicting the next token in a sequence",
      "offset": 13013.359,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "and then we're looking at the row that",
      "offset": 13015.399,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "has the lowest average loss and that's",
      "offset": 13017.359,
      "duration": 7.04
    },
    {
      "lang": "en",
      "text": "the uh option that we pick as the",
      "offset": 13021.479,
      "duration": 5.081
    },
    {
      "lang": "en",
      "text": "prediction and then we do some stats and",
      "offset": 13024.399,
      "duration": 4.361
    },
    {
      "lang": "en",
      "text": "prints and stuff like that so that is a",
      "offset": 13026.56,
      "duration": 4.6
    },
    {
      "lang": "en",
      "text": "way to evaluate L swag now if you go up",
      "offset": 13028.76,
      "duration": 5.4
    },
    {
      "lang": "en",
      "text": "here I'm showing that for GPT 2124m if",
      "offset": 13031.16,
      "duration": 5.239
    },
    {
      "lang": "en",
      "text": "you run this script you're going to see",
      "offset": 13034.16,
      "duration": 5.079
    },
    {
      "lang": "en",
      "text": "that H swag gets",
      "offset": 13036.399,
      "duration": 5.641
    },
    {
      "lang": "en",
      "text": "29.5% um so that's the performance we",
      "offset": 13039.239,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "get here now remember that random Chan",
      "offset": 13042.04,
      "duration": 5.279
    },
    {
      "lang": "en",
      "text": "is 25% so we haven't gone too far and",
      "offset": 13043.8,
      "duration": 7.439
    },
    {
      "lang": "en",
      "text": "gpt2 XL which is the biggest the gpt2",
      "offset": 13047.319,
      "duration": 7.561
    },
    {
      "lang": "en",
      "text": "gets all the way up to 49% roughly so uh",
      "offset": 13051.239,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "these are pretty low values considering",
      "offset": 13054.88,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "that today's state-ofthe-art is more",
      "offset": 13056.439,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "like 95% uh so these are definitely",
      "offset": 13057.84,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "older models by now and then there's one",
      "offset": 13060.04,
      "duration": 4.92
    },
    {
      "lang": "en",
      "text": "more thing called Uther harness which is",
      "offset": 13062.72,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "a very piece of infrastructure for",
      "offset": 13064.96,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "running evals for language models and",
      "offset": 13066.479,
      "duration": 3.96
    },
    {
      "lang": "en",
      "text": "they get slightly different numbers and",
      "offset": 13068.56,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "I'm not 100% sure what the discrepancy",
      "offset": 13070.439,
      "duration": 4.201
    },
    {
      "lang": "en",
      "text": "is for these um it could be that they",
      "offset": 13072.319,
      "duration": 4.92
    },
    {
      "lang": "en",
      "text": "actually do the multiple choice uh",
      "offset": 13074.64,
      "duration": 4.759
    },
    {
      "lang": "en",
      "text": "instead of just the completions and that",
      "offset": 13077.239,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "could be the um uh the discrepancy but",
      "offset": 13079.399,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "I'm not 100% sure about that i' have to",
      "offset": 13082.439,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "take a look but for now our script",
      "offset": 13084.279,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "reports 2955 and so that is the number",
      "offset": 13086.199,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "that we'd like to beat if we are",
      "offset": 13088.92,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "training a GPD 2124m from scratch and",
      "offset": 13090.439,
      "duration": 5.96
    },
    {
      "lang": "en",
      "text": "ourselves um",
      "offset": 13093.64,
      "duration": 5.96
    },
    {
      "lang": "en",
      "text": "so now I'm going to go into actually",
      "offset": 13096.399,
      "duration": 6.521
    },
    {
      "lang": "en",
      "text": "incorporating this eval into our main",
      "offset": 13099.6,
      "duration": 6.719
    },
    {
      "lang": "en",
      "text": "training script and um and basically",
      "offset": 13102.92,
      "duration": 5.08
    },
    {
      "lang": "en",
      "text": "because we want to evaluate it in a",
      "offset": 13106.319,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "periodic manner so that we can track H",
      "offset": 13108,
      "duration": 4.52
    },
    {
      "lang": "en",
      "text": "swag and how it evolves over time and",
      "offset": 13110.319,
      "duration": 6.321
    },
    {
      "lang": "en",
      "text": "see when when and if we cross uh this",
      "offset": 13112.52,
      "duration": 8.719
    },
    {
      "lang": "en",
      "text": "2955 um sort of region so let's now walk",
      "offset": 13116.64,
      "duration": 5.799
    },
    {
      "lang": "en",
      "text": "through some of the changes to train",
      "offset": 13121.239,
      "duration": 3.881
    },
    {
      "lang": "en",
      "text": "gpt2 thatp the first thing I did here is",
      "offset": 13122.439,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "I actually made use compile optional",
      "offset": 13125.12,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "kind of and I disabled it by default and",
      "offset": 13127.64,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "the problem with that is the problem",
      "offset": 13131.12,
      "duration": 4.119
    },
    {
      "lang": "en",
      "text": "with compile is that unfortunately it",
      "offset": 13133,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "does make our code faster but it",
      "offset": 13135.239,
      "duration": 3.24
    },
    {
      "lang": "en",
      "text": "actually breaks the evaluation code and",
      "offset": 13136.84,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "the sampling code it gives me a very",
      "offset": 13138.479,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "gnarly message and I don't know why so",
      "offset": 13140.12,
      "duration": 4.119
    },
    {
      "lang": "en",
      "text": "hopefully by the time you get to the",
      "offset": 13142.16,
      "duration": 4.119
    },
    {
      "lang": "en",
      "text": "codebase when I put it up on GitHub uh",
      "offset": 13144.239,
      "duration": 3.641
    },
    {
      "lang": "en",
      "text": "we're going to fix that by then but for",
      "offset": 13146.279,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "now I'm running without torch compile",
      "offset": 13147.88,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "which is why you see this be a bit",
      "offset": 13149.479,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "slower so we're running without torch",
      "offset": 13151.319,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "compile I also create cre a log",
      "offset": 13153.8,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "directory log where we can place our",
      "offset": 13155.96,
      "duration": 6.04
    },
    {
      "lang": "en",
      "text": "log.txt which will record the train loss",
      "offset": 13158.359,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "validation loss and the H swag",
      "offset": 13162,
      "duration": 3.88
    },
    {
      "lang": "en",
      "text": "accuracies so a very simple text file",
      "offset": 13163.56,
      "duration": 4.759
    },
    {
      "lang": "en",
      "text": "and we're going to uh open for writing",
      "offset": 13165.88,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "so that it sort of starts empty and then",
      "offset": 13168.319,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "we're going to append to",
      "offset": 13170.6,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "it I created a simple variable that um",
      "offset": 13172.56,
      "duration": 6.44
    },
    {
      "lang": "en",
      "text": "helps tell us when we have a last step",
      "offset": 13176.12,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "and then basically periodically inside",
      "offset": 13179,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "this Loop every 250th iteration or at",
      "offset": 13180.92,
      "duration": 5.279
    },
    {
      "lang": "en",
      "text": "the last step we're going to evaluate",
      "offset": 13184.52,
      "duration": 5.799
    },
    {
      "lang": "en",
      "text": "the validation loss and then every 250th",
      "offset": 13186.199,
      "duration": 7.761
    },
    {
      "lang": "en",
      "text": "iteration um we are going to evaluate H",
      "offset": 13190.319,
      "duration": 6.441
    },
    {
      "lang": "en",
      "text": "swag but only if we are not using",
      "offset": 13193.96,
      "duration": 5.8
    },
    {
      "lang": "en",
      "text": "compile because compile breaks it so I'm",
      "offset": 13196.76,
      "duration": 4.599
    },
    {
      "lang": "en",
      "text": "going to come back to this code for",
      "offset": 13199.76,
      "duration": 4.439
    },
    {
      "lang": "en",
      "text": "evaluating H swag in a second and then",
      "offset": 13201.359,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "every 250th iteration as well we're also",
      "offset": 13204.199,
      "duration": 4.12
    },
    {
      "lang": "en",
      "text": "going to sample from the model and so",
      "offset": 13206.479,
      "duration": 3.88
    },
    {
      "lang": "en",
      "text": "you should recognize this as our ancient",
      "offset": 13208.319,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "code from way back when we started the",
      "offset": 13210.359,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "video and we're just sampling from the",
      "offset": 13212.16,
      "duration": 3.319
    },
    {
      "lang": "en",
      "text": "model",
      "offset": 13213.96,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "and then finally here um these are if",
      "offset": 13215.479,
      "duration": 6.121
    },
    {
      "lang": "en",
      "text": "we're not after we validate sample and",
      "offset": 13218.359,
      "duration": 5.241
    },
    {
      "lang": "en",
      "text": "evaluate hell swag we actually do a",
      "offset": 13221.6,
      "duration": 4.679
    },
    {
      "lang": "en",
      "text": "training step here and so this is one",
      "offset": 13223.6,
      "duration": 4.839
    },
    {
      "lang": "en",
      "text": "step of uh training and you should be",
      "offset": 13226.279,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "pretty familiar with all of what this",
      "offset": 13228.439,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "does and at the end here once we get our",
      "offset": 13230.199,
      "duration": 5.561
    },
    {
      "lang": "en",
      "text": "training laws we write it to the file so",
      "offset": 13232.76,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "the only thing that changed that I",
      "offset": 13235.76,
      "duration": 2.84
    },
    {
      "lang": "en",
      "text": "really added is this entire section for",
      "offset": 13237,
      "duration": 4.439
    },
    {
      "lang": "en",
      "text": "H swag eval and the way this works is",
      "offset": 13238.6,
      "duration": 4.52
    },
    {
      "lang": "en",
      "text": "I'm trying to get all the gpus to",
      "offset": 13241.439,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "collaborate on the H swag and so we're",
      "offset": 13243.12,
      "duration": 5.64
    },
    {
      "lang": "en",
      "text": "iterating all the examples and then each",
      "offset": 13245.6,
      "duration": 6.56
    },
    {
      "lang": "en",
      "text": "process only picks the examples that",
      "offset": 13248.76,
      "duration": 5.8
    },
    {
      "lang": "en",
      "text": "assigned to it so we sort of take I and",
      "offset": 13252.16,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "moded by the world size and we have to",
      "offset": 13254.56,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "make it equal to rank otherwise we",
      "offset": 13256.479,
      "duration": 4.681
    },
    {
      "lang": "en",
      "text": "continue and then we render an example",
      "offset": 13258.64,
      "duration": 5.48
    },
    {
      "lang": "en",
      "text": "put it on the GPU we get the low jits",
      "offset": 13261.16,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "then I create a helper function that",
      "offset": 13264.12,
      "duration": 3.88
    },
    {
      "lang": "en",
      "text": "helps us basically predict the option",
      "offset": 13265.72,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "with the lowest loss so this comes here",
      "offset": 13268,
      "duration": 4.92
    },
    {
      "lang": "en",
      "text": "the prediction and then if it's correct",
      "offset": 13270.68,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "we sort of keep count and then if",
      "offset": 13272.92,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "multiple processes were collaborating on",
      "offset": 13275.399,
      "duration": 3.561
    },
    {
      "lang": "en",
      "text": "all this then we need to synchronize",
      "offset": 13277.319,
      "duration": 4.201
    },
    {
      "lang": "en",
      "text": "their stats and so the way one way to do",
      "offset": 13278.96,
      "duration": 4.6
    },
    {
      "lang": "en",
      "text": "that is to package up our statistics",
      "offset": 13281.52,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "here into tensors which we can then call",
      "offset": 13283.56,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "this. alberon and",
      "offset": 13286.72,
      "duration": 6.4
    },
    {
      "lang": "en",
      "text": "sum and then here we sort of um unwrap",
      "offset": 13289.08,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "them from tensors so that we just have",
      "offset": 13293.12,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "ins and then here the master process",
      "offset": 13295.08,
      "duration": 4.92
    },
    {
      "lang": "en",
      "text": "will print and log the hellis swag",
      "offset": 13297.439,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "accuracy",
      "offset": 13300,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "so that's kind of the that's kind of it",
      "offset": 13301.76,
      "duration": 5.519
    },
    {
      "lang": "en",
      "text": "and that's what I'm running right here",
      "offset": 13305.6,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "so you see this optimization here and uh",
      "offset": 13307.279,
      "duration": 5.481
    },
    {
      "lang": "en",
      "text": "we just had a generation and this is",
      "offset": 13310.6,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "Step 10,000 out of about 20,000 right so",
      "offset": 13312.76,
      "duration": 5.32
    },
    {
      "lang": "en",
      "text": "we are halfway done and these are the",
      "offset": 13315.56,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "kinds of samples that uh we are getting",
      "offset": 13318.08,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "at this stage so let's take a look hello",
      "offset": 13319.8,
      "duration": 4.92
    },
    {
      "lang": "en",
      "text": "I'm a language model so I'd like to use",
      "offset": 13322.64,
      "duration": 4.679
    },
    {
      "lang": "en",
      "text": "it to generate some kinds of output",
      "offset": 13324.72,
      "duration": 3.88
    },
    {
      "lang": "en",
      "text": "hello I'm a language model and I'm a",
      "offset": 13327.319,
      "duration": 2.84
    },
    {
      "lang": "en",
      "text": "developer for a lot of",
      "offset": 13328.6,
      "duration": 4.36
    },
    {
      "lang": "en",
      "text": "companies Al language",
      "offset": 13330.159,
      "duration": 7.12
    },
    {
      "lang": "en",
      "text": "model uh let's see if I can find fun",
      "offset": 13332.96,
      "duration": 7.319
    },
    {
      "lang": "en",
      "text": "one",
      "offset": 13337.279,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "um I don't know you can go through this",
      "offset": 13348.12,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "yourself but certainly the predictions",
      "offset": 13350.399,
      "duration": 4.04
    },
    {
      "lang": "en",
      "text": "are getting less and less random uh it",
      "offset": 13352.04,
      "duration": 3.72
    },
    {
      "lang": "en",
      "text": "seems like the model is a little bit",
      "offset": 13354.439,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "more self-aware and using language uh",
      "offset": 13355.76,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "that is a bit",
      "offset": 13358.52,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "more uh specific to it being language",
      "offset": 13359.76,
      "duration": 5.719
    },
    {
      "lang": "en",
      "text": "model hello I'm a language model and",
      "offset": 13363.239,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "like how the language is used to",
      "offset": 13365.479,
      "duration": 3.481
    },
    {
      "lang": "en",
      "text": "communicate I'm a language model and I'm",
      "offset": 13366.84,
      "duration": 5.439
    },
    {
      "lang": "en",
      "text": "going to be speaking English and German",
      "offset": 13368.96,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "okay I don't know so let's just wait",
      "offset": 13372.279,
      "duration": 3.96
    },
    {
      "lang": "en",
      "text": "until this optimization finishes and uh",
      "offset": 13373.84,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "we'll see what kind of samples we get",
      "offset": 13376.239,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "and we're also going to look at the",
      "offset": 13377.92,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "train Val and the hway accuracy and see",
      "offset": 13379.84,
      "duration": 6.479
    },
    {
      "lang": "en",
      "text": "how we're doing with respect to",
      "offset": 13383.6,
      "duration": 6.2
    },
    {
      "lang": "en",
      "text": "gpt2 okay good morning so focusing For a",
      "offset": 13386.319,
      "duration": 4.92
    },
    {
      "lang": "en",
      "text": "Moment On The jupyter Notebook here on",
      "offset": 13389.8,
      "duration": 4.12
    },
    {
      "lang": "en",
      "text": "the right I created a new cell that",
      "offset": 13391.239,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "basically allows us to visualize the the",
      "offset": 13393.92,
      "duration": 5.559
    },
    {
      "lang": "en",
      "text": "train Val and Hela and um the hel score",
      "offset": 13395.239,
      "duration": 5.96
    },
    {
      "lang": "en",
      "text": "and you can step through this it",
      "offset": 13399.479,
      "duration": 3.481
    },
    {
      "lang": "en",
      "text": "basically like parses the log file that",
      "offset": 13401.199,
      "duration": 4.761
    },
    {
      "lang": "en",
      "text": "we are writing and um a lot of this is",
      "offset": 13402.96,
      "duration": 5.239
    },
    {
      "lang": "en",
      "text": "just like boring ma plot lip code but",
      "offset": 13405.96,
      "duration": 4.199
    },
    {
      "lang": "en",
      "text": "basically this is what our optimization",
      "offset": 13408.199,
      "duration": 4.521
    },
    {
      "lang": "en",
      "text": "looks like",
      "offset": 13410.159,
      "duration": 6.481
    },
    {
      "lang": "en",
      "text": "so we ran for",
      "offset": 13412.72,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "19,731 billion tokens which is whoops oh",
      "offset": 13418.199,
      "duration": 6.2
    },
    {
      "lang": "en",
      "text": "my gosh which is one Epoch of the sample",
      "offset": 13421.439,
      "duration": 7.241
    },
    {
      "lang": "en",
      "text": "10B of webd on the left we have the loss",
      "offset": 13424.399,
      "duration": 6.241
    },
    {
      "lang": "en",
      "text": "and the in blue we have the training",
      "offset": 13428.68,
      "duration": 3.88
    },
    {
      "lang": "en",
      "text": "loss in Orange we have the validation",
      "offset": 13430.64,
      "duration": 5.08
    },
    {
      "lang": "en",
      "text": "loss and red as a horizontal line we",
      "offset": 13432.56,
      "duration": 6.2
    },
    {
      "lang": "en",
      "text": "have the opening IG gpt2 124 M model",
      "offset": 13435.72,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "checkpoint when it's just evaluated on",
      "offset": 13438.76,
      "duration": 5.399
    },
    {
      "lang": "en",
      "text": "the validation set of um of this fine",
      "offset": 13440.76,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "web edu uh so you can see that we are",
      "offset": 13444.159,
      "duration": 5.16
    },
    {
      "lang": "en",
      "text": "surpassing this orange is below the red",
      "offset": 13446.92,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "so we're surpassing the validation set",
      "offset": 13449.319,
      "duration": 4.281
    },
    {
      "lang": "en",
      "text": "of this data set and like I mentioned",
      "offset": 13451.399,
      "duration": 3.721
    },
    {
      "lang": "en",
      "text": "the data set distribution is very",
      "offset": 13453.6,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "different from what gpt2 trained on so",
      "offset": 13455.12,
      "duration": 3.96
    },
    {
      "lang": "en",
      "text": "this is not an exactly fair comparison",
      "offset": 13456.96,
      "duration": 5.479
    },
    {
      "lang": "en",
      "text": "but it's a good cross check uh to uh to",
      "offset": 13459.08,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "look at now we would ideally like",
      "offset": 13462.439,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "something that is withheld and",
      "offset": 13465,
      "duration": 5.439
    },
    {
      "lang": "en",
      "text": "comparable and somewhat standard um and",
      "offset": 13467.64,
      "duration": 5.679
    },
    {
      "lang": "en",
      "text": "so for us that is helis swag and so on",
      "offset": 13470.439,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "here we see the H swag progress we made",
      "offset": 13473.319,
      "duration": 5.761
    },
    {
      "lang": "en",
      "text": "from 25% all the way here in red we see",
      "offset": 13475.239,
      "duration": 8.92
    },
    {
      "lang": "en",
      "text": "the open gpt2 124 M model in red so it",
      "offset": 13479.08,
      "duration": 7.96
    },
    {
      "lang": "en",
      "text": "achieves this h bag here and the the",
      "offset": 13484.159,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "gpt3 model 124 M which was trained on",
      "offset": 13487.04,
      "duration": 7.48
    },
    {
      "lang": "en",
      "text": "300 billion tokens achieves green so",
      "offset": 13490.239,
      "duration": 6.2
    },
    {
      "lang": "en",
      "text": "that's over here so you see that we",
      "offset": 13494.52,
      "duration": 5.639
    },
    {
      "lang": "en",
      "text": "basically surpassed the gbt2 24m uh",
      "offset": 13496.439,
      "duration": 7.321
    },
    {
      "lang": "en",
      "text": "model right here uh which is uh really",
      "offset": 13500.159,
      "duration": 6.881
    },
    {
      "lang": "en",
      "text": "nice now interestingly we were able to",
      "offset": 13503.76,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "do so with only training on 10 billion",
      "offset": 13507.04,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "tokens while gpt2 was trained on 100",
      "offset": 13508.76,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "billion tokens so uh for some reason we",
      "offset": 13511.6,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "were able to get away with significantly",
      "offset": 13514.6,
      "duration": 3.879
    },
    {
      "lang": "en",
      "text": "fewer tokens for training there are many",
      "offset": 13516.319,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "possibilities to as to why we could",
      "offset": 13518.479,
      "duration": 6.041
    },
    {
      "lang": "en",
      "text": "match or surpass this accuracy um with",
      "offset": 13521.279,
      "duration": 6.361
    },
    {
      "lang": "en",
      "text": "only 10 million training so number one",
      "offset": 13524.52,
      "duration": 5.679
    },
    {
      "lang": "en",
      "text": "um it could be that opening gbt2 was",
      "offset": 13527.64,
      "duration": 4.519
    },
    {
      "lang": "en",
      "text": "trained on a much wider data",
      "offset": 13530.199,
      "duration": 3.96
    },
    {
      "lang": "en",
      "text": "distribution so in particular fine web",
      "offset": 13532.159,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "edu is all English it's not multilingual",
      "offset": 13534.159,
      "duration": 6.681
    },
    {
      "lang": "en",
      "text": "and there's not that much math and code",
      "offset": 13538.239,
      "duration": 5.601
    },
    {
      "lang": "en",
      "text": "um and so math and code and multilingual",
      "offset": 13540.84,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "could have been stealing capacity from",
      "offset": 13543.84,
      "duration": 6.2
    },
    {
      "lang": "en",
      "text": "the original gpt2 model and um basically",
      "offset": 13545.8,
      "duration": 7
    },
    {
      "lang": "en",
      "text": "that could be partially the reason why",
      "offset": 13550.04,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "uh this is not working out there's many",
      "offset": 13552.8,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "other reasons um so for example the H",
      "offset": 13554.68,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "swag eval is fairly old uh maybe 5 years",
      "offset": 13557.04,
      "duration": 5.399
    },
    {
      "lang": "en",
      "text": "or so it is possible that aspects of H",
      "offset": 13559.319,
      "duration": 5.281
    },
    {
      "lang": "en",
      "text": "swag in some way or even identically",
      "offset": 13562.439,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "have made it into the training Set uh of",
      "offset": 13564.6,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "fine web we don't know for sure but if",
      "offset": 13567.319,
      "duration": 4.12
    },
    {
      "lang": "en",
      "text": "that was the case then we are basically",
      "offset": 13570.12,
      "duration": 2.56
    },
    {
      "lang": "en",
      "text": "looking at the training curve instead of",
      "offset": 13571.439,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "the validation curve so long story short",
      "offset": 13572.68,
      "duration": 3.96
    },
    {
      "lang": "en",
      "text": "this is not a perfect eval and there's",
      "offset": 13575.04,
      "duration": 3.96
    },
    {
      "lang": "en",
      "text": "some caveats here uh but at least we",
      "offset": 13576.64,
      "duration": 4.12
    },
    {
      "lang": "en",
      "text": "have some confidence that that we're not",
      "offset": 13579,
      "duration": 4.6
    },
    {
      "lang": "en",
      "text": "doing something completely wrong and",
      "offset": 13580.76,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "um and uh it's probably the case that",
      "offset": 13583.6,
      "duration": 4.12
    },
    {
      "lang": "en",
      "text": "when people try to create these data",
      "offset": 13586.52,
      "duration": 2.839
    },
    {
      "lang": "en",
      "text": "sets they try to make sure that test",
      "offset": 13587.72,
      "duration": 3.88
    },
    {
      "lang": "en",
      "text": "sets that are very common are not part",
      "offset": 13589.359,
      "duration": 4.361
    },
    {
      "lang": "en",
      "text": "of the training set for example uh when",
      "offset": 13591.6,
      "duration": 4.04
    },
    {
      "lang": "en",
      "text": "hugging face created the fine web BDU",
      "offset": 13593.72,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "they use H swag as an eval so I would",
      "offset": 13595.64,
      "duration": 4.12
    },
    {
      "lang": "en",
      "text": "hope that they make sure that they D",
      "offset": 13597.72,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "duplicate and that there's no hella swag",
      "offset": 13599.76,
      "duration": 5.24
    },
    {
      "lang": "en",
      "text": "in the training set but we can't be sure",
      "offset": 13601.56,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "uh the other thing I wanted to address",
      "offset": 13605,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "briefly is look at this loss curve this",
      "offset": 13606.279,
      "duration": 4.12
    },
    {
      "lang": "en",
      "text": "looks really this looks really wrong",
      "offset": 13608.439,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "here I don't actually know 100% what",
      "offset": 13610.399,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "this is and I suspect it's because the",
      "offset": 13612.439,
      "duration": 5.88
    },
    {
      "lang": "en",
      "text": "uh 10 billion sample of fine web edu was",
      "offset": 13615.279,
      "duration": 6.681
    },
    {
      "lang": "en",
      "text": "not properly shuffled um and there's",
      "offset": 13618.319,
      "duration": 6.321
    },
    {
      "lang": "en",
      "text": "some issue here uh with the data that I",
      "offset": 13621.96,
      "duration": 4.12
    },
    {
      "lang": "en",
      "text": "don't fully understand yet and there's",
      "offset": 13624.64,
      "duration": 4.04
    },
    {
      "lang": "en",
      "text": "some weird periodicity to it um and",
      "offset": 13626.08,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "because we are in a very lazy way sort",
      "offset": 13628.68,
      "duration": 3.559
    },
    {
      "lang": "en",
      "text": "of serializing all the tokens and just",
      "offset": 13630.64,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "iterating all them from scratch without",
      "offset": 13632.239,
      "duration": 3.96
    },
    {
      "lang": "en",
      "text": "doing any permutation or any random",
      "offset": 13634,
      "duration": 4.439
    },
    {
      "lang": "en",
      "text": "sampling ourselves I think we're",
      "offset": 13636.199,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "inheriting some of the ordering that",
      "offset": 13638.439,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "they have in the data set so uh this is",
      "offset": 13641.199,
      "duration": 5.601
    },
    {
      "lang": "en",
      "text": "not ideal but hopefully by the time you",
      "offset": 13644.359,
      "duration": 4.281
    },
    {
      "lang": "en",
      "text": "get to this repo uh some of these things",
      "offset": 13646.8,
      "duration": 5.32
    },
    {
      "lang": "en",
      "text": "by the way will hopefully be fixed and I",
      "offset": 13648.64,
      "duration": 7.16
    },
    {
      "lang": "en",
      "text": "will release this build n GPT repo and",
      "offset": 13652.12,
      "duration": 5.239
    },
    {
      "lang": "en",
      "text": "right now it looks a little ugly and",
      "offset": 13655.8,
      "duration": 3.559
    },
    {
      "lang": "en",
      "text": "preliminary uh so hopefully by the time",
      "offset": 13657.359,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "you get here it's nicer but down here",
      "offset": 13659.359,
      "duration": 5.08
    },
    {
      "lang": "en",
      "text": "I'm going to show aada and I'm going to",
      "offset": 13661.76,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "talk about about some of the things that",
      "offset": 13664.439,
      "duration": 3.641
    },
    {
      "lang": "en",
      "text": "happened after the video and I expect",
      "offset": 13665.84,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "that we will have fixed uh the small",
      "offset": 13668.08,
      "duration": 4.359
    },
    {
      "lang": "en",
      "text": "issue uh but for now basically this",
      "offset": 13670.239,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "shows that uh our training is not uh",
      "offset": 13672.439,
      "duration": 5.561
    },
    {
      "lang": "en",
      "text": "completely wrong and it shows that uh",
      "offset": 13675.239,
      "duration": 4.92
    },
    {
      "lang": "en",
      "text": "we're able to surpass the accuracy with",
      "offset": 13678,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "only 10x the token budget um and",
      "offset": 13680.159,
      "duration": 5.401
    },
    {
      "lang": "en",
      "text": "possibly it could be also that the data",
      "offset": 13683.92,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "set may have improved so uh the original",
      "offset": 13685.56,
      "duration": 5.679
    },
    {
      "lang": "en",
      "text": "uh gpt2 data set was web text it's",
      "offset": 13688.96,
      "duration": 3.88
    },
    {
      "lang": "en",
      "text": "possible that not a lot of care and",
      "offset": 13691.239,
      "duration": 3.281
    },
    {
      "lang": "en",
      "text": "attention went into the data set this",
      "offset": 13692.84,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "was very early in llms whereas now",
      "offset": 13694.52,
      "duration": 4.36
    },
    {
      "lang": "en",
      "text": "there's a lot more scrutiny on good",
      "offset": 13697,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "practices around uh D duplication",
      "offset": 13698.88,
      "duration": 4.599
    },
    {
      "lang": "en",
      "text": "filtering uh quality filtering and so on",
      "offset": 13700.92,
      "duration": 3.72
    },
    {
      "lang": "en",
      "text": "and it's possible that the data that",
      "offset": 13703.479,
      "duration": 2.441
    },
    {
      "lang": "en",
      "text": "we're training on is just of higher",
      "offset": 13704.64,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "quality per token and that could be",
      "offset": 13705.92,
      "duration": 4.439
    },
    {
      "lang": "en",
      "text": "giving us a boost as well so a number of",
      "offset": 13707.92,
      "duration": 4.279
    },
    {
      "lang": "en",
      "text": "cave has to think about but for now uh",
      "offset": 13710.359,
      "duration": 5.681
    },
    {
      "lang": "en",
      "text": "we're pretty happy with this um and yeah",
      "offset": 13712.199,
      "duration": 5.24
    },
    {
      "lang": "en",
      "text": "now the next thing I was interested in",
      "offset": 13716.04,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "is as you see it's a morning now so",
      "offset": 13717.439,
      "duration": 3.96
    },
    {
      "lang": "en",
      "text": "there was an overnight and I wanted to",
      "offset": 13719.479,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "basically see how far I could push the",
      "offset": 13721.399,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "result so uh to do an overnight run I",
      "offset": 13723.239,
      "duration": 5.401
    },
    {
      "lang": "en",
      "text": "basically did instead of one Epoch which",
      "offset": 13726.439,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "took roughly two hours I just did a",
      "offset": 13728.64,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "times four so that that would take eight",
      "offset": 13730.6,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "hours while I was sleeping and so we did",
      "offset": 13732.399,
      "duration": 4.521
    },
    {
      "lang": "en",
      "text": "four Epoch or roughly 40 billion uh",
      "offset": 13734.359,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "tokens of training and I was trying to",
      "offset": 13736.92,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "see how far we could get um and so this",
      "offset": 13739.399,
      "duration": 3.96
    },
    {
      "lang": "en",
      "text": "was the only change and I reran the",
      "offset": 13741.92,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "script and when I point uh and read the",
      "offset": 13743.359,
      "duration": 5.401
    },
    {
      "lang": "en",
      "text": "log file at uh at the 40b uh this is",
      "offset": 13745.76,
      "duration": 4.76
    },
    {
      "lang": "en",
      "text": "what the curve look",
      "offset": 13748.76,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "like okay so to narrate this number one",
      "offset": 13750.52,
      "duration": 4.759
    },
    {
      "lang": "en",
      "text": "we are seeing this issue here here with",
      "offset": 13753.64,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "the periodicity through the different",
      "offset": 13755.279,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "Epoch and something really weird with",
      "offset": 13757.319,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "the fine web edu data set and that is to",
      "offset": 13759.12,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "be determined uh but otherwise we are",
      "offset": 13762.12,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "seeing that the H swag actually went up",
      "offset": 13765.12,
      "duration": 6.159
    },
    {
      "lang": "en",
      "text": "by a lot and we almost we almost made it",
      "offset": 13767.8,
      "duration": 7.8
    },
    {
      "lang": "en",
      "text": "uh to the GPT 324m accuracy uh up here",
      "offset": 13771.279,
      "duration": 6.601
    },
    {
      "lang": "en",
      "text": "uh but not quite so uh it's too bad that",
      "offset": 13775.6,
      "duration": 5.96
    },
    {
      "lang": "en",
      "text": "I didn't sleep slightly longer um and uh",
      "offset": 13777.88,
      "duration": 6.479
    },
    {
      "lang": "en",
      "text": "I think if this was an uh five Epoch run",
      "offset": 13781.56,
      "duration": 5.48
    },
    {
      "lang": "en",
      "text": "we may have gotten here now one thing to",
      "offset": 13784.359,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "point out is that if you're doing multi",
      "offset": 13787.04,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "Epoch runs uh we're not actually being",
      "offset": 13789.08,
      "duration": 3.96
    },
    {
      "lang": "en",
      "text": "very careful in our data loader and",
      "offset": 13791.439,
      "duration": 5.281
    },
    {
      "lang": "en",
      "text": "we're not um I this data loader goes",
      "offset": 13793.04,
      "duration": 6.04
    },
    {
      "lang": "en",
      "text": "through the data in exactly the same",
      "offset": 13796.72,
      "duration": 5.16
    },
    {
      "lang": "en",
      "text": "format and exactly the same order and",
      "offset": 13799.08,
      "duration": 4.6
    },
    {
      "lang": "en",
      "text": "this is kind of suboptimal and you would",
      "offset": 13801.88,
      "duration": 3.479
    },
    {
      "lang": "en",
      "text": "want to look into extensions where you",
      "offset": 13803.68,
      "duration": 4.36
    },
    {
      "lang": "en",
      "text": "actually permute the data uh randomly",
      "offset": 13805.359,
      "duration": 4.92
    },
    {
      "lang": "en",
      "text": "you permute the documents around in",
      "offset": 13808.04,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "Every Single Shard on every single new",
      "offset": 13810.279,
      "duration": 5.88
    },
    {
      "lang": "en",
      "text": "Epoch um and po even permute the",
      "offset": 13812.12,
      "duration": 6.079
    },
    {
      "lang": "en",
      "text": "shards and that would go a long way into",
      "offset": 13816.159,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "decreasing the pricity and it's also",
      "offset": 13818.199,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "better for the optimization so that",
      "offset": 13820.159,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "you're not seeing things ident in the",
      "offset": 13822.04,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "identical format and you're introducing",
      "offset": 13823.76,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "some of the some uh Randomness in how",
      "offset": 13825.88,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "the documents follow each other because",
      "offset": 13828,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "you have to remember that in every",
      "offset": 13829.88,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "single row these documents follow each",
      "offset": 13831.04,
      "duration": 3.48
    },
    {
      "lang": "en",
      "text": "other and then there's the end of text",
      "offset": 13833.239,
      "duration": 3.401
    },
    {
      "lang": "en",
      "text": "token and then the next document so the",
      "offset": 13834.52,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "documents are currently glued together",
      "offset": 13836.64,
      "duration": 4.92
    },
    {
      "lang": "en",
      "text": "in the exact same identical manner but",
      "offset": 13839.08,
      "duration": 4.279
    },
    {
      "lang": "en",
      "text": "we actually want to break break up the",
      "offset": 13841.56,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "documents and shuffle them around",
      "offset": 13843.359,
      "duration": 3.08
    },
    {
      "lang": "en",
      "text": "because the order of the documents",
      "offset": 13845.239,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "shouldn't matter and they shouldn't um",
      "offset": 13846.439,
      "duration": 3.96
    },
    {
      "lang": "en",
      "text": "basically we want to break up that",
      "offset": 13849.159,
      "duration": 2.681
    },
    {
      "lang": "en",
      "text": "dependence because it's a kind of a",
      "offset": 13850.399,
      "duration": 4.281
    },
    {
      "lang": "en",
      "text": "spous correlation and so our data lad is",
      "offset": 13851.84,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "not currently doing that and that's one",
      "offset": 13854.68,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "Improvement uh you could think of",
      "offset": 13856.159,
      "duration": 5.16
    },
    {
      "lang": "en",
      "text": "making um the other thing to point out",
      "offset": 13858.359,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "is we're almost matching gpt3 accuracy",
      "offset": 13861.319,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "with only 40 billion tokens gpt3 trained",
      "offset": 13863.239,
      "duration": 5.24
    },
    {
      "lang": "en",
      "text": "on 300 billion tokens so again we're",
      "offset": 13866.199,
      "duration": 5.321
    },
    {
      "lang": "en",
      "text": "seeing about a 10x um Improvement here",
      "offset": 13868.479,
      "duration": 5.88
    },
    {
      "lang": "en",
      "text": "with respect to learning efficiency uh",
      "offset": 13871.52,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "the other thing I wanted to and I don't",
      "offset": 13874.359,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "actually know exactly what to attribute",
      "offset": 13876.8,
      "duration": 2.76
    },
    {
      "lang": "en",
      "text": "this to other than some of the things",
      "offset": 13878.199,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "that I already mentioned previously for",
      "offset": 13879.56,
      "duration": 3.96
    },
    {
      "lang": "en",
      "text": "the previous run uh the other thing I",
      "offset": 13881.319,
      "duration": 5.361
    },
    {
      "lang": "en",
      "text": "wanted to briefly mention is uh the max",
      "offset": 13883.52,
      "duration": 5.879
    },
    {
      "lang": "en",
      "text": "LR here I saw some people already play",
      "offset": 13886.68,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "with this a little bit in a previous",
      "offset": 13889.399,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "related repository um and it turns out",
      "offset": 13891,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "that you can actually almost like three",
      "offset": 13893.96,
      "duration": 3.279
    },
    {
      "lang": "en",
      "text": "xas so it's possible that the maximum",
      "offset": 13895.159,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "learning rate can be a lot higher and",
      "offset": 13897.239,
      "duration": 3.321
    },
    {
      "lang": "en",
      "text": "for some reason the gpt3 hyper",
      "offset": 13899.08,
      "duration": 3.319
    },
    {
      "lang": "en",
      "text": "parameters that we are inheriting are",
      "offset": 13900.56,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "actually extremely conservative and you",
      "offset": 13902.399,
      "duration": 2.84
    },
    {
      "lang": "en",
      "text": "can actually get away with a Higher",
      "offset": 13904.08,
      "duration": 3.48
    },
    {
      "lang": "en",
      "text": "Learning rate and it would train faster",
      "offset": 13905.239,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "so a lot of these hyper parameters um",
      "offset": 13907.56,
      "duration": 4.839
    },
    {
      "lang": "en",
      "text": "are quite tunable and feel free to play",
      "offset": 13910.239,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "with them and they're probably not set",
      "offset": 13912.399,
      "duration": 6.681
    },
    {
      "lang": "en",
      "text": "precisely correctly and um it's possible",
      "offset": 13914.8,
      "duration": 6.72
    },
    {
      "lang": "en",
      "text": "that you can get away with doing this",
      "offset": 13919.08,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "basically and if you wanted to exactly",
      "offset": 13921.52,
      "duration": 6.28
    },
    {
      "lang": "en",
      "text": "be faithful to gpt3 you would also want",
      "offset": 13923.8,
      "duration": 6.28
    },
    {
      "lang": "en",
      "text": "to make the following difference you'd",
      "offset": 13927.8,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "want to come here and the sequence",
      "offset": 13930.08,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "length of gpt3 is 2x it's 20 48 instead",
      "offset": 13931.8,
      "duration": 5.599
    },
    {
      "lang": "en",
      "text": "of 1,24 so you would come here change",
      "offset": 13935.12,
      "duration": 5.159
    },
    {
      "lang": "en",
      "text": "this to 248 for T and then if you want",
      "offset": 13937.399,
      "duration": 5.361
    },
    {
      "lang": "en",
      "text": "the exact same number of tokens uh half",
      "offset": 13940.279,
      "duration": 5.361
    },
    {
      "lang": "en",
      "text": "a million per iteration or per step you",
      "offset": 13942.76,
      "duration": 5.479
    },
    {
      "lang": "en",
      "text": "want to then decrease this to 32 so they",
      "offset": 13945.64,
      "duration": 5.679
    },
    {
      "lang": "en",
      "text": "still multiply to half a mil so that",
      "offset": 13948.239,
      "duration": 5.521
    },
    {
      "lang": "en",
      "text": "would give your model sequence length",
      "offset": 13951.319,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "equal to that of gpt3 and in that case",
      "offset": 13953.76,
      "duration": 3.719
    },
    {
      "lang": "en",
      "text": "basically the",
      "offset": 13956.12,
      "duration": 4.76
    },
    {
      "lang": "en",
      "text": "um the models would be roughly identical",
      "offset": 13957.479,
      "duration": 5.241
    },
    {
      "lang": "en",
      "text": "as far as I'm as far as I'm aware",
      "offset": 13960.88,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "because again gpt2 and gpt3 are very",
      "offset": 13962.72,
      "duration": 4.36
    },
    {
      "lang": "en",
      "text": "very similar models now we can also look",
      "offset": 13964.64,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "at some of the samples here from the",
      "offset": 13967.08,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "model that was trained overnight so this",
      "offset": 13968.479,
      "duration": 4.281
    },
    {
      "lang": "en",
      "text": "is",
      "offset": 13971.56,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "the optimization and you see that here",
      "offset": 13972.76,
      "duration": 5.08
    },
    {
      "lang": "en",
      "text": "we stepped all the way to",
      "offset": 13975.479,
      "duration": 6.88
    },
    {
      "lang": "en",
      "text": "76290 also or so and these are the hos",
      "offset": 13977.84,
      "duration": 8.359
    },
    {
      "lang": "en",
      "text": "mag we achieved was 33.2 4 and these are",
      "offset": 13982.359,
      "duration": 6.361
    },
    {
      "lang": "en",
      "text": "some of the samples from the model and",
      "offset": 13986.199,
      "duration": 4.04
    },
    {
      "lang": "en",
      "text": "you can see that if you read through",
      "offset": 13988.72,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "this and pause the video briefly you can",
      "offset": 13990.239,
      "duration": 3.96
    },
    {
      "lang": "en",
      "text": "see that they are a lot more coherent uh",
      "offset": 13991.84,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "so",
      "offset": 13994.199,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "um and they're actually addressing the",
      "offset": 13995.6,
      "duration": 5.679
    },
    {
      "lang": "en",
      "text": "fact that it's a language model almost",
      "offset": 13997.239,
      "duration": 7.441
    },
    {
      "lang": "en",
      "text": "so uh hello I'm a language model and I",
      "offset": 14001.279,
      "duration": 5.761
    },
    {
      "lang": "en",
      "text": "try to be as accurate as",
      "offset": 14004.68,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "possible um I'm a language model not a",
      "offset": 14007.04,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "programming",
      "offset": 14009.72,
      "duration": 4.92
    },
    {
      "lang": "en",
      "text": "language I know how to communicate uh I",
      "offset": 14011.68,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "use",
      "offset": 14014.64,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "Python",
      "offset": 14015.92,
      "duration": 4.279
    },
    {
      "lang": "en",
      "text": "um I don't know if you pause this and",
      "offset": 14017.92,
      "duration": 3.8
    },
    {
      "lang": "en",
      "text": "look at it and then compare it to the",
      "offset": 14020.199,
      "duration": 3.321
    },
    {
      "lang": "en",
      "text": "one to the model that was only trained",
      "offset": 14021.72,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "for 10 billion uh you will see that",
      "offset": 14023.52,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "these are a lot more coherent and you",
      "offset": 14025.479,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "can play with this uh",
      "offset": 14027.04,
      "duration": 3.399
    },
    {
      "lang": "en",
      "text": "yourself one more thing I added to The",
      "offset": 14028.479,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "Code by the way is this chunk of code",
      "offset": 14030.439,
      "duration": 4.281
    },
    {
      "lang": "en",
      "text": "here so basically right after we",
      "offset": 14032.64,
      "duration": 4.12
    },
    {
      "lang": "en",
      "text": "evaluate the validation loss if we are",
      "offset": 14034.72,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "the master process in addition to",
      "offset": 14036.76,
      "duration": 4.599
    },
    {
      "lang": "en",
      "text": "logging the validation loss every 5,000",
      "offset": 14038.88,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "steps we're also going to save the",
      "offset": 14041.359,
      "duration": 3.161
    },
    {
      "lang": "en",
      "text": "checkpoint which is really just the",
      "offset": 14042.64,
      "duration": 4.44
    },
    {
      "lang": "en",
      "text": "state dictionary of the model and so",
      "offset": 14044.52,
      "duration": 4.679
    },
    {
      "lang": "en",
      "text": "checkpointing is nice just because uh",
      "offset": 14047.08,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "you can save the model and later you can",
      "offset": 14049.199,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "uh use it in some way if you wanted to",
      "offset": 14051.399,
      "duration": 3.96
    },
    {
      "lang": "en",
      "text": "resume the optimiz ation then in",
      "offset": 14053.6,
      "duration": 4.28
    },
    {
      "lang": "en",
      "text": "addition to saving the model we have to",
      "offset": 14055.359,
      "duration": 5.08
    },
    {
      "lang": "en",
      "text": "also save the optimizer State dict",
      "offset": 14057.88,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "because remember that the optimizer has",
      "offset": 14060.439,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "a few additional buffers because of adom",
      "offset": 14061.88,
      "duration": 6.519
    },
    {
      "lang": "en",
      "text": "so it's got the m and V and uh you need",
      "offset": 14064.52,
      "duration": 5.839
    },
    {
      "lang": "en",
      "text": "to also resume the optimizer properly",
      "offset": 14068.399,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "you have to be careful with your RNG",
      "offset": 14070.359,
      "duration": 3.321
    },
    {
      "lang": "en",
      "text": "seeds uh random number generators and so",
      "offset": 14071.6,
      "duration": 3.96
    },
    {
      "lang": "en",
      "text": "on so if you wanted to exactly be able",
      "offset": 14073.68,
      "duration": 4.12
    },
    {
      "lang": "en",
      "text": "to resume optimization you have to think",
      "offset": 14075.56,
      "duration": 4.679
    },
    {
      "lang": "en",
      "text": "through the state of the of the training",
      "offset": 14077.8,
      "duration": 4.04
    },
    {
      "lang": "en",
      "text": "process but if you just want to save the",
      "offset": 14080.239,
      "duration": 3.641
    },
    {
      "lang": "en",
      "text": "model this is how you would do it and",
      "offset": 14081.84,
      "duration": 3.8
    },
    {
      "lang": "en",
      "text": "one one nice reason why you might want",
      "offset": 14083.88,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "to do this is because you may want to",
      "offset": 14085.64,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "evaluate the model a lot more carefully",
      "offset": 14087.64,
      "duration": 5.16
    },
    {
      "lang": "en",
      "text": "so here we are only kind of like winging",
      "offset": 14090.12,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "the hell swag eval but you may want to",
      "offset": 14092.8,
      "duration": 5.08
    },
    {
      "lang": "en",
      "text": "use something um nicer like for example",
      "offset": 14094.84,
      "duration": 7.12
    },
    {
      "lang": "en",
      "text": "the Luther uh Luther evaluation hardness",
      "offset": 14097.88,
      "duration": 8.16
    },
    {
      "lang": "en",
      "text": "evaluation hardness hardness um so this",
      "offset": 14101.96,
      "duration": 6.72
    },
    {
      "lang": "en",
      "text": "is a way to also evaluate language",
      "offset": 14106.04,
      "duration": 7.64
    },
    {
      "lang": "en",
      "text": "models and um so it's possible that um",
      "offset": 14108.68,
      "duration": 6.519
    },
    {
      "lang": "en",
      "text": "you may want to use basically different",
      "offset": 14113.68,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "infrastructure to more thoroughly",
      "offset": 14115.199,
      "duration": 4.841
    },
    {
      "lang": "en",
      "text": "evaluate the models on different um",
      "offset": 14117.359,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "evaluations and compare it to the",
      "offset": 14120.04,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "opening gbt2 model on many other um",
      "offset": 14121.439,
      "duration": 5.401
    },
    {
      "lang": "en",
      "text": "tasks like for example that involve math",
      "offset": 14125.04,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "code or different languages and so on so",
      "offset": 14126.84,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "this is a nice functionality to have as",
      "offset": 14129.199,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "well",
      "offset": 14130.68,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "um and then the other thing I wanted to",
      "offset": 14132.239,
      "duration": 4.601
    },
    {
      "lang": "en",
      "text": "mention is that everything we've built",
      "offset": 14134.84,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "here this is only the pre-training step",
      "offset": 14136.84,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "so um the GPT here is a it dreams",
      "offset": 14139.479,
      "duration": 5.361
    },
    {
      "lang": "en",
      "text": "documents it just predicts the next to",
      "offset": 14142.84,
      "duration": 3.519
    },
    {
      "lang": "en",
      "text": "you can't talk to it like you can talk",
      "offset": 14144.84,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "to chat GPT uh chat GPT if you wanted to",
      "offset": 14146.359,
      "duration": 5.241
    },
    {
      "lang": "en",
      "text": "talk to the model we have to fine-tune",
      "offset": 14149.64,
      "duration": 4.44
    },
    {
      "lang": "en",
      "text": "it into the chat format and it's not",
      "offset": 14151.6,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "actually like that complicated if you're",
      "offset": 14154.08,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "looking at supervised fine-tuning or sft",
      "offset": 14155.439,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "really what that means is we're just",
      "offset": 14158.159,
      "duration": 3.401
    },
    {
      "lang": "en",
      "text": "swapping out a data set into a data set",
      "offset": 14159.359,
      "duration": 3.8
    },
    {
      "lang": "en",
      "text": "that is a lot more conversational and",
      "offset": 14161.56,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "there's a user assistant user assistant",
      "offset": 14163.159,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "kind of structure and we just fine-tune",
      "offset": 14164.92,
      "duration": 4.68
    },
    {
      "lang": "en",
      "text": "on it and then we um we basically fill",
      "offset": 14166.84,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "in the user tokens and we sample the",
      "offset": 14169.6,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "assistant tokens it's not a lot more",
      "offset": 14171.84,
      "duration": 3.96
    },
    {
      "lang": "en",
      "text": "deeper than that uh but basically we",
      "offset": 14173.76,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "swap out the data set and continue",
      "offset": 14175.8,
      "duration": 3.88
    },
    {
      "lang": "en",
      "text": "training uh but for now we're going to",
      "offset": 14177.439,
      "duration": 4.441
    },
    {
      "lang": "en",
      "text": "stop at uh pre-training one more thing",
      "offset": 14179.68,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "that I wanted to briefly show you is",
      "offset": 14181.88,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "that of course what we've built up today",
      "offset": 14183.359,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "was building towards nanog GPT which is",
      "offset": 14185.88,
      "duration": 4.92
    },
    {
      "lang": "en",
      "text": "this repository from earlier uh but also",
      "offset": 14187.92,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "there's actually another nanog GPT",
      "offset": 14190.8,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "implementation and it's hiding in a more",
      "offset": 14192.399,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "recent project that I've been working on",
      "offset": 14194.64,
      "duration": 6.96
    },
    {
      "lang": "en",
      "text": "called llm Doc and lm. C is a pure Cuda",
      "offset": 14196.399,
      "duration": 8.481
    },
    {
      "lang": "en",
      "text": "implementation of gpt2 or gpt3 training",
      "offset": 14201.6,
      "duration": 6.04
    },
    {
      "lang": "en",
      "text": "and it just directly uses uh Cuda and is",
      "offset": 14204.88,
      "duration": 6.319
    },
    {
      "lang": "en",
      "text": "written as Cuda now the nanog gbt here",
      "offset": 14207.64,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "acts as reference code in pytorch to the",
      "offset": 14211.199,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "C implementation so we're trying to",
      "offset": 14213.72,
      "duration": 3.559
    },
    {
      "lang": "en",
      "text": "exactly match up the two but we're",
      "offset": 14215.359,
      "duration": 4.441
    },
    {
      "lang": "en",
      "text": "hoping that the C Cuda is faster and of",
      "offset": 14217.279,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "course currently that seems to be the",
      "offset": 14219.8,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "case um because it is a direct optimized",
      "offset": 14221.199,
      "duration": 5.721
    },
    {
      "lang": "en",
      "text": "implementation so train gpt2 Pi in LL",
      "offset": 14224.04,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "M.C is basically the nanog GPT and when",
      "offset": 14226.92,
      "duration": 5.96
    },
    {
      "lang": "en",
      "text": "you scroll through this file you'll find",
      "offset": 14230.04,
      "duration": 6.199
    },
    {
      "lang": "en",
      "text": "a lot of things that very much look like",
      "offset": 14232.88,
      "duration": 6.2
    },
    {
      "lang": "en",
      "text": "um things that we've built up in this",
      "offset": 14236.239,
      "duration": 5.16
    },
    {
      "lang": "en",
      "text": "lecture and then when you look at train",
      "offset": 14239.08,
      "duration": 6.079
    },
    {
      "lang": "en",
      "text": "gpt2 docu uh this is the C Cuda",
      "offset": 14241.399,
      "duration": 6.161
    },
    {
      "lang": "en",
      "text": "implementation so there's a lot of MPI",
      "offset": 14245.159,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "nickel GPU Cuda",
      "offset": 14247.56,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "cc++ and you have to be familiar with",
      "offset": 14250.199,
      "duration": 7.2
    },
    {
      "lang": "en",
      "text": "that but uh um when this is built up we",
      "offset": 14252.52,
      "duration": 7
    },
    {
      "lang": "en",
      "text": "can actually run the two side by side",
      "offset": 14257.399,
      "duration": 3.401
    },
    {
      "lang": "en",
      "text": "and they're going to produce the exact",
      "offset": 14259.52,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "same results but lm. C actually runs",
      "offset": 14260.8,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "faster so let's see that so on the left",
      "offset": 14263.52,
      "duration": 6.28
    },
    {
      "lang": "en",
      "text": "I have pytorch a nanog GPT looking thing",
      "offset": 14265.92,
      "duration": 6.399
    },
    {
      "lang": "en",
      "text": "on the right I have the llmc call and",
      "offset": 14269.8,
      "duration": 4.439
    },
    {
      "lang": "en",
      "text": "here I'm going to launch the",
      "offset": 14272.319,
      "duration": 3.641
    },
    {
      "lang": "en",
      "text": "two both of these are going to be",
      "offset": 14274.239,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "running on a single GPU and here I'm",
      "offset": 14275.96,
      "duration": 4.359
    },
    {
      "lang": "en",
      "text": "putting the lm. C on GPU 1 and this one",
      "offset": 14277.84,
      "duration": 7.399
    },
    {
      "lang": "en",
      "text": "will grab uh gpu0 by default and",
      "offset": 14280.319,
      "duration": 8.361
    },
    {
      "lang": "en",
      "text": "then we can see here that lm. c",
      "offset": 14285.239,
      "duration": 6.481
    },
    {
      "lang": "en",
      "text": "compiled and then allocate space and",
      "offset": 14288.68,
      "duration": 3.96
    },
    {
      "lang": "en",
      "text": "it's",
      "offset": 14291.72,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "stepping so",
      "offset": 14292.64,
      "duration": 5.08
    },
    {
      "lang": "en",
      "text": "basically uh meanwhile P torch is still",
      "offset": 14295.399,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "compiling because torch compile is a bit",
      "offset": 14297.72,
      "duration": 6.679
    },
    {
      "lang": "en",
      "text": "slower here than the lm. C nbcc Cuda",
      "offset": 14299.88,
      "duration": 6.64
    },
    {
      "lang": "en",
      "text": "compile and so this program has already",
      "offset": 14304.399,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "started running and uh we're still",
      "offset": 14306.52,
      "duration": 4.28
    },
    {
      "lang": "en",
      "text": "waiting here for torch compile now of",
      "offset": 14308.64,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "course uh this is a very specific",
      "offset": 14310.8,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "implementation to gpt2 and 3 a pytorch",
      "offset": 14313.04,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "is a very general neural network",
      "offset": 14315.6,
      "duration": 2.799
    },
    {
      "lang": "en",
      "text": "framework so they're not exactly",
      "offset": 14317.12,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "comparable but if you're only interested",
      "offset": 14318.399,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "in training gpt2 and 3 lm. C is very",
      "offset": 14319.92,
      "duration": 6.6
    },
    {
      "lang": "en",
      "text": "fast it takes less space it's faster to",
      "offset": 14323.12,
      "duration": 6.84
    },
    {
      "lang": "en",
      "text": "start and it's faster per",
      "offset": 14326.52,
      "duration": 6.759
    },
    {
      "lang": "en",
      "text": "step and so P started to Stepping here",
      "offset": 14329.96,
      "duration": 4.84
    },
    {
      "lang": "en",
      "text": "and as you can see we're running at",
      "offset": 14333.279,
      "duration": 4.601
    },
    {
      "lang": "en",
      "text": "about 223,000 tokens per second here and",
      "offset": 14334.8,
      "duration": 8.2
    },
    {
      "lang": "en",
      "text": "about 185,000 tokens per second here um",
      "offset": 14337.88,
      "duration": 7.279
    },
    {
      "lang": "en",
      "text": "so quite a bit slower but I don't have",
      "offset": 14343,
      "duration": 5.239
    },
    {
      "lang": "en",
      "text": "full confidence that I exactly squeezed",
      "offset": 14345.159,
      "duration": 4.28
    },
    {
      "lang": "en",
      "text": "out all the juice from the pytorch",
      "offset": 14348.239,
      "duration": 3.401
    },
    {
      "lang": "en",
      "text": "implementation but the important thing",
      "offset": 14349.439,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "here is notice that if I Aline up the",
      "offset": 14351.64,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "steps you will see that the losses and",
      "offset": 14354.239,
      "duration": 4.281
    },
    {
      "lang": "en",
      "text": "Norms that are printed between these two",
      "offset": 14356.439,
      "duration": 2.92
    },
    {
      "lang": "en",
      "text": "are",
      "offset": 14358.52,
      "duration": 3.08
    },
    {
      "lang": "en",
      "text": "identical so on the left we have the pie",
      "offset": 14359.359,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "torch and on the right this C",
      "offset": 14361.6,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "implementation and they're the same",
      "offset": 14364,
      "duration": 4.76
    },
    {
      "lang": "en",
      "text": "except this one runs faster uh so that's",
      "offset": 14365.92,
      "duration": 4.68
    },
    {
      "lang": "en",
      "text": "kind of I wanted to show you also",
      "offset": 14368.76,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "briefly lm. C and this is a parallel",
      "offset": 14370.6,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "implementation and it's also something",
      "offset": 14373.399,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "that you may want to uh play with or",
      "offset": 14375.08,
      "duration": 4.6
    },
    {
      "lang": "en",
      "text": "look at and um it's kind of interesting",
      "offset": 14377,
      "duration": 3.96
    },
    {
      "lang": "en",
      "text": "okay so at this point I should probably",
      "offset": 14379.68,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "start wrapping up the video because I",
      "offset": 14380.96,
      "duration": 3.399
    },
    {
      "lang": "en",
      "text": "think it's getting way longer than I",
      "offset": 14382.72,
      "duration": 4.04
    },
    {
      "lang": "en",
      "text": "anticipated uh but we did Cover a lot of",
      "offset": 14384.359,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "ground and we built everything from",
      "offset": 14386.76,
      "duration": 4.2
    },
    {
      "lang": "en",
      "text": "scratch so as a brief summary we were",
      "offset": 14388.04,
      "duration": 6.96
    },
    {
      "lang": "en",
      "text": "looking at the gpt2 and GPT 3",
      "offset": 14390.96,
      "duration": 6.479
    },
    {
      "lang": "en",
      "text": "papers we were looking at how you set up",
      "offset": 14395,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "these training runs uh and all the",
      "offset": 14397.439,
      "duration": 3.8
    },
    {
      "lang": "en",
      "text": "considerations involved we wrote",
      "offset": 14399.479,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "everything from scratch and then we saw",
      "offset": 14401.239,
      "duration": 3.561
    },
    {
      "lang": "en",
      "text": "that over the duration of either a",
      "offset": 14403.399,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "2-hour training run or an overnight run",
      "offset": 14404.8,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "we can actually match the 124 million",
      "offset": 14407.399,
      "duration": 5.281
    },
    {
      "lang": "en",
      "text": "parameter checkpoints of gbt2 and gpt3",
      "offset": 14409.84,
      "duration": 4.84
    },
    {
      "lang": "en",
      "text": "uh to a very large extent",
      "offset": 14412.68,
      "duration": 3.96
    },
    {
      "lang": "en",
      "text": "um in principle the code that we wrote",
      "offset": 14414.68,
      "duration": 3.4
    },
    {
      "lang": "en",
      "text": "would be able to train even bigger",
      "offset": 14416.64,
      "duration": 2.92
    },
    {
      "lang": "en",
      "text": "models if you have the patients or the",
      "offset": 14418.08,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "Computing resources uh and so you could",
      "offset": 14419.56,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "potentially think about training some of",
      "offset": 14421.84,
      "duration": 4.359
    },
    {
      "lang": "en",
      "text": "the bigger checkpoints as well um there",
      "offset": 14423.479,
      "duration": 5.161
    },
    {
      "lang": "en",
      "text": "are a few remaining issues to address",
      "offset": 14426.199,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "what's happening with the loss here",
      "offset": 14428.64,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "which I suspect has to do with the fine",
      "offset": 14430.279,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "web edu data sampling uh why can't we",
      "offset": 14431.84,
      "duration": 5.08
    },
    {
      "lang": "en",
      "text": "turn on Torch compile uh it currently",
      "offset": 14434.92,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "breaks generation and H swag what's up",
      "offset": 14436.92,
      "duration": 4.519
    },
    {
      "lang": "en",
      "text": "with that in the data loader we should",
      "offset": 14439.479,
      "duration": 3.72
    },
    {
      "lang": "en",
      "text": "probably be permuting our data when we",
      "offset": 14441.439,
      "duration": 4.121
    },
    {
      "lang": "en",
      "text": "reach boundaries so there's a few more",
      "offset": 14443.199,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "issues like that and I expect to be",
      "offset": 14445.56,
      "duration": 3.879
    },
    {
      "lang": "en",
      "text": "documenting some of those over time in",
      "offset": 14447.199,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "the uh build n GPT repository here which",
      "offset": 14449.439,
      "duration": 5.601
    },
    {
      "lang": "en",
      "text": "I'm going to be releasing with this",
      "offset": 14453.279,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "video if you have any questions or like",
      "offset": 14455.04,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "to talk about anything that we covered",
      "offset": 14457.68,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "please go to discussions tab uh so we",
      "offset": 14459.68,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "can talk here uh or please go to issues",
      "offset": 14462.08,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "or pull request pull requests um",
      "offset": 14464.68,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "depending on what you'd like to",
      "offset": 14467.12,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "contribute or also have a look at the uh",
      "offset": 14468.68,
      "duration": 5.519
    },
    {
      "lang": "en",
      "text": "Zero to Hero Discord and uh I'm going to",
      "offset": 14471.52,
      "duration": 5.639
    },
    {
      "lang": "en",
      "text": "be hanging out here on N GPT",
      "offset": 14474.199,
      "duration": 5.96
    },
    {
      "lang": "en",
      "text": "um otherwise for now I'm pretty happy",
      "offset": 14477.159,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "about where we got um and I hope you",
      "offset": 14480.159,
      "duration": 4.841
    },
    {
      "lang": "en",
      "text": "enjoyed the video and I will see you",
      "offset": 14483.239,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "later",
      "offset": 14485,
      "duration": 2.8
    }
  ],
  "cleanText": null,
  "dumpedAt": "2025-07-21T18:43:25.414Z"
}