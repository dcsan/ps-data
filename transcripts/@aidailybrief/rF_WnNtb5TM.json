{
  "episodeId": "rF_WnNtb5TM",
  "channelSlug": "@aidailybrief",
  "title": "Everything We Know About GPT-5 So Far",
  "publishedAt": "2025-07-09T23:23:00.000Z",
  "rawLines": [
    {
      "lang": "en",
      "text": "Welcome back to the AI daily brief.",
      "offset": 0.16,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "Today we are talking about one of",
      "offset": 2.24,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "potentially the next big things to hit",
      "offset": 4.24,
      "duration": 6.56
    },
    {
      "lang": "en",
      "text": "AI, which is of course GPT5.",
      "offset": 6.879,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "There is a growing sense that this model",
      "offset": 10.8,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "is right around the corner and that it",
      "offset": 12.719,
      "duration": 5.041
    },
    {
      "lang": "en",
      "text": "may be very significant. So today we're",
      "offset": 15.04,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "going to talk about everything that we",
      "offset": 17.76,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "know about it, how it might impact the",
      "offset": 19.279,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "AI space, and yes, a little tea leaf",
      "offset": 21.439,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "speculation around when we might",
      "offset": 23.84,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "actually get the thing. Now, GPT5 has",
      "offset": 25.199,
      "duration": 5.041
    },
    {
      "lang": "en",
      "text": "been coming soon for almost a year at",
      "offset": 28,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "this point, but the model that finally",
      "offset": 30.24,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "gets released will be very different",
      "offset": 32.079,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "from where it started. In the middle of",
      "offset": 33.52,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "last year, rumors started circulating",
      "offset": 35.52,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "about the next big models from OpenAI",
      "offset": 37.2,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "codeamed Orion. Rumors in the fall were",
      "offset": 39.28,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "that the model was going to come as",
      "offset": 41.76,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "early as December. But it wasn't too",
      "offset": 43.28,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "long after that in around November that",
      "offset": 45.36,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "we started hearing about industry-wide",
      "offset": 47.36,
      "duration": 3.519
    },
    {
      "lang": "en",
      "text": "issues around training with a building",
      "offset": 48.96,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "narrative that pre-training had hit a",
      "offset": 50.879,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "wall. What we ultimately got in the",
      "offset": 52.399,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "place of a new flagship model in the",
      "offset": 54.719,
      "duration": 5.041
    },
    {
      "lang": "en",
      "text": "series of GBT3, 3.5, and 4 was instead",
      "offset": 56.559,
      "duration": 6.081
    },
    {
      "lang": "en",
      "text": "OpenAI's first reasoning model. First 01",
      "offset": 59.76,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "and then 03. Now, as we've previously",
      "offset": 62.64,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "discussed, the launch of reasoning",
      "offset": 65.28,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "models, it turns out, were a huge",
      "offset": 66.96,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "inflection point in the adoption of AI.",
      "offset": 68.64,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "A whole array of new use cases came",
      "offset": 71.04,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "online. Enterprise adoption started",
      "offset": 72.72,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "going up significantly, and the new era",
      "offset": 74.72,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "of Agentics became a viable possibility",
      "offset": 76.64,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "rather than just something for the",
      "offset": 78.799,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "future. Now, in and around the reasoning",
      "offset": 80,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "models, we did get a GPT 4.5. It clearly",
      "offset": 82.24,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "wasn't a big enough leap to warrant the",
      "offset": 85.84,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "title. And while the model was a niche",
      "offset": 87.36,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "hit among people who wanted a better LLM",
      "offset": 89.6,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "for writing, it failed to capture much",
      "offset": 91.759,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "attention or usage in the broader",
      "offset": 93.759,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "public. In fact, the model is actually",
      "offset": 95.2,
      "duration": 5.279
    },
    {
      "lang": "en",
      "text": "being sunset in the API next week. Swix",
      "offset": 97.6,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "from latent space had this interesting",
      "offset": 100.479,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "observation saying I think the quote",
      "offset": 101.92,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "unquote failure of GPT4.5 relative to 03",
      "offset": 103.759,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "and 04 mini is actually fantastic",
      "offset": 106.72,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "validation of the bet on the reasoning",
      "offset": 108.799,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "paradigm 10 to 100x larger model",
      "offset": 110.56,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "consistently lost out to smaller",
      "offset": 113.04,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "reasoning model for an important open",
      "offset": 114.479,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "domain task and still hold aside what's",
      "offset": 116,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "great about the reasoning models the",
      "offset": 118.56,
      "duration": 3.519
    },
    {
      "lang": "en",
      "text": "point is that there hasn't been a new",
      "offset": 120.479,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "open AI flagship in the series of GPTs",
      "offset": 122.079,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "for about a year now and it's been over",
      "offset": 125.36,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "2 years since they felt confident enough",
      "offset": 127.439,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "to herald led a new generation of models",
      "offset": 129.119,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "with a full numerical upgrade from GBT4.",
      "offset": 130.959,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "At the same time, the industry has",
      "offset": 133.92,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "changed quite a bit since the last time",
      "offset": 135.68,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "we got excited about GBT 5. Last fall,",
      "offset": 137.599,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "as we just discussed, the narrative was",
      "offset": 140.48,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "that scaling hit a wall, and people were",
      "offset": 141.84,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "really wondering if there was any more",
      "offset": 143.52,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "juice left to squeeze when it came to",
      "offset": 145.12,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "pre-training or if we were just in the",
      "offset": 146.64,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "era of new methodologies like test time",
      "offset": 148.4,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "compute, the reasoning era, and agentics",
      "offset": 150.48,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "with tool use being the vectors for",
      "offset": 152.48,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "improvement going forward. At this",
      "offset": 153.92,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "point, now 6 to9 months later, there is",
      "offset": 155.92,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "a pervasive sense of optimism that AI is",
      "offset": 158.08,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "nowhere near its zenith and that there",
      "offset": 160.08,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "are still plenty of avenues to pursue in",
      "offset": 162.239,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "order to improve the technology. Logan",
      "offset": 163.76,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "Kilpatrick, the head of product at",
      "offset": 166,
      "duration": 3.519
    },
    {
      "lang": "en",
      "text": "Google AI studio, recently posted, &quot;The",
      "offset": 167.28,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "next 6 months of AI are likely to be the",
      "offset": 169.519,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "most wild we have seen so far.",
      "offset": 171.2,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "Everything keeps scaling up. More",
      "offset": 173.2,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "hardware, more model progress, more",
      "offset": 175.04,
      "duration": 3.279
    },
    {
      "lang": "en",
      "text": "product knowledge, more AI momentum,",
      "offset": 176.72,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "more product market fit.&quot; Now, Logan",
      "offset": 178.319,
      "duration": 3.521
    },
    {
      "lang": "en",
      "text": "went to Pays to say that this wasn't",
      "offset": 180.4,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "inspired by any specific thing. He said",
      "offset": 181.84,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "it was just inspired by me feeling the",
      "offset": 184.08,
      "duration": 2.799
    },
    {
      "lang": "en",
      "text": "progress and then looking out over the",
      "offset": 185.68,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "next 6 months and being reminded it's",
      "offset": 186.879,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "going to continue. So bringing it back",
      "offset": 188.48,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "to GPT5. What part of that excitement",
      "offset": 190.48,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "can we attribute to this new model? The",
      "offset": 193.04,
      "duration": 5.279
    },
    {
      "lang": "en",
      "text": "main expectation around GPT5 is that it",
      "offset": 195.68,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "will represent a unification of OpenAI's",
      "offset": 198.319,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "technology. In a recent podcast,",
      "offset": 200.239,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "OpenAI's head of developer experience",
      "offset": 202.4,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "Roma Huitt said, &quot;We're truly excited",
      "offset": 204,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "not to just make a net new great",
      "offset": 206.08,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "frontier model. We're also going to",
      "offset": 207.68,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "unify our two series. The breakthrough",
      "offset": 209.36,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "of reasoning in the O series and the",
      "offset": 211.519,
      "duration": 2.961
    },
    {
      "lang": "en",
      "text": "breakthroughs in multimodality in the",
      "offset": 213.04,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "GPT series will be unified and that will",
      "offset": 214.48,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "be GBT5. One of the core promises is",
      "offset": 216.64,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "that GBT5 will do away with model",
      "offset": 219.28,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "switching. In a Reddit AMA from May,",
      "offset": 221.12,
      "duration": 5.119
    },
    {
      "lang": "en",
      "text": "OpenAI VP Jerry Tour wrote, &quot;GBT5 is our",
      "offset": 223.28,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "next foundation model that is meant to",
      "offset": 226.239,
      "duration": 2.881
    },
    {
      "lang": "en",
      "text": "just make everything our models can",
      "offset": 227.76,
      "duration": 3.119
    },
    {
      "lang": "en",
      "text": "currently do better and with less model",
      "offset": 229.12,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "switching.&quot; Interestingly, he also",
      "offset": 230.879,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "referred to their operator agent as a",
      "offset": 232.799,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "quote product surface, suggesting",
      "offset": 234.56,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "perhaps that GPT5 will feature tighter",
      "offset": 236.48,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "integration with Agentic Tools. Back in",
      "offset": 238.4,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "February, Sam Alman had said something",
      "offset": 240.959,
      "duration": 3.521
    },
    {
      "lang": "en",
      "text": "similar about the model switching idea.",
      "offset": 242.48,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "In a post on X, he wrote, &quot;We want AI to",
      "offset": 244.48,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "just work for you. We realize how",
      "offset": 247.12,
      "duration": 3.039
    },
    {
      "lang": "en",
      "text": "complicated our model and product",
      "offset": 248.799,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "offerings have gotten. We hate the model",
      "offset": 250.159,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "picker as much as you do and want to",
      "offset": 252.159,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "return to Magic Unified Intelligence.&quot;",
      "offset": 253.92,
      "duration": 3.999
    },
    {
      "lang": "en",
      "text": "Now, this post was a couple weeks before",
      "offset": 256.239,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "GPT5, and he said, &quot;We will next ship",
      "offset": 257.919,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "GPT4.5, the model we called Orion",
      "offset": 260.32,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "internally, as our last non-chain of",
      "offset": 262.56,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "thought model. After that, a top goal",
      "offset": 264.4,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "for us is to unify O series models and",
      "offset": 266.32,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "GPT series models by creating systems",
      "offset": 268.24,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "that can use all of our tools, know when",
      "offset": 270.56,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "to think for a long time or not, and",
      "offset": 272.4,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "generally be useful for a wide range of",
      "offset": 274,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "tasks. Now, getting a little bit more up",
      "offset": 275.68,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "to date, developer Bueno compiled some",
      "offset": 277.68,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "other nuggets of information gleaned",
      "offset": 280.08,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "from recent interviews and leaks. He",
      "offset": 281.52,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "expects a 256k context window, which",
      "offset": 283.44,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "would put GBT5 roughly in line with most",
      "offset": 286.32,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "competitors, but not as large as",
      "offset": 288.479,
      "duration": 3.521
    },
    {
      "lang": "en",
      "text": "Google's million token context window.",
      "offset": 289.919,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "Multimodality likely means we'll see",
      "offset": 292,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "native video, image, and audio inputs",
      "offset": 293.759,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "and perhaps even outputs. Another",
      "offset": 295.6,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "current rumor is that OpenAI will adopt",
      "offset": 297.6,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "the mixture of experts architecture",
      "offset": 299.36,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "brought to prominence recently by",
      "offset": 300.8,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "several Chinese labs. This architecture",
      "offset": 302.08,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "means that only part of the model is",
      "offset": 304.08,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "engaged at a time, allowing for a higher",
      "offset": 305.44,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "number of total parameters while keeping",
      "offset": 307.68,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "inference costs down. There are",
      "offset": 309.199,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "estimates that inference costs could be",
      "offset": 311.12,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "60% lower per token than GBT40.",
      "offset": 312.56,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "Memory is another factor that could see",
      "offset": 315.52,
      "duration": 2.64
    },
    {
      "lang": "en",
      "text": "improvement which would lead to more",
      "offset": 317.039,
      "duration": 3.521
    },
    {
      "lang": "en",
      "text": "performant agent operation as well. not",
      "offset": 318.16,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "commented. For builders, this likely",
      "offset": 320.56,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "means you have to rethink prompt design",
      "offset": 322.32,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "for giant context. Expect richer tool",
      "offset": 323.759,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "calling that mixes text with timebased",
      "offset": 325.919,
      "duration": 3.521
    },
    {
      "lang": "en",
      "text": "media and budget for lower latency,",
      "offset": 327.52,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "cheaper API calls despite a larger",
      "offset": 329.44,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "model. GBT5 looks less like a parameter",
      "offset": 331.28,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "bump and more like a systems integration",
      "offset": 334.479,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "milestone, folding multiple specialized",
      "offset": 336.4,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "capabilities into one cohesive model.",
      "offset": 338.639,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "And frankly, even without the rumors,",
      "offset": 341.12,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "that seems like a reasonable assumption.",
      "offset": 342.639,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "Since its release, OpenAI has been",
      "offset": 344.56,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "tacking features on to GPT4, including",
      "offset": 346.32,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "things like memory and updated image",
      "offset": 348.639,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "generation. GPT5 represents their first",
      "offset": 350.24,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "opportunity to bake these features in",
      "offset": 352.8,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "natively, allowing the model to be",
      "offset": 354.24,
      "duration": 2.799
    },
    {
      "lang": "en",
      "text": "trained from the ground up on how to",
      "offset": 355.84,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "make best use of the tools it has access",
      "offset": 357.039,
      "duration": 5.041
    },
    {
      "lang": "en",
      "text": "to. Vasserax commented, &quot;GT5 might be",
      "offset": 358.72,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "the first model that feels like true",
      "offset": 362.08,
      "duration": 4.959
    },
    {
      "lang": "en",
      "text": "AGI. If OpenAI integrates full O4 or O4",
      "offset": 363.44,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "Pro reasoning plus agentic tool use",
      "offset": 367.039,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "within the chain of thought, operator",
      "offset": 368.8,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "codecs and deep research, we're talking",
      "offset": 370.24,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "about a model that can think, plan, act,",
      "offset": 372,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "and adapt like never before. AGI vibes",
      "offset": 373.68,
      "duration": 4.959
    },
    {
      "lang": "en",
      "text": "incoming. Now, one of the reasons we",
      "offset": 376.72,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "think that GBT 5 is nearing release is",
      "offset": 378.639,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "that users are starting to report seeing",
      "offset": 380.8,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "a ton of AB testing of something new on",
      "offset": 382.56,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "the platform. Specifically, it appears",
      "offset": 384.479,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "that OpenAI is testing how reasoning",
      "offset": 386.4,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "traces are presented. One X user, for",
      "offset": 388.16,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "example, suggested that a recent",
      "offset": 390.479,
      "duration": 2.961
    },
    {
      "lang": "en",
      "text": "interaction they had looked like hybrid",
      "offset": 391.759,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "reasoning. In response to a prompt using",
      "offset": 393.44,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "the 40 model, which again is not a",
      "offset": 395.84,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "natively reasoning model, chatbt said,",
      "offset": 398,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "&quot;Just a moment. I want to give this one",
      "offset": 400.4,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "the extra thought it deserves. That",
      "offset": 402,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "certainly would imply that OpenAI has",
      "offset": 403.759,
      "duration": 2.961
    },
    {
      "lang": "en",
      "text": "figured out a way to differentiate",
      "offset": 405.52,
      "duration": 2.399
    },
    {
      "lang": "en",
      "text": "between the prompts that should use",
      "offset": 406.72,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "reasoning and those that shouldn't.&quot;",
      "offset": 407.919,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "Another interesting feature on that post",
      "offset": 409.52,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "is a button labeled answer. Now, that",
      "offset": 411.039,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "presumably cuts the reasoning short and",
      "offset": 413.28,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "forces the model to just spit out an",
      "offset": 415.12,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "answer based on how much thinking it's",
      "offset": 416.72,
      "duration": 3.039
    },
    {
      "lang": "en",
      "text": "done at that point. One of the problems",
      "offset": 418,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "with hybrid reasoning is that if the",
      "offset": 419.759,
      "duration": 2.961
    },
    {
      "lang": "en",
      "text": "model starts thinking and goes down some",
      "offset": 421.199,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "rabbit hole, there hasn't historically",
      "offset": 422.72,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "been a good way to make it stop. So",
      "offset": 424.56,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "again, while it's just a guess, it seems",
      "offset": 426.72,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "like answer now might be a UX change to",
      "offset": 428.319,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "address that issue. So if the idea is",
      "offset": 430.479,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "that GPT5 will natively bring together",
      "offset": 433.759,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "all of these features that have been",
      "offset": 435.919,
      "duration": 3.041
    },
    {
      "lang": "en",
      "text": "bolted on, perhaps we also need a",
      "offset": 437.039,
      "duration": 3.121
    },
    {
      "lang": "en",
      "text": "slightly different way of thinking about",
      "offset": 438.96,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "what an LLM actually is. Karina Nuen, a",
      "offset": 440.16,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "researcher and product staffer at",
      "offset": 443.52,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "OpenAI, recently posted super",
      "offset": 444.72,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "intelligent operating system. Believe it",
      "offset": 446.72,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "or not, she is not talking about my",
      "offset": 448.96,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "startup. Instead, cryptically, it sounds",
      "offset": 450.56,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "like it's sort of what's being described",
      "offset": 452.56,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "in the rumors of GPT5.",
      "offset": 454.08,
      "duration": 3.519
    },
    {
      "lang": "en",
      "text": "Rather than treating it like an",
      "offset": 456.319,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "individual model you interact with, GPT5",
      "offset": 457.599,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "kind of sounds like an AI operating",
      "offset": 460.08,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "system. TJ Rididgeway writes, &quot;So GPT5",
      "offset": 461.68,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "is the AGI framework. This is why",
      "offset": 464.96,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "they're all pivoting to super",
      "offset": 466.88,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "intelligence discussion, the one ring to",
      "offset": 468,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "rule them all.&quot; In response to the idea",
      "offset": 470,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "that super intelligence won't just be a",
      "offset": 472.88,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "scaled LLM, he added, &quot;Yes, I pretty",
      "offset": 474.639,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "much agree. What I'm saying here is this",
      "offset": 476.72,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "roadmap they put out is the framework",
      "offset": 478.72,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "through which AGI will be achieved, not",
      "offset": 480.24,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "AGI itself. I believe innovations in",
      "offset": 482,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "long-term memory are also a crucial",
      "offset": 484.4,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "aspect that is just now being explored.&quot;",
      "offset": 485.919,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "It is worth noting that back in January,",
      "offset": 488.24,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "Sam Alman did mention, quote, &quot;We're now",
      "offset": 490.319,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "confident we know how to build AGI as we",
      "offset": 492.639,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "have traditionally understood it.&quot;",
      "offset": 494.72,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "Still, a practical question after all of",
      "offset": 496.72,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "this is whether any of it will be enough",
      "offset": 498.479,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "to amaze heavy AI users. Specifically,",
      "offset": 500.319,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "as much as we're summing up all of these",
      "offset": 503.36,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "different rumors here, none of them",
      "offset": 504.96,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "really suggest, at least not yet, a",
      "offset": 506.72,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "massive change in capability or anything",
      "offset": 508.56,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "particularly new. Instead, these rumors",
      "offset": 510.319,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "all focus on bringing everything",
      "offset": 512.64,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "together and making the user experience",
      "offset": 513.839,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "more seamless. Chubby, for example,",
      "offset": 515.44,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "wrote, &quot;For hardcore users, GPT5 will be",
      "offset": 517.68,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "a bit of a disappointment if the rumors",
      "offset": 520.08,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "are to be believed.&quot; Rumor has it that",
      "offset": 521.519,
      "duration": 3.521
    },
    {
      "lang": "en",
      "text": "Sam Alman is not particularly impressed",
      "offset": 523.44,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "with the performance and improvements",
      "offset": 525.04,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "compared to older models such as GPT40",
      "offset": 526.24,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "and 03. GBT5 is more of an iterative",
      "offset": 528.24,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "improvement that certainly shows",
      "offset": 531.04,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "significant leaps in benchmarks. But",
      "offset": 532.48,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "compared to benchmarks such as reasoning",
      "offset": 534.32,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "at the end of last year 01 or deep",
      "offset": 535.839,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "research at the beginning of this year,",
      "offset": 537.6,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "GPT5 is more of the same just slightly",
      "offset": 539.279,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "better. In this respect, GPT5 is",
      "offset": 541.36,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "probably not the qualitative leap that",
      "offset": 543.44,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "hardcore users had hoped for. And yet,",
      "offset": 544.88,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "Chubby wrote, &quot;Maybe the hardcore users",
      "offset": 547.279,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "are not the point.&quot; They continue for",
      "offset": 549.12,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "the vast majority. However, GPT5 will be",
      "offset": 551.279,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "a quantum leap. When I talk to friends,",
      "offset": 553.44,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "I almost always hear the same thing.",
      "offset": 555.68,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "ChatGpt is great. They say it will help",
      "offset": 557.279,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "them get more out of their university",
      "offset": 559.44,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "studies, answer all their questions, and",
      "offset": 560.88,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "even provide excellent advice on medical",
      "offset": 562.64,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "matters. When asked which model they",
      "offset": 564.48,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "would use for this, the answer is always",
      "offset": 566.72,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "the same. GPT40. Of course, they either",
      "offset": 568.08,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "don't know anything about 03 or due to",
      "offset": 570.72,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "the complicated nomenclature, they",
      "offset": 572.48,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "consider 03 to be the inferior model",
      "offset": 574.24,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "because it's older, because three",
      "offset": 576,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "becomes before four. Some people think",
      "offset": 577.2,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "that simply. However, GPT5 will be an",
      "offset": 579.2,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "all-in-one model. Depending on the",
      "offset": 581.519,
      "duration": 3.041
    },
    {
      "lang": "en",
      "text": "request, the appropriate amount of",
      "offset": 583.44,
      "duration": 2.56
    },
    {
      "lang": "en",
      "text": "inference will be applied and reasoning",
      "offset": 584.56,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "will be carried out. This means that all",
      "offset": 586,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "those who previously used only GPT40",
      "offset": 587.76,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "will suddenly receive much better",
      "offset": 589.92,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "answers with GPT5 than before because",
      "offset": 591.12,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "they did not use or were not aware of",
      "offset": 593.44,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "the full strength and range of ChatGpt's",
      "offset": 594.959,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "model capacity. Now, this I think is a",
      "offset": 596.8,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "super important point. The vast majority",
      "offset": 599.6,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "of ChatGpt users, even at this stage,",
      "offset": 601.6,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "are not subscribers. They haven't used",
      "offset": 603.6,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "deep research and they don't understand",
      "offset": 605.76,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "what a reasoning model is or at least",
      "offset": 607.2,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "how it's different. In other words,",
      "offset": 608.72,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "removing ChatJBT's model selector isn't",
      "offset": 610.399,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "just a minor UX improvement. Instead, it",
      "offset": 612.56,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "fundamentally will broaden the average",
      "offset": 615.12,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "user's experience by making all of the",
      "offset": 616.8,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "myriad features available to them",
      "offset": 618.64,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "without them having to know about what",
      "offset": 620.16,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "those features actually do. Think back",
      "offset": 621.44,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "to the DeepSeek moment at the beginning",
      "offset": 623.68,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "of the year. The viral breakthrough was",
      "offset": 625.04,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "not that there was better reasoning. The",
      "offset": 627.04,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "full version of 01 had already been out",
      "offset": 628.959,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "for 2 months and was clearly the better",
      "offset": 630.56,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "model. The innovation was simply putting",
      "offset": 632.399,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "reasoning right in front of users who",
      "offset": 634.48,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "had never experienced it before with the",
      "offset": 635.839,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "full reasoning traces on display. The",
      "offset": 637.44,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "average comment about DeepSeek wasn't",
      "offset": 639.839,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "just that it was powerful. It's that it",
      "offset": 641.44,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "was really cute or cool because it",
      "offset": 643.279,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "talked to itself before responding.",
      "offset": 644.56,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "OpenAI then has the opportunity to give",
      "offset": 646.48,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "the average inexperienced user that type",
      "offset": 648.48,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "of moment of delight with GPT5. Even if",
      "offset": 650.32,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "every single feature already exists and",
      "offset": 653.04,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "the performance bump is minor, improved",
      "offset": 654.72,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "accessibility is a huge deal for the",
      "offset": 656.72,
      "duration": 5.359
    },
    {
      "lang": "en",
      "text": "average GPT5 user. And yet, I do think",
      "offset": 658.64,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "that unfortunately for OpenAI, that",
      "offset": 662.079,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "still might not be enough, or at least",
      "offset": 664.32,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "not for long. It's very clear that in",
      "offset": 666.64,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "the wake of Mark Zuckerberg's poaching",
      "offset": 668.88,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "spree, the stakes for OpenAI are",
      "offset": 670.48,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "building. Zuck super intelligence team",
      "offset": 672.48,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "is now largely in place. And then it's",
      "offset": 674.48,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "worth then trying to speculate on what",
      "offset": 676.56,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "their big play is. In the leadup to the",
      "offset": 678.079,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "release of Llama 4 during the spring,",
      "offset": 680.32,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "Zuckerberg set off on a media tour",
      "offset": 681.839,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "discussing his AI plans at length. There",
      "offset": 683.519,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "were really two big themes. The first",
      "offset": 685.68,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "was making an automated advertising",
      "offset": 687.839,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "platform and the second was introducing",
      "offset": 689.44,
      "duration": 3.519
    },
    {
      "lang": "en",
      "text": "AI friends to meta social media",
      "offset": 691.44,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "platforms. Hold aside what you think of",
      "offset": 692.959,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "those ideas. It is pretty undeniable",
      "offset": 695.04,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "that they're both fairly modest",
      "offset": 697.12,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "ambitions. In other words, neither is",
      "offset": 698.56,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "something you would necessarily want to",
      "offset": 700.56,
      "duration": 2.48
    },
    {
      "lang": "en",
      "text": "spend hundreds of millions of dollars in",
      "offset": 701.76,
      "duration": 3.519
    },
    {
      "lang": "en",
      "text": "payroll to achieve. And so whether",
      "offset": 703.04,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "Zuckerberg is aiming for a straightshot",
      "offset": 705.279,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "super intelligence play or iterative",
      "offset": 707.12,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "model releases that are actually keeping",
      "offset": 709.44,
      "duration": 2.48
    },
    {
      "lang": "en",
      "text": "up with and pushing the",
      "offset": 710.959,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "state-of-the-art, he clearly has",
      "offset": 711.92,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "something much larger than just better",
      "offset": 713.839,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "advertising automation in mind. One",
      "offset": 715.279,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "observation that many have made is that",
      "offset": 717.839,
      "duration": 3.281
    },
    {
      "lang": "en",
      "text": "Zuckerberg has put together a team of",
      "offset": 719.68,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "experts with a wide range of skills. He",
      "offset": 721.12,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "poached the reasoning team lead from",
      "offset": 723.36,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "OpenAI, a multimodal expert from Google,",
      "offset": 724.72,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "an edge model developer from Apple, and",
      "offset": 727.36,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "the list goes on. In other words, a full",
      "offset": 729.12,
      "duration": 3.519
    },
    {
      "lang": "en",
      "text": "stack team capable of recreating",
      "offset": 731.12,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "everyone OpenAI or anyone else has on",
      "offset": 732.639,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "offer from scratch. Point being,",
      "offset": 734.48,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "whatever they're building, it's not AI",
      "offset": 736.72,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "friends. Alman says he's not concerned.",
      "offset": 738.48,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "In an interview at the Sun Valley",
      "offset": 741.44,
      "duration": 2.48
    },
    {
      "lang": "en",
      "text": "Conference, he was asked how he's",
      "offset": 742.8,
      "duration": 2.32
    },
    {
      "lang": "en",
      "text": "feeling about the talent war and",
      "offset": 743.92,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "responded, &quot;Fine, good. We have",
      "offset": 745.12,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "obviously an incredibly talented team",
      "offset": 747.04,
      "duration": 2.56
    },
    {
      "lang": "en",
      "text": "and I think they really love what",
      "offset": 748.56,
      "duration": 2.64
    },
    {
      "lang": "en",
      "text": "they're doing. Obviously, some people",
      "offset": 749.6,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "will go to different places. There's a",
      "offset": 751.2,
      "duration": 2.879
    },
    {
      "lang": "en",
      "text": "lot of excitement, I guess you could",
      "offset": 753.04,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "say, in the industry, but no, I think we",
      "offset": 754.079,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "feel fine. At the same time, it is",
      "offset": 755.92,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "pretty clearly undeniable that whether",
      "offset": 758.32,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "Sam is a part of this or not, OpenAI",
      "offset": 760.079,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "leadership in general is starting to",
      "offset": 762.24,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "feel the heat. We've seen changes in",
      "offset": 763.839,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "compensation packages, memos that",
      "offset": 766.079,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "suggest the feeling was of having their",
      "offset": 768.16,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "house broken into. And so for this",
      "offset": 769.68,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "reason, I think that the GPT5 moment,",
      "offset": 771.6,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "whether OpenAI wants it to be or not, is",
      "offset": 773.279,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "going to be seen as hugely significant",
      "offset": 775.6,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "and reflective of the state of play when",
      "offset": 777.92,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "it comes to the broader industry. Back",
      "offset": 779.92,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "in April, even before this aggressive",
      "offset": 782.24,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "talent war, Alman tweeted, &quot;Change of",
      "offset": 783.839,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "plans. We are going to release 03 and 04",
      "offset": 786.079,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "mini after all probably in a couple of",
      "offset": 787.839,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "weeks and then do GBT5 in a few months.",
      "offset": 789.44,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "There are a bunch of reasons for this,",
      "offset": 792.24,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "but the most exciting one is that we are",
      "offset": 793.6,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "going to be able to make GBT much better",
      "offset": 795.12,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "than we originally thought. We also",
      "offset": 796.8,
      "duration": 3.039
    },
    {
      "lang": "en",
      "text": "found it harder than we thought it was",
      "offset": 798.56,
      "duration": 2.48
    },
    {
      "lang": "en",
      "text": "going to be to smoothly integrate",
      "offset": 799.839,
      "duration": 2.481
    },
    {
      "lang": "en",
      "text": "everything, and we want to make sure we",
      "offset": 801.04,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "have enough capacity to support what we",
      "offset": 802.32,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "expect to be unprecedented demand. So,",
      "offset": 803.92,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "just how soon is this thing coming?",
      "offset": 806.959,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "Ultimately, that among all of this is",
      "offset": 808.56,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "the biggest rumor. We have gotten a",
      "offset": 810.079,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "bunch of hints recently from OpenAI",
      "offset": 812.16,
      "duration": 3.119
    },
    {
      "lang": "en",
      "text": "insiders that something big is coming in",
      "offset": 813.68,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "the next week or two, but I also think",
      "offset": 815.279,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "that OpenAI knows the stakes of GPT5. I",
      "offset": 817.12,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "don't think that they're feeling so much",
      "offset": 820,
      "duration": 2.24
    },
    {
      "lang": "en",
      "text": "pressure that they're going to release",
      "offset": 821.12,
      "duration": 2.48
    },
    {
      "lang": "en",
      "text": "something that is anything less than",
      "offset": 822.24,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "extremely impressive. Still, maybe we",
      "offset": 823.6,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "have an exciting Midsummer tree coming",
      "offset": 826.079,
      "duration": 3.521
    },
    {
      "lang": "en",
      "text": "up. Certainly, the chorus of rumors is",
      "offset": 827.68,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "getting louder and as they get more",
      "offset": 829.6,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "credible, I will be sure to let you know",
      "offset": 831.36,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "it here. For now though, that is going",
      "offset": 833.44,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "to do it for today's AI daily brief.",
      "offset": 835.279,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "Until next time, peace.",
      "offset": 837.12,
      "duration": 3.36
    }
  ],
  "cleanText": "Welcome back to The AI Daily Brief.\nToday we are talking about one of potentially the next big things to hit AI, which is of course GPT-5.\nThere is a growing sense that this model is right around the corner and that it may be very significant.\nSo today we're going to talk about everything that we know about it, how it might impact the AI space, and yes, a little tea leaf speculation around when we might actually get the thing.\nNow, GPT-5 has been coming soon for almost a year at this point, but the model that finally gets released will be very different from where it started.\nIn the middle of last year, rumors started circulating about the next big models from OpenAI, codeamed Orion.\nRumors in the fall were that the model was going to come as early as December.\nBut it wasn't too long after that, in around November, that we started hearing about industry-wide issues around training with a building narrative that pre-training had hit a wall.\nWhat we ultimately got in the place of a new flagship model in the series of GPT3, 3.5, and 4 was instead OpenAI's first reasoning model, first 01 and then 03.\nNow, as we've previously discussed, the launch of reasoning models, it turns out, were a huge inflection point in the adoption of AI.\nA whole array of new use cases came online.\nEnterprise adoption started going up significantly, and the new era of Agentics became a viable possibility rather than just something for the future.\nNow, in and around the reasoning models, we did get a GPT 4.5.\nIt clearly wasn't a big enough leap to warrant the title.\nAnd while the model was a niche hit among people who wanted a better LLM for writing, it failed to capture much attention or usage in the broader public.\nIn fact, the model is actually being sunset in the API next week.\nSwix from latent space had this interesting observation saying, \"I think the quote unquote failure of GPT4.5 relative to 03 and 04 mini is actually fantastic validation of the bet on the reasoning paradigm. 10 to 100x larger model consistently lost out to smaller reasoning model for an important open domain task and still hold aside what's great about the reasoning models.\"\nThe point is that there hasn't been a new OpenAI flagship in the series of GPTs for about a year now, and it's been over 2 years since they felt confident enough to herald a new generation of models with a full numerical upgrade from GBT4.\nAt the same time, the industry has changed quite a bit since the last time we got excited about GBT 5.\nLast fall, as we just discussed, the narrative was that scaling hit a wall, and people were really wondering if there was any more juice left to squeeze when it came to pre-training or if we were just in the era of new methodologies like test time compute, the reasoning era, and agentics with tool use being the vectors for improvement going forward.\nAt this point, now 6 to 9 months later, there is a pervasive sense of optimism that AI is nowhere near its zenith and that there are still plenty of avenues to pursue in order to improve the technology.\nLogan Kilpatrick, the head of product at Google AI studio, recently posted, \"The next 6 months of AI are likely to be the most wild we have seen so far. Everything keeps scaling up. More hardware, more model progress, more product knowledge, more AI momentum, more product market fit.\"\nNow, Logan went to Pays to say that this wasn't inspired by any specific thing.\nHe said it was just inspired by me feeling the progress and then looking out over the next 6 months and being reminded it's going to continue.\nSo bringing it back to GPT-5.\nWhat part of that excitement can we attribute to this new model?\nThe main expectation around GPT-5 is that it will represent a unification of OpenAI's technology.\nIn a recent podcast, OpenAI's head of developer experience, Roma Huitt, said, \"We're truly excited not to just make a net new great frontier model. We're also going to unify our two series. The breakthrough of reasoning in the O series and the breakthroughs in multimodality in the GPT series will be unified and that will be GBT5.\"\nOne of the core promises is that GBT5 will do away with model switching.\nIn a Reddit AMA from May, OpenAI VP Jerry Tour wrote, \"GBT5 is our next foundation model that is meant to just make everything our models can currently do better and with less model switching.\"\nInterestingly, he also referred to their operator agent as a quote product surface, suggesting perhaps that GPT-5 will feature tighter integration with Agentic Tools.\nBack in February, Sam Altman had said something similar about the model switching idea.\nIn a post on X, he wrote, \"We want AI to just work for you. We realize how complicated our model and product offerings have gotten. We hate the model picker as much as you do and want to return to Magic Unified Intelligence.\"\nNow, this post was a couple weeks before GPT-5, and he said, \"We will next ship GPT4.5, the model we called Orion internally, as our last non-chain of thought model. After that, a top goal for us is to unify O series models and GPT series models by creating systems that can use all of our tools, know when to think for a long time or not, and generally be useful for a wide range of tasks.\"\nNow, getting a little bit more up to date, developer Bueno compiled some other nuggets of information gleaned from recent interviews and leaks.\nHe expects a 256k context window, which would put GBT5 roughly in line with most competitors, but not as large as Google's million token context window.\nMultimodality likely means we'll see native video, image, and audio inputs and perhaps even outputs.\nAnother current rumor is that OpenAI will adopt the mixture of experts architecture brought to prominence recently by several Chinese labs.\nThis architecture means that only part of the model is engaged at a time, allowing for a higher number of total parameters while keeping inference costs down.\nThere are estimates that inference costs could be 60% lower per token than GBT40.\nMemory is another factor that could see improvement which would lead to more performant agent operation as well.\nNot commented.\nFor builders, this likely means you have to rethink prompt design for giant context.\nExpect richer tool calling that mixes text with timebased media and budget for lower latency, cheaper API calls despite a larger model.\nGBT5 looks less like a parameter bump and more like a systems integration milestone, folding multiple specialized capabilities into one cohesive model.\nAnd frankly, even without the rumors, that seems like a reasonable assumption.\nSince its release, OpenAI has been tacking features on to GPT4, including things like memory and updated image generation.\nGPT-5 represents their first opportunity to bake these features in natively, allowing the model to be trained from the ground up on how to make best use of the tools it has access to.\nVasserax commented, \"GT5 might be the first model that feels like true AGI. If OpenAI integrates full O4 or O4 Pro reasoning plus agentic tool use within the chain of thought, operator codecs and deep research, we're talking about a model that can think, plan, act, and adapt like never before. AGI vibes incoming.\"\nNow, one of the reasons we think that GBT 5 is nearing release is that users are starting to report seeing a ton of AB testing of something new on the platform.\nSpecifically, it appears that OpenAI is testing how reasoning traces are presented.\nOne X user, for example, suggested that a recent interaction they had looked like hybrid reasoning.\nIn response to a prompt using the 40 model, which again is not a natively reasoning model, ChatBT said, \"Just a moment. I want to give this one the extra thought it deserves.\"\nThat certainly would imply that OpenAI has figured out a way to differentiate between the prompts that should use reasoning and those that shouldn't.\nAnother interesting feature on that post is a button labeled answer.\nNow, that presumably cuts the reasoning short and forces the model to just spit out an answer based on how much thinking it's done at that point.\nOne of the problems with hybrid reasoning is that if the model starts thinking and goes down some rabbit hole, there hasn't historically been a good way to make it stop.\nSo again, while it's just a guess, it seems like answer now might be a UX change to address that issue.\nSo if the idea is that GPT-5 will natively bring together all of these features that have been bolted on, perhaps we also need a slightly different way of thinking about what an LLM actually is.\nKarina Nuen, a researcher and product staffer at OpenAI, recently posted super intelligent operating system.\nBelieve it or not, she is not talking about my startup.\nInstead, cryptically, it sounds like it's sort of what's being described in the rumors of GPT-5.\nRather than treating it like an individual model you interact with, GPT-5 kind of sounds like an AI operating system.\nTJ Rididgeway writes, \"So GPT-5 is the AGI framework. This is why they're all pivoting to super intelligence discussion, the one ring to rule them all.\"\nIn response to the idea that super intelligence won't just be a scaled LLM, he added, \"Yes, I pretty much agree. What I'm saying here is this roadmap they put out is the framework through which AGI will be achieved, not AGI itself. I believe innovations in long-term memory are also a crucial aspect that is just now being explored.\"\nIt is worth noting that back in January, Sam Altman did mention, quote, \"We're now confident we know how to build AGI as we have traditionally understood it.\"\nStill, a practical question after all of this is whether any of it will be enough to amaze heavy AI users.\nSpecifically, as much as we're summing up all of these different rumors here, none of them really suggest, at least not yet, a massive change in capability or anything particularly new.\nInstead, these rumors all focus on bringing everything together and making the user experience more seamless.\nChubby, for example, wrote, \"For hardcore users, GPT-5 will be a bit of a disappointment if the rumors are to be believed.\"\nRumor has it that Sam Altman is not particularly impressed with the performance and improvements compared to older models such as GPT40 and 03.\nGBT5 is more of an iterative improvement that certainly shows significant leaps in benchmarks.\nBut compared to benchmarks such as reasoning at the end of last year 01 or deep research at the beginning of this year, GPT-5 is more of the same just slightly better.\nIn this respect, GPT-5 is probably not the qualitative leap that hardcore users had hoped for.\nAnd yet, Chubby wrote, \"Maybe the hardcore users are not the point.\"\nThey continue for the vast majority.\nHowever, GPT-5 will be a quantum leap.\nWhen I talk to friends, I almost always hear the same thing.\nChatGpt is great.\nThey say it will help them get more out of their university studies, answer all their questions, and even provide excellent advice on medical matters.\nWhen asked which model they would use for this, the answer is always the same.\nGPT40.\nOf course, they either don't know anything about 03 or due to the complicated nomenclature, they consider 03 to be the inferior model because it's older, because three becomes before four.\nSome people think that simply.\nHowever, GPT-5 will be an all-in-one model.\nDepending on the request, the appropriate amount of inference will be applied and reasoning will be carried out.\nThis means that all those who previously used only GPT40 will suddenly receive much better answers with GPT-5 than before because they did not use or were not aware of the full strength and range of ChatGpt's model capacity.\nNow, this I think is a super important point.\nThe vast majority of ChatGpt users, even at this stage, are not subscribers.\nThey haven't used deep research and they don't understand what a reasoning model is or at least how it's different.\nIn other words, removing ChatJBT's model selector isn't just a minor UX improvement.\nInstead, it fundamentally will broaden the average user's experience by making all of the myriad features available to them without them having to know about what those features actually do.\nThink back to the DeepSeek moment at the beginning of the year.\nThe viral breakthrough was not that there was better reasoning.\nThe full version of 01 had already been out for 2 months and was clearly the better model.\nThe innovation was simply putting reasoning right in front of users who had never experienced it before with the full reasoning traces on display.\nThe average comment about DeepSeek wasn't just that it was powerful.\nIt's that it was really cute or cool because it talked to itself before responding.\nOpenAI then has the opportunity to give the average inexperienced user that type of moment of delight with GPT-5.\nEven if every single feature already exists and the performance bump is minor, improved accessibility is a huge deal for the average GPT-5 user.\nAnd yet, I do think that unfortunately for OpenAI, that still might not be enough, or at least not for long.\nIt's very clear that in the wake of Mark Zuckerberg's poaching spree, the stakes for OpenAI are building.\nZuck super intelligence team is now largely in place.\nAnd then it's worth then trying to speculate on what their big play is.\nIn the leadup to the release of Llama 4 during the spring, Zuckerberg set off on a media tour discussing his AI plans at length.\nThere were really two big themes.\nThe first was making an automated advertising platform and the second was introducing AI friends to Meta social media platforms.\nHold aside what you think of those ideas.\nIt is pretty undeniable that they're both fairly modest ambitions.\nIn other words, neither is something you would necessarily want to spend hundreds of millions of dollars in payroll to achieve.\nAnd so whether Zuckerberg is aiming for a straightshot super intelligence play or iterative model releases that are actually keeping up with and pushing the state-of-the-art, he clearly has something much larger than just better advertising automation in mind.\nOne observation that many have made is that Zuckerberg has put together a team of experts with a wide range of skills.\nHe poached the reasoning team lead from OpenAI, a multimodal expert from Google, an edge model developer from Apple, and the list goes on.\nIn other words, a full stack team capable of recreating everyone OpenAI or anyone else has on offer from scratch.\nPoint being, whatever they're building, it's not AI friends.\nAlman says he's not concerned.\nIn an interview at the Sun Valley Conference, he was asked how he's feeling about the talent war and responded, \"Fine, good. We have obviously an incredibly talented team and I think\n\n\nThey really love what they're doing.\nObviously, some people will go to different places.\nThere's a lot of excitement, I guess you could say, in the industry, but no, I think we feel fine.\nAt the same time, it is pretty clearly undeniable that whether Sam is a part of this or not, OpenAI leadership in general is starting to feel the heat.\nWe've seen changes in compensation packages, memos that suggest the feeling was of having their house broken into.\nAnd so for this reason, I think that the GPT-5 moment, whether OpenAI wants it to be or not, is going to be seen as hugely significant and reflective of the state of play when it comes to the broader industry.\nBack in April, even before this aggressive talent war, Alman tweeted, \"Change of plans.\nWe are going to release 03 and 04 mini after all probably in a couple of weeks and then do GBT5 in a few months.\nThere are a bunch of reasons for this, but the most exciting one is that we are going to be able to make GBT much better than we originally thought.\nWe also found it harder than we thought it was going to be to smoothly integrate everything, and we want to make sure we have enough capacity to support what we expect to be unprecedented demand.\nSo, just how soon is this thing coming?\nUltimately, that among all of this is the biggest rumor.\nWe have gotten a bunch of hints recently from OpenAI insiders that something big is coming in the next week or two, but I also think that OpenAI knows the stakes of GPT-5.\nI don't think that they're feeling so much pressure that they're going to release something that is anything less than extremely impressive.\nStill, maybe we have an exciting Midsummer tree coming up.\nCertainly, the chorus of rumors is getting louder and as they get more credible, I will be sure to let you know it here.\nFor now though, that is going to do it for today's The AI Daily Brief.\nUntil next time, peace.\n",
  "dumpedAt": "2025-07-21T18:43:24.577Z"
}