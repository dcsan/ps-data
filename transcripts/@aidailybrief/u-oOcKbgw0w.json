{
  "episodeId": "u-oOcKbgw0w",
  "channelSlug": "@aidailybrief",
  "title": "Does AI Secretly Slow Developers Down?",
  "publishedAt": "2025-07-16T01:33:31.000Z",
  "rawLines": [
    {
      "lang": "en",
      "text": "Are AI coding tools actually slowing",
      "offset": 0.32,
      "duration": 4.959
    },
    {
      "lang": "en",
      "text": "developers down? Welcome back to the AI",
      "offset": 2.8,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "daily brief. Today we are talking about",
      "offset": 5.279,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "a study that is getting an absolute ton",
      "offset": 7.359,
      "duration": 5.841
    },
    {
      "lang": "en",
      "text": "of buzz. A group of developers were",
      "offset": 10.719,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "tested to see how much more productive",
      "offset": 13.2,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "AI coding tools would make them. They",
      "offset": 15.519,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "assumed going into the study that they",
      "offset": 17.92,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "would be about 24 25% more productive",
      "offset": 19.439,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "and even after the study concluded",
      "offset": 22,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "thought that AI had made them 20% more",
      "offset": 23.76,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "productive. But the study actually found",
      "offset": 25.6,
      "duration": 5.679
    },
    {
      "lang": "en",
      "text": "that they were 19% less productive, 19%",
      "offset": 27.599,
      "duration": 6.721
    },
    {
      "lang": "en",
      "text": "slower on this set of coding tasks. Now,",
      "offset": 31.279,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "as you might imagine, this has been",
      "offset": 34.32,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "widely reported on outlets like CNBC,",
      "offset": 35.6,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "suggesting that it makes for a crack in",
      "offset": 38,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "the AI productivity bullcase. The",
      "offset": 40.079,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "implications of something like this are",
      "offset": 42.399,
      "duration": 3.281
    },
    {
      "lang": "en",
      "text": "big. Billions and billions, if not",
      "offset": 43.76,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "trillions of dollars, are being spent",
      "offset": 45.68,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "assuming that AI is going to make us",
      "offset": 47.2,
      "duration": 3.519
    },
    {
      "lang": "en",
      "text": "more productive. Does this all throw it",
      "offset": 48.879,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "into question? Somehow, my guess is at",
      "offset": 50.719,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "this point, if you are a regular",
      "offset": 53.36,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "listener to this show, you will",
      "offset": 54.879,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "virtually hear the cracking of my",
      "offset": 56.719,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "knuckles and neck as I prepare to",
      "offset": 58.8,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "critique this particular study. Now, I",
      "offset": 60.64,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "do want to caveat things. In general, I",
      "offset": 63.44,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "am always interested to see what this",
      "offset": 65.76,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "group comes out with. They were the team",
      "offset": 67.2,
      "duration": 3.279
    },
    {
      "lang": "en",
      "text": "who developed the methodology that",
      "offset": 69.04,
      "duration": 3.119
    },
    {
      "lang": "en",
      "text": "suggested that agent capabilities are",
      "offset": 70.479,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "doubling every 7 months. And so, I don't",
      "offset": 72.159,
      "duration": 3.521
    },
    {
      "lang": "en",
      "text": "think that this is from some shoddy",
      "offset": 74.159,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "organization or anything like that. I",
      "offset": 75.68,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "just happen to disagree pretty",
      "offset": 78.159,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "fundamentally with at least one",
      "offset": 79.52,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "particular assumption that I think is",
      "offset": 81.439,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "fairly important to the study and even",
      "offset": 83.04,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "more than that the way that it's being",
      "offset": 85.52,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "reported. But let's get into what it",
      "offset": 87.04,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "actually said before I get into my",
      "offset": 88.88,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "critique. Researchers from MET, which by",
      "offset": 90.32,
      "duration": 3.119
    },
    {
      "lang": "en",
      "text": "the way, I don't even know if they call",
      "offset": 92.4,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "it MET, that's what I call it, MER,",
      "offset": 93.439,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "which is a nonprofit AI research firm,",
      "offset": 95.52,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "recently tested 16 developers with what",
      "offset": 98.079,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "they identified as moderate AI skills,",
      "offset": 101.04,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "something that we will come back to,",
      "offset": 104.159,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "across hundreds of tasks in which they",
      "offset": 105.84,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "had roughly 5 years of experience. Each",
      "offset": 107.439,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "task was randomly assigned to either",
      "offset": 109.759,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "allow or disallow AI usage. Before the",
      "offset": 111.36,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "test began, the programmers said that",
      "offset": 113.759,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "they believed that AI would reduce",
      "offset": 115.119,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "completion time by 24%. And after they",
      "offset": 116.479,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "finished, they believed that the AI had",
      "offset": 119.119,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "helped them get a 20% speed boost, but",
      "offset": 120.64,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "the actual results, as I mentioned,",
      "offset": 122.96,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "found that AI had actually slowed them",
      "offset": 125.04,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "down by 19%. The study showed a wide",
      "offset": 126.56,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "range of results across different",
      "offset": 129.36,
      "duration": 3.519
    },
    {
      "lang": "en",
      "text": "complexities of tasks. For tasks that",
      "offset": 130.64,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "take up to 1 hour, developers were",
      "offset": 132.879,
      "duration": 2.881
    },
    {
      "lang": "en",
      "text": "basically the same speed whether they",
      "offset": 134.4,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "used AI or not. The same was true for",
      "offset": 135.76,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "extremely long tasks that took 7 or 8",
      "offset": 138.08,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "hours. The only range where there was a",
      "offset": 140.239,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "big difference was in moderately complex",
      "offset": 142.16,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "tasks that take between 1 hour and 6",
      "offset": 143.92,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "hours. The results were extremely",
      "offset": 145.76,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "consistent with AI assisted programmers",
      "offset": 148,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "slowing down as the task stretched to",
      "offset": 149.84,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "the 2-hour mark. AI and non-AI",
      "offset": 151.28,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "programming again converged as the task",
      "offset": 153.44,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "got even longer with very little gap",
      "offset": 155.04,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "once they reached 8 hours. The study",
      "offset": 156.8,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "also used screen recordings to break",
      "offset": 159.12,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "down how the programmers used their time",
      "offset": 160.72,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "across AI and nonAI coding. When using",
      "offset": 162.319,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "AI, the researchers found the time spent",
      "offset": 164.64,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "actively coding, reading or searching,",
      "offset": 166.4,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "testing and debugging, and dealing with",
      "offset": 167.84,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "the git and environment all went down.",
      "offset": 169.12,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "Active coding and reading or searching",
      "offset": 171.2,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "saw the sharpest drops in time spent.",
      "offset": 172.64,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "idle or overhead time was the only",
      "offset": 174.56,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "factor that went up when using AI and",
      "offset": 175.84,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "the difference was in time spent",
      "offset": 177.84,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "prompting, waiting for AI outputs and",
      "offset": 179.12,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "reviewing the generated code. Now, the",
      "offset": 180.64,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "researchers bundled the potential causes",
      "offset": 182.72,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "of the slowdown into five major",
      "offset": 184.48,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "categories. The first was simply that",
      "offset": 186,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "programmers were overly optimistic about",
      "offset": 188.64,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "AI usefulness. Second, they noted that",
      "offset": 190.239,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "some developers were too familiar with",
      "offset": 192.879,
      "duration": 3.121
    },
    {
      "lang": "en",
      "text": "the codebase they were working on, so AI",
      "offset": 194.319,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "didn't have much to offer. Third, others",
      "offset": 196,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "were working on larger complex",
      "offset": 198,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "repositories where AI ran into context",
      "offset": 199.2,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "window limits. Fourth, and a big one,",
      "offset": 201.2,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "was low AI reliability. The programmers",
      "offset": 203.44,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "only accepted 44% of AI generations and",
      "offset": 205.92,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "spent 9% of their time cleaning up",
      "offset": 208.08,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "generated code. And fifth, and finally,",
      "offset": 209.84,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "the developers reported more generalized",
      "offset": 212.08,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "context issues where the AI didn't",
      "offset": 214.08,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "recognize the repository properly. Now,",
      "offset": 215.519,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "to their credit, the researchers here",
      "offset": 218.319,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "are not at all suggesting that we should",
      "offset": 220.48,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "throw the baby out with the bathwater.",
      "offset": 222.08,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "There are caveats up and down this",
      "offset": 224.319,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "thing, qualifications that try to not",
      "offset": 226.159,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "overstate the case. They said, for",
      "offset": 228.08,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "example, the slowdown we observe does",
      "offset": 230,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "not imply that current AI tools do not",
      "offset": 231.92,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "often improve developers productivity.",
      "offset": 233.68,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "We find evidence that the high developer",
      "offset": 235.68,
      "duration": 3.279
    },
    {
      "lang": "en",
      "text": "familiarity with repositories and the",
      "offset": 237.28,
      "duration": 3.519
    },
    {
      "lang": "en",
      "text": "size and maturity of the repositories",
      "offset": 238.959,
      "duration": 3.521
    },
    {
      "lang": "en",
      "text": "both contribute to the observed slowdown",
      "offset": 240.799,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "and these factors do not apply in many",
      "offset": 242.48,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "software development settings. So let's",
      "offset": 244.239,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "actually talk about what some of the",
      "offset": 246.799,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "challenges here could have been. First",
      "offset": 248.4,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "of all, I think it's completely correct",
      "offset": 250.319,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "to acknowledge that this is a different",
      "offset": 252.319,
      "duration": 5.121
    },
    {
      "lang": "en",
      "text": "type of working. Coding with coding",
      "offset": 255.12,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "tools involves entirely new processes",
      "offset": 257.44,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "and entirely new emphasis in different",
      "offset": 260.72,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "types of work categories. One of the",
      "offset": 262.479,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "devs in the study, Quinton Anthony,",
      "offset": 264.56,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "actually talked about this. Regarding",
      "offset": 266.24,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "the idea of distractions, he writes,",
      "offset": 268.16,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "\"It's super easy to get distracted in",
      "offset": 269.919,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "the downtime while LLMs are generating.",
      "offset": 271.68,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "The social media attention economy is",
      "offset": 273.84,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "brutal, and I think people spend 30",
      "offset": 275.52,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "minutes scrolling while quote unquote",
      "offset": 277.04,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "waiting for their 30-cond generation.\"",
      "offset": 278.32,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "All I can say on this one is that we",
      "offset": 280.32,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "should know our own pitfalls and try to",
      "offset": 281.84,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "fill this LLM generation time",
      "offset": 283.12,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "productively. If the task requires high",
      "offset": 284.56,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "focus, spend this time either working on",
      "offset": 286.72,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "a subtask or thinking about follow-up",
      "offset": 288.24,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "questions. Even if the model one shots",
      "offset": 289.84,
      "duration": 2.639
    },
    {
      "lang": "en",
      "text": "your question, \"What else don't I",
      "offset": 291.36,
      "duration": 3.119
    },
    {
      "lang": "en",
      "text": "understand?\" If the task requires low",
      "offset": 292.479,
      "duration": 3.521
    },
    {
      "lang": "en",
      "text": "focus, do another small task in the",
      "offset": 294.479,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "meantime. As always, small digital",
      "offset": 296,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "hygiene steps helps with this. And",
      "offset": 298.24,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "holding aside any sort of focus on",
      "offset": 300.32,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "social media intrusions and distractions",
      "offset": 302.24,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "while you're waiting for a prompt to",
      "offset": 304.16,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "resolve, there also is inevitably just",
      "offset": 305.36,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "going to be a shift in the type of work",
      "offset": 307.52,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "that you have to do. Maybe you are",
      "offset": 309.12,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "writing less actual code, but you might",
      "offset": 311.199,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "spend more time debugging. That was part",
      "offset": 313.68,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "of what the researchers actually",
      "offset": 315.919,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "directly found. And so I think the",
      "offset": 317.12,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "summary of these two parts and something",
      "offset": 319.28,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "we'll come back to at the end of this is",
      "offset": 320.88,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "that coding with AI tools is not just",
      "offset": 322.8,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "the same as coding but faster. It is a",
      "offset": 325.12,
      "duration": 5.359
    },
    {
      "lang": "en",
      "text": "new process that requires new thinking.",
      "offset": 327.44,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "Next, let's talk about the models that",
      "offset": 330.479,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "were used. Now, bad models might be an",
      "offset": 331.84,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "overstatement for the sake of space in",
      "offset": 333.919,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "an AI generated image here, but this",
      "offset": 335.28,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "study was conducted at the beginning of",
      "offset": 337.199,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "this year. And while that seems like a",
      "offset": 338.639,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "short time ago, all of the models that",
      "offset": 340.639,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "people use to code are much advanced",
      "offset": 342.8,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "from the ones that they were using in",
      "offset": 344.88,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "this study. Ruben Bloom, who works on",
      "offset": 346.32,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "Less Wrong, also participated in the",
      "offset": 348.56,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "study and said, \"As a developer in the",
      "offset": 350.56,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "study, it's striking to me how much more",
      "offset": 352.4,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "capable the models have gotten since",
      "offset": 354.16,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "February when I was participating. I'm",
      "offset": 355.6,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "trying to recall if I was even using",
      "offset": 357.759,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "agents at the start. Certainly, the",
      "offset": 359.199,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "later models, Opus 4, Gemini 2.5 Pro, 03",
      "offset": 361.199,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "could do just vastly more with less",
      "offset": 364,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "guidance than 3601, etc. For me, not",
      "offset": 365.68,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "going over my own data in the study, I",
      "offset": 368.88,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "could buy that maybe I was being slowed",
      "offset": 370.56,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "down a few months ago, but it's much",
      "offset": 372.24,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "harder to believe now. Now, Ruben or",
      "offset": 373.84,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "Ruby, as he goes by, also did validate",
      "offset": 376.319,
      "duration": 3.041
    },
    {
      "lang": "en",
      "text": "the other piece that we were just",
      "offset": 378.319,
      "duration": 2.641
    },
    {
      "lang": "en",
      "text": "discussing about as well, saying, \"I",
      "offset": 379.36,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "feel like historically a lot of my AI",
      "offset": 380.96,
      "duration": 2.959
    },
    {
      "lang": "en",
      "text": "speedup gains were eaten by the fact",
      "offset": 382.56,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "that while a prompt was running, I'd",
      "offset": 383.919,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "look at something else, Facebook X,",
      "offset": 385.28,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "etc., and continue to do so for much",
      "offset": 387.199,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "longer than it took the prompt to run. I",
      "offset": 388.96,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "discovered 2 days ago that cursor has or",
      "offset": 391.12,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "now has a feature you can enable to ring",
      "offset": 392.88,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "a bell when the prompt is done. I expect",
      "offset": 394.56,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "to reclaim a lot of AI gains this way.",
      "offset": 396.479,
      "duration": 5.041
    },
    {
      "lang": "en",
      "text": "Point being that while 3.5 and 3.7",
      "offset": 398.8,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "sonnet aren't bad models contrary to my",
      "offset": 401.52,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "image here, they are certainly less",
      "offset": 404.4,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "performant than all the tools we have",
      "offset": 406.24,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "now. Right? This is before Claude code.",
      "offset": 407.68,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "This is before 03. This is before 2.5.",
      "offset": 409.919,
      "duration": 5.761
    },
    {
      "lang": "en",
      "text": "A fourth category that is incredibly",
      "offset": 413.44,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "important and is the one acknowledged",
      "offset": 415.68,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "most by the authors is the codebase",
      "offset": 417.52,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "context. Remember the authors wrote, \"We",
      "offset": 419.52,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "find evidence that the high developer",
      "offset": 421.84,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "familiarity with repositories and the",
      "offset": 423.44,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "size and maturity of the repositories",
      "offset": 425.36,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "both contributed to the observed",
      "offset": 427.36,
      "duration": 2.959
    },
    {
      "lang": "en",
      "text": "slowdown and these factors do not apply",
      "offset": 428.72,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "in many software development settings.\"",
      "offset": 430.319,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "Fellow AI podcaster Nathan Leb put it",
      "offset": 432.56,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "more simply, \"Expert developers working",
      "offset": 434.88,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "in large code bases is known to be the",
      "offset": 437.039,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "setting where AI can help least.\" Both",
      "offset": 438.96,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "of these factors matter. The fact that",
      "offset": 441.68,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "they're working in large code bases and",
      "offset": 443.12,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "that they are experts in those code",
      "offset": 444.72,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "bases is, as Nathan points out,",
      "offset": 446.08,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "something of a mismatched use case for",
      "offset": 448.479,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "some of these AI coding tools, which",
      "offset": 450.16,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "does not mean at all, by the way, that",
      "offset": 452.4,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "it's not valuable to study them. What it",
      "offset": 454,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "means, and this is something that's",
      "offset": 456.56,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "going to run throughout this analysis,",
      "offset": 457.759,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "is that it's very difficult to draw",
      "offset": 459.759,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "general conclusions across the entire",
      "offset": 461.68,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "field of software developers based on",
      "offset": 463.84,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "these 16 that were studied. If you want",
      "offset": 465.599,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "to be generous to the researchers,",
      "offset": 468,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "that's not exactly what they're trying",
      "offset": 469.52,
      "duration": 3.519
    },
    {
      "lang": "en",
      "text": "to do. But when you put out a study like",
      "offset": 470.88,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "this, you know it's going to get",
      "offset": 473.039,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "amplified. Now, Nathan though points out",
      "offset": 474.16,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "that there is another piece here that",
      "offset": 476.8,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "the fact that it's known that this isn't",
      "offset": 478.479,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "the best use case for AI coding meant",
      "offset": 480.56,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "that the participants didn't have as",
      "offset": 482.879,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "much AI coding experience coming in. And",
      "offset": 484.879,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "as he points out, not wrongly, given the",
      "offset": 487.52,
      "duration": 5.119
    },
    {
      "lang": "en",
      "text": "work they do. And this gets us to the",
      "offset": 489.199,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "biggest debate, which is about learning",
      "offset": 492.639,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "curves and how to designate this set of",
      "offset": 494.639,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "developers when it comes to their AI",
      "offset": 496.879,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "experience. This is where some of the",
      "offset": 498.56,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "loudest disagreement comes in and where",
      "offset": 501.039,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "I have some of my biggest issues. Now, I",
      "offset": 503.039,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "am not alone in this. In fact, perhaps",
      "offset": 505.44,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "the loudest critique of this paper has",
      "offset": 507.36,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "come from Emtt Shear. Emmerit was a",
      "offset": 509.28,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "co-founder at Twitch and spent a very",
      "offset": 511.28,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "hectic weekend as the CEO of OpenAI when",
      "offset": 512.959,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "Sam Alman was deposed. He tweeted,",
      "offset": 515.12,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "\"Metter's analysis of this experiment is",
      "offset": 517.76,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "wildly misleading. The results indicate",
      "offset": 519.839,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "that people who have approximately never",
      "offset": 522.159,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "used AI tools before are less productive",
      "offset": 524.159,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "while learning to use the tools and says",
      "offset": 526.399,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "nothing about experienced AI tool users.",
      "offset": 528.32,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "EMTT continues, I immediately found the",
      "offset": 531.36,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "claim suspect because it didn't jive",
      "offset": 533.6,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "with my own experience working with",
      "offset": 535.2,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "people who were using coding assistants,",
      "offset": 536.64,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "but sometimes there are surprising",
      "offset": 538.48,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "results, so I dug in. The first",
      "offset": 539.92,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "question, who were these developers in",
      "offset": 542,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "the study getting such poor results? He",
      "offset": 543.92,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "then quoted from the methodology. We",
      "offset": 546.16,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "recruited 16 experienced open source",
      "offset": 548.24,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "developers to work on 246 real tasks in",
      "offset": 550.399,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "their own repositories. So EMTT writes,",
      "offset": 553.12,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "\"They sound like reasonably experienced",
      "offset": 555.36,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "software devs. Back to the study,",
      "offset": 557.04,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "developers have a range of experience",
      "offset": 558.959,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "using AI tools. 93% have prior",
      "offset": 560.399,
      "duration": 5.041
    },
    {
      "lang": "en",
      "text": "experience with tools like chatbt, but",
      "offset": 563.2,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "only 44% have experience using cursor.\"",
      "offset": 565.44,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "Uh-oh, writes EMTT. So they haven't",
      "offset": 568.16,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "actually used AI coding tools. They've",
      "offset": 570.16,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "like tried prompting an LLM to write",
      "offset": 572.24,
      "duration": 3.279
    },
    {
      "lang": "en",
      "text": "code for them. But that's an entirely",
      "offset": 573.92,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "different kind of experience as anyone",
      "offset": 575.519,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "who has used these tools can tell you.",
      "offset": 577.04,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "They claim a range of experience using",
      "offset": 578.88,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "AI tools. Yet only a single developer of",
      "offset": 580.64,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "their 16 had more than a single week of",
      "offset": 582.959,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "experience using cursor. They make it",
      "offset": 584.959,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "look like a range by breaking less than",
      "offset": 587.2,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "a week into under 1 hour, 1 to 10 hours,",
      "offset": 588.8,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "10 to 30 hours, and 30 to 50 hours of",
      "offset": 591.279,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "experience. Given the steep learning",
      "offset": 593.36,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "curve for effectively using these AI",
      "offset": 595.279,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "tools, well, this division betrays what",
      "offset": 597.04,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "I hope is just grossly negligent",
      "offset": 598.959,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "ignorance about the reality rather than",
      "offset": 600.72,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "intentional deception. Of course, the",
      "offset": 602.16,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "one developer who did have more than one",
      "offset": 604.399,
      "duration": 3.521
    },
    {
      "lang": "en",
      "text": "week of experience was 20% faster",
      "offset": 606.16,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "instead of 20% slower. The authors note",
      "offset": 607.92,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "this fact, but then say, \"We are",
      "offset": 610.32,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "underpowered to draw strong conclusions",
      "offset": 611.6,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "from this analysis and bury it in a",
      "offset": 613.04,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "figure's description in an appendex.\" If",
      "offset": 614.72,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "the authors of the paper had made the",
      "offset": 616.959,
      "duration": 3.041
    },
    {
      "lang": "en",
      "text": "claim, \"We tested experienced developers",
      "offset": 618.32,
      "duration": 3.519
    },
    {
      "lang": "en",
      "text": "using AI tools for the first time and",
      "offset": 620,
      "duration": 2.959
    },
    {
      "lang": "en",
      "text": "found that at least during the first",
      "offset": 621.839,
      "duration": 2.321
    },
    {
      "lang": "en",
      "text": "week, they were slower rather than",
      "offset": 622.959,
      "duration": 2.401
    },
    {
      "lang": "en",
      "text": "faster,\" that would have been a",
      "offset": 624.16,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "moderately interesting finding and true.",
      "offset": 625.36,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "Alas, that is not the claim they made.",
      "offset": 627.2,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "Now, David Rean, one of the researchers,",
      "offset": 629.92,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "stood behind the methodology,",
      "offset": 631.76,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "responding, \"Devs had roughly the",
      "offset": 633.12,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "following prior LLM experience. Seven",
      "offset": 635.2,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "out of the 16 had over hundreds of",
      "offset": 637.6,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "hours, seven of the 16 had 10 to 100",
      "offset": 639.44,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "hours, and two of the 16 had 1 to 10",
      "offset": 641.76,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "hours. We think describing this as",
      "offset": 644,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "moderate AI experience is fair. Now, in",
      "offset": 646,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "the thread, David said, \"My guess is",
      "offset": 648.32,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "we'll have to agree to disagree.\" And",
      "offset": 649.92,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "respectfully, I firmly firmly disagree",
      "offset": 652.24,
      "duration": 6.32
    },
    {
      "lang": "en",
      "text": "here. First of all, using chatbt even to",
      "offset": 654.959,
      "duration": 5.921
    },
    {
      "lang": "en",
      "text": "code is not the same as using a",
      "offset": 658.56,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "dedicated agentic IDE. Second, this is",
      "offset": 660.88,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "not a significant period of time when it",
      "offset": 664,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "comes to tool use. 40 hours, one work",
      "offset": 666,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "week is not a moderate amount of time to",
      "offset": 668.72,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "use a new tool, especially when we were",
      "offset": 671.04,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "just discussing the fact that it",
      "offset": 672.64,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "involves totally new patterns of",
      "offset": 674.24,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "working. EMTT again wrote, \"It's clear",
      "offset": 676,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "that the source of disagreement is that",
      "offset": 678.16,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "I think using cursor effectively is a",
      "offset": 679.6,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "distinct skill from talking to GDP while",
      "offset": 681.519,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "you program and expect fairly low",
      "offset": 683.44,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "transfer. And the authors think it's the",
      "offset": 685.36,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "similar skill and expect much higher.\"",
      "offset": 687.12,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "When Megan Kinnman from Matter pointed",
      "offset": 689.12,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "out that devs whose primary IDE was",
      "offset": 690.8,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "cursor before the experiment were also",
      "offset": 692.56,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "slowed down on average, although by less",
      "offset": 694.24,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "than the average in the study, developer",
      "offset": 696.24,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "Tyler John pointed out this is useful,",
      "offset": 698.079,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "but there's only three of them. And it",
      "offset": 699.76,
      "duration": 3.519
    },
    {
      "lang": "en",
      "text": "sounds like the most experienced one was",
      "offset": 701.76,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "dramatically sped up. I think a study",
      "offset": 703.279,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "with experienced cursor users is",
      "offset": 705.76,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "warranted to test the hypothesis. Now,",
      "offset": 707.44,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "it's not just me and EMTT and a handful",
      "offset": 710.16,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "of Twitter commenters who are having the",
      "offset": 711.839,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "same response. AI programmer Simon",
      "offset": 713.36,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "Willis shared his thoughts, writing, \"My",
      "offset": 715.44,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "personal theory is that getting a",
      "offset": 717.68,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "significant productivity boost from LLM",
      "offset": 719.04,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "assistants and AI tools has a much",
      "offset": 720.8,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "steeper learning curve than most people",
      "offset": 722.48,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "expect. We see positive speed up for the",
      "offset": 724,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "one developer who has more than 50 hours",
      "offset": 726.32,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "of cursor experience. So, it's plausible",
      "offset": 728,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "that there is a high skill ceiling for",
      "offset": 729.92,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "using cursor such that developers with",
      "offset": 731.36,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "significant experience see positive",
      "offset": 733.2,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "speed up. My intuition here is that the",
      "offset": 734.72,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "study mainly demonstrates that the",
      "offset": 736.959,
      "duration": 2.721
    },
    {
      "lang": "en",
      "text": "learning curve on AI assisted",
      "offset": 738.32,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "development is high enough that asking",
      "offset": 739.68,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "developers to bake it into their",
      "offset": 741.6,
      "duration": 3.039
    },
    {
      "lang": "en",
      "text": "existing workflows reduces their",
      "offset": 742.8,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "performance while they climb that",
      "offset": 744.639,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "learning curve. And part of why EMTT is",
      "offset": 746,
      "duration": 5.519
    },
    {
      "lang": "en",
      "text": "so frustrated here is something which",
      "offset": 749.12,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "while outside of Met's control he",
      "offset": 751.519,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "believes effectively that they should",
      "offset": 753.839,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "have anticipated which is how the",
      "offset": 755.12,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "mainstream media is going to amplify",
      "offset": 757.2,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "these results. Again, I mentioned the",
      "offset": 758.72,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "headline is study finds AI tools made",
      "offset": 761.36,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "open source software developers 19%",
      "offset": 763.2,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "slower. All over Twitter/X, there are",
      "offset": 765.12,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "graphics like this one from TechJuice.",
      "offset": 767.6,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "Shocking studies suggest AI coding tools",
      "offset": 769.68,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "are slowing veteran developers by 19%.",
      "offset": 771.76,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "And then there's the mainstream media.",
      "offset": 774.56,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "Tech giants like Microsoft and Google",
      "offset": 776.56,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "are outsourcing more and more coding to",
      "offset": 778.24,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "AI in a productivity push, but some new",
      "offset": 780.48,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "research shows the tools might not be as",
      "offset": 783.279,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "helpful as some expect. These are",
      "offset": 785.44,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "stories that have the ability to impact",
      "offset": 787.6,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "markets in significant ways despite the",
      "offset": 789.36,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "fact that there are all these questions.",
      "offset": 791.6,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "Now, it is an entirely different episode",
      "offset": 793.76,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "on what researchers find their",
      "offset": 796.399,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "responsibility to be when it comes to",
      "offset": 798.48,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "the potential for amplification by",
      "offset": 800.8,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "mainstream media given how unbelievably",
      "offset": 802.639,
      "duration": 5.601
    },
    {
      "lang": "en",
      "text": "politicized AI is and will continue to",
      "offset": 805.68,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "be. Perhaps there is a higher burden",
      "offset": 808.24,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "there, but like I said, that's sort of",
      "offset": 810.399,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "the subject for a different show. The",
      "offset": 812.079,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "TLDDR for me is not that I think that",
      "offset": 814.16,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "the study isn't useful. It's that I",
      "offset": 816.56,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "don't think ultimately that it's saying",
      "offset": 818.8,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "what the researchers think it's saying.",
      "offset": 820.24,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "I think it's much closer to what EMTT",
      "offset": 822.16,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "Shear argued that a specific type of",
      "offset": 823.76,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "developer working on a specific type of",
      "offset": 825.76,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "codebase with a specific limited",
      "offset": 827.44,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "experience set with this particular set",
      "offset": 829.44,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "of tools encountered all sorts of issues",
      "offset": 831.12,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "that made them temporarily slower than",
      "offset": 833.92,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "had they not been using the tools. So",
      "offset": 835.839,
      "duration": 5.041
    },
    {
      "lang": "en",
      "text": "where does that leave us? Well, from a",
      "offset": 838.32,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "research perspective, there are obvious",
      "offset": 840.88,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "needs for follow-ups here. I think",
      "offset": 843.199,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "having developers working on different",
      "offset": 845.04,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "types of code bases with different",
      "offset": 846.8,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "levels of experience and specifically",
      "offset": 848.32,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "those who have actually worked more",
      "offset": 850.24,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "deeply with Cursor or any other agentic",
      "offset": 851.92,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "IDE would be a really valuable",
      "offset": 854.16,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "follow-up. At the same time, as Simon",
      "offset": 856.24,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "Willis points out, measuring developer",
      "offset": 858.48,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "productivity is notoriously difficult.",
      "offset": 860.16,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "So even with that, we're still going to",
      "offset": 862,
      "duration": 2.959
    },
    {
      "lang": "en",
      "text": "have to take everything with a grain of",
      "offset": 863.6,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "salt. And by the way, to their credit,",
      "offset": 864.959,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "it appears that Meter is actually",
      "offset": 866.72,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "thinking about expanding this study, and",
      "offset": 868.16,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "I hope that they do. We'll certainly",
      "offset": 870,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "report the updated results on the show",
      "offset": 871.68,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "when they come out. But holding aside",
      "offset": 873.12,
      "duration": 4.959
    },
    {
      "lang": "en",
      "text": "the specifics and trying to give credit",
      "offset": 875.76,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "where credit is due for what the study",
      "offset": 878.079,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "uncovered, I do believe that it does",
      "offset": 879.76,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "show that we need to think about this as",
      "offset": 882,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "a different type of work. As we shift",
      "offset": 884.8,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "the balance of quote unquote coding work",
      "offset": 887.44,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "away from actually typing and writing",
      "offset": 890.32,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "out code, new types of work are going to",
      "offset": 892.48,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "emerge. Things like debugging and",
      "offset": 894.959,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "checking results and new types of",
      "offset": 896.48,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "challenges such as social media time",
      "offset": 898.399,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "management are going to become even more",
      "offset": 900.72,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "significant. So if you are a company",
      "offset": 902.8,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "trying to understand these results, the",
      "offset": 905.36,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "worst possible takeaway is to say, \"Ah,",
      "offset": 908.24,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "see, it was all just overhyped. I guess",
      "offset": 911.12,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "we're just going to ignore those tools.\"",
      "offset": 912.88,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "The best takeaway is to understand that",
      "offset": 914.959,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "these productivity gains are not free.",
      "offset": 917.279,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "They come with a learning curve. They",
      "offset": 919.68,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "come with real work to reorganize the",
      "offset": 921.6,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "work. The faster you start and the more",
      "offset": 923.44,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "quickly you get to those serious hours",
      "offset": 925.839,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "of reps that seem to make a big",
      "offset": 928,
      "duration": 3.519
    },
    {
      "lang": "en",
      "text": "difference, the more likely to actually",
      "offset": 929.519,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "get this value you are. I don't know why",
      "offset": 931.519,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "people think it would be any different.",
      "offset": 934.399,
      "duration": 3.521
    },
    {
      "lang": "en",
      "text": "If you've ever tried to use any type of",
      "offset": 936.16,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "complex software in the past, whether",
      "offset": 937.92,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "it's Salesforce or Adobe Photoshop or",
      "offset": 939.519,
      "duration": 5.361
    },
    {
      "lang": "en",
      "text": "anything, you don't get mastery quickly,",
      "offset": 942.16,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "you don't even get competence quickly.",
      "offset": 944.88,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "Powerful tools, even agentic tools,",
      "offset": 946.959,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "require practice. And if anything, this",
      "offset": 948.8,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "study shows that we can't shortcut that",
      "offset": 950.88,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "step. But hey, man, look, if the goal is",
      "offset": 952.72,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "to generate a conversation, well done,",
      "offset": 954.8,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "because this has been a huge point of",
      "offset": 957.44,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "discussion for the entire AI engineering",
      "offset": 959.519,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "community and beyond, and that in",
      "offset": 961.519,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "general is almost always a good thing.",
      "offset": 963.199,
      "duration": 3.121
    },
    {
      "lang": "en",
      "text": "For now, that's going to do it for",
      "offset": 965.279,
      "duration": 2.961
    },
    {
      "lang": "en",
      "text": "today's AI daily brief. Thanks as always",
      "offset": 966.32,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "for listening or watching.",
      "offset": 968.24,
      "duration": 2.959
    }
  ],
  "cleanText": "Are AI coding tools actually slowing developers down? Welcome back to the AI Daily Brief. Today we are talking about a study that is getting an absolute ton of buzz. A group of developers were tested to see how much more productive AI coding tools would make them. They assumed going into the study that they would be about 2425% more productive, and even after the study concluded, thought that AI had made them 20% more productive. But the study actually found that they were 19% less productive, 19% slower on this set of coding tasks.\n\nNow, as you might imagine, this has been widely reported on outlets like CNBC, suggesting that it makes for a crack in the AI productivity bullcase. The implications of something like this are big. Billions and billions, if not trillions of dollars, are being spent assuming that AI is going to make us more productive. Does this all throw it into question?\n\nSomehow, my guess is at this point, if you are a regular listener to this show, you will virtually hear the cracking of my knuckles and neck as I prepare to critique this particular study. Now, I do want to caveat things. In general, I am always interested to see what this group comes out with. They were the team who developed the methodology that suggested that agent capabilities are doubling every 7 months. And so, I don't think that this is from some shoddy organization or anything like that. I just happen to disagree pretty fundamentally with at least one particular assumption that I think is fairly important to the study and even more than that the way that it's being reported.\n\nBut let's get into what it actually said before I get into my critique. Researchers from METR, which by the way, I don't even know if they call it METR, that's what I call it, MER, which is a nonprofit AI research firm, recently tested 16 developers with what they identified as moderate AI skills, something that we will come back to, across hundreds of tasks in which they had roughly 5 years of experience. Each task was randomly assigned to either allow or disallow AI usage. Before the test began, the programmers said that they believed that AI would reduce completion time by 24%. And after they finished, they believed that the AI had helped them get a 20% speed boost, but the actual results, as I mentioned, found that AI had actually slowed them down by 19%.\n\nThe study showed a wide range of results across different complexities of tasks. For tasks that take up to 1 hour, developers were basically the same speed whether they used AI or not. The same was true for extremely long tasks that took 7 or 8 hours. The only range where there was a big difference was in moderately complex tasks that take between 1 hour and 6 hours. The results were extremely consistent with AI assisted programmers slowing down as the task stretched to the 2-hour mark. AI and non-AI programming again converged as the task got even longer with very little gap once they reached 8 hours.\n\nThe study also used screen recordings to break down how the programmers used their time across AI and non-AI coding. When using AI, the researchers found the time spent actively coding, reading or searching, testing and debugging, and dealing with the git and environment all went down. Active coding and reading or searching saw the sharpest drops in time spent. Idle or overhead time was the only factor that went up when using AI and the difference was in time spent prompting, waiting for AI outputs and reviewing the generated code.\n\nNow, the researchers bundled the potential causes of the slowdown into five major categories. The first was simply that programmers were overly optimistic about AI usefulness. Second, they noted that some developers were too familiar with the codebase they were working on, so AI didn't have much to offer. Third, others were working on larger complex repositories where AI ran into context window limits. Fourth, and a big one, was low AI reliability. The programmers only accepted 44% of AI generations and spent 9% of their time cleaning up generated code. And fifth, and finally, the developers reported more generalized context issues where the AI didn't recognize the repository properly.\n\nNow, to their credit, the researchers here are not at all suggesting that we should throw the baby out with the bathwater. There are caveats up and down this thing, qualifications that try to not overstate the case. They said, for example, the slowdown we observe does not imply that current AI tools do not often improve developers productivity. We find evidence that the high developer familiarity with repositories and the size and maturity of the repositories both contribute to the observed slowdown and these factors do not apply in many software development settings.\n\nSo let's actually talk about what some of the challenges here could have been. First of all, I think it's completely correct to acknowledge that this is a different type of working. Coding with coding tools involves entirely new processes and entirely new emphasis in different types of work categories. One of the devs in the study, Quinton Anthony, actually talked about this. Regarding the idea of distractions, he writes, \"It's super easy to get distracted in the downtime while LLMs are generating. The social media attention economy is brutal, and I think people spend 30 minutes scrolling while quote unquote waiting for their 30-cond generation.\" All I can say on this one is that we should know our own pitfalls and try to fill this LLM generation time productively. If the task requires high focus, spend this time either working on a subtask or thinking about follow-up questions. Even if the model one shots your question, \"What else don't I understand?\" If the task requires low focus, do another small task in the meantime. As always, small digital hygiene steps helps with this.\n\nAnd holding aside any sort of focus on social media intrusions and distractions while you're waiting for a prompt to resolve, there also is inevitably just going to be a shift in the type of work that you have to do. Maybe you are writing less actual code, but you might spend more time debugging. That was part of what the researchers actually directly found. And so I think the summary of these two parts and something we'll come back to at the end of this is that coding with AI tools is not just the same as coding but faster. It is a new process that requires new thinking.\n\nNext, let's talk about the models that were used. Now, bad models might be an overstatement for the sake of space in an AI generated image here, but this study was conducted at the beginning of this year. And while that seems like a short time ago, all of the models that people use to code are much advanced from the ones that they were using in this study. Ruben Bloom, who works on Less Wrong, also participated in the study and said, \"As a developer in the study, it's striking to me how much more capable the models have gotten since February when I was participating. I'm trying to recall if I was even using agents at the start. Certainly, the later models, Opus 4, Gemini 2.5 Pro, 03 could do just vastly more with less guidance than 3601, etc. For me, not going over my own data in the study, I could buy that maybe I was being slowed down a few months ago, but it's much harder to believe now.\n\nNow, Ruben or Ruby, as he goes by, also did validate the other piece that we were just discussing about as well, saying, \"I feel like historically a lot of my AI speedup gains were eaten by the fact that while a prompt was running, I'd look at something else, Facebook X, etc., and continue to do so for much longer than it took the prompt to run. I discovered 2 days ago that Cursor has or now has a feature you can enable to ring a bell when the prompt is done. I expect to reclaim a lot of AI gains this way. Point being that while 3.5 and 3.7 sonnet aren't bad models contrary to my image here, they are certainly less performant than all the tools we have now. Right? This is before Claude code. This is before 03. This is before 2.5.\n\nA fourth category that is incredibly important and is the one acknowledged most by the authors is the codebase context. Remember the authors wrote, \"We find evidence that the high developer familiarity with repositories and the size and maturity of the repositories both contributed to the observed slowdown and these factors do not apply in many software development settings.\" Fellow AI podcaster Nathan Leb put it more simply, \"Expert developers working in large code bases is known to be the setting where AI can help least.\" Both of these factors matter. The fact that they're working in large code bases and that they are experts in those code bases is, as Nathan points out, something of a mismatched use case for some of these AI coding tools, which does not mean at all, by the way, that it's not valuable to study them. What it means, and this is something that's going to run throughout this analysis, is that it's very difficult to draw general conclusions across the entire field of software developers based on these 16 that were studied. If you want to be generous to the researchers, that's not exactly what they're trying to do. But when you put out a study like this, you know it's going to get amplified.\n\nNow, Nathan though points out that there is another piece here that the fact that it's known that this isn't the best use case for AI coding meant that the participants didn't have as much AI coding experience coming in. And as he points out, not wrongly, given the work they do. And this gets us to the biggest debate, which is about learning curves and how to designate this set of developers when it comes to their AI experience. This is where some of the loudest disagreement comes in and where I have some of my biggest issues. Now, I am not alone in this. In fact, perhaps the loudest critique of this paper has come from Emmett Shear. Emmett was a co-founder at Twitch and spent a very hectic weekend as the CEO of OpenAI when Sam Altman was deposed. He tweeted, \"Metter's analysis of this experiment is wildly misleading. The results indicate that people who have approximately never used AI tools before are less productive while learning to use the tools and says nothing about experienced AI tool users.\n\nEmmett continues, I immediately found the claim suspect because it didn't jive with my own experience working with people who were using coding assistants, but sometimes there are surprising results, so I dug in. The first question, who were these developers in the study getting such poor results? He then quoted from the methodology. We recruited 16 experienced open source developers to work on 246 real tasks in their own repositories. So Emmett writes, \"They sound like reasonably experienced software devs. Back to the study, developers have a range of experience using AI tools. 93% have prior experience with tools like chatbt, but only 44% have experience using Cursor.\" Uh-oh, writes Emmett. So they haven't actually used AI coding tools. They've like tried prompting an LLM to write code for them. But that's an entirely different kind of experience as anyone who has used these tools can tell you. They claim a range of experience using AI tools. Yet only a single developer of their 16 had more than a single week of experience using Cursor. They make it look like a range by breaking less than a week into under 1 hour, 1 to 10 hours, 10 to 30 hours, and 30 to 50 hours of experience. Given the steep learning curve for effectively using these AI tools, well, this division betrays what I hope is just grossly negligent ignorance about the reality rather than intentional deception. Of course, the one developer who did have more than one week of experience was 20% faster instead of 20% slower. The authors note this fact, but then say, \"We are underpowered to draw strong conclusions from this analysis and bury it in a figure's description in an appendex.\" If the authors of the paper had made the claim, \"We tested experienced developers using AI tools for the first time and found that at least during the first week, they were slower rather than faster,\" that would have been a moderately interesting finding and true. Alas, that is not the claim they made.\n\nNow, David Rean, one of the researchers, stood behind the methodology, responding, \"Devs had roughly the following prior LLM experience. Seven out of the 16 had over hundreds of hours, seven of the 16 had 10 to 100 hours, and two of the 16 had 1 to 10 hours. We think describing this as moderate AI experience is fair. Now, in the thread, David said, \"My guess is we'll have to agree to disagree.\" And respectfully, I firmly firmly disagree here. First of all, using chatbt even to code is not the same as using a dedicated agentic IDE. Second, this is not a significant period of time when it comes to tool use. 40 hours, one work week is not a moderate amount of time to use a new tool, especially when we were just discussing the fact that it involves totally new patterns of working.\n\nEmmett again wrote, \"It's clear that the source of disagreement is that I think using Cursor effectively is a distinct skill from talking to GDP while you program and expect fairly low transfer. And the authors think it's the similar skill and expect much higher.\" When Megan Kinnman from Matter pointed out that devs whose primary IDE was Cursor before the experiment were also slowed down on average, although by less than the average in the study, developer Tyler John pointed out this is useful, but there's only three of them. And it sounds like the most experienced one was dramatically sped up. I think a study with experienced Cursor users is warranted to test the hypothesis.\n\nNow, it's not just me and Emmett and a handful of Twitter commenters who are having the same response. AI programmer Simon Willis shared his thoughts, writing, \"My personal theory is that getting a significant productivity boost from LLM assistants and AI tools has a much steeper learning curve than most people expect. We see positive speed up for the one developer who has more than 50 hours of Cursor experience. So, it's plausible that there is a high skill ceiling for using Cursor such that developers with significant experience see positive speed up. My intuition here is that the study mainly demonstrates that the learning curve on AI assisted development is high enough that asking developers to bake it into their existing workflows reduces their performance while they climb that learning curve. And part of why Emmett is so frustrated here is something which while outside of Met's control he believes effectively that they should have anticipated which is how the mainstream media is going to amplify these results. Again, I mentioned the headline is study finds AI tools made open source software developers 19% slower. All over Twitter/X, there are graphics like this one from TechJuice. Shocking studies suggest AI coding tools are slowing veteran developers by 19%. And then there's the mainstream media. Tech giants like Microsoft and Google are outsourcing more and more coding to AI in a productivity push, but some new research shows the tools might not be as helpful as some expect. These are stories that have the ability to impact markets in significant ways despite the fact that there are all these questions. Now, it is an entirely different\n\n\nEpisode on what researchers find their responsibility to be when it comes to the potential for amplification by mainstream media, given how unbelievably politicized AI is and will continue to be. Perhaps there is a higher burden there, but like I said, that's sort of the subject for a different show. The TLDDR for me is not that I think that the study isn't useful. It's that I don't think ultimately that it's saying what the researchers think it's saying. I think it's much closer to what Emmett Shear argued that a specific type of developer working on a specific type of codebase with a specific limited experience set with this particular set of tools encountered all sorts of issues that made them temporarily slower than had they not been using the tools.\n\nSo where does that leave us? Well, from a research perspective, there are obvious needs for follow-ups here. I think having developers working on different types of code bases with different levels of experience and specifically those who have actually worked more deeply with Cursor or any other agentic IDE would be a really valuable follow-up. At the same time, as Simon Willis points out, measuring developer productivity is notoriously difficult. So even with that, we're still going to have to take everything with a grain of salt. And by the way, to their credit, it appears that Meta is actually thinking about expanding this study, and I hope that they do. We'll certainly report the updated results on the show when they come out. But holding aside the specifics and trying to give credit where credit is due for what the study uncovered, I do believe that it does show that we need to think about this as a different type of work. As we shift the balance of quote unquote coding work away from actually typing and writing out code, new types of work are going to emerge. Things like debugging and checking results and new types of challenges such as social media time management are going to become even more significant.\n\nSo if you are a company trying to understand these results, the worst possible takeaway is to say, \"Ah, see, it was all just overhyped. I guess we're just going to ignore those tools.\" The best takeaway is to understand that these productivity gains are not free. They come with a learning curve. They come with real work to reorganize the work. The faster you start and the more quickly you get to those serious hours of reps that seem to make a big difference, the more likely to actually get this value you are. I don't know why people think it would be any different. If you've ever tried to use any type of complex software in the past, whether it's Salesforce or Adobe Photoshop or anything, you don't get mastery quickly, you don't even get competence quickly. Powerful tools, even agentic tools, require practice. And if anything, this study shows that we can't shortcut that step. But hey, man, look, if the goal is to generate a conversation, well done, because this has been a huge point of discussion for the entire AI engineering community and beyond, and that in general is almost always a good thing.\n\nFor now, that's going to do it for today's AI Daily Brief. Thanks as always for listening or watching.\n",
  "dumpedAt": "2025-07-21T18:43:25.379Z"
}