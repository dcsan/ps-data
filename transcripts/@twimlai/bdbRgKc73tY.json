{
  "episodeId": "bdbRgKc73tY",
  "channelSlug": "@twimlai",
  "title": "Distilling Transformers and Diffusion Models for Robust Edge Use Cases [Fatih Porikli] - 738",
  "publishedAt": "2025-07-09T16:16:06.000Z",
  "rawLines": [
    {
      "lang": "en",
      "text": "The word knowledge representation",
      "offset": 0.4,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "capability of LLMs act as a regularizer",
      "offset": 2.8,
      "duration": 6.96
    },
    {
      "lang": "en",
      "text": "or conditioner to generalize the",
      "offset": 6.56,
      "duration": 6.079
    },
    {
      "lang": "en",
      "text": "solution. So we don't need to really go",
      "offset": 9.76,
      "duration": 8.24
    },
    {
      "lang": "en",
      "text": "every time for each long tail specific",
      "offset": 12.639,
      "duration": 9.121
    },
    {
      "lang": "en",
      "text": "very rare scenario and try to learn them",
      "offset": 18,
      "duration": 7.68
    },
    {
      "lang": "en",
      "text": "model them. We want to harness the LLM's",
      "offset": 21.76,
      "duration": 6.4
    },
    {
      "lang": "en",
      "text": "word knowledge with the efficiency of",
      "offset": 25.68,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "this you know vision based low-level",
      "offset": 28.16,
      "duration": 6.439
    },
    {
      "lang": "en",
      "text": "perception st.",
      "offset": 30.72,
      "duration": 3.879
    },
    {
      "lang": "en",
      "text": "All right everyone welcome to another",
      "offset": 45.12,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "episode of the Twinball AI podcast. I am",
      "offset": 46.879,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "of course your host Sam Charington.",
      "offset": 49.76,
      "duration": 5.439
    },
    {
      "lang": "en",
      "text": "Today I'm joined by Fati Puriki. Fati is",
      "offset": 52.079,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "senior director of technology at",
      "offset": 55.199,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "Qualcomm. Before we get going, be sure",
      "offset": 57.28,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "to take a moment to hit that subscribe",
      "offset": 59.6,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "button wherever you're listening to",
      "offset": 61.199,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "today's show. Fati, welcome back to the",
      "offset": 62.96,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "podcast. You're somewhat of a veteran",
      "offset": 65.92,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "with us.",
      "offset": 68.08,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "Thank you so much, Sam. I I really love",
      "offset": 68.72,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "this podcast and I'm I'm very happy to",
      "offset": 71.439,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "be back.",
      "offset": 74.4,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "I'm looking forward to our chat. It's",
      "offset": 75.439,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "been just about a year since we last",
      "offset": 78.159,
      "duration": 6.481
    },
    {
      "lang": "en",
      "text": "spoke and we are back once again to dig",
      "offset": 80.799,
      "duration": 7.041
    },
    {
      "lang": "en",
      "text": "into Qualcomm's papers from this year's",
      "offset": 84.64,
      "duration": 6.159
    },
    {
      "lang": "en",
      "text": "CVPR conference and in particular we'll",
      "offset": 87.84,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "be digging into a couple of them. uh the",
      "offset": 90.799,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "DIMA paper which looks at distilling",
      "offset": 93.68,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "multimodal LLMs in an autonomous driving",
      "offset": 96.159,
      "duration": 6.801
    },
    {
      "lang": "en",
      "text": "context as well as the sharp depth paper",
      "offset": 99.36,
      "duration": 6.079
    },
    {
      "lang": "en",
      "text": "which looks at diffusion distillation",
      "offset": 102.96,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "for computing absolute depth maps and",
      "offset": 105.439,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "some of the new capabilities that that",
      "offset": 107.92,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "unlocks which kind of speaks to",
      "offset": 109.6,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "multimodal foundation models. Of course,",
      "offset": 111.92,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "another unifying thread that is often",
      "offset": 114.079,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "present in my conversations with you and",
      "offset": 117.119,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "your colleagues at Qualcomm AI research",
      "offset": 120.159,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "is the idea of efficiency and",
      "offset": 122.399,
      "duration": 6.161
    },
    {
      "lang": "en",
      "text": "distillation. Um, and that's a key theme",
      "offset": 125.119,
      "duration": 6.721
    },
    {
      "lang": "en",
      "text": "in these papers. But, uh, I guess let's",
      "offset": 128.56,
      "duration": 6.399
    },
    {
      "lang": "en",
      "text": "jump in and and talk through the DIMA",
      "offset": 131.84,
      "duration": 5.119
    },
    {
      "lang": "en",
      "text": "paper. The the full title of that paper",
      "offset": 134.959,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "is distilling multimodal large language",
      "offset": 136.959,
      "duration": 5.761
    },
    {
      "lang": "en",
      "text": "models for autonomous driving. Maybe",
      "offset": 139.2,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "let's start with kind of your broad take",
      "offset": 142.72,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "on the state-of-the-art for autonomous",
      "offset": 145.68,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "driving and in particular the shift",
      "offset": 148.72,
      "duration": 6.4
    },
    {
      "lang": "en",
      "text": "towards endtoend autonomous uh vehicular",
      "offset": 150.8,
      "duration": 7.439
    },
    {
      "lang": "en",
      "text": "systems. IMA is about autonomous driving",
      "offset": 155.12,
      "duration": 6.56
    },
    {
      "lang": "en",
      "text": "as you just mentioned and specifically",
      "offset": 158.239,
      "duration": 7.041
    },
    {
      "lang": "en",
      "text": "it aims to provide safe motion planning",
      "offset": 161.68,
      "duration": 7.04
    },
    {
      "lang": "en",
      "text": "in longtail scenarios rare events and",
      "offset": 165.28,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "those rare events are very critical",
      "offset": 168.72,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "events because those represents",
      "offset": 170.319,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "accidents scenarios as well. So we don't",
      "offset": 172.64,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "want them to have right previous trend",
      "offset": 175.599,
      "duration": 5.761
    },
    {
      "lang": "en",
      "text": "it was mostly modular systems they",
      "offset": 178.4,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "sequentially and independently trained",
      "offset": 181.36,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "models components and the reason for",
      "offset": 184.4,
      "duration": 6.8
    },
    {
      "lang": "en",
      "text": "that there are kind of smaller data sets",
      "offset": 187.36,
      "duration": 6.4
    },
    {
      "lang": "en",
      "text": "and for instance for object detection",
      "offset": 191.2,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "vehicle detection lane detection lane",
      "offset": 193.76,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "marking detection road detection road",
      "offset": 196.72,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "segmentation so there are small data",
      "offset": 198.72,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "sets and it's easy to maybe train such",
      "offset": 200.959,
      "duration": 6.161
    },
    {
      "lang": "en",
      "text": "component then you know kind of build a",
      "offset": 204.08,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "larger system. But when we do that it is",
      "offset": 207.12,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "you know kind of easy to train such",
      "offset": 210.56,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "things quickly. But uh since we are now",
      "offset": 212.64,
      "duration": 7.28
    },
    {
      "lang": "en",
      "text": "focusing at each module uh their own",
      "offset": 216.4,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "performance uh goals for better",
      "offset": 219.92,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "segmentation, better detection, better",
      "offset": 222.56,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "tracking. It doesn't mean that at the",
      "offset": 224.56,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "end you know all together they will work",
      "offset": 227.12,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "in harmony and our goal is to drive safe",
      "offset": 229.28,
      "duration": 7.28
    },
    {
      "lang": "en",
      "text": "right drive our destination I mean when",
      "offset": 233.04,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "we are driving in our brain we don't",
      "offset": 236.56,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "really segment the things detect the",
      "offset": 238.48,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "things or train out track them",
      "offset": 240.319,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "explicitly but so this is the",
      "offset": 243.04,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "shortcoming of such uh approaches and",
      "offset": 245.28,
      "duration": 7.039
    },
    {
      "lang": "en",
      "text": "end to end now is a breakthrough in a",
      "offset": 248.72,
      "duration": 7.12
    },
    {
      "lang": "en",
      "text": "way that training objective applies to",
      "offset": 252.319,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "all components at the same time. So we",
      "offset": 255.84,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "are not really focusing on test",
      "offset": 258.479,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "specifically trained modules on limited",
      "offset": 260.56,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "data set but now you know kind of we are",
      "offset": 263.04,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "trying to optimize everything all the",
      "offset": 265.52,
      "duration": 6.239
    },
    {
      "lang": "en",
      "text": "processing with the end goal in mind",
      "offset": 268.56,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "that's why it is called end to end",
      "offset": 271.759,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "frequent listeners to the podcast will",
      "offset": 274.08,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "know that this idea of you know",
      "offset": 276.56,
      "duration": 6.079
    },
    {
      "lang": "en",
      "text": "physics-based versus modelbased or",
      "offset": 280,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "modular versus end to end is kind of a",
      "offset": 282.639,
      "duration": 5.681
    },
    {
      "lang": "en",
      "text": "battle that's been raging in the uh",
      "offset": 285.6,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "machine learning community and the these",
      "offset": 288.32,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "various communities like autonomous",
      "offset": 290.32,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "driving and robotics for quite some",
      "offset": 291.84,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "time. We've been talking about related",
      "offset": 294.4,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "themes for many years here on the",
      "offset": 296.72,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "podcast. There's a big reason for that",
      "offset": 298.88,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "because end to end systems doesn't only",
      "offset": 301.52,
      "duration": 7.2
    },
    {
      "lang": "en",
      "text": "offer better more robust solutions for",
      "offset": 304.56,
      "duration": 6.079
    },
    {
      "lang": "en",
      "text": "you know wide variety of longtail",
      "offset": 308.72,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "scenarios but also uh they are uh better",
      "offset": 310.639,
      "duration": 6.321
    },
    {
      "lang": "en",
      "text": "in terms of interpretability and they",
      "offset": 314,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "their KPIs you know kind of are much",
      "offset": 316.96,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "better. So DIMA is an end to end",
      "offset": 319.6,
      "duration": 6.8
    },
    {
      "lang": "en",
      "text": "solution. Um, it establishes the new",
      "offset": 322.72,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "state-of-the-art, literally 25",
      "offset": 326.4,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "state-of-the-art for autonomous driving",
      "offset": 328.88,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "and AI. Can I pause you there because",
      "offset": 331.199,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "you said something that",
      "offset": 333.12,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "um when I think about modular versus end",
      "offset": 335.36,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "to end or when I reflect on the",
      "offset": 338.4,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "conversations that I've had with folks",
      "offset": 340.16,
      "duration": 7.84
    },
    {
      "lang": "en",
      "text": "uh in which this comes up the",
      "offset": 342.4,
      "duration": 9.359
    },
    {
      "lang": "en",
      "text": "it's usually positioned as given enough",
      "offset": 348,
      "duration": 5.919
    },
    {
      "lang": "en",
      "text": "data we think it will perform better",
      "offset": 351.759,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "because we're optimizing against the",
      "offset": 353.919,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "thing that we really care about. But",
      "offset": 356.08,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "modular is still useful because of",
      "offset": 358.16,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "explanability and interpretability",
      "offset": 360.96,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "because you can look at you know the",
      "offset": 362.88,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "output of each of these modules and use",
      "offset": 364.56,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "that to get some intuition about why",
      "offset": 367.28,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "decisions are happening at the planning",
      "offset": 370,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "layer. But you just said that end to end",
      "offset": 371.759,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "system has some interpretability",
      "offset": 374.639,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "advantages. Can you explain that?",
      "offset": 376.639,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "That interpretability is more the one",
      "offset": 378.56,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "that I implied is semantic",
      "offset": 381.199,
      "duration": 5.681
    },
    {
      "lang": "en",
      "text": "interpretability. For instance, we have",
      "offset": 384.16,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "AI planner. There is a planner at the",
      "offset": 386.88,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "end, right? We do all uh such low-level",
      "offset": 389.039,
      "duration": 6.481
    },
    {
      "lang": "en",
      "text": "perception things that I mentioned to",
      "offset": 391.919,
      "duration": 6.881
    },
    {
      "lang": "en",
      "text": "make a decision to whether accelerate,",
      "offset": 395.52,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "slow down, change lanes, you know, or",
      "offset": 398.8,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "turn those type of things. And when you",
      "offset": 400.8,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "are sitting in a vehicle you know kind",
      "offset": 403.44,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "of yes uh like maybe modular system can",
      "offset": 405.52,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "tell you how many people are in the",
      "offset": 408.4,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "scene or you know kind of what are the",
      "offset": 410.72,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "locations through the positions of the",
      "offset": 412.96,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "kind of other vehicles but kind of when",
      "offset": 415.199,
      "duration": 5.921
    },
    {
      "lang": "en",
      "text": "it is it decides to change the lane you",
      "offset": 417.52,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "know you don't know why it's happening",
      "offset": 421.12,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "it is very stressful experience if you",
      "offset": 423.52,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "do not know why vehicle is making such",
      "offset": 426.16,
      "duration": 7.039
    },
    {
      "lang": "en",
      "text": "decisions. So advanced systems and dimma",
      "offset": 429.52,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "in particular the way that we kind of",
      "offset": 433.199,
      "duration": 5.041
    },
    {
      "lang": "en",
      "text": "like incorporated the language model",
      "offset": 435.28,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "into overall system provides",
      "offset": 438.24,
      "duration": 5.679
    },
    {
      "lang": "en",
      "text": "explanation. Look I'm slowing down",
      "offset": 441.52,
      "duration": 5.519
    },
    {
      "lang": "en",
      "text": "because there is a congestion ahead or",
      "offset": 443.919,
      "duration": 5.521
    },
    {
      "lang": "en",
      "text": "we are approaching a zebra crossing",
      "offset": 447.039,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "those type of things. So analogous to",
      "offset": 449.44,
      "duration": 5.759
    },
    {
      "lang": "en",
      "text": "explaining uh thought traces in a",
      "offset": 452.319,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "reasoning model the the model can to",
      "offset": 455.199,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "some degree explain what it's doing.",
      "offset": 458.319,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "Yeah. High level explanation useful",
      "offset": 460.639,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "explanations. But you are right also you",
      "offset": 462.96,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "know kind of like the modular system uh",
      "offset": 464.88,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "we know kind of their detection results",
      "offset": 467.36,
      "duration": 5.839
    },
    {
      "lang": "en",
      "text": "for for instance the thin advanced",
      "offset": 470.4,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "system sometimes they do not need to you",
      "offset": 473.199,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "know reveal that because maybe that is",
      "offset": 476.08,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "not critical at that point",
      "offset": 478.639,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "and that's why I described it as a bit",
      "offset": 480.96,
      "duration": 5.639
    },
    {
      "lang": "en",
      "text": "of a battle uh",
      "offset": 482.639,
      "duration": 3.96
    },
    {
      "lang": "en",
      "text": "okay",
      "offset": 487.52,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "yeah I'm trying to be like a judge but",
      "offset": 488.479,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "you know honestly I'm more",
      "offset": 490.8,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "kind of supporting at the end at this",
      "offset": 493.36,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "point.",
      "offset": 495.44,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "Yeah. All right. Uh so sorry I",
      "offset": 496.08,
      "duration": 6.239
    },
    {
      "lang": "en",
      "text": "interrupted you. You were talking about",
      "offset": 499.44,
      "duration": 6.159
    },
    {
      "lang": "en",
      "text": "the I was talking about Dimma's kind of",
      "offset": 502.319,
      "duration": 8.241
    },
    {
      "lang": "en",
      "text": "position advantages. So for dimma uh it",
      "offset": 505.599,
      "duration": 6.32
    },
    {
      "lang": "en",
      "text": "I mentioned it is the new",
      "offset": 510.56,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "state-of-the-art. It can reduce the",
      "offset": 511.919,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "collision rate which is a very important",
      "offset": 514.88,
      "duration": 5.839
    },
    {
      "lang": "en",
      "text": "KPI for autonomous driving system 80%.",
      "offset": 516.959,
      "duration": 7.2
    },
    {
      "lang": "en",
      "text": "with respect to the 2024",
      "offset": 520.719,
      "duration": 8.8
    },
    {
      "lang": "en",
      "text": "or latest 25 baseline uh B sort solution",
      "offset": 524.159,
      "duration": 8.961
    },
    {
      "lang": "en",
      "text": "um so it is a big improvement and also",
      "offset": 529.519,
      "duration": 6.801
    },
    {
      "lang": "en",
      "text": "40% reduction in for instance waypoint",
      "offset": 533.12,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "trajectory estimation another KPI uh",
      "offset": 536.32,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "let's say I know how vehicles should",
      "offset": 539.12,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "drive how close this planner making",
      "offset": 541.36,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "better decision so that is also you know",
      "offset": 544.08,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "kind of a 40% reduction in the error",
      "offset": 546.56,
      "duration": 7.44
    },
    {
      "lang": "en",
      "text": "rate so and more than 40% improvement in",
      "offset": 549.04,
      "duration": 8.08
    },
    {
      "lang": "en",
      "text": "accuracy for long tail. So uh on in the",
      "offset": 554,
      "duration": 6.399
    },
    {
      "lang": "en",
      "text": "paper we show uh it's a long paper uh",
      "offset": 557.12,
      "duration": 6.56
    },
    {
      "lang": "en",
      "text": "and we have many results comparisons and",
      "offset": 560.399,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "uh uh you know kind of I invite everyone",
      "offset": 563.68,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "to take a look at that. So it says the",
      "offset": 565.839,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "new sort we compared with the previous",
      "offset": 568,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "sort including BAD vectorized autonomous",
      "offset": 570.64,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "driving which is an amazing paper also",
      "offset": 573.76,
      "duration": 5.519
    },
    {
      "lang": "en",
      "text": "and uh this NEMA can also do the things",
      "offset": 576.32,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "that there are in the training data for",
      "offset": 579.279,
      "duration": 5.361
    },
    {
      "lang": "en",
      "text": "instance there was a zero shot scenario",
      "offset": 581.6,
      "duration": 5.919
    },
    {
      "lang": "en",
      "text": "about three point turn we didn't have",
      "offset": 584.64,
      "duration": 7.52
    },
    {
      "lang": "en",
      "text": "any such example for but then when we",
      "offset": 587.519,
      "duration": 8.88
    },
    {
      "lang": "en",
      "text": "gave it for testing such examples it was",
      "offset": 592.16,
      "duration": 6.88
    },
    {
      "lang": "en",
      "text": "uh capable of you know kind of doing the",
      "offset": 596.399,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "right planning and then I mentioned that",
      "offset": 599.04,
      "duration": 5.359
    },
    {
      "lang": "en",
      "text": "it can answer questions or the system",
      "offset": 601.6,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "can proactively you know kind of system",
      "offset": 604.399,
      "duration": 5.521
    },
    {
      "lang": "en",
      "text": "prompted and then provide explanations",
      "offset": 607.04,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "while it is driving itself you know it",
      "offset": 609.92,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "can tell you why it is making such",
      "offset": 611.839,
      "duration": 5.921
    },
    {
      "lang": "en",
      "text": "decision so overall it is know there's a",
      "offset": 614.16,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "low-level perception state there's an",
      "offset": 617.76,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "end to end planner AI planner state they",
      "offset": 619.76,
      "duration": 6.56
    },
    {
      "lang": "en",
      "text": "work together it integrates this all the",
      "offset": 622.56,
      "duration": 7.12
    },
    {
      "lang": "en",
      "text": "perception coming from the cameras and",
      "offset": 626.32,
      "duration": 7.28
    },
    {
      "lang": "en",
      "text": "other sensors uh the vision based front",
      "offset": 629.68,
      "duration": 6.8
    },
    {
      "lang": "en",
      "text": "end lowlevel perception into this",
      "offset": 633.6,
      "duration": 6.239
    },
    {
      "lang": "en",
      "text": "language model LLM based you know kind",
      "offset": 636.48,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "of AI planner this improves",
      "offset": 639.839,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "generalizability it improves the",
      "offset": 642,
      "duration": 5.279
    },
    {
      "lang": "en",
      "text": "robustness it is now the explanations",
      "offset": 644.48,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "are semantically grounded and also we",
      "offset": 647.279,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "have two versions you know we can run",
      "offset": 649.92,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "this uh uh AI planner a",
      "offset": 652.16,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "transformer-based model still leveraging",
      "offset": 654.72,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "the language model or language model",
      "offset": 656.72,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "itself you know as the you know overall",
      "offset": 659.519,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "AI planner. So both of them are in the",
      "offset": 662.48,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "paper. Both of them are better than the",
      "offset": 664.959,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "you know existing sort of",
      "offset": 666.8,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "is the transformerbased model the",
      "offset": 668.8,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "distilled model",
      "offset": 671.76,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "that is the distilled model right we are",
      "offset": 672.88,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "using model to distill it. Yeah. When I",
      "offset": 675.04,
      "duration": 7.039
    },
    {
      "lang": "en",
      "text": "think about the idea of using LLMs in an",
      "offset": 677.6,
      "duration": 8.32
    },
    {
      "lang": "en",
      "text": "autonomous driving context, the you know",
      "offset": 682.079,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "the elephant in the room if you will is",
      "offset": 685.92,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "that these things are slow right",
      "offset": 687.839,
      "duration": 7.201
    },
    {
      "lang": "en",
      "text": "inference for LLMs is very expensive and",
      "offset": 689.68,
      "duration": 8
    },
    {
      "lang": "en",
      "text": "um I'm presuming that's where the",
      "offset": 695.04,
      "duration": 5.359
    },
    {
      "lang": "en",
      "text": "distillation in dimma comes in you know",
      "offset": 697.68,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "kind of",
      "offset": 700.399,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "yes absolutely you have a very good",
      "offset": 703.36,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "point lance At this uh point of course",
      "offset": 705.519,
      "duration": 5.521
    },
    {
      "lang": "en",
      "text": "we are talking about the systems that we",
      "offset": 709.04,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "can run on the vehicle not like on a",
      "offset": 711.04,
      "duration": 6.64
    },
    {
      "lang": "en",
      "text": "very expensive H100 you know uh GPU that",
      "offset": 713.68,
      "duration": 5.599
    },
    {
      "lang": "en",
      "text": "would be more expensive than the vehicle",
      "offset": 717.68,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "itself. So uh using this not um",
      "offset": 719.279,
      "duration": 7.12
    },
    {
      "lang": "en",
      "text": "reasonable price accelerators",
      "offset": 723.2,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "uh the token rates are you know not that",
      "offset": 726.399,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "high maybe I can say that depending on",
      "offset": 729.36,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "the model size of the model anywhere",
      "offset": 731.12,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "from 30 token per second to maybe a",
      "offset": 733.519,
      "duration": 6.961
    },
    {
      "lang": "en",
      "text": "thousand token per second but running",
      "offset": 737.2,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "everything end to end using an LLM would",
      "offset": 740.48,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "be much more comput intensive than the",
      "offset": 742.8,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "transformer model that's why we have the",
      "offset": 746.48,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "transformer models We are saying well if",
      "offset": 748.48,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "you are interested in efficiency here is",
      "offset": 750.88,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "a transformer model still leverage this",
      "offset": 753.04,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "tilt using the language model but there",
      "offset": 755.2,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "is also a language model version of it a",
      "offset": 756.959,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "multimodel version of it you are right",
      "offset": 759.6,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "let me take a step back and kind of",
      "offset": 761.839,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "recap where we are so uh we've got this",
      "offset": 763.519,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "autonomous driving problem we're moving",
      "offset": 766,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "quickly towards autonomous driving",
      "offset": 768.079,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "solutions a promising",
      "offset": 770.24,
      "duration": 7.2
    },
    {
      "lang": "en",
      "text": "direction for autonomous driving is to",
      "offset": 773.839,
      "duration": 7.361
    },
    {
      "lang": "en",
      "text": "introduce LLMs into the autonomous",
      "offset": 777.44,
      "duration": 6.88
    },
    {
      "lang": "en",
      "text": "driving loop. Why even",
      "offset": 781.2,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "LLM given? We, you know, we kind of",
      "offset": 784.32,
      "duration": 2.959
    },
    {
      "lang": "en",
      "text": "jumped ahead and talked a little bit",
      "offset": 786,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "about the challenges or the cost of LLM.",
      "offset": 787.279,
      "duration": 5.281
    },
    {
      "lang": "en",
      "text": "Like what is the role of L of an LLM in",
      "offset": 789.36,
      "duration": 7.12
    },
    {
      "lang": "en",
      "text": "this loop and why do we even want it?",
      "offset": 792.56,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "Like when I think about when I'm",
      "offset": 796.48,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "driving, I'm not verbalizing questions",
      "offset": 797.76,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "to myself about what I'm seeing or what",
      "offset": 800.56,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "I'm doing. Why do we think an LLM should",
      "offset": 802.72,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "even be part of uh an AV system?",
      "offset": 805.04,
      "duration": 6.479
    },
    {
      "lang": "en",
      "text": "Yeah, that's that's a good question. We",
      "offset": 809.2,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "were also asking the question this",
      "offset": 811.519,
      "duration": 5.041
    },
    {
      "lang": "en",
      "text": "question to ourselves. And the reason we",
      "offset": 813.92,
      "duration": 6.64
    },
    {
      "lang": "en",
      "text": "want to use LLMs is that they represent",
      "offset": 816.56,
      "duration": 7.44
    },
    {
      "lang": "en",
      "text": "world knowledge many things. So they",
      "offset": 820.56,
      "duration": 6.56
    },
    {
      "lang": "en",
      "text": "capture all the knowledge or most of the",
      "offset": 824,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "knowledge. Yes, they hallucinate at the",
      "offset": 827.12,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "end when they are generating responses,",
      "offset": 829.36,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "but they are much better than the things",
      "offset": 831.04,
      "duration": 6.64
    },
    {
      "lang": "en",
      "text": "that we manually crafted. So there is",
      "offset": 834.16,
      "duration": 6.64
    },
    {
      "lang": "en",
      "text": "something there that would make it",
      "offset": 837.68,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "richer. The word knowledge",
      "offset": 840.8,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "representation capability of LLMs",
      "offset": 843.36,
      "duration": 6.4
    },
    {
      "lang": "en",
      "text": "act as a regularizer or you know",
      "offset": 846.56,
      "duration": 6.639
    },
    {
      "lang": "en",
      "text": "conditioner uh to",
      "offset": 849.76,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "generalize the solution. So we don't",
      "offset": 853.199,
      "duration": 5.841
    },
    {
      "lang": "en",
      "text": "need to really go every time for each uh",
      "offset": 855.12,
      "duration": 7.12
    },
    {
      "lang": "en",
      "text": "long tail specific",
      "offset": 859.04,
      "duration": 7.039
    },
    {
      "lang": "en",
      "text": "very rare scenario and try to learn them",
      "offset": 862.24,
      "duration": 8.24
    },
    {
      "lang": "en",
      "text": "model them. We rely we want to take",
      "offset": 866.079,
      "duration": 6.961
    },
    {
      "lang": "en",
      "text": "advantage of this word knowledge of the",
      "offset": 870.48,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "LLMs to do it automatically. So that's",
      "offset": 873.04,
      "duration": 6.159
    },
    {
      "lang": "en",
      "text": "why we want to harness the LLM's word",
      "offset": 875.92,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "knowledge with the efficiency of this",
      "offset": 879.199,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "you know vision based low-level",
      "offset": 881.76,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "perception stick and so is that world",
      "offset": 883.92,
      "duration": 6.96
    },
    {
      "lang": "en",
      "text": "knowledge access via traditional",
      "offset": 887.12,
      "duration": 6.079
    },
    {
      "lang": "en",
      "text": "prompting or are we doing talking about",
      "offset": 890.88,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "doing something like taking the",
      "offset": 893.199,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "embeddings of an LLM and somehow",
      "offset": 895.199,
      "duration": 6.721
    },
    {
      "lang": "en",
      "text": "directly accessing them uh to to unlock",
      "offset": 897.6,
      "duration": 6.72
    },
    {
      "lang": "en",
      "text": "this world knowledge. By the way, Dimma",
      "offset": 901.92,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "is not the first advanced solution.",
      "offset": 904.32,
      "duration": 5.519
    },
    {
      "lang": "en",
      "text": "There is Uniad and VA. I mean, it's, you",
      "offset": 906.56,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "know, let's say big brothers, but we do,",
      "offset": 909.839,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "I think, better than them by",
      "offset": 912.8,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "incorporating lamps there. So, those",
      "offset": 914.399,
      "duration": 6.8
    },
    {
      "lang": "en",
      "text": "models they try to represent the scene",
      "offset": 917.199,
      "duration": 7.041
    },
    {
      "lang": "en",
      "text": "in terms of vectors and replacing this,",
      "offset": 921.199,
      "duration": 5.281
    },
    {
      "lang": "en",
      "text": "you know, kind of dense representations",
      "offset": 924.24,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "to reduce the cost and also, you know,",
      "offset": 926.48,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "kind of like provide better improved",
      "offset": 929.279,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "capability. So there is this thing",
      "offset": 931.68,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "called as scene representation and I",
      "offset": 933.92,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "will kind of we can also consider it as",
      "offset": 936.72,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "tokenizing",
      "offset": 939.12,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "tokens are very common term for language",
      "offset": 940.72,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "models lms so what goes into those",
      "offset": 943.519,
      "duration": 5.841
    },
    {
      "lang": "en",
      "text": "models separately let's kind of think",
      "offset": 946.32,
      "duration": 7.28
    },
    {
      "lang": "en",
      "text": "them as tokens and we have visual data",
      "offset": 949.36,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "and this visual data is coming from",
      "offset": 953.6,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "multiple cameras six cameras eight",
      "offset": 955.36,
      "duration": 5.599
    },
    {
      "lang": "en",
      "text": "cameras and for each camera or better we",
      "offset": 957.199,
      "duration": 6.161
    },
    {
      "lang": "en",
      "text": "are taking them and lowlevel perception",
      "offset": 960.959,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "step projecting them into a bird's eye",
      "offset": 963.36,
      "duration": 6.96
    },
    {
      "lang": "en",
      "text": "view map and uh then we have things that",
      "offset": 965.68,
      "duration": 8.079
    },
    {
      "lang": "en",
      "text": "ego vehicle does and then we have things",
      "offset": 970.32,
      "duration": 6.079
    },
    {
      "lang": "en",
      "text": "uh for agent other agents other vehicles",
      "offset": 973.759,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "and the things moving in the scene like",
      "offset": 976.399,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "pedestrians and also there is map we",
      "offset": 978.56,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "have a map so all together so let me let",
      "offset": 980.959,
      "duration": 7.521
    },
    {
      "lang": "en",
      "text": "me let me hit pause there so the the ego",
      "offset": 984,
      "duration": 6.56
    },
    {
      "lang": "en",
      "text": "view is kind of this bird's eye view and",
      "offset": 988.48,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "that is familiar here. Yeah, I've got",
      "offset": 990.56,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "that in my car. Like it's got cameras on",
      "offset": 992.48,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "the mirrors and in the back and in the",
      "offset": 994.56,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "front and it kind of projects into this",
      "offset": 996.24,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "top down view. Top down.",
      "offset": 998.24,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "That's that's kind of the",
      "offset": 1000,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "Exactly.",
      "offset": 1002.32,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "So top down view vehicle, ego vehicle is",
      "offset": 1003.04,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "in the center and everything is around",
      "offset": 1006.32,
      "duration": 6.56
    },
    {
      "lang": "en",
      "text": "looks like 2D from topdown. And so the",
      "offset": 1009.04,
      "duration": 7.28
    },
    {
      "lang": "en",
      "text": "the other did you say v the view from",
      "offset": 1012.88,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "other vehicles like",
      "offset": 1016.32,
      "duration": 7.92
    },
    {
      "lang": "en",
      "text": "no uh the tokens or their um kind of not",
      "offset": 1018,
      "duration": 8.319
    },
    {
      "lang": "en",
      "text": "necessarily the views but the useful",
      "offset": 1024.24,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "information that they uh kind of provide",
      "offset": 1026.319,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "into",
      "offset": 1029.52,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "ah so maybe localizing other things in",
      "offset": 1030.48,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "the scene or something.",
      "offset": 1033.6,
      "duration": 5.439
    },
    {
      "lang": "en",
      "text": "Yes. And also their previous trajectory,",
      "offset": 1035.36,
      "duration": 6.4
    },
    {
      "lang": "en",
      "text": "their headings, that type of information",
      "offset": 1039.039,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "not necessarily available in one bird's",
      "offset": 1041.76,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "eye view picture but because uh kind",
      "offset": 1044.24,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "they are about other vehicle like maybe",
      "offset": 1047.36,
      "duration": 6.88
    },
    {
      "lang": "en",
      "text": "also uh how they intend to drive which",
      "offset": 1050,
      "duration": 7.36
    },
    {
      "lang": "en",
      "text": "is not available in bird's eye view. So",
      "offset": 1054.24,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "this system is not only you know looking",
      "offset": 1057.36,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "at a frozen bird but you know thinking",
      "offset": 1060.24,
      "duration": 6.72
    },
    {
      "lang": "en",
      "text": "on behalf of the other vehicles agents",
      "offset": 1063.84,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "how they may move are they accelerating",
      "offset": 1066.96,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "up slow down you know will they do",
      "offset": 1070,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "something unexpected so we are trying to",
      "offset": 1072.72,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "anticipate what other person is going to",
      "offset": 1075.44,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "do other vehicle is going to do like a",
      "offset": 1077.6,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "real person right we are on a traffic",
      "offset": 1079.44,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "stop and then I'm looking at the other",
      "offset": 1081.44,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "vehicle I'm thinking it's gonna go ahead",
      "offset": 1083.84,
      "duration": 5.839
    },
    {
      "lang": "en",
      "text": "or it's going to wait for me to go",
      "offset": 1087.6,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "ahead. So in other words, the",
      "offset": 1089.679,
      "duration": 6.801
    },
    {
      "lang": "en",
      "text": "LM input is kind of a string of tokens",
      "offset": 1092.96,
      "duration": 6.079
    },
    {
      "lang": "en",
      "text": "that represent this top down view from",
      "offset": 1096.48,
      "duration": 6.64
    },
    {
      "lang": "en",
      "text": "the perspective of the vehicle, but also",
      "offset": 1099.039,
      "duration": 7.121
    },
    {
      "lang": "en",
      "text": "um almost like other features, you know,",
      "offset": 1103.12,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "representing the vehicle's trajectory,",
      "offset": 1106.16,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "what it thinks other agents in the scene",
      "offset": 1108.08,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "are doing, whether those are vehicles or",
      "offset": 1111.039,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "pedestrians, and like somehow mapping",
      "offset": 1112.799,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "out the scene. So it's like a a",
      "offset": 1115.12,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "featurization of the space that the",
      "offset": 1118,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "vehicle is operating in.",
      "offset": 1120.96,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "Exactly. That is what we call as seen",
      "offset": 1122.96,
      "duration": 6.719
    },
    {
      "lang": "en",
      "text": "representation and tokens. Everything",
      "offset": 1126.72,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "now kind of standardized in terms of",
      "offset": 1129.679,
      "duration": 6.081
    },
    {
      "lang": "en",
      "text": "tokens. We have this ego vehicle tokens",
      "offset": 1132,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "you know coming from its own queue",
      "offset": 1135.76,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "former and then other vehicles tokens",
      "offset": 1137.44,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "coming from their cube former and",
      "offset": 1140.32,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "intense and there is this bird's eye",
      "offset": 1142.16,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "view seeing at them we are tokenizing it",
      "offset": 1144.16,
      "duration": 6.639
    },
    {
      "lang": "en",
      "text": "also and there's map also there's tokens",
      "offset": 1146.96,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "for that but they are not all the tokens",
      "offset": 1150.799,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "when we are training with LLM these are",
      "offset": 1153.12,
      "duration": 5.439
    },
    {
      "lang": "en",
      "text": "going in but then there is also a",
      "offset": 1156.799,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "question right or a prompt text prompt",
      "offset": 1158.559,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "going into there and then this language",
      "offset": 1161.6,
      "duration": 7.76
    },
    {
      "lang": "en",
      "text": "model what it does uh provides an answer",
      "offset": 1165.039,
      "duration": 7.52
    },
    {
      "lang": "en",
      "text": "and when we are you know giving in",
      "offset": 1169.36,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "training time we have the answers we",
      "offset": 1172.559,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "have this you know scene representation",
      "offset": 1174.24,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "tokens and we have the question so we",
      "offset": 1176.4,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "are updating this language model at the",
      "offset": 1179.039,
      "duration": 6.88
    },
    {
      "lang": "en",
      "text": "same time we are also uh looking at this",
      "offset": 1182.32,
      "duration": 6.32
    },
    {
      "lang": "en",
      "text": "tokens are also updated you know we this",
      "offset": 1185.919,
      "duration": 7.201
    },
    {
      "lang": "en",
      "text": "ego or uh agent or uh bird's eye view or",
      "offset": 1188.64,
      "duration": 7.44
    },
    {
      "lang": "en",
      "text": "met tokens they are also updated. These",
      "offset": 1193.12,
      "duration": 6.32
    },
    {
      "lang": "en",
      "text": "tokens goes into language model and then",
      "offset": 1196.08,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "within this language model when we are",
      "offset": 1199.44,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "running",
      "offset": 1202,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "layers each layer has cross retention",
      "offset": 1204.08,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "and also self attention layers. This",
      "offset": 1206.32,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "self reates those tokens. So in a way",
      "offset": 1208.88,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "that you can think that oh these tokens",
      "offset": 1211.84,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "are now changing in a way that they are",
      "offset": 1214.16,
      "duration": 7.44
    },
    {
      "lang": "en",
      "text": "more consistent and they are more useful",
      "offset": 1217.76,
      "duration": 6.4
    },
    {
      "lang": "en",
      "text": "and they are answering for instance this",
      "offset": 1221.6,
      "duration": 7.04
    },
    {
      "lang": "en",
      "text": "input from better. So at the end we will",
      "offset": 1224.16,
      "duration": 6.8
    },
    {
      "lang": "en",
      "text": "have updated representations of for",
      "offset": 1228.64,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "those tokens. So some row tokens goes",
      "offset": 1230.96,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "into and then better tokens comes out.",
      "offset": 1234,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "So those better tokens are used to",
      "offset": 1236.159,
      "duration": 6.4
    },
    {
      "lang": "en",
      "text": "update two versions again uh transformer",
      "offset": 1239.12,
      "duration": 6.4
    },
    {
      "lang": "en",
      "text": "versions. We can distill the pling",
      "offset": 1242.559,
      "duration": 6.161
    },
    {
      "lang": "en",
      "text": "transformer with uh to capture you know",
      "offset": 1245.52,
      "duration": 7.76
    },
    {
      "lang": "en",
      "text": "how uh pling transformer will have some",
      "offset": 1248.72,
      "duration": 7.439
    },
    {
      "lang": "en",
      "text": "additional compute. So row tokens will",
      "offset": 1253.28,
      "duration": 5.759
    },
    {
      "lang": "en",
      "text": "become better tokens and then the tokens",
      "offset": 1256.159,
      "duration": 6.321
    },
    {
      "lang": "en",
      "text": "will allow us to make better decisions.",
      "offset": 1259.039,
      "duration": 5.361
    },
    {
      "lang": "en",
      "text": "So that's why we are improving the",
      "offset": 1262.48,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "performance these planning constraints.",
      "offset": 1264.4,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "Yeah.",
      "offset": 1266.72,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "And I should note that there's a a",
      "offset": 1267.76,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "really good image or figure two in the",
      "offset": 1270.72,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "paper that is kind of a block diagram of",
      "offset": 1273.52,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "all the things that we're talking about.",
      "offset": 1275.919,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "And hopefully if you're watching the",
      "offset": 1277.679,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "video we will have dropped that in so",
      "offset": 1279.679,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "that you can follow along. Certainly if",
      "offset": 1282.48,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "if not or if you're watching the audio",
      "offset": 1284.64,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "then uh we'll be including a link to the",
      "offset": 1286.72,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "paper in the show notes and you can pull",
      "offset": 1289.039,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "that up. Part of what I'm trying to do",
      "offset": 1290.799,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "here is reconcile what you're saying",
      "offset": 1292.72,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "with the image, which is, you know, it's",
      "offset": 1294.159,
      "duration": 5.921
    },
    {
      "lang": "en",
      "text": "obviously a static thing. And so I'm",
      "offset": 1297.6,
      "duration": 5.439
    },
    {
      "lang": "en",
      "text": "trying to understand how the things that",
      "offset": 1300.08,
      "duration": 7.599
    },
    {
      "lang": "en",
      "text": "you're saying happen in time. like so",
      "offset": 1303.039,
      "duration": 8.721
    },
    {
      "lang": "en",
      "text": "you've got you know uh I think your data",
      "offset": 1307.679,
      "duration": 7.201
    },
    {
      "lang": "en",
      "text": "set consists of",
      "offset": 1311.76,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "um",
      "offset": 1314.88,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "at a given point in time six images from",
      "offset": 1316.559,
      "duration": 7.921
    },
    {
      "lang": "en",
      "text": "the the cameras on a vehicle right and",
      "offset": 1320.24,
      "duration": 6.799
    },
    {
      "lang": "en",
      "text": "you just have a many of those is there",
      "offset": 1324.48,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "additional data associated with that or",
      "offset": 1327.039,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "is it just those images",
      "offset": 1328.799,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "uh we represent all of these current six",
      "offset": 1330.559,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "images and let's say t minus one six",
      "offset": 1333.76,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "images. It could be t minus n also but",
      "offset": 1335.919,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "looks like t minus one is sufficient in",
      "offset": 1338.72,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "terms of their tokens. There's this",
      "offset": 1341.039,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "scene encoder s and then",
      "offset": 1342.88,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "that scene encoder is kind of like the",
      "offset": 1345.039,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "feature extractor that we talked a",
      "offset": 1346.559,
      "duration": 3.041
    },
    {
      "lang": "en",
      "text": "little bit about right",
      "offset": 1348.24,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "it generates tokens you know kind of at",
      "offset": 1349.6,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "the end there are this token for the six",
      "offset": 1351.919,
      "duration": 6.721
    },
    {
      "lang": "en",
      "text": "images current images and then previous",
      "offset": 1355.6,
      "duration": 6.8
    },
    {
      "lang": "en",
      "text": "images as we have those tokens already",
      "offset": 1358.64,
      "duration": 8
    },
    {
      "lang": "en",
      "text": "in the LLM. So again two versions to uh",
      "offset": 1362.4,
      "duration": 7.92
    },
    {
      "lang": "en",
      "text": "kind of distinguish uh if LLM is making",
      "offset": 1366.64,
      "duration": 6.64
    },
    {
      "lang": "en",
      "text": "the AI planner decisions those are in",
      "offset": 1370.32,
      "duration": 6.96
    },
    {
      "lang": "en",
      "text": "the KV cache long context of the model.",
      "offset": 1373.28,
      "duration": 7.2
    },
    {
      "lang": "en",
      "text": "So model retains those and then when it",
      "offset": 1377.28,
      "duration": 5.519
    },
    {
      "lang": "en",
      "text": "is going to make for instance a waypoint",
      "offset": 1380.48,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "estimation or a driving decision, a",
      "offset": 1382.799,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "planning decision, it will leverage",
      "offset": 1385.44,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "those previous tokens up to the you know",
      "offset": 1387.919,
      "duration": 6.961
    },
    {
      "lang": "en",
      "text": "buffer size of 2 KVK and then uh the",
      "offset": 1390.48,
      "duration": 8.16
    },
    {
      "lang": "en",
      "text": "transformer version we use previous",
      "offset": 1394.88,
      "duration": 7.52
    },
    {
      "lang": "en",
      "text": "token instance and current tokens and uh",
      "offset": 1398.64,
      "duration": 6.56
    },
    {
      "lang": "en",
      "text": "update this pling transformer. It's a",
      "offset": 1402.4,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "transformer but it's not like language",
      "offset": 1405.2,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "model but uh transformers are the",
      "offset": 1407.44,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "building blocks of language models by",
      "offset": 1410.72,
      "duration": 4.959
    },
    {
      "lang": "en",
      "text": "the way it's not explicitly answering a",
      "offset": 1412.72,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "question you know it doesn't have that",
      "offset": 1415.679,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "capacity but it work with tokens so",
      "offset": 1417.039,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "those tokens what I mentioned are now in",
      "offset": 1419.28,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "training time we learn better tokens and",
      "offset": 1422.159,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "now we take it and we learn an adapter",
      "offset": 1424.159,
      "duration": 5.281
    },
    {
      "lang": "en",
      "text": "you know kind of for this planning",
      "offset": 1427.919,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "transformer so next time in the trans",
      "offset": 1429.44,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "meaning so you can get the tokens that",
      "offset": 1432.24,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "come out of the scene encoder and give",
      "offset": 1434.32,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "them to the planning transformer",
      "offset": 1436.08,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "directly and get a prediction as opposed",
      "offset": 1437.76,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "to going through all of the machinery of",
      "offset": 1440.159,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "the LLM.",
      "offset": 1442.799,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "Exactly. So that version is transformer",
      "offset": 1443.919,
      "duration": 5.441
    },
    {
      "lang": "en",
      "text": "version is more efficient. Now Cena",
      "offset": 1447.12,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "encoder again generates the raw tokens.",
      "offset": 1449.36,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "It goes through this adapter and then uh",
      "offset": 1451.76,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "goes to the pling transformer and uh",
      "offset": 1455.52,
      "duration": 5.519
    },
    {
      "lang": "en",
      "text": "that is there's no LLM anymore in that",
      "offset": 1458,
      "duration": 5.919
    },
    {
      "lang": "en",
      "text": "version but uh you know kind of a better",
      "offset": 1461.039,
      "duration": 5.041
    },
    {
      "lang": "en",
      "text": "uh transformer for planning",
      "offset": 1463.919,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "and this is the core idea behind",
      "offset": 1466.08,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "distillation in general. So it's like",
      "offset": 1468.64,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "you've got this you know surrogate model",
      "offset": 1471.12,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "in this case a transformer",
      "offset": 1473.52,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "uh or the student model I'm referring to",
      "offset": 1476.24,
      "duration": 5.919
    },
    {
      "lang": "en",
      "text": "in this case that is uh you know it's a",
      "offset": 1478.32,
      "duration": 6.239
    },
    {
      "lang": "en",
      "text": "a function approximator and so the",
      "offset": 1482.159,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "distillation process is kind of teaching",
      "offset": 1484.559,
      "duration": 5.441
    },
    {
      "lang": "en",
      "text": "it how to approximate uh you know the",
      "offset": 1486.32,
      "duration": 6.88
    },
    {
      "lang": "en",
      "text": "function in this case here the teacher",
      "offset": 1490,
      "duration": 7.84
    },
    {
      "lang": "en",
      "text": "model it's output tokens given the the",
      "offset": 1493.2,
      "duration": 7.12
    },
    {
      "lang": "en",
      "text": "beam tokens from the scene encoder are",
      "offset": 1497.84,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "the the function that we're trying to",
      "offset": 1500.32,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "approximate.",
      "offset": 1502,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "Absolutely. Of course,",
      "offset": 1503.12,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "devil is in the details how you do the",
      "offset": 1505.76,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "distilleration, how you do surrogate",
      "offset": 1508,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "tasks, uh how you do VQA is a part of",
      "offset": 1511.12,
      "duration": 6.32
    },
    {
      "lang": "en",
      "text": "it, what are those data sets and what",
      "offset": 1514.24,
      "duration": 7.439
    },
    {
      "lang": "en",
      "text": "are the you know uh those functions. Uh",
      "offset": 1517.44,
      "duration": 7.359
    },
    {
      "lang": "en",
      "text": "we tried many things and uh you know",
      "offset": 1521.679,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "kind of more details are in the paper.",
      "offset": 1524.799,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "You know you mentioned surrogate tasks",
      "offset": 1528.159,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "and VQA. What are surrogate tasks in",
      "offset": 1530,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "this context?",
      "offset": 1533.36,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "For instance, they are mostly around",
      "offset": 1534.32,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "trajectory prediction.",
      "offset": 1536.88,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "Some of them are about also mass token",
      "offset": 1539.12,
      "duration": 7.6
    },
    {
      "lang": "en",
      "text": "prediction, future BV token prediction",
      "offset": 1542,
      "duration": 7.679
    },
    {
      "lang": "en",
      "text": "to encourage the LLM to learn spatial",
      "offset": 1546.72,
      "duration": 6.079
    },
    {
      "lang": "en",
      "text": "temporal cues useful for planning and",
      "offset": 1549.679,
      "duration": 5.281
    },
    {
      "lang": "en",
      "text": "scene editing. Scene editing is a part",
      "offset": 1552.799,
      "duration": 5.681
    },
    {
      "lang": "en",
      "text": "of it. because we have to hypotize where",
      "offset": 1554.96,
      "duration": 6.88
    },
    {
      "lang": "en",
      "text": "the other things agents are going to be.",
      "offset": 1558.48,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "So I mean this is in training time um",
      "offset": 1561.84,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "how the surrounding agents other",
      "offset": 1564.72,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "vehicles will impact the ego vehicles",
      "offset": 1566.88,
      "duration": 6.64
    },
    {
      "lang": "en",
      "text": "future path you know kind of a uh so",
      "offset": 1570.32,
      "duration": 5.599
    },
    {
      "lang": "en",
      "text": "this is what I meant by scene editing",
      "offset": 1573.52,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "tests but that's a part of the surrogate",
      "offset": 1575.919,
      "duration": 7.041
    },
    {
      "lang": "en",
      "text": "task or trajectory prediction and uh and",
      "offset": 1578.32,
      "duration": 7.599
    },
    {
      "lang": "en",
      "text": "there are a couple of more also",
      "offset": 1582.96,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "and so they're not necessarily tasks",
      "offset": 1585.919,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "that you want to use but rather they're",
      "offset": 1588.24,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "exposed as tasks so that you have a",
      "offset": 1590.799,
      "duration": 5.841
    },
    {
      "lang": "en",
      "text": "separate loss function that you can uh",
      "offset": 1593.36,
      "duration": 6.799
    },
    {
      "lang": "en",
      "text": "incorporate so that the model attends to",
      "offset": 1596.64,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "those things. Is that a fair way to",
      "offset": 1600.159,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "think about it?",
      "offset": 1603.12,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "Exactly. We are not explicitly saying",
      "offset": 1604.08,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "here is the trajectory, here's the you",
      "offset": 1606,
      "duration": 6.96
    },
    {
      "lang": "en",
      "text": "know form equation. We are using that to",
      "offset": 1608.08,
      "duration": 6.959
    },
    {
      "lang": "en",
      "text": "make sure that the temporal aspects of",
      "offset": 1612.96,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "driving is captured through those",
      "offset": 1615.039,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "surrogate test like prediction. I said",
      "offset": 1617.2,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "prediction right? uh the scene itself",
      "offset": 1619.52,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "the other agents and these are future",
      "offset": 1622.08,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "predicting um and also leveraging on",
      "offset": 1624.64,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "what kind of it observed before so the",
      "offset": 1627.6,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "model would learn a better you know",
      "offset": 1630.32,
      "duration": 5.839
    },
    {
      "lang": "en",
      "text": "representation itself but it is not like",
      "offset": 1633.2,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "I'm we are telling model to go optimize",
      "offset": 1636.159,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "for this future instances something like",
      "offset": 1638.48,
      "duration": 6.319
    },
    {
      "lang": "en",
      "text": "that or change it is through the",
      "offset": 1641.36,
      "duration": 6.4
    },
    {
      "lang": "en",
      "text": "training it learns itself changes it",
      "offset": 1644.799,
      "duration": 7.441
    },
    {
      "lang": "en",
      "text": "coefficients to better um uh understand",
      "offset": 1647.76,
      "duration": 7.84
    },
    {
      "lang": "en",
      "text": "the dynamics of the traffic scenarios",
      "offset": 1652.24,
      "duration": 5.679
    },
    {
      "lang": "en",
      "text": "and what's the role of incorporating a",
      "offset": 1655.6,
      "duration": 5.439
    },
    {
      "lang": "en",
      "text": "VQA component visual question answering",
      "offset": 1657.919,
      "duration": 7.841
    },
    {
      "lang": "en",
      "text": "yeah um we want this capability the",
      "offset": 1661.039,
      "duration": 7.041
    },
    {
      "lang": "en",
      "text": "that's an important part also this word",
      "offset": 1665.76,
      "duration": 5.919
    },
    {
      "lang": "en",
      "text": "knowledge to be a part of overall um",
      "offset": 1668.08,
      "duration": 7.76
    },
    {
      "lang": "en",
      "text": "kind of solution we do not want to just",
      "offset": 1671.679,
      "duration": 8
    },
    {
      "lang": "en",
      "text": "drive steer LLM towards surrogate task",
      "offset": 1675.84,
      "duration": 7.68
    },
    {
      "lang": "en",
      "text": "you uh future prediction or you know",
      "offset": 1679.679,
      "duration": 6.72
    },
    {
      "lang": "en",
      "text": "generating better tokens for the planner",
      "offset": 1683.52,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "but we want it to be grounded also. So",
      "offset": 1686.399,
      "duration": 6.481
    },
    {
      "lang": "en",
      "text": "that is keeping it real connected to the",
      "offset": 1689.12,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "semantic information. For instance,",
      "offset": 1692.88,
      "duration": 5.919
    },
    {
      "lang": "en",
      "text": "those uh questions could be uh okay in",
      "offset": 1694.96,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "this scenario what are the safe actions",
      "offset": 1698.799,
      "duration": 6.161
    },
    {
      "lang": "en",
      "text": "to take for ego vehicle that is the",
      "offset": 1701.2,
      "duration": 6.88
    },
    {
      "lang": "en",
      "text": "literally the question and the answer is",
      "offset": 1704.96,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "could be the action is to break gently",
      "offset": 1708.08,
      "duration": 6.88
    },
    {
      "lang": "en",
      "text": "to a stop. So for this input we want",
      "offset": 1710.24,
      "duration": 7.52
    },
    {
      "lang": "en",
      "text": "model to be still able to generate such",
      "offset": 1714.96,
      "duration": 6.4
    },
    {
      "lang": "en",
      "text": "answer uh given the prompt and all the",
      "offset": 1717.76,
      "duration": 7.919
    },
    {
      "lang": "en",
      "text": "visual data. So that is going to uh",
      "offset": 1721.36,
      "duration": 7.28
    },
    {
      "lang": "en",
      "text": "allow us to retain the semantic world",
      "offset": 1725.679,
      "duration": 5.441
    },
    {
      "lang": "en",
      "text": "information of this LLM without",
      "offset": 1728.64,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "destroying the LLA. And also the second",
      "offset": 1731.12,
      "duration": 6.559
    },
    {
      "lang": "en",
      "text": "version we can ask questions and then it",
      "offset": 1734,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "can answer you know kind of uh again two",
      "offset": 1737.679,
      "duration": 5.281
    },
    {
      "lang": "en",
      "text": "version transformer version doesn't uh",
      "offset": 1740.24,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "answer the questions but the LLM",
      "offset": 1742.96,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "versions in inference time can answer",
      "offset": 1745.12,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "the question. So it has two purpose",
      "offset": 1747.12,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "keeping it grounded semantically",
      "offset": 1749.679,
      "duration": 5.761
    },
    {
      "lang": "en",
      "text": "grounded still not destroying the word",
      "offset": 1751.76,
      "duration": 5.919
    },
    {
      "lang": "en",
      "text": "knowledge but then also having",
      "offset": 1755.44,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "capability to answer questions if you",
      "offset": 1757.679,
      "duration": 7.081
    },
    {
      "lang": "en",
      "text": "are running LLM as the planner",
      "offset": 1760.48,
      "duration": 4.28
    },
    {
      "lang": "en",
      "text": "uh and so you've talked about the",
      "offset": 1765.12,
      "duration": 7.279
    },
    {
      "lang": "en",
      "text": "kind of the the outcome here so the",
      "offset": 1769.2,
      "duration": 5.839
    },
    {
      "lang": "en",
      "text": "state-of-the-art performance on some of",
      "offset": 1772.399,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "these longtail on both trajectory three",
      "offset": 1775.039,
      "duration": 6.801
    },
    {
      "lang": "en",
      "text": "uh error and kind of longtail collision",
      "offset": 1778.399,
      "duration": 7.041
    },
    {
      "lang": "en",
      "text": "detection or avoidance. Um what you",
      "offset": 1781.84,
      "duration": 7.52
    },
    {
      "lang": "en",
      "text": "didn't mention yet is",
      "offset": 1785.44,
      "duration": 8.479
    },
    {
      "lang": "en",
      "text": "um any kind of SLAs's or performance",
      "offset": 1789.36,
      "duration": 7.039
    },
    {
      "lang": "en",
      "text": "around the distilled model. Is it still",
      "offset": 1793.919,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "fast? Is it fast enough yet to be able",
      "offset": 1796.399,
      "duration": 6.081
    },
    {
      "lang": "en",
      "text": "to use in a real-time AV loop or does",
      "offset": 1799.039,
      "duration": 7.12
    },
    {
      "lang": "en",
      "text": "more work need to be done to um make the",
      "offset": 1802.48,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "model more efficient? This model we can",
      "offset": 1806.159,
      "duration": 8.561
    },
    {
      "lang": "en",
      "text": "run as of now on Qualcomm uh accelerator",
      "offset": 1808.72,
      "duration": 10.16
    },
    {
      "lang": "en",
      "text": "uh faster than real time.",
      "offset": 1814.72,
      "duration": 6.64
    },
    {
      "lang": "en",
      "text": "It is real solution. I mean this paper",
      "offset": 1818.88,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "is not for you know kind of publication",
      "offset": 1821.36,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "research sake but it has a very",
      "offset": 1824,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "practical purpose also. We hope to have",
      "offset": 1826.72,
      "duration": 7.76
    },
    {
      "lang": "en",
      "text": "a vehicle running around soon and yeah",
      "offset": 1829.36,
      "duration": 7.439
    },
    {
      "lang": "en",
      "text": "showcasing this technology most likely",
      "offset": 1834.48,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "it's going to be at CES but you know",
      "offset": 1836.799,
      "duration": 5.041
    },
    {
      "lang": "en",
      "text": "kind of I don't know the future you know",
      "offset": 1838.88,
      "duration": 6.64
    },
    {
      "lang": "en",
      "text": "kind of but it is there you know",
      "offset": 1841.84,
      "duration": 6.64
    },
    {
      "lang": "en",
      "text": "given what you've done with this paper",
      "offset": 1845.52,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "where do you see this kind of direction",
      "offset": 1848.48,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "of of research going this particular",
      "offset": 1850.399,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "line of research what's next",
      "offset": 1852.559,
      "duration": 5.281
    },
    {
      "lang": "en",
      "text": "so we see that you know such models are",
      "offset": 1854.64,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "useful",
      "offset": 1857.84,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "And other companies are also looking",
      "offset": 1858.64,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "into that. And I see one uh trend of",
      "offset": 1860.72,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "running both transformer and language",
      "offset": 1864.64,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "model together. Language model is as you",
      "offset": 1867.2,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "you know kind of as we talked about is",
      "offset": 1869.84,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "slower compute intensive. So maybe you",
      "offset": 1871.84,
      "duration": 6.4
    },
    {
      "lang": "en",
      "text": "know not that fast at this moment. So it",
      "offset": 1874.559,
      "duration": 6.561
    },
    {
      "lang": "en",
      "text": "is on a longer horizon you know in a",
      "offset": 1878.24,
      "duration": 6.559
    },
    {
      "lang": "en",
      "text": "lower frame rate. It is estimating doing",
      "offset": 1881.12,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "its own thing you know estimating way",
      "offset": 1884.799,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "points and you know some useful AI",
      "offset": 1886.48,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "planning decisions at the current AI",
      "offset": 1888.64,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "planner using transformer is also",
      "offset": 1891.52,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "running on 3D or 2D perception st and",
      "offset": 1893.44,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "then these are combined so it is there",
      "offset": 1896,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "is redundancy in the system if it is",
      "offset": 1898.96,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "something unexpected for the you know",
      "offset": 1901.76,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "low-level perception sake at least we",
      "offset": 1904,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "have high level you know understanding",
      "offset": 1906,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "that would provide some safety mechanism",
      "offset": 1908.48,
      "duration": 6.88
    },
    {
      "lang": "en",
      "text": "for the longtail. So this this like a",
      "offset": 1912.48,
      "duration": 6.079
    },
    {
      "lang": "en",
      "text": "trend kind of many companies research",
      "offset": 1915.36,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "labs are looking into that. So I think",
      "offset": 1918.559,
      "duration": 5.041
    },
    {
      "lang": "en",
      "text": "it will become more popular but more as",
      "offset": 1920.64,
      "duration": 5.279
    },
    {
      "lang": "en",
      "text": "important as this one using language",
      "offset": 1923.6,
      "duration": 5.439
    },
    {
      "lang": "en",
      "text": "model in the vehicle to interact with",
      "offset": 1925.919,
      "duration": 5.441
    },
    {
      "lang": "en",
      "text": "the vehicle decision mechanism change",
      "offset": 1929.039,
      "duration": 7.12
    },
    {
      "lang": "en",
      "text": "its behavior customize for a geospatial",
      "offset": 1931.36,
      "duration": 6.88
    },
    {
      "lang": "en",
      "text": "location like for a city or for a",
      "offset": 1936.159,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "country because rules are different and",
      "offset": 1938.24,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "the driving dynamics are different and",
      "offset": 1940.159,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "preferences are different. Maybe you",
      "offset": 1942.559,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "want more aggressive driving or more",
      "offset": 1944.32,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "conservative, you know, kind of. Yeah.",
      "offset": 1946.799,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "Uh they are also happening. I think kind",
      "offset": 1949.2,
      "duration": 7.28
    },
    {
      "lang": "en",
      "text": "of Yeah. Um I can see that we actually",
      "offset": 1952.559,
      "duration": 7.201
    },
    {
      "lang": "en",
      "text": "we are in the process of uh having an",
      "offset": 1956.48,
      "duration": 7.36
    },
    {
      "lang": "en",
      "text": "even more uh capable version of DIMA uh",
      "offset": 1959.76,
      "duration": 7.2
    },
    {
      "lang": "en",
      "text": "which is domain adapting to some of",
      "offset": 1963.84,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "these scenarios that I mentioned.",
      "offset": 1966.96,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "Very cool. Very cool. All right. So",
      "offset": 1969.12,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "shifting gears to the second paper that",
      "offset": 1971.6,
      "duration": 7.439
    },
    {
      "lang": "en",
      "text": "we want to uh dig deep into oh deep uh",
      "offset": 1974.08,
      "duration": 8.079
    },
    {
      "lang": "en",
      "text": "no pun intended is sharp depth.",
      "offset": 1979.039,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "Uh and the full title of this paper",
      "offset": 1982.159,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "sharp depth sharpening metric depth",
      "offset": 1984.399,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "predictions using diffusion",
      "offset": 1986.72,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "distillation. So distillation again uh a",
      "offset": 1988.559,
      "duration": 5.921
    },
    {
      "lang": "en",
      "text": "theme here. My impression is the general",
      "offset": 1991.519,
      "duration": 7.04
    },
    {
      "lang": "en",
      "text": "space that this paper is exploring is",
      "offset": 1994.48,
      "duration": 6.72
    },
    {
      "lang": "en",
      "text": "moninocular depth estimation. So you've",
      "offset": 1998.559,
      "duration": 7.041
    },
    {
      "lang": "en",
      "text": "got a uh 2D image, a photo, and you want",
      "offset": 2001.2,
      "duration": 8.8
    },
    {
      "lang": "en",
      "text": "to essentially reconstruct it as a 3D uh",
      "offset": 2005.6,
      "duration": 6.559
    },
    {
      "lang": "en",
      "text": "in a 3D space. Is is that the right way",
      "offset": 2010,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "to think about this?",
      "offset": 2012.159,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "Very good. Yeah, you said it very well.",
      "offset": 2013.12,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "And so talk a little bit about, you",
      "offset": 2015.679,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "know, the background here and",
      "offset": 2017.519,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "state-of-the-art prior to this work. Um",
      "offset": 2019.76,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "and then we'll dig into what this work",
      "offset": 2022.32,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "continues.",
      "offset": 2024.799,
      "duration": 2.961
    },
    {
      "lang": "en",
      "text": "Absolutely. So this is molecular depth",
      "offset": 2025.679,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "estimation but more specifically this",
      "offset": 2027.76,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "paper chart that is about metric depth",
      "offset": 2029.84,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "estimation. You don't know whether it's",
      "offset": 2032.48,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "a tiny chair or a huge chair or you know",
      "offset": 2034.559,
      "duration": 5.761
    },
    {
      "lang": "en",
      "text": "some reasonable chair. It is so there",
      "offset": 2037.44,
      "duration": 6.64
    },
    {
      "lang": "en",
      "text": "are two ways two separate you know um uh",
      "offset": 2040.32,
      "duration": 7.52
    },
    {
      "lang": "en",
      "text": "type of algorithms. One uh are maybe",
      "offset": 2044.08,
      "duration": 8.319
    },
    {
      "lang": "en",
      "text": "let's call them as generative models. Um",
      "offset": 2047.84,
      "duration": 8.88
    },
    {
      "lang": "en",
      "text": "and those are uh fine grain they provide",
      "offset": 2052.399,
      "duration": 7.601
    },
    {
      "lang": "en",
      "text": "uh no very sharp depth molecular depth",
      "offset": 2056.72,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "estimation because not there's a lot",
      "offset": 2060,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "they can use synthetic data there's a",
      "offset": 2062.56,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "lot of such data like my gold and lotus",
      "offset": 2064.48,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "and many you know molecular depth",
      "offset": 2067.119,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "estimation algorithms are like that so",
      "offset": 2068.72,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "they have also dance ground truth",
      "offset": 2071.2,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "because you know it's synthetic is there",
      "offset": 2073.2,
      "duration": 7.52
    },
    {
      "lang": "en",
      "text": "uh but their you know skills uh it this",
      "offset": 2076.079,
      "duration": 7.441
    },
    {
      "lang": "en",
      "text": "point you know it is not like inches or",
      "offset": 2080.72,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "millimeter or anything like that you",
      "offset": 2083.52,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "don't know what that point each pixel",
      "offset": 2085.52,
      "duration": 5.599
    },
    {
      "lang": "en",
      "text": "how big it is it could be 1 meter or 1",
      "offset": 2088.399,
      "duration": 5.121
    },
    {
      "lang": "en",
      "text": "millimeter you know very different so",
      "offset": 2091.119,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "there are",
      "offset": 2093.52,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "it's relative within the reconstructed",
      "offset": 2094.079,
      "duration": 7.201
    },
    {
      "lang": "en",
      "text": "image but it's not absolute so you can't",
      "offset": 2098.079,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "measure with it",
      "offset": 2101.28,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "yeah it is relative so those are uh",
      "offset": 2102,
      "duration": 5.119
    },
    {
      "lang": "en",
      "text": "generative solutions and there is",
      "offset": 2105.2,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "discriminative solutions like metric 3D",
      "offset": 2107.119,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "and Unidep and when they do uh provide",
      "offset": 2109.52,
      "duration": 7.12
    },
    {
      "lang": "en",
      "text": "these estimations depth estimation cell",
      "offset": 2113.599,
      "duration": 5.601
    },
    {
      "lang": "en",
      "text": "they either inches or you know",
      "offset": 2116.64,
      "duration": 6.64
    },
    {
      "lang": "en",
      "text": "millimeters centimeters uh however they",
      "offset": 2119.2,
      "duration": 6.72
    },
    {
      "lang": "en",
      "text": "are trained with such data doesn't exist",
      "offset": 2123.28,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "in quantity because usually LAR sensors",
      "offset": 2125.92,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "images and light LAR sensors or",
      "offset": 2128.96,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "structured light sensors are used. So",
      "offset": 2131.2,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "first of all they are very low",
      "offset": 2134.8,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "resolution and not big in quantity it's",
      "offset": 2136.16,
      "duration": 6.8
    },
    {
      "lang": "en",
      "text": "real measurement. So there's a",
      "offset": 2140.8,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "challenge. So when you use those",
      "offset": 2142.96,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "solutions you get you know kind of",
      "offset": 2144.96,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "correct depth estimation from the camera",
      "offset": 2147.04,
      "duration": 8.24
    },
    {
      "lang": "en",
      "text": "or 3D in accurate in terms of scale.",
      "offset": 2150.24,
      "duration": 8.24
    },
    {
      "lang": "en",
      "text": "However they look like very blurred in",
      "offset": 2155.28,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "the paper. You may see some examples.",
      "offset": 2158.48,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "You know all the details are missing. If",
      "offset": 2160.32,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "there is a chair legs will be missing.",
      "offset": 2162.16,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "Genative method because of scientific",
      "offset": 2165.44,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "data they are leveraging they will show",
      "offset": 2168,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "very nice details very fine but we don't",
      "offset": 2169.76,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "know how far away that chair or how big",
      "offset": 2172.079,
      "duration": 6.561
    },
    {
      "lang": "en",
      "text": "the chair is but this dep methods they",
      "offset": 2174.48,
      "duration": 8.08
    },
    {
      "lang": "en",
      "text": "will give exact absolute distance",
      "offset": 2178.64,
      "duration": 6.719
    },
    {
      "lang": "en",
      "text": "accuracy is great but then very low",
      "offset": 2182.56,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "resolution maybe you will not even see",
      "offset": 2185.359,
      "duration": 5.281
    },
    {
      "lang": "en",
      "text": "the chair. So there like there are two",
      "offset": 2187.04,
      "duration": 6.319
    },
    {
      "lang": "en",
      "text": "uh kind of existing solutions. So this",
      "offset": 2190.64,
      "duration": 6.88
    },
    {
      "lang": "en",
      "text": "paper sharp bridges these two approaches",
      "offset": 2193.359,
      "duration": 7.121
    },
    {
      "lang": "en",
      "text": "integrating metric accuracy with the",
      "offset": 2197.52,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "detailed boundary preservation of the",
      "offset": 2200.48,
      "duration": 6.72
    },
    {
      "lang": "en",
      "text": "the generative methods. It is generative",
      "offset": 2204,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "method. It means taking advantage of for",
      "offset": 2207.2,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "instance a diffusion denoising diffusion",
      "offset": 2209.92,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "and it has actually in the architecture",
      "offset": 2212.48,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "it has a unit uh model running in the",
      "offset": 2215.28,
      "duration": 6.96
    },
    {
      "lang": "en",
      "text": "background. I I'll jump in to note that",
      "offset": 2218.72,
      "duration": 8.08
    },
    {
      "lang": "en",
      "text": "anyone that wants to dig into unit uh a",
      "offset": 2222.24,
      "duration": 7.2
    },
    {
      "lang": "en",
      "text": "bit I think we talked about that uh in",
      "offset": 2226.8,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "our last CVPR CVPR review and maybe even",
      "offset": 2229.44,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "the one before that. I I think we've",
      "offset": 2232.96,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "been unit has been a theme that you and",
      "offset": 2234.8,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "your your research group have been",
      "offset": 2237.44,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "working on for a while now.",
      "offset": 2238.96,
      "duration": 7.76
    },
    {
      "lang": "en",
      "text": "Unit actually kind of a the name U shape",
      "offset": 2241.359,
      "duration": 8
    },
    {
      "lang": "en",
      "text": "is the shape of the architecture. It has",
      "offset": 2246.72,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "been very popular. Of course there is",
      "offset": 2249.359,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "also a trend of now using DIT diffusion",
      "offset": 2251.599,
      "duration": 6.881
    },
    {
      "lang": "en",
      "text": "transformers which has a different",
      "offset": 2255.839,
      "duration": 5.921
    },
    {
      "lang": "en",
      "text": "structure but unit is still one of the",
      "offset": 2258.48,
      "duration": 6.96
    },
    {
      "lang": "en",
      "text": "choices for uh geneti for visual data.",
      "offset": 2261.76,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "You're right and there are lots of",
      "offset": 2265.44,
      "duration": 6.56
    },
    {
      "lang": "en",
      "text": "applications of this uh uh Sam why you",
      "offset": 2268.24,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "may wonder why we want to do molecular",
      "offset": 2272,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "depth uh estimation should I talk about",
      "offset": 2274.16,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "this?",
      "offset": 2277.04,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "Yeah, I'm imagining something like, you",
      "offset": 2278.56,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "know, I'm in a room and I want to take a",
      "offset": 2281.04,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "picture of something with my camera and",
      "offset": 2282.64,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "know how big it is. That that would be",
      "offset": 2285.44,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "pretty cool.",
      "offset": 2287.2,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "Absolutely. Then you can place, you",
      "offset": 2288.48,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "know, for you go buy from a furniture",
      "offset": 2290.64,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "store I and you know how exactly it's",
      "offset": 2293.92,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "going to fit. It's not relative because",
      "offset": 2296.56,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "now you know the actual measurement. So",
      "offset": 2298.72,
      "duration": 7.119
    },
    {
      "lang": "en",
      "text": "this virtual try out, room planning,",
      "offset": 2302.16,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "object placement, 3D model",
      "offset": 2305.839,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "reconstruction. You can use your uh you",
      "offset": 2308.16,
      "duration": 6.4
    },
    {
      "lang": "en",
      "text": "know phone to create a real size uh with",
      "offset": 2310.32,
      "duration": 9.44
    },
    {
      "lang": "en",
      "text": "metric absolute scale models. Of course,",
      "offset": 2314.56,
      "duration": 7.2
    },
    {
      "lang": "en",
      "text": "people are using for robotic navigation",
      "offset": 2319.76,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "because we know exactly how far away",
      "offset": 2321.76,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "oluders and other objects in the scene",
      "offset": 2324.079,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "and robots are interacting with the",
      "offset": 2326.56,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "scene. When it is reaching out",
      "offset": 2329.359,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "something, it knows where it is exactly",
      "offset": 2331.28,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "not, you know, how many pixels, you",
      "offset": 2333.119,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "know, it doesn't know whether it's far",
      "offset": 2335.52,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "away or nearby. Yeah, there are lots of",
      "offset": 2337.119,
      "duration": 5.681
    },
    {
      "lang": "en",
      "text": "applications in for instance immersive",
      "offset": 2339.28,
      "duration": 7.44
    },
    {
      "lang": "en",
      "text": "property walkthroughs to understanding",
      "offset": 2342.8,
      "duration": 6.88
    },
    {
      "lang": "en",
      "text": "things in 3D space even surgical",
      "offset": 2346.72,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "assistance you know kind of you do",
      "offset": 2349.68,
      "duration": 6.56
    },
    {
      "lang": "en",
      "text": "minimally invasive uh procedures using a",
      "offset": 2352,
      "duration": 7.119
    },
    {
      "lang": "en",
      "text": "single camera and because they need to",
      "offset": 2356.24,
      "duration": 5.119
    },
    {
      "lang": "en",
      "text": "be very tiny right you may not be able",
      "offset": 2359.119,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "to put like two cameras with white",
      "offset": 2361.359,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "baseline into a very tiny blood vessel",
      "offset": 2364.079,
      "duration": 5.601
    },
    {
      "lang": "en",
      "text": "so but You can squeeze something like a",
      "offset": 2367.119,
      "duration": 5.041
    },
    {
      "lang": "en",
      "text": "camera, single camera, but then you know",
      "offset": 2369.68,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "how far away everything is in the sensor",
      "offset": 2372.16,
      "duration": 4.959
    },
    {
      "lang": "en",
      "text": "maybe I mean these are real applications",
      "offset": 2375.04,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "like drones or you know kind of",
      "offset": 2377.119,
      "duration": 5.921
    },
    {
      "lang": "en",
      "text": "let's say monitoring to construction",
      "offset": 2380.64,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "sites and other type of things parking",
      "offset": 2383.04,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "automated parking is a part of it also.",
      "offset": 2385.44,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "Yeah. Uh you mentioned that part of",
      "offset": 2387.359,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "what's happening here is you're kind of",
      "offset": 2390.4,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "fusing",
      "offset": 2393.119,
      "duration": 5.841
    },
    {
      "lang": "en",
      "text": "uh a generative approach uh with uh",
      "offset": 2394.64,
      "duration": 8.24
    },
    {
      "lang": "en",
      "text": "metricbased approach. And",
      "offset": 2398.96,
      "duration": 7.6
    },
    {
      "lang": "en",
      "text": "it's still not clear to me like it seems",
      "offset": 2402.88,
      "duration": 6.239
    },
    {
      "lang": "en",
      "text": "like you need some piece of absolute",
      "offset": 2406.56,
      "duration": 5.519
    },
    {
      "lang": "en",
      "text": "data somewhere in order to start to do",
      "offset": 2409.119,
      "duration": 5.921
    },
    {
      "lang": "en",
      "text": "this. Like, you know, if you're taking a",
      "offset": 2412.079,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "picture with a phone, you know, you",
      "offset": 2415.04,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "might need some like LAR, you know, that",
      "offset": 2417.359,
      "duration": 5.601
    },
    {
      "lang": "en",
      "text": "says the distance from objects or like",
      "offset": 2420.16,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "maybe stereoscopic where you've got the",
      "offset": 2422.96,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "distance between the lenses or some",
      "offset": 2425.2,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "piece of",
      "offset": 2427.2,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "uh piece of additional information from",
      "offset": 2429.2,
      "duration": 6.879
    },
    {
      "lang": "en",
      "text": "the real world to ground you. How does",
      "offset": 2432.64,
      "duration": 7.04
    },
    {
      "lang": "en",
      "text": "this method overcome that gap?",
      "offset": 2436.079,
      "duration": 8.641
    },
    {
      "lang": "en",
      "text": "Yes. Um so we are running two things to",
      "offset": 2439.68,
      "duration": 9.12
    },
    {
      "lang": "en",
      "text": "start the overall process. One is this",
      "offset": 2444.72,
      "duration": 7.359
    },
    {
      "lang": "en",
      "text": "kind of discriminative metric depth",
      "offset": 2448.8,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "model is running. So we run it because",
      "offset": 2452.079,
      "duration": 6.161
    },
    {
      "lang": "en",
      "text": "it exists right such models I mentioned",
      "offset": 2455.04,
      "duration": 7.039
    },
    {
      "lang": "en",
      "text": "we run it we get an estimation of the",
      "offset": 2458.24,
      "duration": 6.72
    },
    {
      "lang": "en",
      "text": "everything in the scene their depth",
      "offset": 2462.079,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "estimations but those are metric",
      "offset": 2464.96,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "estimation but it is just not high",
      "offset": 2467.04,
      "duration": 5.279
    },
    {
      "lang": "en",
      "text": "resolution or you know kind of sharp",
      "offset": 2469.359,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "enough then we also generate the other",
      "offset": 2472.319,
      "duration": 6.161
    },
    {
      "lang": "en",
      "text": "one um is it clear you know there is",
      "offset": 2475.839,
      "duration": 5.041
    },
    {
      "lang": "en",
      "text": "such an algorithm already running and",
      "offset": 2478.48,
      "duration": 5.119
    },
    {
      "lang": "en",
      "text": "this sharp can use any of those you know",
      "offset": 2480.88,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "kind of this is an overall methodology",
      "offset": 2483.599,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "to incorporate like these two different",
      "offset": 2486.16,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "type of models.",
      "offset": 2488.8,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "We are not you know reinventing the",
      "offset": 2490.96,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "wheel but we are putting those wheels in",
      "offset": 2493.359,
      "duration": 5.521
    },
    {
      "lang": "en",
      "text": "a way that they would go you know align",
      "offset": 2496.4,
      "duration": 5.919
    },
    {
      "lang": "en",
      "text": "and whatever we are driving goes better",
      "offset": 2498.88,
      "duration": 6.479
    },
    {
      "lang": "en",
      "text": "faster. So you so you've got this you've",
      "offset": 2502.319,
      "duration": 6.081
    },
    {
      "lang": "en",
      "text": "got this coarse measurement that we know",
      "offset": 2505.359,
      "duration": 6.081
    },
    {
      "lang": "en",
      "text": "how to do and then you've got the fine",
      "offset": 2508.4,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "grained",
      "offset": 2511.44,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "uh relative thing from the generative",
      "offset": 2513.119,
      "duration": 9.441
    },
    {
      "lang": "en",
      "text": "side and the the idea is that the latter",
      "offset": 2516.88,
      "duration": 8.239
    },
    {
      "lang": "en",
      "text": "is good enough or or you know they're",
      "offset": 2522.56,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "each good enough so that together you",
      "offset": 2525.119,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "can get fine grained depth predictions",
      "offset": 2528,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "if you put them together in the right",
      "offset": 2531.119,
      "duration": 2
    },
    {
      "lang": "en",
      "text": "Right.",
      "offset": 2532.56,
      "duration": 3.039
    },
    {
      "lang": "en",
      "text": "Absolutely. And the thing that you",
      "offset": 2533.119,
      "duration": 5.841
    },
    {
      "lang": "en",
      "text": "mention right way is the big question.",
      "offset": 2535.599,
      "duration": 6.561
    },
    {
      "lang": "en",
      "text": "How I'm going to know that which one is",
      "offset": 2538.96,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "which? Because I don't know this the new",
      "offset": 2542.16,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "scene you know one is blurry the other",
      "offset": 2544.64,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "one is sharp. I don't know which one to",
      "offset": 2546.72,
      "duration": 6.56
    },
    {
      "lang": "en",
      "text": "trust. Um so what we did our intuition",
      "offset": 2548.88,
      "duration": 8.16
    },
    {
      "lang": "en",
      "text": "is we take these two estimations",
      "offset": 2553.28,
      "duration": 7.2
    },
    {
      "lang": "en",
      "text": "and then we compare them. we have this",
      "offset": 2557.04,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "adaptive substraction",
      "offset": 2560.48,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "scale substraction which generates a",
      "offset": 2562.96,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "difference map. So the intention is in",
      "offset": 2565.52,
      "duration": 6.88
    },
    {
      "lang": "en",
      "text": "this let's say difference map the",
      "offset": 2569.04,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "regions with minimal differences",
      "offset": 2572.4,
      "duration": 6.959
    },
    {
      "lang": "en",
      "text": "um uh are more reliable in terms of",
      "offset": 2575.52,
      "duration": 7.36
    },
    {
      "lang": "en",
      "text": "their metric depth estimation and while",
      "offset": 2579.359,
      "duration": 7.841
    },
    {
      "lang": "en",
      "text": "and other areas with larger uh kind of",
      "offset": 2582.88,
      "duration": 7.36
    },
    {
      "lang": "en",
      "text": "differences uh will require maybe",
      "offset": 2587.2,
      "duration": 7.119
    },
    {
      "lang": "en",
      "text": "updates. So this is actually kind of",
      "offset": 2590.24,
      "duration": 7.76
    },
    {
      "lang": "en",
      "text": "allowing us to I mean I we keep in mind",
      "offset": 2594.319,
      "duration": 6.241
    },
    {
      "lang": "en",
      "text": "that this a diffusion uh solution with",
      "offset": 2598,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "unit and unit when we run unit there is",
      "offset": 2600.56,
      "duration": 8.08
    },
    {
      "lang": "en",
      "text": "the input and uh for textic generation",
      "offset": 2604,
      "duration": 8.16
    },
    {
      "lang": "en",
      "text": "input is just gian noise in this case it",
      "offset": 2608.64,
      "duration": 8
    },
    {
      "lang": "en",
      "text": "is not in this case it is this",
      "offset": 2612.16,
      "duration": 7.12
    },
    {
      "lang": "en",
      "text": "let's say pure gashion noise it is",
      "offset": 2616.64,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "changing depending on the the difference",
      "offset": 2619.28,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "map and we have uh we are conditioning",
      "offset": 2621.52,
      "duration": 7.44
    },
    {
      "lang": "en",
      "text": "on this. So minimal differences we don't",
      "offset": 2624.72,
      "duration": 6.639
    },
    {
      "lang": "en",
      "text": "like the diffusion process to change",
      "offset": 2628.96,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "them big differences we like it to",
      "offset": 2631.359,
      "duration": 6.801
    },
    {
      "lang": "en",
      "text": "change and then uh we learn in training",
      "offset": 2633.52,
      "duration": 8.48
    },
    {
      "lang": "en",
      "text": "time a model that takes such difference",
      "offset": 2638.16,
      "duration": 8.56
    },
    {
      "lang": "en",
      "text": "map and makes the best use of it uh to",
      "offset": 2642,
      "duration": 7.599
    },
    {
      "lang": "en",
      "text": "uh refine itself. uh so there is fine",
      "offset": 2646.72,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "tuning going on in training time. So we",
      "offset": 2649.599,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "have one model we run the model we",
      "offset": 2652.72,
      "duration": 5.359
    },
    {
      "lang": "en",
      "text": "generate this a fine invariant depth but",
      "offset": 2655.359,
      "duration": 5.121
    },
    {
      "lang": "en",
      "text": "that is not good enough. we have the you",
      "offset": 2658.079,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "know kind of",
      "offset": 2660.48,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "metric dep. So we compute this",
      "offset": 2662.4,
      "duration": 5.679
    },
    {
      "lang": "en",
      "text": "difference. difference dictates how much",
      "offset": 2665.359,
      "duration": 7.041
    },
    {
      "lang": "en",
      "text": "diffusion is going to make a change in",
      "offset": 2668.079,
      "duration": 6.881
    },
    {
      "lang": "en",
      "text": "the depth estimations and in training",
      "offset": 2672.4,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "time. This is optimized because we have",
      "offset": 2674.96,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "you know kind of the uh even the low",
      "offset": 2677.119,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "resolution we have some metric",
      "offset": 2680.48,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "estimation and also uh we can use you",
      "offset": 2681.68,
      "duration": 7.28
    },
    {
      "lang": "en",
      "text": "know existing metric data set we also",
      "offset": 2684.64,
      "duration": 6.64
    },
    {
      "lang": "en",
      "text": "show in the paper that you don't really",
      "offset": 2688.96,
      "duration": 7.04
    },
    {
      "lang": "en",
      "text": "need a lot of it uh we are using maybe",
      "offset": 2691.28,
      "duration": 8.48
    },
    {
      "lang": "en",
      "text": "150 times smaller than the amount of",
      "offset": 2696,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "data used to train such discriminative",
      "offset": 2699.76,
      "duration": 5.839
    },
    {
      "lang": "en",
      "text": "models. So kind of in training time we",
      "offset": 2702.24,
      "duration": 6.879
    },
    {
      "lang": "en",
      "text": "learn a better diffusion model or can we",
      "offset": 2705.599,
      "duration": 6.081
    },
    {
      "lang": "en",
      "text": "just use it you know in in first time.",
      "offset": 2709.119,
      "duration": 6.161
    },
    {
      "lang": "en",
      "text": "So overall take a look at the paper it",
      "offset": 2711.68,
      "duration": 6.8
    },
    {
      "lang": "en",
      "text": "is accurate it is sort of you know in",
      "offset": 2715.28,
      "duration": 6.4
    },
    {
      "lang": "en",
      "text": "terms of the resolution but it is a",
      "offset": 2718.48,
      "duration": 6.72
    },
    {
      "lang": "en",
      "text": "symmetric estimation. Recently I've been",
      "offset": 2721.68,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "seeing a lot of interesting applications",
      "offset": 2725.2,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "of diffusion models uh beyond kind of",
      "offset": 2726.8,
      "duration": 6.64
    },
    {
      "lang": "en",
      "text": "the you know stable diffusion image you",
      "offset": 2730.16,
      "duration": 7.04
    },
    {
      "lang": "en",
      "text": "know text to image and they all seem to",
      "offset": 2733.44,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "revolve around creative ways to",
      "offset": 2737.2,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "manipulate the noise to do interesting",
      "offset": 2739.68,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "things and so this is that same kind of",
      "offset": 2742.88,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "idea. Yes, in this case we are",
      "offset": 2745.76,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "controlling the noise. You are",
      "offset": 2748,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "absolutely right. By the way, you can we",
      "offset": 2750.079,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "can use diffusion",
      "offset": 2752.079,
      "duration": 6.641
    },
    {
      "lang": "en",
      "text": "uh framework to make a robot uh planning",
      "offset": 2754,
      "duration": 7.2
    },
    {
      "lang": "en",
      "text": "you know or trajectory estimation or",
      "offset": 2758.72,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "solving a puzzle. Diffusion has many",
      "offset": 2761.2,
      "duration": 5.119
    },
    {
      "lang": "en",
      "text": "applications. In this case we are",
      "offset": 2764.72,
      "duration": 3.119
    },
    {
      "lang": "en",
      "text": "controlling to know it. You are right.",
      "offset": 2766.319,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "Uh ultimately is this supervised or",
      "offset": 2767.839,
      "duration": 5.041
    },
    {
      "lang": "en",
      "text": "totally self-supervised?",
      "offset": 2770.56,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "self-supervised because we have this uh",
      "offset": 2772.88,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "metric depth model even though low",
      "offset": 2775.92,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "resolution we can in the training time",
      "offset": 2778.4,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "we can just use to learn you know a",
      "offset": 2780.8,
      "duration": 6.559
    },
    {
      "lang": "en",
      "text": "better uh shark net but we this thing",
      "offset": 2784.4,
      "duration": 6.32
    },
    {
      "lang": "en",
      "text": "can also like I said can use leverage",
      "offset": 2787.359,
      "duration": 8.161
    },
    {
      "lang": "en",
      "text": "much less uh supervised data so we can",
      "offset": 2790.72,
      "duration": 8
    },
    {
      "lang": "en",
      "text": "do SFT supervised finetuning and we show",
      "offset": 2795.52,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "you know the difference in the paper you",
      "offset": 2798.72,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "know how they will compare both of them",
      "offset": 2800.319,
      "duration": 5.361
    },
    {
      "lang": "en",
      "text": "are still know providing very sharp uh",
      "offset": 2802.64,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "depth maps, monocular depth maps but",
      "offset": 2805.68,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "metric also accurate depth maps.",
      "offset": 2807.92,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "So you show",
      "offset": 2810.56,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "uh both supervised and unsupervised",
      "offset": 2812.72,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "versions of the the model.",
      "offset": 2816.8,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "Exactly. And uh the reason we showed",
      "offset": 2818.72,
      "duration": 5.599
    },
    {
      "lang": "en",
      "text": "both of them so people can go you know",
      "offset": 2821.76,
      "duration": 5.599
    },
    {
      "lang": "en",
      "text": "if they design their own metric de they",
      "offset": 2824.319,
      "duration": 5.441
    },
    {
      "lang": "en",
      "text": "can also incorporate in this framework",
      "offset": 2827.359,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "leveraging the ideas that we talk about",
      "offset": 2829.76,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "in the paper and then they may come up",
      "offset": 2831.76,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "with even a better version of you know",
      "offset": 2833.92,
      "duration": 6.88
    },
    {
      "lang": "en",
      "text": "kind of sharp depth the idea is same you",
      "offset": 2837.28,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "know kind of I mean this paper",
      "offset": 2840.8,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "facilities further research in that",
      "offset": 2842.8,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "sense.",
      "offset": 2845.119,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "Very cool. Are there additional areas",
      "offset": 2846,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "you foresee in terms of future research",
      "offset": 2848.48,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "uh along these lines?",
      "offset": 2850.8,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "Of course, uh this is molecular depth",
      "offset": 2852.319,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "estimation, but you don't do monocular",
      "offset": 2854.4,
      "duration": 5.439
    },
    {
      "lang": "en",
      "text": "only if you have multiple cameras or if",
      "offset": 2857.28,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "you have video, right? You can do",
      "offset": 2859.839,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "structure from motion if there's video",
      "offset": 2862.079,
      "duration": 6.081
    },
    {
      "lang": "en",
      "text": "or uh multi- view stereo or multiv- view",
      "offset": 2864.72,
      "duration": 6.399
    },
    {
      "lang": "en",
      "text": "depth estimation. So but overall idea",
      "offset": 2868.16,
      "duration": 6.72
    },
    {
      "lang": "en",
      "text": "would be same um at the mechanisms like",
      "offset": 2871.119,
      "duration": 6.881
    },
    {
      "lang": "en",
      "text": "how this noise aware gate thing for",
      "offset": 2874.88,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "using the difference map is explained in",
      "offset": 2878,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "the paper I didn't go into those details",
      "offset": 2880.8,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "maybe little bit met heavy and I may",
      "offset": 2883.28,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "need a board to",
      "offset": 2885.599,
      "duration": 5.441
    },
    {
      "lang": "en",
      "text": "kind of um there might be",
      "offset": 2887.68,
      "duration": 7.84
    },
    {
      "lang": "en",
      "text": "of course extensions to uh video and",
      "offset": 2891.04,
      "duration": 7.76
    },
    {
      "lang": "en",
      "text": "multiv- view uh as future work we are",
      "offset": 2895.52,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "actually looking into that and also",
      "offset": 2898.8,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "maybe kind of alternative ways of",
      "offset": 2900.48,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "computing these uh noise maps and cross",
      "offset": 2902.96,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "attending them into the model. Yeah,",
      "offset": 2906.48,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "this is active research you know kind of",
      "offset": 2908.96,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "we try things sometimes they generate",
      "offset": 2911.04,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "amazing results sometimes you know kind",
      "offset": 2913.52,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "of they to learn and you know refine the",
      "offset": 2915.2,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "idea.",
      "offset": 2917.68,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "Great. So",
      "offset": 2919.28,
      "duration": 8
    },
    {
      "lang": "en",
      "text": "uh in past years we've talked about uh",
      "offset": 2921.599,
      "duration": 9.601
    },
    {
      "lang": "en",
      "text": "you know a a broad set of Qualcomm's",
      "offset": 2927.28,
      "duration": 6.559
    },
    {
      "lang": "en",
      "text": "research and demos and workshops and",
      "offset": 2931.2,
      "duration": 6.56
    },
    {
      "lang": "en",
      "text": "tutorials in our CVPR show uh and",
      "offset": 2933.839,
      "duration": 6.321
    },
    {
      "lang": "en",
      "text": "touched on a a bunch of the individual",
      "offset": 2937.76,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "papers. This time we wanted to go deep",
      "offset": 2940.16,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "into",
      "offset": 2942.4,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "uh a couple of papers but there's still",
      "offset": 2944.079,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "a lot of other you know papers as well",
      "offset": 2946.72,
      "duration": 6.639
    },
    {
      "lang": "en",
      "text": "as activities that you presented at the",
      "offset": 2949.119,
      "duration": 7.601
    },
    {
      "lang": "en",
      "text": "conference. Um yeah let's maybe choose a",
      "offset": 2953.359,
      "duration": 6.561
    },
    {
      "lang": "en",
      "text": "few of those to touch on and then we'll",
      "offset": 2956.72,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "refer folks to your blog your blog post",
      "offset": 2959.92,
      "duration": 6.639
    },
    {
      "lang": "en",
      "text": "to get the full picture. um you know",
      "offset": 2962.72,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "what did you do for demos this time",
      "offset": 2966.559,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "in addition to 11 papers you know we had",
      "offset": 2969.2,
      "duration": 6.56
    },
    {
      "lang": "en",
      "text": "uh maybe more than 10 demos with me you",
      "offset": 2972.559,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "know kind of talk about three of them",
      "offset": 2975.76,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "the first one is going to be text to 3D",
      "offset": 2978.559,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "demo the second one is going to be",
      "offset": 2981.2,
      "duration": 6.8
    },
    {
      "lang": "en",
      "text": "either video to video or image to video",
      "offset": 2983.839,
      "duration": 7.361
    },
    {
      "lang": "en",
      "text": "geneti demo these are all multi demos",
      "offset": 2988,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "running on device and the third one is",
      "offset": 2991.2,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "going to be multimodel vqa Okay, if you",
      "offset": 2992.64,
      "duration": 7.28
    },
    {
      "lang": "en",
      "text": "have time. Um, text to 3D uh is a model",
      "offset": 2995.44,
      "duration": 7.76
    },
    {
      "lang": "en",
      "text": "literally user speaks through audio",
      "offset": 2999.92,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "interface or text prompt and you",
      "offset": 3003.2,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "describe an object like a cactus sword,",
      "offset": 3005.28,
      "duration": 5.279
    },
    {
      "lang": "en",
      "text": "you know, kind of a hippo wearing a",
      "offset": 3008.4,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "sweater, you know, anything you can",
      "offset": 3010.559,
      "duration": 6.481
    },
    {
      "lang": "en",
      "text": "imagine. Then it will generate on device",
      "offset": 3012.4,
      "duration": 8
    },
    {
      "lang": "en",
      "text": "a 3D mesh and also associated texture",
      "offset": 3017.04,
      "duration": 7.44
    },
    {
      "lang": "en",
      "text": "map like color everything in less than 3",
      "offset": 3020.4,
      "duration": 6.56
    },
    {
      "lang": "en",
      "text": "seconds. I mean such models we are not",
      "offset": 3024.48,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "the first to come up with such model but",
      "offset": 3026.96,
      "duration": 5.599
    },
    {
      "lang": "en",
      "text": "in terms of the existing models comp",
      "offset": 3029.68,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "computational complexity there is an",
      "offset": 3032.559,
      "duration": 7.361
    },
    {
      "lang": "en",
      "text": "amazing model called as MV dream uh yeah",
      "offset": 3034.559,
      "duration": 8.721
    },
    {
      "lang": "en",
      "text": "it takes 194",
      "offset": 3039.92,
      "duration": 6.56
    },
    {
      "lang": "en",
      "text": "minutes not seconds",
      "offset": 3043.28,
      "duration": 6.72
    },
    {
      "lang": "en",
      "text": "and there's dream fusion I mean maybe",
      "offset": 3046.48,
      "duration": 6.32
    },
    {
      "lang": "en",
      "text": "quality wise you know maybe we are much",
      "offset": 3050,
      "duration": 7.44
    },
    {
      "lang": "en",
      "text": "better it takes 22 minutes you know kind",
      "offset": 3052.8,
      "duration": 7.2
    },
    {
      "lang": "en",
      "text": "of and now we are saying that hey you",
      "offset": 3057.44,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "can do it on your phone in 3 seconds and",
      "offset": 3060,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "the demo uh let me describe the demo so",
      "offset": 3062.96,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "it is a game video game and then the",
      "offset": 3066.24,
      "duration": 6.079
    },
    {
      "lang": "en",
      "text": "player goes into this table and then",
      "offset": 3068.4,
      "duration": 7.76
    },
    {
      "lang": "en",
      "text": "This this a game where you know the uh",
      "offset": 3072.319,
      "duration": 6.081
    },
    {
      "lang": "en",
      "text": "person is fighting with other things in",
      "offset": 3076.16,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "the scene you know kind of like monsters",
      "offset": 3078.4,
      "duration": 4.959
    },
    {
      "lang": "en",
      "text": "and other people and it says it",
      "offset": 3080.88,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "describes it weapon maybe a battle or",
      "offset": 3083.359,
      "duration": 6.081
    },
    {
      "lang": "en",
      "text": "sword s and that's why I mentioned about",
      "offset": 3086.4,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "you know kind of a dragon or cactus",
      "offset": 3089.44,
      "duration": 6.96
    },
    {
      "lang": "en",
      "text": "sword or watermelon sword you know then",
      "offset": 3092.24,
      "duration": 7.76
    },
    {
      "lang": "en",
      "text": "it generates things accordingly in less",
      "offset": 3096.4,
      "duration": 5.679
    },
    {
      "lang": "en",
      "text": "than 3 seconds than the person grabs it",
      "offset": 3100,
      "duration": 5.599
    },
    {
      "lang": "en",
      "text": "and then uh continue playing killing the",
      "offset": 3102.079,
      "duration": 5.441
    },
    {
      "lang": "en",
      "text": "monsters in the scene. That was the demo",
      "offset": 3105.599,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "we were explicitly showing. But the",
      "offset": 3107.52,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "thing that it can generate anything and",
      "offset": 3109.92,
      "duration": 6.399
    },
    {
      "lang": "en",
      "text": "uh this is world first uh on device",
      "offset": 3112.88,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "takes to 3D demo literally the first",
      "offset": 3116.319,
      "duration": 5.921
    },
    {
      "lang": "en",
      "text": "time we also showed and it allows",
      "offset": 3118.96,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "offline personalization at the edge. You",
      "offset": 3122.24,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "know it uses diffusion models again uh",
      "offset": 3124.88,
      "duration": 6.479
    },
    {
      "lang": "en",
      "text": "to image generation in particular SSD.",
      "offset": 3127.68,
      "duration": 6.159
    },
    {
      "lang": "en",
      "text": "There is SDXL version of it as well.",
      "offset": 3131.359,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "Both of them generate high resolution",
      "offset": 3133.839,
      "duration": 5.361
    },
    {
      "lang": "en",
      "text": "data. So it geni either one image or six",
      "offset": 3135.92,
      "duration": 7.36
    },
    {
      "lang": "en",
      "text": "images multiv- view. Then we run another",
      "offset": 3139.2,
      "duration": 7.52
    },
    {
      "lang": "en",
      "text": "unit you know another SSD to take it and",
      "offset": 3143.28,
      "duration": 7.839
    },
    {
      "lang": "en",
      "text": "map it to a mesh. That's the text to 3D",
      "offset": 3146.72,
      "duration": 8.48
    },
    {
      "lang": "en",
      "text": "for image to video and uh video to",
      "offset": 3151.119,
      "duration": 6.561
    },
    {
      "lang": "en",
      "text": "video. There are two genti video",
      "offset": 3155.2,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "generation demos. Uh we showed both of",
      "offset": 3157.68,
      "duration": 7.12
    },
    {
      "lang": "en",
      "text": "them next to each other. Uh so video to",
      "offset": 3160.88,
      "duration": 6.719
    },
    {
      "lang": "en",
      "text": "video is like video editing or",
      "offset": 3164.8,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "stylization demo. Uh it takes text as an",
      "offset": 3167.599,
      "duration": 6.72
    },
    {
      "lang": "en",
      "text": "input uh conditioning. It could be let's",
      "offset": 3171.28,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "say I have my uh you know selfie and",
      "offset": 3174.319,
      "duration": 7.52
    },
    {
      "lang": "en",
      "text": "then I see that oh change it to um let's",
      "offset": 3177.52,
      "duration": 7.92
    },
    {
      "lang": "en",
      "text": "say pencil drawing style or m painting",
      "offset": 3181.839,
      "duration": 6.561
    },
    {
      "lang": "en",
      "text": "style but it is not about faces it could",
      "offset": 3185.44,
      "duration": 6.639
    },
    {
      "lang": "en",
      "text": "be anything any object any scene uh or",
      "offset": 3188.4,
      "duration": 6.8
    },
    {
      "lang": "en",
      "text": "make it look like Albert Einstein or you",
      "offset": 3192.079,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "know kind of looks zombie you know kind",
      "offset": 3195.2,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "of the challenge of course how to do it",
      "offset": 3197.28,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "in a way that it is temporally",
      "offset": 3200.079,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "consistent. So this is another unit",
      "offset": 3201.839,
      "duration": 6.561
    },
    {
      "lang": "en",
      "text": "based model. Uh it runs uh quite",
      "offset": 3205.359,
      "duration": 6.72
    },
    {
      "lang": "en",
      "text": "efficiently on device. It runs at 12 uh",
      "offset": 3208.4,
      "duration": 6.719
    },
    {
      "lang": "en",
      "text": "frame per second speed generating 512 by",
      "offset": 3212.079,
      "duration": 8.881
    },
    {
      "lang": "en",
      "text": "384 video frames. So um kind of is a",
      "offset": 3215.119,
      "duration": 8.48
    },
    {
      "lang": "en",
      "text": "kind of example I can say that when we",
      "offset": 3220.96,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "started this model original model was",
      "offset": 3223.599,
      "duration": 7.681
    },
    {
      "lang": "en",
      "text": "taking 7 seconds um um",
      "offset": 3226.8,
      "duration": 6.319
    },
    {
      "lang": "en",
      "text": "and there's an existing model you know",
      "offset": 3231.28,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "kind of uh someone outside Qualcomm",
      "offset": 3233.119,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "developed it when we put it on device it",
      "offset": 3235.52,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "was 7 seconds but after we optimize uh",
      "offset": 3237.76,
      "duration": 5.599
    },
    {
      "lang": "en",
      "text": "and I can talk about how it's done uh",
      "offset": 3240.88,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "now it is running around 80 millisecond",
      "offset": 3243.359,
      "duration": 6.321
    },
    {
      "lang": "en",
      "text": "like this is mic it up. Yeah, this is",
      "offset": 3246.48,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "the video video styization in which",
      "offset": 3249.68,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "video is even more aggressive. When we",
      "offset": 3252.24,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "started, you know, kind of uh it was",
      "offset": 3254.72,
      "duration": 6.72
    },
    {
      "lang": "en",
      "text": "taking 2200 teraflops",
      "offset": 3257.92,
      "duration": 6.88
    },
    {
      "lang": "en",
      "text": "we decreased more than 500 times. The",
      "offset": 3261.44,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "how does that uh compare in terms of",
      "offset": 3264.8,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "time?",
      "offset": 3267.2,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "Time the original model even didn't fit",
      "offset": 3268.8,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "out of memory and very slow. And I",
      "offset": 3271.359,
      "duration": 5.041
    },
    {
      "lang": "en",
      "text": "remember playing with uh one of the",
      "offset": 3273.92,
      "duration": 6.8
    },
    {
      "lang": "en",
      "text": "demos at uh the Google event. I think it",
      "offset": 3276.4,
      "duration": 9.28
    },
    {
      "lang": "en",
      "text": "was their new Veo3 model. And uh image",
      "offset": 3280.72,
      "duration": 8.08
    },
    {
      "lang": "en",
      "text": "to video in particular. And I mean it",
      "offset": 3285.68,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "looked great, but it took a really long",
      "offset": 3288.8,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "time. Like it was, you know, go away and",
      "offset": 3290.72,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "we'll send you a notification when it's",
      "offset": 3292.64,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "done kind of time frames.",
      "offset": 3294.319,
      "duration": 5.441
    },
    {
      "lang": "en",
      "text": "uh to to hear this happening on device",
      "offset": 3297.359,
      "duration": 5.361
    },
    {
      "lang": "en",
      "text": "is a whole whole new idea",
      "offset": 3299.76,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "and it is happening almost real time",
      "offset": 3302.72,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "that kind of",
      "offset": 3304.72,
      "duration": 3.119
    },
    {
      "lang": "en",
      "text": "that's crazy. Now the images it sounds",
      "offset": 3305.44,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "like they're a little smaller but uh",
      "offset": 3307.839,
      "duration": 5.121
    },
    {
      "lang": "en",
      "text": "image the video demo we had it is",
      "offset": 3310.4,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "generating 1024 512 it's a funny",
      "offset": 3312.96,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "resolution but it is quite high",
      "offset": 3315.76,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "resolution actually without any video",
      "offset": 3318.16,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "frame interpolation we can go to you",
      "offset": 3320.64,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "know very fast interpolation to increase",
      "offset": 3322.88,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "the frame rate but without that or we",
      "offset": 3325.119,
      "duration": 5.121
    },
    {
      "lang": "en",
      "text": "can generate two seconds of video in 3",
      "offset": 3327.76,
      "duration": 6.079
    },
    {
      "lang": "en",
      "text": "seconds. So you can uh imagine you know",
      "offset": 3330.24,
      "duration": 8.079
    },
    {
      "lang": "en",
      "text": "this is very fast and uh yeah uh but but",
      "offset": 3333.839,
      "duration": 6.72
    },
    {
      "lang": "en",
      "text": "the challenge is again these models are",
      "offset": 3338.319,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "big models they are comput intensive how",
      "offset": 3340.559,
      "duration": 5.921
    },
    {
      "lang": "en",
      "text": "to get it down and run efficiently on",
      "offset": 3343.2,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "device that I think qualcom secret",
      "offset": 3346.48,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "source I mean many things we do are",
      "offset": 3349.04,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "aiming for power efficiency know speed",
      "offset": 3351.76,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "memory efficiency making sure people can",
      "offset": 3355.2,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "use on their own device they don't need",
      "offset": 3357.68,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "cloud connection or anything like that",
      "offset": 3359.44,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "to generate such know or content. Yeah.",
      "offset": 3361.119,
      "duration": 6.96
    },
    {
      "lang": "en",
      "text": "Very good. Uh so when we're back here in",
      "offset": 3364.88,
      "duration": 6.239
    },
    {
      "lang": "en",
      "text": "about a year, what should uh we expect",
      "offset": 3368.079,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "to hear from you in terms of things that",
      "offset": 3371.119,
      "duration": 5.121
    },
    {
      "lang": "en",
      "text": "were hot in the next 12 months in",
      "offset": 3373.359,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "computer vision? What are you excited",
      "offset": 3376.24,
      "duration": 2.24
    },
    {
      "lang": "en",
      "text": "about?",
      "offset": 3377.68,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "There are two things, two",
      "offset": 3378.48,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "wild cards, you know, I can say that,",
      "offset": 3382.4,
      "duration": 4.959
    },
    {
      "lang": "en",
      "text": "you know, I'm betting on them. One is",
      "offset": 3384.48,
      "duration": 5.359
    },
    {
      "lang": "en",
      "text": "Yeah. uh if it that's the correct term",
      "offset": 3387.359,
      "duration": 5.601
    },
    {
      "lang": "en",
      "text": "you know kind of or to joker I mean two",
      "offset": 3389.839,
      "duration": 5.121
    },
    {
      "lang": "en",
      "text": "things I'm really excited about it one",
      "offset": 3392.96,
      "duration": 6.96
    },
    {
      "lang": "en",
      "text": "is aantic AI extending to visual content",
      "offset": 3394.96,
      "duration": 7.76
    },
    {
      "lang": "en",
      "text": "the second one is visual reasoning",
      "offset": 3399.92,
      "duration": 7.439
    },
    {
      "lang": "en",
      "text": "models so the agent AI is I mean system",
      "offset": 3402.72,
      "duration": 7.359
    },
    {
      "lang": "en",
      "text": "capable of autonomous planning reasoning",
      "offset": 3407.359,
      "duration": 5.601
    },
    {
      "lang": "en",
      "text": "and interacting with environment and",
      "offset": 3410.079,
      "duration": 6.881
    },
    {
      "lang": "en",
      "text": "user multi-turn you like AI assistant",
      "offset": 3412.96,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "but Then such systems for instance it is",
      "offset": 3416.96,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "running on XR glass you know Ray-B band",
      "offset": 3419.2,
      "duration": 7.2
    },
    {
      "lang": "en",
      "text": "and then you are you know giving it okay",
      "offset": 3422.24,
      "duration": 7.359
    },
    {
      "lang": "en",
      "text": "make a dinner reservation for the people",
      "offset": 3426.4,
      "duration": 5.439
    },
    {
      "lang": "en",
      "text": "in this scene based on their",
      "offset": 3429.599,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "preferences.",
      "offset": 3431.839,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "This is literally the question and now",
      "offset": 3433.599,
      "duration": 5.361
    },
    {
      "lang": "en",
      "text": "the system has to go understand who are",
      "offset": 3436.4,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "those people based on you know exchanges",
      "offset": 3438.96,
      "duration": 6.639
    },
    {
      "lang": "en",
      "text": "like text or emails or anything we did",
      "offset": 3442.4,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "together before understand their you",
      "offset": 3445.599,
      "duration": 5.841
    },
    {
      "lang": "en",
      "text": "know preferences in food you know what",
      "offset": 3448.88,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "kind of cuisine they like Italian or",
      "offset": 3451.44,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "sushi or something like that and then go",
      "offset": 3453.52,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "and make a find a restaurant and then",
      "offset": 3456,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "make a booking based on their",
      "offset": 3458.96,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "availability it has to access their you",
      "offset": 3460.48,
      "duration": 5.599
    },
    {
      "lang": "en",
      "text": "know kind let's say calendars. So this",
      "offset": 3462.559,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "is happening. We had an agentic AI demo.",
      "offset": 3466.079,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "Actually I didn't mention components are",
      "offset": 3468.559,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "there. So but it is now using I think",
      "offset": 3470.64,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "next year we will see a lot of things",
      "offset": 3472.96,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "through screenshots or through cameras.",
      "offset": 3474.96,
      "duration": 5.599
    },
    {
      "lang": "en",
      "text": "Such content will be a part of agentic",
      "offset": 3477.359,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "AI. Second one is reasoning models. You",
      "offset": 3480.559,
      "duration": 6.961
    },
    {
      "lang": "en",
      "text": "may remember Sam Dipsik made a huge you",
      "offset": 3483.599,
      "duration": 6.641
    },
    {
      "lang": "en",
      "text": "know kind of impact on how people are",
      "offset": 3487.52,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "training models. They said that",
      "offset": 3490.24,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "okay you don't need huge compute to",
      "offset": 3491.76,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "train models and these models now are",
      "offset": 3494.16,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "going to smaller models are going to",
      "offset": 3496.64,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "perform as good as big models because",
      "offset": 3498.799,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "what they do in inference time they",
      "offset": 3501.2,
      "duration": 6.8
    },
    {
      "lang": "en",
      "text": "don't just give an answer but they think",
      "offset": 3504.079,
      "duration": 6.881
    },
    {
      "lang": "en",
      "text": "I in a way that they generate tokens",
      "offset": 3508,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "respond respond and they say oh wait a",
      "offset": 3510.96,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "minute maybe there is another train of",
      "offset": 3513.52,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "thought maybe I will follow it so this",
      "offset": 3515.68,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "is like chain of thought models of",
      "offset": 3517.52,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "thought models or graph of thought",
      "offset": 3520.4,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "there's a lot of XT of thought models",
      "offset": 3522.4,
      "duration": 6.8
    },
    {
      "lang": "en",
      "text": "happening but now what I see that they",
      "offset": 3526.48,
      "duration": 6.079
    },
    {
      "lang": "en",
      "text": "are extending to visual data it's",
      "offset": 3529.2,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "already happening there's multimodel",
      "offset": 3532.559,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "coot there is compositional coot you",
      "offset": 3534.4,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "know",
      "offset": 3537.359,
      "duration": 3.281
    },
    {
      "lang": "en",
      "text": "many things are coming we are looking",
      "offset": 3538.96,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "into that what does it mean for visual",
      "offset": 3540.64,
      "duration": 7.04
    },
    {
      "lang": "en",
      "text": "data uh to do reasoning so I think it's",
      "offset": 3543.44,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "going to be the other area very exciting",
      "offset": 3547.68,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "to",
      "offset": 3549.52,
      "duration": 2.559
    },
    {
      "lang": "en",
      "text": "We will see who is going to get best",
      "offset": 3550.48,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "paper, you know, in those two area.",
      "offset": 3552.079,
      "duration": 5.361
    },
    {
      "lang": "en",
      "text": "Awesome. Awesome. Very good. Well, Fati,",
      "offset": 3554.319,
      "duration": 6.161
    },
    {
      "lang": "en",
      "text": "uh thank you once again for taking the",
      "offset": 3557.44,
      "duration": 5.679
    },
    {
      "lang": "en",
      "text": "time to share uh what you've been up to",
      "offset": 3560.48,
      "duration": 5.599
    },
    {
      "lang": "en",
      "text": "and uh some of your team's papers from",
      "offset": 3563.119,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "CVPR. It's great to catch up.",
      "offset": 3566.079,
      "duration": 5.601
    },
    {
      "lang": "en",
      "text": "It was a real pleasure. I'm very excited",
      "offset": 3568.559,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "about what we do and know it's always",
      "offset": 3571.68,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "great to talk to you, Sam. I love this",
      "offset": 3574.319,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "podcast. Oh,",
      "offset": 3576.4,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "thanks so much.",
      "offset": 3577.92,
      "duration": 3.04
    }
  ],
  "cleanText": "The word knowledge representation capability of LLMs act as a regularizer or conditioner to generalize the solution. So we don't need to really go every time for each long tail specific very rare scenario and try to learn them model them. We want to harness the LLM's word knowledge with the efficiency of this you know vision based low-level perception st.\n\nAll right everyone welcome to another episode of the Twinball AI podcast. I am of course your host Sam Charington. Today I'm joined by Fatih Porikli. Fatih is senior director of technology at Qualcomm. Before we get going, be sure to take a moment to hit that subscribe button wherever you're listening to today's show. Fatih, welcome back to the podcast. You're somewhat of a veteran with us.\n\nThank you so much, Sam. I I really love this podcast and I'm I'm very happy to be back.\n\nI'm looking forward to our chat. It's been just about a year since we last spoke and we are back once again to dig into Qualcomm's papers from this year's CVPR conference and in particular we'll be digging into a couple of them. The DiMA paper which looks at distilling multimodal LLMs in an autonomous driving context as well as the SharpDepth paper which looks at diffusion distillation for computing absolute depth maps and some of the new capabilities that that unlocks which kind of speaks to multimodal foundation models. Of course, another unifying thread that is often present in my conversations with you and your colleagues at Qualcomm AI Research is the idea of efficiency and distillation. Um, and that's a key theme in these papers. But, uh, I guess let's jump in and and talk through the DiMA paper. The the full title of that paper is distilling multimodal large language models for autonomous driving. Maybe let's start with kind of your broad take on the state-of-the-art for autonomous driving and in particular the shift towards endtoend autonomous vehicular systems.\n\nDiMA is about autonomous driving as you just mentioned and specifically it aims to provide safe motion planning in longtail scenarios rare events and those rare events are very critical events because those represents accidents scenarios as well. So we don't want them to have right previous trend it was mostly modular systems they sequentially and independently trained models components and the reason for that there are kind of smaller data sets and for instance for object detection vehicle detection lane detection lane marking detection road detection road segmentation so there are small data sets and it's easy to maybe train such component then you know kind of build a larger system. But when we do that it is you know kind of easy to train such things quickly. But uh since we are now focusing at each module uh their own performance uh goals for better segmentation, better detection, better tracking. It doesn't mean that at the end you know all together they will work in harmony and our goal is to drive safe right drive our destination I mean when we are driving in our brain we don't really segment the things detect the things or train out track them explicitly but so this is the shortcoming of such uh approaches and end to end now is a breakthrough in a way that training objective applies to all components at the same time. So we are not really focusing on test specifically trained modules on limited data set but now you know kind of we are trying to optimize everything all the processing with the end goal in mind that's why it is called end to end.\n\nFrequent listeners to the podcast will know that this idea of you know physics-based versus modelbased or modular versus end to end is kind of a battle that's been raging in the uh machine learning community and the these various communities like autonomous driving and robotics for quite some time. We've been talking about related themes for many years here on the podcast. There's a big reason for that because end to end systems doesn't only offer better more robust solutions for you know wide variety of longtail scenarios but also uh they are uh better in terms of interpretability and they their KPIs you know kind of are much better. So DiMA is an end to end solution. Um, it establishes the new state-of-the-art, literally 25 state-of-the-art for autonomous driving and AI.\n\nCan I pause you there because you said something that um when I think about modular versus end to end or when I reflect on the conversations that I've had with folks uh in which this comes up the it's usually positioned as given enough data we think it will perform better because we're optimizing against the thing that we really care about. But modular is still useful because of explanability and interpretability because you can look at you know the output of each of these modules and use that to get some intuition about why decisions are happening at the planning layer. But you just said that end to end system has some interpretability advantages. Can you explain that?\n\nThat interpretability is more the one that I implied is semantic interpretability. For instance, we have AI planner. There is a planner at the end, right? We do all uh such low-level perception things that I mentioned to make a decision to whether accelerate, slow down, change lanes, you know, or turn those type of things. And when you are sitting in a vehicle you know kind of yes uh like maybe modular system can tell you how many people are in the scene or you know kind of what are the locations through the positions of the kind of other vehicles but kind of when it is it decides to change the lane you know you don't know why it's happening it is very stressful experience if you do not know why vehicle is making such decisions. So advanced systems and dimma in particular the way that we kind of like incorporated the language model into overall system provides explanation. Look I'm slowing down because there is a congestion ahead or we are approaching a zebra crossing those type of things. So analogous to explaining uh thought traces in a reasoning model the the model can to some degree explain what it's doing.\n\nYeah. High level explanation useful explanations. But you are right also you know kind of like the modular system uh we know kind of their detection results for for instance the thin advanced system sometimes they do not need to you know reveal that because maybe that is not critical at that point and that's why I described it as a bit of a battle uh\n\nokay\n\nyeah I'm trying to be like a judge but you know honestly I'm more kind of supporting at the end at this point.\n\nYeah. All right. Uh so sorry I interrupted you. You were talking about the I was talking about Dimma's kind of position advantages. So for dimma uh it I mentioned it is the new state-of-the-art. It can reduce the collision rate which is a very important KPI for autonomous driving system 80%. with respect to the 2024 or latest 25 baseline uh B sort solution um so it is a big improvement and also 40% reduction in for instance waypoint trajectory estimation another KPI uh let's say I know how vehicles should drive how close this planner making better decision so that is also you know kind of a 40% reduction in the error rate so and more than 40% improvement in accuracy for long tail. So uh on in the paper we show uh it's a long paper uh and we have many results comparisons and uh uh you know kind of I invite everyone to take a look at that. So it says the new sort we compared with the previous sort including BAD vectorized autonomous driving which is an amazing paper also and uh this NEMA can also do the things that there are in the training data for instance there was a zero shot scenario about three point turn we didn't have any such example for but then when we gave it for testing such examples it was uh capable of you know kind of doing the right planning and then I mentioned that it can answer questions or the system can proactively you know kind of system prompted and then provide explanations while it is driving itself you know it can tell you why it is making such decision so overall it is know there's a low-level perception state there's an end to end planner AI planner state they work together it integrates this all the perception coming from the cameras and other sensors uh the vision based front end lowlevel perception into this language model LLM based you know kind of AI planner this improves generalizability it improves the robustness it is now the explanations are semantically grounded and also we have two versions you know we can run this uh uh AI planner a transformer-based model still leveraging the language model or language model itself you know as the you know overall AI planner. So both of them are in the paper. Both of them are better than the you know existing sort of\n\nis the transformerbased model the distilled model\n\nthat is the distilled model right we are using model to distill it. Yeah.\n\nWhen I think about the idea of using LLMs in an autonomous driving context, the you know the elephant in the room if you will is that these things are slow right inference for LLMs is very expensive and um I'm presuming that's where the distillation in dimma comes in you know kind of\n\nyes absolutely you have a very good point lance At this uh point of course we are talking about the systems that we can run on the vehicle not like on a very expensive H100 you know uh GPU that would be more expensive than the vehicle itself. So uh using this not um reasonable price accelerators uh the token rates are you know not that high maybe I can say that depending on the model size of the model anywhere from 30 token per second to maybe a thousand token per second but running everything end to end using an LLM would be much more comput intensive than the transformer model that's why we have the transformer models We are saying well if you are interested in efficiency here is a transformer model still leverage this tilt using the language model but there is also a language model version of it a multimodel version of it you are right let me take a step back and kind of recap where we are so uh we've got this autonomous driving problem we're moving quickly towards autonomous driving solutions a promising direction for autonomous driving is to introduce LLMs into the autonomous driving loop. Why even LLM given? We, you know, we kind of jumped ahead and talked a little bit about the challenges or the cost of LLM. Like what is the role of L of an LLM in this loop and why do we even want it? Like when I think about when I'm driving, I'm not verbalizing questions to myself about what I'm seeing or what I'm doing. Why do we think an LLM should even be part of uh an AV system?\n\nYeah, that's that's a good question. We were also asking the question this question to ourselves. And the reason we want to use LLMs is that they represent world knowledge many things. So they capture all the knowledge or most of the knowledge. Yes, they hallucinate at the end when they are generating responses, but they are much better than the things that we manually crafted. So there is something there that would make it richer. The word knowledge representation capability of LLMs act as a regularizer or you know conditioner uh to generalize the solution. So we don't need to really go every time for each uh long tail specific very rare scenario and try to learn them model them. We rely we want to take advantage of this word knowledge of the LLMs to do it automatically. So that's why we want to harness the LLM's word knowledge with the efficiency of this you know vision based low-level perception stick and so is that world knowledge access via traditional prompting or are we doing talking about doing something like taking the embeddings of an LLM and somehow directly accessing them uh to to unlock this world knowledge. By the way, Dimma is not the first advanced solution. There is Uniad and VA. I mean, it's, you know, let's say big brothers, but we do, I think, better than them by incorporating lamps there. So, those models they try to represent the scene in terms of vectors and replacing this, you know, kind of dense representations to reduce the cost and also, you know, kind of like provide better improved capability. So there is this thing called as scene representation and I will kind of we can also consider it as tokenizing tokens are very common term for language models lms so what goes into those models separately let's kind of think them as tokens and we have visual data and this visual data is coming from multiple cameras six cameras eight cameras and for each camera or better we are taking them and lowlevel perception step projecting them into a bird's eye view map and uh then we have things that ego vehicle does and then we have things uh for agent other agents other vehicles and the things moving in the scene like pedestrians and also there is map we have a map so all together so let me let me let me hit pause there so the the ego view is kind of this bird's eye view and that is familiar here. Yeah, I've got that in my car. Like it's got cameras on the mirrors and in the back and in the front and it kind of projects into this top down view. Top down.\n\nThat's that's kind of the\n\nExactly.\n\nSo top down view vehicle, ego vehicle is in the center and everything is around looks like 2D from topdown. And so the the other did you say v the view from other vehicles like\n\nno uh the tokens or their um kind of not necessarily the views but the useful information that they uh kind of provide into\n\nah so maybe localizing other things in the scene or something.\n\nYes. And also their previous trajectory, their headings, that type of information not necessarily available in one bird's eye view picture but because uh kind they are about other vehicle like maybe also uh how they intend to drive which is not available in bird's eye view. So this system is not only you know looking at a frozen bird but you know thinking on behalf of the other vehicles agents how they may move are they accelerating up slow down you know will they do something unexpected so we are trying to anticipate what other person is going to do other vehicle is going to do like a real person right we are on a traffic stop and then I'm looking at the other vehicle I'm thinking it's gonna go ahead or it's going to wait for me to go ahead. So in other words, the LM input is kind of a string of tokens that represent this top down view from the perspective of the vehicle, but also um almost like other features, you know, representing the vehicle's trajectory, what it thinks other agents in the scene are doing, whether those are vehicles or pedestrians, and like somehow mapping out the scene. So it's like a a featurization of the space that the vehicle is operating in.\n\nExactly. That is what we call as seen representation and tokens. Everything now kind of standardized in terms of tokens. We have this ego vehicle tokens you know coming from its own queue former and then other vehicles tokens coming from their cube former and intense and there is this bird's eye view seeing at them we are tokenizing it also and there's map also there's tokens for that but they are not all the tokens when we are training with LLM these are going in but then there is also a question right or a prompt text prompt going into there and then this language model what it does uh provides an answer and when\n\n\nWe are, you know, giving in training time. We have the answers. We have this, you know, scene representation tokens, and we have the question. So we are updating this language model. At the same time, we are also, uh, looking at this. Tokens are also updated, you know, we, this ego or, uh, agent or, uh, bird's eye view or met tokens, they are also updated. These tokens go into language model, and then within this language model, when we are running layers, each layer has cross retention and also self-attention layers. This self reates those tokens. So in a way that you can think that, oh, these tokens are now changing in a way that they are more consistent, and they are more useful, and they are answering, for instance, this input from better. So at the end, we will have updated representations for those tokens. So some row tokens goes into, and then better tokens comes out. So those better tokens are used to update two versions again, uh, transformer versions. We can distill the pling transformer with, uh, to capture, you know, how, uh, pling transformer will have some additional compute. So row tokens will become better tokens, and then the tokens will allow us to make better decisions. So that's why we are improving the performance, these planning constraints. Yeah.\n\nAnd I should note that there's a really good image or figure two in the paper that is kind of a block diagram of all the things that we're talking about. And hopefully, if you're watching the video, we will have dropped that in so that you can follow along. Certainly, if not, or if you're watching the audio, then, uh, we'll be including a link to the paper in the show notes, and you can pull that up. Part of what I'm trying to do here is reconcile what you're saying with the image, which is, you know, it's obviously a static thing. And so I'm trying to understand how the things that you're saying happen in time. Like, so you've got, you know, uh, I think your data set consists of, um, at a given point in time, six images from the cameras on a vehicle, right? And you just have a many of those. Is there additional data associated with that, or is it just those images?\n\nUh, we represent all of these current six images and let's say t minus one, six images. It could be t minus n also, but looks like t minus one is sufficient in terms of their tokens. There's this scene encoder s, and then that scene encoder is kind of like the feature extractor that we talked a little bit about, right? It generates tokens, you know, kind of at the end. There are this token for the six images, current images, and then previous images, as we have those tokens already in the LLM. So again, two versions to, uh, kind of distinguish, uh, if LLM is making the AI planner decisions, those are in the KV cache, long context of the model. So model retains those, and then when it is going to make, for instance, a waypoint estimation or a driving decision, a planning decision, it will leverage those previous tokens up to the, you know, buffer size of 2 KVK, and then, uh, the transformer version, we use previous token instance and current tokens and, uh, update this pling transformer. It's a transformer, but it's not like language model, but, uh, transformers are the building blocks of language models, by the way. It's not explicitly answering a question, you know, it doesn't have that capacity, but it works with tokens. So those tokens, what I mentioned, are now in training time. We learn better tokens, and now we take it, and we learn an adapter, you know, kind of for this planning transformer. So next time in the trans meaning, so you can get the tokens that come out of the scene encoder and give them to the planning transformer directly and get a prediction as opposed to going through all of the machinery of the LLM.\n\nExactly. So that version is transformer version is more efficient. Now, Cena encoder again generates the raw tokens. It goes through this adapter, and then, uh, goes to the pling transformer, and, uh, that is, there's no LLM anymore in that version, but, uh, you know, kind of a better, uh, transformer for planning. And this is the core idea behind distillation in general. So it's like you've got this, you know, surrogate model, in this case, a transformer, uh, or the student model, I'm referring to in this case, that is, uh, you know, it's a function approximator. And so the distillation process is kind of teaching it how to approximate, uh, you know, the function, in this case here, the teacher model, it's output tokens, given the the beam tokens from the scene encoder are the the function that we're trying to approximate.\n\nAbsolutely. Of course, devil is in the details, how you do the distilleration, how you do surrogate tasks, uh, how you do VQA is a part of it, what are those data sets, and what are the, you know, uh, those functions. Uh, we tried many things, and, uh, you know, kind of more details are in the paper.\n\nYou know, you mentioned surrogate tasks and VQA. What are surrogate tasks in this context?\n\nFor instance, they are mostly around trajectory prediction. Some of them are about also mass token prediction, future BV token prediction to encourage the LLM to learn spatial temporal cues useful for planning and scene editing. Scene editing is a part of it, because we have to hypotize where the other things agents are going to be. So I mean, this is in training time, um, how the surrounding agents, other vehicles will impact the ego vehicles future path, you know, kind of a, uh, so this is what I meant by scene editing tests, but that's a part of the surrogate task or trajectory prediction, and, uh, and there are a couple of more also.\n\nAnd so they're not necessarily tasks that you want to use, but rather they're exposed as tasks so that you have a separate loss function that you can, uh, incorporate so that the model attends to those things. Is that a fair way to think about it?\n\nExactly. We are not explicitly saying, here is the trajectory, here's the, you know, form equation. We are using that to make sure that the temporal aspects of driving is captured through those surrogate test like prediction. I said prediction, right? Uh, the scene itself, the other agents, and these are future predicting, um, and also leveraging on what kind of it observed before, so the model would learn a better, you know, representation itself, but it is not like I'm, we are telling model to go optimize for this future instances, something like that, or change it. It is through the training, it learns itself, changes it coefficients to better, um, uh, understand the dynamics of the traffic scenarios.\n\nAnd what's the role of incorporating a VQA component, visual question answering?\n\nYeah, um, we want this capability, that's an important part also, this word knowledge to be a part of overall, um, kind of solution. We do not want to just drive steer LLM towards surrogate task, you, uh, future prediction, or you know, generating better tokens for the planner, but we want it to be grounded also. So that is keeping it real, connected to the semantic information. For instance, those, uh, questions could be, uh, okay, in this scenario, what are the safe actions to take for ego vehicle? That is the literally the question, and the answer is could be the action is to break gently to a stop. So for this input, we want model to be still able to generate such answer, uh, given the prompt and all the visual data. So that is going to, uh, allow us to retain the semantic world information of this LLM without destroying the LLA. And also the second version, we can ask questions, and then it can answer, you know, kind of, uh, again, two version, transformer version doesn't, uh, answer the questions, but the LLM versions in inference time can answer the question. So it has two purpose, keeping it grounded, semantically grounded, still not destroying the word knowledge, but then also having capability to answer questions if you are running LLM as the planner.\n\nUh, and so you've talked about the kind of the the outcome here, so the state-of-the-art performance on some of these longtail on both trajectory three, uh, error and kind of longtail collision detection or avoidance. Um, what you didn't mention yet is, um, any kind of SLAs's or performance around the distilled model. Is it still fast? Is it fast enough yet to be able to use in a real-time AV loop, or does more work need to be done to, um, make the model more efficient?\n\nThis model we can run as of now on Qualcomm, uh, accelerator, uh, faster than real time. It is real solution. I mean, this paper is not for, you know, kind of publication research sake, but it has a very practical purpose also. We hope to have a vehicle running around soon, and yeah, showcasing this technology, most likely it's going to be at CES, but you know, kind of, I don't know the future, you know, kind of, but it is there, you know.\n\nGiven what you've done with this paper, where do you see this kind of direction of of research going, this particular line of research, what's next?\n\nSo we see that, you know, such models are useful. And other companies are also looking into that. And I see one, uh, trend of running both transformer and language model together. Language model is, as you, you know, kind of, as we talked about, is slower, compute intensive. So maybe, you know, not that fast at this moment. So it is on a longer horizon, you know, in a lower frame rate. It is estimating, doing its own thing, you know, estimating way points and, you know, some useful AI planning decisions. At the current AI planner using transformer is also running on 3D or 2D perception st, and then these are combined. So it is there is redundancy in the system. If it is something unexpected for the, you know, low-level perception sake, at least we have high level, you know, understanding that would provide some safety mechanism for the longtail. So this, this like a trend, kind of many companies, research labs are looking into that. So I think it will become more popular, but more as important as this one, using language model in the vehicle to interact with the vehicle decision mechanism, change its behavior, customize for a geospatial location, like for a city or for a country, because rules are different, and the driving dynamics are different, and preferences are different. Maybe you want more aggressive driving or more conservative, you know, kind of. Yeah. Uh, they are also happening. I think kind of Yeah. Um, I can see that we actually, we are in the process of, uh, having an even more, uh, capable version of DiMA, uh, which is domain adapting to some of these scenarios that I mentioned.\n\nVery cool. Very cool. All right. So shifting gears to the second paper that we want to, uh, dig deep into, oh, deep, uh, no pun intended, is SharpDepth. Uh, and the full title of this paper, SharpDepth: Sharpening Metric Depth Predictions Using Diffusion Distillation. So distillation again, uh, a theme here. My impression is the general space that this paper is exploring is moninocular depth estimation. So you've got a, uh, 2D image, a photo, and you want to essentially reconstruct it as a 3D, uh, in a 3D space. Is is that the right way to think about this?\n\nVery good. Yeah, you said it very well. And so talk a little bit about, you know, the background here and state-of-the-art prior to this work. Um, and then we'll dig into what this work continues.\n\nAbsolutely. So this is molecular depth estimation, but more specifically, this paper chart that is about metric depth estimation. You don't know whether it's a tiny chair or a huge chair, or you know, some reasonable chair. It is, so there are two ways, two separate, you know, um, uh, type of algorithms. One, uh, are maybe let's call them as generative models. Um, and those are, uh, fine grain, they provide, uh, no very sharp depth molecular depth estimation, because not there's a lot, they can use synthetic data, there's a lot of such data, like my gold and lotus, and many, you know, molecular depth estimation algorithms are like that. So they have also dance ground truth, because, you know, it's synthetic is there, uh, but their, you know, skills, uh, it this point, you know, it is not like inches or millimeter or anything like that, you don't know what that point, each pixel, how big it is. It could be 1 meter or 1 millimeter, you know, very different. So there are, it's relative within the reconstructed image, but it's not absolute, so you can't measure with it.\n\nYeah, it is relative. So those are, uh, generative solutions, and there is discriminative solutions like metric 3D and Unidep, and when they do, uh, provide these estimations depth estimation cell, they either inches or, you know, millimeters, centimeters, uh, however, they are trained with such data doesn't exist in quantity, because usually LAR sensors images and light LAR sensors or structured light sensors are used. So first of all, they are very low resolution and not big in quantity. It's real measurement. So there's a challenge. So when you use those solutions, you get, you know, kind of correct depth estimation from the camera or 3D, inaccurate in terms of scale. However, they look like very blurred in the paper. You may see some examples. You know, all the details are missing. If there is a chair, legs will be missing. Genative method, because of scientific data they are leveraging, they will show very nice details, very fine, but we don't know how far away that chair or how big the chair is, but this dep methods, they will give exact absolute distance, accuracy is great, but then very low resolution, maybe you will not even see the chair. So there like there are two, uh, kind of existing solutions. So this paper sharp bridges these two approaches, integrating metric accuracy with the detailed boundary preservation of the the generative methods. It is generative method. It means taking advantage of, for instance, a diffusion denoising diffusion, and it has actually in the architecture, it has a unit, uh, model running in the background.\n\nI I'll jump in to note that anyone that wants to dig into unit, uh, a bit, I think we talked about that, uh, in our last CVPR CVPR review and maybe even the one before that. I I think we've been unit has been a theme that you and your your research group have been working on for a while now.\n\nUnit actually kind of a the name U shape is the shape of the architecture. It has been very popular. Of course, there is also a trend of now using DIT diffusion transformers, which has a different structure, but unit is still one of the choices for, uh, geneti for visual data.\n\nYou're right, and there are lots of applications of this, uh, uh, Sam, why you may wonder why we want to do molecular depth, uh, estimation. Should I talk about this?\n\nYeah, I'm imagining something like, you know, I'm in a room and I want to take a picture of something with my camera and know how big it is. That that would be pretty cool.\n\nAbsolutely. Then you can place, you know, for you go buy from a furniture store, I and you know how exactly it's going to fit. It's not relative because now you know the actual measurement. So this virtual try out, room planning, object placement, 3D model reconstruction. You can use your, uh, you know, phone to create a real size, uh, with metric absolute scale models. Of course, people are using for robotic navigation, because we know exactly how far away oluders and other objects in the scene, and robots are interacting with the scene. When it is reaching out something, it knows where it is exactly, not, you know, how many pixels, you know, it doesn't know whether it's far away or nearby. Yeah, there are lots of applications in, for instance, immersive property walkthroughs to understanding things in 3D space, even surgical assistance, you know, kind of, you do minimally invasive, uh, procedures using\n\n\nA single camera and because they need to be very tiny, right? You may not be able to put like two cameras with white baseline into a very tiny blood vessel. So, but you can squeeze something like a camera, single camera, but then you know how far away everything is in the sensor. Maybe, I mean, these are real applications like drones or, you know, kind of, let's say, monitoring to construction sites and other type of things. Automated parking is a part of it also. Yeah. Uh, you mentioned that part of what's happening here is you're kind of fusing uh, a generative approach uh with uh metric-based approach. And it's still not clear to me, like it seems like you need some piece of absolute data somewhere in order to start to do this. Like, you know, if you're taking a picture with a phone, you know, you might need some like LiDAR, you know, that says the distance from objects, or like maybe stereoscopic where you've got the distance between the lenses, or some piece of uh, piece of additional information from the real world to ground you. How does this method overcome that gap?\n\nYes. Um, so we are running two things to start the overall process. One is this kind of discriminative metric depth model is running. So we run it because it exists, right? Such models, I mentioned, we run it, we get an estimation of the everything in the scene, their depth estimations, but those are metric estimation, but it is just not high resolution or, you know, kind of sharp enough. Then we also generate the other one. Um, is it clear, you know, there is such an algorithm already running, and this SharpDepth can use any of those, you know, kind of this is an overall methodology to incorporate like these two different type of models. We are not, you know, reinventing the wheel, but we are putting those wheels in a way that they would go, you know, align, and whatever we are driving goes better, faster. So you, so you've got this, you've got this coarse measurement that we know how to do, and then you've got the fine-grained uh relative thing from the generative side, and the the idea is that the latter is good enough, or or, you know, they're each good enough so that together you can get fine-grained depth predictions if you put them together in the right.\n\nRight. Absolutely. And the thing that you mention, right, way is the big question. How am I going to know that which one is which? Because I don't know this the new scene, you know, one is blurry, the other one is sharp. I don't know which one to trust. Um, so what we did, our intuition is, we take these two estimations and then we compare them. We have this adaptive subtraction, scale subtraction, which generates a difference map. So the intention is in this, let's say, difference map, the regions with minimal differences um uh are more reliable in terms of their metric depth estimation, and while and other areas with larger uh kind of differences uh will require maybe updates. So this is actually kind of allowing us to, I mean, I, we keep in mind that this a diffusion uh solution with unit and unit. When we run unit, there is the input, and uh for textic generation, input is just Gaussian noise. In this case, it is not. In this case, it is this, let's say, pure Gaussian noise. It is changing depending on the the difference map, and we have uh, we are conditioning on this. So minimal differences, we don't like the diffusion process to change them, big differences, we like it to change, and then uh, we learn in training time a model that takes such difference map and makes the best use of it uh to uh refine itself. Uh, so there is fine-tuning going on in training time. So we have one model, we run the model, we generate this a fine invariant depth, but that is not good enough. We have the, you know, kind of metric depth. So we compute this difference. Difference dictates how much diffusion is going to make a change in the depth estimations, and in training time. This is optimized because we have, you know, kind of the uh, even the low resolution, we have some metric estimation, and also uh, we can use, you know, existing metric data set. We also show in the paper that you don't really need a lot of it. Uh, we are using maybe 150 times smaller than the amount of data used to train such discriminative models. So kind of in training time, we learn a better diffusion model, or can we just use it, you know, in in first time. So overall, take a look at the paper, it is accurate, it is sort of, you know, in terms of the resolution, but it is a symmetric estimation. Recently, I've been seeing a lot of interesting applications of diffusion models uh beyond kind of the, you know, stable diffusion image, you know, text to image, and they all seem to revolve around creative ways to manipulate the noise to do interesting things, and so this is that same kind of idea.\n\nYes, in this case, we are controlling the noise. You are absolutely right. By the way, you can, we can use diffusion uh framework to make a robot uh planning, you know, or trajectory estimation or solving a puzzle. Diffusion has many applications. In this case, we are controlling to know it. You are right. Uh, ultimately, is this supervised or totally self-supervised?\n\nSelf-supervised because we have this uh metric depth model, even though low resolution, we can in the training time, we can just use to learn, you know, a better uh SharpNet, but we this thing can also, like I said, can use leverage much less uh supervised data, so we can do SFT, supervised finetuning, and we show, you know, the difference in the paper, you know, how they will compare. Both of them are still know providing very sharp uh depth maps, monocular depth maps, but metric also accurate depth maps.\n\nSo you show uh both supervised and unsupervised versions of the model.\n\nExactly. And uh the reason we showed both of them, so people can go, you know, if they design their own metric depth, they can also incorporate in this framework, leveraging the ideas that we talk about in the paper, and then they may come up with even a better version of, you know, kind of SharpDepth. The idea is same, you know, kind of, I mean, this paper facilitates further research in that sense.\n\nVery cool. Are there additional areas you foresee in terms of future research uh along these lines?\n\nOf course, uh this is monocular depth estimation, but you don't do monocular only if you have multiple cameras or if you have video, right? You can do structure from motion if there's video or uh multi-view stereo or multiview depth estimation. So, but overall idea would be same, um, at the mechanisms like how this noise aware gate thing for using the difference map is explained in the paper. I didn't go into those details, maybe little bit met heavy, and I may need a board to kind of um, there might be, of course, extensions to uh video and multiview uh as future work. We are actually looking into that, and also maybe kind of alternative ways of computing these uh noise maps and cross-attending them into the model. Yeah, this is active research, you know, kind of, we try things, sometimes they generate amazing results, sometimes, you know, kind of they to learn and, you know, refine the idea.\n\nGreat. So uh in past years, we've talked about uh, you know, a broad set of Qualcomm's research and demos and workshops and tutorials in our CVPR show uh and touched on a a bunch of the individual papers. This time, we wanted to go deep into uh a couple of papers, but there's still a lot of other, you know, papers as well as activities that you presented at the conference. Um, yeah, let's maybe choose a few of those to touch on, and then we'll refer folks to your blog, your blog post to get the full picture. Um, you know, what did you do for demos this time? In addition to 11 papers, you know, we had uh maybe more than 10 demos. With me, you know, kind of talk about three of them. The first one is going to be text-to-3D demo. The second one is going to be either video-to-video or image-to-video GenAI demo. These are all multi demos running on device, and the third one is going to be multimodal VQA. Okay, if you have time. Um, text-to-3D uh is a model, literally user speaks through audio interface or text prompt, and you describe an object like a cactus sword, you know, kind of a hippo wearing a sweater, you know, anything you can imagine. Then it will generate on device a 3D mesh and also associated texture map, like color, everything in less than 3 seconds. I mean, such models, we are not the first to come up with such model, but in terms of the existing models, comp computational complexity, there is an amazing model called as MV Dream. Uh, yeah, it takes 194 minutes, not seconds, and there's DreamFusion. I mean, maybe quality wise, you know, maybe we are much better. It takes 22 minutes, you know, kind of, and now we are saying that, hey, you can do it on your phone in 3 seconds. And the demo, uh, let me describe the demo. So it is a game, video game, and then the player goes into this table, and then this is a game where, you know, the uh, person is fighting with other things in the scene, you know, kind of like monsters and other people, and it says it describes it weapon, maybe a battle or sword, s, and that's why I mentioned about, you know, kind of a dragon or cactus sword or watermelon sword, you know, then it generates things accordingly in less than 3 seconds, then the person grabs it and then uh, continue playing, killing the monsters in the scene. That was the demo we were explicitly showing. But the thing that it can generate anything, and uh, this is world first uh, on device takes to 3D demo, literally the first time we also showed, and it allows offline personalization at the edge. You know, it uses diffusion models again uh to image generation in particular SSD. There is SDXL version of it as well. Both of them generate high resolution data. So it geni either one image or six images, multiview. Then we run another unit, you know, another SSD to take it and map it to a mesh. That's the text-to-3D for image-to-video and uh, video-to-video. There are two GenAI video generation demos. Uh, we showed both of them next to each other. Uh, so video-to-video is like video editing or stylization demo. Uh, it takes text as an input uh conditioning. It could be, let's say, I have my uh, you know, selfie, and then I see that, oh, change it to um, let's say, pencil drawing style or m painting style, but it is not about faces, it could be anything, any object, any scene uh, or make it look like Albert Einstein or, you know, kind of looks zombie, you know, kind of the challenge, of course, how to do it in a way that it is temporally consistent. So this is another unit-based model. Uh, it runs uh, quite efficiently on device. It runs at 12 uh frame per second speed, generating 512 by 384 video frames. So um, kind of is a kind of example, I can say that when we started this model, original model was taking 7 seconds, um, um, and there's an existing model, you know, kind of uh, someone outside Qualcomm developed it. When we put it on device, it was 7 seconds, but after we optimize uh, and I can talk about how it's done, uh, now it is running around 80 milliseconds, like this is mic it up. Yeah, this is the video video stylization in which video is even more aggressive. When we started, you know, kind of uh, it was taking 2200 teraflops. We decreased more than 500 times. The how does that uh compare in terms of time?\n\nTime, the original model even didn't fit out of memory and very slow. And I remember playing with uh one of the demos at uh the Google event. I think it was their new Veo3 model. And uh image to video in particular. And I mean it looked great, but it took a really long time. Like it was, you know, go away and we'll send you a notification when it's done kind of time frames. uh to to hear this happening on device is a whole whole new idea, and it is happening almost real time. That kind of, that's crazy. Now the images, it sounds like they're a little smaller, but uh image the video demo we had, it is generating 1024 512. It's a funny resolution, but it is quite high resolution actually, without any video frame interpolation. We can go to, you know, very fast interpolation to increase the frame rate, but without that, or we can generate two seconds of video in 3 seconds. So you can uh imagine, you know, this is very fast, and uh yeah, uh but but the challenge is again, these models are big models, they are compute intensive, how to get it down and run efficiently on device. That I think Qualcomm secret source. I mean, many things we do are aiming for power efficiency, know, speed, memory efficiency, making sure people can use on their own device, they don't need cloud connection or anything like that to generate such know or content. Yeah.\n\nVery good. Uh, so when we're back here in about a year, what should uh we expect to hear from you in terms of things that were hot in the next 12 months in computer vision? What are you excited about?\n\nThere are two things, two wild cards, you know, I can say that, you know, I'm betting on them. One is Yeah. uh, if it that's the correct term, you know, kind of or to joker, I mean, two things I'm really excited about it. One is agentic AI extending to visual content. The second one is visual reasoning models. So the agent AI is, I mean, system capable of autonomous planning, reasoning, and interacting with environment and user multi-turn, you like AI assistant, but then such systems, for instance, it is running on XR glass, you know, Ray-Ban band, and then you are, you know, giving it, okay, make a dinner reservation for the people in this scene based on their preferences. This is literally the question, and now the system has to go understand who are those people based on, you know, exchanges like text or emails or anything we did together before, understand their, you know, preferences in food, you know, what kind of cuisine they like, Italian or sushi or something like that, and then go and make a find a restaurant and then make a booking based on their availability. It has to access their, you know, kind, let's say, calendars. So this is happening. We had an agentic AI demo. Actually, I didn't mention components are there. So, but it is now using, I think next year, we will see a lot of things through screenshots or through cameras. Such content will be a part of agentic AI. Second one is reasoning models. You may remember Sam Dipsik made a huge, you know, kind of impact on how people are training models. They said that, okay, you don't need huge compute to train models, and these models now are going to smaller models are going to perform as good as big models because what they do in inference time, they don't just give an answer, but they think, I in a way that they generate tokens, respond, respond, and they say, oh, wait a minute, maybe there is another train of thought, maybe I will follow it. So this is like chain of thought models of thought models or graph of thought. There's a lot of XT of thought models happening, but now what I see that they are extending to visual data. It's already happening. There's multimodal coot, there is compositional coot, you know, many things are coming. We are looking into that. What does it mean for visual data uh to do reasoning? So I think it's going to be the other area very exciting to.\n\nWe will see who is going to get best paper, you know, in those two area.\n\nAwesome. Awesome. Very good. Well,\n\n\nFatih,\n\nUh, thank you once again for taking the time to share uh what you've been up to and uh some of your team's papers from CVPR.\nIt's great to catch up.\n\nIt was a real pleasure.\nI'm very excited about what we do and know it's always great to talk to you, Sam.\nI love this podcast.\n\nOh, thanks so much.\n",
  "dumpedAt": "2025-07-21T18:43:25.223Z"
}