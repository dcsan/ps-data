{
  "episodeId": "bKvfCJt0U3s",
  "channelSlug": "@twimlai",
  "title": "Building Voice AI Agents That Donâ€™t Suck [Kwindla Kramer] - 739",
  "publishedAt": "2025-07-15T18:59:08.000Z",
  "rawLines": [
    {
      "lang": "en",
      "text": "I think there's an existence proof that",
      "offset": 0.16,
      "duration": 6.719
    },
    {
      "lang": "en",
      "text": "you can use LLMs in conversation very",
      "offset": 2.399,
      "duration": 7.361
    },
    {
      "lang": "en",
      "text": "flexibly from the growth of the",
      "offset": 6.879,
      "duration": 5.601
    },
    {
      "lang": "en",
      "text": "enterprise voice AI stuff we see and I",
      "offset": 9.76,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "think the delta between what we're",
      "offset": 12.48,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "seeing there on the enterprise side and",
      "offset": 14.08,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "and what you're seeing on the you know",
      "offset": 15.36,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "kind of chat GPD advanced voice Gemini",
      "offset": 17.76,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "live side if you want the hot take",
      "offset": 19.68,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "expression of it those are demos not",
      "offset": 21.6,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "products the version of it you interact",
      "offset": 24.16,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "with is a demo not a product they could",
      "offset": 26.48,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "be products",
      "offset": 28.8,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "But for a whole variety of structural",
      "offset": 29.84,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "reasons at at at OpenAI and and Google,",
      "offset": 32,
      "duration": 6.84
    },
    {
      "lang": "en",
      "text": "they are not products today.",
      "offset": 34.88,
      "duration": 3.96
    },
    {
      "lang": "en",
      "text": "All right, everyone. Welcome to another",
      "offset": 49.2,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "episode of the Twimmel AI podcast. I am",
      "offset": 50.719,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "your host Sam Sharington. Today I'm",
      "offset": 52.96,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "joined by Quinn Kramer. Quinnler is",
      "offset": 55.199,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "co-founder and CEO of Daily and the",
      "offset": 57.52,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "creator of Pipecat. Before we get going,",
      "offset": 60.719,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "be sure to take a moment to hit that",
      "offset": 63.44,
      "duration": 3.039
    },
    {
      "lang": "en",
      "text": "subscribe button wherever you're",
      "offset": 64.879,
      "duration": 3.521
    },
    {
      "lang": "en",
      "text": "listening to today's show. Quinn,",
      "offset": 66.479,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "welcome to the podcast.",
      "offset": 68.4,
      "duration": 3.039
    },
    {
      "lang": "en",
      "text": "Thank you for having me. I'm a big fan",
      "offset": 69.92,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "of what you do on the show. Excited to",
      "offset": 71.439,
      "duration": 2.561
    },
    {
      "lang": "en",
      "text": "be here.",
      "offset": 72.96,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "I appreciate that. I guess this is",
      "offset": 74,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "technically your second time on the show",
      "offset": 75.84,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "because we did that kind of panel",
      "offset": 78.24,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "discussion interview at the most recent",
      "offset": 81.28,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "Google IO, which was a lot of fun. And",
      "offset": 84.08,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "shout out to Swix from Leighton Space",
      "offset": 86,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "for introducing us and putting that all",
      "offset": 88.72,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "together. But uh I think we found a lot",
      "offset": 90.799,
      "duration": 5.121
    },
    {
      "lang": "en",
      "text": "of interesting things to kind of talk",
      "offset": 94,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "about and vibe on and I wanted to dig in",
      "offset": 95.92,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "a little bit deeper about what you've",
      "offset": 98.079,
      "duration": 6.241
    },
    {
      "lang": "en",
      "text": "been up to and uh for folks that didn't",
      "offset": 100,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "hear or don't know you that's going to",
      "offset": 104.32,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "be primarily around voice AI is what",
      "offset": 106,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "you've been focused on. Uh, but you",
      "offset": 108.399,
      "duration": 6.161
    },
    {
      "lang": "en",
      "text": "know, let's give you an opportunity to",
      "offset": 111.6,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "introduce yourself to the audience.",
      "offset": 114.56,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "Yeah, I'm Quinn Lman Kramer. I'm an",
      "offset": 116.88,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "engineer. I've been doing large scale",
      "offset": 118.88,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "real-time network audio and video stuff",
      "offset": 120.479,
      "duration": 5.041
    },
    {
      "lang": "en",
      "text": "for most of my career. Uh, I co-founded",
      "offset": 122.719,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "a company called Daily. We make audio",
      "offset": 125.52,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "and video infrastructure for developers.",
      "offset": 127.759,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "So, if you're building something like a",
      "offset": 130.16,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "teleahalth app or an education app and",
      "offset": 131.52,
      "duration": 2.799
    },
    {
      "lang": "en",
      "text": "you're trying to connect people",
      "offset": 133.36,
      "duration": 2.959
    },
    {
      "lang": "en",
      "text": "together, you can use our infrastructure",
      "offset": 134.319,
      "duration": 5.841
    },
    {
      "lang": "en",
      "text": "on our SDKs. When GPT4 came out, it",
      "offset": 136.319,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "started to look to us uh like not only",
      "offset": 140.16,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "could computers do all these amazing new",
      "offset": 142.319,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "things around structured data extra",
      "offset": 143.92,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "extraction and kind of open-ended",
      "offset": 146.239,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "conversation, but that those things felt",
      "offset": 148.239,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "like maybe you could have humans talking",
      "offset": 150.8,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "to computers in a new way. So, we built",
      "offset": 152.72,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "a bunch of stuff, experiment stuff with",
      "offset": 154.879,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "customers. Uh, and I got more and more",
      "offset": 157.44,
      "duration": 5.439
    },
    {
      "lang": "en",
      "text": "convinced that voice AI and real-time",
      "offset": 160.239,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "voice AI was a big part of this platform",
      "offset": 162.879,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "shift we're all excited about. So we",
      "offset": 165.44,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "open sourced all the tools we built",
      "offset": 167.68,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "internally at daily that became Pipcat",
      "offset": 169.2,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "which is now the most widely used voice",
      "offset": 172,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "agent framework or before 2025 I would",
      "offset": 174.64,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "have called it a an orchestration layer",
      "offset": 177.68,
      "duration": 6.68
    },
    {
      "lang": "en",
      "text": "for real time AI.",
      "offset": 180.16,
      "duration": 4.2
    },
    {
      "lang": "en",
      "text": "That's awesome. Yeah, it's amazing how",
      "offset": 184.48,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "quickly these terms are evolving. Uh, I",
      "offset": 186.08,
      "duration": 8.159
    },
    {
      "lang": "en",
      "text": "find it funny that voice tends to be",
      "offset": 188.879,
      "duration": 8.161
    },
    {
      "lang": "en",
      "text": "um, you know, people either love it or",
      "offset": 194.239,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "hate it as an idea for the way to",
      "offset": 197.04,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "interact with AI and computers in",
      "offset": 199.519,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "general. It has always been fascinating",
      "offset": 201.76,
      "duration": 6.399
    },
    {
      "lang": "en",
      "text": "for me and really exciting. Like I don't",
      "offset": 205.599,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "know if I was in high school or junior",
      "offset": 208.159,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "high school and was like into dialogic",
      "offset": 209.599,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "boards and stuff like that. Uh, and I",
      "offset": 211.84,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "was super excited about, you know,",
      "offset": 214.159,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "Twilio, knew them when they were really",
      "offset": 215.599,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "early on. Uh, and this, you know, idea",
      "offset": 217.36,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "that we can like control computers, uh,",
      "offset": 220.319,
      "duration": 5.681
    },
    {
      "lang": "en",
      "text": "and interact with computers via, you",
      "offset": 223.44,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "know, just via natural speech, I I find",
      "offset": 226,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "fascinating. How did you, you know, get",
      "offset": 229.12,
      "duration": 5.759
    },
    {
      "lang": "en",
      "text": "into it? What, uh, was the the spark",
      "offset": 231.92,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "beyond the kind of business opportunity",
      "offset": 234.879,
      "duration": 2.961
    },
    {
      "lang": "en",
      "text": "you saw?",
      "offset": 236.799,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "I mean, I am like you. I think it's",
      "offset": 237.84,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "super interesting to be able to actually",
      "offset": 240.72,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "talk to a computer and have that be a",
      "offset": 242.48,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "big component of the user interface. Uh,",
      "offset": 245.28,
      "duration": 4.959
    },
    {
      "lang": "en",
      "text": "and it seems obvious to me, like I think",
      "offset": 247.76,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "it does to a lot of people who've been",
      "offset": 250.239,
      "duration": 2.961
    },
    {
      "lang": "en",
      "text": "sort of consciously trying to experiment",
      "offset": 251.599,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "with this, that this platform shift",
      "offset": 253.2,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "towards generative AI is going to",
      "offset": 255.84,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "require bunch of new interface building",
      "offset": 257.28,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "blocks. And we haven't even started to",
      "offset": 259.6,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "scratch the surface there yet. And I am",
      "offset": 261.68,
      "duration": 6.4
    },
    {
      "lang": "en",
      "text": "pretty sure that a big big part of those",
      "offset": 264.24,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "building blocks is going to be figuring",
      "offset": 268.08,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "out what voice first UI looks like.",
      "offset": 269.6,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "People are really comfortable talking.",
      "offset": 272.479,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "And even though those of us who like are",
      "offset": 274.72,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "used to doing this to interact with",
      "offset": 276.72,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "computers feel maybe a little weird when",
      "offset": 278.96,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "we talk to our computers, you get over",
      "offset": 281.12,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "that pretty fast. And all of a sudden it",
      "offset": 283.36,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "opens up this whole sort of like uh",
      "offset": 285.6,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "efficiency channel that's very very",
      "offset": 289.04,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "different from from the mouse and the",
      "offset": 291.12,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "keyboard. And we we just couldn't do it",
      "offset": 292.72,
      "duration": 3.039
    },
    {
      "lang": "en",
      "text": "before because we didn't have a way to",
      "offset": 294.08,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "take that unstructured conversation",
      "offset": 295.759,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "actually turn it into something",
      "offset": 298.4,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "computers could do something with. But",
      "offset": 300,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "LLMs can totally do that. And they're",
      "offset": 302.32,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "just as good at processing your voice as",
      "offset": 304.56,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "they are at processing a text stream",
      "offset": 306.56,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "from your keyboard. Um, and the step",
      "offset": 308.639,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "function difference between typing and",
      "offset": 311.039,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "voice as input when you add an LLM to",
      "offset": 313.6,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "the mix is, I think, even bigger. So,",
      "offset": 315.759,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "you know, I'm encouraging everybody I",
      "offset": 318.32,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "know to talk to your computer as much as",
      "offset": 320.08,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "you can. If you're a programmer and",
      "offset": 322.16,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "you're interested in this stuff,",
      "offset": 324.24,
      "duration": 2.56
    },
    {
      "lang": "en",
      "text": "experiment with what like a little",
      "offset": 325.28,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "building block for for a voice first",
      "offset": 326.8,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "experience looks like because you're",
      "offset": 329.44,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "totally living in the living in the",
      "offset": 331.44,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "future when you do that. It frustrates",
      "offset": 333.12,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "me to no end that every web page that",
      "offset": 335.12,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "has a form on it doesn't have a",
      "offset": 337.12,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "microphone that I can press and uh do",
      "offset": 338.56,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "it. And you know, obviously I've got it,",
      "offset": 340.8,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "you know, with the various keyboards on",
      "offset": 342.88,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "the phone, but you have to go into the",
      "offset": 344.88,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "fields and I just want something to just",
      "offset": 346.72,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "take care of that. Let me talk to the",
      "offset": 350.72,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "web. I have probably had the same",
      "offset": 352.08,
      "duration": 6.959
    },
    {
      "lang": "en",
      "text": "conversation maybe, you know, 50 times",
      "offset": 355.039,
      "duration": 7.281
    },
    {
      "lang": "en",
      "text": "over the last month or so with both",
      "offset": 359.039,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "friends and people I'm working with.",
      "offset": 362.32,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "professionally about voice and it always",
      "offset": 363.759,
      "duration": 3.121
    },
    {
      "lang": "en",
      "text": "goes like and it's always with",
      "offset": 365.84,
      "duration": 2.479
    },
    {
      "lang": "en",
      "text": "programmers and it always goes like",
      "offset": 366.88,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "this. I say I'm trying to talk to my",
      "offset": 368.319,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "computer as much as I can and these days",
      "offset": 370.88,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "when I'm programming I talk more than I",
      "offset": 372.4,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "type and people are like yeah I I don't",
      "offset": 374.56,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "see it like I'm not really like I don't",
      "offset": 378,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "really like talking to the computer and",
      "offset": 380.16,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "also like what do I do in my open plan",
      "offset": 381.919,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "office and my response to that is",
      "offset": 383.84,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "twofold. First I totally hear you. It's",
      "offset": 386.08,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "a shift. You know changing a big part of",
      "offset": 388.16,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "your like professional day like that's a",
      "offset": 390.24,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "big deal. I'm not discounting that at",
      "offset": 392.4,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "all. But also, we started daily to do",
      "offset": 393.84,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "video and audio communication stuff in",
      "offset": 397.28,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "2016. And I've been doing startups long",
      "offset": 400.08,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "enough that I was lucky that I only had",
      "offset": 402.72,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "to pitch investors that I already knew.",
      "offset": 404.8,
      "duration": 4.959
    },
    {
      "lang": "en",
      "text": "So, very easy conversations, people who",
      "offset": 407.759,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "knew me, people who were sort of biased",
      "offset": 409.759,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "towards, you know, taking what I was",
      "offset": 412.16,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "saying I wanted to do for a new company",
      "offset": 414.08,
      "duration": 5.119
    },
    {
      "lang": "en",
      "text": "seriously. Still 12 out of 15 of those",
      "offset": 415.759,
      "duration": 6.241
    },
    {
      "lang": "en",
      "text": "initial pitch conversations in the 2016",
      "offset": 419.199,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "summer of 2016 for daily went like this.",
      "offset": 422,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "I said I think we're all going to be",
      "offset": 424.319,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "doing real-time audio and video all the",
      "offset": 426.08,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "time on the internet. That's what I'm",
      "offset": 427.68,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "starting a company around. And",
      "offset": 429.36,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "professional tech investors would say to",
      "offset": 431.36,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "me, I don't know, man. Like I I like I",
      "offset": 433.28,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "like phone calls. Like if I want to talk",
      "offset": 437.44,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "to somebody, I just want to talk on the",
      "offset": 438.96,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "phone. I don't want to have to set up a",
      "offset": 440.56,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "video call. I was like, okay, you will.",
      "offset": 442,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "You will.",
      "offset": 444.16,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "That's nuts. That's nuts. Uh it's been",
      "offset": 447.199,
      "duration": 5.601
    },
    {
      "lang": "en",
      "text": "astounding like how quickly the",
      "offset": 450.24,
      "duration": 5.359
    },
    {
      "lang": "en",
      "text": "technology evolves you or has evolved",
      "offset": 452.8,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "over the past, you know, and this I",
      "offset": 455.599,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "guess is also like preaching to the",
      "offset": 457.28,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "choir. We all have been living this and",
      "offset": 458.8,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "holding on with like white knuckles. But",
      "offset": 461.039,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "um I remember it couldn't be more than",
      "offset": 464.4,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "six months ago like before or maybe a",
      "offset": 467.199,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "little more than six months ago like a",
      "offset": 470.08,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "little bit before vibe coding was the",
      "offset": 471.84,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "cool word. I was essentially vibe coding",
      "offset": 474.08,
      "duration": 4.959
    },
    {
      "lang": "en",
      "text": "an application that let me I think at",
      "offset": 477.12,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "the time I was like trying to track",
      "offset": 479.039,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "macros. And so I was I like built this",
      "offset": 480.4,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "app that let me like speak what I ate,",
      "offset": 483.199,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "you know, with units and quantities and",
      "offset": 485.68,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "stuff like that. Uh so a little bit more",
      "offset": 488.4,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "granularity than just take a picture and",
      "offset": 490.479,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "it would like parse that and uh then go",
      "offset": 492.72,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "find the macros and stuff from a a",
      "offset": 495.44,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "database.",
      "offset": 497.52,
      "duration": 5.359
    },
    {
      "lang": "en",
      "text": "But the I mentioned it because the way",
      "offset": 499.52,
      "duration": 6.32
    },
    {
      "lang": "en",
      "text": "that I did the voice was like capture a",
      "offset": 502.879,
      "duration": 6.32
    },
    {
      "lang": "en",
      "text": "segment of voice locally and uh send",
      "offset": 505.84,
      "duration": 5.919
    },
    {
      "lang": "en",
      "text": "that into I think Gemini or some model",
      "offset": 509.199,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "doesn't really matter to transcribe it",
      "offset": 511.759,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "and get that back and then like ask it",
      "offset": 513.76,
      "duration": 6.8
    },
    {
      "lang": "en",
      "text": "to pull out the quantities and probably",
      "offset": 516.399,
      "duration": 5.921
    },
    {
      "lang": "en",
      "text": "like two weeks after like I stopped",
      "offset": 520.56,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "working on that the way I would do it",
      "offset": 522.32,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "like totally changed like OpenAI came",
      "offset": 523.76,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "out with like the live voice and you",
      "offset": 526,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "know all other auto providers have",
      "offset": 528.16,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "since, you know, come out with their",
      "offset": 530.08,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "versions of that. Um, and so I think I",
      "offset": 531.92,
      "duration": 7.28
    },
    {
      "lang": "en",
      "text": "bring that up to say that, you know, two",
      "offset": 536.08,
      "duration": 5.759
    },
    {
      "lang": "en",
      "text": "things, like just to to nod at the way",
      "offset": 539.2,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "this technology has been evolving, but",
      "offset": 541.839,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "also",
      "offset": 543.839,
      "duration": 5.761
    },
    {
      "lang": "en",
      "text": "uh to note that, you know, for me like",
      "offset": 545.76,
      "duration": 5.519
    },
    {
      "lang": "en",
      "text": "the okay, I'm going to like capture a",
      "offset": 549.6,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "recording and send it into an LLM was",
      "offset": 551.279,
      "duration": 3.521
    },
    {
      "lang": "en",
      "text": "like very easy cognitively to",
      "offset": 553.04,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "understand, but then the, you know, the",
      "offset": 554.8,
      "duration": 6.64
    },
    {
      "lang": "en",
      "text": "live APIs and like, you know, WebRTC and",
      "offset": 557.76,
      "duration": 6.639
    },
    {
      "lang": "en",
      "text": "all this other stuff seem like you know",
      "offset": 561.44,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "if I spent more than a couple minutes",
      "offset": 564.399,
      "duration": 3.521
    },
    {
      "lang": "en",
      "text": "like digging into it like I'm sure it",
      "offset": 565.92,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "would have been naturally natural and",
      "offset": 567.92,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "easy to work with but you know it was a",
      "offset": 570.24,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "little bit more complicated and so I",
      "offset": 572.32,
      "duration": 3.519
    },
    {
      "lang": "en",
      "text": "wanted to use this opportunity to ask",
      "offset": 573.76,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "you to give us like a primer on like",
      "offset": 575.839,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "getting started with voice like what's",
      "offset": 578.56,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "the way to think about it as a or the",
      "offset": 580.399,
      "duration": 5.601
    },
    {
      "lang": "en",
      "text": "the dominant like abstractions for",
      "offset": 583.6,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "building voice AI applications nowadays.",
      "offset": 586,
      "duration": 4.959
    },
    {
      "lang": "en",
      "text": "Yeah, I I have this conversation a lot",
      "offset": 588.8,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "too because there's so much new interest",
      "offset": 590.959,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "in voice and I I'll try out my latest",
      "offset": 592.48,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "version. You can tell me if it lands",
      "offset": 595.279,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "well. So, because you've got a technical",
      "offset": 596.64,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "audience, I think it's worth just",
      "offset": 598.88,
      "duration": 2.639
    },
    {
      "lang": "en",
      "text": "talking about the stack. Like talking",
      "offset": 600,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "about what the stack is is probably",
      "offset": 601.519,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "helpful. At the bottom of the stack,",
      "offset": 603.04,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "you've got the models. So, you've got",
      "offset": 604.88,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "the weights basically. Uh so whatever",
      "offset": 606.24,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "those models are including like the",
      "offset": 608.959,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "multimodal models you're talking about",
      "offset": 610.8,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "like the open AI real-time model the",
      "offset": 612.399,
      "duration": 5.921
    },
    {
      "lang": "en",
      "text": "Gemini uh live model uh or you can have",
      "offset": 614.32,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "you know text mode LLMs and you can do",
      "offset": 618.32,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "text to speech and speech to text and",
      "offset": 620.24,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "kind of glue everything together or you",
      "offset": 622.32,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "might have a bunch of small fine-tuned",
      "offset": 624.079,
      "duration": 3.521
    },
    {
      "lang": "en",
      "text": "LLMs all collaborating but at the bottom",
      "offset": 625.76,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "you got the weights on top of those",
      "offset": 627.6,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "you've got the APIs that the model",
      "offset": 630.32,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "providers or whatever your inference",
      "offset": 632.56,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "jack is providing. So like you're you're",
      "offset": 634.72,
      "duration": 5.359
    },
    {
      "lang": "en",
      "text": "hitting an HTTP endpoint from OpenAI or",
      "offset": 637.12,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "from Google or a websocket endpoint from",
      "offset": 640.079,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "somebody above that because generally",
      "offset": 642,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "we're trying to do non-trivial things",
      "offset": 644.88,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "with this technology. We're trying to go",
      "offset": 646.399,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "beyond building demos. Above the APIs,",
      "offset": 647.76,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "you've got some kind of orchestration.",
      "offset": 650.48,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "You're gluing things together. You're",
      "offset": 652.399,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "implementing the kind of pipelining of",
      "offset": 654.56,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "data to make it possible to do the",
      "offset": 657.36,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "multi-turn real-time conversation. And",
      "offset": 658.959,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "then on top of that you've got your",
      "offset": 661.68,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "application code which is sort of",
      "offset": 663.12,
      "duration": 2.56
    },
    {
      "lang": "en",
      "text": "sitting on top of that kind of",
      "offset": 664.48,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "orchestration layer. And so if you if",
      "offset": 665.68,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "you put all those things together you've",
      "offset": 668.32,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "got a real application. Now you can get",
      "offset": 670,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "started with voice AI today using a",
      "offset": 672.64,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "platform where everything is all those",
      "offset": 674.8,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "things are bundled for you into one kind",
      "offset": 677.12,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "of interface where you build something",
      "offset": 679.76,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "maybe even you just build in a",
      "offset": 681.6,
      "duration": 2.56
    },
    {
      "lang": "en",
      "text": "dashboard. You don't even have to write",
      "offset": 682.959,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "any code. So companies like Vappy have",
      "offset": 684.16,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "pioneered that all-in-one",
      "offset": 686.88,
      "duration": 4.959
    },
    {
      "lang": "en",
      "text": "combined batteries included with really",
      "offset": 689.36,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "nice dashboards, really nice uh kind of",
      "offset": 691.839,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "developer tooling. On the other end of",
      "offset": 694.24,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "the spectrum, you could build kind of",
      "offset": 696.959,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "everything yourself. You could make all",
      "offset": 698.48,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "of those individual choices in the stack",
      "offset": 700.32,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "yourself. You mix and match. You put",
      "offset": 702.8,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "everything together kind of as a as a",
      "offset": 704.48,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "programmer, you know, writing code with",
      "offset": 706.64,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "libraries. What I work on a lot is this",
      "offset": 708.24,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "orchestration layer called Pipecat which",
      "offset": 711.519,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "tries to give you a little bit of have",
      "offset": 713.36,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "your cake and eat it too where it's easy",
      "offset": 715.2,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "to get started because the core",
      "offset": 716.72,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "implementation of things like",
      "offset": 718.399,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "interruption handling and turn detection",
      "offset": 720.16,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "and multi-turn context management are",
      "offset": 722.24,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "all there for you as Python functions",
      "offset": 724.8,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "basically but you also have complete",
      "offset": 727.44,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "control and you can you know mix and",
      "offset": 730.24,
      "duration": 2.48
    },
    {
      "lang": "en",
      "text": "match all those parts",
      "offset": 731.76,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "and that's an open source project.",
      "offset": 732.72,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "Yeah, totally open source totally vendor",
      "offset": 734.48,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "neutral. I spend most of my time these",
      "offset": 736.48,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "days on pipecat uh because it's such an",
      "offset": 738.56,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "interesting new vector for everything we",
      "offset": 741.2,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "do in the real time world. Uh but it is",
      "offset": 743.12,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "a completely open source completely",
      "offset": 745.839,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "vendor neutral project. Lot most of the",
      "offset": 747.68,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "big labs contribute to pipcat. Hundreds",
      "offset": 749.76,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "of startups contribute to pipcat.",
      "offset": 751.839,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "There's probably 120 contributors now in",
      "offset": 753.44,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "the GitHub repo. In thinking about the",
      "offset": 755.92,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "relationship between Pipcat and Daily,",
      "offset": 757.76,
      "duration": 5.519
    },
    {
      "lang": "en",
      "text": "talk a little bit about the overlap just",
      "offset": 761.44,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "so I can kind of understand. It's not",
      "offset": 763.279,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "that Daily is commercializing Pipcat.",
      "offset": 764.88,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "It's that DY is providing infrastructure",
      "offset": 768.079,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "for people who are building these",
      "offset": 770.72,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "applications and Pipcat just makes it",
      "offset": 772.079,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "easier to build those applications",
      "offset": 773.92,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "whether they're hosted on Daily or",
      "offset": 775.68,
      "duration": 2.159
    },
    {
      "lang": "en",
      "text": "elsewhere.",
      "offset": 777.12,
      "duration": 2.56
    },
    {
      "lang": "en",
      "text": "Yeah, that's exactly right. Many many",
      "offset": 777.839,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "more people use Pipcat without daily",
      "offset": 779.68,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "than use Pipcat with Daily, which I",
      "offset": 781.68,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "think is a mark of success for an open",
      "offset": 783.519,
      "duration": 5.521
    },
    {
      "lang": "en",
      "text": "source project. Um, we at Daily are the",
      "offset": 785.36,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "very low-level network infrastructure.",
      "offset": 789.04,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "So, we move the audio and video bytes",
      "offset": 791.04,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "around the network at super high",
      "offset": 793.6,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "reliability and super low latency. Uh,",
      "offset": 795.279,
      "duration": 5.601
    },
    {
      "lang": "en",
      "text": "so anytime you need to do things very",
      "offset": 797.68,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "fast in a real-time interaction on the",
      "offset": 800.88,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "internet, uh, we'd love for you to think",
      "offset": 803.6,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "about using our network infrastructure.",
      "offset": 805.36,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "Uh, but, you know, Pipecat supports lots",
      "offset": 807.519,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "of different options for network",
      "offset": 809.68,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "transport. Daily is just one of them. Uh",
      "offset": 811.44,
      "duration": 6.079
    },
    {
      "lang": "en",
      "text": "we do increasingly try to help our",
      "offset": 814.399,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "customers get up and running with",
      "offset": 817.519,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "production quality voice AI",
      "offset": 819.519,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "infrastructure and we have built a on",
      "offset": 821.519,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "top of our global infrastructure we've",
      "offset": 824.16,
      "duration": 5.119
    },
    {
      "lang": "en",
      "text": "built a hosting platform for pipecat or",
      "offset": 825.76,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "related voice AI things called pipcat",
      "offset": 829.279,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "cloud but that's again totally separate",
      "offset": 831.44,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "from the the open source project which",
      "offset": 834,
      "duration": 3.519
    },
    {
      "lang": "en",
      "text": "has no has no commercial dependencies at",
      "offset": 835.76,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "all",
      "offset": 837.519,
      "duration": 3.521
    },
    {
      "lang": "en",
      "text": "and so pipcat cloud would be kind of",
      "offset": 839.12,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "analogous to like Cloudflare and",
      "offset": 841.04,
      "duration": 7.12
    },
    {
      "lang": "en",
      "text": "Cloudflare functions like the cloud is",
      "offset": 844.8,
      "duration": 7.36
    },
    {
      "lang": "en",
      "text": "like the the runtime and the uh Pipcat",
      "offset": 848.16,
      "duration": 5.359
    },
    {
      "lang": "en",
      "text": "cloud would be like the runtime",
      "offset": 852.16,
      "duration": 3.119
    },
    {
      "lang": "en",
      "text": "environment and daily would be like the",
      "offset": 853.519,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "level infrastructure. It's not a perfect",
      "offset": 855.279,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "analogy but",
      "offset": 857.68,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "yeah I mean I I've been trying to figure",
      "offset": 859.12,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "out what the best analogies are in a",
      "offset": 861.36,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "bunch of ways for this new era of like",
      "offset": 863.12,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "for voice AI agents.",
      "offset": 866,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "Yeah, the the original like web hosting",
      "offset": 867.76,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "platform that I thought kind of",
      "offset": 870.72,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "did a really good job like balancing uh",
      "offset": 872.959,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "flexibility with with high level",
      "offset": 875.519,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "abstractions early on was Heroku. So I",
      "offset": 877.199,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "sort of think of pipecat cloud as as",
      "offset": 879.519,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "Heroku for voice AI. Um but you know",
      "offset": 881.12,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "maybe to a technical audience it's even",
      "offset": 885.279,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "more clear to just say you push us a",
      "offset": 886.88,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "docker container and we autoscale it and",
      "offset": 889.279,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "monitor it for you and everything else",
      "offset": 891.68,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "you you get to choose. So it we're just",
      "offset": 894.24,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "taking our global infrastructure that's",
      "offset": 897.12,
      "duration": 3.279
    },
    {
      "lang": "en",
      "text": "very good at scaling things everywhere",
      "offset": 898.72,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "in the world and we're hosting a Docker",
      "offset": 900.399,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "container that is wired up for ultra low",
      "offset": 902.079,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "latency voice AI for you",
      "offset": 905.12,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "running on Kubernetes or something else.",
      "offset": 906.8,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "Yeah. Yeah. It's it's a lot of",
      "offset": 909.12,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "Kubernetes under the covers and I'm sure",
      "offset": 910.959,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "we will talk more about this from a",
      "offset": 912.959,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "bunch of angles, but there are a lot of",
      "offset": 914.399,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "things that make voice workloads",
      "offset": 916.72,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "different from you know the HTTP",
      "offset": 918.88,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "workloads or even the websocket",
      "offset": 921.279,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "workloads that we all spend a lot of our",
      "offset": 922.8,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "time building. And one of those things",
      "offset": 925.519,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "is you have to have this very low",
      "offset": 927.519,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "latency network transport and you have",
      "offset": 930,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "to support longunning conversations. And",
      "offset": 932.24,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "we we get a lot of people who come to us",
      "offset": 934.639,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "and say I tried to build this on AWS",
      "offset": 936.72,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "Lambda or I tried to build this on GCP",
      "offset": 938.959,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "cloud run both of which are fantastic",
      "offset": 940.8,
      "duration": 6.56
    },
    {
      "lang": "en",
      "text": "platforms but do not have the components",
      "offset": 943.199,
      "duration": 6.56
    },
    {
      "lang": "en",
      "text": "you actually need to support the voice",
      "offset": 947.36,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "workflows. I'm sure they will at some",
      "offset": 949.759,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "point because this space is growing so",
      "offset": 952.399,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "fast, but there's just a bunch of things",
      "offset": 954.32,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "you have to do that are not the normal",
      "offset": 956.56,
      "duration": 5.839
    },
    {
      "lang": "en",
      "text": "Kubernetes config to get the voice",
      "offset": 959.199,
      "duration": 5.841
    },
    {
      "lang": "en",
      "text": "platform stuff to to kind of run to",
      "offset": 962.399,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "scale to have the cold starts be right",
      "offset": 965.04,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "for voice cold start times and stuff",
      "offset": 967.04,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "like that. Yeah, we spend so much time",
      "offset": 969.519,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "back.",
      "offset": 971.68,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "Let's dig into those challenges because",
      "offset": 973.68,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "I think that's uh in a lot of ways where",
      "offset": 976.16,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "the rubber meets the road and kind of",
      "offset": 978.16,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "differentiating",
      "offset": 979.68,
      "duration": 5.279
    },
    {
      "lang": "en",
      "text": "um you know web applications as you",
      "offset": 981.519,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "mentioned from uh from voice",
      "offset": 984.959,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "applications. I think uh at a high level",
      "offset": 986.959,
      "duration": 5.521
    },
    {
      "lang": "en",
      "text": "the things I talk a lot about with",
      "offset": 989.92,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "people building like going from",
      "offset": 992.48,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "prototype to production on voice AI",
      "offset": 994.16,
      "duration": 6.32
    },
    {
      "lang": "en",
      "text": "today are evaliability",
      "offset": 996.16,
      "duration": 6.88
    },
    {
      "lang": "en",
      "text": "latency and then the sort of",
      "offset": 1000.48,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "fundamentally multi-model nature of",
      "offset": 1003.04,
      "duration": 5.599
    },
    {
      "lang": "en",
      "text": "almost every production voice app. Um",
      "offset": 1006,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "and we can take those in order if you",
      "offset": 1008.639,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "want or you can throw some out that you",
      "offset": 1010.8,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "Yeah. Well, let's start from let's start",
      "offset": 1012.56,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "from like lower level uh types of",
      "offset": 1014.56,
      "duration": 6.88
    },
    {
      "lang": "en",
      "text": "concerns which is latency. So for",
      "offset": 1018.48,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "example",
      "offset": 1021.44,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "when you describe the stack I was",
      "offset": 1022.959,
      "duration": 5.041
    },
    {
      "lang": "en",
      "text": "wondering",
      "offset": 1026.24,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "you know qu I had questions like did you",
      "offset": 1028,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "have to you know write your own",
      "offset": 1030.319,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "container runtime or is there like a",
      "offset": 1032.24,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "latency optimized container runtime or",
      "offset": 1034.64,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "like how how much tinkering in the stack",
      "offset": 1036.88,
      "duration": 7.039
    },
    {
      "lang": "en",
      "text": "you know do you have to do? um to",
      "offset": 1040.16,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "support voice applications like you know",
      "offset": 1043.919,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "custom kernels and all kinds of weird",
      "offset": 1046,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "stuff or because that says what's",
      "offset": 1047.76,
      "duration": 3.279
    },
    {
      "lang": "en",
      "text": "different between that and just like",
      "offset": 1049.76,
      "duration": 3.039
    },
    {
      "lang": "en",
      "text": "spinning something up in you know",
      "offset": 1051.039,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "digital ocean or AWS or wherever so",
      "offset": 1052.799,
      "duration": 5.281
    },
    {
      "lang": "en",
      "text": "everything is trade-offs uh probably",
      "offset": 1055.76,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "earlier in my career we would have",
      "offset": 1058.08,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "written our own container runtime um",
      "offset": 1059.6,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "because there are advantages doing that",
      "offset": 1062.4,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "but what we did this time was we said",
      "offset": 1064,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "okay every developer is going to be able",
      "offset": 1066.799,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "to use docker let's stick let's optimize",
      "offset": 1068.64,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "as other places. Let's stick to Docker",
      "offset": 1071.679,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "compatibility because that's going to",
      "offset": 1073.679,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "make the onboarding and the growth for",
      "offset": 1075.12,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "you know every developer who uses Pipcat",
      "offset": 1077.76,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "Cloud a lot easier. So we kept vanilla",
      "offset": 1080.16,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "Docker but then we have to surround that",
      "offset": 1082.799,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "with a bunch of fairly specialized",
      "offset": 1084.72,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "Kubernetes stuff on a couple levels. One",
      "offset": 1086.48,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "is just all the cold starts and rolling",
      "offset": 1088.559,
      "duration": 3.521
    },
    {
      "lang": "en",
      "text": "deployment stuff that's specific to",
      "offset": 1090.72,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "voice AI that that you were mentioning.",
      "offset": 1092.08,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "Um so you got to get those Docker",
      "offset": 1094.48,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "containers loaded. You got to get them",
      "offset": 1095.919,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "wired up to UDP networking. You've got",
      "offset": 1097.36,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "to uh have uh auto auto you've got to",
      "offset": 1099.84,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "have schedulers and deployment logic",
      "offset": 1103.36,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "that doesn't terminate halfhour long",
      "offset": 1105.6,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "running conversations which you know all",
      "offset": 1107.919,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "the all the all the default Kubernetes",
      "offset": 1110.32,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "stuff when you push code there's fairly",
      "offset": 1112.559,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "short drain times you have to have long",
      "offset": 1114.32,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "drain times and a bunch of stuff that",
      "offset": 1116.24,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "goes with that the other layer is you've",
      "offset": 1117.28,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "got to support UDP networking so you",
      "offset": 1119.36,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "have to support WebRTC",
      "offset": 1121.6,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "uh because for edge device to cloud",
      "offset": 1123.76,
      "duration": 6.72
    },
    {
      "lang": "en",
      "text": "realtime audio you need to not be using",
      "offset": 1127.2,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "websockets or TCP based protocols. You",
      "offset": 1130.48,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "need to be using UDP based protocols,",
      "offset": 1132.96,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "which the wrapper for those these days",
      "offset": 1135.039,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "is WebRTC. Um, so that's a big deal to",
      "offset": 1136.32,
      "duration": 5.599
    },
    {
      "lang": "en",
      "text": "wire up Kubernetes properly to UDP and",
      "offset": 1139.44,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "do all the routing and be able to start",
      "offset": 1141.919,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "the WebRTC conversations. Uh, there's a",
      "offset": 1144.08,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "whole bunch of little things you have to",
      "offset": 1147.12,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "kind of customize in Kubernetes. And so",
      "offset": 1148.88,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "the biggest single thing that you do for",
      "offset": 1151.52,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "latency is you you get that network",
      "offset": 1154,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "layer right with the UDP networking.",
      "offset": 1156.4,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "Then on top of that, you just try to you",
      "offset": 1158.96,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "try to like pull out every tiny bit of",
      "offset": 1161.6,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "extra few milliseconds of latency",
      "offset": 1164.32,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "everywhere in the data processing",
      "offset": 1166.16,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "pipeline. Um, which you never really",
      "offset": 1167.52,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "have to worry about doing if you're",
      "offset": 1169.84,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "doing kind of text mode HTTP based",
      "offset": 1171.12,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "inference cuz a few tens of milliseconds",
      "offset": 1173.919,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "here and there like you don't really",
      "offset": 1176.16,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "notice it. But you really really notice",
      "offset": 1177.6,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "it in a voice conversation where you're",
      "offset": 1179.44,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "trying to get below a second of",
      "offset": 1181.039,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "voicetovoice latency. I'm curious in",
      "offset": 1182.72,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "talking about the networking stuff when",
      "offset": 1185.52,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "we were chatting before you mentioned",
      "offset": 1187.44,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "that you listened to the uh recent",
      "offset": 1188.88,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "episode with Vjoy Pande from Cisco and",
      "offset": 1191.52,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "one of the things that he talked about",
      "offset": 1194.08,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "that I found interesting was uh this",
      "offset": 1195.44,
      "duration": 6.32
    },
    {
      "lang": "en",
      "text": "slim protocol that uh they're building",
      "offset": 1198.16,
      "duration": 7.92
    },
    {
      "lang": "en",
      "text": "and promoting or starting to promote. um",
      "offset": 1201.76,
      "duration": 6.72
    },
    {
      "lang": "en",
      "text": "any takes on, you know, how that fits",
      "offset": 1206.08,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "into meeting the requirements that",
      "offset": 1208.48,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "you're describing or are there other",
      "offset": 1210.88,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "efforts like that? Yeah,",
      "offset": 1212.88,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "I like that direction. Uh I'd actually",
      "offset": 1214.559,
      "duration": 5.281
    },
    {
      "lang": "en",
      "text": "love to talk to Vjoy uh because I'm",
      "offset": 1216.64,
      "duration": 5.919
    },
    {
      "lang": "en",
      "text": "interested in how that work he's doing",
      "offset": 1219.84,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "can play nicely with the stuff we're",
      "offset": 1222.559,
      "duration": 5.521
    },
    {
      "lang": "en",
      "text": "doing. Uh I Slim has a bunch of stuff",
      "offset": 1224.4,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "built in that I think is really",
      "offset": 1228.08,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "important. I don't think it has UDP",
      "offset": 1229.2,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "support yet. So from my perspective that",
      "offset": 1231.28,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "would be like a really good thing to",
      "offset": 1233.36,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "add. There aren't any other",
      "offset": 1234.64,
      "duration": 7.84
    },
    {
      "lang": "en",
      "text": "uh real time oriented transport sort of",
      "offset": 1238.559,
      "duration": 5.921
    },
    {
      "lang": "en",
      "text": "standards yet other than what we're",
      "offset": 1242.48,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "doing in the pipecat ecosystem where",
      "offset": 1244.48,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "we've defined a standard that lets",
      "offset": 1246.159,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "everybody plug into pipecat. I think",
      "offset": 1247.919,
      "duration": 5.041
    },
    {
      "lang": "en",
      "text": "there will be a a real need for a",
      "offset": 1250.08,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "real-time standard. The way I talk about",
      "offset": 1252.96,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "it with the partners we sort of bring",
      "offset": 1255.28,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "into the pipecat ecosystem is for better",
      "offset": 1257.2,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "and worse because standards are always",
      "offset": 1260.32,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "like that. The uh open AI chat",
      "offset": 1261.919,
      "duration": 5.681
    },
    {
      "lang": "en",
      "text": "completions HTTP standard became the",
      "offset": 1264.72,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "standard for everybody who does uh",
      "offset": 1267.6,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "textbased inference and now people have",
      "offset": 1270.48,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "built a bunch of stuff on top of that",
      "offset": 1272.88,
      "duration": 3.039
    },
    {
      "lang": "en",
      "text": "and a bunch of improvements to it. But",
      "offset": 1274.24,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "chat completions is what we all use if",
      "offset": 1275.919,
      "duration": 5.921
    },
    {
      "lang": "en",
      "text": "we're say I I might use OpenAI or I",
      "offset": 1279.2,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "might deploy my own model, you know,",
      "offset": 1281.84,
      "duration": 2.959
    },
    {
      "lang": "en",
      "text": "using VLM or whatever. I'm going to use",
      "offset": 1283.2,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "chat completions. We don't yet have that",
      "offset": 1284.799,
      "duration": 6.88
    },
    {
      "lang": "en",
      "text": "standard for real time multimedia. Uh we",
      "offset": 1287.36,
      "duration": 6.799
    },
    {
      "lang": "en",
      "text": "need it and we are definitely working",
      "offset": 1291.679,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "towards that in the Pipecat ecosystem",
      "offset": 1294.159,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "because I think we've built a lot of a",
      "offset": 1295.6,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "lot of we've learned a lot of lessons",
      "offset": 1297.919,
      "duration": 2.801
    },
    {
      "lang": "en",
      "text": "about what that standard needs to look",
      "offset": 1299.36,
      "duration": 2.24
    },
    {
      "lang": "en",
      "text": "like.",
      "offset": 1300.72,
      "duration": 2.959
    },
    {
      "lang": "en",
      "text": "Yeah. Yeah. And I think this kind of",
      "offset": 1301.6,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "goes back to",
      "offset": 1303.679,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "uh something we were talking about",
      "offset": 1305.84,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "earlier which is uh or even uh goes back",
      "offset": 1307.12,
      "duration": 7.039
    },
    {
      "lang": "en",
      "text": "to the story I I mentioned",
      "offset": 1310.72,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "uh with my own experience like when I",
      "offset": 1314.159,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "think when a developer who's not used to",
      "offset": 1315.84,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "living in the voice world hears you know",
      "offset": 1317.84,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "UDP and WebRTC they're like wait I don't",
      "offset": 1319.76,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "usually have to think about that stuff.",
      "offset": 1322.159,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "Uh so talk a little bit about the you",
      "offset": 1324.159,
      "duration": 7.681
    },
    {
      "lang": "en",
      "text": "know the APIs and abstractions uh that",
      "offset": 1327.679,
      "duration": 7.841
    },
    {
      "lang": "en",
      "text": "you know make sense in the voice context",
      "offset": 1331.84,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "you know whether it's you know what",
      "offset": 1335.52,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "you're doing uh with pipecat or you know",
      "offset": 1337.12,
      "duration": 7.039
    },
    {
      "lang": "en",
      "text": "other popular options if they differ and",
      "offset": 1340.48,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "how developers should think about you",
      "offset": 1344.159,
      "duration": 5.361
    },
    {
      "lang": "en",
      "text": "know kind of the way that they suggest",
      "offset": 1346.24,
      "duration": 6.72
    },
    {
      "lang": "en",
      "text": "inter interacting with uh voice LLMs.",
      "offset": 1349.52,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "Yeah, I mean the basic idea that you",
      "offset": 1352.96,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "know you sort of have to use as the",
      "offset": 1355.12,
      "duration": 2.559
    },
    {
      "lang": "en",
      "text": "first building block when you're",
      "offset": 1356.48,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "thinking about writing these voice",
      "offset": 1357.679,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "agents is you've got to move the audio",
      "offset": 1359.36,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "and there's video agents too, but I'll",
      "offset": 1361.6,
      "duration": 2.959
    },
    {
      "lang": "en",
      "text": "I'll just because they're they're",
      "offset": 1363.12,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "earlier in the growth curve and they're",
      "offset": 1364.559,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "super exciting, but let's just talk",
      "offset": 1366.72,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "about audio for for the moment is",
      "offset": 1368.24,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "simpler. Um, you got to move the audio",
      "offset": 1370.08,
      "duration": 6.56
    },
    {
      "lang": "en",
      "text": "from the user's device to the cloud",
      "offset": 1372.72,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "where you're running some piece of code",
      "offset": 1376.64,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "you wrote that takes the audio,",
      "offset": 1378.24,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "processes it however you need to process",
      "offset": 1380.799,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "it, runs one or more inference steps",
      "offset": 1382.4,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "with one or more models,",
      "offset": 1385.12,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "then generates audio at the end of that",
      "offset": 1387.52,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "processing loop, and sends it back to",
      "offset": 1389.919,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "the user, and then does that over and",
      "offset": 1391.52,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "over and over again for every turn in",
      "offset": 1393.76,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "the conversation. managing things like",
      "offset": 1395.44,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "knowing that the user might interrupt",
      "offset": 1398.72,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "the LLM and needing to handle that",
      "offset": 1400.32,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "gracefully or you might even have",
      "offset": 1401.84,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "longunning tool or MCP or function calls",
      "offset": 1403.76,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "that are running in the background and",
      "offset": 1407.039,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "the LLM might actually want to interrupt",
      "offset": 1408.32,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "the user at certain points. So as you",
      "offset": 1410.48,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "start to build these things out and",
      "offset": 1412.72,
      "duration": 2.959
    },
    {
      "lang": "en",
      "text": "production cover more and more use",
      "offset": 1414.24,
      "duration": 2.319
    },
    {
      "lang": "en",
      "text": "cases, you have more and more",
      "offset": 1415.679,
      "duration": 3.041
    },
    {
      "lang": "en",
      "text": "complexity. But the basic idea is move",
      "offset": 1416.559,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "the audio to the cloud because that's",
      "offset": 1418.72,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "where you have the processing power to",
      "offset": 1420.559,
      "duration": 5.281
    },
    {
      "lang": "en",
      "text": "get the best results from inference and",
      "offset": 1422.88,
      "duration": 5.679
    },
    {
      "lang": "en",
      "text": "then move the generated audio back to",
      "offset": 1425.84,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "the user so you can play it out in real",
      "offset": 1428.559,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "time over the speakers or you know",
      "offset": 1430.4,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "headphones that the user is wearing.",
      "offset": 1432.24,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "Uh I think that maybe gets us to",
      "offset": 1434,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "challenges. One of the ones that uh well",
      "offset": 1436.159,
      "duration": 6.88
    },
    {
      "lang": "en",
      "text": "we I guess we were kind of going through",
      "offset": 1440,
      "duration": 6.559
    },
    {
      "lang": "en",
      "text": "challenges maybe doesn't matter. One of",
      "offset": 1443.039,
      "duration": 7.281
    },
    {
      "lang": "en",
      "text": "the things you just mentioned there is",
      "offset": 1446.559,
      "duration": 6.401
    },
    {
      "lang": "en",
      "text": "um like activity detection and",
      "offset": 1450.32,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "interruption handling",
      "offset": 1452.96,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "and",
      "offset": 1455.6,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "uh that is something that I think that",
      "offset": 1457.44,
      "duration": 5.359
    },
    {
      "lang": "en",
      "text": "folks who have tried to use voice AI",
      "offset": 1460.08,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "systems you know whether it's open AAI",
      "offset": 1462.799,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "or Gemini live um you know that strikes",
      "offset": 1464.64,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "me as the biggest like user experience",
      "offset": 1467.919,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "hurdle that we have right now you know",
      "offset": 1469.76,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "curious whether you agree with that but",
      "offset": 1472.32,
      "duration": 6.56
    },
    {
      "lang": "en",
      "text": "also like how you see that evolving. Um,",
      "offset": 1474,
      "duration": 8.72
    },
    {
      "lang": "en",
      "text": "I find that I enjoy using, you know,",
      "offset": 1478.88,
      "duration": 7.12
    },
    {
      "lang": "en",
      "text": "chat GPT advanced voice mode, uh, Gemini",
      "offset": 1482.72,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "Live. Uh, but the conditions have to be",
      "offset": 1486,
      "duration": 6.32
    },
    {
      "lang": "en",
      "text": "fairly perfect for it to not feel like a",
      "offset": 1488.4,
      "duration": 6.399
    },
    {
      "lang": "en",
      "text": "kind of stunted conversation like you",
      "offset": 1492.32,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "know, forget about doing it in the car",
      "offset": 1494.799,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "like it it's very difficult. uh so",
      "offset": 1496.64,
      "duration": 7.519
    },
    {
      "lang": "en",
      "text": "what's the path as an industry for us to",
      "offset": 1500,
      "duration": 6.159
    },
    {
      "lang": "en",
      "text": "uh overcome that is it better models is",
      "offset": 1504.159,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "it better infrastructure is it at the",
      "offset": 1506.159,
      "duration": 5.361
    },
    {
      "lang": "en",
      "text": "API level you know glue or you know",
      "offset": 1508.64,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "pre-processing or something how do we",
      "offset": 1511.52,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "you know get there and you know start",
      "offset": 1513.679,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "starting with do you agree that that's a",
      "offset": 1516.32,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "big hurdle like are you seeing that also",
      "offset": 1517.679,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "or am I just using old models or",
      "offset": 1519.52,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "something",
      "offset": 1521.36,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "so I I think there's an existence proof",
      "offset": 1523.12,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "that you can use LLM",
      "offset": 1526.08,
      "duration": 6.64
    },
    {
      "lang": "en",
      "text": "in conversation very flexibly from the",
      "offset": 1529.36,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "growth of the enterprise voice AI stuff",
      "offset": 1532.72,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "we see. Uh it's a little bit under the",
      "offset": 1535.44,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "radar to people who are building",
      "offset": 1537.919,
      "duration": 3.521
    },
    {
      "lang": "en",
      "text": "consumer stuff or or or just",
      "offset": 1539.679,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "experimenting with the new tech. And I",
      "offset": 1541.44,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "mean that like in the best possible way.",
      "offset": 1542.88,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "But I I have had multiple industry",
      "offset": 1544.96,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "analysts tell me that the the fastest",
      "offset": 1547.84,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "growing Gen AI use cases today from a",
      "offset": 1550.72,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "monetization perspective are programming",
      "offset": 1552.96,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "tools and the second fastest is",
      "offset": 1555.12,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "enterprise voice AI. So there are things",
      "offset": 1557.279,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "like call centers that are now answering",
      "offset": 1559.52,
      "duration": 6.56
    },
    {
      "lang": "en",
      "text": "80% of their calls with voice agents. uh",
      "offset": 1562.08,
      "duration": 5.839
    },
    {
      "lang": "en",
      "text": "financial services companies that where",
      "offset": 1566.08,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "somebody's just taken out a new mortgage",
      "offset": 1567.919,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "and they really really really want to",
      "offset": 1569.84,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "remind people that you know the first",
      "offset": 1571.84,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "mortgage payment is coming up because",
      "offset": 1573.2,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "that's a known failure mode. You taking",
      "offset": 1574.64,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "out a new mortgage, you thought you've",
      "offset": 1576.559,
      "duration": 2.961
    },
    {
      "lang": "en",
      "text": "done all the paperwork to like get your",
      "offset": 1577.76,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "bank account wired up so that you know",
      "offset": 1579.52,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "and you haven't and it's it's it's the",
      "offset": 1581.679,
      "duration": 5.521
    },
    {
      "lang": "en",
      "text": "end user it's the customer's fault if",
      "offset": 1584.64,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "that happens but nobody wants it to",
      "offset": 1587.2,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "happen. Everybody wants that you know",
      "offset": 1588.88,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "mortgage payment to happen seamlessly.",
      "offset": 1590.72,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "So, you know, you didn't have the human",
      "offset": 1593.12,
      "duration": 6.32
    },
    {
      "lang": "en",
      "text": "staff bandwidth to call every single new",
      "offset": 1596.08,
      "duration": 5.839
    },
    {
      "lang": "en",
      "text": "customer 5 days before their first",
      "offset": 1599.44,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "payment is due. Before, you just",
      "offset": 1601.919,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "couldn't do it cost- effectively. Now,",
      "offset": 1603.84,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "you can do that with voice AI. Um, we",
      "offset": 1605.52,
      "duration": 6.32
    },
    {
      "lang": "en",
      "text": "see uh a number of our partners and",
      "offset": 1608.159,
      "duration": 5.441
    },
    {
      "lang": "en",
      "text": "customers doing things like answering",
      "offset": 1611.84,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "the phone for small businesses and they",
      "offset": 1613.6,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "start out answering the phone with an AI",
      "offset": 1615.36,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "agent when the business is closed, when",
      "offset": 1617.6,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "they didn't have anybody answer the",
      "offset": 1620.559,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "phone before. That goes so well after",
      "offset": 1621.919,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "three or four or five months. They're",
      "offset": 1624.64,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "answering the phone all the time. And",
      "offset": 1626.4,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "humans are only picking up the phone",
      "offset": 1628.559,
      "duration": 3.521
    },
    {
      "lang": "en",
      "text": "when you actually really need a human,",
      "offset": 1630,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "which is, you know, 20% or less of the",
      "offset": 1632.08,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "calls usually. So there's just a huge",
      "offset": 1634.88,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "amount of growth in these really working",
      "offset": 1636.64,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "enterprise voice agents. And I think the",
      "offset": 1638.799,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "delta between what we're seeing there on",
      "offset": 1641.44,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "the enterprise side and and what you're",
      "offset": 1643.2,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "seeing on the, you know, kind of chat",
      "offset": 1645.12,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "GPD advanced voice, Gemini live side,",
      "offset": 1646.96,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "which I agree with, is that if you",
      "offset": 1648.88,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "really are strongly incentivized to",
      "offset": 1651.76,
      "duration": 3.279
    },
    {
      "lang": "en",
      "text": "build a product that has a particular",
      "offset": 1653.279,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "surface area that works, you're taking",
      "offset": 1655.039,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "certain approaches. If you're building",
      "offset": 1657.279,
      "duration": 5.361
    },
    {
      "lang": "en",
      "text": "from the models up and your your goal is",
      "offset": 1659.679,
      "duration": 5.041
    },
    {
      "lang": "en",
      "text": "to build the state-of-the-art model and",
      "offset": 1662.64,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "then wrap it in functionality that sort",
      "offset": 1664.72,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "of shows how to use that model, you're",
      "offset": 1666.64,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "doing something very different and your",
      "offset": 1668.799,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "your pain points are different, your",
      "offset": 1670.32,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "timelines are different, your goals are",
      "offset": 1672,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "different. I love what the uh live API",
      "offset": 1673.6,
      "duration": 5.439
    },
    {
      "lang": "en",
      "text": "team and the real-time API team are",
      "offset": 1677.44,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "doing at those two big labs. I also",
      "offset": 1679.039,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "think that if you want the hottake",
      "offset": 1681.76,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "expression of it, those are demos, not",
      "offset": 1683.6,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "products. the version of it you interact",
      "offset": 1686.08,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "with is a demo, not a product. They",
      "offset": 1688.48,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "could be products, but for a whole",
      "offset": 1690.64,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "variety of structural reasons at at at",
      "offset": 1693.12,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "at OpenAI and and Google, they are not",
      "offset": 1694.88,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "products today. It would be interesting",
      "offset": 1697.52,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "to me just from a thought experiment",
      "offset": 1699.84,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "perspective what it would look like if",
      "offset": 1701.679,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "you know either of those companies were",
      "offset": 1703.6,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "super super serious about that product",
      "offset": 1704.88,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "surface area and they might become, but",
      "offset": 1706.96,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "today they're not. So you can solve all",
      "offset": 1709.919,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "those problems with like background",
      "offset": 1712.32,
      "duration": 6.239
    },
    {
      "lang": "en",
      "text": "noise um or interruption handling or",
      "offset": 1714,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "maintaining the context in flexible ways",
      "offset": 1718.559,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "depending on exactly what is happening",
      "offset": 1720.48,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "in the conversation. Those are solvable",
      "offset": 1722.24,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "problems today, but they're products and",
      "offset": 1723.84,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "you have to have a product team that's",
      "offset": 1726.159,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "like working on those problems",
      "offset": 1727.76,
      "duration": 2.399
    },
    {
      "lang": "en",
      "text": "full-time.",
      "offset": 1729.039,
      "duration": 2.481
    },
    {
      "lang": "en",
      "text": "Yeah, that's a super interesting take",
      "offset": 1730.159,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "and I don't think it would surprise",
      "offset": 1731.52,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "anyone, right? Like chat GPT wasn't ever",
      "offset": 1733.039,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "meant to be a product itself, right? is",
      "offset": 1735.36,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "meant to be a demonstration of",
      "offset": 1737.679,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "capability and you can see that",
      "offset": 1738.799,
      "duration": 7.041
    },
    {
      "lang": "en",
      "text": "uh if nothing else like there's open AI",
      "offset": 1741.84,
      "duration": 6.959
    },
    {
      "lang": "en",
      "text": "has a lot more to gain by getting folks",
      "offset": 1745.84,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "excited about the idea of using voice",
      "offset": 1748.799,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "and chewing through a lot of voice",
      "offset": 1751.039,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "tokens than they do necessarily",
      "offset": 1752.72,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "uh you know for investing a lot of money",
      "offset": 1755.279,
      "duration": 5.441
    },
    {
      "lang": "en",
      "text": "in a specific voice product. Now, you",
      "offset": 1757.279,
      "duration": 5.681
    },
    {
      "lang": "en",
      "text": "know, there's always uh a product",
      "offset": 1760.72,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "manager's view on that, like how good",
      "offset": 1762.96,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "does it have to be to really inspire",
      "offset": 1764.64,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "people versus not, but like you said,",
      "offset": 1766.24,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "that's a product decision as opposed to",
      "offset": 1767.84,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "the technology. I think it raises the",
      "offset": 1770.159,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "question that you know their view of the",
      "offset": 1772.48,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "the world and this is something that",
      "offset": 1775.279,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "came up in our conversation with Google",
      "offset": 1777.76,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "as well like is a very much a kind of a",
      "offset": 1779.52,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "you know single big model view of the",
      "offset": 1783.52,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "world as opposed to building modular",
      "offset": 1785.2,
      "duration": 7.599
    },
    {
      "lang": "en",
      "text": "systems and it sounded like one of the",
      "offset": 1788.64,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "distinctions you were making between",
      "offset": 1792.799,
      "duration": 2.721
    },
    {
      "lang": "en",
      "text": "what they're doing and what you might",
      "offset": 1794.24,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "need to do from a product perspective is",
      "offset": 1795.52,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "you know build out a specific subsystem",
      "offset": 1798.399,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "that's looking for background noise or",
      "offset": 1800.96,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "looking for, you know, trying to detect",
      "offset": 1802.799,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "interruptions. Is is that kind of the",
      "offset": 1804.559,
      "duration": 2.48
    },
    {
      "lang": "en",
      "text": "direction you were going?",
      "offset": 1806,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "Yeah, almost all of the production voice",
      "offset": 1807.039,
      "duration": 5.281
    },
    {
      "lang": "en",
      "text": "agents today, you know, especially in",
      "offset": 1810.24,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "the enterprise side are multimodel. So,",
      "offset": 1812.32,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "they've got uh, you know, a a",
      "offset": 1815.12,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "transcription model and then an LLM",
      "offset": 1817.6,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "operating text mode and then a voice",
      "offset": 1819.919,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "generation model. And you've usually got",
      "offset": 1822.159,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "a little dedicated voice activity",
      "offset": 1824.64,
      "duration": 3.279
    },
    {
      "lang": "en",
      "text": "detection model to help you with turn",
      "offset": 1826.48,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "detection. And you might actually have a",
      "offset": 1827.919,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "semantic uh turn detection model as well",
      "offset": 1829.44,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "in that pipeline. If you're an",
      "offset": 1832.32,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "enterprise and you're really concerned",
      "offset": 1834.48,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "about certain kinds of supply compliance",
      "offset": 1836.159,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "and regulatory stuff, you might also",
      "offset": 1837.919,
      "duration": 3.521
    },
    {
      "lang": "en",
      "text": "have a model doing some inference in",
      "offset": 1839.52,
      "duration": 3.519
    },
    {
      "lang": "en",
      "text": "parallel with the main voice",
      "offset": 1841.44,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "conversation pipeline. That's like a",
      "offset": 1843.039,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "guardrails content model. You might be",
      "offset": 1844.88,
      "duration": 5.279
    },
    {
      "lang": "en",
      "text": "doing a bunch of other stuff. So, one of",
      "offset": 1847.679,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "the things I think that distinguishes",
      "offset": 1850.159,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "voice AI from a lot of other use cases",
      "offset": 1851.44,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "is basically every voice AI agent today",
      "offset": 1853.039,
      "duration": 6.161
    },
    {
      "lang": "en",
      "text": "is multi-model as well as multimodal.",
      "offset": 1855.6,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "And that's just a very different",
      "offset": 1859.2,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "architecture from what you know Google",
      "offset": 1860.399,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "and and and OpenAI are pushing towards",
      "offset": 1862.32,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "this this worldview where these",
      "offset": 1864.559,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "incredible state-of-the-art models kind",
      "offset": 1866.559,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "of do everything and they're much less",
      "offset": 1868.48,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "multi-model in their in their",
      "offset": 1870.48,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "philosophy. I think that's actually a",
      "offset": 1872.559,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "really interesting question for all of",
      "offset": 1874.72,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "generative AI. you know that I think all",
      "offset": 1876.32,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "of us who are building these solutions",
      "offset": 1878.72,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "think about at least a little bit which",
      "offset": 1880.08,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "is how much does the future world where",
      "offset": 1881.6,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "we're building this stuff look like",
      "offset": 1883.76,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "we're using those SOTA models and how",
      "offset": 1884.96,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "much are we using you know smaller",
      "offset": 1887.6,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "midsized maybe fine-tuned models or are",
      "offset": 1889.679,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "we sort of doing all of it depending on",
      "offset": 1892.64,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "you know what what we're doing at the",
      "offset": 1894.64,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "moment I think nobody really knows",
      "offset": 1896.399,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "because as you said this techn is",
      "offset": 1898,
      "duration": 3.279
    },
    {
      "lang": "en",
      "text": "evolving so quickly.",
      "offset": 1899.519,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "Yeah. Yeah. I think that theme for folks",
      "offset": 1901.279,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "that listen to the the podcast that",
      "offset": 1904.559,
      "duration": 6.401
    },
    {
      "lang": "en",
      "text": "theme of you know build a system out of",
      "offset": 1906.559,
      "duration": 7.921
    },
    {
      "lang": "en",
      "text": "you know modules uh versus train some",
      "offset": 1910.96,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "endto-end thing with lots of data and uh",
      "offset": 1914.48,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "solve the problem in that way is one",
      "offset": 1917.12,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "that comes up you know quite a bit. The",
      "offset": 1918.64,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "example that comes to mind is autonomous",
      "offset": 1920.72,
      "duration": 7.199
    },
    {
      "lang": "en",
      "text": "vehicles or robotics uh embodied AI uh",
      "offset": 1924.08,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "more broadly because we've got this rich",
      "offset": 1927.919,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "history of kind of physics-based models",
      "offset": 1929.6,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "that uh or you know slam based models in",
      "offset": 1932.559,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "the case of you know autonomous uh",
      "offset": 1936.08,
      "duration": 6.319
    },
    {
      "lang": "en",
      "text": "vehicles that um",
      "offset": 1938.799,
      "duration": 5.521
    },
    {
      "lang": "en",
      "text": "you know can play a role in in the",
      "offset": 1942.399,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "solution and have a lot of interesting",
      "offset": 1944.32,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "you know properties. uh you know but",
      "offset": 1946.32,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "that is you know often put up against",
      "offset": 1948.64,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "the promise of the model being able to",
      "offset": 1950.559,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "figure out things on its own that we",
      "offset": 1953.84,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "can't teach the model you know based on",
      "offset": 1955.36,
      "duration": 6.159
    },
    {
      "lang": "en",
      "text": "you know our own uh view of the world.",
      "offset": 1957.679,
      "duration": 8
    },
    {
      "lang": "en",
      "text": "Uh and so it's interesting to hear that",
      "offset": 1961.519,
      "duration": 7.841
    },
    {
      "lang": "en",
      "text": "in this space as well like that modular",
      "offset": 1965.679,
      "duration": 5.681
    },
    {
      "lang": "en",
      "text": "approach is kind of where folks are",
      "offset": 1969.36,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "building today.",
      "offset": 1971.36,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "And I do think that will change but I",
      "offset": 1972.64,
      "duration": 3.279
    },
    {
      "lang": "en",
      "text": "think it'll change in complicated ways",
      "offset": 1974.48,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "that are are hard to predict. I mean we",
      "offset": 1975.919,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "definitely feel that tension you're",
      "offset": 1977.679,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "talking about every day because the",
      "offset": 1979.279,
      "duration": 5.361
    },
    {
      "lang": "en",
      "text": "speechtoech models from OpenAI and",
      "offset": 1982.48,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "Google are genuinely better at audio",
      "offset": 1984.64,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "understanding and at natural voice",
      "offset": 1987.6,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "output. So, if you're what one thing I",
      "offset": 1989.6,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "often tell people who are asking me for",
      "offset": 1991.84,
      "duration": 2.719
    },
    {
      "lang": "en",
      "text": "advice is if you're building something",
      "offset": 1993.279,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "like a language learning app or a",
      "offset": 1994.559,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "storytelling app for kids, you probably",
      "offset": 1996.96,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "want to use those speech-to-pech models.",
      "offset": 1999.12,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "But if you're building something where",
      "offset": 2001.519,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "you've got to go through a checklist",
      "offset": 2003.039,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "like collect a bunch of information from",
      "offset": 2005.12,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "a healthcare patient before their visit,",
      "offset": 2007.44,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "you really probably want to use the text",
      "offset": 2009.919,
      "duration": 5.921
    },
    {
      "lang": "en",
      "text": "mode LLMs and a multi-model system",
      "offset": 2012.72,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "because you can guide and control and",
      "offset": 2015.84,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "and eval in real time whether you're",
      "offset": 2018.24,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "getting what you need just much much",
      "offset": 2021.2,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "much more reliably.",
      "offset": 2022.88,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "And you know, we've I always said we",
      "offset": 2025.519,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "would never train models at daily",
      "offset": 2028.64,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "because what we do is infrastructure,",
      "offset": 2030.159,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "but I got so frustrated by the turn",
      "offset": 2032,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "detection problem, uh, you know, late",
      "offset": 2034.799,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "last year that when Christmas break came",
      "offset": 2037.2,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "around and I didn't have to, you know,",
      "offset": 2039.519,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "do actual meetings style work all day.",
      "offset": 2041.039,
      "duration": 6.321
    },
    {
      "lang": "en",
      "text": "Um, I trained a version of a of a turn",
      "offset": 2044.799,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "detection model, an audio input turn",
      "offset": 2047.36,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "detection model that came out well",
      "offset": 2049.839,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "enough that we released it and now",
      "offset": 2051.919,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "there's like a pipecat ecosystem around",
      "offset": 2053.599,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "it. And there's a totally, you know,",
      "offset": 2055.28,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "really, really good, totally open",
      "offset": 2057.839,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "source, totally open data, open training",
      "offset": 2059.359,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "code turn detection model. And I fully",
      "offset": 2061.679,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "anticipate that turn detection model",
      "offset": 2064.879,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "will not be useful two years from now",
      "offset": 2066.48,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "because we will have embedded that",
      "offset": 2068.72,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "functionality into these bigger LLMs.",
      "offset": 2070.96,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "But you sure need it now to build",
      "offset": 2073.359,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "something that's kind of best performing",
      "offset": 2075.599,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "conversational dynamic uh agent.",
      "offset": 2078,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "Yeah, that's super interesting. And I",
      "offset": 2080.56,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "I'd like to maybe dig into that in a",
      "offset": 2082.48,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "little bit more detail if only to help",
      "offset": 2085.28,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "folks get a sense for like how these",
      "offset": 2088,
      "duration": 5.839
    },
    {
      "lang": "en",
      "text": "modules fit into a bigger system. So",
      "offset": 2091.2,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "talk a little bit about what are the",
      "offset": 2093.839,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "inputs to this turn detection system,",
      "offset": 2095.28,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "what are the outputs and how those are",
      "offset": 2097.2,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "used in you know orchestrating a an AI",
      "offset": 2098.96,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "flow.",
      "offset": 2102.48,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "Yeah. So the the classic pipeline looks",
      "offset": 2103.359,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "like you've got audio coming in from the",
      "offset": 2106.4,
      "duration": 5.679
    },
    {
      "lang": "en",
      "text": "network connection. You have to chunk",
      "offset": 2109.359,
      "duration": 6.081
    },
    {
      "lang": "en",
      "text": "that audio up into segments because no",
      "offset": 2112.079,
      "duration": 5.361
    },
    {
      "lang": "en",
      "text": "matter what kind of LLM it is today, the",
      "offset": 2115.44,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "LLMs all expect you to ask them to do",
      "offset": 2117.44,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "one thing like at a time. you you have",
      "offset": 2119.839,
      "duration": 5.121
    },
    {
      "lang": "en",
      "text": "to fire inference. And this is another a",
      "offset": 2122.16,
      "duration": 4.959
    },
    {
      "lang": "en",
      "text": "tiny little aside, uh, but another big",
      "offset": 2124.96,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "architecture leap I expect to happen in",
      "offset": 2127.119,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "these LLMs in the near future is yeah,",
      "offset": 2129.44,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "100%. Like birectional streaming all the",
      "offset": 2132.32,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "time. You're always streaming tokens in,",
      "offset": 2135.04,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "you're always streaming tokens out. When",
      "offset": 2136.48,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "you're not when the LM isn't talking,",
      "offset": 2138.64,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "those tokens are like silence tokens or",
      "offset": 2140.96,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "stop tokens or whatever you want to call",
      "offset": 2143.04,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "them. um when the LLM is talking they're",
      "offset": 2144.32,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "you know meaningful tokens but you",
      "offset": 2147.119,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "should always be streaming or thought",
      "offset": 2148.8,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "tokens totally right and and there's",
      "offset": 2150.96,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "some architectural experiments where you",
      "offset": 2153.04,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "actually have multiple output streams",
      "offset": 2154.64,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "you have an audio output stream a text",
      "offset": 2157.28,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "output stream some kind of internal",
      "offset": 2159.2,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "dialogue output stream that's being fed",
      "offset": 2161.359,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "back in all the time so like there is",
      "offset": 2163.92,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "going to be new architecture stuff that",
      "offset": 2166.56,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "changes how we think about these things",
      "offset": 2168.48,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "but today you take that audio you chunk",
      "offset": 2170,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "it so you and start to think about how",
      "offset": 2172.96,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "to feed it to the LLM. You decide those",
      "offset": 2175.44,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "you're you're making those chunks based",
      "offset": 2178.4,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "on trying to decide when the user feels",
      "offset": 2180.079,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "like they're done and they expect the",
      "offset": 2182.32,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "LLM to respond. And that's called turn",
      "offset": 2184.4,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "detection. So the the voice activity",
      "offset": 2186.64,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "detection you're talking about as not",
      "offset": 2190,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "feeling fully natural is today just a",
      "offset": 2191.68,
      "duration": 8.24
    },
    {
      "lang": "en",
      "text": "fixed window of the user is not talking",
      "offset": 2196.48,
      "duration": 6.32
    },
    {
      "lang": "en",
      "text": "anymore. It's like 800 milliseconds. If",
      "offset": 2199.92,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "the user doesn't talk for 800",
      "offset": 2202.8,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "milliseconds, you decide to respond.",
      "offset": 2204.16,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "That is not great because often I pause",
      "offset": 2206.24,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "longer than 800 milliseconds when I'm",
      "offset": 2210,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "trying to figure out what to say to a",
      "offset": 2211.52,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "human or an LLM.",
      "offset": 2213.44,
      "duration": 7.44
    },
    {
      "lang": "en",
      "text": "Well, just people like they have to like",
      "offset": 2216.96,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "look off to the side and figure out what",
      "offset": 2220.88,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "they want to say and come back, right?",
      "offset": 2222.4,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "and and that depending on the",
      "offset": 2223.76,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "conversational flow that can be you know",
      "offset": 2225.44,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "very short or that can be very long even",
      "offset": 2227.52,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "in one sentence even with one person's",
      "offset": 2229.359,
      "duration": 5.921
    },
    {
      "lang": "en",
      "text": "speaking patterns. The thing that I",
      "offset": 2232,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "sorry the thing that I experience the",
      "offset": 2235.28,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "most though I think it's the flip side",
      "offset": 2237.92,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "of that and it is",
      "offset": 2240.079,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "uh maybe overaggressive turn detection.",
      "offset": 2243.52,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "I don't know what it would be like, but",
      "offset": 2245.839,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "it's like the, you know, the advanced",
      "offset": 2246.96,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "voice mode speaking and then it just",
      "offset": 2249.599,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "stops. Like I said something, but I said",
      "offset": 2252.16,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "nothing. It just heard some background",
      "offset": 2254.16,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "noise and it got thrown off track and",
      "offset": 2255.68,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "it's like waiting for me to say",
      "offset": 2257.839,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "something, but it can't figure out that",
      "offset": 2260,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "I'm not saying anything.",
      "offset": 2261.599,
      "duration": 2.48
    },
    {
      "lang": "en",
      "text": "So those two things are linked",
      "offset": 2262.96,
      "duration": 2.879
    },
    {
      "lang": "en",
      "text": "slightly different pro. Okay, talk about",
      "offset": 2264.079,
      "duration": 2.561
    },
    {
      "lang": "en",
      "text": "the linkage. Yeah,",
      "offset": 2265.839,
      "duration": 2.161
    },
    {
      "lang": "en",
      "text": "they're linked because they're",
      "offset": 2266.64,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "implemented in the pipeline by the same",
      "offset": 2268,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "components. And that that's a good call",
      "offset": 2270.16,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "out that maybe they should be more",
      "offset": 2271.839,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "specialized components as we evolve this",
      "offset": 2273.28,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "stuff. But the the the the the beginning",
      "offset": 2275.119,
      "duration": 5.041
    },
    {
      "lang": "en",
      "text": "of almost every voice pipeline is a",
      "offset": 2278.48,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "small specialized model uh called a",
      "offset": 2280.16,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "voice activity detection model. And that",
      "offset": 2282.64,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "voice activity detection model's job is",
      "offset": 2284.96,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "to take you know 30 milliseconds or so",
      "offset": 2286.88,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "of audio and say this looks like human",
      "offset": 2289.599,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "speech or this doesn't look like human",
      "offset": 2292.079,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "speech. It's a classification model. Um,",
      "offset": 2293.839,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "and then you decide you do both turn",
      "offset": 2296.72,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "detection and interruption handling",
      "offset": 2299.04,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "based on that model's classification of",
      "offset": 2300.88,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "those speech frames. So the turn",
      "offset": 2302.96,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "detection would be say there's an 800",
      "offset": 2305.68,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "millisecond gap. That's a turn. The",
      "offset": 2307.76,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "interruption handling would be I got",
      "offset": 2309.92,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "three frames in a row that look like",
      "offset": 2312.32,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "speech. That's an interruption. Um, and",
      "offset": 2313.92,
      "duration": 5.679
    },
    {
      "lang": "en",
      "text": "so you know if that's not tuned exactly",
      "offset": 2316.8,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "right, you cough. it can cause the model",
      "offset": 2319.599,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "to be interrupted or somebody's playing",
      "offset": 2322.64,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "a really loud radio in the next car over",
      "offset": 2325.119,
      "duration": 5.521
    },
    {
      "lang": "en",
      "text": "and the radio announcer is like call K",
      "offset": 2327.92,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "105 now that's an interruption.",
      "offset": 2330.64,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "So the next step in both turn detection",
      "offset": 2333.28,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "and interruption handling is to make",
      "offset": 2335.52,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "those two components more sophisticated.",
      "offset": 2337.359,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "Make them more semantic. Make them more",
      "offset": 2339.68,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "aware that some kinds of background",
      "offset": 2342.079,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "speech are background speech not primary",
      "offset": 2344,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "speech. And there are a bunch of",
      "offset": 2346.72,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "techniques for that. that those that we",
      "offset": 2348.079,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "are making progress on both those",
      "offset": 2350.8,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "problems, but we're definitely not, you",
      "offset": 2352.16,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "know, universally there yet.",
      "offset": 2353.68,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "Yeah, I think it was actually at IO they",
      "offset": 2355.44,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "did a demo where like someone's talking",
      "offset": 2358,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "to a a voice agent and then like someone",
      "offset": 2360.32,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "comes into the room and is like talking",
      "offset": 2363.44,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "to them and it doesn't throw it off at",
      "offset": 2365.119,
      "duration": 5.121
    },
    {
      "lang": "en",
      "text": "all. Um, so that's maybe an existence",
      "offset": 2366.8,
      "duration": 6.4
    },
    {
      "lang": "en",
      "text": "proof of progress there. I don't recall",
      "offset": 2370.24,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "uh what specifically they were doing to",
      "offset": 2373.2,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "enable that. You you may know. So this",
      "offset": 2375.44,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "is another good example of the small",
      "offset": 2377.839,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "model versus big model approach. Both of",
      "offset": 2379.28,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "which are valuable. But the big models",
      "offset": 2381.28,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "are starting to be trained to understand",
      "offset": 2383.76,
      "duration": 6.56
    },
    {
      "lang": "en",
      "text": "interruptions natively and to be able to",
      "offset": 2386.8,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "understand both because if they're",
      "offset": 2390.32,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "multimodal, they have access to the",
      "offset": 2392.48,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "audio and they also have a lot of like",
      "offset": 2394.48,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "semantic understanding of how language",
      "offset": 2396.64,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "works. So when you combine those two",
      "offset": 2398.64,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "things, you ought to be able to tell,",
      "offset": 2401.119,
      "duration": 3.521
    },
    {
      "lang": "en",
      "text": "hey, this is a radio in the background",
      "offset": 2402.72,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "talking. this is not the person I'm",
      "offset": 2404.64,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "talking to talking and ignore everything",
      "offset": 2407.04,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "that's not the person you're talking to.",
      "offset": 2409.28,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "So you can do that with the big model.",
      "offset": 2411.52,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "You can also specially train a small",
      "offset": 2413.2,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "model to try to separate out foreground",
      "offset": 2415.52,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "and background speech. So one of the",
      "offset": 2417.28,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "models I often recommend to people which",
      "offset": 2419.359,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "is extremely good at that is a model by",
      "offset": 2421.68,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "the company Crisp that you may know of",
      "offset": 2423.68,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "KRISP because they have some really good",
      "offset": 2425.52,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "desktop audio processing applications.",
      "offset": 2427.599,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "They also have models that are designed",
      "offset": 2430.4,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "to be run as part of these generative AI",
      "offset": 2432.24,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "workflows to do exactly this kind of",
      "offset": 2434,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "primary speaker isolation. And running",
      "offset": 2436.079,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "those models as part of that initial",
      "offset": 2438.48,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "stage of the pipeline makes a huge",
      "offset": 2440.48,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "difference in enterprise reliability.",
      "offset": 2442,
      "duration": 5.599
    },
    {
      "lang": "en",
      "text": "Okay. And so",
      "offset": 2444.16,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "what if someone is starting and they're",
      "offset": 2447.599,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "listening to what we're saying and",
      "offset": 2449.76,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "uh you know they were thinking that they",
      "offset": 2453.119,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "had this problem to solve and they",
      "offset": 2454.64,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "needed to call an API and now the",
      "offset": 2456.48,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "problem just got a lot bigger because",
      "offset": 2458.4,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "they need all these different components",
      "offset": 2459.76,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "as part of an orchestrated system.",
      "offset": 2462.16,
      "duration": 6.8
    },
    {
      "lang": "en",
      "text": "uh yeah, should they have that fear or",
      "offset": 2466.24,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "are there like templates or something",
      "offset": 2468.96,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "that they can get with Pipecat that like",
      "offset": 2470.72,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "does all of the crap that they don't",
      "offset": 2473.04,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "care about and they could just plug in",
      "offset": 2474.4,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "their thing?",
      "offset": 2475.92,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "Yeah, there's uh various starter kits",
      "offset": 2477.28,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "for different use cases in the Pipcat",
      "offset": 2479.52,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "open source repos that are, you know, 75",
      "offset": 2481.68,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "or something lines of Python code,",
      "offset": 2483.92,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "including all the imports, and have all",
      "offset": 2485.359,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "of these pieces totally standard in the",
      "offset": 2487.28,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "pipeline. and you can just change out",
      "offset": 2489.28,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "the prompt and you've got a working",
      "offset": 2491.599,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "voice agent that you can run locally,",
      "offset": 2493.52,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "you can deploy to the cloud and you can",
      "offset": 2495.92,
      "duration": 2.64
    },
    {
      "lang": "en",
      "text": "start to iterate.",
      "offset": 2497.599,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "Uh you suggested this earlier as we were",
      "offset": 2498.56,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "kind of ticking off challenges, but you",
      "offset": 2501.44,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "know eval is got to be a big one. It's a",
      "offset": 2503.76,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "challenge for folks that are building",
      "offset": 2506.64,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "textbased applications. Now we're, you",
      "offset": 2507.76,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "know, starting to make progress there,",
      "offset": 2510.48,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "but it's a you know, it's an evolving",
      "offset": 2511.92,
      "duration": 6.159
    },
    {
      "lang": "en",
      "text": "practice. What's the state-of-the-art or",
      "offset": 2514.48,
      "duration": 7.04
    },
    {
      "lang": "en",
      "text": "landscape like from a voice perspective?",
      "offset": 2518.079,
      "duration": 6.081
    },
    {
      "lang": "en",
      "text": "Last year almost all of us had only vibe",
      "offset": 2521.52,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "based evals for our voice agents and you",
      "offset": 2524.16,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "know",
      "offset": 2527.28,
      "duration": 2.4
    },
    {
      "lang": "en",
      "text": "sounds like textbased agents actually.",
      "offset": 2527.52,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "Yeah, it kind of does but I think we're",
      "offset": 2529.68,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "probably I mean I think we're probably a",
      "offset": 2531.839,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "little bit you know six months behind or",
      "offset": 2533.599,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "something the the the uh the the",
      "offset": 2535.52,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "textbased agent teams in getting all the",
      "offset": 2537.76,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "way there. Although we're making",
      "offset": 2540,
      "duration": 3.119
    },
    {
      "lang": "en",
      "text": "progress and there's a couple of things",
      "offset": 2541.359,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "that are harder for voice agents about",
      "offset": 2543.119,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "evals. One is they're always multi-turn",
      "offset": 2545.2,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "conversations. Like just the definition",
      "offset": 2547.92,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "of a voice agent is that it's a fairly",
      "offset": 2549.68,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "long multi-turn conversation. Um and the",
      "offset": 2551.839,
      "duration": 5.121
    },
    {
      "lang": "en",
      "text": "other is that whatever your pipeline is,",
      "offset": 2554.72,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "if it's the three kind of uh",
      "offset": 2556.96,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "transcription, LM speech model pipeline",
      "offset": 2559.119,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "or if it's the voicetovoice pipelines",
      "offset": 2561.44,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "from you know the live API or the",
      "offset": 2564.319,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "real-time API, you've got audio in there",
      "offset": 2565.839,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "as well. So you've got this end to end",
      "offset": 2568.319,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "problem that includes not just text but",
      "offset": 2570.079,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "audio. You have to figure out do you",
      "offset": 2571.76,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "just do your eval based on text or do",
      "offset": 2574.079,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "you try to incorporate all the failure",
      "offset": 2576,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "modes that are additional to the text",
      "offset": 2577.44,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "failure modes and audio. Um so those are",
      "offset": 2579.839,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "the things we grapple with kind of",
      "offset": 2582.079,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "uniquely in the voice space. I think",
      "offset": 2583.52,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "it's worth talking a little bit. I'm",
      "offset": 2585.44,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "curious how you're thinking about this",
      "offset": 2586.96,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "because you talked to lots and lots of",
      "offset": 2588.96,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "people. What we have learned in the",
      "offset": 2590.24,
      "duration": 5.119
    },
    {
      "lang": "en",
      "text": "voice space is the multi-turn stuff",
      "offset": 2592.64,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "takes you way out of distribution for",
      "offset": 2595.359,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "the current training data from the big",
      "offset": 2597.28,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "models. And you can look at all the",
      "offset": 2599.28,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "benchmarks for like here's how good",
      "offset": 2601.2,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "instruction following is, here's how",
      "offset": 2602.64,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "good function calling is. Those are",
      "offset": 2604.4,
      "duration": 5.679
    },
    {
      "lang": "en",
      "text": "totally a good uh a good guide to how",
      "offset": 2606.64,
      "duration": 4.959
    },
    {
      "lang": "en",
      "text": "well your agent will perform for the",
      "offset": 2610.079,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "first five turns of the conversation. as",
      "offset": 2611.599,
      "duration": 5.441
    },
    {
      "lang": "en",
      "text": "you get 10, 15, 20 turns deep, those",
      "offset": 2613.92,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "benchmarks just sort of your your actual",
      "offset": 2617.04,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "performance on instruction following",
      "offset": 2619.68,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "function calling falls off a cliff. So",
      "offset": 2621.119,
      "duration": 5.121
    },
    {
      "lang": "en",
      "text": "you almost have to build custom evals in",
      "offset": 2623.04,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "the voice space because you kind of",
      "offset": 2626.24,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "don't have benchmarks. You're kind of",
      "offset": 2627.839,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "out of distribution. Like every agent is",
      "offset": 2630.48,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "just different. I've had people who tell",
      "offset": 2632.64,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "me Gemini 25 Flash just doesn't do what",
      "offset": 2634.24,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "I want to do at all. And other people",
      "offset": 2636.96,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "tell me Gemini 25 Flash is the best",
      "offset": 2638.96,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "model by like a factor of five for my",
      "offset": 2640.96,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "voice agents. Like yeah, I get it. We're",
      "offset": 2643.2,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "just kind of",
      "offset": 2645.52,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "Do you see that in the nonvoice space as",
      "offset": 2647.359,
      "duration": 2.24
    },
    {
      "lang": "en",
      "text": "much?",
      "offset": 2649.119,
      "duration": 1.841
    },
    {
      "lang": "en",
      "text": "I was just going to say I don't think",
      "offset": 2649.599,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "that that's unique to voice. I think um",
      "offset": 2650.96,
      "duration": 7.84
    },
    {
      "lang": "en",
      "text": "you know for a while now the you know",
      "offset": 2655.839,
      "duration": 6.081
    },
    {
      "lang": "en",
      "text": "public benchmarks have become you know",
      "offset": 2658.8,
      "duration": 6.64
    },
    {
      "lang": "en",
      "text": "noisier and noisier with regards to an",
      "offset": 2661.92,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "individual you know engineers's ability",
      "offset": 2665.44,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "to you know get the results they want",
      "offset": 2668,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "for their thing and so you know often",
      "offset": 2669.839,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "times now you know when there's a new",
      "offset": 2674,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "model you know you you look at the",
      "offset": 2676.319,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "model's performance on the benchmarks",
      "offset": 2678.96,
      "duration": 2.879
    },
    {
      "lang": "en",
      "text": "but then you're also going to social",
      "offset": 2680.48,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "media and hearing People talk about like",
      "offset": 2681.839,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "you know their private benchmarks and",
      "offset": 2683.839,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "you know you're running against your own",
      "offset": 2685.839,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "you know whatever your pet problem is or",
      "offset": 2687.599,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "your you know product you know",
      "offset": 2689.52,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "requirements uh are and how you've",
      "offset": 2691.92,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "captured those in a you know eval or",
      "offset": 2693.839,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "benchmark. I I think it's the same",
      "offset": 2696.319,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "across the board. Uh, but it does strike",
      "offset": 2698.8,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "me as being harder, you know, with voice",
      "offset": 2701.04,
      "duration": 5.279
    },
    {
      "lang": "en",
      "text": "for the the reasons that you mentioned.",
      "offset": 2704,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "Like if text is my intermediary, that",
      "offset": 2706.319,
      "duration": 5.361
    },
    {
      "lang": "en",
      "text": "maybe solves a lot of the problems. But",
      "offset": 2709.28,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "if you know for example the problem we",
      "offset": 2711.68,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "were talking about with voice activity",
      "offset": 2713.599,
      "duration": 3.041
    },
    {
      "lang": "en",
      "text": "detection and turn taking like it",
      "offset": 2715.119,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "doesn't necessarily",
      "offset": 2716.64,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "help me evaluate that part of the",
      "offset": 2718.319,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "process unless I'm like end to end",
      "offset": 2720.88,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "feeding some voice in and uh you know",
      "offset": 2722.88,
      "duration": 7.28
    },
    {
      "lang": "en",
      "text": "somehow instrumenting the system so that",
      "offset": 2726.96,
      "duration": 7.359
    },
    {
      "lang": "en",
      "text": "I can you know evaluate yeah even you",
      "offset": 2730.16,
      "duration": 5.679
    },
    {
      "lang": "en",
      "text": "know thinking about like how I might do",
      "offset": 2734.319,
      "duration": 5.361
    },
    {
      "lang": "en",
      "text": "that like uh it's nonobvious not obvious",
      "offset": 2735.839,
      "duration": 6.401
    },
    {
      "lang": "en",
      "text": "um how I would do that end to end as",
      "offset": 2739.68,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "opposed to you know yeah sure you can",
      "offset": 2742.24,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "evaluate a voice activity detector in",
      "offset": 2744.48,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "isolation but um you know as part of an",
      "offset": 2746.8,
      "duration": 5.279
    },
    {
      "lang": "en",
      "text": "endto-end problem it becomes I think a",
      "offset": 2750,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "little bit more interesting",
      "offset": 2752.079,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "yeah and how do you how do you kind of",
      "offset": 2753.44,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "build a success metrics",
      "offset": 2755.839,
      "duration": 5.041
    },
    {
      "lang": "en",
      "text": "rubric when you've got even more moving",
      "offset": 2758.4,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "parts including things like conversation",
      "offset": 2760.88,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "length and you know number of pauses in",
      "offset": 2763.28,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "the conversation were those pauses",
      "offset": 2766.16,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "expected were they not expected number",
      "offset": 2767.599,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "of interruptions in the conversation. Is",
      "offset": 2769.359,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "that good? Is that bad? You really have",
      "offset": 2771.119,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "to build up the intuition. And I and I",
      "offset": 2773.28,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "think this is similar to all val, but",
      "offset": 2775.119,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "you know, your domain is going to be",
      "offset": 2777.119,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "specific for your application always. I",
      "offset": 2778.48,
      "duration": 5.119
    },
    {
      "lang": "en",
      "text": "often tell people get to the point where",
      "offset": 2780.72,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "you feel confident that the agent works",
      "offset": 2783.599,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "based on everything you've been able to",
      "offset": 2786,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "throw at it from a Vibes perspective and",
      "offset": 2787.52,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "then do a little bit of production roll",
      "offset": 2789.92,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "out.",
      "offset": 2792.24,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "But before you do the production roll",
      "offset": 2793.839,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "out, make sure you can capture all the",
      "offset": 2795.52,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "traces, at least capture all the text.",
      "offset": 2797.359,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "And then you will start that data",
      "offset": 2800.16,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "flywheel where you've got enough",
      "offset": 2802,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "captures that you'll be able to just",
      "offset": 2804.4,
      "duration": 4.959
    },
    {
      "lang": "en",
      "text": "kind of manually start to try to build",
      "offset": 2806.319,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "that intuition up about what success is",
      "offset": 2809.359,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "and what what failure modes are. And",
      "offset": 2811.359,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "then you can iterate on that in a bunch",
      "offset": 2814.24,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "of ways, including just do textbased",
      "offset": 2815.68,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "evals for a while. That's totally better",
      "offset": 2818.4,
      "duration": 5.439
    },
    {
      "lang": "en",
      "text": "than nothing. or start to try to either",
      "offset": 2820.319,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "build yourself or leverage some like",
      "offset": 2823.839,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "eval or ops platform tooling or that's",
      "offset": 2825.599,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "more specific to voice which more and",
      "offset": 2827.92,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "more of the eval tooling folks are",
      "offset": 2830,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "starting to build audio support which is",
      "offset": 2833.119,
      "duration": 5.121
    },
    {
      "lang": "en",
      "text": "great. Um there's I think at least half",
      "offset": 2835.76,
      "duration": 5.839
    },
    {
      "lang": "en",
      "text": "a dozen uh pipecat integrations with you",
      "offset": 2838.24,
      "duration": 6.079
    },
    {
      "lang": "en",
      "text": "know good ops and eval tools uh that",
      "offset": 2841.599,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "hopefully make it easier once you get",
      "offset": 2844.319,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "some of that data flowing into the",
      "offset": 2846,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "system to do that kind of end toend",
      "offset": 2847.52,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "analysis you're talking about.",
      "offset": 2848.88,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "Yeah. Yeah. Yeah. Yeah. One of the",
      "offset": 2850.4,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "things that I I find interesting in this",
      "offset": 2852.4,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "conversation and I think it goes back to",
      "offset": 2855.68,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "a conversation I had not too long ago",
      "offset": 2858.16,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "with uh Scott Stevenson who founded Deep",
      "offset": 2860,
      "duration": 6.319
    },
    {
      "lang": "en",
      "text": "Graham. It's I guess I would put it as",
      "offset": 2863.04,
      "duration": 7.36
    },
    {
      "lang": "en",
      "text": "like in in the the context of this",
      "offset": 2866.319,
      "duration": 8.081
    },
    {
      "lang": "en",
      "text": "modular versusn or like modular versus",
      "offset": 2870.4,
      "duration": 7.12
    },
    {
      "lang": "en",
      "text": "single model architecture like text as",
      "offset": 2874.4,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "an intermediary is an observability",
      "offset": 2877.52,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "strategy. Right. It's like you don't",
      "offset": 2879.44,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "necessarily need observability like you",
      "offset": 2882.24,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "don't we don't even know you know unless",
      "offset": 2883.76,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "you're talking about anthropic circuit",
      "offset": 2885.92,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "tracing class things like how to observe",
      "offset": 2887.68,
      "duration": 8.8
    },
    {
      "lang": "en",
      "text": "inside that single yeah multimodal LLM",
      "offset": 2890.64,
      "duration": 10.56
    },
    {
      "lang": "en",
      "text": "like the you get get a lot just by doing",
      "offset": 2896.48,
      "duration": 7.119
    },
    {
      "lang": "en",
      "text": "you know text as an intermediary in",
      "offset": 2901.2,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "terms of being able to evaluate and",
      "offset": 2903.599,
      "duration": 5.121
    },
    {
      "lang": "en",
      "text": "monitor what the system is doing and and",
      "offset": 2906.24,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "enforce some controls. s etc.",
      "offset": 2908.72,
      "duration": 5.839
    },
    {
      "lang": "en",
      "text": "Yeah, you really do. If you have that uh",
      "offset": 2911.44,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "what I would one one way to put it is",
      "offset": 2914.559,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "almost every enterprise use case needs",
      "offset": 2916.88,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "that text for observability for",
      "offset": 2919.04,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "compliance, you know, for other other",
      "offset": 2922,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "reasons. You kind of have to have it. I",
      "offset": 2924.64,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "think it's true that I think it's really",
      "offset": 2926.96,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "interesting to think about the uh ways",
      "offset": 2928.72,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "you could use text and audio together",
      "offset": 2930.72,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "too in consumer applications. I we have",
      "offset": 2932.24,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "been building enough of these things",
      "offset": 2935.44,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "long enough that we've started to have a",
      "offset": 2937.2,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "lot of fun I think and some some",
      "offset": 2939.76,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "opinions about how these evol these",
      "offset": 2941.359,
      "duration": 6.161
    },
    {
      "lang": "en",
      "text": "these um UIs need to evolve. Right. I",
      "offset": 2943.76,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "was having a conversation with one of",
      "offset": 2947.52,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "the big labs people before they released",
      "offset": 2948.96,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "a voice product and they said to me,",
      "offset": 2951.68,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "\"Yeah, nobody wants to see the text. You",
      "offset": 2954.64,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "you know when you're in voice mode, you",
      "offset": 2956.96,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "just want to talk.\" And I was like, \"No,",
      "offset": 2958.4,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "actually not. There's a whole slice of",
      "offset": 2960.72,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "these use cases where what you want is",
      "offset": 2962.88,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "to talk and then you actually primarily",
      "offset": 2964.8,
      "duration": 5.279
    },
    {
      "lang": "en",
      "text": "read and you maybe have the voice on cuz",
      "offset": 2967.2,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "like that's a useful channel and you can",
      "offset": 2970.079,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "look away if you want to but like the",
      "offset": 2971.599,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "mode is audio in text out from a",
      "offset": 2973.52,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "cognitive perspective. And then there",
      "offset": 2975.92,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "are other voice applications where you",
      "offset": 2978.079,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "literally have no way to display the",
      "offset": 2979.68,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "text because I've like called the agent",
      "offset": 2981.68,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "on the phone or whatever. And so there's",
      "offset": 2983.28,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "just this huge spread of use cases. Then",
      "offset": 2985.2,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "as we move towards these kind of next",
      "offset": 2987.04,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "generation UIs, you you have to figure",
      "offset": 2988.88,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "out how to support a huge variety of",
      "offset": 2991.04,
      "duration": 3.519
    },
    {
      "lang": "en",
      "text": "things that people are going to actually",
      "offset": 2993.359,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "want to do and text matters, voice",
      "offset": 2994.559,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "matters, images matter. Increasingly",
      "offset": 2997.04,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "video input and output are super useful",
      "offset": 2999.839,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "modalities. So there's just a ton of new",
      "offset": 3002.48,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "user interface experimentation that I",
      "offset": 3006.319,
      "duration": 5.121
    },
    {
      "lang": "en",
      "text": "that I think we are just barely starting",
      "offset": 3008.96,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "to do.",
      "offset": 3011.44,
      "duration": 2.399
    },
    {
      "lang": "en",
      "text": "You've mentioned video a few times. what",
      "offset": 3012.16,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "are some of the use cases you're",
      "offset": 3013.839,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "starting to see and uh you know what are",
      "offset": 3015.839,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "you excited about in terms of",
      "offset": 3018.4,
      "duration": 2.959
    },
    {
      "lang": "en",
      "text": "opportunities there?",
      "offset": 3020,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "I'm excited about the real time video",
      "offset": 3021.359,
      "duration": 5.841
    },
    {
      "lang": "en",
      "text": "avatar and real-time video scene models",
      "offset": 3023.68,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "getting out of the uncanny valley into a",
      "offset": 3027.2,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "place where they're as good as the",
      "offset": 3029.68,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "real-time voice models we have today. I",
      "offset": 3031.359,
      "duration": 5.521
    },
    {
      "lang": "en",
      "text": "think video dem I mean we have this like",
      "offset": 3033.92,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "progress of technology throughout",
      "offset": 3036.88,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "history it's always like text audio",
      "offset": 3038.88,
      "duration": 6.32
    },
    {
      "lang": "en",
      "text": "video video right and as you add video",
      "offset": 3042.48,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "you know you get sort of this more kind",
      "offset": 3045.2,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "of deeper level of engagement and and uh",
      "offset": 3047.68,
      "duration": 6.56
    },
    {
      "lang": "en",
      "text": "and connection so I'm I'm a big believer",
      "offset": 3050.72,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "that a lot of the things we do with real",
      "offset": 3054.24,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "time AI conversation are going to have a",
      "offset": 3056.4,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "video component I think it's a year or",
      "offset": 3058.72,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "two away before we're all the way there",
      "offset": 3060.64,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "but I think we're starting to see uh",
      "offset": 3062.48,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "from models models from people like",
      "offset": 3064.88,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "Tavis and Lemon Slice uh really",
      "offset": 3066.72,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "interesting",
      "offset": 3070,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "adoption. We're seeing the adoption",
      "offset": 3071.52,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "there I think mostly in things like",
      "offset": 3074.079,
      "duration": 5.601
    },
    {
      "lang": "en",
      "text": "education and corporate training and job",
      "offset": 3077.2,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "interviews but I think we're as the cost",
      "offset": 3079.68,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "comes down and the quality goes up I",
      "offset": 3082.48,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "think we're just going to see a huge",
      "offset": 3084.48,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "amount of social and gaming use cases. I",
      "offset": 3085.76,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "mean, one of one of the thought",
      "offset": 3089.92,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "experiments for me is, you know, what",
      "offset": 3091.28,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "would Tik Tok look like if it wasn't",
      "offset": 3093.28,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "feeding me a bunch of really",
      "offset": 3095.68,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "welltailored to my uh revealed",
      "offset": 3097.839,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "preferences pre-recorded video, but if",
      "offset": 3100.96,
      "duration": 6.32
    },
    {
      "lang": "en",
      "text": "it were generating it? Yeah. 100%. Like",
      "offset": 3103.839,
      "duration": 5.361
    },
    {
      "lang": "en",
      "text": "that that's what the next Tik Tok is",
      "offset": 3107.28,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "going to be, right? Well, we've seen",
      "offset": 3109.2,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "Amazon start to experiment with like",
      "offset": 3111.68,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "this choose your own adventure style of",
      "offset": 3113.599,
      "duration": 5.921
    },
    {
      "lang": "en",
      "text": "production. Uh but you know that's very",
      "offset": 3116.64,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "granular you know it's still very much a",
      "offset": 3119.52,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "traditional production model like if",
      "offset": 3122,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "those things can be generated on the fly",
      "offset": 3124.319,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "that's you know a very different world",
      "offset": 3126.4,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "for for them and for media you know",
      "offset": 3128.24,
      "duration": 5.599
    },
    {
      "lang": "en",
      "text": "production and consumption. One of the",
      "offset": 3131.28,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "first voice AI things we released",
      "offset": 3133.839,
      "duration": 5.841
    },
    {
      "lang": "en",
      "text": "publicly in like 2023 was a choose your",
      "offset": 3136.64,
      "duration": 6.32
    },
    {
      "lang": "en",
      "text": "own adventure voice interactive story",
      "offset": 3139.68,
      "duration": 7.12
    },
    {
      "lang": "en",
      "text": "generator for kids. Um, and it it was a",
      "offset": 3142.96,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "very like clarifying moment for me when",
      "offset": 3146.8,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "we built that and thought it was",
      "offset": 3149.44,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "compelling enough to show other people",
      "offset": 3150.96,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "cuz lots of us have kids uh at Daily and",
      "offset": 3152.88,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "our kids were just like, \"Oh, yeah. No,",
      "offset": 3155.92,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "I would obviously talk to this thing",
      "offset": 3158,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "forever.\" Um, and you you see like you",
      "offset": 3159.28,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "you can see technological progress",
      "offset": 3162.96,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "sometimes best when you see people",
      "offset": 3164.88,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "younger than you take to it in a new",
      "offset": 3166.64,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "way.",
      "offset": 3168.96,
      "duration": 2.32
    },
    {
      "lang": "en",
      "text": "You know, we've talked a little bit",
      "offset": 3170.16,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "about the challenges, you know, from a",
      "offset": 3171.28,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "voice perspective. Like, are those",
      "offset": 3173.2,
      "duration": 5.359
    },
    {
      "lang": "en",
      "text": "challenges kind of the same but more for",
      "offset": 3175.52,
      "duration": 6.799
    },
    {
      "lang": "en",
      "text": "video or does video introduce new",
      "offset": 3178.559,
      "duration": 5.121
    },
    {
      "lang": "en",
      "text": "challenges or what are the new",
      "offset": 3182.319,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "challenges? I'm imagining it's yes and",
      "offset": 3183.68,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "so the single biggest challenge for",
      "offset": 3186.559,
      "duration": 3.121
    },
    {
      "lang": "en",
      "text": "video right now is that it's so much",
      "offset": 3188.4,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "more expensive that the use cases are",
      "offset": 3189.68,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "limited. So that's going to take some",
      "offset": 3191.2,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "time to, you know, kind of push the",
      "offset": 3194,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "video per minute cost down of the",
      "offset": 3196.079,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "basically the GPUs.",
      "offset": 3199.04,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "So infrastructure, that was my question.",
      "offset": 3201.599,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "So I'm sorry, not infrastructure. Uh",
      "offset": 3203.52,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "inference as opposed to transport.",
      "offset": 3206.079,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "That's right.",
      "offset": 3209.2,
      "duration": 2.639
    },
    {
      "lang": "en",
      "text": "And storage and",
      "offset": 3210.16,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "it's the inference. It's it's the GPU",
      "offset": 3211.839,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "time. Um you just can't you can't run",
      "offset": 3214,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "that many simultaneous video generations",
      "offset": 3216.16,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "on a you know H100 or whatever. Um, so",
      "offset": 3218.4,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "you just you just have a lot of GPU",
      "offset": 3222,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "cost. Um, as that comes down, I think",
      "offset": 3223.92,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "that will go away. And then the next",
      "offset": 3226.48,
      "duration": 3.119
    },
    {
      "lang": "en",
      "text": "interesting thing is there sort of all",
      "offset": 3228.4,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "the things we talked about with voice",
      "offset": 3229.599,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "latency matters. Everything is sort of",
      "offset": 3231.28,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "multi-model.",
      "offset": 3233.68,
      "duration": 2.879
    },
    {
      "lang": "en",
      "text": "You have a bunch of stuff going on that",
      "offset": 3235.2,
      "duration": 3.119
    },
    {
      "lang": "en",
      "text": "you've got to orchestrate. That's even",
      "offset": 3236.559,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "more true for video because if you think",
      "offset": 3238.319,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "about video, what you've really got",
      "offset": 3239.839,
      "duration": 5.841
    },
    {
      "lang": "en",
      "text": "going on is a an avatar or more than one",
      "offset": 3241.839,
      "duration": 6.321
    },
    {
      "lang": "en",
      "text": "avatar. There's voice generation,",
      "offset": 3245.68,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "there's body pose, there's facial",
      "offset": 3248.16,
      "duration": 4.959
    },
    {
      "lang": "en",
      "text": "expression. Those are like three",
      "offset": 3250.72,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "different things. Now, maybe it's one",
      "offset": 3253.119,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "model producing those things or maybe",
      "offset": 3254.88,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "it's not, but those are sort of three",
      "offset": 3256.319,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "different things that something is",
      "offset": 3257.68,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "orchestrating. Then there's the scene",
      "offset": 3259.52,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "and the lighting and the camera",
      "offset": 3261.92,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "movement. That's three more things that",
      "offset": 3264.24,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "if you're creating a really dynamic",
      "offset": 3266.559,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "real-time video experience, you at least",
      "offset": 3268.72,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "in you may want to change all of those",
      "offset": 3270.88,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "things dynamically. Um, so you're just",
      "offset": 3273.119,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "sort of layering on more and more like",
      "offset": 3275.28,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "multimodality or or multimodal",
      "offset": 3276.88,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "complexity.",
      "offset": 3279.119,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "That makes me wonder what you've seen in",
      "offset": 3280.24,
      "duration": 6.879
    },
    {
      "lang": "en",
      "text": "terms of pushing inference to the edge.",
      "offset": 3283.68,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "You know, probably not much for video,",
      "offset": 3287.119,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "but like for voice, it seems like we",
      "offset": 3288.88,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "could be close to that. And if that's",
      "offset": 3291.44,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "the case, how does the pipeline or the",
      "offset": 3294.16,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "orchestration need to change to",
      "offset": 3296.16,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "accommodate it? That's a little bit",
      "offset": 3298.24,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "related to the question about whether",
      "offset": 3300.559,
      "duration": 3.121
    },
    {
      "lang": "en",
      "text": "we're going to use these really big",
      "offset": 3302.48,
      "duration": 2.56
    },
    {
      "lang": "en",
      "text": "models for everything or whether going",
      "offset": 3303.68,
      "duration": 2.639
    },
    {
      "lang": "en",
      "text": "to we're going to use a bunch of",
      "offset": 3305.04,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "different models. If you can use",
      "offset": 3306.319,
      "duration": 5.601
    },
    {
      "lang": "en",
      "text": "medium-sized or small models for the the",
      "offset": 3309.44,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "all for everything in the pipeline, you",
      "offset": 3311.92,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "do you can run a bunch of stuff on the",
      "offset": 3313.839,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "edge. Now, like on my, you know, fancy",
      "offset": 3315.52,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "Mac laptop that I paid a bunch of money",
      "offset": 3318.16,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "for, I can actually run a really good",
      "offset": 3319.76,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "local voice agent. I can use uh one of",
      "offset": 3321.68,
      "duration": 6.399
    },
    {
      "lang": "en",
      "text": "the open source transcription models. I",
      "offset": 3325.52,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "can use something like uh Google's Jimma",
      "offset": 3328.079,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "openweights, you know, 27B model or the",
      "offset": 3331.04,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "Quinn 3 series of of of LLMs. And then",
      "offset": 3333.599,
      "duration": 5.361
    },
    {
      "lang": "en",
      "text": "there are several really good open",
      "offset": 3337.28,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "source voice models. Um, and I can just",
      "offset": 3338.96,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "wire those things all up locally with no",
      "offset": 3341.119,
      "duration": 5.121
    },
    {
      "lang": "en",
      "text": "network at all. That's out of reach of",
      "offset": 3343.2,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "most people's devices. You know, the",
      "offset": 3346.24,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "sort of typical laptop can't can't run",
      "offset": 3348.079,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "good enough models to do a good",
      "offset": 3350.559,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "real-time voice agent. The typical phone",
      "offset": 3352.4,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "can't. But, you know, we're 2 3 4 5",
      "offset": 3354.799,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "years from the typical device being good",
      "offset": 3357.599,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "enough and and and being able to run a",
      "offset": 3359.52,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "lot more stuff locally or being able to",
      "offset": 3361.92,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "run parts of the pipeline locally and",
      "offset": 3363.68,
      "duration": 5.119
    },
    {
      "lang": "en",
      "text": "call out to the cloud only when you",
      "offset": 3366.4,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "really need more inference horsepower.",
      "offset": 3368.799,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "And I do think that's the future. I",
      "offset": 3371.44,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "think there's so many advantages to",
      "offset": 3373.28,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "running a bunch of stuff locally that",
      "offset": 3375.44,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "the hybrid pipeline in the way I think",
      "offset": 3376.96,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "about the world the sort of processing",
      "offset": 3380.4,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "pipeline the hybrid pipeline is where",
      "offset": 3381.76,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "we're going to get",
      "offset": 3384.96,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "and is the pipeline",
      "offset": 3386.799,
      "duration": 6.32
    },
    {
      "lang": "en",
      "text": "amenable to that or where are there",
      "offset": 3389.28,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "tight coupling so for example I'm",
      "offset": 3393.119,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "thinking about like voice activity",
      "offset": 3395.28,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "detection like that's probably a small",
      "offset": 3397.92,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "enough model that I could run it on my",
      "offset": 3400.16,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "device But is the latency between that",
      "offset": 3401.92,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "and the cloud where everything else is",
      "offset": 3404.96,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "being processed such, you know, so large",
      "offset": 3406.799,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "that it's going to be that that the",
      "offset": 3409.04,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "signal from the voice activity detector",
      "offset": 3411.76,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "is out of date by the time it gets to",
      "offset": 3413.52,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "the rest of the pipeline. Like those",
      "offset": 3414.96,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "issues, you know, have to be, you know,",
      "offset": 3417.28,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "significant barriers to hybrid",
      "offset": 3420.4,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "pipelines.",
      "offset": 3422.4,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "Where are there specific places where",
      "offset": 3424,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "you see opportunity or, you know, to",
      "offset": 3426.079,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "shift out to the edge?",
      "offset": 3428.64,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "Yeah. No, it's it's a great question and",
      "offset": 3430.24,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "there's no one-sizefits-all answer.",
      "offset": 3432.4,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "Partly because the technology is moving",
      "offset": 3434.24,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "so fast and partly because there's a big",
      "offset": 3435.68,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "diversity of use cases. In some ways,",
      "offset": 3437.28,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "the biggest reason just to do everything",
      "offset": 3440,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "in the cloud today is you really do want",
      "offset": 3441.599,
      "duration": 5.281
    },
    {
      "lang": "en",
      "text": "every if you're doing multiple inference",
      "offset": 3444.72,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "calls, you really want everything to be",
      "offset": 3446.88,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "as close as possible to the inference",
      "offset": 3448.559,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "servers.",
      "offset": 3450.319,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "You don't want to be making multiple",
      "offset": 3451.92,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "round trips from that client to the",
      "offset": 3453.52,
      "duration": 5.599
    },
    {
      "lang": "en",
      "text": "cloud because the worst connection is",
      "offset": 3456.4,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "always going to be the edge device to",
      "offset": 3459.119,
      "duration": 3.281
    },
    {
      "lang": "en",
      "text": "the cloud. The best connection is going",
      "offset": 3460.64,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "to be server to server once you're",
      "offset": 3462.4,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "already in the cloud. So it is it is",
      "offset": 3464.079,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "easier to sort of engineer everything",
      "offset": 3466.559,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "with send the audio and video to the",
      "offset": 3468.24,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "cloud, do whatever inference you need,",
      "offset": 3470,
      "duration": 6.64
    },
    {
      "lang": "en",
      "text": "send audio and video back. But cost,",
      "offset": 3472.16,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "privacy,",
      "offset": 3476.64,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "uh, flexibility definitely motivate",
      "offset": 3478.64,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "towards running pieces of those on",
      "offset": 3482.24,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "device. And even though it's harder, I",
      "offset": 3483.839,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "think it's kind of just engineering to",
      "offset": 3486.24,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "figure out how to build the abstractions",
      "offset": 3488.4,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "that make it pretty easy to build the",
      "offset": 3490.24,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "the pipelines. Yeah. Just more code to",
      "offset": 3491.76,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "write, more orchestration layer code to",
      "offset": 3494,
      "duration": 5.839
    },
    {
      "lang": "en",
      "text": "write. Uh along those lines like you",
      "offset": 3496.319,
      "duration": 7.04
    },
    {
      "lang": "en",
      "text": "know with Vipcat being new uh and this",
      "offset": 3499.839,
      "duration": 6.321
    },
    {
      "lang": "en",
      "text": "technology evolving quickly",
      "offset": 3503.359,
      "duration": 6.96
    },
    {
      "lang": "en",
      "text": "um do the you know codegen models, vibe",
      "offset": 3506.16,
      "duration": 6.88
    },
    {
      "lang": "en",
      "text": "coding platforms and the like do they",
      "offset": 3510.319,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "know about it? Are folks having good",
      "offset": 3513.04,
      "duration": 3.519
    },
    {
      "lang": "en",
      "text": "success like you know building",
      "offset": 3514.799,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "applications using those kind of tools",
      "offset": 3516.559,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "or are there ones that do better than",
      "offset": 3518.16,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "others or you know cloud code and the",
      "offset": 3520.72,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "usual suspects for everything now? Yeah,",
      "offset": 3523.44,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "I mean this is much very much on my mind",
      "offset": 3526,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "because I increasingly see a lot of code",
      "offset": 3528.319,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "in like the pipecat discord that's",
      "offset": 3530.079,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "clearly AI generated. Uh which I think",
      "offset": 3531.839,
      "duration": 6.641
    },
    {
      "lang": "en",
      "text": "is great like we are we we are going to",
      "offset": 3535.04,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "have a new generation of programming",
      "offset": 3538.48,
      "duration": 3.119
    },
    {
      "lang": "en",
      "text": "tools that make all of us more",
      "offset": 3540,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "productive. Um it is challenging partly",
      "offset": 3541.599,
      "duration": 6.321
    },
    {
      "lang": "en",
      "text": "with uh open source stuff that has",
      "offset": 3545.2,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "changed a lot in the year since you know",
      "offset": 3547.92,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "there was a beta and is now stable but",
      "offset": 3550.64,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "there's old versions floating around. I",
      "offset": 3552.799,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "think the coaching tools have trouble.",
      "offset": 3554.88,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "I've also been trying to figure out how",
      "offset": 3557.52,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "do you package up? So I don't think this",
      "offset": 3559.119,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "is a solved problem and I would love to",
      "offset": 3561.2,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "hear from people who have solved it",
      "offset": 3562.88,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "better than we have. There are a bunch",
      "offset": 3564.88,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "of good canonical examples of what code",
      "offset": 3567.44,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "structure and pipecat should look like",
      "offset": 3570.079,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "and they're all in the main repo. They",
      "offset": 3572.16,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "aren't necessarily installed in your",
      "offset": 3574.799,
      "duration": 5.361
    },
    {
      "lang": "en",
      "text": "Python in locally because they're",
      "offset": 3577.44,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "examples. So it's not clear to me how to",
      "offset": 3580.16,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "make cursor and windsurf and claude code",
      "offset": 3583.2,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "know that those are the canonical",
      "offset": 3585.52,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "examples and the project is big enough",
      "offset": 3587.92,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "that none of those tools as far as I can",
      "offset": 3590.559,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "tell today can pull everything into",
      "offset": 3592.48,
      "duration": 7.2
    },
    {
      "lang": "en",
      "text": "context. So the more agentic the tooling",
      "offset": 3594.079,
      "duration": 7.441
    },
    {
      "lang": "en",
      "text": "is the better job it does generating",
      "offset": 3599.68,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "pipecat code. It's like cloud code is",
      "offset": 3601.52,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "pretty good. Vanilla winds surf without",
      "offset": 3603.44,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "a bunch of help which I use every day is",
      "offset": 3605.92,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "not so good. Um, so I would like to",
      "offset": 3608.079,
      "duration": 5.681
    },
    {
      "lang": "en",
      "text": "figure out how do we how do we point a",
      "offset": 3611.04,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "programming tool at like the canonical",
      "offset": 3613.76,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "examples so they're always in the",
      "offset": 3615.76,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "context and like you don't have the",
      "offset": 3617.359,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "mistakes that seem super solvable like",
      "offset": 3619.52,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "the import is wrong. Like if I add",
      "offset": 3622,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "something to the middle of my Python",
      "offset": 3624.16,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "code, it sure seems like the import that",
      "offset": 3625.68,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "gets autogenerated at the top of the",
      "offset": 3628.64,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "file should always be right. But that's",
      "offset": 3630,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "not not true today. Um, and I I feel",
      "offset": 3632.319,
      "duration": 5.841
    },
    {
      "lang": "en",
      "text": "like that's solvable, but not completely",
      "offset": 3635.68,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "solved yet.",
      "offset": 3638.16,
      "duration": 4.959
    },
    {
      "lang": "en",
      "text": "Is part of it a like a conventions.mmd",
      "offset": 3639.119,
      "duration": 7.68
    },
    {
      "lang": "en",
      "text": "file that you have in your repo that you",
      "offset": 3643.119,
      "duration": 5.921
    },
    {
      "lang": "en",
      "text": "can then instruct people like at least",
      "offset": 3646.799,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "until you know that becomes a convention",
      "offset": 3649.04,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "and the tools look for that file, you",
      "offset": 3651.2,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "can tell people to register with their,",
      "offset": 3653.44,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "you know, I know cursor has its version",
      "offset": 3656.16,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "of project files and the other tools do",
      "offset": 3657.68,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "as well. Like is that part of it or does",
      "offset": 3660.16,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "that not fully get you to where you're",
      "offset": 3662.079,
      "duration": 2.081
    },
    {
      "lang": "en",
      "text": "trying to go?",
      "offset": 3663.44,
      "duration": 2.24
    },
    {
      "lang": "en",
      "text": "No, I think you're right. I think that's",
      "offset": 3664.16,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "the approach and maybe it's just",
      "offset": 3665.68,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "somebody needs to take a week and figure",
      "offset": 3668,
      "duration": 5.599
    },
    {
      "lang": "en",
      "text": "out how to make that file or maybe that",
      "offset": 3670,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "MCP server or whatever for Pipcat work",
      "offset": 3673.599,
      "duration": 5.361
    },
    {
      "lang": "en",
      "text": "across all the tools. Um I've hacked,",
      "offset": 3675.76,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "you know, improvements in for my own",
      "offset": 3678.96,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "workflow, but it's clearly not like a",
      "offset": 3680.64,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "packaged",
      "offset": 3682.88,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "thing yet. And it and it really should",
      "offset": 3684.72,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "be. And yeah, maybe it's just somebody",
      "offset": 3686.72,
      "duration": 2.639
    },
    {
      "lang": "en",
      "text": "needs to take a week and sit down and",
      "offset": 3688.079,
      "duration": 2.961
    },
    {
      "lang": "en",
      "text": "make it work across all the all the",
      "offset": 3689.359,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "common AI AI editors.",
      "offset": 3691.04,
      "duration": 5.759
    },
    {
      "lang": "en",
      "text": "You mentioned MCP. uh what are the",
      "offset": 3693.28,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "interaction points or opportunities with",
      "offset": 3696.799,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "regard to MCP and these other agentic",
      "offset": 3699.52,
      "duration": 6.079
    },
    {
      "lang": "en",
      "text": "protocols uh and",
      "offset": 3701.68,
      "duration": 8.399
    },
    {
      "lang": "en",
      "text": "voice AI voice agents generally pipecat",
      "offset": 3705.599,
      "duration": 6.321
    },
    {
      "lang": "en",
      "text": "in particular you know in that whole",
      "offset": 3710.079,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "space like what are you seeing with",
      "offset": 3711.92,
      "duration": 6.159
    },
    {
      "lang": "en",
      "text": "regards to the use of MCP a what have",
      "offset": 3714.16,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "you",
      "offset": 3718.079,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "lots of excitement about MCP uh there's",
      "offset": 3718.799,
      "duration": 5.921
    },
    {
      "lang": "en",
      "text": "native pipecat MCP client support in the",
      "offset": 3721.52,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "in the repo. So you can just sort of add",
      "offset": 3724.72,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "MCP servers and then you know like",
      "offset": 3726.4,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "everything in AI you have to like prompt",
      "offset": 3729.119,
      "duration": 3.521
    },
    {
      "lang": "en",
      "text": "appropriately so that you get what you",
      "offset": 3731.119,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "need from those MCP servers but like",
      "offset": 3732.64,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "then they're just in the pipeline",
      "offset": 3734.48,
      "duration": 5.839
    },
    {
      "lang": "en",
      "text": "meaning so in your pipecat orchestrated",
      "offset": 3736.319,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "workflow",
      "offset": 3740.319,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "that pipecat can call out to an MCP",
      "offset": 3742.079,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "server. So in that sense, Pipcat is like",
      "offset": 3746.079,
      "duration": 5.601
    },
    {
      "lang": "en",
      "text": "a voice AI application server and it's",
      "offset": 3747.839,
      "duration": 6.881
    },
    {
      "lang": "en",
      "text": "calling out to uh you know various MCP",
      "offset": 3751.68,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "servers as a back end. So you don't have",
      "offset": 3754.72,
      "duration": 2.639
    },
    {
      "lang": "en",
      "text": "to wire that up.",
      "offset": 3756.16,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "Exactly. And we built that on top of the",
      "offset": 3757.359,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "native function calling abstractions in",
      "offset": 3759.92,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "Pipcat because I think in general that's",
      "offset": 3762.799,
      "duration": 6.721
    },
    {
      "lang": "en",
      "text": "how MCP servers are accessed by LLM",
      "offset": 3766.16,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "driven workflows, right? Like there's",
      "offset": 3769.52,
      "duration": 2.799
    },
    {
      "lang": "en",
      "text": "different ways to access MCP servers,",
      "offset": 3770.96,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "but if what if what you're doing is",
      "offset": 3772.319,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "talking to an LLM and that LLM is",
      "offset": 3773.76,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "talking to an MCP server, usually what's",
      "offset": 3776.4,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "happening is there's like a tool call",
      "offset": 3778.4,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "set of tool call definitions that are",
      "offset": 3780.24,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "the are the glue. But the difference",
      "offset": 3781.68,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "between statically defining all the tool",
      "offset": 3783.68,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "calls and just defining the tool calls",
      "offset": 3786.24,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "that can access the MCP servers is you",
      "offset": 3788.319,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "have all that beautiful brittle",
      "offset": 3790.319,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "non-determinism that you've talked about",
      "offset": 3792.88,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "in other podcasts, which gets you a long",
      "offset": 3794.559,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "way and gets you more problems too,",
      "offset": 3797.039,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "right? Um, so, so you can sort of just",
      "offset": 3798.64,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "just use MCP servers in a in a Pipecat",
      "offset": 3801.68,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "voice agent. What I usually tell people",
      "offset": 3804.16,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "is don't use an MCP server unless you",
      "offset": 3806.48,
      "duration": 6.96
    },
    {
      "lang": "en",
      "text": "have a very good reason to use MCP for",
      "offset": 3810.64,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "two reasons. One is that",
      "offset": 3813.44,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "non-determinism.",
      "offset": 3814.48,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "Like start with determinism if you know",
      "offset": 3816.24,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "you need something. Move to",
      "offset": 3819.2,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "non-determinism if you have a specific",
      "offset": 3820.559,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "need for non-determinism.",
      "offset": 3822.799,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "Um, the reasons you might want an MCP",
      "offset": 3824.72,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "server are you are building an ecosystem",
      "offset": 3827.599,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "and you want other people to be able to",
      "offset": 3829.52,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "add stuff to your ecosystem. MCP is a",
      "offset": 3831.2,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "very good abstraction for letting other",
      "offset": 3834.079,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "people add stuff into your agent",
      "offset": 3835.76,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "ecosystem. Uh, another reason might be",
      "offset": 3838.24,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "you really do have a specific workflow",
      "offset": 3841.76,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "where that non-determinism is valuable",
      "offset": 3843.92,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "and packaging up a bunch of endpoints",
      "offset": 3846.16,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "into a single MCP server is a lot more",
      "offset": 3848.88,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "kind of maintainable and modular. If",
      "offset": 3851.44,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "what you're trying to do is have the LLM",
      "offset": 3853.359,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "that's driving the conversation pick",
      "offset": 3856.16,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "from a whole bunch of different things",
      "offset": 3857.92,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "to do anyway. But if you just have four",
      "offset": 3858.88,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "or five things you know your agent needs",
      "offset": 3861.119,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "to do, hardcode those tools. Don't wrap",
      "offset": 3862.88,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "them in an MCP server because you're",
      "offset": 3865.44,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "going to get more evaluable, better",
      "offset": 3867.92,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "results, and you're going to get lower",
      "offset": 3870.16,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "latency. It's interesting because I",
      "offset": 3872.319,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "think about it almost the opposite. Um,",
      "offset": 3874.4,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "you said start with determinism and if",
      "offset": 3877.119,
      "duration": 2.881
    },
    {
      "lang": "en",
      "text": "you have a specific need for",
      "offset": 3878.88,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "non-determinism, like then use the the",
      "offset": 3880,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "MCP server. I tend to think of it as",
      "offset": 3882.48,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "like you've got this generic MCP server,",
      "offset": 3885.76,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "use that for your proof of concept, but",
      "offset": 3887.76,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "then you know there's going to be some",
      "offset": 3889.52,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "set subset of all the the tools that",
      "offset": 3891.359,
      "duration": 5.361
    },
    {
      "lang": "en",
      "text": "that thing exposes and you will figure",
      "offset": 3893.599,
      "duration": 5.921
    },
    {
      "lang": "en",
      "text": "out through your you know PC and user",
      "offset": 3896.72,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "interactions like what those are then",
      "offset": 3899.52,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "like take off that outer wrapper of the",
      "offset": 3902.16,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "API and just use APIs.",
      "offset": 3904.24,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "Totally no that that that 100% makes",
      "offset": 3906.72,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "sense. The gloss I would put on top of",
      "offset": 3909.039,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "that for the voice specific development",
      "offset": 3910.96,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "cycle is know that in that first",
      "offset": 3912.96,
      "duration": 5.599
    },
    {
      "lang": "en",
      "text": "iteration you're going to have much much",
      "offset": 3916.16,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "higher latency than you're going to aim",
      "offset": 3918.559,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "for in production. And you're going to",
      "offset": 3920.4,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "you know you're going to get to a point",
      "offset": 3922.88,
      "duration": 2.719
    },
    {
      "lang": "en",
      "text": "where you're like okay I need to bring",
      "offset": 3924.319,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "my latency down from 3 seconds to 1.2",
      "offset": 3925.599,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "seconds. One of my big things I'm going",
      "offset": 3929.119,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "to have to do is rip out as many of",
      "offset": 3931.359,
      "duration": 5.281
    },
    {
      "lang": "en",
      "text": "those MCP uh calls as possible.",
      "offset": 3933.68,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "Okay. Makes sense. Makes sense. And it",
      "offset": 3936.64,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "it calls to mind a question",
      "offset": 3938.319,
      "duration": 8.161
    },
    {
      "lang": "en",
      "text": "um you know in this space like picking",
      "offset": 3941.68,
      "duration": 7.84
    },
    {
      "lang": "en",
      "text": "apart you know what is observability",
      "offset": 3946.48,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "versus you know what is eval versus what",
      "offset": 3949.52,
      "duration": 5.839
    },
    {
      "lang": "en",
      "text": "is test and measurement like you know",
      "offset": 3952.72,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "there are all these different terms but",
      "offset": 3955.359,
      "duration": 6.881
    },
    {
      "lang": "en",
      "text": "I'm envisioning in the voice space in",
      "offset": 3957.76,
      "duration": 6.96
    },
    {
      "lang": "en",
      "text": "particular you know there's a category",
      "offset": 3962.24,
      "duration": 5.599
    },
    {
      "lang": "en",
      "text": "of tool that I might want that shows me",
      "offset": 3964.72,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "like step-by-step latency throughout my",
      "offset": 3967.839,
      "duration": 5.041
    },
    {
      "lang": "en",
      "text": "workflow. Does that exist or is it easy",
      "offset": 3970,
      "duration": 5.599
    },
    {
      "lang": "en",
      "text": "to do by hand or is it something that is",
      "offset": 3972.88,
      "duration": 6.4
    },
    {
      "lang": "en",
      "text": "like lacking and really needed? The",
      "offset": 3975.599,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "Pipecat pipeline will produce metrics",
      "offset": 3979.28,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "frames that show the latency of each",
      "offset": 3982.079,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "step in the pipeline. Uh so it'll give",
      "offset": 3985.52,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "you kind of a good starting point.",
      "offset": 3988.559,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "There's a couple of things that are",
      "offset": 3990.64,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "harder. Like everything uh when you're",
      "offset": 3992.72,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "really trying to dig down and pull out",
      "offset": 3995.44,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "the last bits of latency, there's a",
      "offset": 3997.28,
      "duration": 2.48
    },
    {
      "lang": "en",
      "text": "couple of things. I",
      "offset": 3998.96,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "the leaky parts of the abstraction.",
      "offset": 3999.76,
      "duration": 3.279
    },
    {
      "lang": "en",
      "text": "Yeah, exactly. All the all the",
      "offset": 4001.76,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "abstractions are leaky. All all of the",
      "offset": 4003.039,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "measurements are uh gappy, right?",
      "offset": 4005.359,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "There's always gaps between your",
      "offset": 4008.079,
      "duration": 2.48
    },
    {
      "lang": "en",
      "text": "measurements. So, you got to be aware of",
      "offset": 4009.2,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "those. There's a couple of things that",
      "offset": 4010.559,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "that people should be aware of. One is",
      "offset": 4012.64,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "the network tren the the pipecap",
      "offset": 4015.28,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "pipeline is running somewhere in the",
      "offset": 4017.2,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "cloud. It's giving you the metrics for",
      "offset": 4018.4,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "everything that's running in the cloud.",
      "offset": 4020.16,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "Then you also have that edgeto cloud",
      "offset": 4021.44,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "latency. You can measure that",
      "offset": 4024.799,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "programmatically. It's actually very",
      "offset": 4027.52,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "hard to measure that programmatically in",
      "offset": 4029.2,
      "duration": 5.359
    },
    {
      "lang": "en",
      "text": "any perfect way. So a little bit like",
      "offset": 4031.28,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "you should do eval by hand to build up",
      "offset": 4034.559,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "an intuition. What I always tell people",
      "offset": 4036.48,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "to do is if you are building a",
      "offset": 4038.559,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "production voice agent, record the",
      "offset": 4039.839,
      "duration": 5.681
    },
    {
      "lang": "en",
      "text": "conversation like offline. Like record",
      "offset": 4042.64,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "the conversation from the client side.",
      "offset": 4045.52,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "load that recording up into an audio",
      "offset": 4047.44,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "editor and measure the the silence",
      "offset": 4049.119,
      "duration": 5.361
    },
    {
      "lang": "en",
      "text": "periods in the waveform. Like that is",
      "offset": 4051.92,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "not you can't cheat. You can't get that",
      "offset": 4054.48,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "wrong. Um and one of the things that",
      "offset": 4056.4,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "that will highlight for example is that",
      "offset": 4058.72,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "a bunch of the voice models will produce",
      "offset": 4060.559,
      "duration": 7.04
    },
    {
      "lang": "en",
      "text": "fairly long silence bites at the",
      "offset": 4063.68,
      "duration": 7.2
    },
    {
      "lang": "en",
      "text": "beginning of their voice output and then",
      "offset": 4067.599,
      "duration": 6.561
    },
    {
      "lang": "en",
      "text": "the the text or the the the the speech.",
      "offset": 4070.88,
      "duration": 5.439
    },
    {
      "lang": "en",
      "text": "You don't know what that is if you're",
      "offset": 4074.16,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "just measuring the time to first bite",
      "offset": 4076.319,
      "duration": 5.121
    },
    {
      "lang": "en",
      "text": "from that inference call. You actually",
      "offset": 4079.039,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "have to look at those bites.",
      "offset": 4081.44,
      "duration": 5.599
    },
    {
      "lang": "en",
      "text": "Yeah. And there's good reasons they do",
      "offset": 4085.119,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "that because like if you if you start",
      "offset": 4087.039,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "with silence, you can tune the model to",
      "offset": 4089.52,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "do, you know, much more complex things.",
      "offset": 4091.599,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "It makes me think of like windowing",
      "offset": 4094.079,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "protocols or something like that. Like",
      "offset": 4095.76,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "Yeah, totally. So there's a bunch of",
      "offset": 4098.239,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "little things like that in the pipeline",
      "offset": 4100.48,
      "duration": 2.56
    },
    {
      "lang": "en",
      "text": "that you actually have to sort of dig",
      "offset": 4101.759,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "down and try to measure uh when you're",
      "offset": 4103.04,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "when you're really trying to squeeze the",
      "offset": 4105.44,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "last, you know, 100 milliseconds or so",
      "offset": 4106.799,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "out. But you get a long way with just",
      "offset": 4108.88,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "sort of the standard metrics of how long",
      "offset": 4110.88,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "did the transcription and turn detection",
      "offset": 4112.4,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "take, how long did the LLM inference",
      "offset": 4113.92,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "take to start streaming, how much did",
      "offset": 4115.6,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "you have to buffer before sending to the",
      "offset": 4117.44,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "voice model, what was the time to first",
      "offset": 4119.6,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "bite from the voice model, and you can",
      "offset": 4121.199,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "sort of add those those up and get a",
      "offset": 4123.04,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "pretty good starting point.",
      "offset": 4124.719,
      "duration": 5.361
    },
    {
      "lang": "en",
      "text": "Very cool. Very cool. any um you know if",
      "offset": 4126.4,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "you had to shout out a few interesting",
      "offset": 4130.08,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "like use cases that we haven't talked",
      "offset": 4132.4,
      "duration": 5.279
    },
    {
      "lang": "en",
      "text": "about that you know are inspirational",
      "offset": 4134.48,
      "duration": 6.96
    },
    {
      "lang": "en",
      "text": "you know for you and the community like",
      "offset": 4137.679,
      "duration": 5.361
    },
    {
      "lang": "en",
      "text": "even you know bonus if they're public",
      "offset": 4141.44,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "and folks can play with them but uh",
      "offset": 4143.04,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "perhaps not you know given that the",
      "offset": 4145.679,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "focus is enterprise like you know what's",
      "offset": 4147.52,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "really cool that folks are doing",
      "offset": 4149.759,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "I mean I'll sort of take it out of the",
      "offset": 4151.6,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "enterprise and give a couple of things",
      "offset": 4153.199,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "that I think are super inspiring that",
      "offset": 4154.64,
      "duration": 3.039
    },
    {
      "lang": "en",
      "text": "I've seen a bunch of good work on and",
      "offset": 4156.239,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "and and that I would love to see even",
      "offset": 4157.679,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "more people work on. One is I'm really",
      "offset": 4159.44,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "convinced that AI and education is going",
      "offset": 4161.6,
      "duration": 5.119
    },
    {
      "lang": "en",
      "text": "to be transformative for our world.",
      "offset": 4163.92,
      "duration": 6.96
    },
    {
      "lang": "en",
      "text": "Giving every kid a tutor is additive to",
      "offset": 4166.719,
      "duration": 5.921
    },
    {
      "lang": "en",
      "text": "everything we all do in the classrooms.",
      "offset": 4170.88,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "Not talking about trying to replace, you",
      "offset": 4172.64,
      "duration": 5.119
    },
    {
      "lang": "en",
      "text": "know, the classroom teacher, but just",
      "offset": 4174.719,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "giving every kid self-directed, you",
      "offset": 4177.759,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "know, totally in infinitely patient,",
      "offset": 4180.159,
      "duration": 5.441
    },
    {
      "lang": "en",
      "text": "infinitely sort of scalable one-on-one",
      "offset": 4182.96,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "attention is amazing. And I don't I",
      "offset": 4185.6,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "don't think we're talking enough about",
      "offset": 4188.88,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "the impact on childhood learning and on",
      "offset": 4190.48,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "adult learning, too. I mean, like I use",
      "offset": 4193.199,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "LLMs in that way. Uh, and voice is a big",
      "offset": 4194.64,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "part of that because kids like all of us",
      "offset": 4198,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "are very voice oriented. So sort of",
      "offset": 4200,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "voice driven but not only voice like",
      "offset": 4201.92,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "tutors I think are are really amazing",
      "offset": 4204.4,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "and I would love to see more more people",
      "offset": 4206.4,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "working on that. Um the other thing I'm",
      "offset": 4208.159,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "obsessed with is like what does it look",
      "offset": 4211.28,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "like when we generate UI on the fly. So",
      "offset": 4213.04,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "if I'm having a voice conversation with",
      "offset": 4215.6,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "an application, I want it to write UI",
      "offset": 4217.12,
      "duration": 5.119
    },
    {
      "lang": "en",
      "text": "code and display that UI code",
      "offset": 4220.56,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "dynamically for whatever I'm talking",
      "offset": 4222.239,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "about doing. And I see this increasingly",
      "offset": 4224,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "in how I use programming tools. like I",
      "offset": 4226.32,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "was debugging some like very low-level",
      "offset": 4229.199,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "audio timing thing over the weekend and",
      "offset": 4231.12,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "in the past I would have dumped out very",
      "offset": 4233.44,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "detailed logs and then I would have",
      "offset": 4235.76,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "written some code myself to analyze",
      "offset": 4237.28,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "those logs. Well, what I did all",
      "offset": 4239.28,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "weekend, I didn't write a single line of",
      "offset": 4241.6,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "log analytics code over the weekend. I",
      "offset": 4243.92,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "captured all those logs and I gave them",
      "offset": 4247.04,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "to claude code and said, \"Here's what I",
      "offset": 4248.96,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "think we need to do to look at these",
      "offset": 4251.679,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "logs. Can you do that?\" And it could. it",
      "offset": 4252.88,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "could debug based on very detailed audio",
      "offset": 4255.84,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "timing logs. The next step would have",
      "offset": 4259.04,
      "duration": 6.56
    },
    {
      "lang": "en",
      "text": "been for it to graph and give me ways to",
      "offset": 4261.44,
      "duration": 6.64
    },
    {
      "lang": "en",
      "text": "drill down totally dynamically into",
      "offset": 4265.6,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "those logs. So I would like to see",
      "offset": 4268.08,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "people doing more experimentation with",
      "offset": 4270.32,
      "duration": 8.64
    },
    {
      "lang": "en",
      "text": "on the-ly generated user interfaces. Um,",
      "offset": 4273.76,
      "duration": 8.08
    },
    {
      "lang": "en",
      "text": "Shrista Basu Malikalik, who's the uh PM",
      "offset": 4278.96,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "of of the Gemini APIs, who you and I",
      "offset": 4281.84,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "were both hanging out with at Google IO,",
      "offset": 4284.64,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "and I did a talk at Swix's AI engineer",
      "offset": 4286.8,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "World's Fair with a little bit of",
      "offset": 4289.04,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "autogenerated UI. Um, and I would have",
      "offset": 4291.12,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "liked to do a whole like long workshop",
      "offset": 4293.84,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "on that uh at the World's Fair because I",
      "offset": 4296,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "think that would have been an amazing",
      "offset": 4298,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "workshop. But I think we should all like",
      "offset": 4299.44,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "figure out how to do that at some big",
      "offset": 4301.04,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "event coming up.",
      "offset": 4302.56,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "Oh, that's awesome. That's awesome.",
      "offset": 4304.48,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "Well, Quinn, thanks so much for jumping",
      "offset": 4306.32,
      "duration": 5.839
    },
    {
      "lang": "en",
      "text": "on and sharing a bit about what you and",
      "offset": 4309.04,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "uh the team have been up to. It's very",
      "offset": 4312.159,
      "duration": 5.121
    },
    {
      "lang": "en",
      "text": "cool and very interesting stuff and uh",
      "offset": 4314.239,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "certainly an exciting, you know, point",
      "offset": 4317.28,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "in time and, you know, voice and video",
      "offset": 4319.199,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "and multimodal AI for sure. Well, thanks",
      "offset": 4321.36,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "for joining me for the conversation,",
      "offset": 4324.08,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "having me on. It's always super fun to",
      "offset": 4326,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "listen to you and super fun to get to",
      "offset": 4327.76,
      "duration": 7.16
    },
    {
      "lang": "en",
      "text": "talk to you. Absolutely. Thanks so much.",
      "offset": 4329.6,
      "duration": 5.32
    },
    {
      "lang": "en",
      "text": "You got heat.",
      "offset": 4346.08,
      "duration": 3.159
    }
  ],
  "cleanText": "I think there's an existence proof that you can use LLMs in conversation very flexibly from the growth of the enterprise Voice AI stuff we see. I think the delta between what we're seeing there on the enterprise side and what you're seeing on the, you know, kind of chat GPD advanced voice Gemini live side, if you want the hot take expression of it, those are demos, not products. The version of it you interact with is a demo, not a product. They could be products. But for a whole variety of structural reasons at OpenAI and Google, they are not products today.\n\nAll right, everyone. Welcome to another episode of the Twimmel AI podcast. I am your host, Sam Sharington. Today I'm joined by Kwindla Kramer. Kwindla is co-founder and CEO of Daily and the creator of Pipecat. Before we get going, be sure to take a moment to hit that subscribe button wherever you're listening to today's show. Kwindla, welcome to the podcast.\n\nThank you for having me. I'm a big fan of what you do on the show. Excited to be here.\n\nI appreciate that. I guess this is technically your second time on the show because we did that kind of panel discussion interview at the most recent Google IO, which was a lot of fun. Shout out to Swix from Leighton Space for introducing us and putting that all together. But I think we found a lot of interesting things to kind of talk about and vibe on, and I wanted to dig in a little bit deeper about what you've been up to. For folks that didn't hear or don't know you, that's going to be primarily around Voice AI is what you've been focused on. But, you know, let's give you an opportunity to introduce yourself to the audience.\n\nYeah, I'm Kwindla Kramer. I'm an engineer. I've been doing large-scale real-time network audio and video stuff for most of my career. I co-founded a company called Daily. We make audio and video infrastructure for developers. So, if you're building something like a telehealth app or an education app and you're trying to connect people together, you can use our infrastructure on our SDKs. When GPT4 came out, it started to look to us like not only could computers do all these amazing new things around structured data extraction and kind of open-ended conversation, but that those things felt like maybe you could have humans talking to computers in a new way. So, we built a bunch of stuff, experiment stuff with customers. And I got more and more convinced that Voice AI and real-time Voice AI was a big part of this platform shift we're all excited about. So we open-sourced all the tools we built internally at Daily that became Pipecat, which is now the most widely used voice agent framework, or before 2025, I would have called it an orchestration layer for real-time AI.\n\nThat's awesome. Yeah, it's amazing how quickly these terms are evolving. I find it funny that voice tends to be, um, you know, people either love it or hate it as an idea for the way to interact with AI and computers in general. It has always been fascinating for me and really exciting. Like I don't know if I was in high school or junior high school and was like into dialogic boards and stuff like that. And I was super excited about, you know, Twilio, knew them when they were really early on. And this, you know, idea that we can like control computers and interact with computers via, you know, just via natural speech, I find fascinating. How did you, you know, get into it? What was the spark beyond the kind of business opportunity you saw?\n\nI mean, I am like you. I think it's super interesting to be able to actually talk to a computer and have that be a big component of the user interface. And it seems obvious to me, like I think it does to a lot of people who've been sort of consciously trying to experiment with this, that this platform shift towards generative AI is going to require a bunch of new interface building blocks. We haven't even started to scratch the surface there yet. I am pretty sure that a big, big part of those building blocks is going to be figuring out what voice-first UI looks like. People are really comfortable talking. Even though those of us who like are used to doing this to interact with computers feel maybe a little weird when we talk to our computers, you get over that pretty fast. All of a sudden it opens up this whole sort of efficiency channel that's very, very different from the mouse and the keyboard. We just couldn't do it before because we didn't have a way to take that unstructured conversation, actually turn it into something computers could do something with. But LLMs can totally do that. They're just as good at processing your voice as they are at processing a text stream from your keyboard. The step function difference between typing and voice as input when you add an LLM to the mix is, I think, even bigger. So, you know, I'm encouraging everybody I know to talk to your computer as much as you can. If you're a programmer and you're interested in this stuff, experiment with what a little building block for a voice-first experience looks like because you're totally living in the future when you do that. It frustrates me to no end that every web page that has a form on it doesn't have a microphone that I can press and do it. You know, obviously I've got it, you know, with the various keyboards on the phone, but you have to go into the fields, and I just want something to just take care of that. Let me talk to the web. I have probably had the same conversation maybe, you know, 50 times over the last month or so with both friends and people I'm working with professionally about voice, and it always goes like, and it's always with programmers, and it always goes like this. I say I'm trying to talk to my computer as much as I can, and these days when I'm programming, I talk more than I type. People are like, yeah, I don't see it. Like, I'm not really, like, I don't really like talking to the computer. Also, like, what do I do in my open-plan office? My response to that is twofold. First, I totally hear you. It's a shift. You know, changing a big part of your professional day like that's a big deal. I'm not discounting that at all. But also, we started Daily to do video and audio communication stuff in 2016. I've been doing startups long enough that I was lucky that I only had to pitch investors that I already knew. So, very easy conversations, people who knew me, people who were sort of biased towards, you know, taking what I was saying I wanted to do for a new company seriously. Still, 12 out of 15 of those initial pitch conversations in the 2016 summer of 2016 for Daily went like this. I said, I think we're all going to be doing real-time audio and video all the time on the internet. That's what I'm starting a company around. Professional tech investors would say to me, I don't know, man. Like, I like phone calls. Like, if I want to talk to somebody, I just want to talk on the phone. I don't want to have to set up a video call. I was like, okay, you will. You will.\n\nThat's nuts. That's nuts. Uh, it's been astounding how quickly the technology evolves, or has evolved over the past, you know, and this I guess is also like preaching to the choir. We all have been living this and holding on with like white knuckles. But I remember it couldn't be more than six months ago, like before, or maybe a little more than six months ago, like a little bit before vibe coding was the cool word. I was essentially vibe coding an application that let me, I think at the time I was like trying to track macros. So I was, I like built this app that let me like speak what I ate, you know, with units and quantities and stuff like that. So a little bit more granularity than just take a picture and it would like parse that and then go find the macros and stuff from a database. But I mentioned it because the way that I did the voice was like capture a segment of voice locally and send that into, I think, Gemini or some model, doesn't really matter, to transcribe it and get that back and then like ask it to pull out the quantities. Probably like two weeks after I stopped working on that, the way I would do it like totally changed. OpenAI came out with like the live voice, and you know, all other auto providers have since, you know, come out with their versions of that. Um, and so I think I bring that up to say that, you know, two things, like just to to nod at the way this technology has been evolving, but also to note that, you know, for me, like the okay, I'm going to like capture a recording and send it into an LLM was like very easy cognitively to understand. But then the, you know, the live APIs and like, you know, WebRTC and all this other stuff seem like, you know, if I spent more than a couple minutes like digging into it, like I'm sure it would have been naturally natural and easy to work with, but you know, it was a little bit more complicated. So I wanted to use this opportunity to ask you to give us like a primer on like getting started with voice, like what's the way to think about it as a, or the dominant, like abstractions for building voice AI applications nowadays.\n\nYeah, I have this conversation a lot too because there's so much new interest in voice, and I'll try out my latest version. You can tell me if it lands well. So, because you've got a technical audience, I think it's worth just talking about the stack. Like talking about what the stack is is probably helpful. At the bottom of the stack, you've got the models. So, you've got the weights basically. So whatever those models are, including like the multimodal models you're talking about, like the OpenAI real-time model, the Gemini live model, or you can have, you know, text mode LLMs, and you can do text to speech and speech to text and kind of glue everything together, or you might have a bunch of small fine-tuned LLMs all collaborating. But at the bottom, you got the weights. On top of those, you've got the APIs that the model providers or whatever your inference jack is providing. So like you're hitting an HTTP endpoint from OpenAI or from Google or a websocket endpoint from somebody. Above that, because generally we're trying to do non-trivial things with this technology, we're trying to go beyond building demos. Above the APIs, you've got some kind of orchestration. You're gluing things together. You're implementing the kind of pipelining of data to make it possible to do the multi-turn real-time conversation. Then on top of that, you've got your application code, which is sort of sitting on top of that kind of orchestration layer. So if you put all those things together, you've got a real application. Now you can get started with Voice AI today using a platform where everything is all those things are bundled for you into one kind of interface where you build something, maybe even you just build in a dashboard. You don't even have to write any code. So companies like Vappy have pioneered that all-in-one combined batteries included with really nice dashboards, really nice kind of developer tooling. On the other end of the spectrum, you could build kind of everything yourself. You could make all of those individual choices in the stack yourself. You mix and match. You put everything together kind of as a programmer, you know, writing code with libraries. What I work on a lot is this orchestration layer called Pipecat, which tries to give you a little bit of have your cake and eat it too, where it's easy to get started because the core implementation of things like interruption handling and turn detection and multi-turn context management are all there for you as Python functions basically, but you also have complete control, and you can, you know, mix and match all those parts.\n\nThat's an open-source project.\n\nYeah, totally open source, totally vendor neutral. I spend most of my time these days on Pipecat because it's such an interesting new vector for everything we do in the real-time world. But it is a completely open-source, completely vendor-neutral project. Most of the big labs contribute to Pipecat. Hundreds of startups contribute to Pipecat. There's probably 120 contributors now in the GitHub repo. In thinking about the relationship between Pipecat and Daily, talk a little bit about the overlap just so I can kind of understand. It's not that Daily is commercializing Pipecat. It's that Daily is providing infrastructure for people who are building these applications, and Pipecat just makes it easier to build those applications whether they're hosted on Daily or elsewhere.\n\nYeah, that's exactly right. Many, many more people use Pipecat without Daily than use Pipecat with Daily, which I think is a mark of success for an open-source project. We at Daily are the very low-level network infrastructure. So, we move the audio and video bytes around the network at super high reliability and super low latency. So, anytime you need to do things very fast in a real-time interaction on the internet, we'd love for you to think about using our network infrastructure. But, you know, Pipecat supports lots of different options for network transport. Daily is just one of them. We do increasingly try to help our customers get up and running with production quality Voice AI infrastructure, and we have built a on top of our global infrastructure, we've built a hosting platform for Pipecat or related Voice AI things called Pipecat Cloud, but that's again totally separate from the open-source project, which has no commercial dependencies at all.\n\nSo Pipecat Cloud would be kind of analogous to like Cloudflare and Cloudflare functions, like the cloud is like the the runtime and the Pipecat Cloud would be like the runtime environment, and Daily would be like the level infrastructure. It's not a perfect analogy, but\n\nYeah, I mean, I've been trying to figure out what the best analogies are in a bunch of ways for this new era of like for Voice AI agents.\n\nYeah, the original like web hosting platform that I thought kind of did a really good job like balancing flexibility with with high-level abstractions early on was Heroku. So I sort of think of Pipecat Cloud as as Heroku for Voice AI. But you know, maybe to a technical audience, it's even more clear to just say you push us a Docker container, and we autoscale it and monitor it for you and everything else you you get to choose. So it we're just taking our global infrastructure that's very good at scaling things everywhere in the world, and we're hosting a Docker container that is wired up for ultra low lat\n\n\nency voice AI for you\nrunning on Kubernetes or something else.\nYeah.\nYeah.\nIt's a lot of Kubernetes under the covers, and I'm sure we will talk more about this from a bunch of angles, but there are a lot of things that make voice workloads different from, you know, the HTTP workloads or even the websocket workloads that we all spend a lot of our time building.\nAnd one of those things is you have to have this very low latency network transport, and you have to support long-running conversations.\nAnd we get a lot of people who come to us and say, \"I tried to build this on AWS Lambda, or I tried to build this on GCP cloud run,\" both of which are fantastic platforms, but do not have the components you actually need to support the voice workflows.\nI'm sure they will at some point because this space is growing so fast, but there's just a bunch of things you have to do that are not the normal Kubernetes config to get the voice platform stuff to run, to scale, to have the cold starts be right for voice cold start times and stuff like that.\nYeah, we spend so much time back.\nLet's dig into those challenges because I think that's, uh, in a lot of ways where the rubber meets the road and kind of differentiating, um, you know, web applications as you mentioned from, uh, from voice applications.\nI think, uh, at a high level, the things I talk a lot about with people building, like, going from prototype to production on voice AI today are availability, latency, and then the sort of fundamentally multi-model nature of almost every production voice app.\nUm, and we can take those in order if you want, or you can throw some out that you...\nYeah.\nWell, let's start from, let's start from like lower level, uh, types of concerns, which is latency.\nSo, for example, when you describe the stack, I was wondering, you know, qu... I had questions like, did you have to, you know, write your own container runtime, or is there like a latency optimized container runtime, or like, how much tinkering in the stack, you know, do you have to do?\nUm, to support voice applications, like, you know, custom kernels and all kinds of weird stuff, or because that says what's different between that and just like spinning something up in, you know, Digital Ocean or AWS or wherever.\nSo everything is trade-offs.\nUh, probably earlier in my career, we would have written our own container runtime, um, because there are advantages doing that, but what we did this time was we said, \"Okay, every developer is going to be able to use Docker.\nLet's stick, let's optimize as other places.\nLet's stick to Docker compatibility because that's going to make the onboarding and the growth for, you know, every developer who uses Pipecat Cloud a lot easier.\"\nSo we kept vanilla Docker, but then we have to surround that with a bunch of fairly specialized Kubernetes stuff on a couple levels.\nOne is just all the cold starts and rolling deployment stuff that's specific to voice AI that that you were mentioning.\nUm, so you got to get those Docker containers loaded.\nYou got to get them wired up to UDP networking.\nYou've got to, uh, have, uh, auto, auto, you've got to have schedulers and deployment logic that doesn't terminate half-hour long-running conversations, which, you know, all the all the all the default Kubernetes stuff when you push code, there's fairly short drain times.\nYou have to have long drain times and a bunch of stuff that goes with that.\nThe other layer is you've got to support UDP networking, so you have to support WebRTC, uh, because for edge device to cloud real-time audio, you need to not be using websockets or TCP based protocols.\nYou need to be using UDP based protocols, which the wrapper for those these days is WebRTC.\nUm, so that's a big deal to wire up Kubernetes properly to UDP and do all the routing and be able to start the WebRTC conversations.\nUh, there's a whole bunch of little things you have to kind of customize in Kubernetes.\nAnd so the biggest single thing that you do for latency is you, you get that network layer right with the UDP networking.\nThen on top of that, you just try to, you try to like pull out every tiny bit of extra few milliseconds of latency everywhere in the data processing pipeline.\nUm, which you never really have to worry about doing if you're doing kind of text mode HTTP based inference, 'cause a few tens of milliseconds here and there, like, you don't really notice it.\nBut you really, really notice it in a voice conversation where you're trying to get below a second of voice-to-voice latency.\nI'm curious, in talking about the networking stuff, when we were chatting before, you mentioned that you listened to the, uh, recent episode with Vijoy Pandey from Cisco, and one of the things that he talked about that I found interesting was, uh, this slim protocol that, uh, they're building and promoting or starting to promote.\nUm, any takes on, you know, how that fits into meeting the requirements that you're describing, or are there other efforts like that?\nYeah, I like that direction.\nUh, I'd actually love to talk to Vijoy, uh, because I'm interested in how that work he's doing can play nicely with the stuff we're doing.\nUh, Slim has a bunch of stuff built in that I think is really important.\nI don't think it has UDP support yet.\nSo from my perspective, that would be like a really good thing to add.\nThere aren't any other, uh, real-time oriented transport sort of standards yet other than what we're doing in the Pipecat ecosystem, where we've defined a standard that lets everybody plug into Pipecat.\nI think there will be a real need for a real-time standard.\nThe way I talk about it with the partners we sort of bring into the Pipecat ecosystem is, for better and worse, because standards are always like that.\nThe, uh, OpenAI chat completions HTTP standard became the standard for everybody who does, uh, text-based inference, and now people have built a bunch of stuff on top of that and a bunch of improvements to it.\nBut chat completions is what we all use if we're, say, I, I might use OpenAI, or I might deploy my own model, you know, using VLM or whatever.\nI'm going to use chat completions.\nWe don't yet have that standard for real-time multimedia.\nUh, we need it, and we are definitely working towards that in the Pipecat ecosystem because I think we've built a lot of, a lot of, we've learned a lot of lessons about what that standard needs to look like.\nYeah.\nYeah.\nAnd I think this kind of goes back to, uh, something we were talking about earlier, which is, uh, or even, uh, goes back to the story I, I mentioned, uh, with my own experience, like, when I think when a developer who's not used to living in the voice world hears, you know, UDP and WebRTC, they're like, \"Wait, I don't usually have to think about that stuff.\"\nUh, so talk a little bit about the, you know, the APIs and abstractions, uh, that, you know, make sense in the voice context, you know, whether it's, you know, what you're doing, uh, with Pipecat, or, you know, other popular options if they differ, and how developers should think about, you know, kind of the way that they suggest interacting with, uh, voice LLMs.\nYeah, I mean, the basic idea that, you know, you sort of have to use as the first building block when you're thinking about writing these voice agents is you've got to move the audio, and there's video agents too, but I'll, I'll just, because they're, they're earlier in the growth curve, and they're super exciting, but let's just talk about audio for, for the moment is simpler.\nUm, you got to move the audio from the user's device to the cloud where you're running some piece of code you wrote that takes the audio, processes it however you need to process it, runs one or more inference steps with one or more models, then generates audio at the end of that processing loop, and sends it back to the user, and then does that over and over and over again for every turn in the conversation, managing things like knowing that the user might interrupt the LLM and needing to handle that gracefully, or you might even have long-running tool or MCP or function calls that are running in the background, and the LLM might actually want to interrupt the user at certain points.\nSo as you start to build these things out and production cover more and more use cases, you have more and more complexity.\nBut the basic idea is move the audio to the cloud because that's where you have the processing power to get the best results from inference, and then move the generated audio back to the user so you can play it out in real time over the speakers or, you know, headphones that the user is wearing.\nUh, I think that maybe gets us to challenges.\nOne of the ones that, uh, well, we, I guess we were kind of going through challenges, maybe doesn't matter.\nOne of the things you just mentioned there is, um, like activity detection and interruption handling, and, uh, that is something that I think that folks who have tried to use voice AI systems, you know, whether it's OpenAI or Gemini Live, um, you know, that strikes me as the biggest, like, user experience hurdle that we have right now, you know, curious whether you agree with that, but also, like, how you see that evolving.\nUm, I find that I enjoy using, you know, chat GPT advanced voice mode, uh, Gemini Live.\nUh, but the conditions have to be fairly perfect for it to not feel like a kind of stunted conversation, like, you know, forget about doing it in the car, like, it, it's very difficult.\nUh, so what's the path as an industry for us to, uh, overcome that?\nIs it better models?\nIs it better infrastructure?\nIs it at the API level, you know, glue, or, you know, pre-processing or something?\nHow do we, you know, get there, and, you know, start, starting with, do you agree that that's a big hurdle?\nLike, are you seeing that also, or am I just using old models or something?\nSo I, I think there's an existence proof that you can use LLM in conversation very flexibly from the growth of the enterprise voice AI stuff we see.\nUh, it's a little bit under the radar to people who are building consumer stuff or, or, or just experimenting with the new tech.\nAnd I mean that like in the best possible way.\nBut I, I have had multiple industry analysts tell me that the, the fastest growing Gen AI use cases today from a monetization perspective are programming tools, and the second fastest is enterprise voice AI.\nSo there are things like call centers that are now answering 80% of their calls with voice agents, uh, financial services companies that where somebody's just taken out a new mortgage, and they really, really, really want to remind people that, you know, the first mortgage payment is coming up because that's a known failure mode.\nYou taking out a new mortgage, you thought you've done all the paperwork to like get your bank account wired up so that you know, and you haven't, and it's, it's, it's the end user, it's the customer's fault if that happens, but nobody wants it to happen.\nEverybody wants that, you know, mortgage payment to happen seamlessly.\nSo, you know, you didn't have the human staff bandwidth to call every single new customer 5 days before their first payment is due.\nBefore, you just couldn't do it cost-effectively.\nNow, you can do that with voice AI.\nUm, we see, uh, a number of our partners and customers doing things like answering the phone for small businesses, and they start out answering the phone with an AI agent when the business is closed, when they didn't have anybody answer the phone before.\nThat goes so well after three or four or five months, they're answering the phone all the time.\nAnd humans are only picking up the phone when you actually really need a human, which is, you know, 20% or less of the calls usually.\nSo there's just a huge amount of growth in these really working enterprise voice agents.\nAnd I think the delta between what we're seeing there on the enterprise side and and what you're seeing on the, you know, kind of chat GPD advanced voice, Gemini live side, which I agree with, is that if you really are strongly incentivized to build a product that has a particular surface area that works, you're taking certain approaches.\nIf you're building from the models up and your your goal is to build the state-of-the-art model and then wrap it in functionality that sort of shows how to use that model, you're doing something very different, and your your pain points are different, your timelines are different, your goals are different.\nI love what the, uh, live API team and the real-time API team are doing at those two big labs.\nI also think that if you want the hottake expression of it, those are demos, not products.\nThe version of it you interact with is a demo, not a product.\nThey could be products, but for a whole variety of structural reasons at at at OpenAI and and Google, they are not products today.\nIt would be interesting to me just from a thought experiment perspective what it would look like if, you know, either of those companies were super, super serious about that product surface area, and they might become, but today they're not.\nSo you can solve all those problems with like background noise, um, or interruption handling or maintaining the context in flexible ways depending on exactly what is happening in the conversation.\nThose are solvable problems today, but they're products, and you have to have a product team that's like working on those problems full-time.\nYeah, that's a super interesting take, and I don't think it would surprise anyone, right?\nLike, chat GPT wasn't ever meant to be a product itself, right?\nIt's meant to be a demonstration of capability, and you can see that, uh, if nothing else, like, there's OpenAI has a lot more to gain by getting folks excited about the idea of using voice and chewing through a lot of voice tokens than they do necessarily, uh, you know, for investing a lot of money in a specific voice product.\nNow, you know, there's always, uh, a product manager's view on that, like, how good does it have to be to really inspire people versus not, but like you said, that's a product decision as opposed to the technology.\nI think it raises the question that, you know, their view of the the world, and this is something that came up in our conversation with Google as well, like, is a very much a kind of a, you know, single big model view of the world as opposed to building modular systems, and it sounded like one of the distinctions you were making between what they're doing and what you might need to do from a product perspective is, you know, build out a specific subsystem that's looking for background noise or looking for, you know, trying to detect interruptions.\nIs, is that kind of the direction you were going?\nYeah, almost all of the production voice agents today, you know, especially in the enterprise side, are multi-model.\nSo, they've got, uh, you know, a transcription model and then an LLM operating text mode and then a voice generation model.\nAnd you've usually got a little dedicated voice activity detection model to help you with turn detection.\nAnd you might actually have a semantic, uh, turn detection model as well in that pipeline.\nIf you're an enterprise and you're really concerned about certain kinds of supply compliance and regulatory stuff, you might also have a model doing some inference in\n\n\nParallel with the main voice conversation pipeline. That's like a guardrails content model. You might be doing a bunch of other stuff. So, one of the things I think that distinguishes Voice AI from a lot of other use cases is basically every Voice AI agent today is multi-model as well as multimodal. And that's just a very different architecture from what you know Google and OpenAI are pushing towards this worldview where these incredible state-of-the-art models kind of do everything, and they're much less multi-model in their philosophy. I think that's actually a really interesting question for all of generative AI. You know that I think all of us who are building these solutions think about at least a little bit, which is how much does the future world where we're building this stuff look like we're using those SOTA models, and how much are we using, you know, smaller, midsized, maybe fine-tuned models, or are we sort of doing all of it depending on, you know, what we're doing at the moment? I think nobody really knows because, as you said, this tech is evolving so quickly.\n\nYeah. I think that theme for folks that listen to the podcast, that theme of, you know, build a system out of, you know, modules versus train some end-to-end thing with lots of data and solve the problem in that way, is one that comes up, you know, quite a bit. The example that comes to mind is autonomous vehicles or robotics, embodied AI more broadly, because we've got this rich history of kind of physics-based models that or, you know, SLAM-based models in the case of, you know, autonomous vehicles that um, you know, can play a role in the solution and have a lot of interesting, you know, properties. Uh, you know, but that is, you know, often put up against the promise of the model being able to figure out things on its own that we can't teach the model, you know, based on, you know, our own view of the world. Uh, and so it's interesting to hear that in this space as well, like that modular approach is kind of where folks are building today.\n\nAnd I do think that will change, but I think it'll change in complicated ways that are hard to predict. I mean, we definitely feel that tension you're talking about every day because the speech-to-speech models from OpenAI and Google are genuinely better at audio understanding and at natural voice output. So, if you're, one thing I often tell people who are asking me for advice is, if you're building something like a language learning app or a storytelling app for kids, you probably want to use those speech-to-speech models. But if you're building something where you've got to go through a checklist, like collect a bunch of information from a healthcare patient before their visit, you really probably want to use the text mode LLMs and a multi-model system because you can guide and control and eval in real time whether you're getting what you need just much, much, much more reliably.\n\nAnd, you know, we've I always said we would never train models at Daily because what we do is infrastructure, but I got so frustrated by the turn detection problem, uh, you know, late last year that when Christmas break came around and I didn't have to, you know, do actual meetings-style work all day, um, I trained a version of a turn detection model, an audio input turn detection model that came out well enough that we released it, and now there's like a Pipecat ecosystem around it. And there's a totally, you know, really, really good, totally open source, totally open data, open training code turn detection model. And I fully anticipate that turn detection model will not be useful two years from now because we will have embedded that functionality into these bigger LLMs. But you sure need it now to build something that's kind of best performing conversational dynamic agent.\n\nYeah, that's super interesting. And I I'd like to maybe dig into that in a little bit more detail, if only to help folks get a sense for like how these modules fit into a bigger system. So talk a little bit about what are the inputs to this turn detection system, what are the outputs, and how those are used in, you know, orchestrating an AI flow.\n\nYeah. So the classic pipeline looks like you've got audio coming in from the network connection. You have to chunk that audio up into segments because no matter what kind of LLM it is today, the LLMs all expect you to ask them to do one thing at a time. You have to fire inference. And this is another a tiny little aside, uh, but another big architecture leap I expect to happen in these LLMs in the near future is, yeah, 100%. Like bidirectional streaming all the time. You're always streaming tokens in, you're always streaming tokens out. When you're not when the LM isn't talking, those tokens are like silence tokens or stop tokens or whatever you want to call them. Um, when the LLM is talking, they're, you know, meaningful tokens, but you should always be streaming or thought tokens, totally right. And and there's some architectural experiments where you actually have multiple output streams. You have an audio output stream, a text output stream, some kind of internal dialogue output stream that's being fed back in all the time. So, like, there is going to be new architecture stuff that changes how we think about these things, but today you take that audio, you chunk it, so you and start to think about how to feed it to the LLM. You decide those, you're making those chunks based on trying to decide when the user feels like they're done and they expect the LLM to respond. And that's called turn detection. So the voice activity detection you're talking about as not feeling fully natural is today just a fixed window of the user is not talking anymore. It's like 800 milliseconds. If the user doesn't talk for 800 milliseconds, you decide to respond. That is not great because often I pause longer than 800 milliseconds when I'm trying to figure out what to say to a human or an LLM.\n\nWell, just people like they have to like look off to the side and figure out what they want to say and come back, right? And and that, depending on the conversational flow, that can be, you know, very short or that can be very long, even in one sentence, even with one person's speaking patterns. The thing that I, sorry, the thing that I experience the most though, I think it's the flip side of that, and it is uh, maybe overaggressive turn detection. I don't know what it would be like, but it's like the, you know, the advanced voice mode speaking and then it just stops. Like I said something, but I said nothing. It just heard some background noise and it got thrown off track and it's like waiting for me to say something, but it can't figure out that I'm not saying anything.\n\nSo those two things are linked slightly different pro. Okay, talk about the linkage. Yeah, they're linked because they're implemented in the pipeline by the same components. And that that's a good call out that maybe they should be more specialized components as we evolve this stuff. But the the the the beginning of almost every voice pipeline is a small specialized model, uh, called a voice activity detection model. And that voice activity detection model's job is to take, you know, 30 milliseconds or so of audio and say this looks like human speech or this doesn't look like human speech. It's a classification model. Um, and then you decide, you do both turn detection and interruption handling based on that model's classification of those speech frames. So the turn detection would be, say there's an 800 millisecond gap. That's a turn. The interruption handling would be, I got three frames in a row that look like speech. That's an interruption. Um, and so, you know, if that's not tuned exactly right, you cough. It can cause the model to be interrupted or somebody's playing a really loud radio in the next car over and the radio announcer is like, call K 105 now, that's an interruption.\n\nSo the next step in both turn detection and interruption handling is to make those two components more sophisticated. Make them more semantic. Make them more aware that some kinds of background speech are background speech, not primary speech. And there are a bunch of techniques for that. That those that we are making progress on both those problems, but we're definitely not, you know, universally there yet.\n\nYeah, I think it was actually at IO, they did a demo where like someone's talking to a voice agent and then like someone comes into the room and is like talking to them and it doesn't throw it off at all. Um, so that's maybe an existence proof of progress there. I don't recall uh what specifically they were doing to enable that. You you may know. So this is another good example of the small model versus big model approach, both of which are valuable. But the big models are starting to be trained to understand interruptions natively and to be able to understand both because if they're multimodal, they have access to the audio and they also have a lot of like semantic understanding of how language works. So when you combine those two things, you ought to be able to tell, hey, this is a radio in the background talking. This is not the person I'm talking to talking and ignore everything that's not the person you're talking to. So you can do that with the big model. You can also specially train a small model to try to separate out foreground and background speech. So one of the models I often recommend to people, which is extremely good at that, is a model by the company KRISP that you may know of, KRISP, because they have some really good desktop audio processing applications. They also have models that are designed to be run as part of these generative AI workflows to do exactly this kind of primary speaker isolation. And running those models as part of that initial stage of the pipeline makes a huge difference in enterprise reliability.\n\nOkay. And so, what if someone is starting and they're listening to what we're saying and uh, you know, they were thinking that they had this problem to solve and they needed to call an API, and now the problem just got a lot bigger because they need all these different components as part of an orchestrated system? Uh, yeah, should they have that fear or are there like templates or something that they can get with Pipecat that like does all of the crap that they don't care about and they could just plug in their thing?\n\nYeah, there's uh various starter kits for different use cases in the Pipecat open source repos that are, you know, 75 or something lines of Python code, including all the imports, and have all of these pieces totally standard in the pipeline. And you can just change out the prompt and you've got a working voice agent that you can run locally, you can deploy to the cloud, and you can start to iterate.\n\nUh, you suggested this earlier as we were kind of ticking off challenges, but you know, eval is got to be a big one. It's a challenge for folks that are building text-based applications. Now we're, you know, starting to make progress there, but it's a, you know, it's an evolving practice. What's the state-of-the-art or landscape like from a voice perspective?\n\nLast year almost all of us had only vibe-based evals for our voice agents and you know, sounds like text-based agents actually.\n\nYeah, it kind of does, but I think we're probably, I mean, I think we're probably a little bit, you know, six months behind or something, the the the uh, the text-based agent teams in getting all the way there. Although we're making progress, and there's a couple of things that are harder for voice agents about evals. One is they're always multi-turn conversations. Like just the definition of a voice agent is that it's a fairly long multi-turn conversation. Um, and the other is that whatever your pipeline is, if it's the three kind of uh, transcription, LM speech model pipeline, or if it's the voice-to-voice pipelines from, you know, the Live API or the real-time API, you've got audio in there as well. So you've got this end-to-end problem that includes not just text but audio. You have to figure out, do you just do your eval based on text, or do you try to incorporate all the failure modes that are additional to the text failure modes and audio? Um, so those are the things we grapple with kind of uniquely in the voice space. I think it's worth talking a little bit. I'm curious how you're thinking about this because you talked to lots and lots of people. What we have learned in the voice space is the multi-turn stuff takes you way out of distribution for the current training data from the big models. And you can look at all the benchmarks for, like, here's how good instruction following is, here's how good function calling is. Those are totally a good uh, a good guide to how well your agent will perform for the first five turns of the conversation. As you get 10, 15, 20 turns deep, those benchmarks just sort of your actual performance on instruction following, function calling falls off a cliff. So you almost have to build custom evals in the voice space because you kind of don't have benchmarks. You're kind of out of distribution. Like every agent is just different. I've had people who tell me Gemini 2.5 Flash just doesn't do what I want to do at all. And other people tell me Gemini 2.5 Flash is the best model by like a factor of five for my voice agents. Like, yeah, I get it. We're just kind of\n\nDo you see that in the non-voice space as much?\n\nI was just going to say, I don't think that that's unique to voice. I think, um, you know, for a while now, the, you know, public benchmarks have become, you know, noisier and noisier with regards to an individual, you know, engineer's ability to, you know, get the results they want for their thing. And so, you know, often times now, you know, when there's a new model, you know, you you look at the model's performance on the benchmarks, but then you're also going to social media and hearing people talk about, like, you know, their private benchmarks and, you know, you're running against your own, you know, whatever your pet problem is or your, you know, product, you know, requirements uh, are, and how you've captured those in a, you know, eval or benchmark. I I think it's the same across the board. Uh, but it does strike me as being harder, you know, with voice for the the reasons that you mentioned. Like, if text is my intermediary, that maybe solves a lot of the problems. But if, you know, for example, the problem we were talking about with voice activity detection and turn taking, like, it doesn't necessarily help me evaluate that part of the process unless I'm like end-to-end feeding some voice in and uh, you know, somehow instrumenting the system so that I can, you know, evaluate, yeah, even, you know, thinking about like how I might do that, like, uh, it's nonobvious, not obvious, um, how I would do that end-to-end as opposed to, you know, yeah, sure, you can evaluate a voice activity detector in isolation, but, um, you know, as part of an end-to-end problem, it becomes, I think, a little bit more interesting.\n\nYeah, and how do you, how do you kind of build a success metrics rubric when you've got even more moving parts, including\n\n\nThings like conversation length and, you know, number of pauses in the conversation, were those pauses expected? Were they not expected? Number of interruptions in the conversation. Is that good? Is that bad? You really have to build up the intuition. And I, and I think this is similar to all Voice AI, but, you know, your domain is going to be specific for your application always. I often tell people, get to the point where you feel confident that the agent works based on everything you've been able to throw at it from a Vibes perspective, and then do a little bit of production rollout.\nBut before you do the production rollout, make sure you can capture all the traces, at least capture all the text. And then you will start that data flywheel where you've got enough captures that you'll be able to just kind of manually start to try to build that intuition up about what success is and what failure modes are. And then you can iterate on that in a bunch of ways, including just do text-based evals for a while. That's totally better than nothing, or start to try to either build yourself or leverage some like eval or ops platform tooling, or that's more specific to voice, which more and more of the eval tooling folks are starting to build audio support, which is great. Um, there's, I think, at least half a dozen Pipecat integrations with, you know, good ops and eval tools, uh, that hopefully make it easier once you get some of that data flowing into the system to do that kind of end-to-end analysis you're talking about.\nYeah. Yeah. Yeah. One of the things that I, I find interesting in this conversation, and I think it goes back to a conversation I had not too long ago with, uh, Scott Stephenson, who founded Deep Graham. It's, I guess I would put it as like, in, in the context of this modular versus, or like modular versus single model architecture, like text as an intermediary is an observability strategy. Right. It's like you don't necessarily need observability, like you don't, we don't even know, you know, unless you're talking about anthropic circuit tracing class things, like how to observe inside that single, yeah, multimodal LLM, like the, you get, get a lot just by doing, you know, text as an intermediary in terms of being able to evaluate and monitor what the system is doing and, and enforce some controls, et cetera.\nYeah, you really do. If you have that, uh, what I would, one, one way to put it is, almost every enterprise use case needs that text for observability, for compliance, you know, for other, other reasons. You kind of have to have it. I think it's true that I think it's really interesting to think about the, uh, ways you could use text and audio together, too, in consumer applications. I, we have been building enough of these things long enough that we've started to have a lot of fun, I think, and some, some opinions about how these evol, these, these, um, UIs need to evolve. Right. I was having a conversation with one of the big labs people before they released a voice product, and they said to me, \"Yeah, nobody wants to see the text. You, you know, when you're in voice mode, you just want to talk.\" And I was like, \"No, actually not. There's a whole slice of these use cases where what you want is to talk, and then you actually primarily read, and you maybe have the voice on, 'cause like, that's a useful channel, and you can look away if you want to, but like the mode is audio in, text out from a cognitive perspective. And then there are other voice applications where you literally have no way to display the text because I've like called the agent on the phone or whatever. And so there's just this huge spread of use cases. Then as we move towards these kind of next generation UIs, you, you have to figure out how to support a huge variety of things that people are going to actually want to do, and text matters, voice matters, images matter. Increasingly, video input and output are super useful modalities. So there's just a ton of new user interface experimentation that I, that I think we are just barely starting to do.\nYou've mentioned video a few times. What are some of the use cases you're starting to see and, uh, you know, what are you excited about in terms of opportunities there?\nI'm excited about the real-time video avatar and real-time video scene models getting out of the uncanny valley into a place where they're as good as the real-time voice models we have today. I think video, I mean, we have this like progress of technology throughout history, it's always like text, audio, video, video, right? And as you add video, you know, you get sort of this more kind of deeper level of engagement and, and, uh, and connection. So I'm, I'm a big believer that a lot of the things we do with real-time AI conversation are going to have a video component. I think it's a year or two away before we're all the way there, but I think we're starting to see, uh, from models, models from people like Tavis and Lemon Slice, uh, really interesting adoption. We're seeing the adoption there, I think, mostly in things like education and corporate training and job interviews, but I think we're as the cost comes down and the quality goes up, I think we're just going to see a huge amount of social and gaming use cases. I mean, one of, one of the thought experiments for me is, you know, what would Tik Tok look like if it wasn't feeding me a bunch of really well-tailored to my, uh, revealed preferences pre-recorded video, but if it were generating it? Yeah. 100%. Like, that, that's what the next Tik Tok is going to be, right? Well, we've seen Amazon start to experiment with like this choose your own adventure style of production. Uh, but you know, that's very granular, you know, it's still very much a traditional production model, like if those things can be generated on the fly, that's, you know, a very different world for, for them and for media, you know, production and consumption. One of the first Voice AI things we released publicly in like 2023 was a choose your own adventure voice interactive story generator for kids. Um, and it, it was a very like clarifying moment for me when we built that and thought it was compelling enough to show other people, 'cause lots of us have kids, uh, at Daily, and our kids were just like, \"Oh, yeah. No, I would obviously talk to this thing forever.\" Um, and you, you see, like, you, you can see technological progress sometimes best when you see people younger than you take to it in a new way.\nYou know, we've talked a little bit about the challenges, you know, from a voice perspective. Like, are those challenges kind of the same, but more for video, or does video introduce new challenges, or what are the new challenges? I'm imagining it's yes, and so the single biggest challenge for video right now is that it's so much more expensive that the use cases are limited. So that's going to take some time to, you know, kind of push the video per minute cost down of the, basically the GPUs.\nSo infrastructure, that was my question. So I'm sorry, not infrastructure. Uh, inference as opposed to transport.\nThat's right.\nAnd storage and\nit's the inference. It's, it's the GPU time. Um, you just can't, you can't run that many simultaneous video generations on a, you know, H100 or whatever. Um, so you just, you just have a lot of GPU cost. Um, as that comes down, I think that will go away. And then the next interesting thing is there's sort of all the things we talked about with voice, latency matters. Everything is sort of multi-model.\nYou have a bunch of stuff going on that you've got to orchestrate. That's even more true for video because if you think about video, what you've really got going on is a an avatar or more than one avatar. There's voice generation, there's body pose, there's facial expression. Those are like three different things. Now, maybe it's one model producing those things, or maybe it's not, but those are sort of three different things that something is orchestrating. Then there's the scene and the lighting and the camera movement. That's three more things that if you're creating a really dynamic real-time video experience, you at least, in you may want to change all of those things dynamically. Um, so you're just sort of layering on more and more like multimodality or, or multimodal complexity.\nThat makes me wonder what you've seen in terms of pushing inference to the edge. You know, probably not much for video, but like for voice, it seems like we could be close to that. And if that's the case, how does the pipeline or the orchestration need to change to accommodate it? That's a little bit related to the question about whether we're going to use these really big models for everything or whether going to, we're going to use a bunch of different models. If you can use medium-sized or small models for the, the all for everything in the pipeline, you do, you can run a bunch of stuff on the edge. Now, like on my, you know, fancy Mac laptop that I paid a bunch of money for, I can actually run a really good local voice agent. I can use, uh, one of the open source transcription models. I can use something like, uh, Google's Jimma openweights, you know, 27B model or the Quinn 3 series of, of, of LLMs. And then there are several really good open source voice models. Um, and I can just wire those things all up locally with no network at all. That's out of reach of most people's devices. You know, the sort of typical laptop can't, can't run good enough models to do a good real-time voice agent. The typical phone can't. But, you know, we're 2, 3, 4, 5 years from the typical device being good enough and, and, and being able to run a lot more stuff locally or being able to run parts of the pipeline locally and call out to the cloud only when you really need more inference horsepower. And I do think that's the future. I think there's so many advantages to running a bunch of stuff locally that the hybrid pipeline, in the way I think about the world, the sort of processing pipeline, the hybrid pipeline is where we're going to get.\nAnd is the pipeline amenable to that, or where are there tight coupling? So, for example, I'm thinking about like voice activity detection, like that's probably a small enough model that I could run it on my device. But is the latency between that and the cloud where everything else is being processed such, you know, so large that it's going to be that, that the signal from the voice activity detector is out of date by the time it gets to the rest of the pipeline? Like those issues, you know, have to be, you know, significant barriers to hybrid pipelines.\nWhere are there specific places where you see opportunity or, you know, to shift out to the edge?\nYeah. No, it's, it's a great question, and there's no one-size-fits-all answer. Partly because the technology is moving so fast, and partly because there's a big diversity of use cases. In some ways, the biggest reason just to do everything in the cloud today is you really do want every, if you're doing multiple inference calls, you really want everything to be as close as possible to the inference servers.\nYou don't want to be making multiple round trips from that client to the cloud because the worst connection is always going to be the edge device to the cloud. The best connection is going to be server to server once you're already in the cloud. So it is, it is easier to sort of engineer everything with send the audio and video to the cloud, do whatever inference you need, send audio and video back. But cost, privacy, uh, flexibility definitely motivate towards running pieces of those on device. And even though it's harder, I think it's kind of just engineering to figure out how to build the abstractions that make it pretty easy to build the, the pipelines. Yeah. Just more code to write, more orchestration layer code to write. Uh, along those lines, like, you know, with Pipecat being new, uh, and this technology evolving quickly, um, do the, you know, codegen models, vibe coding platforms and the like, do they know about it? Are folks having good success, like, you know, building applications using those kind of tools, or are there ones that do better than others, or, you know, cloud code and the usual suspects for everything now? Yeah, I mean, this is much, very much on my mind because I increasingly see a lot of code in like the Pipecat Discord that's clearly AI generated, uh, which I think is great, like, we are, we, we are going to have a new generation of programming tools that make all of us more productive. Um, it is challenging, partly with, uh, open source stuff that has changed a lot in the year since, you know, there was a beta and is now stable, but there's old versions floating around. I think the coaching tools have trouble.\nI've also been trying to figure out how do you package up? So I don't think this is a solved problem, and I would love to hear from people who have solved it better than we have. There are a bunch of good canonical examples of what code structure and Pipecat should look like, and they're all in the main repo. They aren't necessarily installed in your Python in locally because they're examples. So it's not clear to me how to make cursor and windsurf and Claude code know that those are the canonical examples, and the project is big enough that none of those tools, as far as I can tell today, can pull everything into context. So the more agentic the tooling is, the better job it does generating Pipecat code. It's like cloud code is pretty good. Vanilla winds surf without a bunch of help, which I use every day, is not so good. Um, so I would like to figure out how do we, how do we point a programming tool at like the canonical examples so they're always in the context, and like, you don't have the mistakes that seem super solvable, like the import is wrong. Like if I add something to the middle of my Python code, it sure seems like the import that gets autogenerated at the top of the file should always be right. But that's not, not true today. Um, and I, I feel like that's solvable, but not completely solved yet.\nIs part of it a like a conventions.mmd file that you have in your repo that you can then instruct people, like, at least until you know that becomes a convention and the tools look for that file, you can tell people to register with their, you know, I know cursor has its version of project files and the other tools do as well. Like, is that part of it, or does that not fully get you to where you're trying to go?\nNo, I think you're right. I think that's the approach, and maybe it's just somebody needs to take a week and figure out how to make that file, or maybe that MCP server or whatever for Pipecat work across all the tools. Um, I've hacked, you know, improvements in for my own workflow, but it's clearly not like a packaged thing yet. And it, and it really should be. And yeah, maybe it's just somebody needs to take a week and sit down and make it work across all the, all the common AI, AI editors.\n\n\nYou mentioned MCP.\nUh, what are the interaction points or opportunities with regard to MCP and these other agentic protocols, uh, and Voice AI voice agents generally, Pipecat in particular?\nYou know, in that whole space, like, what are you seeing with regards to the use of MCP?\nA, what have you?\nLots of excitement about MCP.\nUh, there's native Pipecat MCP client support in the in the repo.\nSo you can just sort of add MCP servers and then, you know, like everything in AI, you have to like prompt appropriately so that you get what you need from those MCP servers, but like then they're just in the pipeline, meaning, so in your Pipecat orchestrated workflow, that Pipecat can call out to an MCP server.\nSo in that sense, Pipcat is like a Voice AI application server, and it's calling out to, uh, you know, various MCP servers as a back end.\nSo you don't have to wire that up.\nExactly.\nAnd we built that on top of the native function calling abstractions in Pipcat because I think in general that's how MCP servers are accessed by LLM driven workflows, right?\nLike there's different ways to access MCP servers, but if what if what you're doing is talking to an LLM and that LLM is talking to an MCP server, usually what's happening is there's like a tool call, set of tool call definitions that are the are the glue.\nBut the difference between statically defining all the tool calls and just defining the tool calls that can access the MCP servers is you have all that beautiful brittle non-determinism that you've talked about in other podcasts, which gets you a long way and gets you more problems too, right?\nUm, so, so you can sort of just use MCP servers in a in a Pipecat voice agent.\nWhat I usually tell people is don't use an MCP server unless you have a very good reason to use MCP for two reasons.\nOne is that non-determinism.\nLike start with determinism if you know you need something.\nMove to non-determinism if you have a specific need for non-determinism.\nUm, the reasons you might want an MCP server are you are building an ecosystem and you want other people to be able to add stuff to your ecosystem.\nMCP is a very good abstraction for letting other people add stuff into your agent ecosystem.\nUh, another reason might be you really do have a specific workflow where that non-determinism is valuable and packaging up a bunch of endpoints into a single MCP server is a lot more kind of maintainable and modular.\nIf what you're trying to do is have the LLM that's driving the conversation pick from a whole bunch of different things to do anyway.\nBut if you just have four or five things you know your agent needs to do, hardcode those tools.\nDon't wrap them in an MCP server because you're going to get more evaluable, better results, and you're going to get lower latency.\nIt's interesting because I think about it almost the opposite.\nUm, you said start with determinism and if you have a specific need for non-determinism, like then use the the MCP server.\nI tend to think of it as like you've got this generic MCP server, use that for your proof of concept, but then you know there's going to be some set subset of all the the tools that that thing exposes and you will figure out through your you know PC and user interactions like what those are then like take off that outer wrapper of the API and just use APIs.\nTotally, no, that that that 100% makes sense.\nThe gloss I would put on top of that for the voice specific development cycle is know that in that first iteration you're going to have much, much higher latency than you're going to aim for in production.\nAnd you're going to, you know, you're going to get to a point where you're like, okay, I need to bring my latency down from 3 seconds to 1.2 seconds.\nOne of my big things I'm going to have to do is rip out as many of those MCP, uh, calls as possible.\nOkay.\nMakes sense.\nMakes sense.\nAnd it it calls to mind a question, um, you know, in this space, like picking apart, you know, what is observability versus, you know, what is eval versus what is test and measurement?\nLike, you know, there are all these different terms, but I'm envisioning in the voice space in particular, you know, there's a category of tool that I might want that shows me like step-by-step latency throughout my workflow.\nDoes that exist or is it easy to do by hand or is it something that is like lacking and really needed?\nThe Pipecat pipeline will produce metrics frames that show the latency of each step in the pipeline.\nUh, so it'll give you kind of a good starting point.\nThere's a couple of things that are harder.\nLike everything, uh, when you're really trying to dig down and pull out the last bits of latency, there's a couple of things.\nI the leaky parts of the abstraction.\nYeah, exactly.\nAll the all the abstractions are leaky.\nAll all of the measurements are, uh, gappy, right?\nThere's always gaps between your measurements.\nSo, you got to be aware of those.\nThere's a couple of things that that people should be aware of.\nOne is the network tren the the pipecap pipeline is running somewhere in the cloud.\nIt's giving you the metrics for everything that's running in the cloud.\nThen you also have that edge-to-cloud latency.\nYou can measure that programmatically.\nIt's actually very hard to measure that programmatically in any perfect way.\nSo a little bit like you should do eval by hand to build up an intuition.\nWhat I always tell people to do is if you are building a production voice agent, record the conversation like offline.\nLike record the conversation from the client side.\nLoad that recording up into an audio editor and measure the the silence periods in the waveform.\nLike that is not you can't cheat.\nYou can't get that wrong.\nUm, and one of the things that that will highlight, for example, is that a bunch of the voice models will produce fairly long silence bites at the beginning of their voice output and then the the text or the the the the speech.\nYou don't know what that is if you're just measuring the time to first bite from that inference call.\nYou actually have to look at those bites.\nYeah.\nAnd there's good reasons they do that because like if you if you start with silence, you can tune the model to do, you know, much more complex things.\nIt makes me think of like windowing protocols or something like that.\nLike Yeah, totally.\nSo there's a bunch of little things like that in the pipeline that you actually have to sort of dig down and try to measure, uh, when you're when you're really trying to squeeze the last, you know, 100 milliseconds or so out.\nBut you get a long way with just sort of the standard metrics of how long did the transcription and turn detection take, how long did the LLM inference take to start streaming, how much did you have to buffer before sending to the voice model, what was the time to first bite from the voice model, and you can sort of add those those up and get a pretty good starting point.\nVery cool.\nVery cool.\nAny, um, you know, if you had to shout out a few interesting, like, use cases that we haven't talked about that you know are inspirational, you know, for you and the community, like, even, you know, bonus if they're public and folks can play with them, but, uh, perhaps not, you know, given that the focus is enterprise, like, you know, what's really cool that folks are doing?\nI mean, I'll sort of take it out of the enterprise and give a couple of things that I think are super inspiring that I've seen a bunch of good work on and and and that I would love to see even more people work on.\nOne is I'm really convinced that AI and education is going to be transformative for our world.\nGiving every kid a tutor is additive to everything we all do in the classrooms.\nNot talking about trying to replace, you know, the classroom teacher, but just giving every kid self-directed, you know, totally in infinitely patient, infinitely sort of scalable one-on-one attention is amazing.\nAnd I don't I don't think we're talking enough about the impact on childhood learning and on adult learning, too.\nI mean, like I use LLMs in that way.\nUh, and voice is a big part of that because kids like all of us are very voice oriented.\nSo sort of voice driven but not only voice like tutors I think are are really amazing and I would love to see more more people working on that.\nUm, the other thing I'm obsessed with is like what does it look like when we generate UI on the fly.\nSo if I'm having a voice conversation with an application, I want it to write UI code and display that UI code dynamically for whatever I'm talking about doing.\nAnd I see this increasingly in how I use programming tools.\nLike I was debugging some like very low-level audio timing thing over the weekend and in the past I would have dumped out very detailed logs and then I would have written some code myself to analyze those logs.\nWell, what I did all weekend, I didn't write a single line of log analytics code over the weekend.\nI captured all those logs and I gave them to Claude code and said, \"Here's what I think we need to do to look at these logs.\nCan you do that?\"\nAnd it could.\nIt could debug based on very detailed audio timing logs.\nThe next step would have been for it to graph and give me ways to drill down totally dynamically into those logs.\nSo I would like to see people doing more experimentation with on-the-fly generated user interfaces.\nUm, Shrista Basu Malikalik, who's the uh PM of of the Gemini APIs, who you and I were both hanging out with at Google IO, and I did a talk at Swix's AI engineer World's Fair with a little bit of autogenerated UI.\nUm, and I would have liked to do a whole like long workshop on that, uh, at the World's Fair because I think that would have been an amazing workshop.\nBut I think we should all like figure out how to do that at some big event coming up.\nOh, that's awesome.\nThat's awesome.\nWell, Quinn, thanks so much for jumping on and sharing a bit about what you and, uh, the team have been up to.\nIt's very cool and very interesting stuff and, uh, certainly an exciting, you know, point in time and, you know, voice and video and multimodal AI for sure.\nWell, thanks for joining me for the conversation, having me on.\nIt's always super fun to listen to you and super fun to get to talk to you.\nAbsolutely.\nThanks so much.\nYou got heat.\n",
  "dumpedAt": "2025-07-21T18:43:24.931Z"
}